[{"issue_title": "The whisper-large-v3 model randomly misses sentences during recognition when return_timestamps=\"word\"", "tags": ["Natural Language Processing", "Speech Recognition", "Transformer Models", "PyTorch", "TensorFlow", "JAX", "Hugging Face", "Open Source", "Machine Learning", "Deep Learning", "Python", "NLP Libraries", "Computer Vision", "Audio Processing", "Sequence-to-Sequence", "BERT", "RoBERTa", "DistilBERT", "ELECTRA", "DeBERTa", "mBART", "mT5", "T5", "BART", "Pegasus", "BARTpho", "BERTweet", "BORT", "ConvBERT", "ConvNeXT", "CvT", "Data2Vec", "DePlot", "Depth Anything", "DETA", "DETR", "DialoGPT", "DiNAT", "DINOv2", "DistilBERT", "DiT", "Donut", "DPR", "DPT", "EfficientFormer", "EfficientNet", "FNet", "FocalNet", "Funnel Transformer", "Fuyu", "Gemma", "GIT", "GLPN", "GPT", "GPT-2", "GPT-J", "GPT-Neo", "GPT-NeoX", "GPT-NeoX-Japanese", "GPT-Sw3", "GPTBigCode", "GPTSAN-Japanese", "Graphormer", "GroupViT", "HerBERT", "Hubert", "I-BERT", "IDEFICS", "ImageGPT", "Informer", "InstructBLIP", "Jukebox", "KOSMOS-2", "LayoutLM", "LayoutLMv2", "LayoutLMv3", "LayoutXLM", "LED", "LeViT", "LiLT"], "summary": "\ud83e\udd17 Transformers is a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow, providing thousands of pretrained models for natural language processing, computer vision, and audio tasks. It offers a unified API for using all pretrained models, allowing users to quickly download and use them on given tasks, fine-tune them on their own datasets, and share them with the community. The library supports multiple frameworks, enabling seamless integration between them."}, {"issue_title": "llava-next does not support batched processing/generation when batched images are not of same size", "tags": ["computer-vision", "nlp", "audio", "multimodal", "transformers", "pytorch", "tensorflow", "flax", "jax", "machine-learning", "deep-learning", "natural-language-processing", "computer-vision-models", "audio-processing", "multimodal-models", "pretrained-models", "huggingface", "open-source", "nlp-library", "deep-learning-library", "machine-learning-library"], "summary": "\ud83e\udd17 Transformers is a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow, providing thousands of pretrained models for natural language processing, computer vision, and audio tasks. It offers a unified API for using all pretrained models, allowing users to quickly download and use them on given tasks, fine-tune them on their own datasets, and share them with the community. The library supports multiple frameworks, enabling seamless integration between them."}, {"issue_title": "About new ClearML Intergrations", "tags": ["Natural Language Processing", "Deep Learning", "Transformers", "PyTorch", "TensorFlow", "JAX", "Hugging Face", "Pretrained Models", "NLP Applications", "Computer Vision", "Speech Recognition", "Multimodal Learning", "Open Source", "Machine Learning", "NLP Toolkit", "NLP Research", "NLP Community"], "summary": "\ud83e\udd17 Transformers is a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow, providing thousands of pretrained models for natural language processing, computer vision, and audio tasks. It offers a unified API for using all pretrained models, with a seamless integration between the three deep learning frameworks. The library also includes `pipeline` for quickly using a model on a given input and `Trainer` API for fine-tuning on a new dataset."}, {"issue_title": "Mixture of All Intelligence (MoAI)", "tags": ["Natural Language Processing", "Computer Vision", "Audio Processing", "Multimodal Learning", "Transformer Models", "PyTorch", "TensorFlow", "JAX", "Pretrained Models", "Fine-tuning", "Natural Language Understanding", "Natural Language Generation", "Vision Language Tasks", "Open Source", "Hugging Face", "Machine Learning", "Deep Learning", "NLP Toolkit", "NLP Library", "Vision Transformers"], "summary": "\ud83e\udd17 Transformers is a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow, providing thousands of pretrained models for natural language processing, computer vision, and audio tasks. It offers a unified API for using all pretrained models, allowing users to quickly download and use them on given tasks, fine-tune them on their own datasets, and share them with the community. The library supports multiple frameworks, enabling seamless integration between them."}, {"issue_title": "Move weight initialization for DeformableDetr", "tags": ["Python", "Natural Language Processing", "Computer Vision", "Audio", "Multimodal tasks", "Transformers", "Deep Learning", "Jax", "PyTorch", "TensorFlow", "Pretrained Models", "NLP", "CV", "Speech", "Vision-Language", "Sequence-to-Sequence", "Text Generation", "Image Classification", "Object Detection", "Semantic Segmentation", "Panoptic Segmentation", "Depth Estimation", "Video Classification", "Universal Segmentation", "Automatic Speech Recognition", "Keyword Spotting", "Audio Classification", "Table Question Answering", "Visual Question Answering", "Image Captioning", "Zero-shot Image Classification", "Document Question Answering", "Zero-shot Video Classification", "Zero-shot Object Detection", "Zero-shot Image Segmentation", "Automatic Mask Generation", "BERT", "RoBERTa", "T5", "DETR", "ViT", "CLIP", "BART", "ELECTRA", "DistilBERT", "ALBERT", "XLNet", "Longformer", "BigBird", "MPNet", "DeBERTa", "LayoutLM", "mBART", "mT5", "Pegasus", "BLOOM", "Falcon", "OPT", "LLaMA", "Llama2", "MT5", "mBART-50", "NLLB", "NLLB-MOE", "Nougat", "OWL-ViT", "OWLv2", "PoolFormer", "PVT", "PVTv2", "Qwen2", "Reformer", "T5X", "U-T5", "XLM-R", "BERTweet", "BARThez", "BARTpho", "BERT"], "summary": "\ud83e\udd17 Transformers is a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow, providing thousands of pretrained models for natural language processing, computer vision, and audio tasks. It offers a unified API for using all pretrained models, with a seamless integration between the three deep learning frameworks. The library also includes `pipeline` for quickly using a model on a given input and `Trainer` API for fine-tuning on a new dataset."}]
[{"title":"The whisper-large-v3 model randomly misses sentences during recognition when return_timestamps=\"word\"","body":"### System Info\r\n\r\n- `transformers` version: 4.40.0.dev0\r\n- Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.21.4\r\n- Safetensors version: 0.4.2\r\n- Accelerate version: 0.28.0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.2.0+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n### Who can help?\r\n\r\n@sanchit-gandhi\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [X] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Download audio from https:\/\/www.youtube.com\/watch?v=CK_wQEX_yS8\r\n```\r\npython3 -m pip install -U yt-dlp[default]\r\nyt-dlp -f 'bestaudio[ext=webm]' -o audio.webm \"https:\/\/www.youtube.com\/watch?v=CK_wQEX_yS8\"\r\nyt-dlp -f 'bestaudio[ext=m4a]' -o audio.m4a \"https:\/\/www.youtube.com\/watch?v=CK_wQEX_yS8\"\r\n```\r\n\r\n2.\r\n```\r\nimport torch\r\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\r\nfrom datasets import load_dataset\r\n\r\n\r\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\r\n\r\nmodel_id = \"openai\/whisper-large-v3\"\r\n\r\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\r\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\r\n)\r\nmodel.to(device)\r\n\r\nprocessor = AutoProcessor.from_pretrained(model_id)\r\n\r\npipe = pipeline(\r\n    \"automatic-speech-recognition\",\r\n    model=model,\r\n    tokenizer=processor.tokenizer,\r\n    feature_extractor=processor.feature_extractor,\r\n    max_new_tokens=128,\r\n    chunk_length_s=30,\r\n    batch_size=4,\r\n    return_timestamps=True, \r\n    torch_dtype=torch_dtype,\r\n\r\n    device=device,\r\n)\r\n\r\ndataset = load_dataset(\"distil-whisper\/librispeech_long\", \"clean\", split=\"validation\")\r\n# sample = dataset[0][\"audio\"]\r\n\r\nsample = 'audio.webm'\r\n# result = pipe(sample,return_timestamps=True,)\r\nresult = pipe(sample,return_timestamps=\"word\",)\r\n\r\nprint('== '*10)\r\nprint(result)\r\n```\r\n3. When I searched for \"Elon said\" in the results, I got \"Elon said, understand it.\" This is incomplete and misses an entire sentence.\r\n\r\n4.  Change the code to result = pipe(sample, return_timestamps=True,), then the result is \"Elon said, When you struggle with a problem, that's when you,\" which is correct and meets expectations.\r\n\r\n### Expected behavior\r\n\r\nIn the case of setting return_timestamps=\"word\", the whisper-large-v3 model randomly misses sentences during recognition.\r\n\r\nEverything works normally when return_timestamps=True.\r\n\r\nThe test audio comes from https:\/\/www.youtube.com\/watch?v=CK_wQEX_yS8 . Different audio formats were downloaded, and the sentences that were missed vary.\r\n\r\nI'm using the latest version available on GitHub right now, and I believe this is a bug.","comments":["I've run into this as well- one unblock I found (haven't tracked why this is the case), is that if you also include return_language=True in your pipe (so have both return_language=True, return_timestamps=\"word\"), then the word level timestamps are correct \/ make sense. We were seeing some pretty nonsense timestamps without this, it could be the case that some other intermediate reps as needed to properly time align, and are only getting passed through when language info is being passed","\r\nThank you for your response. \r\nEven after I added return_language=True, the issue still persists. \r\nThis parameter does not affect the problem I've encountered.\r\n\r\n> I've run into this as well- one unblock I found (haven't tracked why this is the case), is that if you also include return_language=True in your pipe (so have both return_language=True, return_timestamps=\"word\"), then the word level timestamps are correct \/ make sense. We were seeing some pretty nonsense timestamps without this, it could be the case that some other intermediate reps as needed to properly time align, and are only getting passed through when language info is being passed\r\n\r\n","also cc @ylacombe "],"labels":["Core: Pipeline","Audio"],"number":29833},{"title":"llava-next does not support batched processing\/generation when batched images are not of same size","body":"### System Info\n\n- `transformers` version: 4.39.1\r\n- Platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.19.4\r\n- Safetensors version: 0.4.2\r\n- Accelerate version: 0.28.0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.2.1+cu121 (True)\r\n- Tensorflow version (GPU?): 2.16.1 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: tes\r\n- Using distributed or parallel set-up in script?: no\n\n### Who can help?\n\n@ArthurZucker @younesbelkada \n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nUse 2 images that has different resolution\r\n\r\n```\r\n        inputs = processor(text=prompts, images=images, padding=True, return_tensors=\"pt\").to(model.device)\r\n```\r\nwhere images is a list of `PIL.Image` and `prompts` too. They are of the same length\r\n\r\nError:\r\n```\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/models\/llava_next\/processing_llava_next.py\", line 105, in __call__\r\n    image_inputs = self.image_processor(images, return_tensors=return_tensors)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/image_processing_utils.py\", line 551, in __call__\r\n    return self.preprocess(images, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/models\/llava_next\/image_processing_llava_next.py\", line 608, in preprocess\r\n    return BatchFeature(data=data, tensor_type=return_tensors)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/feature_extraction_utils.py\", line 78, in __init__\r\n    self.convert_to_tensors(tensor_type=tensor_type)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/feature_extraction_utils.py\", line 188, in convert_to_tensors\r\n    raise ValueError(\r\nValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.\r\n```\r\n\r\nThis issue does not appear in the original implementation\r\n\r\nI see that https:\/\/github.com\/huggingface\/transformers\/blob\/cbe58b4269457a6ca66a556224b23f9ef246f905\/src\/transformers\/models\/llava_next\/convert_llava_next_weights_to_hf.py#L296 exists because of this issue and I hope that is not the solution since changing it out of no where surely causes some issues with the model.\n\n### Expected behavior\n\nable to use images with different resolutions.","comments":["cc @NielsRogge "],"labels":["Should Fix","Vision"],"number":29832},{"title":"About new ClearML Intergrations","body":"### Feature request\n\nWe need add set \/ filter metrics that displayed on ClearML plots. It's hard to use it with a tons of metrics that have diffrerent scale on the one plot.\n\n### Motivation\n\nThis new implementation ClearML callback's output all metrics from log as multiple series at one plot, that's looks not readable. \r\nfrom https:\/\/github.com\/huggingface\/transformers\/pull\/28559\r\n![image](https:\/\/github.com\/huggingface\/transformers\/assets\/6096108\/614fabc2-0d1c-4320-b4cc-7ae61bbae88d)\r\nFor example: runtime and samples_per_seconds has different scale than other metrics.\r\n\r\nPrevious version report metrics as one metric - one plot.\r\n\r\nAny ideas how can we set \/ filter metrics without code changes?\r\nAs temporarily solution I add my own callback for send some metrics to ClearML at other plots.\n\n### Your contribution\n\nYes, I can.","comments":[],"labels":["Feature request","Integrations"],"number":29827},{"title":"Mixture of All Intelligence (MoAI)","body":"### Model description\n\nA new large language and vision model (LLVM) that uses auxiliary visual information and natural language for prediction.\r\n\r\nIt uses 2 modules: \ud835\ude48\ud835\ude64\ud835\ude3c\ud835\ude44-\ud835\ude3e\ud835\ude64\ud835\ude62\ud835\ude65\ud835\ude67\ud835\ude5a\ud835\ude68\ud835\ude68\ud835\ude64\ud835\ude67 and \ud835\ude48\ud835\ude64\ud835\ude3c\ud835\ude44-\ud835\ude48\ud835\ude5e\ud835\ude6d\ud835\ude5a\ud835\ude67. Here \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddff\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddfc\ud835\uddff condenses the verbalized outputs of the external CV models into auxiliary visual information and \ud835\udde0\ud835\uddf6\ud835\ude05\ud835\uddf2\ud835\uddff blends three types of intelligence \u2014 visual features, auxiliary features from external CV models and language features into a cohesive whole.\r\n\r\nMoAI-7B surpasses both open-source and closed-source LLVMs in vision language tasks.\r\n\r\nModel repo: https:\/\/github.com\/ByungKwanLee\/MoAI\r\n\n\n### Open source status\n\n- [X] The model implementation is available\n- [X] The model weights are available\n\n### Provide useful links for the implementation\n\n_No response_","comments":["@NielsRogge Can I work on this to add it to the library?","Sure! Free free to open a PR and let us know when it's ready for review or you need help integrating into the library. \r\n\r\nIn general, we prioritise reviewing based on PRs opened rather than comments on issues, as we find this prevents issues from becoming stale. You're free to work on something if there's no active linked PRs open. ","Thanks @amyeroberts I'll start working on this as I don't see any open PR regarding this."],"labels":["New model","Multimodal"],"number":29823},{"title":"Move weight initialization for DeformableDetr","body":"### System Info\r\n\r\nNot relevant\r\n\r\n### Reproduction\r\n\r\nSee [Deformable Detr Modeling](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/src\/transformers\/models\/deformable_detr\/modeling_deformable_detr.py#L653). \r\n\r\n### Expected behavior\r\n\r\nAll weight initializations should be done in `_init_weights` of the `xxxPretrainedModel` class","comments":["c.c. @amyeroberts ","A note for anyone coming to this: this is for all DETR related models\r\n\r\ncc @NielsRogge "],"labels":["Should Fix","Vision"],"number":29818}]
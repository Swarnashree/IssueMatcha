[{"title":"CTC Decoding for JAX and Tensorflow","body":"Hello,\r\n\r\nThis is WIP, but I wanted to create a PR for the changes so far as I continue to work. The PR currently contains:\r\n\r\n- A Keras op for decoding using greedy and beam search strategies\r\n- Implementation for both greedy and beam search strategies in TF\r\n- Implementation for greedy strategy in JAX (beam search is WIP)\r\n\r\nI will add torch support and unit testing.\r\n\r\nThank you!","comments":[],"labels":["size:M"],"number":19366},{"title":"fix bug for numerical difference in losses","body":"Related to #19267 .\r\nThis fixes one of the numerical differences between Keras 3 & Keras 2.\r\n\r\nThe loss was averaged across multi batches without considering whether it is a full batch or not.\r\nThe last batch contains less examples, should be averaged dividing the actual number of samples..\r\n\r\nThe fix make the loss into a vector of same values with the same size as the input batch size.\r\nSo the `Mean` class would average it considering the number of samples.\r\n\r\nAn alternative way to fix it is to use `sample_weights` argument in `Mean.update_state()`, which would be confusing to the readers as it is not the same as the `sample_weights` set by the user.\r\n\r\nAdded 2 tests, which fail before the change, but succeed after the change.","comments":[],"labels":["size:S"],"number":19363},{"title":"Request for Assistance with JAX Backend: Custom Model Training with Multiple Submodels","body":"Hello Keras Team,\r\n\r\nI've been struck with JAX Backend for some time. I have got a problem when creating a custom model (by subclassing `keras.Model`), that contains multiple submodels - for training and inference purposes. Is it possible to train each model separately by separating trainable, non-trainable, and optimizer variables for each of the sub-models?\r\n\r\n\r\nHere's a colab file: https:\/\/colab.research.google.com\/drive\/1WRofKEETa_wUdFfC2r-Oof0CUZgs_49S?usp=sharing\r\n[Whole code has been written, to reproduce the issue.]\r\n\r\nThe main points of the issue are:\r\n- If each sub-model has its own unique optimizer, let's say - then is it possible to get those `sub_model.optimizer.variables` in the main CustomModel `train_step` state or not?\r\n- For some reason, the `CustomModel`'s optimizer has more number of variables than individual sub-models and encounters a `NoneType` bug immediately after `train_step` is executed exactly once! (Is that a bug?)\r\n- None of the docs do deal with the problem specific for JAX. \r\n\r\nThank You.","comments":[],"labels":["type:support","backend:jax"],"number":19358},{"title":"Conv2D.compute_output_shape() returns a shape while the layer failed to generate an output","body":"In Conv2D.build, compute_output_shape() is called to check whether an output with valid shape (non-negative dimension) can be generated. An Error will be raised if the output shape contains a negative dimension. \r\n\r\nHowever, in this case [gist](https:\/\/colab.research.google.com\/drive\/11X0PgGEQfmMVATXU6zlJe1SMug7_7Ppx?usp=sharing)\uff0cConv2D can generate a valid output shape while tf.nn.convolution failed to generate the output. I guess something goes wrong in one of them.\r\n\r\nPlease check it out.","comments":["In the Gist you have provided `compute_output_shape` and `layer output shape` both matches, is there anything I'm missing here?"],"labels":["stat:awaiting response from contributor","type:Bug"],"number":19357},{"title":"Introduce QLoRA-like technique","body":"## Highlights\r\n\r\nThis PR enables training with frozen int8 weights for `Dense` and `EinsumDense`.\r\n\r\nOverall, we will have a similar training speed and lower memory usage (about 68~50%) compared to floating-point LoRA.\r\n\r\n## Notes\r\n\r\nSimilar to QLoRA, but this PR lacks the following:\r\n- NF4 is not utilized because there is no backend supports it.\r\n- Double quantization is not used.\r\n- Paged optimizer is not included because it is more like a low-level hardware optimization\r\n\r\nThe training speed with torch backend is slower due to the lack of hardware-accelerated matmul\/einsum.\r\n\r\n## Results\r\n\r\n- MNIST classification\r\n- backend: tensorflow (to measure GPU memory usage)\r\n\r\n|layer|quantized|`compute_dtype`|acc. (LoRA unmerged\/merged)|inference time (LoRA unmerged\/merged)|peak gpu memory (fitting with LoRA)|\r\n|-|-|-|-|-|-|\r\n|`Dense`|\u274c|float32|0.95790 \/ 0.95790|0.00395s \/ 0.00338s|0.528GB|\r\n|`Dense`|\u274c|bfloat16|0.96030 \/ 0.96060|0.00270s \/ 0.00246s|0.452GB|\r\n|`Dense`|int8|float32|0.95860\u2020\/ 0.95920|0.00282s \/ 0.00208s|0.264GB|\r\n|`Dense`|int8|bfloat16|0.95790\u2020\/ 0.95830|0.00254s \/ 0.00207s|0.263GB|\r\n|`Einsum`|\u274c|float32|0.96030 \/ 0.96030|0.00385s \/ 0.00331s|0.526GB|\r\n|`Einsum`|\u274c|bfloat16|0.96300 \/ 0.96310|0.00258s \/ 0.00237s|0.451GB|\r\n|`Einsum`|int8|float32|0.95940\u2020\/ 0.95860|0.00499s\\*\/ 0.00191s|0.360GB|\r\n|`Einsum`|int8|bfloat16|0.96400\u2020\/ 0.96370|0.00364s\\*\/ 0.00200s|0.285GB|\r\n\r\n> \\*: The performance of the quantized `Einsum` with `lora_enabled=True` is suboptimal due to the current implementation of LoRA calculation.\r\n> \u2020: Merging LoRA weights into int8 kernels results in lossy compression, leading to slightly differences in the final outputs.\r\n\r\nStandalone benchmark script:\r\n\r\n```bash\r\n# Usage\r\n# Train a float model\r\npython3 benchmark.py --type train [--use-einsum] [--dtype-policy mixed_bfloat16]\r\n# Finetune with quantized weights\r\npython3 benchmark.py --type finetune --path [model_int8.keras|model_fp32.keras] [--use-einsum] [--dtype-policy mixed_bfloat16]\r\n```\r\n\r\n<details>\r\n\r\n<summary>benchmark.py<\/summary>\r\n\r\n```python\r\nimport argparse\r\nimport os\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport keras\r\nfrom keras import backend\r\nfrom keras import dtype_policies\r\nfrom keras import layers\r\nfrom keras import models\r\nfrom keras import ops\r\nfrom keras import saving\r\nfrom keras.utils.traceback_utils import disable_traceback_filtering\r\n\r\ndisable_traceback_filtering()\r\n\r\n\r\ndef get_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \"--type\",\r\n        default=\"train\",\r\n        choices=[\"train\", \"finetune\"],\r\n    )\r\n    parser.add_argument(\"--path\")\r\n    parser.add_argument(\r\n        \"--dtype-policy\",\r\n        default=\"float32\",\r\n        choices=[\"float32\", \"mixed_bfloat16\"],\r\n    )\r\n    parser.add_argument(\"--use-einsum\", action=\"store_true\")\r\n    return parser.parse_args()\r\n\r\n\r\ndef build_model(num_layers=32, units=1024, use_einsum=False):\r\n    inputs = layers.Input([28, 28])\r\n    x = layers.Flatten()(inputs)\r\n    for _ in range(num_layers):\r\n        if use_einsum:\r\n            x = layers.EinsumDense(\"ab,bc->ac\", output_shape=[units])(x)\r\n        else:\r\n            x = layers.Dense(units)(x)\r\n        x = layers.BatchNormalization()(x)\r\n        x = layers.ReLU()(x)\r\n    outputs = layers.Dense(10, use_bias=True, activation=\"softmax\")(x)\r\n    model = models.Model(inputs, outputs)\r\n    return model\r\n\r\n\r\ndef benchmark(model, batch_size=1024, input_shape=(28, 28), iterations=200):\r\n    def fn(x):\r\n        return model(x, training=False)\r\n\r\n    if backend.backend() == \"tensorflow\":\r\n        import tensorflow as tf\r\n\r\n        jit_fn = tf.function(fn, jit_compile=True)\r\n    elif backend.backend() == \"jax\":\r\n        import jax\r\n\r\n        jit_fn = jax.jit(fn)\r\n    elif backend.backend() == \"torch\":\r\n        jit_fn = fn\r\n    else:\r\n        jit_fn = fn\r\n\r\n    # warmup\r\n    x = ops.ones([batch_size, *input_shape])\r\n    for _ in range(10):\r\n        _ = ops.convert_to_numpy(jit_fn(x))\r\n\r\n    times = []\r\n    for _ in range(iterations):\r\n        t0 = time.time()\r\n        _ = ops.convert_to_numpy(jit_fn(x))\r\n        t1 = time.time()\r\n        times.append(t1 - t0)\r\n    avg_time = sum(times) \/ len(times)\r\n    return avg_time\r\n\r\n\r\nclass GPUMemoryCallback(keras.callbacks.Callback):\r\n    def __init__(self, target_batches, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.target_batches = target_batches\r\n        self.memory_usage = []\r\n\r\n    def _compute_memory_usage(self):\r\n        try:\r\n            memory_stats = tf.config.experimental.get_memory_info(\"GPU:0\")\r\n        except ValueError:\r\n            memory_stats = {\"peak\": 0}\r\n        # Convert bytes to GB and store in list.\r\n        peak_usage = round(memory_stats[\"peak\"] \/ (2**30), 3)\r\n        self.memory_usage.append(peak_usage)\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        self._compute_memory_usage()\r\n\r\n    def on_train_batch_begin(self, batch, logs=None):\r\n        if batch in self.target_batches:\r\n            self._compute_memory_usage()\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        self._compute_memory_usage()\r\n\r\n\r\ndef train(args, dtype):\r\n    # Model \/ data parameters\r\n    use_einsum = args.use_einsum\r\n    num_classes = 10\r\n    input_shape = (28, 28, 1)\r\n    epochs = 1\r\n\r\n    # Load the data and split it between train and test sets\r\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n    x_train = x_train.astype(\"float32\") \/ 255\r\n    x_test = x_test.astype(\"float32\") \/ 255\r\n    x_train = np.expand_dims(x_train, -1)\r\n    x_test = np.expand_dims(x_test, -1)\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    model = build_model(num_layers=32, units=1024, use_einsum=use_einsum)\r\n    model.compile(\r\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\r\n    )\r\n\r\n    \"\"\"Train float model\"\"\"\r\n    print(\"=====Start training float model=====\")\r\n    model.fit(\r\n        x_train, y_train, batch_size=128, epochs=epochs, validation_split=0.1\r\n    )\r\n    print(f\"Performance of {dtype}:\")\r\n    score = model.evaluate(x_test, y_test, verbose=0)\r\n    print(f\"  Test accuracy: {score[1]:.5f}\")\r\n    avg_time = benchmark(model, input_shape=input_shape)\r\n    print(f\"  Avg. inference time (batch_size=1024): {avg_time:.5f}s\")\r\n\r\n    \"\"\"Save trained model\"\"\"\r\n    model.save(\"model_fp32.keras\")\r\n    model.quantize(\"int8\")\r\n    model.save(\"model_int8.keras\")\r\n    print(\"Size of saved model:\")\r\n    print(f\"  fp32: {os.path.getsize('model_fp32.keras') >> 20}MB\")\r\n    print(f\"  int8: {os.path.getsize('model_int8.keras') >> 20}MB\")\r\n\r\n\r\ndef finetune(args, model):\r\n    use_einsum = args.use_einsum\r\n    \"\"\"Enable LoRA\"\"\"\r\n    for layer in model.layers:\r\n        if hasattr(layer, \"enable_lora\"):\r\n            layer.enable_lora(2)\r\n\r\n    # Model \/ data parameters\r\n    num_classes = 10\r\n    input_shape = (28, 28, 1)\r\n    epochs = 1\r\n\r\n    # Load the data and split it between train and test sets\r\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n    x_train = x_train.astype(\"float32\") \/ 255\r\n    x_test = x_test.astype(\"float32\") \/ 255\r\n    x_train = np.expand_dims(x_train, -1)\r\n    x_test = np.expand_dims(x_test, -1)\r\n    y_train = keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    gpu_memory_callback = GPUMemoryCallback(\r\n        target_batches=[5, 10, 25, 50, 100, 150, 200, 300, 400, 500],\r\n    )\r\n    model.compile(\r\n        loss=\"categorical_crossentropy\",\r\n        optimizer=\"adam\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\n    model.fit(\r\n        x_train,\r\n        y_train,\r\n        batch_size=128,\r\n        epochs=epochs,\r\n        callbacks=[gpu_memory_callback],\r\n        validation_split=0.1,\r\n    )\r\n    lora_memory_usage = gpu_memory_callback.memory_usage\r\n    print(\"Performance of fine-tuned lora weights:\")\r\n    score = model.evaluate(x_test, y_test, verbose=0)\r\n    print(f\"  Test accuracy: {score[1]:.5f}\")\r\n    avg_time = benchmark(model, input_shape=input_shape)\r\n    print(f\"  Avg. inference time (batch_size=1024): {avg_time:.5f}s\")\r\n    print(f\"  GPU Memory Usage (in GB): {max(lora_memory_usage)}\")\r\n\r\n    \"\"\"Saving & loading\"\"\"\r\n    model_path = \"finetune.keras\"\r\n    weights_path = \"finetune.weights.h5\"\r\n    model.save(model_path)\r\n    model.save_weights(weights_path)\r\n    reloaded_model = saving.load_model(model_path)\r\n    reloaded_score = reloaded_model.evaluate(x_test, y_test, verbose=0)\r\n    print(f\"Reloaded model test accuracy: {reloaded_score[1]:.5f}\")\r\n    # Load the file into a fresh, non-lora model\r\n    new_model = build_model(num_layers=32, units=1024, use_einsum=use_einsum)\r\n    new_model.build(input_shape)\r\n    if isinstance(\r\n        model.layers[2].dtype_policy, dtype_policies.QuantizedDTypePolicy\r\n    ):\r\n        new_model.quantize(\"int8\")\r\n    new_model.load_weights(weights_path)\r\n    new_model.compile(\r\n        loss=\"categorical_crossentropy\",\r\n        optimizer=\"adam\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\n    reloaded_score = new_model.evaluate(x_test, y_test, verbose=0)\r\n    print(\"Non-lora model:\")\r\n    print(f\"  Test accuracy: {reloaded_score[1]:.5f}\")\r\n    avg_time = benchmark(new_model, input_shape=input_shape)\r\n    print(f\"  Avg. inference time (batch_size=1024): {avg_time:.5f}s\")\r\n    # Try loading a normal checkpoint into a lora model\r\n    new_model.save_weights(weights_path)\r\n    model.load_weights(weights_path)\r\n    reloaded_score = model.evaluate(x_test, y_test, verbose=0)\r\n    print(f\"Lora model test accuracy: {reloaded_score[1]:.5f}\")\r\n\r\n\r\ndef main():\r\n    args = get_args()\r\n\r\n    # Set dtype policy\r\n    dtype = args.dtype_policy\r\n    dtype_policies.dtype_policy.set_dtype_policy(dtype)\r\n    print(f\"Global dtype policy: {dtype_policies.dtype_policy.dtype_policy()}\")\r\n\r\n    if args.type == \"train\":\r\n        train(args, dtype)\r\n    elif args.type == \"finetune\":\r\n        model = saving.load_model(args.path)\r\n        finetune(args, model)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n<\/details>","comments":["## [Codecov](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) Report\nAttention: Patch coverage is `79.82456%` with `23 lines` in your changes are missing coverage. Please review.\n> Project coverage is 75.88%. Comparing base [(`aa3a61b`)](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/commit\/aa3a61bccca5770279ed83efd4737c0857212a9d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) to head [(`fb35741`)](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?dropdown=coverage&src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team).\n> Report is 2 commits behind head on master.\n\n| [Files](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | Patch % | Lines |\n|---|---|---|\n| [keras\/layers\/core\/einsum\\_dense.py](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team#diff-a2VyYXMvbGF5ZXJzL2NvcmUvZWluc3VtX2RlbnNlLnB5) | 69.69% | [10 Missing and 10 partials :warning: ](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) |\n| [keras\/layers\/core\/dense.py](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team#diff-a2VyYXMvbGF5ZXJzL2NvcmUvZGVuc2UucHk=) | 93.75% | [1 Missing and 2 partials :warning: ](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) |\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master   #19356      +\/-   ##\n==========================================\n+ Coverage   75.86%   75.88%   +0.01%     \n==========================================\n  Files         366      366              \n  Lines       40417    40466      +49     \n  Branches     7855     7870      +15     \n==========================================\n+ Hits        30661    30706      +45     \n- Misses       8061     8064       +3     \n- Partials     1695     1696       +1     \n```\n\n| [Flag](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flags&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | Coverage \u0394 | |\n|---|---|---|\n| [keras](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | `75.73% <79.82%> (+0.01%)` | :arrow_up: |\n| [keras-jax](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | `60.14% <71.05%> (+0.03%)` | :arrow_up: |\n| [keras-numpy](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | `54.09% <11.40%> (-0.29%)` | :arrow_down: |\n| [keras-tensorflow](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | `61.33% <79.82%> (+0.05%)` | :arrow_up: |\n| [keras-torch](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356\/flags?src=pr&el=flag&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team) | `60.43% <71.05%> (+0.01%)` | :arrow_up: |\n\nFlags with carried forward coverage won't be shown. [Click here](https:\/\/docs.codecov.io\/docs\/carryforward-flags?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team#carryforward-flags-in-the-pull-request-comment) to find out more.\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/keras-team\/keras\/pull\/19356?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=keras-team).\n","We should wait for #19302 ","Is there a way we could try this out with an LLM? E.g. adapt this guide -> https:\/\/ai.google.dev\/gemma\/docs\/lora_tuning\r\n\r\nIIUC, this should probably allow us to fine tune a gemma 7b model checkpoint on less than 16gb GPU RAM, because lora will essentially zero out the size of optimizer variables relative to model weights, and quantizing our weights to int8 should bring us to a little over 8gb of space.\r\n\r\nAn end to end test with a massive model might validate a lot.","How do we want to handle embeddings and quantization? Embeddings are usually the biggest individual memory hogs for models that might want quantization. We might want to add some quant support to our layer (though does not need to be this PR!).\r\n\r\nhttps:\/\/github.com\/google\/gemma_pytorch\/blob\/cf8658c186255379194ba5b62612321eacde1b6b\/gemma\/model.py#L132-L154\r\nhttps:\/\/pytorch.org\/docs\/stable\/generated\/torch.ao.nn.quantized.Embedding.html","Since `Embedding` is just a lookup table, presumably just storing the weights in `int8` should do it? There's no input scaling either (inputs are integer indices).","> Since Embedding is just a lookup table, presumably just storing the weights in int8 should do it? There's no input scaling either (inputs are integer indices).\r\n\r\nYeah I think it should be pretty simple.\r\n\r\nThen there's the question of lora + quantization + an embedding layer. I don't think practically doing lora + quantization will be that important, as most people probably use lora with any embeddings frozen, but it might be worth adding for consistency (since we have enable_lora on the layer).","> Is there a way we could try this out with an LLM? E.g. adapt this guide -> https:\/\/ai.google.dev\/gemma\/docs\/lora_tuning\r\n\r\nI have encountered an issue to try this PR with KerasNLP:\r\nWhen loading a quantized model using `keras.saving.load_model(...)`, I noticed that KerasNLP omits the `dtype` and always use the global dtype policy during loading.\r\n\r\nThis issue will cause the following to fail:\r\n```python\r\npreprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\r\n    \"gpt2_base_en\", sequence_length=128\r\n)\r\nlora_model = keras_nlp.models.GPT2CausalLM.from_preset(\r\n    \"gpt2_base_en\", preprocessor=preprocessor\r\n)\r\nlora_model.quantize(\"int8\")\r\nlora_model.save(\"model_int8.keras\")\r\nreloaded_model = keras.saving.load_model(\"model_int8.keras\")  # <- this line\r\n```\r\n\r\nThe above is neccesary to accurately record the peak GPU memory usage because tensorflow doesn't release GPU memory after using `quantize(\"int8\")` on float model.\r\n\r\n> How do we want to handle embeddings and quantization?\r\n\r\nI can add  `quantize` to `Embedding` layer in another PR. It should be feasible."],"labels":["size:L"],"number":19356}]
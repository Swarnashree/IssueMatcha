[{"issue_title": "AllenNLP biased towards BERT", "summary": "The issue discusses differences in performance between BERT and RoBERTa models in the same repository, specifically in the context of Named Entity Recognition (NER) tasks. The main questions revolve around the use of the same optimizer and the use of CRF models. The primary suspect for a bug is identified as the `PretrainedTransformerMismatched(Embedder|TokenIndexer)`. Suggestions for further testing include running unit tests from the AllenNLP repository, comparing embeddings produced by a raw RobertaModel and the PretrainedTransformerMismatchedEmbedder, and examining the exact tokens used for each word by the PretrainedTransformerMismatchedEmbedder."}, {"issue_title": "Computing rouge is significantly slow", "summary": "There is an issue with latency when using the ROUGE metric with longer sequences in ALLENNLP's implementation. The user reported latency, but the maintainer was unable to reproduce it with smaller sequences. The maintainer was able to reproduce the issue with longer sequences and will look into it. There are also friendly pings to the user to check on the status of the issue."}, {"issue_title": "structured-prediction-constituency-parser adds extra spaces", "summary": "The issue is caused by the Spacy tokenizer used in the predictor, which discards spaces. This results in incorrect span reconstruction and extra spaces in the output when there are special characters. The proposed solution is to modify several components to accept and return `Token` objects instead of strings, so that the exact input span can be recreated using the additional information in each token. A contributor has offered to work on a PR for this issue."}, {"issue_title": "Windows", "summary": "Discussion on compatibility of AllenNLP with Windows systems, with a friendly ping to the maintainer to not forget about the issue."}, {"issue_title": "Why not add transformer tokens to vocabulary in init phrase", "summary": "The issue is related to the `PretrainedTransformerIndexer` not having access to the vocab in its `__init__` method. The current hack to shortcut vocabulary discovery is not elegant and takes a long time for big datasets. A better solution is encouraged for improvement."}]
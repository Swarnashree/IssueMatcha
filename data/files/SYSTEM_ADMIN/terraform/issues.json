[{"title":"`terraform fmt` mangles hyphenated-heredoc `error_message`  with newlines embedded in interpolation","body":"### Terraform Version\n\n```shell\nTerraform v1.7.5\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\nThis is a contrived example to show the issue. Each hyphenated heredoc `error_message`  that includes an interpolation which itself is broken over multiple lines results in the indenting of the heredoc and remainder of the enclosing resource being mangled.\r\n\r\n```terraform\r\nvariable \"demo\" {\r\n  type        = list(string)\r\n  default     = [\"a\", \"b\", \"c\"]\r\n  description = \"A list of demo values\"\r\n\r\n  validation {\r\n    condition     = length(var.demo) == 3\r\n    error_message = <<-EOT\r\n      The demo variable must have exactly 3 values, provided values are:\r\n\r\n      ${join(\r\n        \", \",\r\n        formatlist(\"\\\"%s\\\"\", var.demo)\r\n      )}\r\n    EOT\r\n  }\r\n}\r\n\r\ncheck \"demo_values\" {\r\n  assert {\r\n    condition     = length(var.demo) == 3\r\n    error_message = <<-EOT\r\n      The demo variable must have exactly 3 values, provided values are:\r\n\r\n      ${join(\r\n        \", \",\r\n        formatlist(\"\\\"%s\\\"\", var.demo)\r\n      )}\r\n    EOT\r\n  }\r\n}\r\n\r\nresource \"terraform_data\" \"broken\" {\r\n  input = var.demo\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = length(var.demo) == 3\r\n      error_message = <<-EOT\r\n        The demo variable must have exactly 3 values, provided values are:\r\n\r\n        ${join(\r\n          \", \",\r\n          formatlist(\"\\\"%s\\\"\", var.demo)\r\n        )}\r\n      EOT\r\n    }\r\n  }\r\n}\r\n\r\nresource \"terraform_data\" \"working\" {\r\n  input = var.demo\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = length(var.demo) == 3\r\n      error_message = <<-EOT\r\n        The demo variable must have exactly 3 values, provided values are:\r\n\r\n        ${join(\", \", formatlist(\"\\\"%s\\\"\", var.demo))}\r\n      EOT\r\n    }\r\n  }\r\n}\r\n```\r\n\n\n### Debug Output\n\n```sh\r\n> TF_DEBUG=trace terraform fmt 2>&1\r\nmain.tf\r\n```\n\n### Expected Behavior\n\nNo changes to the code.\n\n### Actual Behavior\n\n```terraform\r\nvariable \"demo\" {\r\n  type        = list(string)\r\n  default     = [\"a\", \"b\", \"c\"]\r\n  description = \"A list of demo values\"\r\n\r\n  validation {\r\n    condition = length(var.demo) == 3\r\n    error_message = <<-EOT\r\n      The demo variable must have exactly 3 values, provided values are:\r\n\r\n      ${join(\r\n    \", \",\r\n    formatlist(\"\\\"%s\\\"\", var.demo)\r\n)}\r\n    EOT\r\n}\r\n}\r\n\r\ncheck \"demo_values\" {\r\n  assert {\r\n    condition = length(var.demo) == 3\r\n    error_message = <<-EOT\r\n      The demo variable must have exactly 3 values, provided values are:\r\n\r\n      ${join(\r\n    \", \",\r\n    formatlist(\"\\\"%s\\\"\", var.demo)\r\n)}\r\n    EOT\r\n}\r\n}\r\n\r\nresource \"terraform_data\" \"broken\" {\r\n  input = var.demo\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition = length(var.demo) == 3\r\n      error_message = <<-EOT\r\n        The demo variable must have exactly 3 values, provided values are:\r\n\r\n        ${join(\r\n      \", \",\r\n      formatlist(\"\\\"%s\\\"\", var.demo)\r\n  )}\r\n      EOT\r\n}\r\n}\r\n}\r\n\r\nresource \"terraform_data\" \"working\" {\r\n  input = var.demo\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = length(var.demo) == 3\r\n      error_message = <<-EOT\r\n        The demo variable must have exactly 3 values, provided values are:\r\n\r\n        ${join(\", \", formatlist(\"\\\"%s\\\"\", var.demo))}\r\n      EOT\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nNotice that `fmt` seems to be treating the blocks with the multiline interpolations as multiline block themselves. This is subtly apparent by the fact that in the `terraform_data.working` resource the `condition` and `error_message` equal sign remains aligned, but on the other examples they do not.\r\n\r\n```terraform\r\n      condition     = length(var.demo) == 3\r\n      error_message = <<-EOT\r\n```\r\n\r\nvs.\r\n\r\n```terraform\r\n    condition = length(var.demo) == 3\r\n    error_message = <<-EOT\r\n```\r\n\r\nAdditionally, the reformatting extends beyond the `EOT` for the heredocs to include the enclosing blocks' braces.\n\n### Steps to Reproduce\n\n1. `terraform fmt`\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":[],"labels":["bug","new"]},{"title":"docs: Terraform style guide","body":"**Note:** This PR requires a redirect from the existing style conventions doc. Once this PR is merged, merge https:\/\/github.com\/hashicorp\/terraform-docs-common\/pull\/584\r\n\r\n:mag: [Deploy preview](https:\/\/terraform-fl6xf6kn7-hashicorp.vercel.app\/terraform\/language\/style)\r\n\r\nThe Well-Architected Framework team has gathered feedback from HashiCorp employees and Terraform users in an attempt to build on our recommended style contentions. The intention of this guide is that it is used by those looking to form their own internal standardized style guide.\r\n\r\nThis guide is split into two parts:\r\n\r\n1. Code style: Syntax conventions (brought from the existing style conventions document), resource naming and ordering, linting, variables, outputs, locals, etc.\r\n2. Workflow style: Versioning, repo structure, module design, state management, secrets management, etc\r\n\r\nThis guide is a mix of prescriptive advice and non-prescriptive considerations, as some scenarios truly depend on the situation.","comments":[],"labels":["documentation"]},{"title":"Interpolate environment variables when configuring dev_overrides","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\nThis pull request adds environment variable interpolation to the `dev_overrides` configuration block, specifically for the local filesystem path being mapped to the provider source address.\r\n\r\n## Example\r\nBefore the proposed change:\r\n```hcl\r\nprovider_installation {\r\n  dev_overrides {\r\n    \"craigsloggett\/github\" = \"\/Users\/craig.sloggett\/.cache\/go\/bin\"\r\n  }\r\n\r\n  direct {}\r\n}\r\n```\r\nAfter the proposed change:\r\n```hcl\r\nprovider_installation {\r\n  dev_overrides {\r\n    \"craigsloggett\/github\" = \"${GOPATH}\/bin\"\r\n  }\r\n\r\n  direct {}\r\n}\r\n```\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #28602\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.8.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### ENHANCEMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n-  Maintaining a `terraform.rc` file that's portable across systems, platforms, or users.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34873) <br\/>All committers have signed the CLA.","Thanks for this submission! "],"labels":["enhancement"]},{"title":"Terraform test expect_failures does not work as expected when type is invalid","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.5\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/azurerm v3.96.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n# Snippet of test configuration\r\n```terraform\r\n#############################################\r\n# Testing var.rule_collection_group_priority\r\n#############################################\r\n\r\nrun \"invalid_rule_collection_group_priority_low_fails\" {\r\n  command = plan\r\n  expect_failures = [ var.rule_collection_group_priority]\r\n  variables {\r\n    rule_collection_group_priority = 100\r\n  }\r\n}\r\n\r\nrun \"invalid_rule_collection_group_priority_string_fails\" {\r\n  command = plan\r\n  expect_failures = [ var.rule_collection_group_priority]\r\n  variables {\r\n    rule_collection_group_priority = \"invalid\"\r\n  }\r\n}\r\n\r\nrun \"valid_rule_collection_group_priority_succeeds\" {\r\n  command = plan\r\n  variables {\r\n    rule_collection_group_priority = 1000\r\n  }\r\n}\r\n```\r\n\r\n# Snippet of variable definition\r\n```terraform\r\nvariable \"rule_collection_group_priority\" {\r\n  type        = number\r\n  description = \"The rule collection group priority number. This will be provided by the network team.\"\r\n\r\n  validation {\r\n    condition     = var.rule_collection_group_priority >= 1000\r\n    error_message = \"The variable 'rule_collection_group_priority' must be greater than 1000.\"\r\n  }\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\nnot provided\r\n\r\n### Expected Behavior\r\n\r\nI would expect terraform test to fail on run invalid_rule_collection_group_priority_string_fails.\r\n\r\nWhilst you could argue this is overkill as the type validation already takes care of it, this can be useful to ensure that someone doesn't incorrectly modify a variable and allow an unexpected type.\r\n\r\n### Actual Behavior\r\n\r\nTerraform test errors saying it is missing an expected failure (it definitely is not missing this block)...\r\n\r\n```\r\ntests\/variable_validations.tftest.hcl... in progress\r\n  run \"invalid_project_name_with_space_fails\"... pass\r\n  run \"invalid_project_name_with_underscore_fails\"... pass\r\n  run \"invalid_project_name_empty_fails\"... pass\r\n  run \"valid_project_name_succeeds\"... pass\r\n  run \"invalid_rule_collection_group_priority_low_fails\"... pass\r\n  run \"invalid_rule_collection_group_priority_string_fails\"... fail\r\n\u2577\r\n\u2502 Error: Missing expected failure\r\n\u2502 \r\n\u2502   on tests\/variable_validations.tftest.hcl line 128, in run \"invalid_rule_collection_group_priority_string_fails\":\r\n\u2502  128:   expect_failures = [ var.rule_collection_group_priority]\r\n\u2502 \r\n\u2502 The checkable object, var.rule_collection_group_priority, was expected to report an error but did not.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid value for input variable\r\n\u2502 \r\n\u2502   on tests\/variable_validations.tftest.hcl line 130, in run \"invalid_rule_collection_group_priority_string_fails\":\r\n\u2502  130:     rule_collection_group_priority = \"invalid\"\r\n\u2502 \r\n\u2502 The given value is not suitable for var.rule_collection_group_priority declared at variables.tf:293,1-42: a number is required.\r\n\u2575\r\n  run \"valid_rule_collection_group_priority_succeeds\"... skip\r\ntests\/variable_validations.tftest.hcl... tearing down\r\ntests\/variable_validations.tftest.hcl... fail\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a variable definition as per provided snippet.\r\n2. Create a terraform test file, and add the check below:\r\n\r\n```terraform\r\nrun \"invalid_rule_collection_group_priority_string_fails\" {\r\n  command = plan\r\n  expect_failures = [ var.rule_collection_group_priority]\r\n  variables {\r\n    rule_collection_group_priority = \"invalid\"\r\n  }\r\n}\r\n```\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @mike-guy, thanks for filing this! I've relabelled this as an enhancement request instead of a bug as Terraform is behaving as intended in this case.\r\n\r\nThe intent of the `expect_failures` block is to allow configuration authors to validate the custom conditions they have written within their configuration. As you've mentioned, type errors are validation errors and these are checked separately to the custom conditions that are being checked by the `expect_failures` problem. In this case, the test isn't really running at all but failing with a mechanism similar to a compilation error within a more traditional programming language. You can trust that Terraform itself will prevent your configurations from being planned or applied if the wrong type is supplied, in the same way you don't need to write tests in, for example, go that validate something won't compile if you attempt to supply the wrong type.\r\n\r\n> this can be useful to ensure that someone doesn't incorrectly modify a variable and allow an unexpected type.\r\n\r\nJust a note here, that this would\/should cause a cascade of errors throughout the system. Terraform performs type checking all the way through the configuration. If the type of a variable changed, anywhere that referenced that variable would report an error as the types would no longer match.\r\n\r\nWe can track extending this as you've described as a feature request, but my feeling is that the underlying functionality for validating types is covered by the `terraform validate` command and not something that authors themselves should be validating manually within the tests.\r\n\r\nThanks!","If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks!"],"labels":["enhancement","new","terraform test"]},{"title":"Optional chaining operation","body":"### Terraform Version\n\n```shell\n1.7.2\n```\n\n\n### Use Cases\n\nSuppose I have an optional variable like:\r\n```terraform\r\nvariable \"test\" {\r\n  type = object({\r\n     field1 = string\r\n     # and some other fields\r\n    })\r\n  default = null\r\n)\r\n```\r\n\r\nand then somewhere else I need to use use the value of `var.test.field`, but use a default value of `var.test` is null.\r\n\r\nI would like to be able to do something like:\r\n\r\n```\r\nsomething = coalesce(var.test?.field1, \"default\") # the default may actually depend on other variables\r\n```\r\n\r\n\r\n\r\n\r\n  \n\n### Attempted Solutions\n\nI can of course do something like:\r\n\r\n```terraform\r\nsomething = var.test != null ? var.test.field1 : \"default\"\r\n```\r\n\r\nbut that is awkward and hard to read. It becomes even more so if there are  multiple levels of possibly null objects that need to be traversed, especially since `&&` isn't short circuiting. \r\n\r\nAnother option is to use try:\r\n\r\n```terraform\r\nsomething = try(var.test.field1, \"default\")\r\n```\r\n\r\nthe problem with this is that if I misspell something in `var.test.field1`, it will silently use the default instead of giving me an error. \r\n\r\n\n\n### Proposal\n\nAdd a .? operator similar to [javascript's optional chaining feature](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Operators\/Optional_chaining). \r\n\r\nOr maybe add a function like `lookup`, except use the default value if the first argument is null. \r\n\r\nOr have a variant of `try` that will still fail for any other error than trying to look up a field on a null value.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Planned value does not match config value for number","body":"### Terraform Version\n\n```shell\nTerraform v1.8.0-dev\r\non darwin_arm64\r\n\r\nv1.8.0-beta1\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nresource \"playground_resource\" \"example\" {\r\n  number_attribute = 9223372036854775808\r\n}\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/bendbennett\/694e16874249f8aaa1a9ab14da84129f\n\n### Expected Behavior\n\nExpecting the plan displayed by the CLI to look as follows:\r\n\r\n```shell\r\nTerraform will perform the following actions:\r\n\r\n  # playground_resource.example will be created\r\n  + resource \"playground_resource\" \"example\" {\r\n      + number_attribute = 9223372036854775808\r\n    }\r\n```\n\n### Actual Behavior\n\nPlan displayed by the CLI looks as follows:\r\n\r\n```shell\r\n  # playground_resource.example will be created\r\n  + resource \"playground_resource\" \"example\" {\r\n      + number_attribute = 9223372036854776000\r\n    }\r\n```\r\n\r\nThe value stored in state matches that from the plan, but differs from that supplied in the configuration:\r\n\r\n```json\r\n{\r\n  \"version\": 4,\r\n  \"terraform_version\": \"1.8.0\",\r\n  \"serial\": 1,\r\n  \"lineage\": \"e3092a01-aadf-643b-a61c-f26cac8e0ca1\",\r\n  \"outputs\": {},\r\n  \"resources\": [\r\n    {\r\n      \"mode\": \"managed\",\r\n      \"type\": \"playground_resource\",\r\n      \"name\": \"example\",\r\n      \"provider\": \"provider[\\\"registry.terraform.io\/bendbennett\/playground\\\"]\",\r\n      \"instances\": [\r\n        {\r\n          \"schema_version\": 0,\r\n          \"attributes\": {\r\n            \"number_attribute\": 9223372036854776000\r\n          },\r\n          \"sensitive_attributes\": []\r\n        }\r\n      ]\r\n    }\r\n  ],\r\n  \"check_results\": null\r\n}\r\n```\n\n### Steps to Reproduce\n\n`terraform apply`\n\n### Additional Context\n\nThis appears to be specific to the value of the integer supplied in the configuration (i.e., 9223372036854775808).\r\n\r\nThe structure of the provider is as follows:\r\n\r\n```go\r\npackage provider\r\n\r\nimport (\r\n\t\"context\"\r\n\r\n\t\"github.com\/hashicorp\/terraform-plugin-framework\/path\"\r\n\t\"github.com\/hashicorp\/terraform-plugin-framework\/resource\"\r\n\t\"github.com\/hashicorp\/terraform-plugin-framework\/resource\/schema\"\r\n\t\"github.com\/hashicorp\/terraform-plugin-framework\/types\"\r\n)\r\n\r\nvar _ resource.Resource = (*playgroundResource)(nil)\r\nvar _ resource.ResourceWithImportState = (*playgroundResource)(nil)\r\n\r\ntype playgroundResource struct {\r\n}\r\n\r\nfunc NewResource() resource.Resource {\r\n\treturn &playgroundResource{}\r\n}\r\n\r\nfunc (e *playgroundResource) Metadata(_ context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {\r\n\tresp.TypeName = req.ProviderTypeName + \"_resource\"\r\n}\r\n\r\nfunc (e *playgroundResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {\r\n\tresp.Schema = schema.Schema{\r\n\t\tAttributes: map[string]schema.Attribute{\r\n\t\t\t\"number_attribute\": schema.NumberAttribute{\r\n\t\t\t\tOptional: true,\r\n\t\t\t\tComputed: true,\r\n\t\t\t},\r\n\t\t},\r\n\t}\r\n}\r\n\r\ntype playgroundResourceData struct {\r\n\tNumberAttribute types.Number `tfsdk:\"number_attribute\"`\r\n}\r\n\r\nfunc (e *playgroundResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {\r\n\tvar data playgroundResourceData\r\n\r\n\tdiags := req.Plan.Get(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n\r\n\tif resp.Diagnostics.HasError() {\r\n\t\treturn\r\n\t}\r\n\r\n\tdiags = resp.State.Set(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n}\r\n\r\nfunc (e *playgroundResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {\r\n\tvar data playgroundResourceData\r\n\r\n\tdiags := req.State.Get(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n\r\n\tif resp.Diagnostics.HasError() {\r\n\t\treturn\r\n\t}\r\n\r\n\tdiags = resp.State.Set(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n}\r\n\r\nfunc (e *playgroundResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {\r\n\tvar data playgroundResourceData\r\n\r\n\tdiags := req.Plan.Get(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n\r\n\tif resp.Diagnostics.HasError() {\r\n\t\treturn\r\n\t}\r\n\r\n\tdiags = resp.State.Set(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n}\r\n\r\nfunc (e *playgroundResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {\r\n\tvar data playgroundResourceData\r\n\r\n\tdiags := req.State.Get(ctx, &data)\r\n\tresp.Diagnostics.Append(diags...)\r\n\r\n\tif resp.Diagnostics.HasError() {\r\n\t\treturn\r\n\t}\r\n}\r\n\r\nfunc (e *playgroundResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\r\n\tresource.ImportStatePassthroughID(ctx, path.Root(\"id\"), req, resp)\r\n}\r\n```\r\n\r\nThe `v1.8.0-beta1` version of Terraform appears to be using `github.com\/zclconf\/go-cty v1.14.3` which contains the fix for a previously reported, somewhat similar issue - #34589 \n\n### References\n\n- #34589","comments":["Thanks @bendbennett!\r\n\r\nThis is quite peculiar, because while the plan appears to have stored the truncated value, at least within Terraform alone the final state still ends up with the correct `9223372036854775808`, meaning the problem is entirely within the rendering of the plan to both the CLI and JSON.\r\n\r\nIf your example (which I haven't tried yet) ends up with the incorrect value in state, then there is a separate bug to track down somewhere in the protocol\/framework.","This is also potentially related to https:\/\/github.com\/hashicorp\/terraform\/issues\/34686, which I thought had been fixed. There might have been another rendering path somewhere that has been missed.","Just as little addition here:\r\n\r\n```~\/terraform\/34866 > tf show -json planfile | jq\r\n{\r\n  \"format_version\": \"1.2\",\r\n  \"terraform_version\": \"1.9.0-dev\",\r\n  \"planned_values\": {\r\n    \"root_module\": {\r\n      \"resources\": [\r\n        {\r\n          \"address\": \"tfcoremock_simple_resource.resource\",\r\n          \"mode\": \"managed\",\r\n          \"type\": \"tfcoremock_simple_resource\",\r\n          \"name\": \"resource\",\r\n          \"provider_name\": \"registry.terraform.io\/hashicorp\/tfcoremock\",\r\n          \"schema_version\": 0,\r\n          \"values\": {\r\n            \"bool\": null,\r\n            \"float\": null,\r\n            \"integer\": null,\r\n            \"number\": 9223372036854776000,\r\n            \"string\": null\r\n          },\r\n          \"sensitive_values\": {}\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  \"resource_changes\": [\r\n    {\r\n      \"address\": \"tfcoremock_simple_resource.resource\",\r\n      \"mode\": \"managed\",\r\n      \"type\": \"tfcoremock_simple_resource\",\r\n      \"name\": \"resource\",\r\n      \"provider_name\": \"registry.terraform.io\/hashicorp\/tfcoremock\",\r\n      \"change\": {\r\n        \"actions\": [\r\n          \"create\"\r\n        ],\r\n        \"before\": null,\r\n        \"after\": {\r\n          \"bool\": null,\r\n          \"float\": null,\r\n          \"integer\": null,\r\n          \"number\": 9223372036854776000,\r\n          \"string\": null\r\n        },\r\n        \"after_unknown\": {\r\n          \"id\": true\r\n        },\r\n        \"before_sensitive\": false,\r\n        \"after_sensitive\": {}\r\n      }\r\n    }\r\n  ],\r\n  \"configuration\": {\r\n    \"provider_config\": {\r\n      \"tfcoremock\": {\r\n        \"name\": \"tfcoremock\",\r\n        \"full_name\": \"registry.terraform.io\/hashicorp\/tfcoremock\"\r\n      }\r\n    },\r\n    \"root_module\": {\r\n      \"resources\": [\r\n        {\r\n          \"address\": \"tfcoremock_simple_resource.resource\",\r\n          \"mode\": \"managed\",\r\n          \"type\": \"tfcoremock_simple_resource\",\r\n          \"name\": \"resource\",\r\n          \"provider_config_key\": \"tfcoremock\",\r\n          \"expressions\": {\r\n            \"number\": {\r\n              \"constant_value\": 9223372036854775808\r\n            }\r\n          },\r\n          \"schema_version\": 0\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  \"timestamp\": \"2024-03-20T14:44:45Z\",\r\n  \"applyable\": true,\r\n  \"complete\": true,\r\n  \"errored\": false\r\n}\r\n```\r\n\r\nThe correct value is written within the `configuration` block in the JSON output, but the truncated value within the `resource_changes`.","The problematic number raised here is `2^63`. `(2^63)-1` and `(2^63)+1` are not affected and other numbers close by seem to be unaffected (at least I couldn't find any). But the following are also affected:\r\n\r\n- `2^64`\r\n- `2^65`\r\n- `2^66`\r\n- `2^67`\r\n- `2^68`\r\n\r\nAnd the same pattern seems to continue, with other numbers around those numbers being unaffected and only the exact whole numbers being truncated. I don't have an explanation for this, and I don't think this is in the rendering code of the human-readable plan as this is being printed by the JSON output as well. Very strange.","Also, no number satisfying `x < 2^63` is affected so `2^62` and `2^61` and so on are fine.","These seem like the \"breakpoints\" where the floating point exponent would increase, making the mantissa the same (1) for all of these examples, right?\r\n\r\nSo it seems like what they have in common is that the exponent is >= 63 and the exponent is 1. I don't know what to conclude from that yet, but it is at least an interesting pattern!\r\n","There's still something in `go-cty` or `msgpack` causing this, I've added some test cases to go-cty that demonstrate this. Might be helpful as a starting point for whoever ends up picking this up.","One way we've seen symptoms like this before was accidentally creating a `big.Float` with only 52-bit precision instead of full precision. I think that mainly arose in converting from `float64` to `big.Float`, where unless you override it the precision of the source float is preserved. But I wonder if there's something similar going on when parsing `big.Float` from a string on certain codepaths.\r\n\r\n","I've debugged enough to at least describe the circumstances that cause the bug:\r\n\r\nIf the given number is a whole number that would fit in an int64 then `cty\/msgpack` encodes as the smallest possible MessagePack integer format. MessagePack does not have integer formats greater than signed 64-bit.\r\n\r\nIf the number is too big for an int64 then `cty\/msgpack` then tries to take the `big\/float` as a `float64` and pass _that_ to MessagePack. It's _that_ entry point that causes this bug to happen. It seems that `big.Float.Float64` is indicating that the result has \"exact\" accuracy in that case, which I think makes sense because mantissa 1 and exponent >=63, <2048 can fit into a float64 without loss. The number one less than that would have a mantissa that _doesn't_ fit in a `float64`, and so `cty\/msgpack` would encode it as a string.\r\n\r\nTherefore it seems that the problem is in the round-tripping of some values when they get encoded as `float64`. What I've not determined yet, but intend to determine next, is exactly where the precision is being lost.\r\n","It does indeed seem to be a repeat of the \"`big.Float` with only `float64` precision\" problem. Specifically, `cty.NumberFloat64` constructs a `big.Float` based on a `float64` and doesn't override the precision, so the result has precision 52.\r\n\r\nExplicitly setting the precision to 512 (which is the convention elsewhere in `cty`) makes the string representation the same before and after. Using a smaller precision apparently causes the formatter to do some rounding.\r\n\r\nI'm going to fix this upstream by explicitly setting the precision to 512 before populating the `big.Float` from the `float64`. That should then fix anything that uses `cty.NumberFloatVal`, which includes the MessagePack unmarshaler.\r\n","Actually, I've changed my mind: unilaterally deciding to expand the precision of all numbers when converting to `float64` has some annoying effects on the string representations of _fractional_ numbers that can't be exactly represented in floating point, which I now remember is why this is written the way it is.\r\n\r\nInstead of pretending incoming values are higher precision than they are, we instead expand the precision \"just in time\" during the operations that would benefit from it, so that we're doing _calculations_ with higher precision when needed, but avoid implying in string representations that there's more precision available than was present in the source value.\r\n\r\nI think the most narrow fix here is to avoid using the `float64` encoding for whole numbers even if they can technically fit there, and instead have the encoder use the string representation of any integer that's too big to represent as one of the MessagePack integer encodings. That'll ensure that they get decoded at full precision, because we _do_ always use precision 512 for numbers parsed from strings.\r\n","I've fixed this upstream in cty v1.14.4. The specific commit is https:\/\/github.com\/zclconf\/go-cty\/commit\/f41ae52fdfa8a9590aa00c3ab3ff13cef4dd872f .\r\n\r\nHowever, it's important to note that I fixed this by changing the MessagePack _encoder_ to use a different encoding for large integers. `terraform-plugin-go` has [its own MessagePack implementation that has the same behavior](https:\/\/github.com\/hashicorp\/terraform-plugin-go\/blob\/1d146a83a4bb50a5b96c5f7e0828c6a5d40b1281\/tftypes\/value_msgpack.go#L449-L453), so upgrading `cty` here should fix it in the core-to-provider direction, but will not fix it in the provider-to-core direction.\r\n\r\nI'm not sure which was causing the problem here -- perhaps it's both, since we round-trip through MessagePack in both directions after all -- but either way I would suggest making a similar change to `terraform-plugin-go` so that the treatment of numbers will be the same whichever way they are travelling.\r\n\r\nI'm not going to propose any upgrade change in Terraform yet, because I expect we'll probably want to coordinate the `cty` change and the `terraform-plugin-go` change to release at a similar time so we can minimize the window during which things are inconsistent. If someone else wants to open a PR to upgrade to newer `cty` once we're feeling ready to do it, please go ahead!\r\n","Thank you all for the comprehensive investigation and feedback everyone. Much appreciated.\r\nThanks also for the fix in cty `v1.14.4`.\r\nI've opened an [issue](https:\/\/github.com\/hashicorp\/terraform-plugin-go\/issues\/391) on terraform-plugin-go to track coordination of the change in core and the change that needs to be made to terraform-plugin-go."],"labels":["bug","new"]},{"title":"Error when loading plugin schemas outputs diagnostics suffixed with double period","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.8.0-dev\r\non darwin_arm64\r\n\r\nv1.8.0-beta1\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nresource \"example_resource\" \"example\" {\r\n  configurable_attribute = provider::example::string_len(\"some-value\")\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/bendbennett\/6e635384603062b983df30f74b9bbacc\r\n\r\n### Expected Behavior\r\n\r\nExpected the last line of the practitioner-facing error message from the CLI to be suffixed with a single period:\r\n\r\n```shell\r\n\u2502 Function \"string_len\": Parameter at position 0 does not have a name.\r\n```\r\n\r\n### Actual Behavior\r\n\r\nThe last line of the practitioner-facing error message from the CLI is suffixed with a double period:\r\n\r\n```shell\r\n\u2502 Function \"string_len\": Parameter at position 0 does not have a name..\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n2. `terraform apply`\r\n\r\n### Additional Context\r\n\r\nThis appears to be because the diagnostics are suffixed with a period in two locations:\r\n\r\n- https:\/\/github.com\/hashicorp\/terraform\/blob\/v1.8.0-beta1\/internal\/terraform\/schemas.go#L109\r\n- https:\/\/github.com\/hashicorp\/terraform\/blob\/v1.8.0-beta1\/internal\/terraform\/context.go#L173\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @bendbennett,\r\n\r\nOur typical convention is that `error.Error()` results should follow the Go convention of _not_ ending with a period, whereas our human-oriented diagnostics are written as full sentences (or paragraphs) and so _do_ use periods to end sentences.\r\n\r\nBecause of that, whenever we're using an `error.Error()` as part of a diagnostic message, we typically add a period after it to translate between the two conventions.\r\n\r\nFrom the locations you linked it seems like this symptom would arise if the `error.Error()` result also had a period, which would be contrary to both our conventions and typical Go idiom. If that's true, I think the fix would be to remove the errant period from the end of the string representation of the error. Is that what's going on here, or did I misunderstand?\r\n","Hi @apparentlymart,\r\n\r\nThanks for looking into this.\r\n\r\nApologies if I've misunderstood. It seems the double period is because the `error` returned by `loadSchemas()` is generated from a call to `diags.Err()`, and the diagnostics  already include a period. So if I'm following your line of reasoning, then perhaps the `error` returned by `diags.Err()` should not contain a suffixed period?","Generating an `error` from a `Diagnostics` and then inserting its string representation into another diagnostic is indeed a situation that can cause this problem. We try to avoid doing that wherever possible -- a diagnostics wrapped in an error should typically be appended directly to another diagnostics, which then automatically unwraps the wrapped diagnostics and appends each of them separately. But if course that can't happen if it's formatted into part of another diagnostic message.\r\n\r\nLooking around at the context near the code snippets you pointed to, I think what we have here is some leftover tech debt from the original shift from using `error` to diagnostics when reporting errors to users. During that transition period we had a lot of APIs that had to still keep returning `error` for a while due to having multiple callsites to update, and so we used this sort of error wrapping and then unwrapping again at the callers that _had_ been updated.\r\n\r\nThere thankfully aren't many example of that left anymore, but this seems like one we missed. The `loadSchemas` function should change to return diagnostics instead of an error, and then the caller should just return those diagnostics directly instead of trying to build its own error diagnostic.\r\n"],"labels":["bug","new"]},{"title":"Can not run `terraform console` when I am in the projects directory","body":"### Terraform Version\n\n```shell\nterraform --version\r\nTerraform v1.7.4\r\non darwin_amd64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.7.5. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\n$ cat main.tf\r\nmodule \"private-cloud\" {\r\n  source                       = \".\/modules\/private-cloud\"\r\n}\r\n```\r\n\n\n### Debug Output\n\n$ terraform console\r\n\u2577\r\n\u2502 Error: Module not installed\r\n\u2502\r\n\u2502   on main.tf line 1:\r\n\u2502    1: module \"private-cloud\" {\r\n\u2502\r\n\u2502 This module is not yet installed. Run \"terraform init\" to install all modules required by this\r\n\u2502 configuration.\r\n\u2575\r\n\n\n### Expected Behavior\n\nNo error should occur\n\n### Actual Behavior\n\nError message returned\n\n### Steps to Reproduce\n\nIf I go into empty directory then `terraform console` works fine.\n\n### Additional Context\n\nI run the command above only to generate couple of UUIDs. I did not expect it to fail.\n\n### References\n\n_No response_","comments":["Hi @EugenKon,\r\n\r\nThe behavior you've described is the intended behavior, because Terraform needs to have this module installed to know what to return if you type `module.private-cloud` into the console. If you run `terraform init` as suggested then that should install the needed dependencies so that the console would work.\r\n\r\nRunning `terraform console` in an empty directory works because Terraform treats that as an entirely empty module that doesn't declare anything and therefore doesn't depend on anything. That's not really an intended way to use Terraform, but it works because a directory without any `.tf` files effectively declares nothing.\r\n\r\nIt seems like this issue is effectively a feature request for some sort of new `terraform console` mode that ignores the configuration completely and only allows evaluating functions with constant values. In other words, to pretend that the current directory is empty even when it isn't. Is that right?\r\n","@apparentlymart You are right. It would be nice to be able to evaluate functions with constant values. \r\n\r\nBased on said, I suggest next: show error message above as is, then show warning message, which will tell to user that he will be able to evaluate function with constant values and then show console like it was shown for empty folder.\r\nThank you.","Thanks for confirming, @EugenKon!\r\n\r\nSo I think the proposal then is: if `terraform console` finds that the configuration is invalid, instead of immediately exiting with an error exit code it could instead behave as if the configuration were totally empty, but still allow evaluating any expression that doesn't refer to something declared in the module.\r\n\r\nAs you note, Terraform should probably generate a warning in that case so that it's clear to the user that the console cannot evaluate anything that refers to objects in the configuration.\r\n\r\n`terraform console` is already able to produce a partial evaluation context when the configuration is statically valid but dynamically incorrect -- it says \"Due to the problems above, some expressions may produce unexpected results.\" and then shows the prompt. The question for this issue is how practical it would be for Terraform to do the same when the configuration can't load at all.\r\n\r\nUnfortunately I think that's not easy because [the configuration gets loaded by the same function that prepares the evaluation context](https:\/\/github.com\/hashicorp\/terraform\/blob\/eecbfbf2cccedfb581469ee63599c4b4abc5c60f\/internal\/command\/console.go#L106-L111), and so it would take at least some refactoring to be able to ask for an evaluation context _without_ loading the configuration.\r\n\r\nI'm going to relabel this as an enhancement to reflect that some further design work is needed to figure out exactly how to solve the problem. In the meantime I suggest using an empty directory -- or, at least, a directory without any `.tf` files -- if you want to evaluate expressions in a constants-only scope.\r\n","@apparentlymart Great! Thank you"],"labels":["enhancement","cli"]},{"title":"Proposal for minimizing\/removing secrets from the Terraform state file","body":"# Proposal for minimizing\/removing secrets from the Terraform state file\r\n\r\nWith the additional benefit of low privilege Terraform plans.\r\n\r\n# Background:\r\n\r\nTerraform is a game changer for infrastructure management, the ability to maintain everything as code is a godsend and our entire organization is using it to manage thousands of resources across 10 teams.\r\n\r\nDuring our 3 years of setting up this infrastructure, we've noticed one issue that has been hard to reconcile: The fact that Terraform is \"all powerful\" whilst it is the only acceptable way of changing our infra. These two concepts are constantly at odds with each other. But it doesn't have to be this way.\r\n\r\n`terraform apply` is really the dangerous operation. It affects the infra, it can delete crucial resources, it has \"write\" permissions. `terraform plan` however, does not do anything like that. It just does a bunch of GET requests, compares to the state, and proposes changes, it has \"read\" permissions.\r\n\r\nThis mean `terraform plan` should be runnable by any engineer at our company. They can make changes to the Terraform code on their git branch, run `terraform plan` locally, evaluate their changes, iterate, and eventually create a pull request, that runs another `terraform plan`, to be reviewed by the gatekeepers of our organization. Works great, everyone can develop with velocity, and the security is maintained.\r\n\r\nHowever, its not that simple. There are a few issues that make `terraform plan` nearly as privileged as `terraform apply`. Resources contain secrets in their configurations. These secrets need to be read by `terraform plan` from a datasource API and later during Terraform refresh read again from the resource API for change detection. This means the underlying accounts needs permissions to read secrets from these APIs. Additionally, these secrets are then kept in the Terraform state, which is also readable by `terraform plan`.\r\n\r\nDuring plan we'd want all operations to be as little privileged as possible, this way all engineers can develop and run `terraform plan`` without needing access to any secret values that could be misused for \"write level\" operations later.\r\n\r\n# Proposal\r\n\r\nPlease consider the following proposal for fixing this:\r\n\r\nRead operations defined by the providers could be split between \"plan\" and \"apply\" time reads.\r\n\r\n1. On plan time reads: Resources and datasources would have the option to omit the secrets and return metadata instead which uniquely identifies the secret value\r\n2. On apply time reads: Resources and datasources would return the same data with the secrets included\r\n\r\nThis would require a change in the contract between Terraform and its providers. And 1 change to the way Terraform calls the providers during \"read\" operations.\r\n\r\nBackwards compatibility should be easy enough, if the provider does not define separate plan and apply time read operations, just the plan time read operation would be called.\r\n\r\nThe information returned during the apply time read would have to be consistent with the earlier plan time read. However this should solvable possible in most situations. (eg. Metadata or a unique version identifier). Also there could be a check that all the other fields haven't changed in the meantime.\r\n\r\nThis also means the underlying platforms and APIs should support separate metadata and read permissions. This is already supported by eg Vault and GCP secrets.\r\n\r\n# An example:\r\n\r\nA datasource \"vault_kv_secret_v2\" is used to configure a resource \"google_secret_manager_secret_version\".\r\n\r\nThe 2 read operations will be referred to as \"plan_read\" and \"apply_read\"\r\n\r\nIn this example, both these resources have been updated to use the following endpoints:\r\n\r\n- vault_kv_secret_v2 plan_read: https:\/\/developer.hashicorp.com\/vault\/api-docs\/secret\/kv\/kv-v2#read-secret-metadata\r\n- vault_kv_secret_v2 apply_read: https:\/\/developer.hashicorp.com\/vault\/api-docs\/secret\/kv\/kv-v2#read-secret-version\r\n- google_secret_manager_secret_version plan_read: https:\/\/cloud.google.com\/secret-manager\/docs\/reference\/rest\/v1\/projects.secrets.versions\/list or https:\/\/cloud.google.com\/secret-manager\/docs\/reference\/rest\/v1\/projects.secrets\/get\r\n- google_secret_manager_secret_version apply_read: https:\/\/cloud.google.com\/secret-manager\/docs\/reference\/rest\/v1\/projects.secrets.versions\/get\r\n\r\nCreation Plan (using low privileges):\r\n1. Terraform calls plan_read of vault_kv_secret_v2, which will return only the metadata.\r\n2. Terraform plans a create action and includes it in the `.tfplan`. This will include a reference to vault_kv_secret_v2 and the metadata (no secrets)\r\n\r\nCreation Apply (using higher privileges):\r\n1. Terraform calls apply_read apply_read of vault_kv_secret_v2, which will return the secret data\r\n2. Terraform creates google_secret_manager_secret_version using the metadata\r\n3. The Terraform state is updated with the new resource, which instead of the secret value, contains the metadata returned by the endpoint, as well as a reference to the datasource that is the source for the secret.\r\n4. For the datasource the metadata is also stored.\r\n\r\nUpdate\/Read Plan:\r\n1. Terraform calls plan_read of vault_kv_secret_v2, which will return only the metadata.\r\n2. Terraform calls plan_read of google_secret_manager_secret_version, which will return only the metadata.\r\n3. Terraform compares the metadata of both to the known metadata in the state\r\n4. If the metadata is the same Terraform does nothing with it and continues.\r\n5. If the metadata is different Terraform will include changes for all dependent resources in the state with a value that won't be known until apply.\r\n\r\nUpdate: Apply\r\n1. Terraform calls apply_read apply_read of vault_kv_secret_v2, which will return the secret data\r\n2. Terraform updates google_secret_manager_secret_version using the metadata\r\n3. The state is updated with the new metadata\r\n\r\n# Our current workarounds\r\n\r\n1. We let all applications that can, fetch directly from our vault. This meant Terraform does not need read permissions at all.\r\n2. We set up \"proxy\" serverless functions, that are privileged to read from vault, and configure the secrets in the destination. `terraform apply` has permissions to grant them access to the vault secrets, without having access itself. `terraform plan` only has permissions to view the permissions of the serverless functions, not the secrets themselves.\r\n3. We don't generate secrets with Terraform, or accept the fact they will be visible in the state. We experimented with serverless functions or null_resources to generate these. But this caused issues in the resource dependencies since the secrets aren't available yet at plan.\r\n\r\n# Expansions on the idea\r\n\r\n1. Secret generation currently done by the \"tls\" or \"random\" providers could be done by secret providers such as HashiCorp Vault.\r\n2. Resources that don't support returning the metadata, but don't return the secrets could just be \"assumed\" to not be updated in the background. (depending on provider implementation or lifecycle.ignore_changes)\r\n3. Resources that don't support returning the metadata, and do return the secrets could be \"assumed\" to not update at all (including non-secret fields). This would require Terraform to add a \"refresh=false\" flag to the lifecycle.\r\n\r\n# Additional benefits of apply time and read time fetches:\r\n\r\nAnother issue to setting up lower privileged plans is that provider configurations could depend on a datasource for authentication eg google_service_account_jwt for accessing vault in our GCP pipelines. If these values could update between plan and apply, it could be swapped for a higher privilege token on apply. In addition to the token being fresh.\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/issues\/32100\r\n\r\n# Related issues:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/issues\/516\r\nhttps:\/\/github.com\/hashicorp\/terraform\/issues\/32100\r\n","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nI've sent this to our product manager for review as well. Thanks again!","Thanks for the write-up @rwblokzijl!\r\n\r\nThis sounds more like a proposal to help remove sensitive information from the plan, which is definitely a valid concern. In the current architecture, delaying the read of secrets until apply would still end up with the same data stored in the state. Just to clarify some things here while other research is in progress, points which can't be addressed by these changes alone are as follows:\r\n\r\n - There is not yet a language-level definition for \"secret\", which is something that needs to be codified first. We can't only make use of attributes marked as \"sensitive\", because providers can't account for the change in protocol when they have already defined many sensitive fields. Many existing sensitive fields need to maintain the current workflow to not break their functionality.\r\n - Secrets as commonly conceptualized are often required for managed resources, which must currently store the value in the state regardless of whether it's provided during plan or apply. A new concept for the type and handling of the data would need to be created to prevent this storage (some research has been done in this area already, but it's not easy to fit in the existing language and protocol), or maybe they aren't allowed within managed resources.\r\n - Many examples of providers and resources require their secrets in order to successfully create a plan; delaying the reading of secrets until apply will break these use cases.\r\n - Only removing the secrets sometimes (i.e. making exceptions when they are needed for planning) means they can still appear in the plan. A solution which does not entirely remove secrets from state is not an improvement in security\r\n\r\nThe idea that `apply` is an action which requires elevated privileges is interesting. That might not be a complete solution for the CLI tool on its own, but within a managed workflow it might be part of a larger solution for secrets management."],"labels":["enhancement","new"]},{"title":"Fix leakage of sensitive variables on HCL syntax error in variable declaration","body":"# Issue\r\n\r\nIf a .tfvar file contains a syntax error the details are printed regardless of potential sensitivity label.\r\nThis occurs when running terraform apply and\/or plan.\r\n\r\n# Solution\r\n\r\nParse the loaded .tf files, for each variable look up the sensitivity label and create a variable-sensitivity map.\r\nThe diagnostics for the error caused by erronous syntax contains a context telling us which part of the .tfvar file the error belongs to, use this context to map the error to the appropriate variable.\r\nUse the variable name to check sensitivity status from the map we built, and add sensitivity info to the diags ExtraInfo property.\r\n\r\nIn the diagnostics section, before printing, check the ExtraInfo section for a sensitivity label, if present, redact the sensitive information.\r\n\r\n## Before\r\n\r\n```bash\r\n$ .\/terraform plan\r\n\u2577\r\n\u2502 Error: Missing key\/value separator\r\n\u2502\r\n\u2502   on terraform.tfvars line 3:\r\n\u2502    1: secretConfig = { \"something\" = \"extremely confidential\",\r\n\u2502    2:                 \"key\" = \"val\",\r\n\u2502    3:                 \"oops\" }\r\n\u2502\r\n\u2502 Expected an equals sign (\"=\") to mark the beginning of the attribute value.\r\n\u2575\r\n```\r\n\r\n## After\r\n\r\n```bash\r\n$ .\/terraformPatched plan\r\n\u2577\r\n\u2502 Error: Missing key\/value separator\r\n\u2502\r\n\u2502   on terraform.tfvars line 3:\r\n\u2502    1: (SENSITIVE)\r\n\u2502\r\n\u2502 Expected an equals sign (\"=\") to mark the beginning of the attribute value.\r\n\u2575\r\n```\r\n\r\nFixes #31946 \r\n\r\n\r\n### BUG FIXES\r\n\r\n- `terraform plan`: Fixed leakage of sensitive values caused by HCL syntax error in .tfvar file.\r\n- `terraform plan`: Fixed leakage of sensitive values caused by HCL syntax error in .tfvar file.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34859) <br\/>All committers have signed the CLA.","Thanks for this submission! I will bring it to triage next week. "],"labels":["bug"]},{"title":"docs(tests): add some info about how the states are managed in memory","body":"# Context\r\n\r\n<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\nAdding information about the fact that the states created by the tests files are not accessible for the users. It gives more meaning to the \"managed internally\" that can be ambiguous.\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes : https:\/\/github.com\/hashicorp\/terraform\/issues\/34668\r\n\r\nRelates to : https:\/\/github.com\/hashicorp\/terraform\/issues\/33786\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\nNext minor or patch release.\r\n\r\n## Draft CHANGELOG entry\r\n\r\n### ENHANCEMENTS\r\n\r\n- docs(tests): add some info about how the states are managed in memory by the tests files","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34858) <br\/>All committers have signed the CLA.","Thanks! I'll bring this up in triage for review. "],"labels":["documentation"]},{"title":"docs: Dependency upgrading","body":"This new document is a mixture of context that was previously just institutional knowledge held by me and a few other Terraform Core members, and of new information I've learned while practicing dependency upgrades under our new multi-module repository layout.\r\n\r\nI've documented this as a starting point for those who are completely new to the codebase, and for those who were already familiar but haven't yet performed upgrades in the new multi-module layout.\r\n\r\nThis all still remains slightly in flux because I'm using real dependency upgrades to practice different interactions between the different modules, and so this is likely to evolve as we learn more, but we need to have at least some initial docs on this around in case we need to perform upgrades in an emergency situation where I am not around to assist directly. (Since I'm currently the only person with direct experience doing upgrades in this new model.)\r\n","comments":[],"labels":["documentation","build"]},{"title":"Implement resource discovery functionality similar to OCI Provider","body":"### Terraform Version\r\n\r\n```shell\r\n1.7.5\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nexport existing configuration, instead of running provider exe\r\n\r\n### Attempted Solutions\r\n\r\nprovider exe can do this also, but maybe just have export option on terraform itself\r\n\r\n### Proposal\r\n\r\nhave export option on terraform itself\r\n\r\n### References\r\n\r\n* [https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/API\/SDKDocs\/terraformresourcediscovery_topic-Using.htm](https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/API\/SDKDocs\/terraformresourcediscovery_topic-Using.htm)\r\n\r\nedit by @crw: The key summary of what the OCI provider is doing, from their documentation:\r\n\r\n> The Oracle Cloud Infrastructure (OCI) Terraform provider\u2019s resource discovery feature uses HashiCorp\u2019s [terraform-exec](https:\/\/github.com\/hashicorp\/terraform-exec\/) to import the discovered OCI resources into Terraform configuration and state files.","comments":["Hi @oeresundsgruppen!\r\n\r\nI'm not sure what you mean by \"export\". Can you say more about what end goal\/result you'd like to achieve? For the moment, let's focus on what this would allow you to do that you can't to today, rather than how\/where it might be implemented.\r\n\r\nThanks!\r\n","well instead of running terraform-provider-oci_v5.33.0.exe -command=export, could have terraform export somefile.tf, and same for other providers","Hi @oeresundsgruppen,\r\n\r\nI don't know what `terraform-provider-oci_v5.33.0.exe -command=export` does. Can you give a full example?\r\n\r\nThanks\r\n","it just generates a TF file for the current config, and this can then be modified and then plan\/apply to update...","Hi @oeresundsgruppen,\r\n\r\nI'm afraid I still don't really understand. I assume you mean something different by \"the current config\" than I am assuming, because for me \"the config\" already _is_ a set of `.tf` files.\r\n\r\nIt really would help if you could show a full example of how you are running that command, _concretely_ what it is producing, and what you are then doing with that result. It's tough to evaluate an enhancement request to incorporate some other tool's functionality into Terraform without knowing anything about what that tool does, how it works, and why it's useful.\r\n","terraform-provider-oci_v5.33.0.exe is used by terraform, but terraform itself does not have a way to export current actual config to tf file ?","kind of terraformer via terraform -export, but should be possible via the providers...","Hi @oeresundsgruppen, thanks for this feature request! After much internal deliberation, the team believes you must be referring to this feature of the Oracle provider: https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/API\/SDKDocs\/terraformresourcediscovery_topic-Using.htm. If this is incorrect, please let us know. \r\n\r\nIf you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"go.mod: Update OpenTelemetry dependencies to v1.24.0","body":"We previously had a strange assortment of different versions, but it's typically best to have all of these fixed to the same version because otherwise there can be quirky interactions where e.g. an interface type has changed without updating its implementors, etc.\r\n\r\nThis intentionally does not yet upgrade the contrib packages or exporters because they imply other dependency upgrades that we want to perform separately first, so we can avoid changing too much at once.\r\n\r\n---\r\n\r\n`kubernetes` and `s3` backend maintainers: your backends also indirectly depend on these. I'd _guess_ that this upgrade is harmless for them too, since we're presumably not activating telemetry in those backends or the SDKs they depend on, but I'll let you all be the judge of that.\r\n\r\nPlease approve this PR if you're unconcerned, or leave a comment if you have concerns. Thanks!\r\n","comments":[],"labels":["dependencies"]},{"title":"Testing\/Mocking Submodules","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.2\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI want to test a terraform module (\"main module\") that creates\/instantiates submodules. In my scenario, the submodule parameters (values passed from the main module as the input variables of the submodule) for the submodules are determined by a more or less complex process.\r\n\r\nA dummy example can be found below:\r\n```\r\n\/\/ main.tf\r\nmodule \"submodule\" {\r\n  source = \".\/modules\/submodule\"\r\n  my_var = 7\r\n}\r\n```\r\n```\r\n\/\/ modules\/submodule\/main.tf\r\nterraform {\r\n  required_providers {\r\n    random = {\r\n      source = \"hashicorp\/random\"\r\n      version = \"3.5.1\"\r\n    }\r\n  }\r\n}\r\n\r\nvariable \"my_var\" {\r\n  type = number\r\n}\r\n\r\nresource \"random_integer\" \"random_int\" {\r\n  min = 0\r\n  max = var.my_var\r\n}\r\n\r\noutput \"submodule_output\" {\r\n  value = 42\r\n}\r\n```\r\n\r\nNow, imagine that instead of simply hardcoding the parameters of the submodule in the main module's code and only instantiating the submodule once, I'm actually creating many instances of the submodule using `for_each` and the parameters of the submodule are determined using a more complex process (e.g. derived by doing some manipulations on the main module's variables and\/or derived from some data fetched using a `data` block).\r\n\r\nI'm looking for a way to unit-test the main module as a whole, especially verifying that the logic computing the submodule parameters is working as intended.\r\n\r\n### Attempted Solutions\r\n\r\nWith the recently added `terraform test` command, I can write unit tests for the submodule, to make sure the submodule works correctly internally. Using tests in the main module, I can write `assert` blocks on the submodule *outputs* (which might give me some hints about the input variables that were passed to the submodule instance), but in my real-world use-case, the submodule doesn't even have outputs.\r\n\r\nI tried writing `assert` blocks on the submodule parameters in the tests of the main module, but the parameters passed as variables to the submodule do not show up as attributes of the resulting `module` object, only the submodule `output`s do.\r\nExample:\r\n```\r\n\/\/ failed attempt to test that the main module passes the right parameter to the submodule\r\nassert {\r\n  condition = module.submodule.my_var == 7\r\n  error_message = \"could not validate submodule input variable\"\r\n}\r\n```\r\n\r\nSimilarly, I tried accessing the resources created by the submodule from the main module, but again, that doesn't appear to be possible\/supported. Example:\r\n```\r\n\/\/ failed attempt to test that the resources within the submodule are created using the right values (breaks isolation)\r\nassert {\r\n  condition = module.submodule.random_integer.random_int <= 7\r\n  error_message = \"expected the random number to be below 7\"\r\n}\r\n```\r\n\r\nI tried if\r\n```\r\noverride_resource {\r\n  target = module.submodule\r\n}\r\n```\r\nwould help somehow, but that just gives me an error that `override_resource` is for overriding resources, not `module`s.\r\n\r\nNext, I could simply add `output`s to my submodule that mirror the submodule input variable values 1-to-1, but that seems like an ugly hack to me.\r\n\r\nFinally, I could try write something like\r\n```\r\nrun \"create_submodule_instance\" {\r\n  module {\r\n    source = \".\/modules\/submodule\"\r\n  }\r\n\r\n  variables {\r\n    my_var = 7\r\n  }\r\n}\r\n\r\nrun \"verify_submodule_resources\" {\r\n\r\n  module {\r\n    source = \".\/test\/comparison-module\"\r\n  }\r\n\r\n  variables = {\r\n    actual_resource = run.create_submodule_instance.random_integer.random_int\r\n  }\r\n\r\n  assert {\r\n    condition = var.actual_resource.value <= 7\r\n    error_message = \"wrong value\"\r\n  }\r\n\r\n}\r\n```\r\nor similar in the tests of my main module. Here, I'd be (ab-)using the fact that one can use resources created in previous `run` blocks as variable values for later `run` blocks as a way of accessing the resources created within the submodule. (The `test\/comparison-module` would have to simply output it's input value\/object unchanged to make it visible outside.) However, this doesn't actually enable me to test the logic (in my main module) that determines the input parameters for the submodule.\r\n\r\nI'm attaching a non-working test file for the main module as a basis for playing around:\r\n```\r\n\/\/ tests\/test-submodule.tftest.hcl\r\nrun \"test_submodule\" {\r\n\r\n  command = plan\r\n\r\n  # override_resource {\r\n  #   target = module.submodule\r\n  # }\r\n  \r\n  assert {\r\n    condition = module.submodule.my_var == 7\r\n    error_message = \"could not validate submodule input variable\"\r\n  }\r\n\r\n  assert {\r\n    condition = module.submodule.random_integer.random_int <= 7\r\n    error_message = \"expected the random number to be below 7\"\r\n  }\r\n\r\n  assert {\r\n    condition = module.submodule.submodule_output == 42\r\n    error_message = \"could not validate submodule output\"\r\n  }\r\n\r\n}\r\n```\r\n\r\n### Proposal\r\n\r\nIdeally, I'd like to be able to *mock* the submodule for my tests (in a similar way that I mock providers using `mock_provider` or override data using `override_data`).\r\n\r\nFor example, I could imagine something like this:\r\n```\r\nmock_module {\r\n\r\n  \/\/ which module shall be mocked by this block?\r\n  module {\r\n    source = \".\/modules\/submodule\"\r\n  }\r\n\r\n  \/\/ mocked outputs of the submodule, replacing the actual module outputs\r\n  outputs {\r\n    submodule_output = 21 \/\/ only half the truth\r\n  }\r\n\r\n  capture_variables = true \/\/ when set to true, each input variable of the submodule module will be available as an attribute of the module object (module.submodule in this case)\r\n\r\n}\r\n```\r\n\r\nThe above would work fine as long as there's only one instance of a given module. However, in my case (since I'm creating many instances of the submodule using `for_each`), maybe a slightly more advanced version would be required. E.g.:\r\n```\r\nmock_module {\r\n\r\n  \/\/ which module shall be mocked by this block?\r\n  module {\r\n    source = \".\/modules\/submodule\"\r\n  }\r\n  \r\n  instances {\r\n    instance {\r\n      variables {\r\n        my_var = 7 \/\/ when the submodule is instantiated using the input variable `my_var` set to `7`, ...\r\n      }\r\n      outputs {\r\n        submodule_output = 21 \/\/ ... the output `submodule_output` of the mocked module will be `21` instead of the real value\r\n      }\r\n   }\r\n\r\n  \/\/ ... more instances\r\n\r\n  }\r\n}\r\n```\r\nHowever, in my case, where the submodule doesn't even have outputs, that wouldn't help me much.\r\n\r\nOr, to give the programmer\/tester even more flexibility:\r\n```\r\nmock_module {\r\n\r\n  \/\/ which module shall be mocked by this block?\r\n  module {\r\n    source = \".\/modules\/submodule\"\r\n  }\r\n  \r\n  \/\/ whenever terraform attempts to instantiate the module given above, it will actually instantiate the replacement referenced below instead\r\n  replacement {\r\n    source = \".\/tests\/replacement-submodule\"\r\n  }\r\n}\r\n```\r\nThen, I could add module in `tests\/replacement-submodule` that does whatever I want (e.g. simply mirror the input variables as `output`s).\r\n\r\nIf having a mocked version of a module is not possible, I'd like to be able to make assertions (in the main module) about the resources created in the submodule.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Terraform support for Migrating Standard to Premium Namespaces in Azure Service Bus","body":"### Terraform Version\n\n```shell\nN\/A\n```\n\n\n### Use Cases\n\nThe end goal of the feature request is to see if there's any upcoming plans or to open a path in order to support Migration from Standard to Premium Namespaces via Terraform for Azure Service Bus. There are scenarios in which infrastructure as code is used to deploy resources to Azure using a CI\/CD pipeline to deploy changes. \r\n\r\nCurrently Service Bus only supports migration via Azure Portal and CLI : https:\/\/learn.microsoft.com\/en-us\/azure\/service-bus-messaging\/service-bus-migrate-standard-premium\r\n\r\nWhen using the above methods, we see issues that causes the terraform state to be incompatible and I'd like to see if there are options to automate this using Terraform.\n\n### Attempted Solutions\n\nWe attempted to alternative approaches:\r\n\r\nOption #1:\r\n\r\n\t1. Perform the migration in the Azure Portal\/CLI then use the 'import' function to import the properties of the new premium namespace https:\/\/registry.terraform.io\/providers\/hashicorp\/azurerm\/latest\/docs\/resources\/servicebus_namespace#import \/ [Import | Terraform | HashiCorp Developer](https:\/\/developer.hashicorp.com\/terraform\/cli\/import) since we're using CI\/CD this appears to be supported also [Import - Configuration Language | Terraform | HashiCorp Developer](https:\/\/developer.hashicorp.com\/terraform\/language\/import)\r\n\r\n\t2. Additionally, there's a tool called terrafy [Azure Terrafy \u2013 Import your existing Azure infrastructure into Terraform (microsoft.com)](https:\/\/techcommunity.microsoft.com\/t5\/itops-talk-blog\/azure-terrafy-import-your-existing-azure-infrastructure-into\/ba-p\/3357653) that we tried do the import and generate the code for existing resources. \r\n\t\r\nOption # 2\r\n\r\n\t1. We tried using the AzApi module [Overview of the Terraform AzAPI provider | Microsoft Learn](https:\/\/learn.microsoft.com\/en-us\/azure\/developer\/terraform\/overview-azapi-provider) to recreate the calls made by the portal for migration. For this we wanted to make sure to add plenty of wait time or else the pipeline will fail. We understand this option would probably take the most development, because we'd have to convert the API calls into the terraform syntax.\n\n### Proposal\n\nProvide a stream-lined method to support Migration from Standard to Premium Azure Service Bus Namespaces via Terraform.\n\n### References\n\n_No response_","comments":["Hi @rokazeme,\r\n\r\nI _think_ you are describing something that would be the responsibility of the Azure provider rather than of Terraform Core, but I'm not familiar enough with Azure to know for sure.\r\n\r\nAside from the `azure` backend for state storage, nothing in this codebase is Azure-specific. [The Azure provider's repository](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm) would be a good place to discuss Azure-specific needs that are not about the `azure` state storage backend. The provider development team can open a new issue in this repository if they conclude that what you're asking for is something they'd like to support but would need something new in Terraform Core to achieve it, but we can't do anything in this repository until the Azure provider team has some specific ideas about what new features they might need.\r\n\r\n"],"labels":["enhancement","provider\/azurerm","new"]},{"title":"yamldecode fail on !!merge","body":"### Terraform Version\n\n```shell\n\u276f terraform version\r\nTerraform v1.7.4ion\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/google v5.15.0\r\n+ provider registry.terraform.io\/hashicorp\/google-beta v5.15.0\r\n+ provider registry.terraform.io\/hashicorp\/helm v2.12.1\r\n+ provider registry.terraform.io\/hashicorp\/kubernetes v2.25.2\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.2\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\r\n+ provider registry.terraform.io\/txomon\/kubernetes v2.23.0\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nnothing\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/kam1kaze\/0f2c83b3c65a9f1909c1f4f1f0f800bc\n\n### Expected Behavior\n\n```\r\n{\r\n  \"var1\" = {\r\n    \"type\" = \"string\"\r\n  }\r\n  \"var2\" = {\r\n    \"type\" = \"string\"\r\n  }\r\n}\r\n```\n\n### Actual Behavior\n\n```\r\n> echo 'yamldecode(file(\".\/test.yaml\"))' | terraform console\r\nError: Error in function call\r\n\r\n  on <console-input> line 1:\r\n  (source code not available)\r\n\r\nCall to function \"yamldecode\" failed: unsupported tag \"tag:yaml.org,2002:merge\".\r\n```\n\n### Steps to Reproduce\n\n```\r\ncat <<EOF> .\/test.yaml\r\nvar1: &test\r\n  type: string\r\nvar2:\r\n  !!merge <<: *test\r\nEOF\r\n\r\necho 'yamldecode(file(\".\/test.yaml\"))' | terraform console\r\n```\n\n### Additional Context\n\nI just faced with this issue when try to yamldecode() such kubernetes resource\r\n\r\nhttps:\/\/github.com\/Altinity\/clickhouse-operator\/blob\/release-0.23.3\/deploy\/helm\/crds\/CustomResourceDefinition-clickhouseinstallations.clickhouse.altinity.com.yaml\r\n\r\nPlease note, that `kubectl -f` and `yamlint` work fine with such resource.\n\n### References\n\n_No response_","comments":["Hi @kam1kaze,\r\n\r\nThanks for filing the issue. I'm going to relabel this as an enhancement request, since the function is currently working as designed. Whether or not an exception would be made for this functionality would need some research.\r\n\r\n The [`yamldecode` function](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/yamldecode) is documented to support a subset of `yaml1.2`. Since `!!merge` is not part of the `yaml1.2` specification at all, it was not included in implementation. It also only appears in the deprecated [yaml1.1 Draft](https:\/\/yaml.org\/type\/merge.html), so I don't think it's required by any conformant decoder, but I'll let someone more familiar with yaml follow up on the specifics.","For what it's worth, for this _particular_ example of the merge operator I don't think it's actually adding anything, since the definition of `var2` would end up matching `var1` anyway. It would be equivalent to write it like this:\r\n\r\n```yaml\r\nvar1: &test\r\n  type: string\r\nvar2: *test\r\n```\r\n\r\n...but I understand that this was probably just a simplified, contrived form to make for a simple reproduction case (thanks!), and that a more realistic example is likely to be shaped like the one in the draft that @jbardin found, where multiple different mappings would be merged together by reference.\r\n\r\nOur implementation is currently focused only on implementing the core standard. I think if we're going to consider this we'll need to figure out what our policy is for YAML extensions, since this particular one seems to have been in draft state for nearly two decades and so is presumably quite esoteric.\r\n\r\nWith Terraform v1.8 adding support for providers to expose their own functions, perhaps someone who is sufficiently motivated could make a YAML provider that offers a more liberal set of YAML features for those who need it, while the builtin one remains focused only on the core standard. (If we'd had support for provider-contributed functions before considering `yamlencode` I suspect we would've chosen to put it in a separate provider in the first place, since Terraform doesn't need a YAML encoder\/decoder library for any other reason than providing this function.)\r\n"],"labels":["enhancement","upstream","new"]},{"title":"Terraform ignores backend config declared in the file","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.5\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nbackend definition\r\n\r\n```terraform\r\nterraform {\r\n  backend \"s3\" {\r\n    acl    = \"bucket-owner-full-control\"\r\n  }\r\n}\r\n```\r\n\r\nbackend config file `config.s3.tfbackend` \r\n\r\n```terraform\r\nbucket   = \"bucket-a\"\r\nkey      = \"key-b\"\r\nrole_arn = \"arn:aws:iam::363883556297:role\/role-c\"\r\n```\r\n\r\nterraform init call\r\n```shell\r\nAWS_REGION=us-east-1 TF_IN_AUTOMATION=true terraform init -backend-config=config.s3.tfbackend\r\n```\r\n\r\n### Debug Output\r\n\r\n```\r\n2024-03-11T08:26:36.575Z [INFO]  Terraform version: 1.5.5\r\n2024-03-11T08:26:36.576Z [DEBUG] using github.com\/hashicorp\/go-tfe v1.26.0\r\n2024-03-11T08:26:36.576Z [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.16.2\r\n2024-03-11T08:26:36.576Z [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.0\r\n2024-03-11T08:26:36.576Z [DEBUG] using github.com\/zclconf\/go-cty v1.12.2\r\n2024-03-11T08:26:36.576Z [INFO]  Go runtime version: go1.20.7\r\n2024-03-11T08:26:36.576Z [INFO]  CLI args: []string{\"terraform\", \"init\", \"-backend-config=config.s3.tfbackend\"}\r\n2024-03-11T08:26:36.576Z [TRACE] Stdout is not a terminal\r\n2024-03-11T08:26:36.576Z [TRACE] Stderr is not a terminal\r\n2024-03-11T08:26:36.576Z [TRACE] Stdin is not a terminal\r\n2024-03-11T08:26:36.576Z [DEBUG] Attempting to open CLI config file: \/root\/.terraformrc\r\n2024-03-11T08:26:36.576Z [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2024-03-11T08:26:36.576Z [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-03-11T08:26:36.576Z [DEBUG] ignoring non-existing provider search directory \/root\/.terraform.d\/plugins\r\n2024-03-11T08:26:36.576Z [DEBUG] ignoring non-existing provider search directory \/root\/.local\/share\/terraform\/plugins\r\n2024-03-11T08:26:36.576Z [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2024-03-11T08:26:36.576Z [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2024-03-11T08:26:36.576Z [INFO]  CLI command args: []string{\"init\", \"-backend-config=config.s3.tfbackend\"}\r\nInitializing the backend...\r\n2024-03-11T08:26:36.579Z [TRACE] Meta.Backend: merging -backend-config=... CLI overrides into backend configuration\r\n2024-03-11T08:26:36.579Z [TRACE] Meta.Backend: built configuration for \"s3\" backend with hash value 1166112984\r\n2024-03-11T08:26:36.579Z [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2024-03-11T08:26:36.579Z [DEBUG] New state was assigned lineage \"b0ddd7df-efe8-dd89-2749-563a30cfbd39\"\r\n2024-03-11T08:26:36.579Z [TRACE] Meta.Backend: moving from default local state only to \"s3\" backend\r\n2024-03-11T08:26:36.579Z [DEBUG] checking for provisioner in \".\"\r\n2024-03-11T08:26:36.580Z [DEBUG] checking for provisioner in \"\/usr\/bin\"\r\n2024-03-11T08:26:36.580Z [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2024-03-11T08:26:36.580Z [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2024-03-11T08:26:36.580Z [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2024-03-11T08:26:36.580Z [TRACE] statemgr.Filesystem: read nil snapshot\r\n2024-03-11T08:26:36.580Z [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2024-03-11T08:26:36.580Z [DEBUG] command: asking for input: \"region\"\r\nregion\r\n  AWS region of the S3 Bucket and DynamoDB Table (if used).\r\n  Enter a value: 2024-03-11T08:26:36.580Z [ERROR] UIInput scan err: EOF\r\n\u2577\r\n\u2502 Error: Error asking for input to configure backend \"s3\": region: EOF\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n\u2577\r\n\u2502 Error: \"region\": required field is not set\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n\u2577\r\n\u2502 Error: \"key\": required field is not set\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n\u2577\r\n\u2502 Error: \"bucket\": required field is not set\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n```\r\n\r\n### Expected Behavior\r\n\r\nTerraform initializes successfully.\r\n\r\n\r\n### Actual Behavior\r\n\r\nTerraform fails to init\r\n\r\n### Steps to Reproduce\r\n\r\n1. create state file with empty s3 backend\r\n2.  create backend config file declaring required fields - bucket, key\r\n3. run `terraform init` as defined above\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @pazderak,\r\n\r\nThanks for filing the issue. Terraform v1.5 is no longer being developed. Please confirm this is still an issue with a current Terraform release (which at this moment is `v1.7.4` or `v1.8.0-beta1`). You may want to consult the changelog for any relevant changes to the `s3` backend.\r\n\r\nThanks!"],"labels":["bug","waiting-response","backend\/s3","new","waiting for reproduction"]},{"title":"Add support for crc32c checksums to http backends","body":"### Terraform Version\n\n```shell\nlatest\n```\n\n\n### Use Cases\n\nToday internal\/backend\/remote-state\/http\/client.go adds md5 checksum headers. I'd like to provide crc32c headers to the http backend. crc32c is generally faster than md5 and is a good fit for checksumming as it is an error detecting code.\r\n\r\nThe codebase I work on discourages use of md5 which is my primary motivation to request this.\n\n### Attempted Solutions\n\nn\/a\n\n### Proposal\n\nI can see two ways to implement this.\r\n\r\n1. (simple) In addition to `Content-MD5` include `Content-CRC32C` headers.\r\n2. (more involved) Add a configuration option for http backends that allows enabling\/disabling checksumming.\r\n\r\n\n\n### References\n\n_No response_","comments":["If helpful I can author a PR for this but wanted to discuss with maintainers before presuming this would be welcome.","Apologies, I am not sure how I lost track of this one. Given the current de-emphasis on backends, we would not be able to review a PR that made these changes. For the moment, the http backend is essentially frozen. I'll update this issue if that changes. Thanks for your offer!","Thanks @crw. If things change, do let me know. \r\n\r\nIs the de-emphasis on backends a priority at the moment, or is there a different way we should be extending our use of Terraform where we have to store state in a bespoke backend\/storage system?"],"labels":["enhancement","new"]},{"title":"Allow configuring default hostname for localterraform.com","body":"### Terraform Version\n\n```shell\nTerraform v1.7.2\r\non linux_amd64\n```\n\n\n### Use Cases\n\nTerraform allows you to use a [generic hostname](https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/registry\/using#generic-hostname-terraform-cloud-and-terraform-enterprise) to reference the current instance of terraform cloud or terraform enterprise as the module registry. \r\n\r\nHowever, when working on  a module that doesn't have a remote backend configured, this domain doesn't work. \r\n\r\nIn my case, I have multiple instances of terraform enterprise,  in different network partitions, that can't talk to each other. and want to be able to share module code between them, so I want to be able to use a single domain that references the current instance. But the current solution for that means I can't run `terraform validate` on my modules on workstations or in CI. \n\n### Attempted Solutions\n\nThe above linked documentation states:\r\n\r\n> To test configurations on a developer workstation without the remote backend configured, you must replace the generic hostname with a literal hostname in all module sources and then change them back before committing to VCS.\r\n\r\nThis is, unfortunately, extremely inconvenient, especially if there are a large number of modules involved. \n\n### Proposal\n\nIf an environment variable is set (maybe TF_REGISTRY_DOMAIN), then replace instances of `localterraform.com` in module sources with that domain  if there is not a terraform remote configured. \n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @tmccombs,\r\n\r\nTerraform does have an existing mechanism for overriding the service discovery process for a particular hostname, which is intended primarily for development but is also, I believe, how Terraform Enterprise is internally making `localterraform.com` work.\r\n\r\nI've not tested this, but you might be able to get a suitable result by placing something like the following in your [CLI configuration](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/config-file) file:\r\n\r\n```hcl\r\nhost \"localterraform.com\" {\r\n  services = {\r\n    \"modules.v1\"   = \"https:\/\/tfe.example.com\/api\/registry\/v1\/modules\/\"\r\n    \"providers.v1\" = \"https:\/\/tfe.example.com\/api\/registry\/v1\/providers\/\"\r\n  }\r\n}\r\n```\r\n\r\nWhat this block does is tell Terraform CLI that any time it would normally perform [remote service discovery](https:\/\/developer.hashicorp.com\/terraform\/internals\/remote-service-discovery) for `localterraform.com` it should use the hard-coded result in the `services` argument instead. Therefore I copied into here the most relevant entries from Terraform Cloud's service discovery document, but replaced `app.terraform.io` with `tfe.example.com` since I understand from your question that you are using Terraform Enterprise rather than Terraform Cloud.\r\n\r\nAs discussed in https:\/\/github.com\/hashicorp\/terraform\/issues\/28309, there is some uncertainty right now about whether this mechanism is \"officially supported\" vs. just a development shim, but it sounds like you are interested in this primarily for development anyway, since presumably in your real Terraform Enterprise environments TFE is configuring this for you automatically anyway.\r\n\r\nThe main caveat with this approach is that it totally replaces the network service discovery for this hostname, and so if Terraform Enterprise changes the endpoint paths used for these services in future then you'll need to manually update your CLI configuration to match.\r\n\r\n---\r\n\r\nThis `localterraform.com` hack seems to be getting a bit out of hand in terms of how much it's sprawling around all of the Terraform codebases and so it might be time to think a little harder about that problem to see if we can find a solution to it that doesn't require sprinkling weird little exceptions for that hostname in different spots.\r\n\r\nPerhaps we could add an explicit way to tell Terraform CLI that it should treat one hostname as an alias for another -- not limited just to `localterraform.com` -- and then Terraform Enterprise could continue to use that domain as its convention but we'd also get the flexibility for any hostname someone might want to configure, if for example they need to migrate their private registry services from one hostname to another and so need a shim to support an old hostname as a locally-configured alias for a while during migration.\r\n","Using the `host` configuration seems to work. Although, I'm a little nervous about using an undocumented\/unsupported feature like that. \r\n\r\nI also figured out that in order for it to work with a token form configuration I had to copy the credentials config in ~\/.terraform.d\/credentials.tfrc.json for my original domain to a value for `localterraform.com`.","Ahh yes, that's a good additional thing that ought to be supported if we choose to expose a \"hostname aliases\" feature more broadly.\r\n\r\nSince we've started, I'll list a variety of things that we should probably ponder if we decide to expose an explicit way to configure aliases for hostnames:\r\n- (As above) Should Terraform use the credentials from the alias target instead of those for the aliased hostname? Or should it support either and have some defined preference order for which to take?\r\n- For hostnames appearing in module and provider addresses, should Terraform automatically rewrite the addresses to use the target hostname? Or vice-versa? For example, this might mean that the dependency lock file ends up tracking `localterraform.com\/foo\/bar` instead of `example.com\/foo\/bar` if `localterraform.com` is defined as an alias for `example.com`.\r\n    - If the answer to the above is yes, should Terraform also accept addresses containing the target hostname but rewrite them to the alias? That is, effectively treating the alias \"backwards\" for the sake of constructing unique identifiers?\r\n- Should `terraform login` accept an alias hostname for logging in? If it does, should it save the credentials it obtained as belonging to the alias, to the target, or both?\r\n\r\nThese questions all seem very answerable, but it'll require some deeper consideration of all of the use-cases associated with each of these applications of service hostnames than I can do immediately right now, so I'm mainly just leaving this as a note for me or someone else picking this up again in future.\r\n\r\nOriginally `localterraform.com` existed only as a special case in Terraform Enterprise, generating a `host` block like I previously described. It was later retrofitted into a small number of special cases in Terraform CLI to make it easier to develop modules intended for Terraform Enterprise. But it does seem like we're finding the limits of treating this as a hard-coded exception, and so I think it's time to design a general answer that can be explicitly configured in the CLI configuration and is then taken into account in a well-defined way by _all_ of Terraform's features that use service hostnames. (Of course, the current special case of implicitly adding an alias for `localterraform.com` if you use a `cloud` block in your root module would need to remain, because that's protected by compatibility promises.)\r\n\r\n---\r\n\r\nFWIW, Since Terraform Enterprise is itself depending on being able to generate `host` blocks in the CLI Configuration, any attempt to remove that capability in future would need to have a replacement immediately available, and the old way would need to remain supported for some time to keep older installations of Terraform Enterprise working, and so as long as you're doing this only for local development of modules that would be used \"for real\" only in Terraform Enterprise I don't think there's much risk in depending on this workaround in the meantime. One day you might need to swap it out for some other setting, but there would be plenty of notice that it's coming.\r\n"],"labels":["enhancement","new"]},{"title":"Don't propagate \"create_before_destroy\" chains through zero-instance resources","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.4\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.2\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n~~https:\/\/github.com\/jilyaluk\/terraform-cbd-issue-repro~~\r\n\r\nSimpler repro:\r\n```tf\r\nresource \"null_resource\" \"non_cbd\" {}\r\n\r\nresource \"null_resource\" \"cbd\" {\r\n  count = 0\r\n  triggers = {\r\n    foo = null_resource.non_cbd.id\r\n  }\r\n  lifecycle {\r\n    create_before_destroy = true\r\n  }\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/jilyaluk\/99bef012532c66968adfc3b7deb1ff3a\r\n\r\n### Expected Behavior\r\n\r\nI expect to see default delete-before-create behaviour for `null_resource.non_cbd`, as no CBD resources depend on it\r\n\r\n### Actual Behavior\r\n\r\n`null_resource.non_cbd` is marked as create_before_delete\r\n\r\n### Steps to Reproduce\r\n\r\n1. terraform init\r\n2. terraform apply -auto-approve\r\n3. terraform taint null_resource.non_cbd\r\n4. terraform plan\r\n\r\n### Additional Context\r\n\r\nSeems that the issue is with ForcedCBDTransformer:\r\n```\r\nForcedCBDTransformer: \"null_resource.non_cbd (expand)\" has CBD descendent \"module.m1.null_resource.cbd (expand)\"\r\nForcedCBDTransformer: forcing create_before_destroy on for \"null_resource.non_cbd (expand)\" (*terraform.nodeExpandPlannableResource)\r\n```\r\n\r\nIt marks `null_resource.non_cbd` as dependee of `module.m1.null_resource.cbd`, however `module \"m1\"` have no instances.\r\n\r\nI'm not sure about the complexity of the fix, but I can take a look and submit a PR.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @jilyaluk!\r\n\r\nThe plan graph from your trace log is:\r\n\r\n```\r\n  module.m1 (close) - *terraform.nodeCloseModule\r\n    module.m1.null_resource.cbd (expand) - *terraform.nodeExpandPlannableResource\r\n  module.m1 (expand) - *terraform.nodeExpandModule\r\n  module.m1.null_resource.cbd (expand) - *terraform.nodeExpandPlannableResource\r\n    module.m1.var.foo (expand) - *terraform.nodeExpandModuleVariable\r\n  module.m1.var.foo (expand) - *terraform.nodeExpandModuleVariable\r\n    module.m1 (expand) - *terraform.nodeExpandModule\r\n    null_resource.non_cbd (expand) - *terraform.nodeExpandPlannableResource\r\n  null_resource.non_cbd (expand) - *terraform.nodeExpandPlannableResource\r\n    provider[\"registry.terraform.io\/hashicorp\/null\"] - *terraform.NodeApplyableProvider\r\n  provider[\"registry.terraform.io\/hashicorp\/null\"] - *terraform.NodeApplyableProvider\r\n  provider[\"registry.terraform.io\/hashicorp\/null\"] (close) - *terraform.graphNodeCloseProvider\r\n    module.m1.null_resource.cbd (expand) - *terraform.nodeExpandPlannableResource\r\n  root - *terraform.nodeCloseModule\r\n    module.m1 (close) - *terraform.nodeCloseModule\r\n    provider[\"registry.terraform.io\/hashicorp\/null\"] (close) - *terraform.graphNodeCloseProvider\r\n```\r\n\r\nThis notation used in the logs is that the indented entries are dependencies of the unindented entry above them.\r\n\r\n`module.m1.null_resource.cbd` seems to depend on `null_resource.non_cbd` indirectly through `var.foo` and so they are in the same dependency chain, and so it's correct that the create before destroy propagates here.\r\n\r\nYou mentioned `for_each` in your report and so I'm assuming you are expecting the create before destroy propagation to take the number of instances of each resource into account. Unfortunately this isn't how Terraform works: the create before destroy rules influence the dependencies in the graph, and `for_each` cannot be evaluated until the graph has been built because the `for_each` expression can create dependencies itself.\r\n\r\nTherefore unfortunately Terraform seems to be working as designed here. If this were to be changed, we'd need to start with a new design for how `create_before_destroy` works that somehow moves its evaluation to the graph walk rather than during graph construction. I don't have any specific ideas for how to achieve that; the current `create_before_destroy` design is the result of many design iterations to hone just the right set of compromises to make it work reliably without creating dependency cycles, so I expect it won't be easy to change it to meet this new requirement.\r\n","Hi @apparentlymart!\r\n\r\nWow, thanks for such a quick and detailed response!\r\n\r\nYeah, now this does make sense, I thought that `create_before_destroy` propagates on a resource level, and it was unexpected to see it without any resources in actual plan.","I don't think there's any way to _not_ propagate `create_before_destroy` through all dependencies if we want to have consistent ordering between plans with a single configuration. A transitive dependency through a zero-instance resource is the same with or without the intermediate instances or not, and not propagating the ordering upstream will result in the same cycles regardless of whether it's skipping over empty resources or not (the destroy edges of the graph only work on instances to begin with)\r\n\r\nThere are two cases where this could possibly be conditional in this situation:\r\n - where `create_before_destroy` was only declared _within_ the resource with no instances, like in the example above.\r\n - where there are no destroy operations upstream from the `create_before_destroy` instance to create a cycle or influence update behavior (but I think that effectively doesn't change anything visible to the user, so maybe a moot point)\r\n\r\nNeither of these have been tried, because they would create inconsistencies in the planning behavior (which tends to lead to confusion from users). Whether or not something is replaced via destroy-then-create or create-then-destroy, which has ramifications for the order which dependents are updated, can make a big difference in how the resources behave in the plan. If that ordering is conditional on what changes and instances are present, it could be difficult to predict what will happen. For example, if there are resources which can not use `create_before_destroy` and it has never been observed during testing, but some combination of changes downstream in an outside module forces a new ordering, then suddenly you have a situation which cannot be planned for and is more difficult to recover from."],"labels":["enhancement","core","new"]},{"title":"Invalid resource name( A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes)","body":"### Terraform Version\r\n\r\n```shell\r\nError: Invalid resource name\r\n\u2502\r\n\u2502   on main.tf line 18, in resource \"aws_instance\" \"43103-8111-1\":\r\n\u2502   18: resource \"aws_instance\" \"3179-4114-2\" {\r\n\u2502\r\n\u2502 A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes.\r\n```\r\n\r\nI understand the actual error where it should start with letter or so but what my question why we have a restriction when cloud providers are able to accept instances which starts with numerical value.\r\n\r\nUsing below versions \r\nTerraform v1.6.5 and other system(v1.7.x) which is latest. \r\non linux_amd64\r\n\r\nboth terraform versions having this issue. \r\n\r\nI wanted to understand why we have a restrictions and do we have any workaround ??\r\n\r\nThanks for your time !!\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n...terraform config...\r\n```\r\n Error: Invalid resource name\r\n\u2502\r\n\u2502   on main.tf line 18, in resource \"aws_instance\" \"43103-8111-1\":\r\n\u2502   18: resource \"aws_instance\" \"3179-4114-2\" {\r\n\u2502\r\n\u2502 A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes.\r\n\r\n\r\n### Debug Output\r\n\r\n Error: Invalid resource name\r\n\u2502\r\n\u2502   on main.tf line 18, in resource \"aws_instance\" \"43103-8111-1\":\r\n\u2502   18: resource \"aws_instance\" \"3179-4114-2\" {\r\n\u2502\r\n\u2502 A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes.\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected VM should be provision even instance name starts with numerical value.\r\n\r\n### Actual Behavior\r\n\r\nThrowing an error - \r\n\r\n Error: Invalid resource name\r\n\u2502\r\n\u2502   on main.tf line 18, in resource \"aws_instance\" \"43103-8111-1\":\r\n\u2502   18: resource \"aws_instance\" \"3179-4114-2\" {\r\n\u2502\r\n\u2502 A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nterraform init -migrate-state \r\nterraform apply\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Note: I know the possible workaround to use any letter or _  before numeric value, but I'm expecting a workaround without modifying the instance name(still I want provision with exact name which starts with numeric value).","Hi @skmtrian,\r\n\r\nThis restriction exists because otherwise a reference to this resource -- `aws_instance.3179-4114-2` -- would appear to be the following tokens:\r\n\r\n* `aws_instance` identifier\r\n* dot\r\n* number 3179\r\n* minus operator\r\n* number 4114\r\n* minus operator\r\n* number 2\r\n\r\nAdding either a letter or an underscore to the start allows it to parse as another identifier instead, which is what's needed here because it would be strange to specify the part after the dot as an arithmetic operation.\r\n\r\nTerraform is working as designed here, so this isn't a bug. We could potentially consider it to be a feature request, but I want to be up front with you that we're unlikely to change this behavior because it would require making the configuration language parser have a special exception for resource names in particular. That would require there to be _very significant_ benefit to making that exception, which is very difficult to justify since resource names are used only within Terraform anyway, and so have no need to exactly match anything outside of Terraform that might happen to start with a digit.\r\n"],"labels":["enhancement","new"]},{"title":"Requesting machine-enumaratable workspace listing","body":"### Terraform Version\n\n```shell\nTerraform v1.6.0\r\non linux_amd64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.7.4. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Use Cases\n\n### ENHANCEMENTS\r\n\r\nThis PR proposition updates the visual indicator for the current workspace in the workspace list command output. \r\n\r\nThe asterisk (*) has been replaced with a greater-than symbol (>) to denote the active workspace and prevent unwanted behavior from wildcard symbol. \r\n\r\nThis change is reflected in both the workspace command tests and the workspace list command implementation.\r\n\r\n### Current\r\n\r\nWhen you call `workspace list` argument, you have an output with your current workspace selected with `*` symbol like this : \r\n```shell\r\n$ terraform workspace list\r\n  * default\r\n ```\r\n\r\nIf you want to iterate over the output you will have an issue because of the wildcard.\r\n```shell\r\n$ for i in $(terraform workspace list) ; do echo -ne \"$i\" ; done\r\ndefault main.tf api.tf modules README.md terraform.tfvars.example test.tfvars variables.tf default test\r\n```\r\n\r\nWe can see here that during the iteration, the for made a `echo *` because of the wildcard and print the files in the current directory.\n\n### Attempted Solutions\n\n### Bypassing\r\n\r\nTo make it work, we have to add a sed command to remove the character.\r\n```shell\r\nfor i in $(terraform workspace list | sed 's\/*\/\/g'); do echo \"$i\"; done\r\ndefault\r\n```\n\n### Proposal\n\n### Proposition\r\n\r\nReplace `*` by `>` (or whatever character that is not a wildcard) so we can simply do :\r\n```shell\r\nfor i in $(terraform workspace list) ; do echo -ne \"$i\" ; done\r\n>\r\ndefault\r\n```\r\n\r\n\r\nNow even if `>` char is present, we do not have unwanted behavior with wildcard substitution when piping.\r\n\r\nThat would be much more easier for shell programming (for eg) to parse output without bad surprise.\n\n### References\n\n_No response_","comments":["Thanks for your interest in this issue! This is just a reminder to please avoid \"+1\" comments, and to use the upvote mechanism (click or add the \ud83d\udc4d emoji to the original post) to indicate your support for this issue. This helps avoid notification spam for issues with high numbers of participants while enabling the maintainers to prioritize issues. Thanks again for the feedback!"],"labels":["enhancement","new"]},{"title":"no schema available for aws_config_resource.conf","body":"### Terraform Version\n\n```shell\nTerraform v1.7.4\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.39.1\n```\n\n\n### Terraform Configuration Files\n\n```resource \"aws_glue_catalog_database\" \"mixpanel_dvb\" {\r\n  name = \"mixpanel_dvb\"\r\n  location_uri = \"null\"\r\n  description = \"null\"\r\n}\r\n```\r\n\n\n### Debug Output\n\nError: no schema available for aws_config_resource.conf to validate for self-references; this is a bug in Terraform and should be reported\n\n### Expected Behavior\n\nError: no schema available for aws_config_resource.conf to validate for self-references; this is a bug in Terraform and should be reported\n\n### Actual Behavior\n\nError: no schema available for aws_config_resource.conf to validate for self-references; this is a bug in Terraform and should be reported\n\n### Steps to Reproduce\n\nterraform import aws_glue_catalog_database.mixpanal_dvb $account_id:$databasename\n\n### Additional Context\n\nterraform import aws_glue_catalog_database.mixpanal_dvb $account_id:$databasename\n\n### References\n\naws_glue_catalog_database.mixpanel_dvb: Importing from ID \"333*******:mixpanel_dvb\"...\r\naws_glue_catalog_database.mixpanel_dvb: Import prepared!\r\n  Prepared aws_glue_catalog_database for import\r\naws_glue_catalog_database.mixpanel_dvb: Refreshing state... [id=333******:mixpanel_dvb]\r\n\u2577\r\n\u2502 Error: no schema available for aws_config_resource.conf to validate for self-references; this is a bug in Terraform and should be reported","comments":["Hi @Bipin-dbdigital-in,\r\n\r\nThanks for filing the issue. The configuration shown appears to be incomplete, do you have a complete example we can reproduce the issue with? I'm also not sure what an `aws_config_resource` resource is, as that provider doesn't have any resources by that name. How did you end up with that resource type in your Terraform configuration?","i have one .tf file \r\n\"resource \"aws_guardduty_detector\" \"f6bb14175c2246c129c79c59\" {\r\n  detector_id = \"f6bb14175c2246c129c79c59a\"\r\n  enable = null\r\n  finding_publishing_frequency = \"null\"\r\n}\"\r\n\r\n\r\nand i want to import it via terraform\r\nso i used cmd \"terraform import aws_guardduty_detector.f6bb14175c2246c129c79c59  f6bb14175c2246c129c79c59\"\r\n\r\nso i got the below error\r\n\r\naws_guardduty_detector.f6bb14175c2246c129c79c59: Importing from ID \"f6bb14175c2246c129c79c59\"...\r\naws_guardduty_detector.f6bb14175c2246c129c79c59: Import prepared!\r\n  Prepared aws_guardduty_detector for import\r\naws_guardduty_detector.f6bb14175c2246c129c79c59: Refreshing state... [id=f6bb14175c2246c129c79c59]\r\n\u2577\r\n\u2502 Error: no schema available for aws_config_resource.conf to validate for self-references; this is a bug in Terraform and should be reported\r\n\u2502 \r\n","Thanks @Bipin-dbdigital-in. There must be some trace of the `aws_config_resource.conf` in your configuration or state. I don't know where it came from, because I don't think there was ever a resource called that, and I also can't replicate the exact error you are seeing with a nonexistent resource. Can you run a normal `terraform plan` and show the output? If you have somehow manually added this resource to your state, you will need to remove it in order to proceed."],"labels":["bug","waiting-response","new","waiting for reproduction"]},{"title":"GitHub module source truncates branch name after slash","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.4\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nmodule \"kubernetes\" {\r\n  source = \"github.com\/osinfra-io\/terraform-google-kubernetes-engine?ref=brettcurtis\/issue17\"\r\n\r\n  istio_gateway_ssl = var.istio_gateway_ssl\r\n  project_id        = module.project.project_id\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\n```none\r\nError: Failed to download module\r\n\u2502 \r\n\u2502   on main.tf line 96:\r\n\u2502   96: module \"kubernetes\" {\r\n\u2502 \r\n\u2502 Could not download module \"kubernetes\" (main.tf:96) source code from\r\n\u2502 \"git::https:\/\/github.com\/osinfra-io\/terraform-google-kubernetes-engine.git?ref=brettcurtis\":\r\n\u2502 error downloading\r\n\u2502 'https:\/\/github.com\/osinfra-io\/terraform-google-kubernetes-engine.git?ref=brettcurtis':\r\n\u2502 \/usr\/bin\/git exited with 1: error: pathspec 'brettcurtis' did not match any\r\n\u2502 file(s) known to git\r\n```\r\n\r\n### Expected Behavior\r\n\r\nI'd expect the code to initialize.\r\n\r\n### Actual Behavior\r\n\r\nIt seems to truncate the name of my branch following the slash.\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @brettcurtis,\r\n\r\nThanks for filing the issue. The branch specifier in this syntax is part of the URL query parameters, and the `\/` is one of the reserved characters within a URI query component. This means that it must appear encoded as `%2F` to be parsed correctly.\r\n\r\nThe following module config initializes successfully:\r\n\r\n```\r\nmodule \"kubernetes\" {\r\n  source = \"github.com\/osinfra-io\/terraform-google-kubernetes-engine?ref=brettcurtis%2Fissue17\"\r\n\r\n  istio_gateway_ssl = var.istio_gateway_ssl\r\n  project_id        = module.project.project_id\r\n}\r\n```\r\n","Ok thanks bud! I could have sworn this worked fine for me in the past so i figured it was a bug."],"labels":["enhancement","documentation"]},{"title":"Colorize `terraform fmt diff`","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.1.2\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v3.76.1\r\n+ provider registry.terraform.io\/hashicorp\/null v3.1.0\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.7.4. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI need to add color to the output of `terraform fmt -diff`\r\nAlthough the argument `-no-color` is shown in the `terraform fmt` help message, `terraform fmt -diff` does not colorize the diff output for me by default\r\nAs a workaround I pipe the `terraform fmt -diff` output to `colordiff` in order to colorize the diff output, but eventually I would like to run `terraform fmt -recursive -check -diff` as part of a github action which I don't have access to install colordiff at.\r\nAt first I thought that maybe my local env was at fault but running `terraform -check -diff` on an ubuntu github runner does not produce a colored diff as well. At the same time `terraform plan` produces a colored output both in my local env and in the github runner\r\n\r\n### Attempted Solutions\r\n\r\n`terraform fmt -diff | colordiff`\r\n\r\n### Proposal\r\n\r\nExtend `terraform fmt -diff` command to colorize the diff output\r\nI'm not sure how this is implemented in the source code but maybe\r\n- https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/backend\/remote\/colorize.go could be extended to support fmt diff ?\r\n- support `PAGER`\/`DIFF`\/`DIFFUTIL` env variable ?\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Cannot get s3:: modules with AWS SSO","body":"### Terraform Version\n\n```shell\nTerraform v1.7.4\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nmodule \"cluster\" {\r\n  source = \"s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz\"\r\n\r\n  ...\r\n}\r\n\r\nmodule \"workers\" {\r\n  source = \"s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz\"\r\n\r\n  ...\r\n}\r\n```\r\n\n\n### Debug Output\n\n```shell\r\n$> TF_LOG=debug tf get -update\r\n```\r\n\r\n```text\r\n2024-03-04T21:34:57.178-0700 [INFO]  Terraform version: 1.7.4\r\n2024-03-04T21:34:57.178-0700 [DEBUG] using github.com\/hashicorp\/go-tfe v1.41.0\r\n2024-03-04T21:34:57.178-0700 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2024-03-04T21:34:57.178-0700 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2024-03-04T21:34:57.178-0700 [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2024-03-04T21:34:57.178-0700 [INFO]  Go runtime version: go1.21.5\r\n2024-03-04T21:34:57.178-0700 [INFO]  CLI args: []string{\"\/User\/someone\/path\/.tfenv\/versions\/1.7.4\/terraform\", \"get\", \"-update\"}\r\n2024-03-04T21:34:57.178-0700 [DEBUG] Attempting to open CLI config file: \/User\/someone\/path\/.terraformrc\r\n2024-03-04T21:34:57.178-0700 [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2024-03-04T21:34:57.178-0700 [INFO]  Loading CLI configuration from \/User\/someone\/path\/.terraform.d\/credentials.tfrc.json\r\n2024-03-04T21:34:57.178-0700 [DEBUG] checking for credentials in \"\/User\/someone\/path\/.terraform.d\/plugins\"\r\n2024-03-04T21:34:57.178-0700 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-03-04T21:34:57.178-0700 [DEBUG] will search for provider plugins in \/User\/someone\/path\/.terraform.d\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/User\/someone\/path\/.local\/share\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/User\/someone\/path\/.local\/share\/flatpak\/exports\/share\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/var\/lib\/flatpak\/exports\/share\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [DEBUG] ignoring non-existing provider search directory \/var\/lib\/snapd\/desktop\/terraform\/plugins\r\n2024-03-04T21:34:57.179-0700 [INFO]  CLI command args: []string{\"get\", \"-update\"}\r\n2024-03-04T21:34:57.182-0700 [DEBUG] Module installer: begin cluster\r\nDownloading s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz for cluster...\r\n2024\/03\/04 21:34:57 ERROR: failed to create session with AWS_SDK_LOAD_CONFIG enabled. Use session.NewSession to handle errors occurring during session creation. Error: profile \"REDACTED\" is configured to use SSO but is missing required configuration: sso_region, sso_start_url\r\n2024-03-04T21:34:57.184-0700 [DEBUG] Module installer: begin workers\r\nDownloading s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz for workers...\r\n2024\/03\/04 21:34:57 ERROR: failed to create session with AWS_SDK_LOAD_CONFIG enabled. Use session.NewSession to handle errors occurring during session creation. Error: profile \"REDACTED\" is configured to use SSO but is missing required configuration: sso_region, sso_start_url\r\n\u2577\r\n\u2502 Error: Failed to download module\r\n\u2502\r\n\u2502   on main.tf line 5:\r\n\u2502    5: module \"cluster\" {\r\n\u2502\r\n\u2502 Could not download module \"cluster\" (main.tf:5) source code from \"s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz\": profile \"REDACTED\" is\r\n\u2502 configured to use SSO but is missing required configuration: sso_region, sso_start_url\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Failed to download module\r\n\u2502\r\n\u2502   on main.tf line 25:\r\n\u2502   25: module \"workers\" {\r\n\u2502\r\n\u2502 Could not download module \"workers\" (main.tf:25) source code from \"s3::https:\/\/s3.amazonaws.com\/redacted-s3-bucket-name\/some-module-name.tar.gz\": profile\r\n\u2502 \"REDACTED\" is configured to use SSO but is missing required configuration: sso_region, sso_start_url\r\n```\n\n### Expected Behavior\n\nShould support profiles configured with: https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/sso-configure-profile-token.html\n\n### Actual Behavior\n\nOnly supports profiles configured with: https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/sso-configure-profile-legacy.html\n\n### Steps to Reproduce\n\n1. Put a module in s3\r\n2. Try to download it when using an AWS IAM Identity Center profile configured with refreshable tokens\r\n\n\n### Additional Context\n\nI believe the issue is similar to https:\/\/github.com\/hashicorp\/terraform\/issues\/32465\n\n### References\n\n_No response_","comments":["As I believe this functionality relies on go-getter, it is likely related to:\r\nhttps:\/\/github.com\/hashicorp\/go-getter\/issues\/323","> As I believe this functionality relies on go-getter, it is likely related to: [hashicorp\/go-getter#323](https:\/\/github.com\/hashicorp\/go-getter\/issues\/323)\r\n\r\nAnything I can help with to get this updated @crw?","@endzyme Unfortunately not. `go-getter` is essentially frozen except in exceptional circumstances. There are a number of open AWS issues (https:\/\/github.com\/hashicorp\/go-getter\/pull\/467, https:\/\/github.com\/hashicorp\/go-getter\/pull\/457). I am linking these via this comment on the off chance they can be resolved together, if the SDKv2 issue gets resolved. Thanks!"],"labels":["bug","upstream","new"]},{"title":"Add ability to disable validation step for s3 backend `kms_key_id`","body":"### Terraform Version\n\n```shell\n1.7.3\n```\n\n\n### Use Cases\n\nHi, we have our own s3 like storage to save terraform state.\r\n\r\n```\r\n  backend \"s3\" {\r\n    endpoints = {\r\n      s3 = \"https:\/\/private.storage.net\"\r\n    }\r\n    bucket   = \"<BUCKET>\"\r\n    region   = \"ua-central1\"\r\n    key      = \"terraform.tfstate\"\r\n\r\n    skip_s3_checksum            = true\r\n    skip_region_validation      = true\r\n    skip_requesting_account_id  = true\r\n    skip_credentials_validation = true\r\n\r\n    encrypt = true\r\n    access_key = \"<KEY>\"\r\n    secret_key = \"<SECRET>\"\r\n    kms_key_id = \"<KEY_ID>\"\r\n  }\r\n```\r\nStarting from `terraform version 1.6.0` there is a validation step for `kms_key_id` field, [here](https:\/\/github.com\/hashicorp\/terraform\/commit\/454eed63e7791aeee929b82908fc5b5b0dcc77e6#diff-c6f102657948029362a41861b9418f65f3830155d90e53b4a5141ddb2d60c2b4).\r\n\r\nAnd so, we have error\r\n\r\n```\r\n2024-02-19T18:36:52.200+0300 [INFO]  Terraform version: 1.7.3\r\n\r\n| Error: Invalid KMS Key ID\r\n| \r\n|   on provider.tf line 29, in terraform:\r\n|   29:     kms_key_id = \"****\"\r\n| \r\n| Value must be a valid KMS Key ID, got \"****\"\r\n```\r\n\n\n### Attempted Solutions\n\nThere is no existing solution.\n\n### Proposal\n\nCan we add new flag\/parameter to s3 backend configuration to disable `kms_key_id` validation?\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nThis issue would be handled by the AWS Provider team at HashiCorp, so the request has been put into their backlog. \r\n\r\nThanks again!"],"labels":["enhancement","backend\/s3","new"]},{"title":"terraform fmt: align equals sign even when the expression is split over multiple lines","body":"### Terraform Version\n\n```shell\nTerraform v1.7.4\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nresource \"azurerm_key_vault_secret\" \"sa_connectionstring\" {\r\n  name         = \"storage-account-connectionstring\"\r\n  value        = azurerm_storage_account.main.primary_connection_string\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  depends_on        = [\r\n    azurerm_key_vault_access_policy.current\r\n  ]\r\n}\r\n```\r\n\n\n### Debug Output\n\njorenthijs@macbook Main % export TF_LOG=trace\r\njorenthijs@macbook Main % terraform fmt .\/storage.tf \r\n2024-03-04T12:34:26.694+0100 [INFO]  Terraform version: 1.7.4\r\n2024-03-04T12:34:26.694+0100 [DEBUG] using github.com\/hashicorp\/go-tfe v1.41.0\r\n2024-03-04T12:34:26.694+0100 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2024-03-04T12:34:26.695+0100 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2024-03-04T12:34:26.695+0100 [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2024-03-04T12:34:26.695+0100 [INFO]  Go runtime version: go1.21.5\r\n2024-03-04T12:34:26.695+0100 [INFO]  CLI args: []string{\"terraform\", \"fmt\", \".\/storage.tf\"}\r\n2024-03-04T12:34:26.695+0100 [TRACE] Stdout is a terminal of width 292\r\n2024-03-04T12:34:26.695+0100 [TRACE] Stderr is a terminal of width 292\r\n2024-03-04T12:34:26.695+0100 [TRACE] Stdin is a terminal\r\n2024-03-04T12:34:26.695+0100 [DEBUG] Attempting to open CLI config file: \/Users\/jorenthijs\/.terraformrc\r\n2024-03-04T12:34:26.695+0100 [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2024-03-04T12:34:26.695+0100 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-03-04T12:34:26.695+0100 [DEBUG] ignoring non-existing provider search directory \/Users\/jorenthijs\/.terraform.d\/plugins\r\n2024-03-04T12:34:26.695+0100 [DEBUG] ignoring non-existing provider search directory \/Users\/jorenthijs\/Library\/Application Support\/io.terraform\/plugins\r\n2024-03-04T12:34:26.695+0100 [DEBUG] ignoring non-existing provider search directory \/Library\/Application Support\/io.terraform\/plugins\r\n2024-03-04T12:34:26.695+0100 [INFO]  CLI command args: []string{\"fmt\", \".\/storage.tf\"}\r\n2024-03-04T12:34:26.696+0100 [TRACE] terraform fmt: Formatting storage.tf\r\nstorage.tf\n\n### Expected Behavior\n\nWhat i expect and want is for depends_on's '=' sign to be vertically aligned with the others\r\n```tf\r\nresource \"azurerm_key_vault_secret\" \"sa_connectionstring\" {\r\n  name         = \"storage-account-connectionstring\"\r\n  value        = azurerm_storage_account.main.primary_connection_string\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  depends_on   = [\r\n    azurerm_key_vault_access_policy.current\r\n  ]\r\n}\r\n```\n\n### Actual Behavior\n\nwhen i format this document (format on save is turned on, but manual formatting gives same result). Terraform collapses the whitespace but does not vertically align the = sign of the depends_on parameter\r\n```tf\r\nresource \"azurerm_key_vault_secret\" \"sa_connectionstring\" {\r\n  name         = \"storage-account-connectionstring\"\r\n  value        = azurerm_storage_account.main.primary_connection_string\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  depends_on = [\r\n    azurerm_key_vault_access_policy.current\r\n  ]\r\n}\r\n```\r\n\r\nIf i write the array differently to make it single line. the issue does not reproduce. However this is not a solution because currently the extension is causing whitespace changes for my coworker who uses windows and doesn't have this problem.\r\n```tf\r\nresource \"azurerm_key_vault_secret\" \"sa_connectionstring\" {\r\n  name         = \"storage-account-connectionstring\"\r\n  value        = azurerm_storage_account.main.primary_connection_string\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  depends_on   = [azurerm_key_vault_access_policy.current]\r\n}\r\n```\n\n### Steps to Reproduce\n\n`terraform fmt` or `terraform fmt specific-file.ts`\n\n### Additional Context\n\nI first thought it was an issue with the VS-code extension so I filed a [bug](https:\/\/github.com\/hashicorp\/vscode-terraform\/issues\/1700) there. They could reproduce it but redirected me here since they also use `terraform fmt`.\n\n### References\n\n- https:\/\/github.com\/hashicorp\/vscode-terraform\/issues\/1700","comments":["Hi @Joren-Thijs,\r\n\r\nWhat you've encountered here is the auto-formatter behaving as designed: a multi-line expression isn't considered to be grouped in with its neighbors for alignment purposes because it represents a break in the sequence of single-line assignments that potentially introduces new nested groups that would get aligned separately.\r\n\r\nI'm going to relabel this as an enhancement request to represent that you are asking for a change to the intended design which therefore requires new technical design work, but I also want to be explicit that you've described a subjective preference that is not inherently better or worse than what Terraform currently does, and _any_ systematic design rules are inevitably going to be considered differently by different people, so we tend to avoid making essentially-arbitrary changes to the established rules to avoid creating churn for people who already have their configurations formatted under the current set of rules. Therefore I'm not optimistic that this request will lead to `terraform fmt` being changed to behave as you described.\r\n\r\nWe also explicitly do not offer any customization options for `terraform fmt` because its purpose is to be a single fixed set of rules that represent idiomatic Terraform style.\r\n\r\n---\r\n\r\nFWIW I'd also note that [the documented style conventions](https:\/\/developer.hashicorp.com\/terraform\/language\/syntax\/style) call for meta-arguments like `depends_on` to be visually separated from other arguments by one blank line and placed first anyway. `terraform fmt` does not currently enforce ordering and blank line rules to avoid significant churn to existing configurations, but if your example were following those conventions it would be written this way and so the `depends_on` would not be included in the same alignment group as the other arguments regardless of whether it were single-line or not:\r\n\r\n```hcl\r\nresource \"azurerm_key_vault_secret\" \"sa_connectionstring\" {\r\n  depends_on = [\r\n    azurerm_key_vault_access_policy.current\r\n  ]\r\n\r\n  name         = \"storage-account-connectionstring\"\r\n  value        = azurerm_storage_account.main.primary_connection_string\r\n  key_vault_id = azurerm_key_vault.main.id\r\n}\r\n```\r\n\r\nOf course there's no requirement that you follow those style rules, just as there's no requirement that you use `terraform fmt`. These style conventions and the tool that automates some of them are offered in the hope that they are useful for achieving consistency and familiarity for folks who have used Terraform in other organizations before, but some organizations prefer to decide and document their own local design rules that differ from the official ones.\r\n"],"labels":["enhancement","new"]},{"title":"terraform test : support for a mechanism for test suites","body":"### Terraform Version\r\n\r\n```shell\r\nVersions above 1.7.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nIt would be great to have a mechanism to allow a suite of tests which execute multiple run blocks across a plan\/apply action. Yes, this is available with multiple assert blocks currently under a single run block. But the resulting output of a `terraform test` run is usually in the form of a single run than showing how many assertions passed.  \r\n\r\nThe single run block currently does help in avoiding the redundant lines of `command=plan\/apply` input.\r\n\r\n**Test inputs:**\r\n\r\n```\r\nrun \"positive\"{\r\n\r\ncommand=plan\r\n    assert{...}\r\n    assert{...}\r\n    assert{...}\r\n    assert{...}\r\n    assert{...}\r\n}\r\n\r\nrun \"failures\"{\r\n\r\ncommand=plan\r\n    assert{...}\r\n    assert{...}\r\n    assert{...}\r\n}\r\n```\r\n\r\n**Output of test runs**\r\n\r\n```\r\n\r\ntests\/main.tftest.hcl.. in progress\r\n   run \"positive\" ... pass\r\n   run \"failures\" ... pass\r\n\r\nSuccess! 2 passed, 0 failed\r\n```\r\n \r\n\r\n### Proposal\r\n\r\nFew ways of representing this could be :\r\n\r\n1.  `run_suite` with multiple `run` blocks\r\n```\r\nrun_suite \"suite_1\"{\r\n    command = plan\r\n    run {....}\r\n    run {....}\r\n    run {....}\r\n}\r\n\r\n```\r\n\r\n2. Continue with a single run block and multiple assert blocks, but have a mechanism to provide a name or identifier for the assertion .\r\n\r\n```\r\nrun \"suite_1\" {\r\n  assert {\r\n    name = \"test1\"\r\n    .....\r\n  }\r\n  assert {\r\n    name = \"test2\"\r\n    .....\r\n  }\r\n\r\n}\r\n\r\n\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["I think that resolving this issue one way or another will be a prerequisite for stabilizing a solution to https:\/\/github.com\/hashicorp\/terraform\/issues\/34264 , since adding a new level of containment into the test model is likely to require a different mapping to the JUnit XML format.\r\n"],"labels":["enhancement","new","terraform test"]},{"title":"Terraform Cloud workspace locks report the calling user, not the user holding the lock","body":"### Terraform Version\n\n```shell\nTerraform v1.7.4\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/archive v2.4.2\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.36.0\r\n+ provider registry.terraform.io\/hashicorp\/dns v3.4.0\r\n+ provider registry.terraform.io\/hashicorp\/external v2.3.2\r\n+ provider registry.terraform.io\/hashicorp\/http v3.4.1\r\n+ provider registry.terraform.io\/hashicorp\/local v2.4.1\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.2\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\n```\n\n\n### Terraform Configuration Files\n\nn\/a\n\n### Debug Output\n\nn\/a\n\n### Expected Behavior\n\nTerraform should report the user _holding_ the lock\n\n### Actual Behavior\n\nTerraform reports the information about the calling user\r\n\n\n### Steps to Reproduce\n\n1. Obtain a Terraform cloud lock in session A\r\n2. In session B, run something like `terraform plan` or `apply`\r\n\r\nThe output will look like this with the \u201cWho\u201d, \u201cVersion\u201d, and \u201cCreated\u201d fields showing information about the job which failed to get the lock rather than the earlier job which actually has it:\r\n\r\n```\r\nError: Error acquiring the state lock\r\n\u2502 \r\n\u2502 Error message: workspace already locked (lock ID: \"example_org\/example_workspace\")\r\n\u2502 Lock Info:\r\n\u2502   ID:        ae0b51a1-5a4a-4d64-8595-177742dc0e86\r\n\u2502   Path:      \r\n\u2502   Operation: OperationTypeApply\r\n\u2502   Who:       sessionB@sessionB\r\n\u2502   Version:   1.7.4\r\n\u2502   Created:   2024-02-26 21:28:15.211985 +0000 UTC\r\n\u2502   Info:      \r\n```\n\n### Additional Context\n\nThis worked normally for years but seems to have regressed at some point in the 1.7.x era. I don't have precise timing because I only notice in sporadic cases where I'm troubleshooting something and racing our CI process.\n\n### References\n\n_No response_","comments":["Hi @acdha! Thanks for sharing this.\r\n\r\nYou're right that this message describes the lock that the system was trying to acquire, rather than the lock that it conflicted with. I believe that what's going on is this error message is just echoing back a string representation of the lock info object constructed here:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/1459825e53565529274a6a31acd7f9b215bcda7b\/internal\/command\/clistate\/state.go#L120-L135\r\n\r\nThis is how the system was designed to behave and so isn't a bug, but it does seem like a reasonable request to ask for the system to retrieve information about the lock that's currently held and describe it. I'm going to relabel this as an enhancement to represent that solving it would require new design work, since right now the backend API doesn't include any means for the backend to describe the lock it's conflicting with, and in particular not all backends are even able to retain the lock details in sufficient detail to reproduce the lock information object in this case, so we'll need to figure out what's possible here and how best to achieve it.\r\n\r\nSince this error object is -- in this particular case -- being constructed by the cloud integration's own state manager, it might be possible to choose a compromise where we solve this only for the cloud integration, where HashiCorp is empowered to change the remote system to return additional information, even if we can't solve it generally for all backends. Relevant code is here, where the error object rendered this issue is being generated:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/1459825e53565529274a6a31acd7f9b215bcda7b\/internal\/cloud\/state.go#L344-L357\r\n\r\nI notice that the API response is actually currently being ignored entirely. I don't know if there's already some information in that API response that could be used to populate a new `LockInfo` object describing the existing lock, but even if not it seems plausible to change the Terraform Cloud API to return suitable additional information.\r\n","I think it would be useful to indicate the current holder of the lock since I had a coworker on a different project hit by that this morning. Maybe that says it should be two requests \u2013 one simply to display the current lock holder when returned by the API, and the other to augment that API to include other details such as the timestamp or client ID they used when locking it?"],"labels":["enhancement","new","cloud"]},{"title":"Don't silently convert object types to map(string) just because their attributes all have primitive types","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform 1.7.4\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n\r\n\/\/ root module \r\n\r\nresource \"random_string\" \"rando\" {\r\n  length  = 6\r\n  special = false\r\n  numeric = false\r\n  upper   = false\r\n}\r\n\r\nmodule \"kms\" {\r\n  source                  = \"git::https:\/\/git.labcorp.com\/scm\/TFCOM\/kms.git?ref=release\/130.2.0\"\r\n  labels                  = \"${var.labels}-${random_string.rando.result}\"\r\n  deletion_window_in_days = 7\r\n  tags                    = local.tags\r\n}\r\n\r\noutput \"security_policy_rules\" {  value = module.vpc-endpoint.security_policy_rules}\r\noutput \"kms_key_config\"        {  value = module.vpc-endpoint.kms_key_config}\r\noutput \"key_config\"           { value = module.vpc-endpoint.key_config }\r\n\r\noutput \"aws_policy\"      { value = module.vpc-endpoint.aws_policy }\r\noutput \"aws_json\"        { value = module.vpc-endpoint.aws_json }\r\noutput \"customer_policy\" { value = module.vpc-endpoint.customer_policy }\r\noutput \"customer_json\"   { value = module.vpc-endpoint.customer_json }\r\noutput \"security_policy\" { value = module.vpc-endpoint.security_policy }\r\noutput \"security_json\"   { value = module.vpc-endpoint.security_json }\r\n\r\nmodule \"vpc-endpoint\" {\r\n  source              = \"..\/\"\r\n  create_vpc_endpoint = true\r\n  labels              = var.labels\r\n  tags                = local.tags\r\n  collection_name     = \"${var.labels}-vpce\"\r\n  vpc_endpoint = {\r\n    name       = \"value\"\r\n    subnet_ids = data.aws_subnets.main.ids\r\n    vpc_id     = data.aws_vpc.main.id\r\n  }\r\n}\r\n\r\noutput \"with-aws_policy\"      { value = module.vpc-endpoint-with-kms.aws_policy }\r\noutput \"with-aws_json\"        { value = module.vpc-endpoint-with-kms.aws_json }\r\noutput \"with-customer_policy\" { value = module.vpc-endpoint-with-kms.customer_policy }\r\noutput \"with-customer_json\"   { value = module.vpc-endpoint-with-kms.customer_json }\r\noutput \"with-security_policy\" { value = module.vpc-endpoint-with-kms.security_policy }\r\noutput \"with-security_json\"   { value = module.vpc-endpoint-with-kms.security_json }\r\n\r\nmodule \"vpc-endpoint-with-kms\" {\r\n  source              = \"..\/\"\r\n  create_vpc_endpoint = true\r\n  labels              = var.labels\r\n  tags                = local.tags\r\n  collection_name     = \"${var.labels}-vpce\"\r\n  use_customer_kms_key = true\r\n  kms_key             = module.kms.key_arn\r\n  vpc_endpoint = {\r\n    name       = \"value\"\r\n    subnet_ids = data.aws_subnets.main.ids\r\n    vpc_id     = data.aws_vpc.main.id\r\n  }\r\n}\r\n\r\n\/\/ module code\r\n\r\nlocals {\r\n  security_policy_rules = {\r\n    Rules = [\r\n      {\r\n        Resource = [\r\n          \"collection\/${local.collection_name}\"\r\n        ],\r\n        ResourceType = \"collection\"\r\n      }\r\n    ]\r\n  }\r\n  kms_key_config = {\r\n    AWSOwnedKey = {\r\n      AWSOwnedKey = true\r\n    }\r\n    CustomerKey = {\r\n      AWSOwnedKey = false\r\n      KmsARN      = var.kms_key\r\n    }\r\n  }\r\n\r\n  \/\/ this converts the Boolean true to String \"true\" \r\n  key_config = var.kms_key == \"AWSOwnedKey\" ? local.kms_key_config[\"AWSOwnedKey\"] : local.kms_key_config[\"CustomerKey\"]\r\n  security_policy = merge(local.security_policy_rules, local.key_config)\r\n  security_json   = jsonencode(local.security_policy)\r\n  \r\n\r\n  \/\/ this does not convert the Boolean true to String \"true\" \r\n  customer_policy = merge(local.security_policy_rules,local.kms_key_config[\"CustomerKey\"])\r\n  customer_json   = jsonencode(local.customer_policy)\r\n  \r\n  \/\/ this does not convert the Boolean false to String \"false\" \r\n  aws_policy = merge(local.security_policy_rules,local.kms_key_config[\"AWSOwnedKey\"])\r\n  aws_json        = jsonencode(local.aws_policy)\r\n\r\n}\r\n\r\n\r\noutput \"security_policy_rules\" {  value = local.security_policy_rules}\r\noutput \"kms_key_config\"        {  value = local.kms_key_config}\r\noutput \"aws_policy\"      { value = local.aws_policy }\r\noutput \"aws_json\"        { value = local.aws_json }\r\noutput \"customer_policy\" { value = local.customer_policy }\r\noutput \"customer_json\"   { value = local.customer_json }\r\n\r\noutput \"key_config\"      { value = local.key_config }\r\noutput \"security_policy\" { value = local.security_policy }\r\noutput \"security_json\"   { value = local.security_json }\r\n\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/jaffel-lc\/4bb463fa4086c11efd7e283a2b05f366\r\n\r\n### Expected Behavior\r\n\r\noutput value with-security_policy,AWSOwnedKey == false\r\noutput value security_policy,AWSOwnedKey == true\r\n \r\n\r\n### Actual Behavior\r\n\r\noutput value security_policy,AWSOwnedKey == \"false\"\r\noutput value with-security_policy,AWSOwnedKey == \"true\"\r\n\r\nThese outputs are based on local variables in the module, and when I try to use those locals in an aws_opensearchserverless_security_policy as the value of the policy parameter I receive this error: \r\n\r\noperation error OpenSearchServerless: CreateSecurityPolicy, https response error StatusCode: 400, RequestID: 086b1f49-5575-47ed-b5db-5c61546e240f, ValidationException: Policy json is invalid, error: [$.AWSOwnedKey: string found, boolean expected,           \u2502 $.AWSOwnedKey: must be a constant value true, $.KmsARN: is missing but it is required, $.AWSOwnedKey: must be a constant value false]   \r\n\r\nThe difference between security_policy and aws_policy is that security_policy uses an additional kms_key_config map lookup into another local variable before the merge(), while aws_policy merges directly from  kms_key_config\r\nIn that extra lookup, the boolean value is being converted to string.\r\n\r\nHere a gist with the outputs: https:\/\/gist.github.com\/jaffel-lc\/909a106c1c299e5a9f6e0be6c7d82b72\r\n\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n2. `terraform apply`\r\n3. \r\n\r\n### Additional Context\r\n\r\nn\/a\r\n\r\n### References\r\n\r\nn\/a","comments":["Hi @jaffel-lc,\r\n\r\nI think in this case the problem is that Terraform is trying its best to find some way to make your conditional expression valid through automatic type conversions:\r\n\r\n```hcl\r\n  key_config = var.kms_key == \"AWSOwnedKey\" ? local.kms_key_config[\"AWSOwnedKey\"] : local.kms_key_config[\"CustomerKey\"]\r\n```\r\n\r\nTerraform needs to decide what type `local.key_config` should have based on the conditional expression results. However, your two result arms have different types:\r\n\r\n* `AWSOwnedKey` is `object({ AWSOwnedKey = bool })`\r\n* `CustomerKey` is `object({ AWSOwnedKey = bool, KMSArn = string })`\r\n\r\nWithout any type conversions this conditional expression would be invalid because the two results are of different types. Terraform is noticing that it's possible to convert a boolean value to a string and therefore both of these can potentially convert to `map(string)`, and is therefore guessing that you probably meant that data type. In this case, that is an incorrect guess.\r\n\r\nI assume from your description that what you really wanted is for this local value to have the type `object({ AWSOwnedKey = bool, KMSArn = string })`. If so then you'll need to add some more information so that Terraform can better understand your intent:\r\n\r\n```hcl\r\n  kms_key_config = tomap({\r\n    AWSOwnedKey = {\r\n      AWSOwnedKey = true\r\n      KmsARN      = null\r\n    }\r\n    CustomerKey = {\r\n      AWSOwnedKey = false\r\n      KmsARN      = var.kms_key\r\n    }\r\n  })\r\n```\r\n\r\nThis will cause `local.kms_key_config` to be of type `map(object({ AWSOwnedKey = bool, KMSArn = string })`, and therefore both of the elements will be of type `object({ AWSOwnedKey = bool, KMSArn = string }`, and thus `local.key_config` will also have that type. Your `merge` call should then introduce the extra attribute from `local.security_policy_rules`, causing the output value types you wanted.\r\n\r\n---\r\n\r\nThis is, then, Terraform behaving as designed, rather than a bug.\r\n\r\nThis is an unfortunate situation where Terraform would ideally have returned a clear error about the type mismatch, but cannot do so because of backward compatibility with Terraform v0.11 and earlier, which didn't yet have either the `number` or `bool` types and so booleans were _always_ represented as strings.\r\n\r\nTo avoid breaking existing modules that were relying on the ability to construct maps in this way, modern Terraform unfortunately tends to guess that authors intended to construct a `map(string)` whenever that's possible. That behavior is protected by the Terraform v1.x compatibility promises and so cannot change at least until a future [edition](https:\/\/log.martinatkins.me\/2021\/09\/25\/future-of-the-terraform-language\/) of the Terraform language.\r\n\r\nNo new editions are planned at this time, but I'm going to relabel this as an enhancement and add it to our list of candidates for potential changes to make in a future language edition. (A new language edition is a relatively expensive proposition, so we're intending to wait until there are enough candidates to justify the additional complexity that would imply.)\r\n","Your reply make a lot of sense. Thank you.\r\n\r\nHowever, I had already found that when AWSOwnedKey is false, passing the KmsARN key - set to anything - caused a key error on KmsARN when amazon tries to verify the resulting policy JSON.  AWS doesn't want a KmsARN key with any value - null or not - when AWSOwnedKey == true \r\n\r\nSo what I wanted was exactly the assignment I wrote,  so that my code would contain the three lines starting with the key_config assignment, instead of the four lines that follow it.\r\n\r\nI guess, that I would call it a misfeature, if not a bug, that terraform doesn't create the resulting variables type verbatim form the right side of the assignment; or at least track that the value was bool, and not string - maintaining the original bool type [with an indicator bit in your symbol table], and not converting it to string. \r\n\r\nBecause just prior to opening this issue, I opened [this issue](https:\/\/github.com\/hashicorp\/terraform\/issues\/34726) about TF not being able to use string comparisons on variables in count operations. So  I would expect \r\n```\r\ncount = var.somevar == \"true\"  ? 1 : 0\r\n```\r\nto generate a plan-time error, rendering string-encoded boolean values useless.\r\n\r\n","Indeed, when you are constructing something that's going to be sent essentially verbatim as JSON (or similar) then you may need to postprocess it to transform it from the conventions of Terraform's type system to the conventions of whatever language you are targeting. That's a common problem when using types from one language to generate data for another language.\r\n\r\nThis is often managed in Terraform by providers offering functionality to automatically transform a Terraform-friendly data structure into an equivalent that's friendly to a particular remote system. However, that's been tough to design for a number of features due to the functionality typically needing to get embedded inside the implementation of a particular resource type.\r\n\r\nTerraform v1.8 is going to introduce the possibility of providers to contribute their own functions (in the same sense that `jsonencode` is a function) and a big reason for that is to make it more convenient for a provider to offer a specialized encoding function like \"encode an AWS IAM policy\" or \"encode a Kubernetes manifest\" which can encapsulate this sort of translation logic so that you don't need to implement it yourself as part of a Terraform module.\r\n\r\nOf course it will take some time for the provider ecosystem to make full use of this new capability, but what you've described here seems like a good use-case for a function in the AWS provider which knows the JSON schema that AWS is expecting in this context and produces it automatically.\r\n","```\r\nIndeed, when you are constructing something that's going to be sent essentially verbatim as JSON (or similar) then you may need to postprocess it to transform it from the conventions of Terraform's type system to the conventions of whatever language you are targeting. That's a common problem when using types from one language to generate data for another language.\r\n```\r\n\r\nIndeed, I consider the map manipulation under debate here to be _pre_-processing. What is at issue is TF performing _implicit_ type conversion of a bool literal to a string literal, and that terraform should be smart enough to tell the difference between \"true\" and true, especially when \"true\" appears nowhere in my code at all.  If I'd mixed them freely, I could maybe understand it in light of the promise you mentioned above.\r\n\r\nSince HCL is dynamically-typed language , instead of getting confused about the what type to use on the left side of the assignment before evaluating the left side of the ternary operator, clone the result type from the results _after_ the right side of the assignment is fully evaluated.\r\n\r\n","The kind of pre-processing I meant was something that should happen _after_ you've built a data structure in Terraform's type system.\r\n\r\nBy analogy to Go: it's typical to first build a data structure made mostly of struct types and then map that to correctly-shaped JSON as a separate step. Terraform object types are analogous to struct types, and so essentially the same principle applies here: you'd start by building a data structure made of Terraform object types (which come with the assumption that all attributes are always present but sometimes they are `null`) and then _as a separate step_ transform that data structure into the JSON format that the underlying system requires, which in this case seems to mean ignoring any attribute whose value is `null`.\r\n\r\nTrying to do both the building of the data structure _and_ the adaptation for another system's encoding format at the same time makes things considerably more difficult, because you end up needing to work around Terraform's expression operators, rather than working _with_ them.\r\n\r\nI'm not sure exactly _which_ AWS-specific format you are trying to generate here -- I assume it's not just an IAM policy document -- but the existing `aws_iam_policy_document` data source is an early example of what I meant in the above, albeit implemented as a data source rather than as a function because Terraform didn't support provider-contributed functions yet. But a future version of the AWS provider could offer a more convenient function that takes a value of a type that captures the same _meaning_ as an IAM policy document but described using Terraform's type system, and then use that to produce a JSON encoding of that policy that conforms to the AWS expectations for that document format. It would presumably be defined to take object types with optional attributes (that therefore get set to `null` when not explicitly defined) and would encapsulate the IAM-policy-specific rendering decision to omit those null attributes when producing its result. The same could be done for the format you are building, assuming it's something different to an IAM policy.\r\n\r\n","These are aws_opensearchserverless_* policies.  I have to write my own opensearchserverless module, because I haven't been able to find one by Hashicorp in the module registry.\r\n\r\nI am not asking terraform to be aware that I wanted to convert to JSON after my lookup, or to more generally do _\"the adaptation for another system's encoding format\"_; I just wanted the bool literal in the original map to survive the lookup and assignment as a bool literal, so I could create the other system's encoding format.","Thanks for that extra context!\r\n\r\nTo be clear, I'm just trying to give you context about why Terraform behaves the way it does and why I perceive it as an example of translating between type systems rather than as a bug, since your earlier feedback seemed to be implicitly asking about that. Terraform perhaps obscures this problem more than some other languages due to it having a structural type system that is built around \"JSON-like\" concepts, but it's important to remember that Terraform's type system and JSON's type system (such that it is) are not exactly equivalent, and so there will be some cases where you can't \"just\" write exactly what you would write in JSON.\r\n\r\nMy understanding of the state of this issue is that it represents the fact that Terraform gave you poor feedback about what was going on here, despite it being intended behavior: it guessed incorrectly about what you wanted your expression to mean, rather than returning a type mismatch error as I think it ideally should have. This issue is therefore representing hopefully finding some way to improve that, which might have to wait until a future edition of the Terraform language.\r\n\r\nMy other remarks about provider-contributed functions are intended as separate ideas for how Terraform could help you solve your problem _despite_ the Terraform type system and the AWS policy document format not being exactly aligned. Specifically (now that I know which resource types you're talking about) I imagine the `hashicorp\/aws` provider offering a new function which is similar to `jsonencode` but that has a specific object type constraint for its argument -- so Terraform Core has more information about what data types are expected here  -- and is aware of the rules the remote API enforces about unset attributes being represented as absent JSON properties, rather than as properties set to JSON `null`. You would then assign the result of that function to the `policy` argument of `aws_opensearchserverless_security_policy`, instead of using the generic `jsonencode`.\r\n\r\nThat would ultimately be a change in the AWS provider rather than in Terraform Core though, so would not be directly in scope for this issue.\r\n","Thank you very much  Martin.\r\nI appreciate your time and effort.","Hi @jaffel-lc \r\n\r\nI have crossed this issue because the same use case (Create module for Opensearch serverless).\r\nI have been able to find a workaround.\r\nYou can check my workaround here (https:\/\/registry.terraform.io\/modules\/fdmsantos\/opensearch-serverless\/aws\/latest)\r\n\r\nAlso you are welcome to use the module and help to improve it"],"labels":["enhancement","config"]},{"title":"Documented TF_IGNORE environment variable appears to no longer exist","body":"### Terraform Version\n\n```shell\nTerraform v1.7.3\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.36.0\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/cli\/config\/environment-variables#tf_ignore\n\n### What is the docs issue?\n\nThe page documents a `TF_IGNORE` environment variable. However, from what I can tell, support for this environment variable was removed in https:\/\/github.com\/hashicorp\/go-slug\/pull\/42.\n\n### Proposal\n\nRemove documentation of something that doesn't exist. However, this was genuinely a useful feature, and was a reasonable bandaid over the lack of the feature described in https:\/\/github.com\/hashicorp\/terraform\/issues\/27654. If there's a recommended replacement for `TF_IGNORE`, I'd love for that to be documented.\n\n### References\n\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/27043\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/27654","comments":["Thanks for this! We looked at the original issue in triage and think it may have something to do with Cloud (versus Core Terraform), but this is correct that within Core, there env var functionally does not exist. "],"labels":["documentation","new"]},{"title":"terraform_data ignoring local-exec destroy provisioner","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.3\r\non windows_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nresource \"terraform_data\" \"destroy_action\" {\r\n  provisioner \"local-exec\" {\r\n    when    = destroy\r\n    command = \"echo 'Destroy-time provisioner'\"\r\n  }\r\n}\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/leuthelt\/aa9760f8ec5d4279261aa0e9857be418\r\n\r\n### Expected Behavior\r\n\r\nTerraform should run the defined local-exec command during destroy.\r\n\r\n### Actual Behavior\r\n\r\nTerraform is removing terraform_data resource from state without executing local-exec command.\r\n\r\n### Steps to Reproduce\r\n\r\n1. terraform init\r\n2. terraform apply\r\n3. comment out whole resource \"terraform_data\"\r\n4. terraform apply\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Current workaround is to use count with terraform_data like it's documented in https:\/\/developer.hashicorp.com\/terraform\/language\/resources\/provisioners\/syntax#destroy-time-provisioners, but that isn't my understanding of a destroy trigger.","Hi @leuthelt,\r\n\r\nIn your reproduction steps you mentioned commenting out the entire resource block. That means that you removed the provisioner from the configuration, and so Terraform didn't execute it.\r\n\r\nTherefore Terraform seems to have worked as designed here. If you want the provisioner to run then you must keep it in the configuration.\r\n\r\nI think therefore that this issue is a feature request for a way to keep the destroy provisioner in the configuration even though the containing resource block has been removed, so that the instructions about what to do when destroying the object can outlive the declaration of the object. Would you agree?\r\n\r\n","Hi @apparentlymart, I would agree to your comment. ","Thank you for confirming.\r\n\r\nThis situation is one of the future improvements we considered when recently introducing the `removed` block type as a way to give Terraform hints and instructions for how to destroy something that isn't in the configuration anymore.\r\n\r\nIf we did that then instead of removing the block entirely you would replace it with a `removed` block like this:\r\n\r\n```hcl\r\n# NOTE: not a working example yet;\r\n# hypothetical future language feature\r\nremoved {\r\n  from = terraform_data.destroy_action\r\n\r\n  provisioner \"local-exec\" {\r\n    when    = destroy\r\n    command = \"echo 'Destroy-time provisioner'\"\r\n  }\r\n}\r\n```\r\n\r\nThe idea here then is that `removed` blocks would support any number of `provisioner` blocks as long as they all specify `when = destroy`, and then Terraform would use them during the apply phase when destroying this object.\r\n\r\nWe have also considered a helper command like `terraform rm terraform_data.destroy_action` that would automatically replace the `resource` block with a `removed` block and copy over just the parts of the resource configuration that are relevant when destroying, to minimize the chance of human error when making changes like this.\r\n\r\n"],"labels":["enhancement","removed"]},{"title":"[terraform test] Support expect_failures of resouces inside local module in run blocks","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.3\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nHello,\r\n\r\nWe would like to `expect_failure` from a local module call done in a `run` block with `terraform test`. You can find some context about our issue down below.\r\n\r\n# The issue \r\n\r\nIt seems that resources, when placed inside a local module, are not handled by the `expect_failures` of the `run` block. Either we missed something and that is possible, either that would be a nice thing to have to be able to test our module with the integration of its own local modules.\r\n\r\n## The `terraform test` output\r\n\r\n```txt\r\n  run \"check_precondition_ssm_parameter_encryption\"... fail\r\n\u2577\r\n\u2502 Error: Invalid `expect_failures` reference\r\n\u2502 \r\n\u2502   on tests\/main.tftest.hcl line 22, in run \"check_precondition_ssm_parameter_encryption\":\r\n\u2502   22:     module.workshop_participant_users.aws_ssm_parameter.example,\r\n\u2502 \r\n\u2502 You cannot expect failures from module.workshop_participant_users.aws_ssm_parameter. You can only expect failures from checkable objects such as input variables, output values,\r\n\u2502 check blocks, managed resources and data sources.\r\n```\r\n\r\n## Files content\r\n\r\n```terraform\r\n# file : tests\/main.tftest.hcl\r\n\r\nrun \"check_precondition_ssm_parameter_encryption\" {\r\n  command = plan\r\n\r\n  # [...]\r\n\r\n  expect_failures = [\r\n    module.workshop_participant_users.aws_ssm_parameter.example,\r\n  ]\r\n}\r\n```\r\n\r\n```terraform\r\n# file : main.tf\r\n\r\nmodule \"workshop_users\" {\r\n  source = \".\/modules\/users\"\r\n\r\n  kms_key_id                    = var.create_kms_key ? try(resource.aws_kms_key.backend[0].arn, \"\") : \"\"\r\n}\r\n\r\n\r\n```\r\n\r\n```terraform\r\n# file : modules\/users\/main.tf\r\n\r\nresource \"aws_ssm_parameter\" \"example\" {\r\n\r\n  # [...]\r\n\r\n  key_id      = var.kms_key_id\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = var.kms_key_id != \"\"\r\n      error_message = \"A key must be created to encrypt the ssm parameter.\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAny ideas on that ? \r\n\r\n\r\n### Attempted Solutions\r\n\r\nWe have tried with the expecting failure on the whole module scope as well as for other resources inside the local module.\r\n\r\n### Proposal\r\n\r\n_No response_\r\n\r\n### References\r\n\r\nWe are not sure whether or not it is also the case for the registry modules as well as the local ones tested here.","comments":["Hi @alexis-renard, thanks for filing this! It's an interesting idea, and I can confirm that this isn't currently supported.\r\n\r\nCurrently, Terraform Test does respect the encapsulation of modules much like regular Terraform. For example, in regular Terraform you can't refer to attributes of resources within child modules. You have to expose the relevant values through outputs. Terraform Test has the same restrictions, which is to say it's not aware of the inner workings of child modules and so can't validate anything within child modules either in `assertions` or `expect_failures` blocks. I will also add that the error message isn't doing a very good job here either. You likely are referencing a checkable object within the module, but it's the scope that is the problem rather than the type of thing being referenced which isn't made clear.\r\n\r\nA purists view here would be to say that you should write tests for the child modules directly, and within those you can validate the expected failures are working as expected. Then when it comes to the parent modules you shouldn't need to worry about testing those edge cases - you can be confident the child modules will behave as expected.\r\n\r\nHowever, we do have a longer term project to look at this encapsulation closer. For example, we want to add the ability to assert whether a resource is being created\/updated\/deleted\/replaced rather than simply looking at the final state of a resource (#34500). For this functionality it is entirely reasonable  that users would want to see what is happening to resources within child modules (as their infrastructure may, for example, rely on a nested resource being simply updated and not replaced). So, I think in the context of revisiting the scope for that functionality, this request is something that we can likely get working at the same time without too much trouble. But, it will have to wait for that work to be prioritised.\r\n\r\nThanks again for the request!\r\n\r\n","Hey @liamcervante, thanks for the confirmation and the detailed insights.\r\n\r\nFor now, I think we will then opt for creating the test files inside the local module and adapt our ci to trigger it specifying the path of the local modules. \r\n\r\nNote: purist or not, it would still be nice to be able to test the entire workflow within the root module, including how it interacts with its own submodules. The idea is to validate that \"the local submodules are well integrated with the root module\" (using tests of at root module level calling the sub modules based on different root input variables) more than validating that \"the local submodules work as intended when called them separately\" (using tests at the sub module level) \r\n\r\nThis is not an urgent matter for us, even though it would have been nice to have something specified in the doc, clearly stating that modules resources can not be tested for now. It would have saved us some research and test time I guess :)\r\n\r\nAnyhow : thanks for the confirmation that it is not available at the time, we'll stay tuned for this one !","Hey @alexis-renard, Thank you for your explanation on this issue. I just ran into a similar issue where I am writing a module that wraps another module. We do this often to build in our sane default on top of existing modules. However, currently I'm limited in what I can validate by what the underlining module exposes in the outputs. This is quit frustrating because I can open my state file and see the resources the module is creating and their properties, but I can't write automated test for it. (using terraform native functionality). Please, any work that could be done to expose the resources and attributes for the purpose of writing good test would be extremely useful and greatly appreciated.  +1 +1 +1 "],"labels":["enhancement","new","terraform test"]},{"title":"null resource local-exec destroy needs API token support","body":"### Terraform Version\n\n```shell\nv1.6.6\n```\n\n\n### Use Cases\n\nWe have a requirement to be able to manage a set of objects in a cloud, but the Terraform provider for that cloud does not yet provide a resource to achieve this. Since this is hopefully a short term solution, we don't want to go to the effort of writing our own Terraform provider resource, and instead need to use a null resource local-exec provisioner calling cURL. Use of cURL is required as the providers own CLI tool also does not yet support this resource, so we have to call the API directly. Our problem is that we need to pass in an API token to cURL in the form of an HTTP header, and that token is sourced from a Terraform data resource. The \"when = create\" provisioner can do this easily, however there is no way to pass this data into the \"when = destroy\" provisioner without triggering a replacement of the resource on every Terraform run.\n\n### Attempted Solutions\n\nWe've tried the following:\r\n1) Passing the API token by referencing the data.blah.api_token within the cURL command, however this results in Terraform throwing an error that we cannot reference such a variable within a destroy provisioner. \r\n2) Passing the API token in via the triggers{} block, however that results in the resource being replaced on every invocation of Terraform, as the token is short lived and replaced every run by design.\r\n3) Writing the API token into a file and passing that to cURL, however our security teams would rightly take issue with this. \n\n### Proposal\n\nWe need a mechanism to pass data which can change, but which does not affect the resources managed by Terraform\/local-exec, and so which does not require a resource to be replaced. This could be achieved by a function, perhaps non_material() , which explicitly marks the data as being data which does not matter to the generated objects configuration. This function would be allowed to call data sources, variables, etc, within destroy local-exec provisioners. The documentation for this function should state clearly that it's only for use for ephemeral data, such as API tokens, which is needed to destroy a resource but the actual value of the data does not affect the configuration of the resource itself. \n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Add `directoryexists` and `directoryset` functions","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes hashicorp\/terraform#34694\r\nRef hashicorp\/terraform#33394\r\nRef hashicorp\/terraform#25316\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.8.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n- A new function `directoryexists` got implemented\r\n- A new function `directoryset` got implemented\r\n","comments":["### \ud83d\udcc4 Content Checks\n\nUpdated: Sat, 17 Feb 2024 16:20:50 GMT\n\n<details><summary>Found 2 error(s)<\/summary>\n\n#### `docs\/language\/functions\/directoryexists.mdx`\n\n| Position | Description | Rule |\n|---|---|---|\n| `1:1-1:1` | This file is not present in the nav data file at data\/language-nav-data.json. Either add a path that maps to this file in the nav data or remove the file. If you want the page to exist but not be linked in the navigation, add a `hidden` property to the associated nav node. | `no-unlinked-pages` |\n\n#### `docs\/language\/functions\/directoryset.mdx`\n\n| Position | Description | Rule |\n|---|---|---|\n| `1:1-1:1` | This file is not present in the nav data file at data\/language-nav-data.json. Either add a path that maps to this file in the nav data or remove the file. If you want the page to exist but not be linked in the navigation, add a `hidden` property to the associated nav node. | `no-unlinked-pages` |\n\n<\/details>\n<!-- Sticky Pull Request Commentcontent-conformance -->","Not sure what I'm doing wrong for the content checks test to fail still.","Thanks for this submission! I will bring it up in triage to see if we will consider it for review. We may want to include these as built-ins given their mirrored file versions, or we may ask for these to be built as provider functions. Thanks again!"],"labels":["enhancement","functions"]},{"title":"Directory equivalent of `fileexists` and `fileset` functions","body":"### Terraform Version\n\n```shell\nTerraform 1.7.3\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nI'd like to be able to have a directory structure of pure data encoded in some formats like YAML, or even HCL.\r\nE.g. a monorepo that contains the configuration of git repositories.\r\n\r\n```console\r\n$ ls -lh\r\ntotal 8\r\n-rw-r--r--  1 bryanhonof  staff    63B Feb 17 16:55 main.tf\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo1.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo10.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo2.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo3.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo4.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo5.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo6.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo7.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo8.git\r\ndrwxr-xr-x  2 bryanhonof  staff    64B Feb 17 16:55 repo9.git\r\n```\r\n\r\nThen, I'd like to be able to do the following in HCL\/Terraform.\r\n\r\n```hcl\r\n# main.tf\r\nlocals {\r\n  repositories = directoryset(path.module, \"*.git\")\r\n}\r\n\r\n# Some logic using modules and `for_each` to create resources\r\n# The module would then internally use `fileset` and `file` to do more complicated operations\r\n# ...\r\n\r\noutput \"scanned_repositories\" {\r\n  value = local.repositories\r\n}\r\n```\r\n\r\n```console\r\n$ terraform init\r\n...\r\n$ terraform plan\r\n...\r\n$ terraform apply\r\n...\r\n$ terraform output\r\nscanned_repositories = toset([\r\n  \"repo1.git\",\r\n  \"repo10.git\",\r\n  \"repo2.git\",\r\n  \"repo3.git\",\r\n  \"repo4.git\",\r\n  \"repo5.git\",\r\n  \"repo6.git\",\r\n  \"repo7.git\",\r\n  \"repo8.git\",\r\n  \"repo9.git\",\r\n])\r\n```\n\n### Attempted Solutions\n\nUsing `fileset` in many ways.\r\nSome other hacky solutions to figure out if something's a directory or not.\n\n### Proposal\n\nImplement the directory equivalent of `fileexists` and `fileset`.\n\n### References\n\n- hashicorp\/terraform#33394\r\n- hashicorp\/terraform#25316","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Testing\u2014Mechanism for identifying breaking changes (e.g.: stateful resource replacement)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.3\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/archive v2.4.2\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.37.0\r\n+ provider registry.terraform.io\/hashicorp\/http v3.4.1\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\r\n+ provider registry.terraform.io\/hashicorp\/time v0.10.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nAs a platform engineer, I want to be able to detect breaking changes, such as replacement of stateful resources, across versions of terraform modules, so that I can reduce such changes, or update the major version of the module appropriately if they cannot be avoided.\r\n\r\n### Attempted Solutions\r\n\r\nn\/a\r\n\r\n### Proposal\r\n\r\nSome potential solutions:\r\n* Pre\/post-run hooks\r\n  Enable execution of actions, such as a `git checkout`, prior to and after `run` blocks.\r\n  \r\n  This could look something like:\r\n  ```\r\n  run \"test_previous\" {\r\n    before = {\r\n      exec   = \"git checkout \\\"$(git describe --tags --abbrev=0)\\\"\"\r\n    }\r\n    after = {\r\n      exec   = \"git checkout \\\"$CI_GIT_SHA\\\"\"\r\n    }\r\n    \r\n    run \"test_current\" {\r\n      ...\r\n    }\r\n    ...\r\n  }\r\n  ```\r\n* Partial execution, allowing control to be returned to the CI tool\/shell\r\n  ```\r\n  run \"test_previous\" {\r\n    pause = true\r\n    # could be a number of different keywords; break, halt, etc.\r\n  }\r\n  \r\n  run \"test_current\" {\r\n    ...\r\n  }\r\n  ```\r\n  ```\r\n  #!\/bin\/sh\r\n  git checkout \"$(git describe --tags --abbrev=0)\"\r\n  terraform test\r\n  git checkout \"$CI_GIT_SHA\"\r\n  terraform test --continue\r\n  ...\r\n  ```\r\n\r\n### References\r\n\r\n- #34356","comments":["Hi @bgshacklett, thanks for filing this!\r\n\r\nWe have another issue https:\/\/github.com\/hashicorp\/terraform\/issues\/34500 tracking a feature that will add the ability to validate the status of a particular resource during a plan (such as checking if it is being created\/updated\/deleted\/replaced). Would that suit your needs here?\r\n\r\nI don't really understand the connection between the issue title and your proposed solutions. Could you elaborate a bit more on how being able to execute git checkout commands will help you identify breaking changes? Is it as simple as wanting to update the configuration being tested between different module versions?\r\n\r\nWe do currently support loading alternate modules for executions into run blocks, so maybe a better solution here would be for us to find a way to allow run blocks from different modules to share the same state file. Then maybe you could write tests like this:\r\n\r\n```hcl\r\nrun \"previous_version\" {\r\n  module {\r\n    source  = \"hashicorp\/my-module\"\r\n    version = \"0.1.0\" # previous version\r\n  }\r\n}\r\n\r\nrun \"current_version\" {\r\n  module {\r\n    source  = \"hashicorp\/my-module\"\r\n    version = \"0.2.0\" # current version\r\n  }\r\n\r\n  # Introduce some way to validate none of the resources\r\n  # are being updated.\r\n  assert {\r\n    # ...\r\n    # not possible yet, tracked in #34500.\r\n    # ...\r\n  }\r\n}\r\n```\r\n \r\nHopefully my idea makes sense! Do you think that would enable you to test what you need to?\r\n\r\nThanks!","My need largely boils down to identifying: \"do the changes in this PR result in significant effort for those who would potentially consume the newer version of the module?\". I left the title broad, because I didn't want to limit the discussion to a particular implementation.\r\n\r\nIn the past, to handle this using a custom test framework, I would:\r\n1. check out the previous tag (not a specific tag, but just the most recent)\r\n2. run an apply\r\n3. check out the current SHA (merged result or branch)\r\n4. run a plan\r\n5. identify any potential problems\r\n    1. errors generating the plan, such as incompatible inputs\r\n    2. recreation of stateful resources or resources which could otherwise cause problems\r\n6. run an apply\r\n7. identify any potential problems...\r\n8. report\r\n\r\nThat's one potential workflow (which TF doesn't currently support), but I suspect there are some other good ways to handle it. The example in your response looks like a good potential solution, with the benefit of being written in a declarative fashion. As written, however, there are two things that could stand to be improved:\r\n* The module source must be specified.\r\n  This isn't a big problem, but could lead to failures in the event that a repository is restructured.\r\n* The version is specified statically.\r\n  This is more problematic. It would require everyone to change the version of the previous module in the test for each pull request. This is easy to forget, and would also require that PRs are updated every time a new tag is added. That can be a significant amount of effort _and_ is a very easy step to forget.","I just ran into another issue with the above suggestion. It does not allow us to apply `*_override.tf` files against the previous version of the module, which we require for mocking\/stubbing certain behaviors. At present, there are a number of capabilities that are lacking fro the built-in `mock_provider` and `override_*` capabilities, and we continue to rely on `*_override.tf` files to overcome those limitations in the meantime. "],"labels":["enhancement","new","terraform test"]},{"title":"`yamldecode()` should support files with `%YAML 1.2` directive","body":"### Terraform Version\n\n```shell\nTerraform v1.7.3\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nlocals {\r\n   doc = <<EOT\r\n%YAML 1.2\r\n---\r\nroot:\r\n  key: value\r\nEOT\r\n   val = yamldecode(local.doc)[\"root\"][\"key\"]\r\n}\r\n\r\noutput \"val\" {\r\n   value = local.val\r\n}\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/asaba-hashi\/bb070ec6caadf4c588209cbd2350344c\n\n### Expected Behavior\n\nThe \"doc\" contents should be correctly parsed as YAML.\r\n\r\n```\r\n\r\nChanges to Outputs:\r\n  + val = \"value\"\r\n```\n\n### Actual Behavior\n\nterraform reported an error\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Error in function call\r\n\u2502\r\n\u2502   on test.tf line 8, in locals:\r\n\u2502    8:    val = yamldecode(local.doc)[\"root\"][\"key\"]\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 while calling yamldecode(src)\r\n\u2502     \u2502 local.doc is \"%YAML 1.2\\n---\\nroot:\\n  key: value\\n\"\r\n\u2502\r\n\u2502 Call to function \"yamldecode\" failed: found incompatible YAML document.\r\n```\n\n### Steps to Reproduce\n\n1. `terraform init`\r\n2. `terraform plan`\n\n### Additional Context\n\nYAML 1.2 introduces the `%YAML` [directive](https:\/\/yaml.org\/spec\/1.2.0\/#directive\/YAML\/) to direct parsers to treat the document as version 1.1 or 1.2.\r\n\r\nIn particular, YAML 1.2 introduces a [Failsafe Schema] that does not include the additional \"truthy\" values like `on`, `no` etc. , so this may be the path towards fixes\/workarounds for problems like #34611. The workaround would be for such documents to include a `%YAML 1.2` directive.\r\n\r\n[Failsafe Schema]: https:\/\/yaml.org\/spec\/1.2.2\/#chapter-10-recommended-schemas\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","The YAML 1.2 spec says:\r\n\r\n> A version 1.2 YAML [processor](https:\/\/yaml.org\/spec\/1.2.2\/#processes-and-models) must accept [documents](https:\/\/yaml.org\/spec\/1.2.2\/#documents) with an explicit \u201c%YAML 1.2\u201d directive, as well as [documents](https:\/\/yaml.org\/spec\/1.2.2\/#documents) lacking a \u201cYAML\u201d directive. Such [documents](https:\/\/yaml.org\/spec\/1.2.2\/#documents) are assumed to conform to the 1.2 version specification.\r\n\r\nWhile the docs for `yamldecode` do say that only a \"limited\" portion of 1.2 is supported, the parser is behaving more like 1.1 parser, so I do still think this is a bug (whether in the docs or implementation), WDTY?","Here is a workaround for interoperating where you can work with strictly 1.1 docs, which will work with a more compliant parser\/encoder like `raumel.yaml`s roundtrip encoder:\r\n\r\n```terraform\r\nlocals {\r\n   doc = <<EOT\r\n%YAML 1.1\r\n---\r\nroot:\r\n  notaboolvalue: 'no'\r\n  'no': no\r\nEOT\r\n   val1 = yamldecode(trimprefix(local.doc, \"%YAML 1.1\\n\"))[\"root\"][\"no\"]\r\n   val2 = yamldecode(trimprefix(local.doc, \"%YAML 1.1\\n\"))[\"root\"][\"notaboolvalue\"]\r\n}\r\n\r\noutput \"notaboolkey\" {\r\n   value = local.val1\r\n}\r\n\r\noutput \"notaboolvalue\" {\r\n   value = local.val2\r\n}\r\n```","The enhancement determination was made due to the \"limited\" disclaimer in the docs, so the thought in triage with @apparentlymart is that any increase in functionality is a new feature (otherwise it would just be documented as supported). Thanks for the work-around!"],"labels":["enhancement","new"]},{"title":"Add ability to Amend Module Resource Attributes","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.3\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nHello \ud83d\udc4b \r\n\r\nI have had a thought about the state of Terarform 'modules', and the difficulty\r\nof relying on 3rd party modules and not having the ability to quickly fix or\r\namend a resource that has been created within a module (If the module becomes\r\nunmaintained)\r\n\r\nOther coding frameworks have appeared to slightly solve this issue by having\r\n'plugins', 'packages', 'add-ons' etc able to have parts of their functionality\r\nre-written.\r\n\r\neg. a WordPress's function in a plugin can be simply overwritten by redefining\r\nit ...\r\n\r\nIf I create a public Terraform module, for a 'secure' AWS RDS instance, but\r\naccidentally make a mistake ...\r\n\r\n```\r\n# main.tf\r\nresource \"aws_db_instance\" \"secure_ish\" {\r\n  [...]\r\n  publicly_accessible = true\r\n  [..]\r\n}\r\n```\r\n\r\nThe user will use it like this:\r\n\r\n```\r\nmodule \"a_secure_rds\" {\r\n  source = \".\/securish-db-instance\"\r\n}\r\n```\r\n\r\nThe users of this module then either have to wait for me to fix it, or the user\r\nwill create a fork, and maintain the fork ...\r\n\r\n### Attempted Solutions\r\n\r\nN\/A\r\n\r\n### Proposal\r\n\r\nMy proposal is, to add an optional `resource_attribute_amendments` block to the `module`\r\nblock\r\n\r\nAllow resource attributes within modules to be added\/deleted\/updated\r\n\r\nPossibly a `resource_attribute_amendments` block could be specified to amend resource\r\nattributes within a module.\r\n\r\neg.\r\n\r\n```\r\nmodule \"a_secure_rds\" {\r\n  source = \".\/securish-db-instance\"\r\n\r\n  resource_attribute_amendments = {\r\n    aws_db_instance.secure_ish.publicly_accessible = false,\r\n  }\r\n}\r\n```\r\n\r\n### References\r\n\r\nN\/A","comments":["Thanks for this feature request, @Stretch96!\r\n\r\nThis is an interesting idea, but also something that doesn't really have any precedent in Terraform today. We would typically expect that, as you said, someone would report a bug with you as the module author and then you would publish a fixed module, or if you don't want to do that then they would fork the module and maintain their own version.\r\n\r\nWe do need to be particularly careful with anything that explicitly pierces the abstraction offered by the module. In the example you shared, for instance, this would mean that if you wanted to rename `aws_db_instance.secure_ish`, or move it into a module, or otherwise refactor it later then you might be blocked from doing so because a caller is depending on your implementation details. We must be cautious about anything which grows the scope of what is covered by \"backward compatibility\" for a module author, because otherwise we risk making it essentially impossible to change a shared module at all after it's been initially written, if everything about how you implemented the module is subject to patching and overriding by callers.\r\n\r\n"],"labels":["enhancement","new"]},{"title":"Terraform test - expect_failures does not display the validation rule error message in verbose mode","body":"### Terraform Version\n\n```shell\n1.7.4\n```\n\n\n### Use Cases\n\nWhen I run a test and want to test the validation rules are working as expected I can set some fake attribute values to force the validation rule to fail. This is expected but when I add the expect_failures = [xxx] the terraform logs don't display the validation rules that has failed.\r\nkeeping those validation rules with the failure message will put an evidence the rule has failed as expected.\n\n### Attempted Solutions\n\nHere a test run [expect_failures commented]:\r\n```hcl\r\nrun \"validation_rules_fails\" {\r\n  command = plan\r\n\r\n  variables {\r\n    name                = \"demo\"\r\n    resource_group_name = \"demo\"\r\n    location            = \"southeastasia\"\r\n    sku = {\r\n      name     = \"Dev(No SLA)_Standard_D11_v2\"\r\n      capacity = 1\r\n    }\r\n\r\n    kusto_cluster_principal_assignments = {\r\n      rbac1 = {\r\n        name                = \"KustoPrincipalAssignment\"\r\n        resource_group_name = run.setup_dependencies.azurerm_resource_group.name\r\n        principal_id        = run.setup_dependencies.client_config.client_id\r\n        tenant_id           = run.setup_dependencies.client_config.tenant_id\r\n        principal_type      = \"Fake\"\r\n        role                = \"Unknown\"\r\n      }\r\n    }\r\n  }\r\n\r\n  # expect_failures = [\r\n  #   var.kusto_cluster_principal_assignments\r\n  # ]\r\n}\r\n```\r\n\r\nresult:\r\n\r\n```bash\r\nrun \"validation_rules_fails\"... fail\r\n\u2577\r\n\u2502 Error: Invalid value for variable\r\n\u2502 \r\n\u2502   on variables.tf line 309:\r\n\u2502  309: variable \"kusto_cluster_principal_assignments\" {\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 var.kusto_cluster_principal_assignments is map of object with 1 element\r\n\u2502 \r\n\u2502 Only the following values are authorised: 'App', 'Group' or 'User'. Fix the value you have set to: [Fake]\r\n\u2502 \r\n\u2502 This was checked by the validation rule at variables.tf:329,3-13.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid value for variable\r\n\u2502 \r\n\u2502   on variables.tf line 309:\r\n\u2502  309: variable \"kusto_cluster_principal_assignments\" {\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 var.kusto_cluster_principal_assignments is map of object with 1 element\r\n\u2502 \r\n\u2502 Only the following values are authorised: 'AllDatabasesAdmin' or 'AllDatabasesViewer'. Fix the value you have set to: [Unknown]\r\n\u2502 \r\n\u2502 This was checked by the validation rule at variables.tf:345,3-13.\r\n\u2575\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... tearing down\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... fail\r\n\r\nFailure! 2 passed, 1 failed.\r\n```\r\n\r\nNow if I set the expect_failures and re-run:\r\n```hcl\r\nrun \"validation_rules_fails\" {\r\n  command = plan\r\n\r\n  variables {\r\n    name                = \"demo\"\r\n    resource_group_name = \"demo\"\r\n    location            = \"southeastasia\"\r\n    sku = {\r\n      name     = \"Dev(No SLA)_Standard_D11_v2\"\r\n      capacity = 1\r\n    }\r\n\r\n    kusto_cluster_principal_assignments = {\r\n      rbac1 = {\r\n        name                = \"KustoPrincipalAssignment\"\r\n        resource_group_name = run.setup_dependencies.azurerm_resource_group.name\r\n        principal_id        = run.setup_dependencies.client_config.client_id\r\n        tenant_id           = run.setup_dependencies.client_config.tenant_id\r\n        principal_type      = \"Fake\"\r\n        role                = \"Unknown\"\r\n      }\r\n    }\r\n  }\r\n\r\n  expect_failures = [\r\n    var.kusto_cluster_principal_assignments\r\n  ]\r\n}\r\n```\r\n\r\nI get the following result:\r\n```bash\r\n run \"validation_rules_fails\"... pass\r\n\r\nPlanning failed. Terraform encountered an error while generating this plan.\r\n\r\n\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... tearing down\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... pass\r\n\r\nSuccess! 3 passed, 0 failed.\r\n```\n\n### Proposal\n\nMy proposition is to get a combined view of the validation rule errors:\r\n\r\nExpected:\r\n```hcl\r\nrun \"validation_rules_fails\" {\r\n  command = plan\r\n\r\n  variables {\r\n    name                = \"demo\"\r\n    resource_group_name = \"demo\"\r\n    location            = \"southeastasia\"\r\n    sku = {\r\n      name     = \"Dev(No SLA)_Standard_D11_v2\"\r\n      capacity = 1\r\n    }\r\n\r\n    kusto_cluster_principal_assignments = {\r\n      rbac1 = {\r\n        name                = \"KustoPrincipalAssignment\"\r\n        resource_group_name = run.setup_dependencies.azurerm_resource_group.name\r\n        principal_id        = run.setup_dependencies.client_config.client_id\r\n        tenant_id           = run.setup_dependencies.client_config.tenant_id\r\n        principal_type      = \"Fake\"\r\n        role                = \"Unknown\"\r\n      }\r\n    }\r\n  }\r\n\r\n  expect_failures = [\r\n    var.kusto_cluster_principal_assignments\r\n  ]\r\n}\r\n```\r\n\r\n```bash\r\nterraform test --verbose\r\n\r\n\r\nrun \"validation_rules_fails\"... pass.  <<<---- The status is set to pass\r\n\u2577\r\n\u2502 Error: Invalid value for variable.\r\n\u2502 \r\n\u2502   on variables.tf line 309:\r\n\u2502  309: variable \"kusto_cluster_principal_assignments\" {\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 var.kusto_cluster_principal_assignments is map of object with 1 element\r\n\u2502 \r\n\u2502 Only the following values are authorised: 'App', 'Group' or 'User'. Fix the value you have set to: [Fake]\r\n\u2502 \r\n\u2502 This was checked by the validation rule at variables.tf:329,3-13.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid value for variable\r\n\u2502 \r\n\u2502   on variables.tf line 309:\r\n\u2502  309: variable \"kusto_cluster_principal_assignments\" {\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 var.kusto_cluster_principal_assignments is map of object with 1 element\r\n\u2502 \r\n\u2502 Only the following values are authorised: 'AllDatabasesAdmin' or 'AllDatabasesViewer'. Fix the value you have set to: [Unknown]\r\n\u2502 \r\n\u2502 This was checked by the validation rule at variables.tf:345,3-13.\r\n\u2575\r\nPlanning failed. Terraform encountered an error while generating this plan, however it was expected. <<<--- add an explanation.\r\n\r\n\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... tearing down\r\ntests\/unit_tests_cluster_principal_assignment.tftest.hcl... pass\r\n\r\nSuccess! 3 passed, 0 failed.\r\n```\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Terraform test - Add support terraform functions in mock objects","body":"### Terraform Version\r\n\r\n```shell\r\n1.7.3\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI want to generate test data based on specific data type or string format:\r\n\r\n- for a GUID use uuid()\r\n- for an Azure resource id use format(\"\/subscriptions\/%s\", uuid())\r\n\r\n\r\n### Attempted Solutions\r\n\r\n```hcl\r\nmock_data \"azurerm_client_config\" {\r\n  defaults = {\r\n    client_id = uuid()\r\n  }\r\n}\r\n\r\n\r\noverride_resource {\r\n  target = azurerm_storage_account.example   \r\n  values = {\r\n      id = format(\"\/subscriptions\/%s\/resourceGroups\/example\/providers\/Microsoft.Storage\/storageAccounts\/myaccount\", uuid())  \r\n }\r\n}\r\n```\r\n\r\nError when trying to use a function:\r\n```bash\r\nError: Function calls not allowed\r\n\u2502 \r\n\u2502   on tests\/mock_data\/data.tfmock.hcl line 9, in mock_data \"azurerm_client_config\":\r\n\u2502    9:     client_id = uuid()\r\n\u2502 \r\n\u2502 Functions may not be called here.\r\n```\r\n\r\n### Proposal\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for suggesting this, @LaurentLesle!\r\n\r\nYour use of `uuid` in this example draws attention to an important design question: we'd need to decide at what point these expressions are going to get evaluated, and in particular whether they get _re-evaluated_ for each run or just evaluated once up front and used many times.\r\n\r\nFor most functions that wouldn't matter, but `uuid` in particular is an \"impure\" function that returns a different result each time it's called. The same is true for `timestamp` and `bcrypt`.\r\n\r\nAs an input to help make that decision, can you say more about what you intended it to mean to write `uuid` there? Would it be important to whatever you are testing for the generated UUID to remain fixed across all of the test runs, or would it be okay (or perhaps, even, better!) for it to change for each call like some real-world data sources do?\r\n\r\n","@apparentlymart - At that stage I would be in favour to keep the current behaviour of the functions as it is with terraform plan\/apply instead of changing it for some of them. Maybe at some point new use cases would emerge and would justify some adjustments. I have noted 1.8 includes provider functions, maybe some edge use cases could be handled that way?\r\n\r\nMy overall goal with this feature request is to ensure I can mock data and resources in order to set correct values (more like a type format, meaning guid or resource ID format) to dependencies required to test the resource I am developing. In this instance I am building an azure verified module for Kusto cluster and need to inject all dependencies to test it accordingly (vnet for private endpoint, storage account for diagnostics profiles....). If I am not overriding the value set by the mock provider I am getting an error saying the attribute is not in the correct format.\r\n\r\nI picked-up uuid() on purpose as it changes indeed at every execution and could potentially be interesting for a unit testing perspective. However for the example above I would favour more uuidv5(xx,xx). What is important to me is to assert the mock value generated vs the one being used or returned by the resources and testing the validation rules of the variables. In this example I am mocking an Azure context and just need to ensure the value is a GUID or for an ID making sure it is a resource ID (8 segments for most of the resources).\r\n\r\n```hcl\r\n  run \"setup_dependencies\"... pass\r\n\r\n# data.azurerm_client_config.current:\r\ndata \"azurerm_client_config\" \"current\" {\r\n    client_id       = \"c7ji9i4m\".    <<<-- Problem I am trying to solve.\r\n    id              = \"9vodmmad\"\r\n    object_id       = \"j69fw94x\"\r\n    subscription_id = \"n5s8apth\"\r\n    tenant_id       = \"2vvpw2fy\"\r\n}\r\n\r\n....\r\nError: Invalid value for variable\r\n\u2502 \r\n\u2502   on main.cluster_principal_assignment.tf line 7, in module \"kusto_cluster_principal_assignment\":\r\n\u2502    7:   principal_id        = each.value.principal_id\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 var.principal_id is a string\r\n\u2502 \r\n\u2502 The value you have set is not a valid GUID.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n","Thanks for that extra context!\r\n\r\nSince this is a situation where functions are not currently allowed, there isn't really an existing precedent to follow: we're going to be implementing something new here either way, but thinking about it as \"as close as possible to normal plan\/apply\" is one plausible way to shape those decisions indeed.\r\n\r\nIn case it's helpful context to someone else considering this issue in future: an interesting characteristic about these \"impure\" functions is that during the plan phase they return unknown values, so that we can wait until the apply phase to commit to their final values. If we were to decide to treat this the same as normal plan\/apply, that would mean that during the plan phase of each run the value would appear unknown. It would also mean that if a test scenario contained two `run` blocks that both include both a plan and apply phase, both plans would find an unknown result, and each apply would have a _different_ result.\r\n\r\nI've not yet thought very deeply about the consequences of that behavior in the context of mocks and overrides. It's interesting that it would (I think?) represent the first situation where it's actually possible for an overridden value to vary between `run` blocks in the same test scenario. It might also be in conflict with current implementation decisions, since right now it's immaterial whether the mocks get evaluated just once before any rounds occur or whether they get evaluated separately for each phase\/round.\r\n\r\n"],"labels":["enhancement","new","terraform test"]},{"title":"Crash on splatting optional object in for_each (...and object is not set)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.3\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/azurerm v3.90.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nmodule: main.tf\r\n```terraform\r\n...\r\n    dynamic \"registry\" {\r\n        for_each = var.registry_config[*]\r\n\r\n        content {\r\n            server = registry.server\r\n            username = registry.username\r\n            password_secret_name = registry.password_secret_name\r\n        }\r\n    }\r\n...\r\n```\r\n\r\nmodule: variables.tf\r\n```terraform\r\n...\r\nvariable \"registry_config\" {\r\n  default = null\r\n  nullable = true\r\n  sensitive = true\r\n  type = object({\r\n    server = string\r\n    username = string\r\n    password_secret_name = string\r\n  })\r\n}\r\n...\r\n```\r\n\r\n### Debug Output\r\n\r\n[Gist crash - terraform 1.6.3](https:\/\/gist.github.com\/maxxiefjv\/bfc357675843cb9690e61cffe9043243)\r\n[Gist crash - terraform 1.7.3](https:\/\/gist.github.com\/maxxiefjv\/e1dc041ce930581c488ba77e179e5673)\r\n\r\n### Expected Behavior\r\n\r\nI would expect an error log saying you cannot splat nullable variables... or variables containing the `null` value.\r\n\r\n### Actual Behavior\r\n\r\nTerraform crashed asking me to report\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n2. `terraform apply`\r\n\r\n### Additional Context\r\n\r\nRan into this crash when setting up [Azure Container App](https:\/\/registry.terraform.io\/providers\/hashicorp\/azurerm\/latest\/docs\/resources\/container_app)","comments":["Hi @maxxiefjv,\r\n\r\nYou mention that this is Terraform version v1.7.3 but the stack trace shown is from v1.6.3. There was a similar bug fixed in the latest version, can you verify that you still see the crash using v1.7.3?","> Hi @maxxiefjv,\r\n> \r\n> You mention that this is Terraform version v1.7.3 but the stack trace shown is from v1.6.3. There was a similar bug fixed in the latest version, can you verify that you still see the crash using v1.7.3?\r\n\r\nHi @jbardin,\r\n\r\nYes, it does still crash for v1.7.3... forgot to update the Gist. In case it might be helpful I also kept the v1.6.3 logs. You can now find the crash for both v1.6.3 and v1.7.3 in my public gists (referenced in the bug description)\r\n","Based on the stack trace, I expect this problem is triggered by the input variable being marked as `sensitive = true`.\r\n\r\nIt seems that that the splat expression logic is trying to dig into the given value in a way that isn't considering that the value might be sensitive. This panic is there to reduce the risk of some code just _accidentally_ losing track of the \"sensitive\" mark when evaluating.\r\n\r\nTherefore I expect the correct behavior is to first \"unmark\" the value (separating the value itself from the \"sensitive\" metadata), perform whatever checks this logic is performing, and then re-apply the same \"sensitive\" metadata back to the final result.\r\n\r\nHowever, in this _particular_ case fixing this would lead only to another problem: `dynamic` block `for_each` does not accept sensitive values, because there isn't any way to avoid disclosing which blocks were generated and therefore to avoid exposing information about the content of the sensitive value.\r\n\r\nOne way to work around both problems is to tell Terraform that the object as a whole isn't sensitive but that the individual attributes are, and therefore the result of the test for whether the object is `null` need not be treated as sensitive:\r\n\r\n```hcl\r\n    dynamic \"registry\" {\r\n      for_each = nonsensitive(var.registry_config)[*]\r\n\r\n      content {\r\n        server               = sensitive(registry.server)\r\n        username             = sensitive(registry.username)\r\n        password_secret_name = sensitive(registry.password_secret_name)\r\n      }\r\n    }\r\n```\r\n\r\nThis structure means that the decision about whether there is a `registry` block or not is _not_ considered as sensitive information, but the attributes inside are.\r\n\r\nOf course this doesn't mean that we shouldn't still fix the crashing bug when sensitive values get used with the `[*]` operator. That's an upstream bug in `hashicorp\/hcl`, so will need to be fixed there. But I shared the above in the hope that it allows you to get something working with today's Terraform, particularly because us fixing this bug would only have revealed another error message caused by the `for_each` value being sensitive and therefore this workaround would be needed regardless of the crasher bug.\r\n","Thanks @apparentlymart, this helps. I also figured out that I - indeed - wasn't allowed to use sensitive values like this anyway, hence found myself (another) workaround. This one looks a lot cleaner though, so will change it to this example instead.\r\n\r\nThanks for the fast replies and have a nice day!","This is related to #29744, in that they both are missing the sensitive validation, though in different ways. Not sure how `[*]` is skipping the validation exactly, but both issues will probably be solved by the same validation."],"labels":["bug","upstream","new","v1.6","v1.7","hcl"]},{"title":"Where are the terraform test tfstate files located : locally or defined by the backend ?","body":"### Terraform Version\n\n```shell\nTerraform v1.7.3\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/tests#modules-state\n\n### What is the docs issue?\n\nIt's really hard to figure out where the \"test\" tfstates are stored in the doc, and if it's possible to store it remotely.\r\n\r\nOur usecase : \r\n\r\nWe would like to start integrating `terraform test` as steps in our CI\/CD pipelines. We have faced the challenge of a non automatically working terraform destroy when tearing down stage of the tests (not being able to destroy for any reason). That left us with some undestroyed and hanging resources, and as the terraform test command has been done in an ephemeral environment, the \"local state file\" lost.\r\n\r\nOne solution could be to enable storing the tests tfstates not locally but remotely. That way at least, we could take over the destruction of a non working test clean up. \r\n\r\nWould it be possible to clarify that point ?\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @alexis-renard, thanks for filing this.\r\n\r\nIn `terraform test` the state is held entirely in memory for the duration of the operation. Each test file starts with an empty state, and populates it via the run blocks, then attempts to destroy everything that was created at the end. For now, there is no way to store or expose any state created by a test file. We do have a feature request for this: https:\/\/github.com\/hashicorp\/terraform\/issues\/33786. You can follow that issue to keep track of any changes on this front. We are actively doing discovery on this issue to figure out the best way we can give configuration authors more control over the state that is created during test executions.\r\n\r\nTerraform does provide a print out of any resources left alive after the test operation completes, and I think it should also mark the the overall test as failed to make it easier to discover. We do appreciate this is not ideal, but the aim for us is to make the cleanup process as reliable as possible while we figure out a good answer to the state problem.\r\n\r\nI'm going to close this as a duplicate of #33786 in terms of creating the ability for customised state management.\r\n\r\nThanks!","Hello @liamcervante, and thanks for the reply ! I think indeed that #33786 will cover the technical aspect of the feature we need.\r\n\r\nYet I have opened this issue as a documentation one, in order to remove the doubt of the current status inside the doc itself. The goal is to clearly state that the test tfstate is managed internally by terraform (which is something relatively new to us users that are used to manage it ourselves). To clearly state that for now, we have no access to it, neither locally or remotely.\r\n\r\nWould that make sense that I propose a documentation update in that sense ?","Yes, that does make sense. Apologies, I missed the `documentation` tag. I've reopened this, and will ask the docs team to have a look!","Hello @liamcervante should I wait for some of the docs team input \/ guidelines or directly submit some proposal ? Thanks !","@alexis-renard Directly submitting a proposal is fine! In that case the docs team will do a style and correctness review after it is submitted. Thanks!"],"labels":["documentation","new","terraform test"]},{"title":"Added a User-Agent header for remote-state http backend client","body":"Added a User-Agent header for the http client to determine which client call to the http backend for change state.\r\n\r\nSometimes need to be able to send specially formatted state in different ways (for example, hiding sensitive data that will be shown in Swagger output but not hidden for terraform cli). In this case, would like to be able to identify which client sends the request for change state.\r\n\r\n## Target Release\r\n\r\n1.8.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n### ENHANCEMENTS\r\n\r\n- When calling http-backend from terraform,  \"User-Agent\" header does not change, which does not allow correctly identifying the type of connecting client. Changed \"User-Agent\" header from \"Go-http-client\/1.1\" to \"Terraform-http-client\/1.1\"","comments":["\n[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/not_signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34662)\n\n\nThank you for your submission! We require that all contributors sign our Contributor License Agreement (\"CLA\") before we can accept the contribution. [Read and sign the agreement](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34662)\n\n[Learn more about why HashiCorp requires a CLA and what the CLA includes](https:\/\/www.hashicorp.com\/cla)\n\n<sub>Have you signed the CLA already but the status is still pending? [Recheck it](https:\/\/cla.hashicorp.com\/check\/hashicorp\/terraform?pullRequest=34662).<\/sub>","Thanks for this submission. Please sign the CLA per https:\/\/github.com\/hashicorp\/terraform\/pull\/34662#issuecomment-1940421282 and I will bring this to the triage meeting. Thanks!"],"labels":["enhancement","backend\/http"]},{"title":"Include base URL for docs links in machine-readable output","body":"## Context\r\n\r\nThere is a number of commands which produce machine-readable output, which is consumed by other projects, e.g. CDKTF or Terraform LS.\r\n\r\nOn occasion, this output may contain _relative_ links and it may not be always clear to the consumer what to do with those relative links, i.e. how to convert them into absolute ones.\r\n\r\nAdditionally, the documentation has moved to different hostnames or paths in the past (from `terraform.io` to `developer.hashicorp.com\/terraform`) and the machine-readable output should be able to reflect that somehow.\r\n\r\nThis has surfaced recently in https:\/\/github.com\/hashicorp\/terraform\/pull\/34510\r\n\r\n## Proposal\r\n\r\n - Either replace all relative links with absolute, i.e. include the whole base URL beginning with `https:\/\/developer.hashicorp.com`, or\r\n - produce an additional field in all these commands, such as `\"baseUri\": \"https:\/\/developer.hashicorp.com\/terraform\"`\r\n\r\n## Considerations\r\n\r\nIt is possible that these same commands may at some point end up as the source of truth for generated docs and it is likely that we want to retain the ability to produce relative links there.\r\n","comments":["I find it kinda unfortunate that these URLs have ended up hard-coded in this way _at all_, but I don't really have a better idea for how to deal with it, so agree that we should accept that they are there and make them useful.\r\n\r\nI don't really see much point in making the caller of these commands do relative URL resolution themselves, since either way both the base URL and the relative URL will be in this codebase. Therefore using absolute URLs seems like the simplest path both for our own implementation (we can treat each link independently under maintenance, and bulk-update them all together if we end up needing to) and for consumer ergonomics (so we can promise downstreams that we'll always give them absolute URLs and they don't need to worry about any local fixup logic).\r\n\r\nWe also keep old URLs redirecting to new URLs for a long as we possibly can after migrating content from place to place, so I'm not super concerned about the possibility of these moving to another place again.\r\n\r\nThere will of course still be some historical Terraform CLI releases that will emit relative URLs forever -- we're not going to retract them over this -- and so our _own_ consumers like `terraform-schema` should probably have some support for resolving these relative to `https:\/\/developer.hashicorp.com\/` as https:\/\/github.com\/hashicorp\/terraform-schema\/issues\/323 describes. But other consumers that don't need to support those older Terraform CLI versions can ignore it if they want to, once these older versions have become old enough to not be that concerning anymore.\r\n\r\n"],"labels":["enhancement","documentation"]},{"title":"terraform graph -draw-cycles  errors instead of showing cycles ","body":"### Terraform Version\r\n\r\n```shell\r\ntested with:\r\n\r\nTerraform v1.6.2\r\non darwin_amd64\r\n\r\nTerraform v1.7.0\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nlocals {\r\n  test1 = local.test2\r\n  test2 = local.test1\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\nterraform graph -draw-cycles -type=apply\r\n2024-02-08T09:07:49.575-0500 [INFO]  Terraform version: 1.6.2\r\n2024-02-08T09:07:49.575-0500 [DEBUG] using github.com\/hashicorp\/go-tfe v1.36.0\r\n2024-02-08T09:07:49.575-0500 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2024-02-08T09:07:49.575-0500 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2024-02-08T09:07:49.575-0500 [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2024-02-08T09:07:49.575-0500 [INFO]  Go runtime version: go1.21.1\r\n2024-02-08T09:07:49.575-0500 [INFO]  CLI args: []string{\"terraform\", \"graph\", \"-draw-cycles\", \"-type=apply\"}\r\n2024-02-08T09:07:49.575-0500 [DEBUG] Attempting to open CLI config file: \/Users\/hsepulv\/.terraformrc\r\n2024-02-08T09:07:49.575-0500 [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2024-02-08T09:07:49.576-0500 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-02-08T09:07:49.576-0500 [DEBUG] ignoring non-existing provider search directory \/Users\/hsepulv\/.terraform.d\/plugins\r\n2024-02-08T09:07:49.576-0500 [DEBUG] ignoring non-existing provider search directory \/Users\/hsepulv\/Library\/Application Support\/io.terraform\/plugins\r\n2024-02-08T09:07:49.576-0500 [DEBUG] ignoring non-existing provider search directory \/Library\/Application Support\/io.terraform\/plugins\r\n2024-02-08T09:07:49.577-0500 [INFO]  CLI command args: []string{\"graph\", \"-draw-cycles\", \"-type=apply\"}\r\n2024-02-08T09:07:49.580-0500 [DEBUG] New state was assigned lineage \"6022a998-fafe-28ff-48cf-139144ad0456\"\r\n2024-02-08T09:07:51.091-0500 [DEBUG] checking for provisioner in \".\"\r\n2024-02-08T09:07:51.091-0500 [DEBUG] checking for provisioner in \"\/Users\/hsepulv\/bin\"\r\n2024-02-08T09:07:51.092-0500 [DEBUG] Not attaching any node states: overall state is nil\r\n2024-02-08T09:07:51.092-0500 [DEBUG] No state, no orphan outputs\r\n2024-02-08T09:07:51.093-0500 [DEBUG] ReferenceTransformer: \"local.test2 (expand)\" references: [local.test1 (expand)]\r\n2024-02-08T09:07:51.093-0500 [DEBUG] ReferenceTransformer: \"local.test1 (expand)\" references: [local.test2 (expand)]\r\n2024-02-08T09:07:51.093-0500 [ERROR] Graph validation failed. Graph:\r\n\r\nlocal.test1 (expand)\r\n  local.test2 (expand)\r\nlocal.test2 (expand)\r\n  local.test1 (expand)\r\nroot\r\n\u2577\r\n\u2502 Error: Cycle: local.test1 (expand), local.test2 (expand)\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n```\r\n\r\n### Expected Behavior\r\n\r\nWe should get a dot output that we can later use to generate an image\r\n\r\n### Actual Behavior\r\n\r\nConsole outout Error: Cycle\r\n\r\n### Steps to Reproduce\r\n\r\nterraform graph -draw-cycles -type=apply\r\n\r\n### Additional Context\r\n\r\nhttps:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/graph#draw-cycles\r\n\r\n> `-draw-cycles` - Highlight any cycles in the graph with colored edges. This helps when diagnosing cycle errors. This option is supported only when selecting one of the real graph operaton types using the -type=... option.\r\n\r\nWhat good is that option if it errors on a cycle\r\n\r\n### References\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/77960427\/cannot-draw-terrform-cycle-with-colors-using-terraform-graph-with-plan-output","comments":["Thanks for this bug report! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. Thanks again!","I have the same issue with Terraform v1.7.3.\r\nI have a cycle in my script file and using the following commands:\r\n```\r\nterraform plan -out tfplan.out\r\n```\r\nor\r\n```\r\nterraform graph -draw-cycles\r\n```\r\n\r\nThey produce no graph on the output (file or console output) and it throw a classic Terraform error to indicate I have a cycle:\r\n```\r\n\u2577\r\n\u2502 Error: Cycle: ...................\r\n\u2502\r\n\u2502\r\n\u2575\r\n```"],"labels":["bug","new"]},{"title":"[Feature] Support for backend state with private link","body":"### Is there an existing issue for this?\n\n- [X] I have searched the existing issues\n\n### Community Note\n\n<!--- Please keep this note for the community --->\r\n\r\n* Please vote on this issue by adding a :thumbsup: [reaction](https:\/\/blog.github.com\/2016-03-10-add-reactions-to-pull-requests-issues-and-comments\/) to the original issue to help the community and maintainers prioritize this request\r\n* Please do not leave \"+1\" or \"me too\" comments, they generate extra noise for issue followers and do not help prioritize the request\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment and review the [contribution guide](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/blob\/main\/contributing\/README.md) to help.\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\n\n### Description\n\nCurrently it seems the backend state domain is hard coded.\r\nWould be super nice to be able to configure the backend state to use a private link.\r\n\r\n**Remark**\r\nIt is possible to tweak the DNS to resolve the public domains to the private endpoint but there are use cases where this is not wanted\/possible.\n\n### New or Affected Resource(s)\/Data Source(s)\n\nbackend azurerm\n\n### Potential Terraform Configuration\n\n```hcl\nterraform {\r\n  required_providers {\r\n    azurerm = {\r\n      source  = \"hashicorp\/azurerm\"\r\n      version = \"~>3.0\"\r\n    }\r\n  }\r\n  backend \"azurerm\" {\r\n      resource_group_name        = \"tfstate\"\r\n      storage_account_name       = \"<storage_account_name>\"\r\n      container_name             = \"tfstate\"\r\n      key                        = \"terraform.tfstate\"\r\n      # new:\r\n      private_link_domain_prefix = <my_privatelink_domain_prefix>\r\n  }\r\n\r\n}\n```\n\n\n### References\n\n_No response_","comments":["Thanks for taking the time to open this feature request. The Azurerm backend lives in the [hashicorp\/terraform](https:\/\/github.com\/hashicorp\/terraform) repo, so I am going to move this feature request there. ","**Remark**\r\n\r\nReading the code:\r\n\r\nPossibly it is already possible to use this feature if you set the `storageAccountName` parameter to \r\n`storageAccountName: <hosted_zone_prefix>.privatelink` and a documentation addon could help.\r\n\r\nThe related part should be located within [https:\/\/github.com\/tombuildsstuff\/giovanni\/blob\/v0.15.1\/storage\/internal\/endpoints\/endpoints.go](https:\/\/github.com\/tombuildsstuff\/giovanni\/blob\/v0.15.1\/storage\/internal\/endpoints\/endpoints.go)\r\n\r\n```golang\r\n\/\/ GetBlobEndpoint returns the endpoint for Blob API Operations on this storage account\r\nfunc GetBlobEndpoint(baseUri string, accountName string) string {\r\n\treturn fmt.Sprintf(\"https:\/\/%s.blob.%s\", accountName, baseUri)\r\n}\r\n```\r\n\r\nWith a fast check of the code I did not find any sanity verification steps prohibiting this behaviour.\r\nAlso  the unit tests are still green with adjusting the accountName (adding `.privatelink`)\r\nMaybe I find time tomorrow to test this.\r\n\r\n\r\n**Update**\r\nUnfortunately this workaround does not work for any reason.","I can confirm that this is already possible.\r\nYou need to do something like this:\r\n```tf\r\nterraform {\r\n  backend \"azurerm\" {\r\n    use_azuread_auth = true\r\n    resource_group_name  = \"$($BackendResourceGroupName)\"\r\n    storage_account_name = \"$($BackendStorageAccountName)\"\r\n    container_name       = \"$($BackendContainerName)\"\r\n    key                  = \"$($BackendKeyName)\"\r\n  }\r\n}\r\n```\r\nAnd make sure to have your env settings set:\r\n```yaml\r\nenv:\r\n      ARM_TENANT_ID: $(ARM_TENANT_ID)\r\n      ARM_SUBSCRIPTION_ID: $(ARM_SUBSCRIPTION_ID)\r\n      ARM_CLIENT_ID: $(ARM_CLIENT_ID)\r\n      ARM_CLIENT_SECRET: $(ARM_CLIENT_SECRET)\r\n```\r\n\r\nI'd say you could close this issue. Good luck! :)","> I can confirm that this is already possible. You need to do something like this:\r\n> \r\n> ```terraform\r\n> terraform {\r\n>   backend \"azurerm\" {\r\n>     use_azuread_auth = true\r\n>     resource_group_name  = \"$($BackendResourceGroupName)\"\r\n>     storage_account_name = \"$($BackendStorageAccountName)\"\r\n>     container_name       = \"$($BackendContainerName)\"\r\n>     key                  = \"$($BackendKeyName)\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> And make sure to have your env settings set:\r\n> \r\n> ```yaml\r\n> env:\r\n>       ARM_TENANT_ID: $(ARM_TENANT_ID)\r\n>       ARM_SUBSCRIPTION_ID: $(ARM_SUBSCRIPTION_ID)\r\n>       ARM_CLIENT_ID: $(ARM_CLIENT_ID)\r\n>       ARM_CLIENT_SECRET: $(ARM_CLIENT_SECRET)\r\n> ```\r\n> \r\n> I'd say you could close this issue. Good luck! :)\r\n\r\n@devvox93 thanks a lot for testing and verifying.\r\nCan you please add some more details how you used the private link for the storage account (e.g. did you use the private link prefix as the account name )?\r\nThanks :)\r\n","@AndreasAugustin We just use the normal storage account name. I believe Azure (almost) always handles private link routing by itself, unless you have custom DNS resolution. Could that be an issue in your case?","> @AndreasAugustin We just use the normal storage account name. I believe Azure (almost) always handles private link routing by itself, unless you have custom DNS resolution. Could that be an issue in your case?\r\n\r\nthere are cases where the recursive DNS resolution from public to private link is not working (e.g. you only have on premise DNS forwarding in place for private links and not a recursive resolve enabled ).\r\nIn those cases it is not possible to resolve the public FQDN to the private link one and it would be super convenient to be able to configure the backend state to use the private link instead of the storage account name (like mentioned in the description).\r\n"],"labels":["enhancement","backend\/azure"]},{"title":"Module cache","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.2\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nCurrently whenever you use a remote module, terraform has to download it and put it in .terraform\/modules.\r\n\r\nIn cases where you have the same module used in many different workspaces, this results in wasting a lot of time and disk space downloading the same module over and over again.\r\n\r\n### Attempted Solutions\r\n\r\nI don't really know of any solutions besides just re-downloading the file every time. \r\n\r\nOr maybe doing some kind of pre-download and changing the source of the module to point to the local file. But that doesn't work very well if you are using terraform cloud or enterprise to run terraform.\r\n\r\n### Proposal\r\n\r\nHave a cache for modules similar to the [provider cache](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/config-file#provider-plugin-cache). \r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Thanks for suggesting this, @tmccombs!\r\n\r\nI think we already have an issue somewhere that overlaps with this request, but I wasn't able to find it in quick searching.\r\n\r\nThe main thing I think of when I imagine doing this is the classic problem that today there are lots of modules in the world that modify their own source directory while doing their work, such as by using `provisioner \"local-exec\"` and redirecting output to disk, or using the badly-behaved `archive_file` data source from `hashicorp\/archive` that does side-effects during the planning phase.\r\n\r\nAs long as that remains true I don't think we could implement something _exactly_ like the provider plugin cache, because self-modifying modules would corrupt the cache when they run. The closest we could get is to deep-copy the module trees from the cache during `terraform init` and thus at least avoid retrieving them over the network, similar to what `terraform init` already does when it can detect that more than one `module` block in the configuration refers to the same module package.\r\n\r\n---\r\n\r\nWe are planning to use the relatively clean slate that Terraform Stacks implies to impose a small number of new constraints that enable implementing some long-wanted features, and one of them is that modules are required to treat their source directories as read-only. Modules that self-modify won't work under Stacks until they are updated to use a different strategy.\r\n\r\nAs long as Terraform Stacks is only accessible through Terraform Cloud for private preview this doesn't really help anything because Terraform Cloud needs to retrieve the source code over the network each round anyway, but maybe this is something to keep in mind for the eventual CLI-driven Stacks workflow, so that whatever ends up being the equivalent to `terraform init` for a stack configuration can support optionally installing through a shared cache directory.\r\n\r\nIf we do that, I expect it would really mean adding cache directory support to [`package sourcebundle`](https:\/\/pkg.go.dev\/github.com\/hashicorp\/go-slug@v0.14.0\/sourcebundle), since that's the mechanism responsible for building the dependency bundle for a stack configuration.\r\n\r\n","@apparentlymart The issue you had in mind might be #16268. \r\n\r\n> The closest we could get is to deep-copy the module trees from the cache during terraform init and thus at least avoid retrieving them over the network, similar to what terraform init already does when it can detect that more than one module block in the configuration refers to the same module package.\r\n\r\nThat would be a major improvement for larger codebases. The local copying is suboptimal, but nothing compared to the cost of redownloading the packages from the network every time, especially on CI. It would also enable the use of GitHub Actions caching for modules.\r\n\r\nI started to look into this and I think this could be done just at the level of the FetchPackage (https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/getmodules\/installer.go#L45), i.e. not touching any of the module resolution logic, only cache go-getter results (and only for remote packages). This wouldn't be 100% offline yet, but caching the resolution in a lock file for modules could be done as the next step. \r\n\r\nDoes that sound like a viable approach?\r\n\r\n"],"labels":["enhancement","new"]},{"title":"Support `locals` block inside `tftest.hcl` Terraform Test file.","body":"### Terraform Version\r\n\r\n```shell\r\nterraform version\r\nTerraform v1.7.2\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWe don't want to repeat the same template string inside `tftest.hcl` Terraform Test file.\r\n\r\nPlease support `locals` block inside `tftest.hcl` Terraform Test file.\r\n\r\n### Attempted Solutions\r\n\r\n```terraform\r\nlocals {\r\n  variable_name = \"${run.setup_tests.variable_value_one}_${run.setup_tests.variable_value_two}\"\r\n}\r\n```\r\n\r\n```console\r\n$ terraform init -backend=false && terraform test --verbose --verbose\r\nInitializing modules...\r\n\u2577\r\n\u2502 Error: Unsupported block type\r\n\u2502 \r\n\u2502   on tests\/main.tftest.hcl line 13:\r\n\u2502   13: locals {\r\n\u2502 \r\n\u2502 Blocks of type \"locals\" are not expected here.\r\n```\r\n\r\n### Proposal\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Let failed validation checks result in warnings (variable validation and lifecycle validation)","body":"### Terraform Version\n\n```shell\n1.7.2\n```\n\n\n### Use Cases\n\nWe maintain internal Terraform modules that allow specific configurations that should not be used by default and only in specific cases.\r\n\r\nSay we have a module that allows to deploy confidential computing VMs, but this should only be used if actually required.\r\n\r\nWe would like to emit a warning to the user, if such a VM type is deployed.\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\nExtend the validation rule with a new property `error_type`:\r\n\r\n`error` (default) will return an error to the caller.\r\n`warning` will return a warning to the caller.\r\n\r\nExample:\r\n\r\n```terraform\r\nvariable \"sku\" {\r\n  type        = string\r\n\r\n  validation {\r\n    condition = strcontains(var.sku, \"confidential\")\r\n    error_message = \"Please only use confidential VMs if required. Please refer to https:\/\/somedocs\/info.\"\r\n    error_type = \"warning\"\r\n  }\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @tiwood, you can approximate something like this already with the `check` block construct. Check blocks can refer to other resources, and will only produce warnings. Might be helpful for you in the absence of the requested feature.\r\n\r\n```hcl\r\nvariable \"sku\" {\r\n  type = string\r\n}\r\n\r\ncheck \"variable_sku\" {\r\n  assert {\r\n    condition = strcontains(var.sku, \"confidential\")\r\n    error_message = \"Please only use confidential VMs if required. Please refer to https:\/\/somedocs\/info.\"\r\n  }\r\n}\r\n``` "],"labels":["enhancement","new"]},{"title":"Support the `self` object in `lifecycle.precondition` for advanced input validation","body":"### Terraform Version\r\n\r\n```shell\r\n1.7.2\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nThe `self` object is currently only available for postcondition checks.\r\n\r\nWe currently have use cases, where we create configurations based on input YAML files. Our goal is to inform the maintainers during `terraform plan` if the YAML file contains invalid inputs\/configurations. This could be done using `lifecycle.precondition` if the self object was available.\r\n\r\nThis is really similar then using input validation for variables, but this is not usable for our use case, as the input data comes from various sources (APIs, data stores..).\r\n\r\n### Attempted Solutions\r\n\r\nN\/A\r\n\r\n### Proposal\r\n\r\nAllow the use of the [self](https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/custom-conditions#self-object) object in `lifecycle.precondition`.\r\n\r\n```terraform\r\nresource \"terraform_data\" \"this\" {\r\n  input = {\r\n    foo = \"bar\"\r\n  }\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = self.input.foo != \"foo\"\r\n      error_message = \"Snap! foo should be 'foo'.\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @tiwood,\r\n\r\nThe main reason this isn't currently allowed is that `self` refers to the result the provider returned from the request to plan changes for this object, and that request is what the precondition is intended to guard, so that value isn't available yet.\r\n\r\nI guess from your example that you are imagining `self` representing something different in this context, which (with Terraform as it currently works) could in principle be either one of the following:\r\n- The same object that Terraform Core would send to the provider as \"configuration\", in which only the values given directly in the configuration are populated and everything else is `null`.\r\n- A special object that Terraform Core constructs as what we call the \"proposed new state\", which is essentially an example of something the provider _could_ return, but in practice most providers ignore this and just construct their own \"planned new state\" (which is then what `self` represents in a postcondition).\r\n\r\nI think the first of these is more likely to be useful because there's no guarantee that the proposed new state will match the planned new state, and so the latter could cause errant failures by checking something that Terraform would not otherwise be using.\r\n\r\nHowever, it would also represent the first example of the raw \"configuration\" object being exposed into the language, rather than just being used as part of the provider protocol. It would mean another new concept for module authors to learn the quirks of, and would add a new meaning of `self` that's inconsistent with what it represents everywhere else.\r\n\r\nI think it would also require changing the order of operations so that configuration evaluation happens before preconditions, meaning that any errors in the configuration would mask the precondition checks and that would be a breaking change for anyone who is relying on a precondition to return a better error message than Terraform would naturally generate.\r\n\r\nWith all that said then: while the use-case here does seem reasonable, it isn't clear to me that we could do exactly what you proposed, and might instead need to search for a different compromise that fits better with the current Terraform language design.\r\n\r\nThanks!"],"labels":["enhancement","waiting-response","new"]},{"title":"Terraform Test: Add ability to run custom teardown scripts","body":"### Terraform Version\n\n```shell\n1.7.2\n```\n\n\n### Use Cases\n\nWhen running tests, sometimes Terraform does not clean up resources properly and some stray objects remain.\r\n\r\nFor example, cleaning up an `aws_kms_key` and `aws_kms_key_alias` sometimes isn't successful if some of the run blocks failed. In order to prevent name clashes with sub-sequent tests, we must clean up at least the alias resource.\r\n\n\n### Attempted Solutions\n\nAdding a reusable sub-module which creates a `null_resource` with a destroy-time provisioner works, but it feels very hacky:\r\n\r\nFor one, it must be placed at the top of the test file (in order to be run last during tear down). Secondly, it does not prevent terraform from reporting that some resources were not destroyed, despite having been cleaned up by the destroy-time provisioner script.\r\n\r\n```terraform\r\n# testcaste.tftest.hcl\r\nrun \"cleanup\" {\r\n  source = \".\/modules\/cleanup\"\r\n}\r\n\r\n[...] \/\/ Additional Run blocks\r\n```\r\n\r\n```terraform\r\n# modules\/cleanup\/main.tf\r\nresource \"null_resource\" \"cleanup_scripts\" {\r\n\r\n  provisioner \"local-exec\" {\r\n    when = destroy\r\n    command = \"bash clean-up-aws-resources.sh\"\r\n  }\r\n}\r\n```\r\n\n\n### Proposal\n\nimplementing a `cleanup` block, or a `post-run` lifecycle block (or similar) to allow running custom commands \/ scripts *after* the teardown step, would make this maintenance easier:\r\n\r\n```\r\nrun \"test\" {\r\n\r\n... }\r\n\r\ncleanup {\r\n\r\n  command \"destroy_aws_kms_alias\" {\r\n     command = \"aws kms delete-alias --alias-name ${aws_kms_alias.my_alias.arn}\"\r\n  }\r\n\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","@deepbrook the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me oismail@hashicorp.com and we can schedule a time to chat!"],"labels":["enhancement","new","terraform test"]},{"title":"Missing information on how to override backend endpoints from command line","body":"### Terraform Version\n\n```shell\nTerraform v1.7.1\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.20.1\r\n+ provider registry.terraform.io\/hashicorp\/kubernetes v2.23.0\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.7.2. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/s3#endpoint\n\n### What is the docs issue?\n\nMissing information on how to override endpoints.s3 and similar using command line arguments\r\n\r\nI tried this:\r\n```\r\nterraform init \\   \r\n  -backend-config=\"endpoints.s3=http:\/\/127.0.0.1:9000\" \\                 \r\n  -input=false -reconfigure\r\n\r\n\r\nInitializing the backend...\r\nInitializing modules...\r\n\u2577\r\n\u2502 Error: Invalid backend configuration argument\r\n\u2502 \r\n\u2502 The backend configuration argument \"endpoints.s3\" given on the command line is not expected for the selected\r\n\u2502 backend type.\r\n\u2575\r\n```\r\n\r\nbut wasn't successful. The deprecated parameter \"endpoint\" can't be used as it's not possible to use it together with already existing endpoints.s3\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["The correct way to specify the override is using this parameter:\r\n```\r\n-backend-config='endpoints={s3=\"http:\/\/127.0.0.1:9000\"}'\r\n```","Thanks for reporting this, @oto-macenauer-absa!\r\n\r\nIn case it's useful to someone who works on this issue in future, I'll note that the documentation about \"overriding\" parts of the backend configuration is in [Partial Configuration](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/configuration#partial-configuration), because that was the terminology we originally used when first introducing the idea that all arguments in a `backend` block are optional as long as they are specified in one of these other ways. I'm not sure that terminology really resonates with how people think of that feature. For example, this issue reasonably uses the word \"override\" to describe it.\r\n\r\nWhen we originally wrote the documentation there weren't yet examples of backend configuration arguments that were of collection or structural types, so [the section on command-line overrides](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/configuration#command-line-key-value-pairs) only describes how to set primitive-typed arguments, and doesn't say anything about how to assign complex-typed values.\r\n\r\nIn practice the syntax here is the same as [setting input variables on the command line](https:\/\/developer.hashicorp.com\/terraform\/language\/values\/variables#variables-on-the-command-line), which _does_ include some examples of how to assign to a list-typed or map-typed input variable. Maybe we can find some way to unify these somewhere so that it's clearer that the patterns are the same.\r\n\r\nFinally, I notice that [the S3 backend's Configuration section](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/s3#configuration) mentions `-backend-config` in passing but doesn't prominently link to [Using a Backend Block](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/configuration#using-a-backend-block) to connect with the general information about how backend configuration works. I guess this was on the assumption that readers would find the general documentation first and only then read the backend-type-specific pages, but of course folks are just as likely to end up on the S3 backend page first, if e.g. they already knew that was what they needed to use from reading something elsewhere on the internet.\r\n","Hi, once again, thanks for the answer @apparentlymart!\r\n\r\nI must admit I have skipped the [setting input variables on the command line](https:\/\/developer.hashicorp.com\/terraform\/language\/values\/variables#variables-on-the-command-line). We're using var files to provide variables to terraform so that's why I didn't get familiar with the command line arguments. That's why I thought it would be useful to put information also to the backend configuration.\r\n\r\nOn the other hand this pointed me out to tfbackend files for configuration, so it might be better idea to switch to those.\r\n\r\nAs for the terminology, the term \"overrides\" is used by helm documentation so that's why it may have stuck in my mind. \r\n\r\nThank you for the explanation. "],"labels":["documentation","new"]},{"title":"yamldecode fails on strings of only + (upstream go-cty-yaml issue)","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nlocals {\r\n  yaml_inline_sample_error = <<-EOT\r\n          element: +\r\n          some_list:\r\n          - foo\r\n          - +\r\n        EOT\r\n}\r\n\r\noutput \"yaml_decode_test\" {\r\n  value = yamldecode(local.yaml_inline_sample_error)\r\n}\r\n```\r\n\n\n### Debug Output\n\nTrace omitted. Shows nothing related to yamldecode and issue is in upstream library.\n\n### Expected Behavior\n\nString `+` decoded as string\n\n### Actual Behavior\n\nError:\r\n```\r\n\u2502 Error: Error in function call\r\n\u2502\r\n\u2502   on main.tf line 45, in output \"yaml_decode_test\":\r\n\u2502   45:   value = yamldecode(local.yaml_inline_sample_error)\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 while calling yamldecode(src)\r\n\u2502     \u2502 local.yaml_inline_sample_error is \"element: +\\nsome_list:\\n- foo\\n- +\\n\"\r\n\u2502\r\n\u2502 Call to function \"yamldecode\" failed: cannot parse \"+\" as tag:yaml.org,2002:int.\r\n```\r\n\n\n### Steps to Reproduce\n\n```sh\r\nterraform init\r\nterraform plan\r\n```\r\n\n\n### Additional Context\n\n_No response_\n\n### References\n\nIssue is in https:\/\/github.com\/zclconf\/go-cty-yaml\/issues\/13\r\n\r\nOnce fixed upstream, pacakage should be bumped.\r\n\r\n(Not sure if there's automation bumping any pacakage, therefore created this bug issue)","comments":[],"labels":["bug","new"]},{"title":"When using 'removed' block, terraform plan should return removed: n in the summary.","body":"### Terraform Version\n\n```shell\n1.7.2\n```\n\n\n### Use Cases\n\nCurrent plan message outputs that TF has no actions to perform.\r\n\r\n{\"@level\":\"info\",\"@message\":\"Plan: 0 to add, 0 to change, 0 to destroy.\",\"@module\":\"terraform.ui\",\"@timestamp\":\"2024-02-02T11:16:28.954581Z\",\"changes\":{\"add\":0,\"change\":0,\"import\":0,\"remove\":0,\"operation\":\"plan\"},\"type\":\"change_summary\"}\r\n\r\nEven though once `terraform apply` is run - there is a change to the state.\r\n\r\n{\"@level\":\"info\",\"@message\":\"module.prodcctn-tgw-attachment.aws_ec2_transit_gateway_vpc_attachment.tgw_attachment: Plan to remove\",\"@module\":\"terraform.ui\",\"@timestamp\":\"2024-02-02T11:16:28.954490Z\",\"change\":{\"resource\":{\"addr\":\"module.xxyyzz.aws_ec2_transit_gateway_vpc_attachment.tgw_attachment\",\"module\":\"module.xxyyzz\",\"resource\":\"aws_ec2_transit_gateway_vpc_attachment.tgw_attachment\",\"implied_provider\":\"aws\",\"resource_type\":\"aws_ec2_transit_gateway_vpc_attachment\",\"resource_name\":\"tgw_attachment\",\"resource_key\":null},\"action\":\"remove\",\"reason\":\"delete_because_no_resource_config\"},\"type\":\"planned_change\"}\r\n\r\nIf using `Plan: 0 to add, 0 to change, 0 to destroy.` as validation of whether to run TF apply or no - this will always fail.\n\n### Attempted Solutions\n\nAlternative solution (more a workaround really), is to grab each line of json, and scan for `\"type\":\"planned_change\"` and use that as a metric for 'Actions taken if we were to apply', but it seems ugly, especially as an import gets added to summary if something is imported.\r\n\r\nThe fact that import is also no in the summary consistently irkes me, but in fairness that is another issue.\n\n### Proposal\n\nChange the summary output to be consistent.\r\n\r\nPlan: 0 to add, 0 to change, 0 to destroy, 0 to remove, 0 to import\n\n### References\n\n_No response_","comments":["Hi @deitChi,\r\n\r\nThanks for reporting this.\r\n\r\nTo start, I do agree that it would be useful to include the summary counts for other kinds of action in the JSON output, and ideally also in the human-readable output, but the latter is harder because various parties have (despite our warnings to the contrary) built integrations that regex-match that line and so it's become a de-facto compatibility constraint that we cannot practically change.\r\n\r\nHowever, I think your report here betrays another more fundamental need: there should be a single, unambiguous signal somewhere in the Terraform JSON output for whether a plan contains at least one action that would make it useful to apply, and then automation built around Terraform should rely on that signal exclusively, rather than trying to make its own guesses based on other details.\r\n\r\nOne possible design for that would be an extra boolean property inside the existing `change_summary` object:\r\n\r\n```json\r\n{\"changes\":{\"add\":0,\"change\":0,\"import\":0,\"remove\":0,\"operation\":\"plan\",\"applyable\":true},\"type\":\"change_summary\"}\r\n```\r\n\r\nIt might also\/instead make sense to include this flag in the `terraform show -json PLANFILE` output.\r\n\r\nOver time there will be even more different kinds of actions that can appear in a plan, and we can arrange for the \"plan should be applied\" flag to be true in those cases, so that software which relies on it will _just work_ without needing to be modified for each new action type.\r\n\r\nWe already started thinking about such a thing as part of the ongoing Terraform Stacks project, because the new stacks language also needs a way to know if a particular component's plan includes actionable changes or not, but Terraform Stacks does not integrate through the Terraform CLI layer, and so this issue could potentially represent the work to also expose the new flag as part of the traditional `terraform plan` command and its associated JSON formats.\r\n\r\nThat doesn't mean that we couldn't\/shouldn't also expose the number of items that are being \"forgotten\" (deleted from the state without actually destroying them), but the reason to do that should _not_ be to allow surrounding automation to decide whether a plan needs to be applied, because that approach would not be robust to future Terraform features.\r\n","Thank you for your detailed reply and consideration.\n\nIt's worth noting here that actually the `-detailed-exitcode` cli parameter did function as expected, and luckily was only a minor adjustment for our integration.\n\nIn order to emulate this functionality previously, I used the `terraform state rm -dry-run` to cycle a `.staterm` file, which appended a staterm count to the summary output ( to help our engineers distinguish between destroyed and state removed resources by count)\n\nThis can still be technically done using some breakout json processing, but I'd rather hoped that the removed block would already handle this.\n\nAn action of intent ( to apply or not) would help here, and if the intent dict could be expanded to include staterm alongside removed count, just to account for teams wishing to add validation actions ( I build github comments with dropdowns for actions, for approval review), dynamically based on the summary json, which matches actions from planned resources."],"labels":["enhancement","new","removed"]},{"title":"`yamldecode` converts certain field names to bool","body":"### Terraform Version\n\n```shell\n1.7.2\n```\n\n\n### Terraform Configuration Files\n\nFiles not needed - running in `terraform console` instead:\r\n\r\n```terraform\r\n> yamldecode(\"{on: 1}\")\r\n{\r\n  \"true\" = 1\r\n}\r\n>\r\n```\r\n\n\n### Debug Output\n\nN\/A\n\n### Expected Behavior\n\n```terraform\r\n> yamldecode(\"{on: 1}\")\r\n{\r\n  \"on\" = 1  \/\/ <-- field name should remain unchanged\r\n}\n\n### Actual Behavior\n\nIt appears that in addition to converting *values* to bools, field names are also affected. This results in unexpected behaviour when, for example, parsing a github actions workflow file (which uses `on` to declare workflow exectuion conditions\/options). \n\n### Steps to Reproduce\n\n1. `terraform console`\r\n2.  `yamldecode(\"{on: 1}\")`\r\n\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @deepbrook, thanks for filing this. \r\n\r\nThis is an issue with our upstream type library, tracked here: https:\/\/github.com\/zclconf\/go-cty-yaml\/issues\/11.\r\n\r\nIt seems like this behaviour is expected in YAML 1.1: https:\/\/yaml.org\/type\/bool.html. You can see other YAML libraries have the same kind of thing filed, for example in PyYAML: https:\/\/github.com\/yaml\/pyyaml\/issues\/613.\r\n\r\nBut, the [docs](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/yamlencode) do say we support YAML 1.2 so I do think this an incorrect modification being made by the library as long as we are claiming YAML 1.2. Unfortunately we are constrained by the upstream library here, so will have to see how practical a fix in that library is.","@liamcervante I'm not sure if the compatibility promises outlined in https:\/\/github.com\/hashicorp\/terraform\/issues\/34301#issuecomment-1828266948 are also applicable in this case as well. Hopefully with provider-defined functions, someone can create YAML 1.2 specific functions without these little quirks.","In practice `yamldecode` supports a similar pragmatic blend of YAML v1.2 and allowances for older versions, because in practice existing YAML in the world tends not to be strictly written for one version, possibly in part because the differences between different YAML versions are widely misunderstood.\r\n\r\nI think the only change we can possibly make here is to update the documentation to be clearer about what YAML versions this function actually supports. I think when I wrote this doc I intended to mean that it supports \"v1.2 _and earlier_\", which was clumsy on my part because most other formats we've supported haven't made such significant breaking changes over time.\r\n\r\nWe cannot change the current behavior of `yamldecode` because, as @bflad suggested, it's protected by the Terraform v1.x compatibility promises. He's also right that someone could potentially make a \"YAML v1.2\" provider that offers a stricter `yamldecode`, if desired, once that capability is supported in the forthcoming Terraform v1.8 series.\r\n\r\nHowever, if you want to keep using the built-in `yamldecode` then it's best to write ambiguous strings in quotes -- either double or single, since YAML supports both -- to remove the ambiguity:\r\n\r\n```\r\n> yamldecode(\"{'on': 1}\")\r\n{\r\n  \"on\" = 1\r\n}\r\n```\r\n\r\n\r\n","Just for the record, i do not believe this has anything to do with the 1.1 spec, but rather with the linked bug in the city lib.\n\nThe spec, afaiu, only states that values are converted to booleans if they match the pattern. Not the keys.\n","As far as I understand the YAML 1.1 spec, it's built around three fundamental structural primitives: scalars, sequences, and mappings, with two different syntaxes defined for each (flow style and block style). Collectively, these three concepts are described as [nodes](https:\/\/yaml.org\/spec\/1.1\/#id861435). The sequence and mapping node types are aggregates of other nodes.\r\n\r\nThe rule about sequences like `yes`, `no`, etc being interpreted as booleans is a semantic rule that the spec describes as being applied to any scalars, in [Scalar Formats](https:\/\/yaml.org\/spec\/1.1\/#id864510).\r\n\r\nI can't find any language in the spec that suggests that \"mapping key\" is a distinct concept from \"node\" and describes different treatment of them, but the spec is very dense and not easy to read only in portions, and it's been a long time since I read the whole thing from top to bottom, so I'm happy to be corrected if you can cite a part of the spec that explicitly says that scalar nodes in a mapping-key position are supposed to get different parsing treatment than scalar nodes in other contexts.\r\n\r\n(There _are_ some special rules about \"simple keys\" in [Block Mappings](https:\/\/yaml.org\/spec\/1.1\/#id934537), but the exceptions there seem to describe only differences in treatment of delimiters adjacent to the scalar and what characters are allowed to appear in the scalar's representation; I don't see any requirement that the scalars in that position should be interpreted into typed values differently once parsing is complete.)\r\n","Here is a workaround I am using, which only works if a) only YAML 1.1 is required and b) other you are interoperating with more completely support the spec. In my case, this works fine when reading and writing the yaml files using Python's `ruamel.yaml`s `rt` (round trip) encoder:\r\n\r\n```terrraform\r\nlocals {\r\n   doc = <<EOT\r\n%YAML 1.1\r\n---\r\nroot:\r\n  notaboolvalue: 'no'\r\n  'no': no\r\nEOT\r\n   val1 = yamldecode(trimprefix(local.doc, \"%YAML 1.1\\n\"))[\"root\"][\"no\"]\r\n   val2 = yamldecode(trimprefix(local.doc, \"%YAML 1.1\\n\"))[\"root\"][\"notaboolvalue\"]\r\n}\r\n\r\noutput \"notaboolkey\" {\r\n   value = local.val1\r\n}\r\n\r\noutput \"notaboolvalue\" {\r\n   value = local.val2\r\n}\r\n```"],"labels":["bug","upstream","confirmed","cty"]},{"title":"Terraform fmt tfvars - split long line array into multiple lines","body":"### Terraform Version\n\n```shell\n% terraform version\r\nTerraform v1.7.2\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nSuppose we have a tfvars file with the following content, where a variable is a array of a type and is a long line\r\n```\r\n% cat example.tfvars \r\nvar1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\r\n```\r\n\r\nRunning `terraform fmt` on this should ideally split it into multiple lines like below for readability\r\n```\r\n% cat formatted.tfvars \r\nvar1 = [\r\n  1,\r\n  2,\r\n  3,\r\n  4,\r\n  5,\r\n  6,\r\n  7,\r\n  8,\r\n  9,\r\n  10,\r\n  11,\r\n  12,\r\n  13,\r\n  14,\r\n  15,\r\n  16,\r\n  17,\r\n  18,\r\n  19,\r\n  20,\r\n  21,\r\n  22,\r\n  23,\r\n  24,\r\n  25,\r\n  26,\r\n  27,\r\n  28,\r\n  29,\r\n  30,\r\n  31,\r\n  32,\r\n  33,\r\n  34,\r\n  35,\r\n  36,\r\n  37,\r\n  38,\r\n  39,\r\n  40\r\n]\r\n```\r\nBoth versions are properly formatted as per `terraform fmt -check`\r\n\r\nThe point at which this is split can be decided based on line length or number of array items\r\n\r\nSplitting into multiple lines helps with readability and easily eyeing diff in scm when objects are removed or added to the list\n\n### Attempted Solutions\n\nTried modifying the tfvars files by introducing a new line in the middle of array to see if it splits all others into multiple lines, but it didn't\n\n### Proposal\n\n_No response_\n\n### References\n\n- #22703 A more generic version of this","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nTo set expectations, this is unlikely to be changed without a big re-think of the purpose of `fmt` - for all intents and purposes, the `fmt` command is \"working as designed\" right now. However, it is useful for us to see the level of support for issues such as this when contemplating any future changes. \r\n\r\nThanks again!"],"labels":["enhancement","new","fmt"]},{"title":"Support `prefix` configuration in s3 backend","body":"### Terraform Version\n\n```shell\n1.7.0\n```\n\n\n### Use Cases\n\nAllow for simplified configuration of S3 backend when a prefix per project is wanted.\n\n### Attempted Solutions\n\nCurrently the only way to have the same prefix for both the default state and all non-default environments is to specify it twice:\r\n\r\n```\r\nbucket = \"my-bucket\"\r\nkey = \"foo\/terraform.tfstate\"\r\nworkspace_key_prefix = \"foo\/env:\"\r\n```\r\n\r\nThis is an easy opportunity for mistakes to happen, particularly since variables\/locals are not allowed in the backend configuration.\n\n### Proposal\n\nAllow for a `prefix` configuration:\r\n\r\n```\r\nbucket = \"my-bucket\"\r\nprefix = \"foo\/\"\r\nkey = \"terraform.tfstate\"\r\n```\r\n\r\nThis would be equivalent to the currently working example and result in the default state to reside at `foo\/terraform.tfstate` and any non-default state to reside at `foo\/env:\/<workspace>\/terraform.tfstate`.\n\n### References\n\n_No response_","comments":["Thanks for suggesting this, @vincer.\r\n\r\nIn principle we could use the addition of this new option as an opportunity to simplify the design further, on an opt-in basis, by removing the special treatment of the \"default\" workspace that is really there only because this backend was around before Terraform had support for a backend offering multiple workspaces.\r\n\r\nSpecifically, I mean that if and only if `prefix` is set:\r\n- `key` and `workspace_key_prefix` are both forbidden. That is, an author must choose to either use `prefix` _or_ the `key`\/`workspace_key_prefix` pair, not both at the same time.\r\n- When `prefix` is provided, it must end with a slash.\r\n- When `prefix` is provided, the full object key for the state of a particular workspace is `prefix + workspaceName + \".tfstate\"`, regardless of the workspace name.\r\n- When `prefix` is provided, the operation of listing all of the existing workspaces is implemented as finding all of the keys with the specified prefix, ignoring any whose full path does not end with `.tfstate`.\r\n\r\nThis would make the layout in the S3 bucket far more uniform than the current approach: `default` is no longer a special case, there's no need for the weird `env:` portion to distinguish non-default workspaces, and because this follows a more conventional layout for S3 bucket objects the set of workspaces for a particular configuration will appear as a \"folder\" in the S3 console:\r\n\r\n* `prefix\/default.tfstate`\r\n* `prefix\/other.tfstate`\r\n\r\nUltimately the design of the S3 backend is up to the AWS provider team, which I'm not a member of, so I mean the above only as a slight variation on the design as proposed which the AWS provider team might consider when deciding what to do with this proposal. However, I'm curious if you (@vincer) think that this variation would work to meet your goals, or if I've missed supporting something important that your proposal would have supported.\r\n\r\n","Hi @apparentlymart !\r\n\r\nYes, that sounds even better to me. In my proposal, I was trying to keep things as backwards compatible as possible while fixing my main nit (the lack of DRYness in the config), but simplifying the S3-side structure is a nice improvement as well IMO.\r\n\r\nLogistically, does this request stay here or does it somehow get moved to AWS provider's issues?","Thanks @vincer!\r\n\r\nIf we did choose to do something like what I proposed, I think a part of it would be to make sure that there's some reasonable way to migrate from the old way to the new way without losing data. I'm not sure if that's actually possible with today's backend design -- it tends to assume that the old and new backends are separate when migrating, rather than having only a subset of the workspaces move physical storage -- and so it might well be that it turns out to be infeasible to do what I proposed, but we'll see.\r\n\r\nAlthough the AWS provider team are the _people_ who would probably work on this, the change would still need to be in this codebase and so this is the best place for this issue to sit. The `backend\/s3` label is there to make it more visible to the folks on that team.\r\n\r\n"],"labels":["enhancement","backend\/s3"]},{"title":"Plan storage backends","body":"### Terraform Version\n\n```shell\n1.6.5\n```\n\n\n### Use Cases\n\nWhen running Terraform in automation, where plan and apply are run on two different machines (PR -> Merge workflow in Github for example), Hashicorp recommend archiving the plan together with the entire working directory, and staging that in remote storage: https:\/\/developer.hashicorp.com\/terraform\/tutorials\/automation\/automate-terraform?product_intent=terraform#plan-and-apply-on-different-machines\r\nHowever, there are no native tools available in Terraform to do this, even though, the challenge is very similar to backend state storage.\r\nThe current approach, if doing this in generic CI\/CD such as Github Actions is to write a significant amount of custom scripts to handle plan storage.\n\n### Attempted Solutions\n\nN\/A. No solution is available in Terraform natively.\n\n### Proposal\n\nI propose a couple of new constructs in Terraform, that are not too dissimilar to \"backends\".\r\n\r\n```hcl\r\nterraform {\r\n  plan_backend \"s3\" {\r\n    bucket         = \"mybucket\"\r\n    key_prefix     = \"path\/to\/my\/key\"\r\n    region         = \"us-east-1\"\r\n    dynamodb_table = \"TableName\"\r\n  }\r\n}\r\n```\r\n\r\n```sh\r\nterraform plan upload --id <id>\r\nterraform plan download --id <id>\r\n```\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"User Managed Identity connection didn't work into azure devops with AzureCLI@2","body":"### Community Note\r\n\r\n<!--- Please keep this note for the community --->\r\n\r\n* Please vote on this issue by adding a :thumbsup: [reaction](https:\/\/blog.github.com\/2016-03-10-add-reactions-to-pull-requests-issues-and-comments\/) to the original issue to help the community and maintainers prioritize this request\r\n* Please do not leave \"+1\" or \"me too\" comments, they generate extra noise for issue followers and do not help prioritize the request\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment and review the [contribution guide](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/blob\/main\/contributing\/README.md) to help.\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\r\n\r\n### Terraform Version\r\n\r\n1.6.6\r\n\r\n### AzureRM Provider Version\r\n\r\n3.85.0\r\n\r\n### Affected Resource(s)\/Data Source(s)\r\n\r\nbackend_state\r\n\r\n### Terraform Configuration Files\r\n\r\n```hcl\r\n- task: AzureCLI@2\r\n    displayName: \"\ud83d\udd26 Plan Terraform\"\r\n    inputs:\r\n      connectedServiceNameARM: '${{ parameters.AZURE_SERVICE_CONNECTION_NAME }}'\r\n      addSpnToEnvironment: true\r\n      scriptType: 'bash'\r\n      scriptLocation: 'inlineScript'\r\n      failOnStandardError: true\r\n      workingDirectory: '${{ parameters.WORKINGDIR }}'\r\n      inlineScript: |\r\n        echo \"##[section] \ud83d\udc88 Start terraform plan for tf-env-folder=${{ parameters.TF_ENVIRONMENT_FOLDER }}\"\r\n\r\n        export cliendId=$(az ad sp show --id ${servicePrincipalId} --query \"appId\" -o tsv) \r\n        echo \"servicePrincipalId: ${servicePrincipalId}\"\r\n        echo \"cliendId: ${cliendId}\"\r\n        \r\n        export ARM_USE_MSI=true\r\n        export ARM_CLIENT_ID=${cliendId}\r\n        export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv)\r\n        export ARM_TENANT_ID=$(az account show --query tenantId --output tsv)\r\n        \r\n        generate_state_output=\"\"\r\n\r\n        if [[ \"${{ parameters.TF_SUMMARIZE }}\" == \"True\" ]]; then\r\n          echo \"[info] Added option to generate tfplan\"\r\n          generate_state_output=\"-out=tfplan\"\r\n        else\r\n          echo \"\ud83d\udce2 Summary not enabled: ${{ parameters.TF_SUMMARIZE }}\"\r\n        fi\r\n\r\n        if [[ \"${{ parameters.AKS_NAME }}\" != \"\" ]]; then\r\n          echo \"[INFO] \ud83d\ude80 Run terraform plan + kubernetes\"\r\n          .\/terraform.sh plan ${{ parameters.TF_ENVIRONMENT_FOLDER }} -var k8s_kube_config_path_prefix=\"$(pwd)\" -lock-timeout=300s -lock=false ${generate_state_output}\r\n        else\r\n          echo \"[INFO] \ud83d\ude80 Run terraform plan\"\r\n          .\/terraform.sh plan ${{ parameters.TF_ENVIRONMENT_FOLDER }} -lock-timeout=300s -lock=false ${generate_state_output}\r\n        fi\r\n\r\n`\r\n\r\nterraform.sh do:\r\n1. az subscription set\r\n2. terraform init\r\n3. terraform apply\r\n```\r\n\r\n\r\n### Debug Output\/Panic Output\r\n\r\n```shell\r\n.\r\n```\r\n\r\n\r\n### Expected Behaviour\r\n\r\nThe terraform is able to load the backend configuration, and continue with terraform init\r\n\r\n### Actual Behaviour\r\n\r\n> the terraform is unable to load the backend config, and can't start terraform init\r\n\r\n\r\nError: Failed to get existing workspaces: Error retrieving keys for Storage Account \"pagopainfraterraformdev\": azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https:\/\/management.azure.com\/subscriptions\/xyz\/resourceGroups\/my-infra-rg\/providers\/Microsoft.Storage\/storageAccounts\/pagopainfraterraformdev\/listKeys?api-version=2021-01-01: StatusCode=400 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: {\"error\":\"invalid_request\",\"error_description\":\"Identity not found\"} Endpoint http:\/\/169.254.169.254\/metadata\/identity\/oauth2\/token?api-version=2018-02-01&resource=https%3A%2F%2Fmanagement.azure.com%2F\r\n\r\n\r\n\r\nError: Backend initialization required, please run \"terraform init\"\r\n\r\nReason: Initial configuration of the requested backend \"azurerm\"\r\n\r\nThe \"backend\" is the interface that Terraform uses to store state,\r\nperform operations, etc. If this message is showing up, it means that the\r\nTerraform configuration you're using is using a custom configuration for\r\nthe Terraform backend.\r\n\r\nChanges to backend configurations require reinitialization. This allows\r\nTerraform to set up the new configuration, copy existing state, etc. Please\r\nrun\r\n\"terraform init\" with either the \"-reconfigure\" or \"-migrate-state\" flags\r\nto\r\nuse the current configuration.\r\n\r\nIf the change reason above is incorrect, please verify your configuration\r\nhasn't changed and try again. At this point, no changes to your existing\r\nconfiguration or state have been made.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nuser managed identity:\r\n\r\n1. create a managed identity with `Contributor` permission on the subscription\r\n\r\n\r\nterraform:\r\n\r\n1. setup the backend into a storage account\r\n2. setup the env variables like this:\r\n  ```\r\n  export ARM_USE_MSI=true\r\n  export ARM_CLIENT_ID=${cliendId}\r\n  export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv)\r\n  export ARM_TENANT_ID=$(az account show --query tenantId --output tsv)\r\n  ```\r\n  that allow to load the managed identity\r\n3. run `terraform init`\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Important Factoids\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n* https:\/\/github.com\/hashicorp\/terraform\/issues\/27923\r\n* https:\/\/github.com\/arkiaconsulting\/terraform-provider-akc\/issues\/5\r\n\r\nThe project iac can be found here:\r\n\r\n* https:\/\/github.com\/pagopa\/pagopa-azure-devops\/tree\/iac-core-migration\/azure-devops\/iac","comments":["Thanks for taking the time to open this issue. Issues related to the Azure backend live in the [hashicorp\/terraform](https:\/\/github.com\/hashicorp\/terraform) repo, so I am going to move this issue there. ","**Azurerm service connections with federated managed identity** [manual](https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/pipelines\/library\/connect-to-azure?view=azure-devops#create-an-azure-resource-manager-service-connection-using-workload-identity-federation) create a user managed identity that federates with azdo via oidc (Configure an identity managed by an external OpenID Connect Provider to access Azure resources as this application)\r\n\r\nCurrently therefore you need to use this code snippet to allow the connection correctly\r\n\r\n```bash\r\n  - task: AzureCLI@2\r\n    displayName: \"\ud83d\udd26 Plan Terraform\"\r\n    inputs:\r\n      connectedServiceNameARM: '${{ parameters.AZURE_SERVICE_CONNECTION_NAME }}'\r\n      addSpnToEnvironment: true\r\n      scriptType: 'bash'\r\n      scriptLocation: 'inlineScript'\r\n      failOnStandardError: true\r\n      workingDirectory: '${{ parameters.WORKINGDIR }}'\r\n      inlineScript: |\r\n        echo \"##[section] \ud83d\udc88 Start terraform plan for tf-env-folder=${{ parameters.TF_ENVIRONMENT_FOLDER }}\"\r\n\r\n        export cliendId=$(az ad sp show --id ${servicePrincipalId} --query \"appId\" -o tsv)\r\n\r\n        echo \"[INFO] Terraform setup variables\"\r\n        export ARM_USE_OIDC=true\r\n        export ARM_OIDC_TOKEN=${idToken}\r\n        export ARM_CLIENT_ID=${cliendId}\r\n        export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv)\r\n        export ARM_TENANT_ID=$(az account show --query tenantId --output tsv)\r\n\r\n        generate_state_output=\"\"\r\n\r\n        if [[ \"${{ parameters.TF_SUMMARIZE }}\" == \"True\" ]]; then\r\n          echo \"[info] Added option to generate tfplan\"\r\n          generate_state_output=\"-out=tfplan\"\r\n        else\r\n          echo \"\ud83d\udce2 Summary not enabled: ${{ parameters.TF_SUMMARIZE }}\"\r\n        fi\r\n\r\n        if [[ \"${{ parameters.AKS_NAME }}\" != \"\" ]]; then\r\n          echo \"[INFO] \ud83d\ude80 Run terraform plan + kubernetes\"\r\n          .\/terraform.sh plan ${{ parameters.TF_ENVIRONMENT_FOLDER }} -var k8s_kube_config_path_prefix=\"$(pwd)\" -lock-timeout=300s -lock=false ${generate_state_output}\r\n        else\r\n          echo \"[INFO] \ud83d\ude80 Run terraform plan\"\r\n          .\/terraform.sh plan ${{ parameters.TF_ENVIRONMENT_FOLDER }} -lock-timeout=300s -lock=false ${generate_state_output}\r\n        fi\r\n\r\n``` "],"labels":["enhancement","backend\/azure","new"]},{"title":"Terraform Test: Allow Setting Terraform Workspaces at run block level","body":"### Terraform Version\n\n```shell\n1.7.0\n```\n\n\n### Use Cases\n\nOur configuration uses a single backend with workspaces, one workspace per team, to deploy github teams and related resources.\r\n\r\nRunning these deployments in the same workspace causes various errors.\n\n### Attempted Solutions\n\nUsing separate provider instances did not help, as it does not run `run` blocks on separate workspaces. \n\n### Proposal\n\nAllow setting the Terraform workspace at `run` block level.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hey @deepbrook, I'd love to learn more about your use cases and workflow. Do you mind emailing me oismail@hashicorp.com and we can chat live about this?","I'd be really keen to see this feature too. We make extensive use of the workspace variable for composing various different values for things like resource naming as well as some boolean logic. It would be really handy to be able to set this per run rather than running the whole suite of tests per workspace.","@m13t mind emailing me and we can find time to chat through and understand your use cases more? oismail@hashicorp.com"],"labels":["enhancement","new","terraform test"]},{"title":"Add support to override the input variables of a module when using terraform test.","body":"### Terraform Version\n\n```shell\n1.7.1\n```\n\n\n### Use Cases\n\nSometimes when using the mock_provider, some values used for for_each can only be known after apply. To get the test scenario working we can provide an override of the input value of that specific module\r\n\r\nAnother scenario is when the input value of a module cannot be just an auto generated value but must follow a regex structure (like a resource id in azurerm).\n\n### Attempted Solutions\n\n```hcl\r\noverride_module {\r\n  target = module.example.module.keyvaults[\"my-keyvault\"]\r\n  # Inputs would override the input values of a module\r\n  inputs = {\r\n   # Input variable access_policies set to {}. If that module iterate on  for_each=var.access_policies, the test will succeed as the value is now known.\r\n   access_policies = {}\r\n  }\r\n  outputs = {\r\n    id = \"\/subscriptions\/12345678-1234-9876-4563-123456789012\/resourceGroups\/example-resource-group\/providers\/Microsoft.KeyVault\/vaults\/vaultValue\"\r\n  }\r\n}\r\n``` \n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Terraform -target Does Not Throw Error on Non-existent Module Name","body":"### Terraform Version\n\n```shell\nv1.7.0\r\non darwin_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.33.0\n```\n\n\n### Terraform Configuration Files\n\nmodule\/ec2\/main.tf\r\n```\r\ndata \"aws_ami\" \"default\" {\r\n  most_recent = true\r\n}\r\n\r\nresource \"aws_instance\" \"default\" {\r\n    ami = data.aws_ami.default.id\r\n    instance_type = \"${var.instance-type}\"\r\n    tags = {\r\n      Name = \"terraform-target-${var.environment}-instance\"\r\n    }\r\n}\r\n```\r\n\r\nmodule\/ec2\/vars.tf\r\n```\r\nvariable \"environment\" {\r\n    type = string  \r\n}\r\nvariable \"instance-type\" {\r\n    type = string \r\n}\r\n```\r\n\r\nanother_module\/live\/ec2_main.tf\r\n```\r\nmodule \"ec2_live\" {\r\n  source = \"..\/..\/module\/ec2\"\r\n  \r\n  instance-type = \"t2.micro\"\r\n  environment = var.environment\r\n}\r\n```\r\n\r\nanother_module\/live\/vars.tf\r\n```\r\nvariable \"environment\" {\r\n  type = string\r\n}\r\n```\r\n\r\nanother_module\/stage\/main.tf\r\n```\r\nmodule \"ec2_stage\" {\r\n  source = \"..\/..\/module\/ec2\"\r\n  \r\n  instance-type = \"t2.micro\"\r\n  environment = var.environment\r\n}\r\n```\r\n\r\nanother_module\/stage\/main.tf\r\n```\r\nvariable \"environment\" {\r\n  type = string\r\n}\r\n```\r\n\r\nanother_module\/main.tf\r\n```\r\nprovider \"aws\" {\r\n  region = \"eu-west-1\"\r\n  profile = \"bayut-sa\"\r\n}\r\n\r\nmodule \"live\" {\r\n  source = \".\/live\"\r\n  environment = \"live\"\r\n}\r\nmodule \"stage\" {\r\n  source = \".\/stage\"\r\n  environment = \"stage\"\r\n}\r\n```\n\n### Debug Output\n\n[Terraform Plan Debug Output](https:\/\/gist.github.com\/MubashirUsman\/f0b546dda5e3240c871189b190ed92c6)\n\n### Expected Behavior\n\n\r\nIt should give an error that no module with that name is found.\n\n### Actual Behavior\n\nWhile planning from child modules, terraform ```-target ``` flag shows **No changes. Your infrastructure matches the configuration.** even when modules being targeted do not exist. In other words it does not give any error.\n\n### Steps to Reproduce\n\n1. `terraform init`\r\n2. `terraform plan -target=\"module.usman.module.ec2_stage\"`\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for reporting this, @MubashirUsman.\r\n\r\nUnfortunately, what you've described is the intended behavior of `-target` and has been true ever since that feature was implemented. It works by just deleting from Terraform's execution graph anything that doesn't match the `-target` argument, and so a `-target` that matches nothing just deletes _everything_ from the graph, making the plan operation into a no-op.\r\n\r\nGiven how long it has behaved this way, I expect it would be disruptive to change it to be an error now. However, what we could potentially do is change the \"No changes.\" message to use some different text when it appears in response to a plan request with `-target` options, to say something like:\r\n\r\n```\r\nNo changes. Either no changes are required, or your -target options\r\nhave excluded all of the changes from consideration.\r\n```\r\n\r\nThat would then at least give a direct hint that `-target` can cause this problem. Since `-target` is not intended for routine use anyway, I don't expect we'd be able to implement anything more substantial in this area.\r\n\r\nI'm going to relabel this as an enhancement because it's a request to change Terraform's design to meet a new requirement, rather than correcting Terraform's behavior to match its intended design. (What you described _was_ the intended design, so any change here would be a design change.)\r\n\r\nThanks again!\r\n"],"labels":["enhancement","cli"]},{"title":"fmt: Should format commas in multi-line maps\/objects","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.0\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nSometimes people write maps with commas between entries, sometimes people write maps with a final trailing comma, and sometimes people write maps with no comas. The `fmt` command should harmonize all variations to a single canonical form to allow undesired style difference to be caught in CI. The terraform docs use no comma, so it seems like a good choice.\r\n\r\n```tf\r\n{\r\n  name = \"John\",\r\n  age  = 52\r\n}\r\n```\r\n\r\n\r\n```tf\r\n{\r\n  name = \"John\",\r\n  age  = 52,\r\n}\r\n```\r\n\r\nBecome:\r\n\r\n```tf\r\n{\r\n  name = \"John\"\r\n  age  = 52\r\n}\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nDocument a style preference within a project.\r\n\r\nPoint out unwanted style differences in review.\r\n\r\nManually edit `*.tf` files.\r\n\r\n### Proposal\r\n\r\nThe `fmt` command removes commas from multi-line maps as styled in the terraform docs.\r\n\r\n### References\r\n\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/types#maps-objects","comments":["Thanks for suggesting this, @jdufresne! I agree that this sort of change is consistent with the goals of `terraform fmt`, and would be happy to see it implemented.\r\n\r\n`terraform fmt` rules are a little tricky to specify, though: the formatter works at a lower semantic level than most other parts of Terraform, dealing mainly with individual tokens rather than higher-level abstract syntax structures like \"object literal\". Therefore I expect we'd need to find a way to describe this rule in terms of the data the formater has available to it.\r\n\r\nYou may have noticed that `terraform fmt` today does far more with the top-level structural objects like blocks and block arguments than it does with details inside expressions, and that is in large part because expression syntax is considerably more variable and so pattern-matching for formatting inside expressions is harder to get right, at least with the current capabilities of the underlying HCL API we rely on.\r\n\r\nDoing this _thoroughly_ will, I think, require some additional investment in the underlying `hclwrite` package to have it expose more information about the boundaries between parts of nested expressions, but we may still be able to achieve a partial implementation of the rule that would, to start, only work for the situation where an object constructor is directly assigned to an argument inside a block, and would not apply if the object constructor were wrapped inside some other kind of expression. We already have some similar rewriting logic for turning a naked `\"${foo}\"` into just `foo` here:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/d7f97ec847aafc100f60612ae82ec0952c9e7249\/internal\/command\/fmt.go#L328-L423\r\n\r\nPerhaps if we were to restructure that to first check the type of the first token in the expression, then we could run the currently-implemented logic if the first token is `TokenOQuote`, but run the new comma-removing rule if the first token is `TokenOBrace`.\r\n\r\nConcretely, that means that the following would get \"fixed\":\r\n\r\n```hcl\r\n  foo = {\r\n    a = 1,\r\n    b = 2,\r\n  }\r\n```\r\n\r\n...but the following would not, because the formatter's analyzer is not yet sophisticated enough to detect that an object constructor is starting in the middle of this expression:\r\n\r\n```\r\n  foo = baz({\r\n    a = 1,\r\n    b = 2,\r\n  )}\r\n```\r\n\r\nEven _that_ might be tricky to achieve today though, because it seems to require the formatter to understand which commas are separating object constructor entries vs. other commas such as those in a function call argument list. My initial idea for dealing with it (as long as we're not changing `hclwrite` to have more expression-boundary-awareness) would be to count opening and closing \"bracket-like\" tokens in the expression and only remove commas that appear when there are no brackets open except the main braces for the object constructor.\r\n\r\n(In HCL there are various \"bracket tokens\" that all follow the naming scheme `TokenOSomething`\/`TokenCSomething`, and all current uses of commas in the language appear wrapped inside _some_ sort of \"brackets\" to create the context for what the comma means, so I believe the above heuristic should work.)\r\n","Ahh, right after I posted I realized a problem:\r\n\r\n```\r\n  foo = {\r\n    for x, y in anything : whatever\r\n  }\r\n```\r\n\r\nThe comma after `x` here is within a pair of braces, but these braces are representing a `for` expression rather than an object constructor, and so it would be incorrect to do anything with that comma. So at the very least we'd need a special exception for when the `TokenOBrace` is followed by the keyword `for`. Maybe there are other examples of this. It might be better to just go directly to figuring out how to improve `hclwrite` to be able to report on the expression boundaries in a sequence of tokens.\r\n"],"labels":["enhancement","new","fmt"]},{"title":"Unable to reference modules with slash in ref name","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.7.0\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n\r\nmodule \"test\" {\r\n  source = \"github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component\"\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/denis256\/ca6eac9fe36b5ee6f27c67db6f1ca79d\r\n\r\n### Expected Behavior\r\n\r\nModule source to be downloaded from git branch with slashes\r\n\r\n### Actual Behavior\r\n\r\n\r\nPath `github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component` is evaluated as `git::https:\/\/github.com\/denis256\/terraform-test-module.git\/\/ABC-1234-component\/modules\/test-file?ref=team\"` which lead to error.\r\n\r\n```\r\n2024-01-22T16:52:28.884Z [TRACE] ModuleInstaller: test source address has changed from \"git::https:\/\/github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=master\" to \"git::https:\/\/github.com\/denis256\/terraform-test-module.git\/\/ABC-1234-component\/modules\/test-file?ref=team\"\r\n2024-01-22T16:52:28.884Z [TRACE] ModuleInstaller: discarding previous record of test prior to reinstall\r\n2024-01-22T16:52:28.884Z [TRACE] ModuleInstaller: cleaning directory .terraform\/modules\/test prior to install of test\r\n2024-01-22T16:52:28.888Z [TRACE] ModuleInstaller: test address \"git::https:\/\/github.com\/denis256\/terraform-test-module.git\/\/ABC-1234-component\/modules\/test-file?ref=team\" will be handled by go-getter\r\nDownloading git::https:\/\/github.com\/denis256\/terraform-test-module.git?ref=team for test...\r\n2024-01-22T16:52:28.888Z [TRACE] getmodules: fetching \"git::https:\/\/github.com\/denis256\/terraform-test-module.git?ref=team\" to \".terraform\/modules\/test\"\r\n2024-01-22T16:52:29.417Z [TRACE] modsdir: writing modules manifest to .terraform\/modules\/modules.json\r\n\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for reporting this, @denis256!\r\n\r\nThis source address seems to be relying on the `github.com` shorthand, which causes some extra preprocessing to turn it into a Git URL for Terraform to use. Although it _should_ work as you described (and I'd still like to fix it), I wonder if skipping that shorthand would be a viable workaround in the meantime:\r\n\r\n```hcl\r\n  source = \"git::https:\/\/github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component\"\r\n```\r\n\r\nor, if you prefer to use SSH instead:\r\n\r\n```hcl\r\n  source = \"git::ssh:\/\/git@github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component\"\r\n```\r\n\r\nI don't actually know if this will make a difference, but if it _does_ turn out to work when written explicitly in this way then that would be a good clue as to where the bug is.\r\n\r\nTerraform actually delegates much of this behavior to an upstream library called [go-getter](https:\/\/github.com\/hashicorp\/go-getter), and so it's likely that this will need to be fixed there rather than here, but we'll need to do some further debugging first to confirm. Thanks again!\r\n","Due to us having delegated much of this work to go-getter, there aren't a lot of detailed unit tests for this behavior in Terraform itself. I wrote a quick temporary test in `internal\/getmodules` to see if this would reproduce easily:\r\n\r\n```go\r\nfunc TestSplitPackageSubdir(t *testing.T) {\r\n\ttests := []struct {\r\n\t\tInput string\r\n\t\tWant  [2]string\r\n\t}{\r\n\t\t{\r\n\t\t\t\"github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component\",\r\n\t\t\t[2]string{\"github.com\/denis256\/terraform-test-module.git?ref=team\/ABC-1234-component\", \"modules\/test-file\"},\r\n\t\t},\r\n\t}\r\n\r\n\tfor _, test := range tests {\r\n\t\tt.Run(test.Input, func(t *testing.T) {\r\n\t\t\tpkg, sub := SplitPackageSubdir(test.Input)\r\n\t\t\tgot := [2]string{pkg, sub}\r\n\t\t\twant := test.Want\r\n\t\t\tif got != want {\r\n\t\t\t\tt.Errorf(\"wrong result\\ninput: %s\\ngot:  %#v\\nwant: %#v\", test.Input, got, want)\r\n\t\t\t}\r\n\t\t})\r\n\t}\r\n}\r\n```\r\n\r\nUnfortunately(?), this test actually passed as written!\r\n\r\n```\r\n--- PASS: TestSplitPackageSubdir (0.00s)\r\n    --- PASS: TestSplitPackageSubdir\/github.com\/denis256\/terraform-test-module.git\/\/modules\/test-file?ref=team\/ABC-1234-component (0.00s)\r\n```\r\n\r\n...so it seems like the problem is more subtle here than just the subpath splitting logic not working right. I would guess that this problem arises in some later step after we've already performed this pkg vs. subpath split.\r\n\r\n---\r\n\r\nIf this _does_ turn out to be a `go-getter` bug and we send a fix upstream, it's possible we'll also need to make the same fix to the [go-slug \"sourceaddrs\" package](https:\/\/github.com\/hashicorp\/go-slug\/tree\/main\/sourceaddrs), since it aims to be backward-compatible with a subset of what Terraform accepts as source addresses today and its parsing and normalization logic was based on that from go-getter, so it might have inherited the same bug.\r\n","Hello, for quick workaround you can use \"\/\" as hex code _%2F_.\r\nIt's working for me:\r\n\r\n```\r\n17:16:57  - main.gcp_app_network_module in .terraform\/modules\/main.gcp_app_network_module\/modules\/gcp-network-module\r\n17:16:57  Downloading git::https:\/\/github.com\/mrachuta\/terraform-resources.git?ref=feature%2Fadd-ssl-to-cloudsql-module for main.gcp_cloudsql_module...\r\n```"],"labels":["bug","new","v1.7"]},{"title":"feat: flatten map function","body":"### Terraform Version\r\n\r\n```shell\r\nterraform v1.7.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWhen dealing with nested data, I am proposing a simple way to generate flattened, resultant maps for use in resource\/module `for_each`.\r\n\r\nThis use case is typical in module design when creating resources that have child resources and you want to use a single data type to represent the resource + child resources.\r\n\r\nE.g. Got:\r\n\r\n```terraform\r\n# in locals\r\nmy_data = {\r\n  first_parent = {\r\n    name = \"first_parent\"\r\n    child_map = {\r\n      first_child = {\r\n        child_name = \"first_child_from_first_parent\"\r\n      }\r\n      second_child = {\r\n        child_name = \"second_child_from_first_parent\"\r\n      }\r\n    }\r\n  }\r\n  second_parent = {\r\n    name = \"second_parent\"\r\n    child_map = {\r\n      first_child = {\r\n        child_name = \"first_child_from_second_parent\"\r\n      }\r\n      second_child = {\r\n        child_name = \"second_child_from_second_parent\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWant:\r\n\r\n```terraform\r\nflattened_map = {\r\n  \"first_parent\/first_child\" = {\r\n    \"child_key\" = \"first_child\"\r\n    \"child_value\" = {\r\n      \"child_name\" = \"first_child_from_first_parent\"\r\n    }\r\n    \"parent_key\" = \"first_parent\"\r\n  }\r\n  \"first_parent\/second_child\" = {\r\n    \"child_key\" = \"second_child\"\r\n    \"child_value\" = {\r\n      \"child_name\" = \"second_child_from_first_parent\"\r\n    }\r\n    \"parent_key\" = \"first_parent\"\r\n  }\r\n  \"second_parent\/first_child\" = {\r\n    \"child_key\" = \"first_child\"\r\n    \"child_value\" = {\r\n      \"child_name\" = \"first_child_from_second_parent\"\r\n    }\r\n    \"parent_key\" = \"second_parent\"\r\n  }\r\n  \"second_parent\/second_child\" = {\r\n    \"child_key\" = \"second_child\"\r\n    \"child_value\" = {\r\n      \"child_name\" = \"second_child_from_second_parent\"\r\n    }\r\n    \"parent_key\" = \"second_parent\"\r\n  }\r\n}\r\n```\r\n\r\nIn order to create the resultant map for a for_each you need multiple nested for expressions, which is cumbersome.\r\n\r\nWhat if there was a `flatmap()` function that you could express like this:\r\n\r\n```\r\nresource \"terraform_data\" \"nested_map\"\r\n  for_each = flatmap(local.my_data, \"child_map\")\r\n  input    = each.value.child_value.child_name\r\n  # ...\r\n```\r\n\r\nThe `flatmap()` function would return a flattened map of objects with a composite key and a value with the following schema:\r\n\r\n```\r\n{\r\n  parent_key  = string\r\n  child_key   = string\r\n  child_value = <dynamic based on input>\r\n}\r\n```\r\n\r\n\r\n\r\n### Attempted Solutions\r\n\r\nThis is possible today but is difficult unless experienced:\r\n\r\n<https:\/\/github.com\/Azure\/terraform-robust-module-design\/blob\/main\/nested_maps\/flatten_nested_map\/main.tf>\r\n\r\n### Proposal\r\n\r\nSee early attempts at implementing a `flatmap()` function:\r\n\r\nhttps:\/\/github.com\/matt-FFFFFF\/terraform\/blob\/d408f6cb14a1fe99d09adebfa767005ec2fa5b4f\/internal\/lang\/funcs\/collection.go#L656C1-L745C3\r\n\r\nI would be happy to refine and submit a PR if there is interest?\r\n\r\n### References\r\n\r\n- #27126 ","comments":["Thanks for this report! ","As I said, happy to make a PR and contribute a more complete solution\r\n\r\nLet me know.\r\n\r\nhttps:\/\/github.com\/matt-FFFFFF\/terraform\/blob\/d408f6cb14a1fe99d09adebfa767005ec2fa5b4f\/internal\/lang\/funcs\/collection.go#L656C1-L745C3","Thanks for the offer! This would most likely be implemented as an external function via the function provider capability currently in development (see https:\/\/github.com\/hashicorp\/terraform\/issues\/27696#issuecomment-1758977989). Once that is available, not only should you build it but you will no longer need any permission to do so. :) Given previous discussions on similar functions, I do not believe the core maintainers would implement this as a built-in function to Terraform, but I will raise the possibility with the team. ","No problem. I'll look into this and await 1.8!\n\nThanks again"],"labels":["enhancement","functions","new"]},{"title":"Terraform init in CI with a fresh environment","body":"### Terraform Version\n\n```shell\n1.7.0\n```\n\n\n### Use Cases\n\nIn Github Actions we would like to create or select a workspace.\n\n### Attempted Solutions\n\nterraform init -select-or-create -workspace=my-existing-or-new-workspace\n\n### Proposal\n\n```\r\nterraform {\r\n  cloud {\r\n    organization = \"my-organization\"\r\n\r\n    workspaces {\r\n      tags = [\"app:myapplication\"]\r\n      \r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis result:\r\n\r\n\r\n```\r\n    \/my\/path # terraform init\r\n    Initializing Terraform Cloud...\r\n    No workspaces found.\r\n    There are no workspaces with the configured tags (app:myapplication) in your Terraform Cloud organization. To finish initializing, Terraform needs at least one workspace available.\r\n    Terraform can create a properly tagged workspace for you now. Please enter a name to create a new Terraform Cloud workspace.\r\n    Enter a value:\r\n```\r\n\r\nThe problem is in CI environment having a non-interactive shell cause troubles and forcing users to create a default or that environment locally. \r\n\r\nAnother solution is using custom api scripts to create workspace by API calls.\r\n\r\nI think all these solutions are not intutitive and cause troubles and extra work. Having a parameter like select command or \r\n\r\n`terraform init -select-or-create -workspace=my-existing-or-new-workspace`\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","@apparentlymart @crw  I think this can be considered as a bug since without a **default** workspace it raises error. The workaround is creating a default workspace for each tag + workspace name and this is pretty much annoying. \r\n\r\nIn order to reproduce problem please init a fresh workspace which has a tag  with `-input=false` to simulate a CI environment. \r\n\r\n  ","In our process the \"enhancement\" label represents changes to Terraform's technical design, while \"bug\" represents fixing situations where Terraform's implementation differs from its design. This separation is important because issues in each category require a different sort of work, sometimes different people to work on them, etc.\r\n\r\nThe Terraform Cloud integration was designed to require at least one matching workspace to exist as a compromise because traditional Terraform backends deal with this situation by implicitly creating a workspace named \"default\", but that approach would not be appropriate in Terraform Cloud. The Terraform Cloud integration is working as designed, even though we might disagree with that design.\r\n\r\nI agree that the current design would be annoying if you primarily work in Terraform CLI and expect to be able to start from that point. Many Terraform Cloud customers seem to create their workspaces prior to using Terraform CLI with them, using either the web UI, the `hashicorp\/tfe` provider, or directly with the API, and so I assume the current design was made with that assumption in mind. The Terraform Cloud teams will need to consider how and whether to support this new possibility of having all workspaces created by Terraform CLI when using the Terraform Cloud integration. This issue therefore primarily needs some design input from Terraform Cloud teams.\r\n\r\nYour proposal of having Terraform CLI optionally automatically create and use a specific workspace name during `terraform init` does seem reasonable to me on the surface, and if the Terraform Cloud teams agree that it's a reasonable usage pattern then it could also potential allow loosening the requirement in traditional backends of always having a workspace named \"default\", since that restriction exists mainly to avoid a non-Terraform-Cloud version of this same error.\r\n\r\n(That won't necessarily be straightforward to achieve, because some backend implementations unfortunately also rely on there always being a `default` workspace for their own purposes. For example, the `s3` backend stores the `default` workspace differently than the others because its design predates the idea of having multiple workspaces. But it'll be something we should consider as part of designing this if we do decide to move forward with the idea of implicitly creating workspaces as part of `terraform init`, since we'll need to figure out what the new option ought to do when _not_ using the Terraform Cloud integration too.)\r\n\r\n","Thank you for explanation as always.\r\n\r\nI think `init` was much time ago implemented than terraform cloud config and after bringing tags support in the cloud config, this part might be missed. Having only only one empty default workspace is fine for many people but in our current situtation, it requires default workspace per tag. Terraform itself recommending to have small workspaces and this requires us to create many not usable workspaces. \r\n\r\nBy the way I tackled this problem by calling API manually to create workspace before starting operation. It is ugly but working. \r\n\r\n\r\n"],"labels":["enhancement","cli","cloud"]},{"title":"Command to move resources to a submodule","body":"### Terraform Version\r\n\r\n```shell\r\n1.6\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI have a 100 resources in a root module, that I want to refactor to a *sub*-module. Namely, moving a subset of resources from `root\/main.tf` to `root\/new_sub_module\/main.tf`: \r\n```\r\nroot \r\n  main.tf\r\n  new_sub_module\r\n     main.tf\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nVerify that there are no unapplied changes\r\n\r\nMove the code for the resources to the new sub-module folder `new_sub_module`\r\n\r\nCreate a new `module` block that includes the new submodule:\r\n```hcl\r\nmodule \"new_sub_module\" {\r\n   source = \".\/new_sub_module\"\r\n}\r\n```\r\n\r\nRun `terraform plan` and confirm all changes are create\/destroy pairs due to the refactor\r\n\r\nThen do this in a linux terminal (requires jq): \r\n```shell\r\nterraform plan -out move_to_submodule.tfplan\r\nterraform show -json move_to_submodule.tfplan > move_to_submodule.tfplan.json\r\nfor r in $( jq -r '.resource_changes[] | select(.change.after == null) | .address' move_to_submodule.tfplan.json ); do \r\n  echo \"moved {\"\r\n  echo \"from = $r\"\r\n  echo \"to = module.new_sub_module.$r\"\r\n  echo \"}\"\r\ndone > move_to_submodule.tf\r\n```\r\n\r\nFinally, run `terraform plan` again to confirm the plan now consists of all moves instead of create\/destroy pairs\r\n\r\n### Proposal\r\n\r\nHave a command that generates the `move_to_submodule.tf`. Eg workflow: \r\n\r\nrefactor code to new submodule (no other changes)\r\nterraform plan -out move_to_submodule.tfplan\r\nterraform move move_to_submodule.tfplan -out moved_to_submodule.tf\r\nterraform plan\r\n\r\nAlternately, extend the `moved` block so we can say \"move everything that has been deleted to module X\": \r\n```hcl \r\nmoved {\r\n  from = ALL_DELETED  # this is a reserved keyword for the value of \"from\"\r\n  to   = module.new_sub_module\r\n}\r\n```\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @schollii! Thanks for sharing this use-case.\r\n\r\nIn your writeup you described using a custom script to code-generate multiple `moved` blocks. Out of curiosity, would you also have found it acceptable if Terraform offered a built-in command for automatically generating the same code you generated through custom scripting?\r\n\r\nI ask because a one-off command that snapshots everything that's _current_ in the situation of having been moved into a child module seems more explicit and less \"tricky\" to me than permanently leaving a rule in your module that would also match anything else that gets removed from the root module in future. `moved` blocks are supposed to be a record of refactoring actions _already taken, and so I'd personally prefer a solution that makes it clear from the code which specific resources have been moved, and ensures that no future changes to the module will get retroactively added to the set of moved resources under future maintenance of the module.\r\n\r\nI don't have an exact proposal for what such a command would look like, but I imagine it taking as input the same information you showed in your proposed `moved` block: some indication that the intent is to match \"all missing resources in the root module\", and the name of the new module that those resources might've been moved into. It would then, similar to what you did manually, search for any resources that are missing from the root module but present with the same name in the specified new module, and generate a `moved` block for each one.\r\n","@apparentlymart I'll try to clarify : The command I'm proposing acts on a plan file, hence the command assumes that the person has refactored some code to a submodule and generated a plan and saved it to a file. If all the person did is move code to a new subfolder, and added a `module` block in root module, then the plan will have 1 destroy and 1 create for each resource refactored and the command can easily match them. So once the command knows the pairs (from the plan), it need only generate the set of `moved` blocks that cancel out all such destroy\/create pairs, like my script did. \n\nSo the new command would do what my script in the proposal section does: reads plan, finds all deleted items, and creates a `moved` block for it, and finally writes all such blocks to the output file specified.\n\nIt could be even smarter and infer what the moves have been based on the common portion of the address (`x` -> `module.something.x`), which would allow a refactoring of root module code to more than 1 submodule. \n\nNote that my proposal has an alternative to the command, which might be better aligned with the behaviour of the `moved` block : the block will currently move sets of resources that match say a module name or a resource type. So the alternative is a different pattern to match. The `ALL_DELETED` is the sign that a function must be called after the plan has been generated, to do what I described earlier, except that it doesn't generate `moved` blocks (just like a module rename does not generate a pile of `moved` blocks, it just knows what that means in terms of changes.\n\nFeel free to contact me if you still have questions ```\n\nI hope this answers your questions. "],"labels":["enhancement","new"]},{"title":"backend\/s3: support ca-west-1 region","body":"### Terraform Version\n\n```shell\nTerraform v1.7.0\r\non linux_amd64\n```\n\n\n### Use Cases\n\nbe able to create and manage resources in the aws region `ca-west-1` and use s3 buckets located in that region as backend for state storage\n\n### Attempted Solutions\n\nusing ```skip_region_validation = true``` as a temporary work around\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Hello,\r\n\r\nThis appears to be an issue or question with the AWS provider, not with Terraform itself. You can see existing issues and file a new one in their repository here: https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues. If you have questions about Terraform or the AWS provider, it's better to use [the community forum](https:\/\/discuss.hashicorp.com\/c\/terraform-providers\/tf-aws\/33) where there are more people ready to help. The GitHub issues here are monitored only by a few core maintainers.\r\n\r\nThanks!","I have the latest version of the aws provider `5.33.0`\r\nbut still get \r\n\r\n```\r\n\u2502 Error: Invalid region value\r\n\u2502\r\n\u2502   on providers.tf line 9, in terraform:\r\n\u2502    9:     region = \"ca-west-1\"\r\n\u2502\r\n\u2502 Invalid AWS Region: ca-west-1\r\n```\r\nthis is about the s3 backend provider supporting the region\r\n\r\n```\r\nterraform {\r\n  backend \"s3\" {\r\n    bucket = \"terraform-state\"\r\n    key    = \"vpc-ca-west-1.tfstate\"\r\n    region = \"ca-west-1\"\r\n  }\r\n}\r\n```","Thanks @jgrammen-agilitypr,\r\n\r\nYou mentioned that you want to \"be able to create and manage resources\", which is under the purview of the provider. Looking at the AWS provider release notes, it shows that `ca-west-1` was added in 5.32: https:\/\/github.com\/hashicorp\/terraform-provider-aws\/blob\/main\/CHANGELOG.md#5320-january-11-2024, so your version should support the resources available in that region.\r\n\r\nThe provider however is not related to the backend, so we can add a feature request here for that!"],"labels":["enhancement","backend\/s3","new"]},{"title":"azure state provider: tls: failed to verify certificate: x509: certificate signed by unknown authority (reopen of #34427)","body":"### Terraform Version\r\n\r\n```shell\r\n1.5.7\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nN\/A\r\n\r\n### Debug Output\r\n\r\nN\/A\r\n\r\n### Expected Behavior\r\n\r\nWhen trying to access azure blob state bucket on azure blob storage, the client should trust certificates installed on the OS.\r\n(this is a reopen of https:\/\/github.com\/hashicorp\/terraform\/issues\/34427)\r\n\r\n### Actual Behavior\r\n\r\nWhen running behind traffic-intercepting proxy, trying to access a state bucket on azure blob storage yields the following:\r\n\r\nError: Failed to load state: blobs.Client#Get: Failure sending request: StatusCode=0 -- Original Error: Get \"https:\/\/some-blob-name.blob.core.windows.net\/some-container-nale\/some-name%2Fterraform.tfstate?st=2023-12-17T13%3A12%3A26Z&se=2023-12-23T13%3A27%3A26Z&sp=racwdl&spr=https&sv=2022-11-02&sr=c&skoid=xxxx-a2d0-xxx-xxx-xxx&sktid=70361cf4-caa3-4dfe-a915-05704b779731&skt=2023-12-17T13%3A12%3A26Z&ske=2023-12-23T13%3A27%3A26Z&sks=b&skv=2022-11-02&sig=xxxxxxxx%3D\": tls: failed to verify certificate: x509: certificate signed by unknown authority\r\n\r\nThis happens on debian 11\/bullseye, where the certificate of the intercepting party (in this case Cloudflare WARP) is already installed as trusted. Also, azure-cli is already configured to work in this environment.\r\n\r\nWasn't able to find any documentation\r\n\r\n### Steps to Reproduce\r\n\r\nConfigure cloudflare-warp (or any other traffic inspecting client)\r\nAdd certificate to OS trusted certificate store\r\nAdd certificate to azure-cli as per (https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/use-cli-effectively?tabs=bash%2Cbash2#work-behind-a-proxy)\r\nrun terraform\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nNote that the team that works on this feature is the Azure Provider Team, and they have been notified of this issue. Thanks again!","Thanks for the feedback. I would think a similar issue would arise with S3 backend or other backend, where certificate validation is not relying on the underlying OS.\r\nFor reference, I raised similar issues for azure-cli (https:\/\/github.com\/Azure\/azure-cli\/issues\/28050) and aws-cli (https:\/\/github.com\/aws\/aws-cli\/issues\/8417), and this also seems to be addressed by https:\/\/github.com\/Azure\/azure-cli\/issues\/26456. Not sure if terraform would need some custom solution or if it could be addressed by configurations at the level of the SDK."],"labels":["bug","backend\/azure","new"]},{"title":"Enrich state file with technical metadata","body":"### Terraform Version\n\n```shell\n>1.2\n```\n\n\n### Use Cases\n\nEnd goal: \r\nClearly and definitively determine all necessary environmental settings and the resulting status of their application\r\n\r\nUsecase:\r\nRestore unambiguous and working  environment\n\n### Attempted Solutions\n\nDelete all state files from the top of the stack versioning states at the backed till reach the one that is the correct\n\n### Proposal\n\nEnrich state file with metadata such as:\r\n- all providers versions\r\n- apply date\r\n- apply result\n\n### References\n\nhttps:\/\/discuss.hashicorp.com\/t\/resource-instance-managed-by-newer-provider-version\/61873","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Consider Exposing Attribute Deprecation Messages in Machine-Readable Output","body":"### Terraform Version\n\n```shell\n1.7.0\n```\n\n\n### Use Cases\n\nCurrently, the Terraform Plugin Protocol implementation and `terraform providers schema -json` output expose schema attribute deprecation as a boolean true\/false value. Tooling such as editor integrations and provider documentation generation relying on this information can only notify practitioners whether the attribute is deprecated or not, without context about what to do in those situations.\r\n\r\nExample JSON today:\r\n\r\n```jsonc\r\n{\r\n  \"format_version\": \"1.0\",\r\n  \"provider_schemas\": {\r\n    \"ADDRESS\": {\r\n      \/\/ ...\r\n      \"resource_schemas\": {\r\n        \"TYPE\": {\r\n          \/\/ ...\r\n          \"block\": {\r\n            \"attributes\": {\r\n              \"NAME\": {\r\n                \/\/ ...\r\n                \"deprecated\": true,\r\n              }\r\n            },\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\n\n### Attempted Solutions\n\nFor provider documentation generation, provider developers can copy the attribute deprecation message information into the attribute description. For practitioners using an editor integration, it would only show up when hovering over configured attributes if those editor integrations include the attribute description in that sort of user interface.\r\n\r\nAnother extremely high lift solution would be to skip using the Terraform Plugin Protocol and `terraform providers schema -json` output at all for these downstream use cases, instead relying on something like the provider binaries outputting their own machine-readable data. There are proposals for what this might look like, however it is still unclear whether it is a reasonable idea giving the ecosystem burden to implement it.\r\n\n\n### Proposal\n\nI think ideally, we would take a look at this more holistically to support what editors call \"code actions\" as more structured data that gets encoded into providers and sent across the protocol, so editors and potentially other tooling can offer configuration refactoring\/remediation support automatically. However, that effort requires some non-trivial discovery effort and needs to be prioritized accordingly.\r\n\r\nTherefore this change is being pragmatically proposed since even with enhancements such as those:\r\n\r\n- There can still be gaps where a deprecation does not exactly fit something that would fit a configuration refactoring.\r\n- The deprecation messaging might be able to convey additional context that might not be well captured in structured data serving a specific, but different purpose.\r\n\r\nSo with that said, provider developers using both terraform-plugin-sdk ([`helper\/schema.Schema.Deprecation`](https:\/\/pkg.go.dev\/github.com\/hashicorp\/terraform-plugin-sdk\/v2\/helper\/schema#Schema.Deprecated)) and terraform-plugin-framework ([`DeprecationMessage`](https:\/\/pkg.go.dev\/github.com\/hashicorp\/terraform-plugin-framework\/resource\/schema#StringAttribute.DeprecationMessage) in each attribute type) already encode this information as a message string, which is included with the warning diagnostic details generated by both SDKs.\r\n\r\nThe proposal here would be to introduce a `deprecation_message` string field in the Terraform Plugin Protocol beside the existing `deprecated` boolean field to keep existing compatibility. We added this for function definitions recently. On the provider side of the protocol, the SDKs would take the existing provider-defined messages and thread that information through to the protocol data. On the core side, it would need to threaded through the provider RPC handlers and into the `terraform providers schema -json` output.\r\n\r\nExample JSON proposed:\r\n\r\n```jsonc\r\n{\r\n  \"format_version\": \"1.0\",\r\n  \"provider_schemas\": {\r\n    \"ADDRESS\": {\r\n      \/\/ ...\r\n      \"resource_schemas\": {\r\n        \"TYPE\": {\r\n          \/\/ ...\r\n          \"block\": {\r\n            \"attributes\": {\r\n              \"NAME\": {\r\n                \/\/ ...\r\n                \"deprecated\": true,\r\n                \"deprecation_message\": \"Use X attribute instead.\",\r\n              }\r\n            },\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThere could also be consideration for exposing the _schema_ level deprecation messaging as well resources and data sources, for similar reasons. The protocol does not expose this deprecation information at all, but all the provider side details are similar as attributes. This would let downstream documentation\/editor tooling also provide this additional context for practitioners.\r\n\n\n### References\n\n- https:\/\/github.com\/hashicorp\/terraform-ls\/issues\/1538\r\n- https:\/\/github.com\/hashicorp\/terraform-plugin-docs\/issues\/34\r\n","comments":["Thanks for suggesting this!\r\n\r\nPersonally I feel that we're quickly reaching the practical limits on how much we can achieve by continuing to extend these ever-growing static JSON artifacts -- and in some cases, like the JSON plan output, I might argue that we're already _past_ the practical limit, but thankfully that's not a concern for this _particular_ issue.\r\n\r\nI do agree that exposing this additional information _somehow_ seems worth doing for the reasons stated, but I would also encourage considering a transition away from `terraform providers schema -json` towards live RPC functions offered by the still-very-new `terraform rpcapi` mechanism, which would then allow Terraform Core to stay running concurrently with callers like the language server and answer \"smaller\" questions like \"what is the schema for _these specific resource types_?\" as the need for that information gradually emerges, rather than dumping a single huge artifact out in one go.\r\n\r\nIf we were to adopt that model instead, I would feel considerably less concerned about each incremental addition. Today the schema artifact is already _huge_ as soon as someone adds even just the `hashicorp\/aws` provider in isolation, and so that creates some pressure to be picky about what level of detail to include in there.\r\n\r\nWe need to let the Terraform Stacks work settle before we add any further `terraform rpcapi` callers, because we're currently using Stacks as a realistic test case to gain experience and revise the design as needed, but I feel pretty confident in asserting that we _can_ offer a function like what I've described above, and the uncertainty is only in the exact details of how we'd present it, since currently RPC API is just one big protobuf service with everything packed into it, and that's probably not going to scale well as we try to maintain support for multiple different use-cases with overlapping but different needs.\r\n","I agree with your sentiment regarding the size of the existing GetProviderSchema RPC response data and it definitely seems desirable to think about longer-term ways that tooling such as editor integrations could utilize the RPC API for more dynamically handling operations, however I'm not exactly sure what the next steps are here to move us towards enhancing downstream tooling with the additional data sourced from providers. It currently looks like the RPC API is designed as an interface between Terraform processes and provider-defined data is being served there as a \"passthrough\" of sorts from the core representation of that data fetched via the existing Terraform Plugin Protocol. Given that providers today only talk over the Terraform Plugin Protocol, it seems that in order to get additional data available to Terraform from providers either that existing protocol will need enhancements, we should start discussions on using a different protocol\/service for providers, or we should start discussions on supporting multiple protocols\/services for providers. There are of course other options, such as introducing other types of additional machine-readable data, but I feel fairly confident that there would not be any desire to design yet-more interfaces with Terraform.\r\n\r\nIn terms of enhancing the existing Terraform Plugin Protocol, over in https:\/\/github.com\/hashicorp\/terraform-proposals\/issues\/81, there was some musing on other protocol-related changes that would help separate both core and providers from needing to deal with the entire schema data at once. In particular, the \"GetProviderSchema Limiting of Resource Types\" and (I personally think preferable) \"New GetXSchema RPCs\" proposals. We didn't go down those routes at the time because it was mentioned that core relies on the entire schema data, so there was not a benefit to introducing RPCs that would not be called.\r\n\r\nIs something like that, e.g. separate RPCs that are limited to individual resource types but expanded in the amount of data they return, a desirable approach? If not, could you help me understand what a more desirable solution would be?\r\n","Creating an ability for systems like the language server to talk directly to provider plugins and thus support optional extra protocols that Terraform Core doesn't need is also a plausible idea! I think the main concern down that road in the past was that it would require those callers to essentially reimplement the provider plugin discovery logic that lives in Terraform CLI today, but I wonder if we could find a compromise where the rpcapi offers a way to discover and\/or launch providers but then any subsequent communication is directly with the providers.\r\n\r\nOne thing I missed in my first read of this issue is that I was reading it as a request to expose some information that Terraform Core is already retrieving for its own purposes anyway, but with fresher brain I remember now that the logic for returning deprecation warnings lives in the SDK and Framework, and so this proposal also implies growing the provider plugin protocol schema model to include some data that Terraform Core would entirely ignore and Terraform CLI would only pass verbatim out into this JSON response. If that's true then I agree this seems like a prompt to revisit the idea of exposing an additional language server and documentation generation support protocol directly from providers, along with the new idea above of using rpcapi to help those callers to find and launch each provider plugin.\r\n\r\nExposing the subset of schema data that Terraform Core already needs anyway via a new rpcapi service could also potentially be useful, but I suppose we should wait to see what use-cases remain in a world where language server and docs tools would be using a specialized new protocol directly.\r\n\r\n(Another user of schema information is Terraform Cloud, to allow it to correctly render resource plan\/state data in its web UI, but in that case its needs are the same as Terraform CLI's -- it's a web equivalent of the CLI output, after all -- and we do tend to want to just snapshot the whole schema once and reuse it many times because Terraform Cloud can't keep a Terraform Core service running permanently to provide data gradually.)\r\n"],"labels":["enhancement","new"]},{"title":"Incorrect usage of term \u201cmodule name\u201d instead of \u201cmodule repository\u201d","body":"### Terraform Version\n\n```shell\nN\/A\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/tutorials\/modules\/module-private-registry-share?variants=module-workflow%3Atag#create-the-repository\n\n### What is the docs issue?\n\nIn the doc the term \u201cmodule name\u201d is used where in fact it should be \u201cmodule repository\u201d. This is causing confusion for consumers. \r\n\r\nIt is documented correctly here: https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/registry\/publish-modules#preparing-a-module-repository\r\n\r\n**Today:**\r\n`In order to publish modules to [the Terraform registry](https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/registry\/publish-modules), module names must have the format terraform-<PROVIDER>-<NAME>, where <NAME> can contain extra hyphens.`\r\n\n\n### Proposal\n\n**Suggestion:**\r\n\r\n`In order to publish modules to [the Terraform registry](https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/registry\/publish-modules), module\u2019s repository name must have the format terraform-<PROVIDER>-<NAME>, where <NAME> can contain extra hyphens.`\n\n### References\n\n_No response_","comments":["Thanks for this report!"],"labels":["documentation","new"]},{"title":"Add `-auto-approve` to `init -migrate-state`","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.31.0\r\n+ provider registry.terraform.io\/hashicorp\/local v2.4.1\r\n+ provider registry.terraform.io\/hashicorp\/tls v4.0.5\n```\n\n\n### Use Cases\n\nTo be able to provision the S3 backend bucket for my projects with the same configuration, chicken-and-egg problem, I came to a solution where I'm switching to and from the local backend using an override file, and migrate state between the two.\r\n\r\nThis is all wrapped in two `bootstrap.sh` and `destroy.sh` scripts that are run only once for project seeding and total annihilation, respectively.\r\n\r\nThis is working nicely, except that I can't make the state migration steps unattended, which gets quite annoying when dev requires running these in a loop. \ud83d\ude22\r\n\n\n### Attempted Solutions\n\nTried the `-input=false`, but this spits out an error\n\n### Proposal\n\nAdd an `-auto-approve` flag to the `init` command so `-migrate-state` can run unattended.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hey @silopolis,\r\n\r\ncan't you use `--force-copy` in your scenario? The `force-copy` is supposed to \"suppress prompts about copying state data when initializing a new state backend. This is equivalent to providing a \"yes\" to all confirmation prompts\", so it can used in combination with the migrate-state, e.g., `terraform init -migrate-state -force-copy`","@carminevassallo hi, thanks for the tip ! :pray:\r\n\r\nActually, I finally found that flag in the meantime, but that took some search, and yes, it does the job in my case. :)\r\nI left this issue open because I think it would make UI more consistent, but your explanation makes sense.\r\n"],"labels":["enhancement","waiting-response","new"]},{"title":"[Feature Request] Add \"deleted_by\" meta-argument to resource lifecycle","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nWith some providers, deleting a resource also deletes subordinate objects associated with that resource (e.g: deleting a resource group deletes its contents, deleting an NGINXaaS for Azure deployment deletes its configurations and certificates, etc.). Where that behaviour is expected, a user should be able to specify in a resource's lifecycle block that deleting a related resource will also delete the subordinate resource; in that case `terraform destroy` should not explicitly delete the subordinate resource if it is already planning on deleting the parent resource.\r\n\r\nThere are two main benefits to this approach:\r\n1) The destroy process is simplified. The resource provider presumably has a better understanding of the best order of operations to delete its subordinate resources. So long as the user is confident that the provider will properly perform the deletion, the implementation details should be left to the provider.\r\n\r\n2) For providers with a less-than-perfect API success rate, this reduces the number of API calls, and therefore the chances of a failure preventing the destroy operation from succeeding.\n\n### Attempted Solutions\n\nCurrently, to prevent removal of a resource, I must delete it from the terraform state with `terraform state rm`. This works, but it is an extra imperative step that should be handled by terraform's order of operations planning.\n\n### Proposal\n\nAdd a \"deleted_by\" meta-parameter to the lifecycle block. The value of this parameter is another resource that, when deleted, will also destroy the resource in question. When `terraform destroy` is run, terraform will not explicitly destroy the subordinate resource if it is already planning to destroy the parent resource.\r\n\r\nThis should be an explicit meta-parameter. Terraform may not be able to know in all cases whether a subordinate resource is deleted by a particular parent resource, and in any event this behaviour may not even be desired, so it should be left to the discretion of the user.\n\n### References\n\nImplements a use case from:\r\n- #2253 \r\n\r\nRelates to:\r\n- #3874 \r\n- #23547","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @cybershoe,\r\n\r\nThis is more related to #22094 than the linked issues (and is in fact one of the use cases for that proposal). Adding the reference to the resource block alone doesn't really really help us here, because the resource configuration applies to one or more instances, and the reference can't describe the relationships between the individual instances.\r\n\r\nMore declarative configuration in this area is also being explored via the new `removed` block, but this particular type of relationship may not turn out to be handled due to not being able to clearly map out how the resource instances between resources are related."],"labels":["enhancement","new"]},{"title":"propose a new lifecycle option \"update_protect_only\"","body":"### Terraform Version\n\n```shell\n1.6.6\n```\n\n\n### Use Cases\n\nProtection During Updates: Often, there is a need to protect resources from accidental deletion during updates, but not during a planned destroy operation.\r\nSelective Resource Management: In scenarios where some resources require safeguards against unintended changes, while others might need to be removed without restrictions.\n\n### Attempted Solutions\n\nUsing prevent_destroy: This lifecycle option protects against deletion but applies to both updates and destroy operations, leading to excessive restriction.\r\nJinja Templating: As Terraform does not support interpolation within lifecycle variables, our team has resorted to using Jinja to handle these limitations, which complicates the configuration process.\n\n### Proposal\n\nIntroduce a new lifecycle option named update_protect_only. This feature would:\r\n\r\nAllow protection against accidental deletion during updates.\r\nNot impede the execution of explicit destroy operations.\r\nMaintain Terraform's design principle of not using interpolation in lifecycle configurations.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","I would like to add some details of the use case in which this feature would be useful. We use terraform to deploy our cloud infrastructure in all our environments (production, staging, development, etc). Obviously, each environment has its own configuration, but Terraform doesn't allow to configure via variables the prevent_destroy lifecycle configuration.\r\n\r\nThe issue is that in production, it's too risky to change resource parameter that implies a recreation of a resource, specially if it is a stateful resource like a database. To avoid the risk, we use the `prevent_destroy` flag to ensure that we won't loose the state. \r\n\r\nHowever, this configuration doesn't allow us to have an ephemeral environment for development. In this case, we would like to create and delete the infrastructure without limitations, in order to save costs, although we would like to test the same terraform code. \r\n\r\nA good solution is precisely to be able to protect the destroy of resources during the `terraform apply`, and then don't apply this protection during an intentional destroy.\r\n\r\nI hope this explanation helps you to understand why we would like to include a new lifecycle flag to achieve this use case. May be the name should be more accurate, as `prevent_destroy_during_apply` or something similar. "],"labels":["enhancement","new"]},{"title":"Feature: more comprehensive assertions, such as resource recreation (or lack thereof)","body":"### Terraform Version\r\n\r\n```shell\r\nzsh \u276f terraform version\r\nTerraform v1.6.2\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.31.0\r\n+ provider registry.terraform.io\/hashicorp\/random v3.6.0\r\n```\r\n\r\n\r\n\r\n### Use Cases\r\n\r\nI'm currently updating an EBS module to supports snapshots being specified. I'd like to assert that the volume is recreated in such circumstances.\r\n\r\n### Attempted Solutions\r\n\r\nI'm currently relying on asserting that the snapshot id is set and the knowledge that this value is immutable.\r\n\r\n### Proposal\r\n\r\nWithout getting into exact implementation details, in my particular use case, a `lifecycle` property might be helpful as part of the assertion toolkit. For example:\r\n\r\n```\r\n  assert {\r\n    condition     = aws_ebs_volume.foo.lifecycle.requires_recreation == true\r\n    error_message = \"Expected EBS volume to be recreated\"\r\n  }\r\n```\r\n\r\nAlternatively, further types of assertion blocks could be used:\r\n\r\n```\r\n  assert_lifecycle {\r\n    resource      = aws_ebs_volume.foo\r\n    condition     = requires_recreation == true\r\n    error_message = \"Expected EBS volume to be recreated\"\r\n  }\r\n```\r\n\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Add Ability to Prevent Overwrite During Initialization If Key Already Exists in Backend","body":"### Terraform Version\n\n```shell\nTerraform v1.6.2\r\non windows_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.31.0\r\n\r\nYour version of Terraform is out of date! The latest version\n```\n\n\n### Use Cases\n\nEnd Goal: Make using terraform easier. Right now, its on the user to prevent overwriting of existing state files if they accidentally specify an existing key for a backend. For example, if the user is copy-pasting configurations and forgets to update the key name- the state file will be overwritten.\r\n\r\n```\r\nterraform {\r\n  backend \"s3\" {\r\n    key            = \"production\/gitlab\/terraform.tfstate\"\r\n    \r\n    # Remains unchanged\r\n    bucket         = \"terraform-state\"\r\n    region         = \"us-west-1\"\r\n    dynamodb_table = \"terraform-locks\"\r\n    encrypt        = true\r\n  }\r\n}\r\n```\r\n\r\nFor example, if a user copy-pastes the above and forgets to change the key from `*\/gitlab\/*` to their new application name- it will overwrite the remote state file.\n\n### Attempted Solutions\n\nI've considered making the key a variable but this does not work within the Terraform block. \n\n### Proposal\n\nAdd a parameter called `check_nonexistent (BOOLEAN)` to instigate verification that the key doesn't exist. If this is `true` and the key does exist, throw an error during init.\n\n### References\n\nNot that I discovered.","comments":["Thanks for this feedback, @MrChadMWood!\r\n\r\nI'll leave the AWS provider team (which also maintains this backend) to consider this more deeply, but I do have an initial question.\r\n\r\nI would have expected this key collision situation to result in an error rather than silently overwriting an existing state snapshot, because when Terraform is persisting a new state snapshot it follows something like the following series of steps:\r\n1. Retrieve the latest state snapshot from the backend.\r\n2. Compare the \"lineage\" and \"serial\" fields of the retrieved snapshot with the new snapshot that Terraform is trying to write.\r\n3. Only if the two snapshots seem consistent, write the new snapshot.\r\n\r\nIn the situation you described, I would expect step 2 to either find an S3 object that isn't a state snapshot at all, or to find one with a different \"lineage\" than the one Terraform is trying to write, both of which should result in some kind of error.\r\n\r\nOf course, what you've experienced doesn't match that expectation and so I'd like to try to understand what's going on to cause that safety check to be ineffective. I think the easiest way to quickly gain some insight into that would be for you to re-perform the same sequence of steps that led to an existing snapshot being overwritten, except that when you run whatever command is the one that caused the silent overwrite to happen, first set the environment variable `TF_LOG=trace` to make Terraform produce a verbose log of exactly what was happening internally. If you can then share that log as a [Github Gist](https:\/\/gist.github.com\/) we should hopefully be able to at least narrow down what might've gone wrong here.\r\n\r\nIt seems likely to me that the result of that investigation will lead to this being reclassified as a bug instead of an enhancement request, since our _intention_ is for Terraform to already catch situations that would silently clobber existing state snapshots, but of course that will depend on what exactly is going on here.\r\n\r\nThanks!\r\n"],"labels":["enhancement","backend\/s3"]},{"title":"support force-unlock for remote http backends. Fixes #28421","body":"Credit: https:\/\/github.com\/hashicorp\/terraform\/pull\/28807\r\n\r\nThe issue #28421 is still present, previous PR was valid, `cloud` stanza mentioned [here](https:\/\/github.com\/hashicorp\/terraform\/pull\/28807#issuecomment-1490487650) has nothing to do with it.","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34517) <br\/>All committers have signed the CLA.","Thanks for the submission! "],"labels":["enhancement","backend\/http"]},{"title":"S3 backend AWS SSO Auth error","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.21\"\r\n    }\r\n  }\r\n  backend \"s3\" {\r\n    profile        = \"default\"\r\n    bucket         = \"<state-bucket>\"\r\n    dynamodb_table = \"terraform-state-lock\"\r\n    key            = \"global\/terraform.tfstate\"\r\n    region         = \"us-west-2\"\r\n  }\r\n\r\n  required_version = \">= 1.2.0, < 2.0.0\"\r\n}\r\n\r\nprovider \"aws\" {\r\n  profile = \"default\"\r\n  region  = \"us-west-2\"\r\n}\r\n\r\n# As of 10.17.23 IAM Identity Center is only available in us-east-2\r\nprovider \"aws\" {\r\n  alias   = \"east\"\r\n  profile = \"default\"\r\n  region  = \"us-east-2\"\r\n}\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/KaplanAlex\/e12c170a9600cd01ff519dc491770c6d\n\n### Expected Behavior\n\nTerraform should have access to s3 backend through the sso profile.\n\n### Actual Behavior\n\nTerraform is not able to access the s3 backend.\n\n### Steps to Reproduce\n\n1. terraform init\n\n### Additional Context\n\n The cache file it's looking for does not exist. There are valid credentials in other cache files and other aws cli commands work (e.g. `aws s3 ls`). How does terraform determine the location of the cache file to find?\n\n### References\n\nAppears similar to https:\/\/github.com\/hashicorp\/terraform\/issues\/34248 but proposed resolutions did not work.","comments":["Thanks for the report!"],"labels":["bug","backend\/s3","new","v1.6"]},{"title":"Undocumented Remote-only Variable loading","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/workspaces\/variables\/managing-variables#edit-variable-sets\n\n### What is the docs issue?\n\nIts undocumented but it deserves to be called out that variables set in Terraform Cloud are not exposed to local-execution runs.\n\n### Proposal\n\nAdd explicit documentation mentioning the contexts of when variables are available to local-execution runs.\n\n### References\n\nhttps:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/workspaces\/variables\/managing-variables#edit-variable-sets","comments":[],"labels":["documentation","new"]},{"title":"terraform test: add support for validating plan diffs \/ attribute changes","body":"### Terraform Version\n\n```shell\nlatest\n```\n\n\n### Use Cases\n\nI am using the `terraform test` command released in the terraform 1.6.\r\nIs there a way to check if there is a diff with the state file after executing terraform `apply` command? I tried to run `plan` after `apply`, but the test command will return `pass` when it is executed successfully. It doesn't realize that it's wrong to have these diffs. \r\n\n\n### Attempted Solutions\n\nDoes the current command support checking diff? Or will it be supported later?\r\n\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @shanye997, as this is a question about Terraform rather than specifically a bug report or feature request, it may be better to use [the community forum](https:\/\/discuss.hashicorp.com\/c\/terraform-core\/27) where there are more people ready to help. The GitHub issues here are monitored only by a few core maintainers.\r\n\r\nI will leave this open in case one of the core maintainers would like to use this issue as a feature request. Thanks again!","Thank you for your reply! I will bring this back up in the community forum. Thank you!","Hi @shanye997, thanks for filing this. As suggested by @crw, I've reworked the issue title into something more requesty rather than a question.\r\n\r\nThis is something we are interested in but we do not currently have concrete plans to work on it at the moment. You can see, for example, we did reserve the `state` and `plan` keywords when we first launched: https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/addrs\/parse_ref.go#L123. The idea would be to allow a reference like `plan.null_resource.id.action == \"noop\"` or something as way to validate a resource in a plan doesn't have any changes. Unfortunately, we need to do more product work to figure out the correct syntax for such checks, and to see how users would want to interact with this kind of thing exactly.\r\n\r\nIf you have any ideas, use cases, or suggestions about how you imagine this would work feel free to share them here with some examples and it could help direct our research when we do pick this up.\r\n\r\nSeparately, anyone else viewing this issue who would like to indicate their interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. ","Another note here is that you can approximate something like this with `output` blocks currently. As `run` blocks can refer to outputs from previous run blocks, you should be able to validate that an output isn't being changed by a follow up plan.\r\n\r\nFor example, imagine we have an `output` block called `output_value` then we should be able to write two run blocks like:\r\n\r\n```hcl\r\nrun \"first\" {\r\n  command = apply\r\n}\r\n\r\nrun \"second\" {\r\n  command = plan\r\n\r\n  assert {\r\n    condition = run.first.output_value == output.output_value\r\n    error_message = \"value for output_value changed when it shouldn't\"\r\n  }\r\n}\r\n```\r\n\r\nYou might be able to use this capture any unintended drift in outputs."],"labels":["enhancement","new","terraform test"]},{"title":"Confusing extra quoting in import error message when resource configuration isn't present.","body":"It seems that when Terraform asks a provider to import something and the resource configuration isn't present in the working directory Terraform is producing an error message which includes the resource address wrapped in escaped quotes, rather than in the typical \"naked\" form:\r\n\r\n```\r\n~\/go\/bin\/terraform import 'google_service_account.bqowner[\"bqowner\"]' \"projects\/1238328119\/serviceAccounts\/foobar123\"\r\nError: resource address \"google_service_account.bqowner[\\\"bqowner\\\"]\" does not exist in the configuration.\r\n\r\nBefore importing this resource, please create its configuration in the root module. For example:\r\n\r\nresource \"google_service_account\" \"bqowner\" {\r\n  # (resource arguments)\r\n}\r\n```\r\n\r\nResource instance addresses should not be presented in quotes, because they aren't written in quotes in the configuration and because (as shown here) they sometimes contain quotes themselves, making the result misleading for someone who isn't familiar with Terraform's internals.\r\n\r\nSimilar to #34496, this was caused by an accidental use of `%q` instead of `%s`.","comments":["Hi @bschaatsbergen,\r\n\r\nThanks for filing the issue. In this case, the quoted output was intentionally chosen because nearly all reports filed by users were due to incorrect command line arguments, rather than configuration, and is intended to show a format compatible with typical shell quoting rules. Most users seeing this error understand already how resource addresses are formatted in the configuration, but didn't properly quote the address argument.\r\n\r\nI can see how this could be confusing if you were expecting the standard diagnostic format, but the `import` command is already a bit of an outlier in its operation. The new plan-able import blocks offer more flexibility, safety, and better diagnostic output.\r\n\r\n\r\n\r\n","I see there are a couple PRs associated with the issue, but they do not fix the quoting problem, rather they remove the escaping of inner quotes which would result in incorrect quoting as opposed to extra quoting.\r\n\r\nI think if we are going to change this, it would be to remove the quoting altogether, but I'll have others chime in if that's worth the change. The output from your example does correspond to the command shown, with `\"google_service_account.bqowner[\\\"bqowner\\\"]\"` being equivalent to `'google_service_account.bqowner[\"bqowner\"]'` directly above it. We could format the error in single quotes, but then a user inputting the former escape style would also get the same type of mismatch.","Thank you for the reply, @jbardin. I'll keep this issue open until the team decides on a way to address it."],"labels":["bug","new"]},{"title":"Populate local state from import blocks without apply","body":"### Terraform Version\n\n```shell\nTerraform v1.6.5\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nThe scenario I have in mind is similar to what is mentioned in https:\/\/github.com\/hashicorp\/terraform\/issues\/34150#issuecomment-1784989461 where infrastructure is created with click ops and retrospectively automated.\r\n\r\nDeveloper should be making concurrent click ops changes to a shared development infrastructure and later codify these with a typical git flow. \r\n\r\nIn this scenario one can never use `terraform apply` against the development infrastructure.\r\n\r\nDevelopers should not need a shared state file and thus not need to manage terraform backend credentials on their machines which would only be available in restricted ci\/cd pipelines.\r\n\n\n### Attempted Solutions\n\n- One workaround is to derive terraform import statements from import blocks to populate local state. This is more involved now as the import blocks provide additional capabilities to specify the id's. \r\n- Use appropriate grep filters for terraform plan perhaps in conjuntion with `-target=resource\" to work without or with an outdated local state file. \n\n### Proposal\n\nWhen developers work on turning click ops changes into terraform automation, they determine what import blocks are needed and add the associated terraform resource configs in the desired minimal form possibly introducing variables etc until there is no drift in the parts they worked on.\r\n\r\nNow if a developer starts with a fresh checkout from the version controlled terraform config repository there is no state file yet.\r\n\r\nAfaik currently one cannot populate the state file from import blocks without doing a terraform apply. Without an up to date state file the terraform plan output is quite large. Workarounds mentioned in attempted solutions feel problematic to me. \r\n\r\nThe proposal is to add support to populate a local state file from the import blocks without doing an apply.\r\n \r\nThis could perhaps become the default behavior of `terraform init`. Alternatively it could be a separate command perhaps `terraform state import`.\r\n\n\n### References\n\n- #34150\r\n","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"No Overview Page for Backend","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.6\r\non linux_amd64\r\n```\r\n\r\n\r\n### Affected Pages\r\n\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/v1.1.x\/settings\/backends\r\n\r\n### What is the docs issue?\r\n\r\nIf you visit the above link you would find the Backend Overview Page for version `v1.1 and earlier`. If you then try to change the version to `v1.2.x`  you would notice that the **Terraform Settings > Backends > Overview** has disappeared from the sidebar. If you then try to select any other version apart from the ones already mentioned you would arrive at a **Page Not Found** error.\r\n\r\nAnother issue is that this page has been referenced in **[Terraform Study Guide > Manage State](https:\/\/developer.hashicorp.com\/terraform\/tutorials\/certification-003\/associate-study-003#manage-state) > Backend management**\r\n\r\n\r\n\r\n### Proposal\r\n\r\nAdd Overview page for Backend for version `1.2.x` and above. \r\n\r\nThere are other ways to solve this issue but I would like to understand what might be the intended behavior.  I would like to solve it if possible.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for reporting this, @kryptoblack.\r\n\r\nIf I recall correctly, it was intentional to merge the overview page with the configuration page to produce what is currently [Backend Configuration](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/configuration).\r\n\r\nHowever, when we do such things we typically add a redirect from the old location, which seems like it was missed here. If so, I imagine the fix here will be to add that redirect so that the old URL of the overview page redirects to the `configuration` page.\r\n\r\nIt's also strange that the guide you mentioned is linking directly to the archived v1.1.x documentation rather than to latest. Those guides live in a non-public repository and so that would need to be corrected in a different place; hopefully whoever on the docs team ends up fixing the missing redirect (assuming I'm right about that) can also find the source of that tutorial and update it to link to the non-version-specific URL.\r\n\r\nThanks again!","If that's the case, you might want to remove the link entirely from Study Guide as there is already a link to Backend Configuration.\n\nAlso, if the link to archive docs is unexpected behaviour you might want to check other links on the Study Guide as a lot of them refer to `1.1.x`","Indeed... unless a tutorial is specifically about the behavior of Terraform v1.1 or earlier (or any other specific version) then it should link to the unversioned URL.\r\n\r\nA guide specifically about Terraform v1.1 would be very surprising at this point, because that series is _long_ obsolete! Therefore I would guess this was probably an editing error, accidentally copying a versioned URL instead of an unversioned one when preparing the tutorial.\r\n"],"labels":["documentation","new"]},{"title":"Clarify Documentation for agent_identity remote-exec parameter.","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non darwin_arm64\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/resources\/provisioners\/connection\r\n<img width=\"655\" alt=\"Screenshot 2024-01-02 at 10 24 15\u202fPM\" src=\"https:\/\/github.com\/hashicorp\/terraform\/assets\/1696454\/b23284c3-9cff-4daa-99ae-979690ed231f\">\r\n\n\n### What is the docs issue?\n\nI was troubleshooting some ssh connectivity issues, and it was unclear what the `agent_identity` parameter actually does.  I thought perhaps terraform found a way to request a specific cert by it's fingerprint from the ssh-agent. Upon reading the source code, I saw these extremely helpful comments that explain how this feature works. https:\/\/github.com\/hashicorp\/terraform\/blob\/6114ec4390db6d419b3302a3de87a39f99a233dc\/internal\/communicator\/ssh\/provisioner.go#L540\r\n\r\nNote: I would love to see the output of the sorted list in the DEBUG console logs so that I know which identities are being tried and in what order. In high security (gov) environments, many servers fail with too many connection attempts after just one try.\r\n\r\n\r\n\n\n### Proposal\n\nI found this comment to be useful\r\n`\r\n\/\/ sortSigners moves a signer with an agent comment field matching the\r\n\/\/ agent_identity to the head of the list when attempting authentication. This\r\n\/\/ helps when there are more keys loaded in an agent than the host will allow\r\n\/\/ attempts.\r\n`\r\n\r\nRewriting it a bit and putting it in the argument reference table here: https:\/\/developer.hashicorp.com\/terraform\/language\/resources\/provisioners\/connection \r\nseems like a good idea. \r\n\r\nPerhaps something like: \r\n\r\nSets the preferred identity from the ssh agent for authentication. When Terraform gets a list of certificates from an agent, it sorts that list placing certificates matching `agent_identity` first in the list to avoid failures when there are more certificates in the list than the host will allow connection attempts.\r\n\r\n\r\n\n\n### References\n\n_No response_","comments":["Thanks for this report! Note that if you wanted to make this edit, we could put it in the queue for a review. "],"labels":["documentation","new"]},{"title":"Terraform Backend with OCI Object Storage","body":"### Terraform Version\n\n```shell\nlatest\n```\n\n\n### Use Cases\n\nA new Terraform backend utilising Oracle Cloud Infrastructure (OCI) Object Storage as the storage backend. Leveraging OCI Object Storage provides a scalable and cost-effective alternative to traditional backends, offering a robust solution for state file storage and management.\n\n### Attempted Solutions\n\nTraditional Terraform backends, such as Amazon S3 and Azure Blob Storage, have been widely used for state file storage. However, for organisations operating within the Oracle Cloud ecosystem, OCI Object Storage presents an attractive option. This custom backend aims to facilitate Terraform deployments within OCI by seamlessly integrating with OCI Object Storage.\r\n\r\n We aim to introduce OCI as a Terraform backend option, leveraging Object Storage. This implementation supports state lock  relying on object storage alone, eliminating the need for DynamoDB when using S3-backed solutions.\n\n### Proposal\n\nThe Terraform user configures the backend using the custom backend module, specifying the OCI Object Storage details.\r\n\r\n```hcl\r\n  backend \"oci\" {\r\n    bucket    = \"<mybucket>\"\r\n    object    = \"<state file name>\"\r\n    namespace = \"<Namespace>\"\r\n     \r\n    # Other OCI authentication details\r\n    tenancy_ocid         = \"<Your Tenancy OCID>\"\r\n    user_ocid            = \"<Your User OCID>\"\r\n    fingerprint          = \"<Your API Key Fingerprint>\"\r\n    private_key_path     = \"<Path to Your Private Key File>\"\r\n    region               = \"<OCI Region>\"\r\n  }\r\n}\r\n--\r\n\r\n2. During Terraform operations, the backend module interacts with OCI IAM to authenticate and obtain the necessary credentials.\r\n3. The state file is read from or written to OCI Object Storage securely.\r\n4. Access control policies ensure that only authorised users and services can interact with the state files.\r\n \n\n### References\n\n[- 32634](https:\/\/github.com\/hashicorp\/terraform\/issues\/32634)","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","> Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!\n\nHi @crw ,\nJust wanted to know If I can start the development or should I wait for approval?","I will run this past product, I didn't realize you were offering to build the backend. We have not been adding new backends in quite some time, just to set expectations, but it is always worth reviewing these policies. ","Hi @crw \r\nplease add this backend feature it will be beneficial for people who are using Oracle Cloud","I know this is not the right platform, but is there a same backend compatibility for Dell ECS Enterprise Object Storage (Dell ECS)? If its doesnt exist, can I request the same way as original requestor did here? Hopefully, its ok to ask since I am in the same boat."],"labels":["enhancement","new-backend","new"]},{"title":"Issues in connecting to 3rd party (Cloudian) S3 bucket after version 1.5.7. ","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.6\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n```\r\nterraform {\r\n  required_providers {\r\n    nsxt = {\r\n      source = \"vmware\/nsxt\"\r\n      version = \"3.3.0\"\r\n    }\r\n  }\r\n  backend \"s3\" {\r\n    endpoint                                  = \"https:\/\/myendpoint.com\"\r\n    region                                      = \"us-west-1\" #this gets ignored but still needs to confirm to the AWS standard\r\n    skip_credentials_validation   = true\r\n    skip_metadata_api_check     = true\r\n    bucket                      = \"....\"\r\n    key                         = \"....\"\r\n    skip_requesting_account_id = true\r\n  }\r\n}\r\n```\r\n### Debug Output\r\n```\r\nInitializing the backend...\r\n\r\nSuccessfully configured the backend \"s3\"! Terraform will automatically\r\nuse this backend unless the backend configuration changes.\r\n\r\nInitializing provider plugins...\r\n- Reusing previous version of vmware\/nsxt from the dependency lock file\r\n- Installing vmware\/nsxt v3.3.0...\r\n- Installed vmware\/nsxt v3.3.0 (signed by a HashiCorp partner, key ID 6B6B0F38607A2264)\r\n\r\nPartner and community providers are signed by their developers.\r\nIf you'd like to know more about provider signing, you can read about it here:\r\nhttps:\/\/www.terraform.io\/docs\/cli\/plugins\/signing.html\r\n\r\nTerraform has been successfully initialized!\r\n\r\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\r\nany changes that are required for your infrastructure. All Terraform commands\r\nshould now work.\r\n\r\nIf you ever set or change modules or backend configuration for Terraform,\r\nrerun this command to reinitialize your working directory. If you forget, other\r\ncommands will detect it and remind you to do so if necessary.\r\n```\r\n\r\n### Expected Behavior\r\n\r\nSuccessful init, (this works)\r\nNo changes should be required following a terraform plan\r\n\r\n### Actual Behavior\r\n\r\nI did try what was suggested in https:\/\/github.com\/hashicorp\/terraform\/issues\/33983.\r\nPretty much adding \"skip_requesting_account_id = true\" to my backend configuration.\r\n\r\nThis does now allow me to successfully init. However when I perform a terraform plan it does not interpret the state correctly \/ at all. It wants to redeploy the entire environment.\r\n\r\nPlan: 103 to add, 0 to change, 0 to destroy.\r\n\r\nIf I downgrade to 1.5.7, do a terraform init and the a plan it recognises the state correctly.\r\n\r\n### Steps to Reproduce\r\n\r\nterraform init\r\nterraform plan\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n- #33983","comments":["Thanks for filing this issue!"],"labels":["bug","backend\/s3","new","v1.6"]},{"title":"fmt `check` should not override `write=true` option","body":"### Terraform Version\n\n```shell\nTerraform v1.6.6\r\non linux_amd64\n```\n\n\n### Use Cases\n\nthe goal is to run `terraform fmt -check -write=true` and preserve the `-write=true` option so in our PR check we can at the same time validate terraform format and if -check returns an error we are able to create a second PR with the changes done by `-write=true`, this way we dont have to run `terraform fmt` twice\n\n### Attempted Solutions\n\nthe `-write=true` is overwritten by `-check`\r\n \r\n      - name: Terraform fmt\r\n        id: fmt\r\n        run: terraform fmt -check -write=true\r\n\r\n      - name: Create Pull Request\r\n        if: ${{ always() && (steps.fmt.outcome == 'success' || steps.fmt.outcome == 'failure') }}\r\n        uses: peter-evans\/create-pull-request@v5\r\n        with:\r\n          commit-message: terraform fmt\r\n          title: Reformat terraform files\r\n          body: Update Terraform files to canonical format using `terraform fmt`\r\n          branch: automated-terraform-fmt\r\n          base: ${{ github.event.pull_request.head.ref }}\r\n          labels: terraform, fmt\r\n          token: ${{ secrets.GITHUB_TOKEN }}\r\n\r\n      - name: Post Format Comment\r\n        if: ${{ always() && (steps.fmt.outcome == 'success' || steps.fmt.outcome == 'failure') }}\r\n        uses: GetTerminus\/terraform-pr-commenter@v3\r\n        with:\r\n          commenter_type: fmt\r\n          commenter_input: ${{ format('{0}{1}', steps.fmt.outputs.stdout, steps.fmt.outputs.stderr) }}\r\n          commenter_exitcode: ${{ steps.fmt.outputs.exitcode }}\n\n### Proposal\n\nThe proposal is that `-write=false` is only the default value for `-check` option but once `-write=true` is set by user it wont be overwritten by `terraform fmt`\n\n### References\n\n_No response_","comments":["Thanks for the proposal. It's unlikely we're going to change the documented behaviour of these options in combination, as to do so could introduce compatibility issues with existing scripts. In particular, it's not clear what the exit codes of `terraform fmt -check -write=true` would represent.\r\n\r\nYour PR check use case makes sense to me, but it doesn't look like it requires the check and the reformat to happen in one command. Is there a reason you can't run `terraform fmt -check` in the \"Terraform fmt\" step, and if that fails, run `terraform fmt` in the \"Create Pull Request\" step?","@kmoe thanks for the quick reply.\r\nthe workaround I have implemented is indeed run 2 separated steps, but this is a bit inefficient and the option to respect -write=true would be much cleaner.\r\n\r\nI believe the issues mentioned by you could be easily managed, the compatibility issue is very unlikely to happen as -write=false is already the default and would not change, with regards to exit codes it would be just a matter to handle the errors accordingly.\r\n\r\nthanks"],"labels":["enhancement","new","fmt"]},{"title":"Error: Error building ARM Config: Authenticating using the Azure CLI is only supported as a User (not a Service Principal).","body":"### Terraform Version\r\n\r\n```shell\r\n1.4.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```        - task: AzureCLI@2\r\n          displayName: 'Terraform > Initialize'\r\n          addSpnToEnvironment: true\r\n          inputs:\r\n            azureSubscription:  'serviceconnectioname'\r\n            scriptType: bash\r\n            addSpnToEnvironment: true\r\n            useGlobalConfig: true\r\n            scriptLocation: inlineScript          \r\n            inlineScript: |\r\n                terraform -chdir=${{ parameters.workdir }} init -backend-config=$(backend.secureFilePath)\r\n\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nterraform initialize should work without any error\r\n\r\n### Expected Behavior\r\n\r\nterraform initialize should work without any error\r\n\r\n### Actual Behavior\r\n\r\n\u2502 Error: Error building ARM Config: Authenticating using the Azure CLI is only supported as a User (not a Service Principal).\r\n\u2502 \r\n\u2502 To authenticate to Azure using a Service Principal, you can use the separate 'Authenticate using a Service Principal'\r\n\u2502 auth method - instructions for which can be found here: https:\/\/registry.terraform.io\/providers\/hashicorp\/azurerm\/latest\/docs\/guides\/service_principal_client_secret\r\n\u2502 \r\n\u2502 Alternatively you can authenticate using the Azure CLI by using a User Account.\r\n\r\n### Steps to Reproduce\r\n\r\n        - task: AzureCLI@2\r\n          displayName: 'Terraform > Initialize'\r\n          addSpnToEnvironment: true\r\n          inputs:\r\n            azureSubscription:  'xxx'\r\n            scriptType: bash\r\n            addSpnToEnvironment: true\r\n            useGlobalConfig: true\r\n            scriptLocation: inlineScript          \r\n            inlineScript: |\r\n                terraform -chdir=${{ parameters.workdir }} init -backend-config=$(backend.secureFilePath)\r\n\r\n\r\n### Additional Context\r\n\r\nHi Team,\r\n\r\nWe are trying to authenticate azure using azurecli with service connection, but the connectivity failed with above error.\r\n\r\n### References\r\n\r\n_No response_","comments":["@krupavanigundraju \r\nI faced same issue with Azure Pipelines and did some testing.\r\n\r\n**My Observation :**\r\n- AzureRm Provider's Backend supports only User Authentication now from AZ CLI, which means we have login via 'az login'. \r\n- Service Principal based authentication works perfectly without backend config. \r\n\r\nThe Work Around I am using is declaring the secrets using environment variables\r\nDo You have any other suggestions?","I noticed the same behavior when I set up the login process using the 'Login With OpenID Connect (OIDC)' method, as described in [Azure Login GitHub Marketplace Action](https:\/\/github.com\/marketplace\/actions\/azure-login). This login variant is marked as recommended.\r\n\r\nHowever, I am still trying to fully understand this behavior. I'm uncertain if it's due to a technical limitation that I've overlooked, or if it's a feature not yet implemented but planned for future release. Information about a timeline for this would be helpful.\r\n\r\nGenerally speaking, logging in via OIDC and using a service principle is highly advantageous, as it eliminates the need to persist secrets.","I am also seeing this issue this morning. As of Friday the same pipeline was able to authenticate with OIDC in GitHub actions but this morning I'm seeing this issue. It appears to be a breaking change in the Azure provider perhaps which released version [v3.87.0](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/releases) on Friday but I'm not sure of the root cause yet.","I also encountered this in github actions quite a while ago. But I opened a issue in the azure provider. Does this need to be fixed in terraform or the provider?\r\n\r\nLink: https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/issues\/22034\r\n\r\nUsing az auth with service principals is allowed for creating ressources but not for backend config. This is really counter intuitive and requires additional configuration which should not be needed.\r\n"],"labels":["bug","backend\/azure","new","v1.4"]},{"title":"Support azurerm backend state encryption with keyvault secret","body":"`azurerm` is currently one of the official supported backends. The statefile is stored as a block blob in plain-text in a container within an Azure storage account. It contains sensitive information of many components managed by Terraform. \r\n\r\nThis PR aims to make `azurerm` backend state more secure by encrypting the remote state with a customer managed key stored in an Azure keyvault. `azurerm` backend supports `use_azuread_auth` option to take advantage of AD authentication, this option `key_vault_secret_uri` uses the same AD authentication to build keyvault secret clients to fetch `AES-256` secret from the keyvault, perform `sha2` of the secret key, encode both `key` & `sha2` with `base64` & pass them as headers to Azure API so that the statefile could be encrypted at server side. \r\n\r\nBlob CMK encryption references :: [Provide an encryption key on a request to Blob storage](https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/encryption-customer-provided-keys#request-headers-for-specifying-customer-provided-keys)\r\n\r\nThis is very similar to `sse_customer_key` option , part of `s3` backend. ","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34455) <br\/>All committers have signed the CLA.","Thanks for this submission. The azure provider team at HashiCorp is the codeowner for the backend, I will notify them of this submission. Thanks!"],"labels":["enhancement","backend\/azure"]},{"title":"Provide a clear follow up status for each resources at the end of the log","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.2\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nFollowing if each resources have been correctly `created`|`modified`|`deleted `can be tedious in case of deployment with a big amount of resources.\r\nMoreover, if some of the action works but others fails it's sometimes difficult to find out which ones have worked and other not.\r\n\r\nIf you take the following log for instance, it's very difficult to follow up each resources status at the end of the apply\r\n\r\n## Initial log (only the apply part)\r\n\r\nhttps:\/\/gist.github.com\/remi-f-artelia\/483cce5f74cf11fd160c44d310d5029a\r\n\r\n\r\n\r\n### Attempted Solutions\r\n\r\nFor now I just tried to parsed the text output with a powershell script for a one shot use case so my script is not robust enough to share it there.\r\n\r\nI saw that it's possible to have a json output so maybe it would be the solution to avoid parsing the text like I did.\r\nHowever I saw in these issues [34438](https:\/\/github.com\/hashicorp\/terraform\/issues\/34438), [33719](https:\/\/github.com\/hashicorp\/terraform\/issues\/33719) that these options do not provide json only and are not working for every commands.\r\nAnd [the doc](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format) says that's not possible to use it for `apply` or `destroy` commands (which would be my use case)\r\n\r\nAnd the [official documentation](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/environment-variables) do not talk about these options (`TF_LOG_PROVIDER` or `TF_LOG_CORE`) so I'm confused on how to have the idea to use them...\r\n\r\n**Edit**: I found the info [here](https:\/\/developer.hashicorp.com\/terraform\/internals\/debugging) but it's not very clear on what these can offer\r\n\r\n> Logging can be enabled separately for Terraform itself and the provider plugins using the TF_LOG_CORE or TF_LOG_PROVIDER environment variables. These take the same level arguments as TF_LOG, but only activate a subset of the logs.\r\n\r\n### Proposal\r\n\r\nCreate a final output with a table showing clearly the action attempted per resource and the final status (even if it fails in the middle) for this resource with at least the following keys\r\n\r\n```json\r\n{\r\n  resource_type = <resource_type>\r\n  resource_name = <resource_name>\r\n  resource_id   = <resource_id>\r\n  initial_state = <resource_state>\r\n  final_state   = <final_state>\r\n}\r\n```\r\n\r\n## Suggestion of output\r\n\r\nhttps:\/\/gist.github.com\/remi-f-artelia\/004670c3c43120e04becbf2eecbb6254\r\n\r\n\r\n### References\r\n\r\n_No response_","comments":["Nice !","Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Is there any progress on this proposal ?\r\nThanks","@remi-f-artelia No. It is possible to do what you describe, but would be a design decision the core team would need to make. There is some other functionality that is being developed that does stream full objects as it applies, so it may make sense in the future to accept this proposal to make the CLI overall more consistent. That said, there is no active work going on for this functionality."],"labels":["enhancement","new"]},{"title":"Bump lib\/pq from v1.10.3 to v1.10.4","body":"Bump pq to make it possible to build terraform on Android.\r\n\r\nhttps:\/\/github.com\/lib\/pq\/commit\/6a102c04ac8dc082f1684b0488275575c374cb4c","comments":["Fascinating... Can you say more about why you want to build Terraform on Android?\r\n\r\nLike @apparentlymart in https:\/\/github.com\/hashicorp\/terraform\/pull\/19529, I'm not against small changes that make Terraform work in Android (and the lib\/pq dependency is rather out of the way, being only used in the Postgres backend), but should emphasise that Android is not a _supported platform_ for building or running Terraform.","Just FYI, we'd probably want to also run this past the codeowner (Remy) if we decide to review this, just to make sure it doesn't introduce any unintended consequences for the backend. I'm sure this is obvious, but just wanted to make this note to remind myself if this moves forward.","@kmoe I also am not expecting that terraform will officially support Android. But, because this bumping seems to be tiny to me, I've thought it is a doable thing. Of course, only when it breaks nothing.\r\n\r\nMy motivation is Termux, an Android terminal emulator. Because of the limitations of the permissions of a normal Android device. It is required to build the go binary on Android to run it for Termux.\r\n(I've recently setup kubectl, helm, aws cli, etc on my Android phone and Terraform remains)\r\n\r\nAnd I did not know there were someone tried to build terraform on Android. It is interesting, thanks for the link!","(fwiw, I just noticed I have to build each provider also on Android so that it is nearly impossible to run Terraform on Android out of the box. At least, `terraform fmt` works)","Interesting, thanks @lens0021.\r\n\r\nYes, Terraform providers are not compiled for `android_arm64` or indeed any architecture not found in the list at e.g. https:\/\/releases.hashicorp.com\/terraform\/1.7.0-rc1\/. If you are using `android_arm64` I'd recommend trying to pretend it's `linux_arm64` instead (like https:\/\/github.com\/cli\/cli\/issues\/5723#issuecomment-1141469170), otherwise you're going to have to try to compile the provider binaries as well.\r\n\r\nWe've had a couple of people report using Terraform on Termux (https:\/\/github.com\/hashicorp\/terraform\/issues\/26235), so it may still be possible. ","We've recently changed how we represent dependencies so we can more easily see which modules are for a specific backend vs. which are for Terraform CLI\/Core itself. That made this PR conflicted, so I've adapted it to work with the new layout.\r\n\r\nHowever, since this change is exclusively for the `pg` backend I would still like to wait for the maintainer's approval before we merge, since I don't have a suitable setup to test this backend myself.\r\n\r\nThanks!\r\n"],"labels":["enhancement","backend\/pg"]},{"title":"Removed block - instance support","body":"### Terraform Version\n\n```shell\nv1.7.0-beta1\n```\n\n\n### Use Cases\n\nAll of our internal modules accept more than one input either in the form of a list of objects or a map of objects. If a user accidentally deletes an object and ends up with state drift or there is a provider issue leading to some form of state drift (this is often apparent with some resources in GCP) then I should be able to remove that singular entity. \n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\n```hcl\r\nremoved {\r\n  from = module.gcs.google_storage_bucket.this[\"my-single-bucket\"]\r\n  lifecycle {\r\n    destroy = false\r\n  }\r\n}\r\n\r\nmodule \"gcs\" {\r\n  source  = \"drandell\/cloud-storage\/google\"\r\n  version = \"1.0.3\"\r\n\r\n  buckets = {\r\n    #\"my-single-bucket\": {}, ## Lets presume this entity was deleted in GCP by a user and now leads to state drift\r\n    \"my-other-bucket\": {}\r\n  }\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Hi @drandell,\r\n\r\nThanks for filing the issue! A `removed` block implies that a target \"resource\" has been removed from the configuration. There are a few problems with identifying individual instances via a removed block, but I think a primary concern is that there is still a `resource` block managing that resource, which may add conflicting instance keys at a later time, or have other conflicting `lifecycle` parameters defined in ways which can't be resolved. There's also problems based on the internal representation, where the necessary metadata is tacked on a per-resource basis, rather than per-instance.\r\n\r\nFor control of individual instances, it's more likely that there will be additional `lifecycle` options for that resource's block, rather than a removed block, since existing resources instances are always controlled by the resource block. You can also see some discussion in an older version of the issue #15485, about how we might go about \"forgetting\" individual instances.\r\n\r\nI would note however, that your example should not require a `removed` block. If the object was deleted in GCP, and you removed the instance key from the configuration, the provider should indicate to Terraform that the instance no longer exists resulting in no changes in the plan.\r\n","I am adding this here, and it was in its own feature request but has been deemed as possibly related to this...\r\n\r\nThe idea here, is not to support telling Terraform that a resource has already been destroyed, but to make it possible to tell Terraform to simply delete the resource from the state during a remove workflow instead of trying to actually delete it. This would be useful when destroying a higher-level object is more reliable and will result in the destruction of the other object anyhow (although it could also allow for abandoning an object as well).\r\n\r\n---\r\n\r\n### Use Cases\r\n\r\nThe core idea is to create a way to tell Terraform to remove a resource from the state file during a `destroy` workflow instead of contacting the owning API to delete the object.\r\n\r\nThis would make it possible to handle nested objects, like `kubernetes_namespace` resources, that exist inside a Kubernetes cluster, which you will destroy in the same workflow, but you do not need Terraform to remove via the owning API.\r\n\r\n\r\n### Attempted Solutions\r\n\r\nHashicorp would recommend that people use a single Terraform workflow to spin up a k8s cluster and then install things into that cluster in another Terraform workflow.\r\n\r\nHowever, there are many times that at least some bootstrapping will occur in the initial Terraform workflow. This fix would allow users to quickly identify resources that need not be destroyed via their API.\r\n\r\n\r\n### Proposal\r\n\r\n```hcl\r\nlifecycle { \r\n  # This would cause Terraform to remove the resource from the state file instead of calling the owning API to delete.\r\n  state_only_destroy = true\r\n}\r\n```\r\n","I believe `state_only_destroy = true` might not be specific enough compared to implementing full support for instances in removed blocks. That is, it would not allow us to specify that an individual, specific resource instance - but _not_ any other instances - should be \"forgotten\" instead of destroyed.\r\n\r\nCurious, why does this work for `import` and `moved` blocks but not for `removed` blocks? It seems natural and good for DX to support symmetry here."],"labels":["enhancement","removed"]},{"title":"How to force json only output from terraform","body":"### Terraform Version\n\n```shell\nterraform --version\r\nTerraform v1.6.4\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\n\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/denis256\/ed30193e5bc82846a0da130f1429ff09\n\n### Expected Behavior\n\nduring execution of \"TF_LOG_PROVIDER=json TF_LOG_CORE=json TF_LOG=json terraform init -no-color\" \r\n\r\nThe produced output contains JSON and plain text which make automated parsing of output more complicated\n\n### Actual Behavior\n\nAll output is JSON\n\n### Steps to Reproduce\n\n1. TF_LOG_PROVIDER=json TF_LOG_CORE=json TF_LOG=json terraform init -no-color \n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nI think this may be an enhancement request, but I am going to leave it as a bug for now as I am not sure what the intended behavior is in this case. Is it possibly sending JSON to STDOUT and then normally-formatted error messages to STDERR? \r\n\r\nThis issue may be similar to https:\/\/github.com\/hashicorp\/terraform\/issues\/33719 once the actual intended behavior is clarified.\r\n\r\nThanks again!","The JSON-formatted messages are sent to stderr, and the others to stdout. So for example, you can run the following to see only JSON messages:\r\n```\r\nTF_LOG=json terraform init -no-color > \/dev\/null\r\n```\r\n\r\nYou don't need the `TF_LOG_CORE` or `TF_LOG_PROVIDER` env vars if you're also using `TF_LOG`. The former two variables control different subsets of the logs.\r\n\r\nThis isn't a bug, but I'm also not sure exactly where it's documented. We may need to improve documentation. Tagging this in case."],"labels":["documentation","explained"]},{"title":"Produce more detailed diagnostics from failing test conditions","body":"### Terraform Version\r\n\r\n```shell\r\nv1.6\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nThe run condition expressions used in tests are going to primarily rely on equality for comparisons, and usually only between 2 values. When the values consist of complex types, the error diagnostics attempt to simplify the output by summarizing the references in the expression, like `is object with 4 attributes`, or `is tuple with 3 elements`. This summary doesn't usually help in the context of a test failure, since the types should be known, and the user is concerned with why the exact values in the expression aren't matching.\r\n\r\n### Attempted Solutions\r\n\r\nIt is possible to break down the expression into multiple assertions, but it would be convenient to enable direct comparison of more complex objects.\r\n\r\n### Proposal\r\n\r\nExtend the diagnostic format for test failures, which can print complete values or even create smart diffs based on those values in the UI.\r\n\r\n### References\r\n\r\n#34425","comments":["This will be very useful, I just face this scenario it was very hard to know what the issue was :("],"labels":["enhancement","terraform test"]},{"title":"failed map indexing - error report could write index and map values to output (facilitate debugging)","body":"### Terraform Version\n\n```shell\n1.5.7, 1.6.x\n```\n\n\n### Use Cases\n\nWhile several cases where terraform aborts do indeed show the content of variables to help with debugging, this case does not.\r\n\r\n```\r\n\u2502   on ..\/..\/some-module\/complex-db-settings.tf line 61, in locals:\r\n\u2502   61:                   database    = local.pg_db_spec[db_k]\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.pg_db_spec is object with 6 attributes\r\n\u2502 \r\n\u2502 The given key does not identify an element in this collection value.\r\n```\r\nIt would be very usefull to have `db_k` being written to the output, as happens in other cases.\r\nAlso great (but not as critical) would be to have the `local.pg_db_spec` being output, as happens in other (fewer) cases.\r\n\r\nI am not sure how else to assist in targetting this, but this is a case where we are using a loop variable - in this case, `db_k` is a variable in a nested list compreension (for loops)\r\n```\r\n  pg_grant__users__schema = {\r\n    for k, v in flatten([\r\n      [\r\n        # users with schema-limited perms\r\n        for u_k, u_spec in local.pg_users_spec_schema : [\r\n          for db_k, db_spec in u_spec.db : [\r\n            [\r\n              {\r\n                # allow user to connect to the DB for which it has schema permissions\r\n                key = format(\"%s:%s\/\/%s\", \"one.db\", db_k, u_k)\r\n                val = {\r\n                  role        = u_spec.name # module.aiven_db_users.aiven_user_names[u_k]\r\n\r\n                  object_type = \"database\"\r\n                  database    = local.pg_db_spec[db_k]\r\n                  schema      = null\r\n\r\n                  privileges  = local.PG_PRIVILEGES.database[\"read\"].priv\r\n                  with_grant_option = try(local.PG_PRIVILEGES.database[u_spec.perms].grant, null)\r\n                }\r\n              },\r\n            ],\r\n...\r\n```\n\n### Attempted Solutions\n\nNone we could find\n\n### Proposal\n\nLocate the cases where this error is generated, and write the values of the index that is not found, as well as the value of the map\/list being indexed.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Allowing to hide some changes in plan","body":"### Terraform Version\n\n```shell\nv1.6.6\n```\n\n\n### Use Cases\n\nHello\r\nI would like to have the ability to not show some particular changes in terraform plan. The use case would be, in my particular case, to ignore changes in tags, but update those. Example\r\n1. I have already state with let's say 10 taggable resources.\r\n2. I am changing one of those resources with new functionality.\r\n3. In addition to this change I need to update the version tag\r\n4. In Plan I see a lot of changes because I have 1 \"business\" change + 10 tags changes.\r\n\r\nIt would be easier for me to look at the plan if I had the opportunity to \"hide\" tags changes, and not have \"noise\" that in other 9 resources also needs an update.\r\n\r\nI think it may be not following terraforms idea of how to use the plan but from my user experience it would be useful\n\n### Attempted Solutions\n\nMaybe we could extend somehow lifecycle, to mark that for this particular resource, to update tags but not show that in plan.\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Adds autocomplete flag options to plan and apply","body":"Fixes - https:\/\/github.com\/hashicorp\/terraform\/issues\/34396\r\n\r\nTerraform plan and apply is possibly the most popular commands. This PR adds auto-complete for flag options.\r\nWhen user types `terraform apply` \/ `terraform plan` followed by `-` the auto-complete flag options are visible to him.\r\n\r\nAuto complete flag options created in this PR is from help section of the command.\r\n\r\n## Target Release\r\n\r\n1.6.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\nENHANCEMENTS\r\n\r\n```\r\nasheshvidyut@asheshvidyut-H2GX766V9T ~\/terraform (gh-issue-34396) \u00bb .\/bin\/terraform apply -                                                     130 \u21b5\r\n-auto-approve      -compact-warnings  -input             -lock-timeout      -parallelism       -state-out\r\n-backup            -destroy           -lock              -no-color          -state\r\n```\r\n\r\n```\r\nasheshvidyut@asheshvidyut-H2GX766V9T ~\/terraform (gh-issue-34396) \u00bb .\/bin\/terraform plan -                                                      130 \u21b5\r\n-compact-warnings     -generate-config-out  -lock-timeout         -parallelism          -replace              -var\r\n-destroy              -input                -no-color             -refresh              -state                -var-file\r\n-detailed-exitcode    -lock                 -out                  -refresh-only         -target\r\n```\r\n\r\n","comments":["Thanks for this submission! I'll add it to the triage queue. "],"labels":["enhancement"]},{"title":"TestJSONHook_create fails for 1.6.6","body":"### Terraform Version\r\n\r\n```shell\r\n1.6.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nNone, test fails\r\n\r\n### Debug Output\r\n\r\n```\r\n--- FAIL: TestJSONHook_create (0.01s)\r\n    hook_json_test.go:184: unexpected output on line 4:\r\n          map[string]any{\r\n          \t\"@level\": string(\"info\"),\r\n          \t\"@message\": strings.Join({\r\n          \t\t\"test_instance.boop: Still creating... [\",\r\n        - \t\t\"1\",\r\n        + \t\t\"2\",\r\n          \t\t\"0s elapsed]\",\r\n          \t}, \"\"),\r\n          \t\"@module\": string(\"terraform.ui\"),\r\n          \t\"hook\": map[string]any{\r\n          \t\t\"action\":          string(\"create\"),\r\n        - \t\t\"elapsed_seconds\": float64(10),\r\n        + \t\t\"elapsed_seconds\": float64(20),\r\n          \t\t\"resource\":        map[string]any{\"addr\": string(\"test_instance.boop\"), \"implied_provider\": string(\"test\"), \"module\": string(\"\"), \"resource\": string(\"test_instance.boop\"), ...},\r\n          \t},\r\n          \t\"type\": string(\"apply_progress\"),\r\n          }\r\nFAIL\r\n```\r\n\r\n### Expected Behavior\r\n\r\nAll tests are green.\r\n\r\n### Actual Behavior\r\n\r\nTest `TestJSONHook_create` fails.\r\n\r\n### Steps to Reproduce\r\n\r\n```\r\ngit checkout v1.6.6\r\ngo mod download\r\ngo build\r\ngo test -mod=readonly .\/...\r\n```\r\n\r\n### Additional Context\r\n\r\nGolang version 1.21.5.\r\n\r\nI'm the maintainer of the Arch Linux terraform package, this blocks the 1.6.6 release.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @hashworks,\r\n\r\nThanks for filing the issue! That failure is unrelated to anything in the v1.6.6 release, and only due to a flaky or overly time-sensitive test fixture. The test is attempting to accurately record the output of a concurrent process over time, and if that concurrent process is slowed slightly due to runtime scheduling delays, which would usually because of being run on a more resource constrained system, the output won't exactly align any longer. \r\n\r\nWhile it's something we'll want to look into to make more robust, because a fix would not change any functional code it's probably not going to warrant another patch release.","The output indicates that the following message is being emitted twice:\r\n\r\n```\r\n\t\t{\r\n\t\t\t\"@level\":   \"info\",\r\n\t\t\t\"@message\": \"test_instance.boop: Still creating... [10s elapsed]\",\r\n\t\t\t\"@module\":  \"terraform.ui\",\r\n\t\t\t\"type\":     \"apply_progress\",\r\n\t\t\t\"hook\": map[string]interface{}{\r\n\t\t\t\t\"action\":          string(\"create\"),\r\n\t\t\t\t\"elapsed_seconds\": float64(10),\r\n\t\t\t\t\"resource\":        wantResource,\r\n\t\t\t},\r\n\t\t},\r\n```\r\n\r\nso perhaps during the second \"time jump\" on line 73, the progress goroutine is not actually advancing.\r\n\r\nWorks fine on a well-resourced `Linux 6.6.3-arch1-1`. Curious as to how resource-constrained the build machine is to cause this. Please post the spec so this can be reproduced.","I changed this to `enhancement` since it seems to be more about optimizing this test to run in a resource-constrained environment which, while a reasonable thing to do, does not reflect a bug in the actual product or even in the test suite as it is run by the official build process. If this is in error, please let me know. ","The test relies on waiting for the goroutine to do its operation. Since `time.Sleep` is never a proper synchronization primitive, that delay always has the chance of failure. If the [heartbeat goroutine](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/command\/views\/hook_json.go#L89) is delayed slightly more than the sleep, the output will be recorded at the wrong time. Sleeping for 1500 microseconds after the first channel operation in the heartbeat goroutine will cause the exact output shown above.\r\n\r\nUnfortunately testing this type of asynchronous UI code is always quite awkward, and settling for `time.Sleep` is a trade-off in convenience vs adding even more test hooks and complexity to make the tests resilient to timing issues. There may be some way to catch the heartbeat output to step the test without a sleep, or we will have to add another test hook to do so.","What I am able to understand, can we use [`sync.WaitGroup`](https:\/\/gobyexample.com\/waitgroups) or maybe an action-based synchronization logic?\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/039cced8aec491d2f6bbcc20b1d47e82cdf70686\/internal\/command\/views\/hook_json_test.go#L148\r\n\r\n\r\nI would love to work on this, I took a pause for a while from open-source. =)","A unit test run on GHA ended up reproducing this: https:\/\/github.com\/hashicorp\/terraform\/actions\/runs\/7276612377\/job\/19826811164?pr=34432","> A unit test run on GHA ended up reproducing this: https:\/\/github.com\/hashicorp\/terraform\/actions\/runs\/7276612377\/job\/19826811164?pr=34432\r\n\r\nSo there's a mismatch between the expected and actual output values for the elapsed time for the 10s and the 20s? Am I right?\r\n\r\n","> Curious as to how resource-constrained the build machine is to cause this. Please post the spec so this can be reproduced.\r\n\r\n@kmoe The build machine is not resource constrained (5800X, 64GB RAM), it might be related to the Arch Linux Build System that was used. You can find the respective PKGBUILD [here](https:\/\/gitlab.archlinux.org\/archlinux\/packaging\/packages\/terraform\/-\/blob\/main\/PKGBUILD?ref_type=heads).\r\n\r\nOn an Arch Linux system:\r\n\r\n```bash\r\npacman -Syu git devtools\r\ngit clone https:\/\/gitlab.archlinux.org\/archlinux\/packaging\/packages\/terraform\r\ncd terraform\/\r\nsed -i '\/-skip TestJSONHook_create\/d' PKGBUILD\r\nextra-x86_64-build\r\n```\r\n\r\nEDIT: Huh, can't reproduce this anymore. It could be that I was building multiple things at the time I ran into this."],"labels":["enhancement","good first issue","explained","tests"]},{"title":"Unpacking arguments for module calls","body":"### Terraform Version\n\n```shell\n1.6\n```\n\n\n### Use Cases\n\nSimplify module calls that have huge amount of input variables. Of course you can refactor the module to just take 1 map as input variable but then you loose the validation and default values from input variable declarations.\r\n\r\nThis is an example how in Python you can pass multiple arguments by just unpacking a dictionary with `**` operator.\r\n\r\n```python\r\ndef my_func(name, age, is_enabled):\r\n    print(name, age, is_enabled)\r\n\r\nconfig = {\r\n    \"name\": \"John\",\r\n    \"age\": 22,\r\n    \"is_enabled\": True\r\n}\r\n\r\nmy_func(**config)\r\n```\n\n### Attempted Solutions\n\n```tf\r\nlocals {\r\n    config = {\r\n        name = \"john\"\r\n        age = 30\r\n        is_enabled = true\r\n    }\r\n}\r\n\r\nmodule \"test\" {\r\n    source    = \".\/module\"\r\n    arguments = **local.config\r\n}\r\n\r\n# instead of:\r\n\r\nmodule \"test\" {\r\n    source     = \".\/module\"\r\n    name       = local.config.name\r\n    age        = local.config.age\r\n    is_enabled = local.config.is_enabled\r\n}\r\n```\n\n### Proposal\n\n```tf\r\nmodule \"test\" {\r\n    source    = \".\/module\"\r\n    arguments = **local.config\r\n}\r\n```\r\n\r\nOr just have the magical `arguments` (or similar parameter) that accepts complex object and under the hood unpacks the object and passes the values to matching input variables declared in the module.\r\n\r\n```tf\r\nmodule \"test\" {\r\n    source    = \".\/module\"\r\n    arguments = local.config\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"CLI option autocompletion","body":"### Terraform Version\n\n```shell\nTerraform v1.6.5\r\non linux_amd64\n```\n\n\n### Use Cases\n\nImprove UX (for users of Terraform CLI (could this qualify as DX - Developer Experience?))\n\n### Attempted Solutions\n\nI have attempted to convince myself this feature wasn't necessary, with limited success.\n\n### Proposal\n\nCurrently, the Terraform CLI autocomplete feature only supports subcommands. Expanding this functionality to include options such as `-auto-approve`, `-upgrade`, `-out` etc. would improve usability tremendously.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Simplify changing module input variables and output values","body":"### Terraform Version\n\n```shell\nlatest\n```\n\n\n### Use Cases\n\nI am a module developer who wants to make changes to my input variables or output fields. Those changes may be...\r\n- a different name\r\n- a different data structure\r\n\r\nCurrently I have no way of directing my users that a change is coming or that a change has occurred.\n\n### Attempted Solutions\n\ncreate a breaking change version and document how to upgrade\n\n### Proposal\n\nIdeally explicit warning (deprecation notices) and errors (post deprecation) would be possible. For example, imagine I am change my input variable from `var.cidr` to `var.cidr_block` to align with a particular resource naming scheme. Today I would:\r\n\r\n1. create a 2nd parameter\r\n2. use a `local` to use either\/or\r\n3. write in the legacy parameter `description` that the param is being deprecated.\r\n\r\nOnce the breaking change occurs (removing the legacy param), the user experience is just an error \"var.cidr does not exist\" - or something like that. How excellent would it be to provide a pre-defined error to direct them. This would require tracking the relationship between 2 variables but, in theory, this could just be done in the statefile.\r\n\r\nA couple other options or ideas come to mind...\r\n\r\n1. Allow `moved {}` to adjust variable names and to warn based on the prior name\r\n5. Add a `deprecated {}` to variable declarations to allow warning users ahead of time of a planned breaking change\r\n6. If variable values are stored in state (im not sure if they are) and a variable has been marked as deprecated and is removed (thus a breaking change), allow customizing the error message\r\n\r\nI believe most of the same ideas can also apply to `outputs` which have the same issue\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Fix resolution message for S3\/DynamoDB state checksum mismatch","body":"The current resolution message incorrectly suggests updating the DynamoDB value to the value already stored there, rather than to the calculated checksum of the S3 state file.\r\n\r\n## Draft CHANGELOG entry\r\n\r\n### BUG FIXES\r\n- Fixed the digest value displayed for DynamoDB\/S3 state checksum mismatches.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34387) <br\/>All committers have signed the CLA.","@jbardin Could someone please review this?","Apologies, somehow I missed this (GitHub notifications seem to be a little flaky). I've notified the AWS provider team. ","@crw Thanks. Is there any update yet, I can't see any reviewers assigned to the PR?"],"labels":["enhancement","backend\/s3"]},{"title":"Unable to launch powershell script as a background process on EC2 instance in AWS using terraform \"remote-exec\" provisioner","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.3\r\non windows_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.9.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nresource \"aws_instance\" \"training_node\" {\r\n  instance_type          = \"t3.small\"\r\n  ami = \"ami-abcdefgxxxxx\"\r\n  key_name               = \"trainingNode-keyPair\"\r\n  vpc_security_group_ids = [aws_security_group.training_sg.id]\r\n  subnet_id              = aws_subnet.training_public_subnet.id\r\n  source_dest_check      = false\r\n\r\n  root_block_device {\r\n    volume_size = 30\r\n  }\r\n\r\n  provisioner \"file\" {\r\n    source = \".\/looper1.ps1\"\r\n    destination = \"C:\/Users\/Administrator\/looper1.ps1\"\r\n  }\r\n\r\n  provisioner \"file\" {\r\n    source = \".\/looper2.ps1\"\r\n    destination = \"C:\/Users\/Administrator\/looper2.ps1\"\r\n  }\r\n\r\n  # Unblock all files so that you can execute them\r\n  provisioner \"remote-exec\" {\r\n    inline = [\r\n      \"powershell \\\"Get-Childitem C:\\\\Users\\\\Administrator -Recurse | Unblock-File\\\"\"\r\n    ]\r\n  }\r\n\r\n  provisioner \"remote-exec\" {\r\n    inline = [\r\n      \"start \/b \\\"looper1\\\" \\\"powershell.exe\\\" \\\"C:\\\\Users\\\\Administrator\\\\looper1.ps1\\\"\",\r\n    ]\r\n  }\r\n\r\n  provisioner \"remote-exec\" {\r\n    inline = [\r\n      \"powershell.exe Start-Process -FilePath \\\"powershell\\\" -ArgumentList C:\\\\Users\\\\Administrator\\\\looper2.ps1\",\r\n    ]\r\n  }\r\n\r\n  connection {\r\n    type = \"ssh\"\r\n    port = \"22\"\r\n    target_platform = \"windows\"\r\n    user = \"Administrator\"\r\n    host = self.public_ip\r\n    timeout = \"5m\"\r\n    private_key = file(\"trainingNode-keyPair.pem\")\r\n  }\r\n\r\n  tags = {\r\n    Name = \"training-node\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\n```\r\naws_instance.training_node: Still creating... [3m30s elapsed]\r\n2023-12-08T14:05:57.312-0500 [TRACE] dag\/walk: vertex \"root\" is waiting for \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\"\r\n2023-12-08T14:05:57.385-0500 [TRACE] dag\/walk: vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\" is waiting for \"aws_instance.training_node\"\r\n2023-12-08T14:05:58.272-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/Users\/Administrator\r\n2023-12-08T14:05:58.330-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:05:58.331-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:05:58.517-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:05:58.517-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:05:58.549-0500 [ERROR] scp stderr: \"Sink: C0644 219 looper1.ps1\\r\\n\"\r\n2023-12-08T14:05:58.549-0500 [TRACE] applyProvisioners: provisioning aws_instance.training_node with \"file\"\r\n2023-12-08T14:05:58.549-0500 [TRACE] terraform.contextPlugins: Initializing provisioner \"file\" to read its schema        \r\n2023-12-08T14:05:58.557-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\n2023-12-08T14:05:58.557-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\naws_instance.training_node: Provisioning with 'file'...\r\n2023-12-08T14:05:58.559-0500 [INFO]  using private key for authentication\r\n2023-12-08T14:05:58.559-0500 [DEBUG] Connecting to 3.99.142.220:22 for SSH\r\n2023-12-08T14:05:58.579-0500 [DEBUG] Connection established. Handshaking for user Administrator\r\n2023-12-08T14:05:58.869-0500 [DEBUG] starting ssh KeepAlives\r\n2023-12-08T14:05:58.891-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:05:59.001-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/Users\/Administrator\r\n2023-12-08T14:05:59.102-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:05:59.103-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:05:59.205-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:05:59.206-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:05:59.233-0500 [ERROR] scp stderr: \"Sink: C0644 219 looper2.ps1\\r\\n\"\r\n2023-12-08T14:05:59.235-0500 [TRACE] applyProvisioners: provisioning aws_instance.training_node with \"remote-exec\"       \r\n2023-12-08T14:05:59.235-0500 [TRACE] terraform.contextPlugins: Initializing provisioner \"remote-exec\" to read its schema \r\n2023-12-08T14:05:59.236-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\n2023-12-08T14:05:59.237-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\naws_instance.training_node: Provisioning with 'remote-exec'...\r\n2023-12-08T14:05:59.241-0500 [INFO]  using private key for authentication\r\naws_instance.training_node (remote-exec): Connecting to remote host via SSH...\r\naws_instance.training_node (remote-exec):   Host: 3.99.142.220\r\naws_instance.training_node (remote-exec):   User: Administrator\r\naws_instance.training_node (remote-exec):   Password: false\r\naws_instance.training_node (remote-exec):   Private key: true\r\naws_instance.training_node (remote-exec):   Certificate: false\r\naws_instance.training_node (remote-exec):   SSH Agent: false\r\naws_instance.training_node (remote-exec):   Checking Host Key: false\r\naws_instance.training_node (remote-exec):   Target Platform: windows\r\n2023-12-08T14:05:59.253-0500 [DEBUG] Connecting to 3.99.142.220:22 for SSH\r\n2023-12-08T14:05:59.275-0500 [DEBUG] Connection established. Handshaking for user Administrator\r\naws_instance.training_node (remote-exec): Connected!\r\n2023-12-08T14:05:59.526-0500 [DEBUG] starting ssh KeepAlives\r\n2023-12-08T14:05:59.533-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:05:59.633-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:05:59.778-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:05:59.779-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:05:59.802-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:05:59.803-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:05:59.832-0500 [ERROR] scp stderr: \"Sink: C0644 74 terraform_1912908404.cmd\\r\\n\"\r\n2023-12-08T14:05:59.832-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:05:59.853-0500 [DEBUG] starting remote command: C:\/windows\/temp\/terraform_1912908404.cmd\r\n\r\naws_instance.training_node (remote-exec): administrator@EC2AMAZ-KO02ITT C:\\Users\\administrator>powershell \"Get-Childitem \r\nC:\\Users\\Administrator -Recurse | Unblock-File\"\r\n2023-12-08T14:06:00.794-0500 [DEBUG] remote command exited with '0': C:\/windows\/temp\/terraform_1912908404.cmd\r\n2023-12-08T14:06:00.802-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:00.825-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:06:00.931-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:06:00.934-0500 [DEBUG] Copying input data into temporary file so we can read the length\r\n2023-12-08T14:06:00.942-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:06:01.056-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:06:01.057-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:06:01.089-0500 [ERROR] scp stderr: \"Sink: C0644 0 terraform_1912908404.cmd\\r\\n\"\r\n2023-12-08T14:06:01.089-0500 [TRACE] applyProvisioners: provisioning aws_instance.training_node with \"remote-exec\"\r\n2023-12-08T14:06:01.090-0500 [TRACE] terraform.contextPlugins: Initializing provisioner \"remote-exec\" to read its schema \r\n2023-12-08T14:06:01.091-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\n2023-12-08T14:06:01.091-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\naws_instance.training_node: Provisioning with 'remote-exec'...\r\n2023-12-08T14:06:01.093-0500 [INFO]  using private key for authentication\r\naws_instance.training_node (remote-exec): Connecting to remote host via SSH...\r\naws_instance.training_node (remote-exec):   Host: 3.99.142.220\r\naws_instance.training_node (remote-exec):   User: Administrator\r\naws_instance.training_node (remote-exec):   Password: false\r\naws_instance.training_node (remote-exec):   Private key: true\r\naws_instance.training_node (remote-exec):   Certificate: false\r\naws_instance.training_node (remote-exec):   SSH Agent: false\r\naws_instance.training_node (remote-exec):   Checking Host Key: false\r\naws_instance.training_node (remote-exec):   Target Platform: windows\r\n2023-12-08T14:06:01.096-0500 [DEBUG] Connecting to 3.99.142.220:22 for SSH\r\n2023-12-08T14:06:01.116-0500 [DEBUG] Connection established. Handshaking for user Administrator\r\naws_instance.training_node (remote-exec): Connected!\r\n2023-12-08T14:06:01.400-0500 [DEBUG] starting ssh KeepAlives\r\n2023-12-08T14:06:01.401-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:01.515-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:06:01.555-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:06:01.563-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:06:01.649-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:06:01.650-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:06:01.681-0500 [ERROR] scp stderr: \"Sink: C0644 73 terraform_106553513.cmd\\r\\n\"\r\n2023-12-08T14:06:01.681-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:01.702-0500 [DEBUG] starting remote command: C:\/windows\/temp\/terraform_106553513.cmd\r\n\r\naws_instance.training_node (remote-exec): administrator@EC2AMAZ-KO02ITT C:\\Users\\administrator>start \/b \"looper1\" \"powershell.exe\" \"C:\\Users\\Administrator\\looper1.ps1\"\r\n2023-12-08T14:06:01.897-0500 [DEBUG] remote command exited with '0': C:\/windows\/temp\/terraform_106553513.cmd\r\n2023-12-08T14:06:01.897-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:01.918-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:06:01.951-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:06:01.953-0500 [DEBUG] Copying input data into temporary file so we can read the length\r\n2023-12-08T14:06:01.961-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:06:02.045-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:06:02.046-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:06:02.079-0500 [ERROR] scp stderr: \"Sink: C0644 0 terraform_106553513.cmd\\r\\n\"\r\n2023-12-08T14:06:02.080-0500 [TRACE] applyProvisioners: provisioning aws_instance.training_node with \"remote-exec\"\r\n2023-12-08T14:06:02.080-0500 [TRACE] terraform.contextPlugins: Initializing provisioner \"remote-exec\" to read its schema \r\n2023-12-08T14:06:02.081-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\n2023-12-08T14:06:02.081-0500 [TRACE] terraform.contextPlugins: Initializing provider \"registry.terraform.io\/hashicorp\/aws\" to read its schema\r\naws_instance.training_node: Provisioning with 'remote-exec'...\r\n2023-12-08T14:06:02.084-0500 [INFO]  using private key for authentication\r\naws_instance.training_node (remote-exec): Connecting to remote host via SSH...\r\naws_instance.training_node (remote-exec):   Host: 3.99.142.220\r\naws_instance.training_node (remote-exec):   User: Administrator\r\naws_instance.training_node (remote-exec):   Password: false\r\naws_instance.training_node (remote-exec):   Private key: true\r\naws_instance.training_node (remote-exec):   Certificate: false\r\naws_instance.training_node (remote-exec):   SSH Agent: false\r\naws_instance.training_node (remote-exec):   Checking Host Key: false\r\naws_instance.training_node (remote-exec):   Target Platform: windows\r\n2023-12-08T14:06:02.087-0500 [DEBUG] Connecting to 3.99.142.220:22 for SSH\r\n2023-12-08T14:06:02.108-0500 [DEBUG] Connection established. Handshaking for user Administrator\r\n2023-12-08T14:06:02.322-0500 [TRACE] dag\/walk: vertex \"root\" is waiting for \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\"\r\n2023-12-08T14:06:02.399-0500 [TRACE] dag\/walk: vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\" is waiting for \"aws_instance.training_node\"\r\naws_instance.training_node (remote-exec): Connected!\r\n2023-12-08T14:06:02.401-0500 [DEBUG] starting ssh KeepAlives\r\n2023-12-08T14:06:02.402-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:02.580-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:06:02.689-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:06:02.690-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:06:02.784-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:06:02.784-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:06:02.852-0500 [ERROR] scp stderr: \"Sink: C0644 101 terraform_781327820.cmd\\r\\n\"\r\n2023-12-08T14:06:02.853-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:02.875-0500 [DEBUG] starting remote command: C:\/windows\/temp\/terraform_781327820.cmd\r\n\r\naws_instance.training_node (remote-exec): administrator@EC2AMAZ-KO02ITT C:\\Users\\administrator>powershell.exe Start-Process -FilePath \"powershell\" -ArgumentList C:\\Users\\Administrator\\looper2.ps1\r\n2023-12-08T14:06:04.162-0500 [DEBUG] remote command exited with '0': C:\/windows\/temp\/terraform_781327820.cmd\r\n2023-12-08T14:06:04.163-0500 [DEBUG] opening new ssh session\r\n2023-12-08T14:06:04.183-0500 [DEBUG] Starting remote scp process:  \"scp\" -vt C:\/windows\/temp\r\n2023-12-08T14:06:04.215-0500 [DEBUG] Started SCP session, beginning transfers...\r\n2023-12-08T14:06:04.216-0500 [DEBUG] Copying input data into temporary file so we can read the length\r\n2023-12-08T14:06:04.224-0500 [DEBUG] Beginning file upload...\r\n2023-12-08T14:06:04.418-0500 [DEBUG] SCP session complete, closing stdin pipe.\r\n2023-12-08T14:06:04.419-0500 [DEBUG] Waiting for SSH session to complete.\r\n2023-12-08T14:06:04.444-0500 [ERROR] scp stderr: \"Sink: C0644 0 terraform_781327820.cmd\\r\\n\"\r\n2023-12-08T14:06:04.445-0500 [TRACE] GRPCProvider: GetProviderSchema\r\n2023-12-08T14:06:04.446-0500 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState to workingState for aws_instance.training_node\r\n2023-12-08T14:06:04.447-0500 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState: writing state object for aws_instance.training_node\r\naws_instance.training_node: Creation complete after 3m37s [id=i-0e7d659e1a0975eac]\r\n2023-12-08T14:06:04.450-0500 [TRACE] statemgr.Filesystem: have already backed up original terraform.tfstate to terraform.tfstate.backup on a previous write\r\n2023-12-08T14:06:04.455-0500 [TRACE] statemgr.Filesystem: state has changed since last snapshot, so incrementing serial to 368\r\n2023-12-08T14:06:04.456-0500 [TRACE] statemgr.Filesystem: writing snapshot at terraform.tfstate\r\n2023-12-08T14:06:04.510-0500 [TRACE] vertex \"aws_instance.training_node\": visit complete\r\n2023-12-08T14:06:04.510-0500 [TRACE] vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\": starting visit (*terraform.graphNodeCloseProvider)\r\n2023-12-08T14:06:04.511-0500 [TRACE] GRPCProvider: Close\r\n2023-12-08T14:06:04.512-0500 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=\"rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n2023-12-08T14:06:04.572-0500 [DEBUG] provider: plugin process exited: path=.terraform\/providers\/registry.terraform.io\/hashicorp\/aws\/5.9.0\/windows_amd64\/terraform-provider-aws_v5.9.0_x5.exe pid=24016\r\n2023-12-08T14:06:04.573-0500 [DEBUG] provider: plugin exited\r\n2023-12-08T14:06:04.573-0500 [TRACE] vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\": visit complete  \r\n2023-12-08T14:06:04.574-0500 [TRACE] vertex \"root\": starting visit (*terraform.nodeCloseModule)\r\n2023-12-08T14:06:04.575-0500 [TRACE] vertex \"root\": visit complete\r\n2023-12-08T14:06:04.575-0500 [TRACE] statemgr.Filesystem: have already backed up original terraform.tfstate to terraform.tfstate.backup on a previous write\r\n2023-12-08T14:06:04.576-0500 [TRACE] statemgr.Filesystem: state has changed since last snapshot, so incrementing serial to 369\r\n2023-12-08T14:06:04.577-0500 [TRACE] statemgr.Filesystem: writing snapshot at terraform.tfstate\r\n2023-12-08T14:06:04.606-0500 [TRACE] statemgr.Filesystem: removing lock metadata file .terraform.tfstate.lock.info\r\n2023-12-08T14:06:04.610-0500 [TRACE] statemgr.Filesystem: unlocked by closing terraform.tfstate\r\n\r\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\r\n```\r\n\r\n### Expected Behavior\r\n\r\nThe powershell scripts looper1.ps1 and looper2.ps1 should have been run as separate processes on the EC2 Windows instance.\r\n\r\n### Actual Behavior\r\n\r\nThe powershell scripts looper1.ps1 and looper2.ps1 do not appear to run at all. \r\n\r\n### Steps to Reproduce\r\n\r\nRun \"terraform apply\"\r\n\r\n### Additional Context\r\n\r\nI am trying to execute a powershell script as a separate process on an EC2 windows instance using \"remote-exec\". However, terraform does not seem to be able to launch the powershell script as a separate process - the powershell script does not run.\r\n\r\nTo illustrate the problem I have created two long-running powershell scripts called \"looper1.ps1\" and \"looper2.ps1\" which simply loop forever and write to log files in C:\\Users\\Administrator\" every 5s (please see scripts below. I can tell whether the scripts are running or not by the presence (or absence) of the log files (called \"Looper1.log\" and \"Looper2.log\" respectively.\r\n\r\nIn the terraform snippet from my \"main.tf\" (which is included above) I try to execute the \"looper1.ps1\" script as a separate process on the EC2 windows instance using remote-exec and the command:\r\n\r\nstart \/b \"looper1\" \"powershell.exe\" \"C:\\Users\\Administrator\\looper1.ps1\"\r\n\r\nI alsoI try to execute the \"looper2.ps1\" script as a separate process on the EC2 windows instance using remote-exec and the command:\r\n\r\npowershell.exe Start-Process -FilePath \"powershell\" -ArgumentList C:\\Users\\Administrator\\looper2.ps1\r\n\r\nIn the terraform debug output it looks like both of these commands execute successfully (they both exit with code 0). However, when I subsequently log in to the EC2 windows instance I do not see the log files \"Looper1.log\" and \"Looper2.log\" (which should be updating every 5s if looper1.ps1 and looper2.ps1 are running), nor do I see any evidence that they are running if I execute a \"tasklist\" command. \r\n\r\nWhen I am logged in to the EC2 instance, if I open a cmd window and manually issue the command:\r\n\r\nstart \/b \"looper1\" \"powershell.exe\" \"C:\\Users\\Administrator\\looper1.ps1\"\r\n\r\nthen I immediately see the \"Looper1.log\" file created and updated every 5s. Issuing a \"tasklist\" command shows the presence of a new powershell.exe process belonging to \"looper1.ps1\".\r\n\r\nIn the same cmd window if I manually issue the command:\r\n\r\npowershell.exe Start-Process -FilePath \"powershell\" -ArgumentList C:\\Users\\Administrator\\looper2.ps1\r\n\r\nThen a new powershell window opens, the command returns control to the cmd window, and I immediately see the \"Looper2.log\" file created and updated every 5s. Issuing a \"tasklist\" command shows the presence of a new powershell process belonging to \"looper2.ps1\".\r\n\r\nSo, it seems that I can manually issue these commands and start the powershell scripts as separate processes, but issuing the same commands through terraform \"remote-exec\" does nothing.\r\n\r\nI have also tried starting the powershell scripts from terraform using \"remote-exec\" but *not as separate processes* (ie; in the foreground). The relevant commands are:\r\n\r\nstart \"looper1\" \"powershell.exe\" \"C:\\Users\\Administrator\\looper1.ps1\"\r\n\r\nand\r\n\r\npowershell.exe C:\\Users\\Administrator\\looper2.ps1\r\n\r\nThe first command is (start \"looper1\"...) is executed by terraform and returns with exit code 0. However, as before, it does not seem to do anything on the EC2 windows instance (in the sense that when I log in to the remote EC2 windows instance there is no \"Looper1.log\" file created), and the \"looper1.ps1\" powershell script does not appear to be running on the EC2 windows instance.\r\n\r\nWhen terraform hits the second command (powershell.exe C:\\Users\\Administrator\\looper2.ps1) terraform executes it on the remote EC2 windows instance and then waits until the command finishes executing (which means that it waits forever because the powershell script is running in the foreground and loops forever and never returns). When I log in to the EC2 windows instance I do see the presence of the \"Looper2.log\" file and I do see a powershell process running in tasklist associated with \"looper2.ps1\". So, it seems like terraform is able to run \"looper2.ps1\" in the foreground (but of course, terraform never returns and keeps running forever too!).\r\n\r\nSo, it looks like, for some reason, terraform is not successful launching the powershell scripts as separate processes using \"remote-exec\" (even though I can issue exactly the same commands in a cmd window on the EC2 instance to successfully launch the powershell scripts as separate processes).\r\n\r\nYou may ask why I want to run the powershell scripts as separate processes in the first place. It is because I the powershell scripts are executing and monitoring long-running installation processes and I want to build several EC2 windows instances in parallel (and not serially).\r\n\r\nAnd, finally, I have been successful launching the powershell scripts as separate processes upon EC2 instance instantiation by embedding the powershell scripts in the AMI and then using EC2Launch (and properly configuring agent-config.yml). However, embedding the powershell scripts in the AMI and using EC2Launch is a brittle and error-prone workaround. I would much rather do it flexibly using terraform and \"remote-exec\" - it just doesn't seem to work.\r\n\r\n# this is looper1.ps1\r\n$looperLogFile = \"C:\\Users\\Administrator\\Looper1.log\"\r\n$sleepTime = 5\r\nwhile ($true) {\r\n    Add-Content -Path $looperLogFile -Value \"Looper1. Waiting $sleepTime.\"\r\n    Start-Sleep -s $sleepTime\r\n}\r\n\r\n# this is looper2.ps2\r\n$looperLogFile = \"C:\\Users\\Administrator\\Looper2.log\"\r\n$sleepTime = 5\r\nwhile ($true) {\r\n    Add-Content -Path $looperLogFile -Value \"Looper2. Waiting $sleepTime.\"\r\n    Start-Sleep -s $sleepTime\r\n}\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this bug report, @papaneeds. While a maintainer may weigh in on the issue, I'll note that provisioners are not receiving active development, per https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md#provisioners. \r\n\r\nThis looks like a fairly sophisticated use case, you may also find help from [the community forum](https:\/\/discuss.hashicorp.com\/c\/terraform-core\/27).\r\n\r\nThanks!"],"labels":["bug","new","v1.6"]},{"title":"terraform console jsonencode result sometimes seems like it has the wrong type (but it's just cosmetic)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.5\r\n```\r\n\r\n### Expected Behavior\r\n\r\n```\r\n> jsonencode(3)\r\n3\r\n```\r\n\r\n### Actual Behavior\r\n\r\nWhen using jsonencode on a map with any integers\/floats, those will be cast to strings in the resulting JSON.\r\n\r\n```\r\n> jsonencode(3)\r\n\"3\"\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n```\r\nterraform console\r\n> jsonencode(3)\r\n\"3\"\r\n```\r\n\r\n### Additional Context\r\n\r\nLooks like a regression of [17033](https:\/\/github.com\/hashicorp\/terraform\/issues\/17033), which was fixed in 0.12.0\r\n\r\n### References\r\n\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/17033","comments":["Hi @drewhemm,\r\n\r\nThanks for filing the issue. The result of calling `jsonencode(number)` seems to have an unfortunate conversion to string in there. We can investigate why that is, though json encoding a single number is a bit of an edge case -- Do you have any other examples where typed input is not converted correctly? Taking something like an object with mixed attribute types works correctly:\r\n\r\n```\r\nvariable \"in\" {\r\n  type = object({\r\n    number = number\r\n    number_string = string\r\n  })\r\n  default = {\r\n    number = 2\r\n    number_string = \"3\"\r\n  }\r\n}\r\n\r\noutput \"out\" {\r\n  value = jsonencode(var.in)\r\n}\r\n```\r\n\r\nresults in:\r\n```\r\n{\"number\":2,\"number_string\":\"3\"}\r\n```","Oh, that's interesting. I actually first hit the issue while encoding a map with a mixture of strings and numbers. Will post the code shortly.","A map can only contain a single data type, you cannot have a map with a mixture of strings and numbers. In that case Terraform probably already converted the input to `map(string)` to create a valid data type before passing to `jsonencode`.","Hi @drewhemm,\r\n\r\nI think what's happened here is just some confusion in the meaning of the output from `terraform console`. The command uses formatting similar to Terraform's expression syntax to show the result, and that means that when the result is a string it appears in quotes, just as it would if the same string were written as input.\r\n\r\nFor example:\r\n\r\n```shellsession\r\n> jsonencode(\"2\")\r\n\"\\\"2\\\"\"\r\n> jsonencode(2)\r\n\"2\"\r\n```\r\n\r\nNotice that the result of the first expression is a string containing the characters `\"2\"`, which is a correct JSON encoding of the string containing the digit 2. The result of the second expression is a string containing only the character `2`, which is the correct JSON encoding of the number 2.\r\n\r\nTo confirm this I temporarily patched my local Terraform CLI to expose the internal representation of the values being shown, effectively disabling the \"render as if a Terraform expression\" step:\r\n\r\n```shellsession\r\n> jsonencode(2)\r\ncty.StringVal(\"2\")\r\n\r\n> jsonencode(\"2\")\r\ncty.StringVal(\"\\\"2\\\"\")\r\n\r\n> jsonencode([2])\r\ncty.StringVal(\"[2]\")\r\n```\r\n\r\nThe result of `jsonencode` is _always_ a string, so its result will always be shown in quotes by `terraform console`. If you `jsonencode` a string then you get a string with a quoted string inside it, because that's what JSON requires.\r\n\r\nAs @jbardin mentioned, this is something separate from what happens when you try to construct a map with inconsistent element types. That's _not_ a formatting thing but instead an implicit type conversion thing: Terraform is guessing that you probably intended the element type to be `string`, as it often does when numbers appear in contexts where strings are required, and so is performing that conversion automatically as an alternative to raising an error:\r\n\r\n```shellsession\r\n> { a = 1, b = \"foo\" }\r\n{\r\n  \"a\" = 1\r\n  \"b\" = \"foo\"\r\n}\r\n> tomap({ a = 1, b = \"foo\" })\r\ntomap({\r\n  \"a\" = \"1\"\r\n  \"b\" = \"foo\"\r\n})\r\n```\r\n\r\nIn the first expression I constructed a a value of type `object({a = number, b = string})`, while in the second expression I converted that result into a map using `tomap`, and so Terraform had to automatically decide what the element type of that map should be, and guessed that I had intended to use `map(string)` because `string` is the one type that all elements can automatically convert to.\r\n\r\nThis effect is perhaps clearer in the situation where there is _no_ common base type for Terraform to automatically convert to, in which case it returns an error:\r\n\r\n```shellsession\r\n> { a = 1, b = [] }\r\n{\r\n  \"a\" = 1\r\n  \"b\" = []\r\n}\r\n> tomap({ a = 1, b = [] })\r\n\u2577\r\n\u2502 Error: Invalid function argument\r\n\u2502 \r\n\u2502   on <console-input> line 1:\r\n\u2502   (source code not available)\r\n\u2502 \r\n\u2502 Invalid value for \"v\" parameter: cannot convert object to map of any single type.\r\n\u2575\r\n```\r\n\r\nThere is no automatic conversion from `tuple([])` to `number` or from `number` to `tuple([])`, so in this case Terraform can find no solution and raises an error.\r\n\r\nOverall, I think this issue suggesting two different parts of Terraform's behavior that are not sufficiently clear:\r\n\r\n1. When the value being printed is itself a JSON string, `terraform console`'s attempt to make it clear that the value it's printing is a string actually makes things _less_ clear, because JSON itself already includes quotes and so it is easy to misunderstand the result as incorrect JSON output rather than as a Terraform expression describing valid JSON output.\r\n\r\n    However, it's not clear exactly what to do about this, because just showing a string \"naked\" would cause the opposite ambiguity: the expressions `2` and `\"2\"` would both produce the same console output. One way or another the user does need to have some awareness of exactly what conventions are being used to format the value in order to understand what the result is.\r\n2. The automatic selection of an element type when converting from object types to map types (instead of always returning an error) hides something that should really be more explicit.\r\n\r\n    This is a historical annoyance we already knew about, but by the time we introduced a properly-designed type system into Terraform in Terraform v0.12 there were already many existing modules depending on the questionable behavior from Terraform v0.11 and earlier of having string as the only available primitive type, and so making this an error would have broken too many existing modules and thus considerably split the ecosystem.\r\n\r\n    Perhaps we can find a way to address this in a future [opt-in language edition](https:\/\/log.martinatkins.me\/2021\/09\/25\/future-of-the-terraform-language\/), but it's not clear to me exactly how to achieve that given that this behavior actually belongs to the underlying HCL library that is shared by many applications aside from Terraform, and so is beyond the reach of a Terraform-specific language editions mechanism.\r\n\r\nWith all of that said, I do agree that these are both real problems but I also don't really know what we can practically do about them. :thinking: \r\n","Since Terraform is behaving as designed here, and we've found ourselves faced with the question of whether and how to _change_ Terraform's design to clarify what's going on, I'm going to reclassify this an enhancement.\r\n\r\nWe use \"bug\" only to represent situations where Terraform is malfunctioning and thus the resolution of the issue would be to change the implementation to match the intended design, whereas \"enhancement\" represents issues that require some design work. Neither label implies anything about priority, only the type of work that's required to resolve it.\r\n"],"labels":["enhancement","cli","config"]},{"title":"cloudplugin: use terminal.Streams to write command output","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\nUse the command meta's `Streams` object to write output rather than pass stdout\/stderr file descriptor between the command and the grpc client. This has two notable benefits:\r\n- `terminal.Streams` is the desired interface commands should use to write output to stdout\/error\r\n- `terminal.Streams` readily provides terminal width information which allows the cloud command to word wrap output. We lose this information implicitly casting `c.Meta.Streams.Stdout.File` (and the stderr equivalent) to an `io.Writer`. \r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.6.x\r\n\r\n","comments":["All right, so I smoke tested this alongside the version -json plugin PR, and it works! \r\n\r\nThings I don't quite fully understand yet: \r\n\r\n- Hey, are there times when we _shouldn't_ be word-wrapping the stdout? like with -json, for example. (Remember that newline literals are forbidden inside a json string value.)\r\n- What can we do to get around the \"diags pretty-printed to stdout\" scenario when doing json output?","Agreed that it seems risky to assume that all output from the plugin ought to be naively wrapped by the client.\r\n\r\nDo you think we could instead send some metadata about stdout (whether it's a terminal at all, and what its width is if so) in the initial request, and then have the plugin do the wrapping itself? (That does mean that the plugin wouldn't be able to respond properly if the terminal gets resized while it's already running, but Terraform's already not responding to terminal resizing in most cases even without the plugin, and in practice that hasn't mattered that much because Terraform output is append-only, not TUI-style.)\r\n\r\nGenerally I think we should try to be consistent in what the level of abstraction is. I could see arguments both for having the plugin just emit raw bytes to copy verbatim to stdout\/stderr, or for having the plugin emit higher-level objects which the client then formats for presentation in the UI, but I would feel nervous about this mixture of the two where the server is _mostly_ emitting raw bytes but the client will sometimes insert extra newline characters into it; that seems likely to be a gotcha for a future maintainer who isn't reading the client code carefully enough. :grimacing: \r\n"],"labels":["enhancement","cloud"]},{"title":"Failed to load plugin schemas - terraform show in offline mode","body":"### Terraform Version\r\n\r\n```shell\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [INFO]  Terraform version: 1.3.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```\r\ncat << EOF > ~\/.terraformrc\r\nprovider_installation {\r\n  network_mirror {\r\n    url    = \"[https:\/\/${user}:${password}@<nexus-host>\/repository\/tf-hosted\/](https:\/\/$%7buser%7d:$%7bpassword%7d@<nexus-host>\/repository\/tf-hosted\/)\"\r\n            include=[\"registry.terraform.io\/*\/*\"]\r\n            }\r\n  direct{\r\n           exclude=[\"registry.terraform.io\/*\/*\"]\r\n}\r\n}\r\nplugin_cach_dir=\"$HOME\/.terraform.d\/plugin-cache\"\r\nEOF\r\n```\r\n\r\n### Debug Output\r\n```\r\n15:10:31.326Z [DEBUG] using github.com\/hashicorp\/go-tfe v1.9.0\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.15.0\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [DEBUG] using github.com\/hashicorp\/terraform-config-inspect v0.0.0-20210209133302-4fd17a0faac2\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.0.0-20200729002733-f050f53b9734\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [DEBUG] using github.com\/zclconf\/go-cty v1.12.1\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [INFO]  Go runtime version: go1.19.3\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [INFO]  CLI args: []string{\"\/storage\/367f74a\/usr\/bin\/terraform\", \"show\", \"-json\", \"\/storage\/367f74a\/workspace\/terraform.plan\"}\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [DEBUG] Attempting to open CLI config file: \/root\/.terraformrc\r\n[15:10:31.326] 2023-12-05T15:10:31.326Z [INFO]  Loading CLI configuration from \/root\/.terraformrc\r\n[15:10:31.327] 2023-12-05T15:10:31.326Z [DEBUG] Explicit provider installation configuration is set\r\n[15:10:31.327] 2023-12-05T15:10:31.327Z [INFO]  CLI command args: []string{\"show\", \"-json\", \"\/storage\/367f74a\/workspace\/terraform.plan\"}\r\n[15:10:31.331] 2023-12-05T15:10:31.331Z [DEBUG] checking for provisioner in \".\"\r\n[15:10:31.331] 2023-12-05T15:10:31.331Z [DEBUG] checking for provisioner in \"\/storage\/367f74a\/usr\/bin\"\r\n[15:10:31.331] \u2577\r\n[15:10:31.331] \u2502 Error: Failed to load plugin schemas\r\n[15:10:31.331] \u2502 \r\n[15:10:31.331] \u2502 Error while loading schemas for plugin components: 3 problems:\r\n[15:10:31.331] \u2502 \r\n[15:10:31.331] \u2502 - Failed to obtain provider schema: Could not load the schema for provider\r\n[15:10:31.331] \u2502 registry.terraform.io\/hashicorp\/archive: failed to instantiate provider\r\n[15:10:31.331] \u2502 \"registry.terraform.io\/hashicorp\/archive\" to obtain schema: unavailable\r\n[15:10:31.331] \u2502 provider \"registry.terraform.io\/hashicorp\/archive\".\r\n[15:10:31.331] \u2502 - Failed to obtain provider schema: Could not load the schema for provider\r\n[15:10:31.331] \u2502 registry.terraform.io\/hashicorp\/aws: failed to instantiate provider\r\n[15:10:31.331] \u2502 \"registry.terraform.io\/hashicorp\/aws\" to obtain schema: unavailable\r\n[15:10:31.331] \u2502 provider \"registry.terraform.io\/hashicorp\/aws\".\r\n[15:10:31.331] \u2502 - Failed to obtain provider schema: Could not load the schema for provider\r\n[15:10:31.331] \u2502 registry.terraform.io\/hashicorp\/random: failed to instantiate provider\r\n[15:10:31.331] \u2502 \"registry.terraform.io\/hashicorp\/random\" to obtain schema: unavailable\r\n[15:10:31.331] \u2502 provider \"registry.terraform.io\/hashicorp\/random\"..\r\n[15:10:31.331] \u2575\r\n```\r\n\r\n### Expected Behavior\r\n\r\nterraform show should output the plan as expected\r\n\r\n### Actual Behavior\r\n\r\nterraform show fails to load plugin schema although terraform init works as expected in offline mode as well as plan, we have a working plan output but need it in json format. We have no internet access from where we execute terraform. \r\n\r\n### Steps to Reproduce\r\n\r\n1. configure terraform to work in \"offline mode\" using .terraformrc and point it to private repository (we're using nexus).\r\n2. terraform init \r\n3. terraform plan > <plan file>\r\n4. terraform show -json <plan file>\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":[],"labels":["bug","new","v1.3"]},{"title":"Terraform Test - Add plan or apply as a command line variable","body":"### Terraform Version\n\n```shell\nTerraform v1.6.5\r\non windows_amd64\n```\n\n\n### Use Cases\n\nFor using terraform test in a code pipeline, it would be useful to override the command variable for plan or apply in order to run a plan first and then an apply step as a manual validation only if plan works. \n\n### Attempted Solutions\n\nRequires different files with different run blocks.\n\n### Proposal\n\nAdd a parameter to specify this on the command line\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"terraform test - skip stage, add a breakpoint","body":"### Terraform Version\r\n\r\n```shell\r\n1.6.5\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nCurrently we use Terratest for testing our modules and one of the features is the ability to skip stages.\r\n\r\nFor example, during development we can skip the destroy stage, run Terratest, review the resources from Azure portal, make module changes, run Terratest again, and when we are happy enable destroy stage again and run Terratest to cleanup.\r\n\r\nUnfortunately, because `terraform test` has state in memory and cleans up every time we can't do this iterative development.\r\n\r\nThe setup and cleanup can also slow things down a lot if the module under test depends on a bunch of resources created during test setup.\r\n\r\n```bash\r\n# Skip terraform destroy stage\r\nexport SKIP_destroy=\"true\"\r\n\r\n# Run Terratest\r\ngo test\r\n\r\n# Enable destroy & re-run to cleanup\r\nunset SKIP_destroy\r\ngo test\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nnone\r\n\r\n### Proposal\r\n\r\nProposals:\r\n\r\n1. Add a `terraform test` option to save test state to the tests directory and have an option to skip destroy temporarily.\r\n2. It would also be useful to have Breakpoint support, similar to Packer Breakpoint Provisioners. e.g add a breakpoint after test setup run block, before or after an assert block.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","I'd definite upvote the breakpoint capability.  Maybe inside a run block, with an object value of either before or after, and a time value in seconds.  The main goal here is that during module development, I'd like to log into the actual system and verify the state, so a pause is needed.  Before would pause before the current run block is executed, after would be after it's executed.\r\n\r\ni.e.\r\n```\r\nrun \"test\" {\r\n  break = { \r\n     when = \"before\"\r\n     duration = 500\r\n  }\r\n}\r\n```"],"labels":["enhancement","new","terraform test"]},{"title":"Misleading error message with at least `terraform init` when Mac OS DNS not working properly","body":"### Terraform Version\n\n```shell\nTerraform v1.6.5\r\non darwin_amd64\n```\n\n\n### Terraform Configuration Files\n\n(none)\n\n### Debug Output\n\n(Couldn't reproduce the broken DNS state, so was too late to get debug output.)\n\n### Expected Behavior\n\nTerraform should indicate that the DNS could not be resolved.\n\n### Actual Behavior\n\nIt gives a cryptic error message:\r\n\r\n```\r\n$ terraform init\r\n\r\nInitializing the backend...\r\n\r\nInitializing provider plugins...\r\n- Finding latest version of hashicorp\/aws...\r\n\u2577\r\n\u2502 Error: Failed to query available provider packages\r\n\u2502\r\n\u2502 Could not retrieve the list of available versions for provider hashicorp\/aws: could not connect to registry.terraform.io:\r\n\u2502 failed to request discovery document: Get \"https:\/\/registry.terraform.io\/.well-known\/terraform.json\": context deadline\r\n\u2502 exceeded (Client.Timeout exceeded while awaiting headers)\r\n```\n\n### Steps to Reproduce\n\n0. (Have broken DNS on Mac OS)\r\n1. `terraform init`\n\n### Additional Context\n\nThe error went away when fixing with this answer:\r\n[DNS Name lookup (was SSH) Not Working After Snow Leopard Upgrade](https:\/\/serverfault.com\/questions\/64837\/dns-name-lookup-was-ssh-not-working-after-snow-leopard-upgrade\/65771#65771)\n\n### References\n\n- 26532","comments":["Hi @nhooey! Thanks for this feedback.\r\n\r\nUnfortunately it seems that what happened here is that Terraform got stuck waiting for _something_ to respond (based on the message I would've guessed it is the HTTPS server, but I can't confirm that) and so I don't think Terraform actually knows what's gone wrong here, beyond that the request took an unusually long time to complete.\r\n\r\nIt isn't clear to me how a failed DNS lookup could cause a slow response from a HTTPS request though. Do you think that your system got an incorrect answer for which IP address to connect to, and then the server it tried to connect to was not ready to accept an HTTPS request?\r\n","Terraform will be able to detect if there is a DNS _failure_, but more often than not misconfigured DNS results in no response at all. Without running some extra diagnostics this is indistinguishable from an unstable or slow connection, bad routing, broken link, etc. You happen to know it was DNS, because resetting the system mDNSResponder resulted in a fix, but Terraform cannot diagnose that type of failure externally. \r\n\r\nSince resolver configuration is a common problem, perhaps we could insert a custom client resolver with a slightly lower timeout, which would give basically the same error except it would at least mention DNS. Unfortunately since we only control some of the many http clients used from within Terraform it can't solve all cases, but might be able to make problems during `init` more obivous."],"labels":["bug","waiting-response","new","v1.6"]},{"title":"Implement exact indentation for here doc","body":"### Terraform Version\n\n```shell\nNA\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/strings\n```\n\n\n### Use Cases\n\n```\r\nblock {\r\n  value = <<-EOT\r\n  hello\r\n    world\r\n  EOT\r\n}\r\n```\r\n\r\n>In this case, Terraform analyses the lines in the sequence to find the one with the smallest number of leading spaces, and then trims that many spaces from the beginning of all of the lines, leading to the following result:\r\n\r\nFrom here it is not clear what happen if here doc will be formatted as next:\r\n\r\n```\r\nblock {\r\n  value = <<-EOT\r\nhello\r\n    world\r\n  EOT\r\n}\r\n```\r\n\r\n`hello` has zero indentation, but `EOT` has 2 spaces.\n\n### Attempted Solutions\n\nna\n\n### Proposal\n\nIt would be nice to have `must` indentation:\r\n```\r\nblock {\r\n  value = <<\"EOT\"\r\n  hello\r\n    world\r\nEOT\r\n}\r\n```\r\n\r\nThis means that here no indentation and \"EOT\" must appear without spaces.\r\n\r\n```\r\nblock {\r\n  value = <<\"  EOT\"\r\n hello\r\n    world\r\n  EOT\r\n}\r\n```\r\n\r\nThis means that two spaces indentation must be used, \"  EOT\" must appear to have exact two spaces before it and \" hello\" with one space should issue an error. This will prevents occasional miss formatting, which could occur when -EOT was used.\r\n\r\n\r\nAlso we can implement soft indentation \"~EOT\":\r\n```\r\nblock {\r\n  value = <<~EOT\r\n  hello\r\n    world\r\n  EOT\r\n}\r\n```\r\nWhich will parse first line and detect required spaces. Here \"  hello\" has two spaces, thus \"  EOT\" must also have two spaces indentation and no lines in heredoc should have less then two spaces, eg. \" world\" is not allowed.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Reorder paragraphs for indented heredocs","body":"### Terraform Version\n\n```shell\nNA\n```\n\n\n### Affected Pages\n\n![image](https:\/\/github.com\/hashicorp\/terraform\/assets\/139780495\/660231df-f5af-46ba-a5ea-0940ec03fbae)\r\n\n\n### What is the docs issue?\n\nYou wrote about here doc, then about something else, then again about heredoc.\n\n### Proposal\n\nWrite paragraphs about heredoc together\n\n### References\n\n_No response_","comments":["Thanks for this request!"],"labels":["documentation","new"]},{"title":"backend\/s3: Validate behaviour with S3 Express One Zone buckets","body":"### Terraform Version\r\n\r\n```shell\r\nN\/A\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nAt re:Invent 2023, AWS announced a new storage class for S3 buckets: [S3 Express One Zone storage](https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/userguide\/\/s3-express-one-zone.html)\r\n\r\nS3 Express can be 10 times faster and 50% cheaper than standard storage to access. S3 Express has the same durability guarantee (99.999999999%) as Standard storage, and a slightly lower availability guarantee (99.95% vs 99.99%) \r\n\r\nDo the following:\r\n\r\n- [ ] Validate that the S3 backend works with S3 Express\r\n- [ ] Document how to use the S3 backend with S3 Express\r\n\r\n### Related\r\n\r\n* Using S3 Multi-Region Access Points with the S3 backend #32080","comments":[],"labels":["enhancement","new"]},{"title":"Enhancement Request: `azurerm` backend authentication upgrade to match provider","body":"### Terraform Version\n\n```shell\n1.6.5\n```\n\n\n### Use Cases\n\nI want to use `az` CLI service principal authentication in my CI \/ CD pipelines. I am able to do that with the upgraded authentication in the provider for `plan` and `apply`, but `init` does not support it for the `azurerm` backend.\r\n\r\nI'd like to have a consistent authentication experience across all commands.\r\n\r\nThe original driver for this is running `terraform test` with OIDC auth. Since the Azure DevOps ID Token is only valid for 10 minutes and each test configures a new provider for each time it times out if there are a few tests in there.\n\n### Attempted Solutions\n\nThere is no solution other than using the existing mechanism to supply service principal credentials.\n\n### Proposal\n\nUpdate the `azurerm` backend authentication [code](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/backend\/remote-state\/azure\/arm_client.go) \r\nto match the provider [code](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/blob\/main\/internal\/clients\/auth.go).\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","backend\/azure","new"]},{"title":"Instance module.abc.data.aws_lambda_invocation.xyz[\"123\"] is marked as having a change pending but that change is not recorded in the plan.","body":"### Terraform Version\n\n```shell\nTerraform v1.3.5\r\non f00\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.26.0\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.2\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\ndata \"aws_lambda_invocation\" \"xyz\" {\r\n  for_each      = { for k, v in local.functions : k => v if lookup(v, \"xyz\", null) != null }\r\n  function_name = module.foo.bar[each.key].baz\r\n  input         = jsonencode(each.value[\"xyz\"][\"foobar\"])\r\n  \/\/ make it happen only during apply (trying to fix the change not recorded in plan)\r\n  depends_on = [null_resource.apply_anchor]\r\n  lifecycle {\r\n    postcondition {\r\n      condition     = jsondecode(self.result) == each.value[\"xyz\"][\"barfoo\"]\r\n      error_message = \"wanted barfoo but got something else\"\r\n    }\r\n  }\r\n}\r\n```\n\n### Debug Output\n\n```\r\n\u2577\r\n\u2502 Error: Missing pending object in plan\r\n\u2502 \r\n\u2502   on abc\/abc.tf line 420:\r\n\u2502  420: data \"aws_lambda_invocation\" \"xyz\" {\r\n\u2502 \r\n\u2502 Instance\r\n\u2502 module.abc.data.aws_lambda_invocation.xyz[\"123\"] is\r\n\u2502 marked as having a change pending but that change is not recorded in the\r\n\u2502 plan. This is a bug in Terraform; please report it.\r\n\u2575\r\nReleasing state lock. This may take a few moments...\r\n```\n\n### Expected Behavior\n\nIt should either work all the time or fail all the time. Ideally work all the time and not fail randomly sometimes.\n\n### Actual Behavior\n\nIt fails sometimes.\n\n### Steps to Reproduce\n\n```\r\nterraform init\r\nterraform plan -out tf.plan\r\nterraform apply tf.plan\r\n```\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @andresvia! Sorry this didn't work as intended, and thanks for reporting it.\r\n\r\nTo understand what happened here I expect we'll need to review the full trace log that we requested in our issue template. You've shared normal output rather than the \"Debug Output\", and so there isn't enough information here to understand why this invariant was not upheld.\r\n\r\nTo produce the full trace output:\r\n\r\n1. Set the environment variable TF_LOG=trace\r\n2. Run the same `terraform plan -out tf.plan` command you ran before, and save the verbose log output to a file.\r\n3. Run the same `terraform apply tf.plan` command you ran before, and save its verbose log output to a separate file.\r\n4. Add both of those files to a [GitHub Gist](https:\/\/gist.github.com\/) and share the link to that Gist here.\r\n\r\nIn this particular case I expect we'll need to see the full trace logs for both the plan phase _and_ the apply phase, because this problem seems to span across both of the phases: the `terraform plan` command seems to have generated an invalid plan file, and the `terraform apply` command is then detecting that error and reporting it with this error message.\r\n\r\nThanks!\r\n","@apparentlymart don't need to be sorry, I'm thankful for the all the amazing work done on Terraform, I did a quick Google for `terraform reporting strip sensitive information when TF_LOG=trace` (could not find anything). Is it there somewhere a canonical way to do it? Maybe a tool or regex to remove all traces of information that may identify the account or AWS user?","Hi @andresvia,\r\n\r\nDifferent organizations have different opinions about what is \"sensitive\", so there is no tool for that. You'd need to review the output yourself and decide what you consider to be sensitive. Of course, if you remove too much then the output won't be useful, so I'd encourage focusing mainly on eliminating personally-identifying information like specific IP addresses or names\/ids that are in global namespaces that others could potentially access, and less on generic information like which resource types you are using and how they are related to one another.\r\n","Hi @andresvia,\r\n\r\nOne other thing to check is that v1.3 was released over a year ago, and there have been many issues fixed since then. You may want to confirm this still happens with a current version of Terraform.\r\n\r\nThanks!","I know I've been very quiet about this issue, I just want to report that I tried it in `1.5.3` and it is still happening. My eventual reproduction of the problem will use `1.5.3`."],"labels":["bug","waiting-response","new","v1.3"]},{"title":"Provide a mechanism for selecting the appropriate provider with -generate-config-out","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non darwin_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v4.22.0\n```\n\n\n### Use Cases\n\nI'd like to generate the configuration for an existing resource! It's in a region that corresponds to a provider I've defined, but unfortunately it's not the default alias for that provider. (e.g. it's in `us-east-2` but the default provider is configured for `us-east-1`).\n\n### Attempted Solutions\n\n`import { ... }` the resource as usual, and `tf plan -generate-config-out=config.tf` to generate the configuration:\r\n\r\n```tf\r\n\u2577\r\n\u2502 Error: Cannot import non-existent remote object\r\n\u2502\r\n\u2502 While attempting to import an existing object to \"<resource-address>\", the provider detected that no object exists with the given id. Only pre-existing objects can be imported; check that the id is correct and that it is associated with the provider's configured region or endpoint, or use \"terraform apply\" to create a new remote object for\r\n\u2502 this resource.\r\n\u2575\r\n```\n\n### Proposal\n\nAdd another attribute to the `import` block to indicate the provider to be used for configuration generation, if no such resource is already defined in code.\n\n### References\n\n_No response_","comments":["Hi @skeggse, thanks for filing this.\r\n\r\nThe `import` block should already accept a provider attribute that matches the provider attribute you'd see in normal resource blocks, for example:\r\n\r\n```hcl\r\nimport {\r\n  provider = aws.secondary\r\n  id = \"some id\"\r\n  to = aws_resource.my_resource\r\n}\r\n``` \r\n\r\nDoes that solve your problem?\r\n\r\nThanks!"],"labels":["enhancement","waiting-response","new"]},{"title":"Allow for secure plan-apply flow in CI","body":"### Terraform Version\r\n\r\n```shell\r\n$ terraform --version                  \r\nTerraform v1.6.4\r\non linux_amd64\r\n+ provider registry.terraform.io\/gitlabhq\/gitlab v16.4.1\r\n+ provider registry.terraform.io\/hashicorp\/azuread v2.43.0\r\n+ provider registry.terraform.io\/hashicorp\/azurerm v3.75.0\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.1\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWhen using CI like gitlab.com you'll often want a users input on if the changes are as expected. \r\nYou'll implement it like:\r\n\r\n1) plan job: does terraform plan -out=plan.out \r\n2) apply job: does terraform apply -auto-approve plan.out\r\n\r\nThe issue here is that plan.out can contain sensitive information which would be exposed to unauthorized users. If you're using gitlab you'll probably be using [artifacts](https:\/\/docs.gitlab.com\/ee\/ci\/yaml\/#artifactspaths) but this also exposes the plan to every user that is reporter (read only access) and above, while fx the backend would require maintainer (next highest level) to read the state.\r\nIn my mind we need to integrate with terraform either by doing 0 planning or by encrypting the plan output, but it is honestly a bit cumbersome. \r\n\r\n### Attempted Solutions\r\n\r\nI'm currently doing implementing encryption of the plan output like this:\r\n\r\nplan:\r\n\r\n```bash\r\nplan_file=\"plan.out\"\r\nterraform init\r\nterraform plan -out \"$plan_file\"\r\ngpg --symmetric --batch --yes --cipher-algo AES256 --passphrase \"$PLAN_PASSPHRASE\" --output \"$ENCRYPTED_PLAN_FILE\" \"$plan_file\"\r\nrm \"$plan_file\"\r\n```\r\n\r\napply:\r\n\r\n```bash\r\nplan_file=\"plan.out\"\r\ngpg --batch --yes --passphrase \"$PLAN_PASSPHRASE\" --output \"${plan_file}\" --decrypt \"${ENCRYPTED_PLAN_FILE}\"\r\n\r\nterraform init\r\nterraform apply -auto-approve \"$plan_file\"\r\n```\r\n\r\nbut I really feel like it is something that terraform should be able to do for me.\r\n\r\n### Proposal\r\n\r\nIn my mind you could do one of the following to secure the output (from my favorite to least favorite):\r\n\r\n1. plan will output a checksum of the changes to be made which will be checked for in apply. In this way you can store the changes as a non-sensitive output and move it around without issues\r\n2. plan will output an encrypted plan, based on fx an envrionmental variable or commandline input, the same goes for apply for decrypting it\r\n3. terraform will output the plan sorted and without timestamps.\r\n\r\nI have tried to implement 1) based on the output that the plan gave but simply had to give up due to the varying output it could give on changes, timestamps, recursive order of json output, etc.\r\n\r\nI'm currently doing 2) (see attempted solutions) - it works fine but I honestly feel like you should support this.\r\n\r\n3) would be a simple implementation (I guess), but I would also expect that if that was simple to do, 1) would also be quite simply (just sha512 of the output from 3.). I think this is a lesser solution as the user would have to do the intermediate steps of doing the checksum and moving to make it secure in moving between locations where unauthorized users have access.\r\n\r\n\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Filesystem state storage (for backend \"local\") should write to disk only in PersistState, not in WriteState, for improved performance","body":"### Terraform Version\n\n```shell\nTerraform v1.5.7, but the same behavior with the latest versions.\n```\n\n\n### Use Cases\n\nApplying a plan with big number of resources (> 1000).\n\n### Attempted Solutions\n\nApplication is very slow due to the need to write the state after each new resource is created - when the state is becoming quite big, the state writing overhead is significant, and basically, TF works single-threaded even with high parallelism. I've looked into the #32680 but it didn't touch the `h.StateMgr.WriteState(new)` ([link](https:\/\/github.com\/hashicorp\/terraform\/pull\/32680\/files#diff-eebe3eb21459699ffeb78fa3ba978ecea3803d5b3726dbae792cecb8ec53dc88R50)) that is still written on each hook invocation.\n\n### Proposal\n\nWrap the `h.StateMgr.WriteState(new)` call into a similar condition and write state every N seconds instead (make N configurable, with the ability to enable snapshots explicitly). \r\n\r\nI did [code modifications](https:\/\/gist.github.com\/alexott\/6d5bcfcf1b3fa46bf10dc7dd8cb8157c) against 1.5.7 branch and was able significantly improved the performance for my use case.\n\n### References\n\n- #32680 ","comments":["Hi @alexott! Thanks for raising this.\r\n\r\nThe current design assumes that `WriteState` should be a relatively cheap operation that only works in-memory and doesn't do expensive stuff like making network requests.\r\n\r\nHowever, I assume based on what you've shared here that at least one state storage implementation _is_ expensive in `WriteState`, despite the interface assumptions.\r\n\r\nWe're already aware that the \"filesystem\" storage implementation implements `WriteState` by writing to disk rather than only to memory and that this is incorrect but is something Terraform has been doing since long before I was working on it and so it seemed risky to change it this late. However, if doing so would improve a real (i.e. non-development-environment) use-case then we could reconsider that risk analysis.\r\n\r\nIf you are using a state storage implementation other than the filesystem one then it would be helpful to know which one, so that we can consider making its `WriteState` implementation more efficient.\r\n\r\nI don't personally think that the state _hook_ is the right place to deal with this problem, because we do ultimately want the state manager to have a state snapshot that's as up-to-date as possible so that it has the best possible chance of successfully persisting the latest snapshot if Terraform is interrupted. I'd prefer to make the `WriteState` implementations more efficient instead, if we can find a way to do so.\r\n\r\n","Thank you for looking into this Martin. I agree with your last sentences after I spent more time looking through the code. I really tested with remote state backend (`azurerm`), and it performed better than local - primarily because it decouples writing of state (into the memory) from state persist to the cloud storage.\r\n\r\nOur case is a bit non-standard - we're trying to use Terraform for large-scale migrations of resources, and we use Terraform code to handle dependencies between objects, etc.","Thanks for confirming, @alexott!\r\n\r\nIn that case then, I guess the main thing here is to improve the performance of the filesystem state storage's `WriteState` function, probably by fixing the long-standing technical debt that it inexplicably treats `WriteState` as if it were `PersistState` and does basically nothing in `PersistState`.\r\n\r\nThis will probably require some renewed research to convince ourselves that such a change is safe to make. The fact that it would make that implementation behave more like the others is encouraging but I think probably not _sufficient_, because the filesystem implementation also has some annoying interactions with filesystem locks which make it quite sensitive to the order in which different actions are taken. However, I do expect we can find a reasonable compromise if we actively try to.\r\n\r\nI can't promise anyone will work on this immediately because everyone is busy with other stuff right now, but it does seem like something worth pursuing when we have the resources to do so. (Others reading this issue could potentially influence the prioritization by adding :+1: reactions to the original issue comment, since that's one of the signals we use to help decide what would be most impactful to work on.)\r\n\r\nI'm going to retitle this to be more similar to what I've proposed we change here, so that it'll hopefully be easier to find for folks who have a similar concern and what to upvote.\r\n","I understand you about priorities, etc. I'll look if I get time to work on it myself."],"labels":["enhancement","new"]},{"title":"Terraform init hangs with 'Building the Container Client from an Access Token (using user credentials)' error ","body":"### Terraform Version\n\n```shell\nTerraform v1.6.4\r\non darwin_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform {\r\n  required_version = \">= 1.5.2\"\r\n\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"rg-terraform\"\r\n    storage_account_name = \"tfstateterraform\"\r\n    container_name       = \"tfstate\"\r\n    key                  = \"dev.terraform.tfstate\"\r\n  }\r\n```\r\n\n\n### Debug Output\n\n```2023-11-23T12:09:02.594+0100 [TRACE] Meta.Backend: built configuration for \"azurerm\" backend with hash value 3057795898\r\n2023-11-23T12:09:02.594+0100 [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2023-11-23T12:09:02.594+0100 [DEBUG] New state was assigned lineage \r\n2023-11-23T12:09:02.594+0100 [TRACE] Meta.Backend: moving from default local state only to \"azurerm\" backend\r\n2023-11-23T12:09:02.595+0100 [DEBUG] checking for provisioner in \".\"\r\n2023-11-23T12:09:02.600+0100 [DEBUG] checking for provisioner in \"\/usr\/local\/bin\"\r\n2023-11-23T12:09:02.601+0100 [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2023-11-23T12:09:02.601+0100 [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2023-11-23T12:09:02.601+0100 [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2023-11-23T12:09:02.601+0100 [TRACE] statemgr.Filesystem: read nil snapshot\r\n2023-11-23T12:09:02.601+0100 [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2023-11-23T12:09:02.601+0100 [INFO]  Testing if Service Principal \/ Client Certificate is applicable for Authentication..\r\n2023-11-23T12:09:02.601+0100 [INFO]  Testing if Multi Tenant Service Principal \/ Client Secret is applicable for Authentication..\r\n2023-11-23T12:09:02.601+0100 [INFO]  Testing if Service Principal \/ Client Secret is applicable for Authentication..\r\n2023-11-23T12:09:02.601+0100 [INFO]  Using Service Principal \/ Client Secret for Authentication\r\n2023-11-23T12:09:02.601+0100 [INFO]  Getting OAuth config for endpoint https:\/\/login.microsoftonline.com\/ with  tenant \r\n2023-11-23T12:09:02.601+0100 [DEBUG] Obtaining an MSAL \/ Microsoft Graph token for Resource Manager..\r\n2023-11-23T12:09:02.602+0100 [DEBUG] New state was assigned lineage \r\n2023-11-23T12:09:02.602+0100 [DEBUG] Building the Container Client from an Access Token (using user credentials)\r\n```\n\n### Expected Behavior\n\nTerraform init should have (run as before) and modules, plugins should have been downloaded.\r\nTerraform state file should have been created and (as before) in the given storage account.\n\n### Actual Behavior\n\nTerraform init hangs.\n\n### Steps to Reproduce\n\n`terraform init`\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["I'm having the exact same issue. Running `TF_LOG=trace terraform init` gives me the same output. Did you find a way around this @blzsadam? Very frustrating because this prevents us from moving forward with the project... I am authenticated through ARM secrets set in my environmment\r\n\r\nEdit: \r\nTurns out the ARM secrets were expired. No idea why this was not returned as feedback....","Hi @blzsadam ,\r\n\r\nDoes your storage account have private endpoints? If yes, can you\/terraform execution environment resolve the DNS records & fetch blobs from that container?"],"labels":["bug","backend\/azure","new","v1.6"]},{"title":"Bump version of \"github.com\/golang-jwt\/jwt\/v4\" to v4.4.3","body":"### Terraform Version\n\n```shell\nTerraform version 1.6.3\n```\n\n\n### Terraform Configuration Files\n\nNA\r\n\n\n### Debug Output\n\nSecurity vulnerability \"PRISMA-2022-0270\" reported because of \"github.com\/golang-jwt\/jwt\/v4\" version v4.4.2.\r\nFixed version available is v4.4.3\r\nRequesting you to update \"github.com\/golang-jwt\/jwt\/v4\" version from v4.4.2 to v4.4.3\n\n### Expected Behavior\n\nVulnerability scanner should not report PRISMA-2022-0270\n\n### Actual Behavior\n\nVulnerability scanner reporting PRISMA-2022-0270 \n\n### Steps to Reproduce\n\nBy running twistlock security scanner over container installed with Terraform\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @Bjyothi2023,\r\n\r\nAccording to the upstream issue https:\/\/github.com\/golang-jwt\/jwt\/issues\/258, this vulnerability report is invalid. The upstream maintainers suggest that the new release does not change anything material about the code and instead they've just clarified the documentation to reflect correct vs. incorrect usage of the library, and so upgrading alone would not be sufficient if there was a problem here.\r\n\r\nFor our part, we will review our usage of this library to ensure we are not using it in the incorrect way that issue discusses.\r\n\r\n"],"labels":["bug","new","v1.6"]},{"title":"Variable & Output Schema References","body":"### Terraform Version\r\n\r\n```shell\r\n`Terraform v1.5.5`\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\n- Easier creation and management of modules with child modules by:\r\n  - Allowing variables in \"parent\" modules to reference, and ideally override where desired, variables in child modules which are exposed to the consumer of the \"parent\" module.\r\n  - Allowing outputs in the \"parent\" module to reference, and ideally override where desired, outputs from child modules which are exposed in the \"parent\" module.\r\n  - This should include all \"supported\" schema, however there's likely a line we can draw on how worthwhile it would be\r\n- This should be something that generators like `terraform-docs` (https:\/\/terraform-docs.io\/) or utilities such as the Terraform LS (https:\/\/github.com\/hashicorp\/terraform-ls) would be able to \"read\" the variable\/output schema such that there's no real difference between a \"referenced' and \"native\/handwritten\" variable or output, since these should all go through a reference merge upon the read action.\r\n\r\n### Attempted Solutions\r\n\r\n- The only solution current available is copy-pasting variable or output blocks from the child modules or using code generation tooling.\r\n\r\n### Proposal\r\n\r\nMy proposal would be creating a special argument on the variable and output blocks, where you could use to reference a variable \/ output in the child module. By doing this reference, variable attributes from the reference child modules would be set as \"defaults\" in the variable where this reference is being used. The user would then have an option to start adding one or more of the standard arguments which would \"override\" the value pulled from the referenced variable.\r\n\r\nFor variables, the focus on what we want to include \/ set in the reference would be these arguments:\r\n- `default`\r\n- `type`\r\n- `description`\r\n- `sensitive`\r\n- `nullable`\r\n\r\nPotential problem areas \/ exceptions to being allowed:\r\n- `validation`: In the current state this could be OK, since these validations can only reference local data. However, if the ability to do cross-variable references in validation blocks is allowed, this probably would be best to not be referenced and just force the user to write their own here if they need it. In theory, since the use case is primarily surrounding DRYing up code created when creating modules of modules, the actual validation block on the \"real\" module should be doing the validation job already for us, making this unnecessary (unless this feature is misused or used incorrectly, which is on the module author). This should probably just be not included in any sort of reference logic.\r\n\r\nFor outputs, the focus on what we want to include in the reference would be these arguments:\r\n- `description`\r\n- `sensitive`\r\n\r\nA \"maybe\" where the mechanics of this probably diverge from the other arguments would be:\r\n- `value`: I think it could be a \"nice to have\" to have the reference assume you want the value to be `modue.child_module.referenced_output`, however that diverges from the \"basic\" behavior of my code proposal below\r\n\r\nPotential problem areas \/ probably should not be included:\r\n- `precondition`: Since these can reference any object in the child module, which the parent may or may not be aware of, this should just be \"left off\". The child module itself will be handling the actual execution of this so it seems pointless to \"clone\/copy\" this.\r\n- `depends_on`: Same as above, these are generally local to the module where they are defined, seems pointless to include for the purpose of this proposal.\r\n\r\nA `from` argument inside of the variable and output block would be the place the reference is configured. The \"referenced\" variables and outputs would be referred to by their address local to the parent module, for example `module.my_child.variable.my_cool_var`. The `from` argument would be the only allowed location for this type of reference, all other usages would cause a validation failure.\r\n\r\nA sample of the TF code I'm thinking would be implementing this is:\r\n\r\nSample child module:\r\n```tf\r\nvariable \"my_cool_var\" {\r\n  type = string\r\n  description = \"hello\"\r\n  default = \"hi\"\r\n}\r\n\r\nresource \"terraform_data\" \"hello\" {\r\n  input = var.my_cool_var\r\n}\r\n\r\noutput \"my_cool_output\" {  \r\n  value = terraform_data.hello.id\r\n  sensitive = true\r\n  description = \"this is a cool sensitive output\"\r\n}\r\n```\r\n\r\nSample parent module:\r\n```tf\r\nvariable \"my_parent_var\" {\r\n  from = module.my_child.variable.my_cool_var\r\n}\r\n\r\nvariable \"my_useless_example_var\" {\r\n  from = module.my_child.variable.my_cool_var\r\n  description = \"hello I'm useless\"\r\n}\r\n\r\nmodule \"my_child\" {\r\n  source = \".\/my-child\"\r\n  my_cool_var = var.my_parent_var\r\n}\r\n\r\noutput \"my_parent_output\" {\r\n  from = module.my_child.output.my_cool_output\r\n}\r\n```\r\n\r\nTo the \"user\" \/ \"schema reader\" the parent module schema should look equivalent to the following (after \"merge\"):\r\n```tf\r\nvariable \"my_parent_var\" {\r\n  type = string\r\n  description = \"hello\"\r\n  default = \"hi\"\r\n}\r\n\r\nvariable \"my_useless_example_var\" {\r\n  type = string\r\n  description = \"hello I'm useless\"\r\n  default = \"hi\"\r\n}\r\n\r\noutput \"my_parent_output\" {\r\n  value = module.my_child.my_cool_output\r\n  sensitive = true\r\n  description = \"this is a cool sensitive output\"\r\n}\r\n```\r\n\r\nPotential further extension could be taken to \"reference\" arguments inside of resources, however the significant different in the schema for a resource argument and a module variable are such that it only makes sense to do:\r\n- `description`\r\n- `type`: if there's a good way to represent the variety of schema types and convert them into a \"what I'd put in the module type field\" easily. Basic stuff should be simple enough, however things requiring the use of `dynamic` and the like could be problematic\r\n- `default`: Since we can have all sorts of ways a default value is set, this probably just just either be not included (aka required var) or just set to `null`, based on the downstream configuration of the `Required` and `Optional` fields in ther resource.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Terraform 1.6.x does not find *.tfstate file in s3 which was created using older versions of terraform","body":"### Terraform Version\n\n```shell\nTerraform v1.6.4\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\nHere is backend.tf file:\r\n\r\n```terraform\r\nterraform {\r\n  backend \"s3\" {\r\n    bucket         = \"terraform-bucket\"\r\n    dynamodb_table = \"terraform-state-lock\"\r\n    encrypt        = true\r\n    key            = \".\/terraform.tfstate\"\r\n    region         = \"eu-central-1\"\r\n  }\r\n```\r\n\n\n### Debug Output\n\nHere is log from terraform 1.5.x (credentials removed) (result - 200 OK):\r\n```\r\n-----------------------------------------------------\r\n2023-11-15T12:11:55.779+0200 [DEBUG] [aws-sdk-go] {}\r\n2023-11-15T12:11:55.780+0200 [DEBUG] [aws-sdk-go] DEBUG: Request s3\/GetObject Details:\r\n---[ REQUEST POST-SIGN ]-----------------------------\r\nGET \/terraform.tfstate HTTP\/1.1\r\nHost: terraform-bucket.s3.eu-central-1.amazonaws.com\r\nUser-Agent: APN\/1.0 HashiCorp\/1.0 Terraform\/1.5.7 aws-sdk-go\/1.44.122 (go1.20.7; linux; amd64)\r\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\nX-Amz-Date: 20231115T101155Z\r\nAccept-Encoding: gzip\r\n\r\n\r\n-----------------------------------------------------\r\n2023-11-15T12:11:55.991+0200 [DEBUG] [aws-sdk-go] DEBUG: Response s3\/GetObject Details:\r\n---[ RESPONSE ]--------------------------------------\r\nHTTP\/1.1 200 OK\r\nConnection: close\r\nContent-Length: 384760\r\nAccept-Ranges: bytes\r\nContent-Type: application\/json\r\nDate: Wed, 15 Nov 2023 10:11:56 GMT\r\nEtag: \"f357341b9645d1643cb8ee79022cd215\"\r\nLast-Modified: Fri, 27 Oct 2023 08:46:51 GMT\r\nServer: AmazonS3\r\nX-Amz-Id-2: Y9LAIgVKl5bjua2wExd1quq5KXdnDq5wCd8eE3GbFJlqo3bNOkVvqjoZITgeZkp9JuMHn8wRw0E=\r\nX-Amz-Request-Id: BQPC6ES63540GTHN\r\nX-Amz-Server-Side-Encryption: AES256\r\nX-Amz-Version-Id: nhKueNmyZNat3tqAlmwAeGFTdcYYwWPN\r\n```\r\n\r\nHere is the log from terraform 1.6.x (404 error):\r\n\r\n```\r\n2023-11-15T12:15:07.874+0200 [INFO]  backend-s3: Downloading remote state: tf_backend.operation=Get tf_backend.req_id=c6b7a8aa-e870-7d70-8964-52935f98b12d tf_backend.s3.bucket=terraform-bucket tf_backend.s3.path=.\/terraform.tfstate\r\n2023-11-15T12:15:07.875+0200 [DEBUG] backend-s3: HTTP Request Sent: aws.operation=HeadObject aws.region=eu-central-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=c6b7a8aa-e870-7d70-8964-52935f98b12d tf_backend.s3.bucket=terraform-bucket tf_backend.s3.path=.\/terraform.tfstate http.request.header.x_amz_date=20231115T101507Z http.method=HEAD net.peer.name=terraform-bucket.s3.eu-central-1.amazonaws.com http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.3 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/linux lang\/go#1.21.3 md\/GOOS#linux md\/GOARCH#amd64 api\/s3#1.38.5\" http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=AKIA************AKO7\/20231115\/eu-central-1\/s3\/aws4_request, SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;x-amz-content-sha256;x-amz-date, Signature=*****\" http.request.header.x_amz_content_sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 http.url=https:\/\/terraform-bucket.s3.eu-central-1.amazonaws.com\/.\/terraform.tfstate http.request.header.amz_sdk_invocation_id=813d4010-7c72-4ccf-a30e-59331be39944 http.request.header.accept_encoding=identity http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.body=\"\"\r\n2023-11-15T12:15:07.921+0200 [DEBUG] backend-s3: HTTP Response Received: aws.operation=HeadObject aws.region=eu-central-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=c6b7a8aa-e870-7d70-8964-52935f98b12d tf_backend.s3.bucket=terraform-bucket tf_backend.s3.path=.\/terraform.tfstate http.response.header.date=\"Wed, 15 Nov 2023 10:15:07 GMT\" http.response.header.server=AmazonS3 http.response.body=\"\" http.duration=46 http.status_code=404 http.response.header.x_amz_request_id=NX3QC1MC0N9N5TW6 http.response.header.x_amz_id_2=\"yUmgCqK3zpSB0bReJ5bxxCy0B88ZhARUw5Omn5VsbCUzGSUbADoqnOBurOZZ+GL0a6eDbY7gl4Q=\" http.response.header.content_type=application\/xml\r\n```\r\n\n\n### Expected Behavior\n\nExpect plan successfully executed\n\n### Actual Behavior\n\nterraform plan returns error:\r\n\r\n\u2502 Error: error loading state: state data in S3 does not have the expected content.\r\n\u2502 \r\n\u2502 The checksum calculated for the state stored in S3 does not match the checksum\r\n\u2502 stored in DynamoDB.\r\n\u2502 \r\n\u2502 Bucket: ocb-ocb-terraform-state\r\n\u2502 Key:    .\/terraform.tfstate\r\n\u2502 Calculated checksum: \r\n\u2502 Stored checksum:     bb17f4c60b305db0f0fd8210c8da8332\r\n\u2502 \r\n\u2502 This may be caused by unusually long delays in S3 processing a previous state\r\n\u2502 update. Please wait for a minute or two and try again.\r\n\u2502 \r\n\u2502 If this problem persists, and neither S3 nor DynamoDB are experiencing an\r\n\u2502 outage, you may need to manually verify the remote state and remove the Digest\r\n\u2502 value stored in the DynamoDB table\r\n\u2502 \r\n\u2502 \r\n\u2502 \r\n\r\n\n\n### Steps to Reproduce\n\nUsing terraform 1.5.x apply plan:\r\n- terraform init\r\n- terraform apply\r\n\r\nUpgrade to 1.6.x and then apply:\r\n- terraform apply\r\n\r\n1.6 version returns error:\r\n\n\n### Additional Context\n\n_No response_\n\n### References\n\nAlso created issue in terragrunt, because backend.tf is actually created by terragrunt:\r\nhttps:\/\/github.com\/gruntwork-io\/terragrunt\/issues\/2792\r\n","comments":["Have you found any workarounds for this? Like manually updating the state file to transition to 1.6?","Hi @jurna and @TheEdgeOfRage. As a workaround, you can remove the leading `.\/` from the `key`.\r\n\r\nTerraform v1.5.x and lower used the AWS SDK for Go v1, which stripped the leading `.\/` from S3 object keys, as well as making some other changes such as removing double slashes. Terraform v1.6 uses the AWS SDK for Go v2, which uses the key as-is"],"labels":["bug","backend\/s3","new","v1.6"]},{"title":"Update environment-variables.mdx","body":"Previous sentence mentions that the variables will be checked last for a value, this is confusing and leads to think that environment variables are the last evaluated and most important in precedence order.\r\n\r\nNo github issue or discussion started since it is a very small contribution to documentation not code, please accept or refuse as convenient. Thank you!","comments":["\n[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/not_signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34271)\n\n\nThank you for your submission! We require that all contributors sign our Contributor License Agreement (\"CLA\") before we can accept the contribution. [Read and sign the agreement](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34271)\n\n[Learn more about why HashiCorp requires a CLA and what the CLA includes](https:\/\/www.hashicorp.com\/cla)\n\n<sub>Have you signed the CLA already but the status is still pending? [Recheck it](https:\/\/cla.hashicorp.com\/check\/hashicorp\/terraform?pullRequest=34271).<\/sub>","Thanks for this submission! Please sign the CLA for this to be considered for inclusion. Thanks again!"],"labels":["waiting-response","documentation"]},{"title":"tfstate file can be corrupted if operation cancelled","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.4\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nN\/A\r\n\r\n### Debug Output\r\n\r\nN\/A\r\n\r\n### Expected Behavior\r\n\r\n**Note:** I originally thought this was a `cdktf` bug but it seems this issue is related to `terraform` directly. The expected behaviour and reproduction steps still used `cdktf` although it shouldn't be necessary to reproduce. \r\n\r\n---\r\n\r\nI am running `cdktf destroy` via a python using `subprocess.run`. The exact command is \r\n```python\r\nimport subprocess\r\nsubprocess.run([\"cdktf\", \"destroy\", \"--auto-approve\"], check=True)\r\n```\r\n\r\nWhen this is destroying instances, pressing `ctrl +c` should stop the destroy and leave the `terraform.<stack>.tfstate` file with an accurate list of state.\r\n\r\n### Actual Behavior\r\n\r\nThe `terraform.<stack>.tfstate` file is empty.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n**Note:** The below instructions are one way to induce the issue but it is likely there are others. Using raw `terraform` it should be possible if the state is large and the `destroy` command is exited forcefully enough.\r\n\r\n---\r\n\r\n1. Start an instance with a lot of resources. For example many files.\r\n2. Run `cdktf destroy` via python\r\n```python\r\nimport subprocess\r\nsubprocess.run([\"cdktf\", \"destroy\", \"--auto-approve\"], check=True)\r\n```\r\n4. When the resources are being destroyed, press `ctrl +c`.\r\n\r\nIt seems like the `python` subprocess call is required for the state to be emptied permanently, however using a raw `cdktf destroy` I noticed that the state file is briefly emptied and then recreated. This seems dependent on the size of the state. I can reliably reproduce this with a ~5MB state file.\r\n\r\n\r\n\r\n### Additional Context\r\n\r\nI believe this issue is related to the `TODO` comment here\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/1f9734619f953ecc7252d7b98a6d40d751b4ea1e\/internal\/states\/statemgr\/filesystem.go#L124C1-L129C18\r\n\r\nWith a large state file, an interrupt can happen between [clearing the file](https:\/\/github.com\/hashicorp\/terraform\/blob\/1f9734619f953ecc7252d7b98a6d40d751b4ea1e\/internal\/states\/statemgr\/filesystem.go#L194), and the updated state being completely [written](https:\/\/github.com\/hashicorp\/terraform\/blob\/1f9734619f953ecc7252d7b98a6d40d751b4ea1e\/internal\/states\/statemgr\/filesystem.go#L219).\r\n\r\nThe solution is likely what the comment suggests. Write the state file to a temporary file and then do a \"swap\" to update the state file. This would ensure the state file is never in a \"corrupted\" state.\r\n\r\n### References\r\n\r\nI originally reported this against under cdktf https:\/\/github.com\/hashicorp\/terraform-cdk\/issues\/3269","comments":["Thanks for reporting this, @MJohnson459.\r\n\r\nWhen running outside of CDK for Terraform, Terraform CLI's behavior for SIGINT is essentially a state machine with three states:\r\n\r\n- In the initial state, no interruption has happened yet and Terraform is running normally.\r\n- On first SIGINT, Terraform CLI enters a \"graceful abort\" state, where it flushes a state snapshot to the configured storage immediately, asks providers to gracefully stop whatever they are currently doing, and also begins proactively flushing state snapshots to persistent storage every time a resource instance operation completes. If Terraform manages to run to completion while in this state then it will exit with at least one error message and with the state persisted correctly.\r\n- On second SIGINT (when Terraform CLI is already in the \"graceful abort\" state, Terraform CLI enters an \"urgent abort\" state where it immediately exits regardless of what else might be going on. This is to allow users to recover if, for example, the state snapshot saving is the thing that's preventing the exit, but it does cause a risk of data loss if the state snapshot saving isn't atomic and gets interrupted, such as in the filesystem storage implementation you linked to here.\r\n\r\nWith all of that said then, I think probably a first step here would be to try to understand how CDK for Terraform is participating in this process. When you are using CDK for Terraform to wrap Terraform CLI, your Ctrl+C will cause SIGINT to be sent to the CDK for Terraform process. That signal might _also_ be sent to Terraform CLI, if CDK for Terraform is starting Terraform CLI in the same process group.\r\n\r\nOne possible cause of the problem you observed here would be if CDK for Terraform _is_ starting Terraform CLI in its same process group, therefore causing your Ctrl+C to be sent as SIGINT to both processes, but _then_ CDK for Terraform sends its own separate SIGINT to Terraform CLI to try to command it to shut down, which would then nudge Terraform CLI into the \"urgent abort\" state.\r\n\r\nI'm not familiar enough with the CDK for Terraform codebase to know if I've found a relevant piece of code, but I do see [some logic for sending Ctrl+C to Terraform CLI](https:\/\/github.com\/hashicorp\/terraform-cdk\/blob\/44c3272e24648f3470a38f8b65ce2cb8948821d8\/packages\/%40cdktf\/cli-core\/src\/lib\/models\/pty-process.ts#L71-L77) which seems to achieve its goal by running Terraform CLI in a pseudoterminal (pty) and then sending the Ctrl+C character code to that pseudoterminal, which presumably causes _something_ running in that terminal to send a SIGINT to Terraform CLI. Therefore my theory above does seem plausible, but I'd need to ask the CDK for Terraform team to confirm.\r\n\r\n---\r\n\r\nIf that is indeed the problem, I think the fix for this would need to be in CDK for Terraform itself, making one of the following changes.\r\n\r\n1. Launch Terraform CLI in such a way that the calling shell's job control will translate Ctrl+C into a SIGINT only for the cdktf process and _not_ also for the Terraform CLI process. Then CDK for Terraform can be responsible for sending its own SIGINT to Terraform CLI.\r\n2. Remove the abort logic that sends Ctrl+C to the pty and instead allow the normal shell job control to send SIGINT to both cdktf and Terraform CLI simultaneously, at which point Terraform CLI will enter its graceful abort state concurrently with cdktf doing any of its own shutdown actions.\r\n\r\nThe main difference between these two options, I think, is that the second one would preserve Terraform CLI's ability to enter its urgent abort state if the user types Ctrl+C twice -- since Terraform CLI would be directly relating to the job control signal -- whereas the first one would make it CDK for Terraform's responsibility to decide whether to send a second SIGINT to Terraform CLI when Ctrl+C is pressed twice.\r\n\r\n\r\n","Thanks @apparentlymart \r\n\r\nI tried and failed to do an `strace` to see what signals were going where but _I think_ I found out what the trigger was in my case.\r\n\r\nIt seems like by default, `Popen` in [python 3.7+ will wait at most 0.25s ](https:\/\/bugs.python.org\/issue25942) for child processes to clean up on a `SIGINT`. Using `subprocess.run` exacerbates this by [sending a kill](https:\/\/github.com\/python\/cpython\/blob\/ce1096f974d3158a92e050f9226700775b8db398\/Lib\/subprocess.py#L566) immediately after that delay. I switched to use `Popen` and waited manually to ensure terraform has time to clean up to fix.\r\n\r\nThat said, I still believe the corruption is a bug in terraform as even in the case where terraform is forcefully shut down, it still shouldn't be able to corrupt the state. As a further example, when I was using `strace` to launch `cdktf` it _also_ corrupted the state. I suspect that if a user did follow the `Ctrl+C` twice path that would also result in a corrupt state file.\r\n\r\nAll in all, saving to an intermediate file and swapping seems a safer way to manage it and avoid any of these problems. I'm not familiar with Go but I am happy to try implement this if you have some pointers.\r\n","Hi @MJohnson459,\r\n\r\nIf I recall correctly, the filesystem backend is implemented the way it is because of how file locking works on Windows: Terraform needs to hold a lock on the state snapshot throughout, and that blocks any kind of renaming or deleting of the existing object on disk.\r\n\r\nThe local filesystem storage is intended primarily for trivial experimentation anyway, and we expect anyone using Terraform \"for real\" should be using one of the remote state backends, which (depending on the features of the underlying storage) bring additional safety such as the preservation of historical state versions.\r\n\r\nEven with local state, there should typically be a separate `terraform.tfstate.backup` file capturing the snapshot of the previous run in case the new snapshot is somehow unacceptable, including this situation of it ending up empty.\r\n\r\nIf we could find a different design that would work across all platforms Terraform supports -- the primary supported platforms are Windows, macOS, and Linux, and the other best-effort-supported unixes _tend_ to follow from what we do for Linux and\/or macOS -- then that would be great! I believe we did look for other options at the time of originally implementing this and ended up concluding that we could either have locks _or_ atomic updates and decided that locks were more important, and the locking behavior is now protected by the Terraform v1.x Compatiblity Promises and so we cannot regress it -- but if we can find a way to achieve both together then of course that would be ideal.\r\n","Hi @apparentlymart \r\n\r\nMy use case is generally large but short term deployments which the local backend seems ideally suited to. Using cloud storage would add a fair bit of complexity and I'm not sure we would get much benefit from it. Would you still recommend using cloud backends for jobs that only last for a couple of hours with no collaboration?\r\n\r\nRegarding the proposed change, it looks like it would be fairly straight forward but there are definitely bits I don't understand. This comment does imply the the intention was to swap the files in the future but I guess things may have changed since then.\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/1f9734619f953ecc7252d7b98a6d40d751b4ea1e\/internal\/states\/statemgr\/filesystem.go#L124C1-L129C18\r\n\r\nIf I understand the code correctly, it\r\n1. Creates a backup file and saves the current state to the backup file\r\n2. Cleans the current state file\r\n3. Checks if the state has changed and increments the serial if it has\r\n4. Save the current state + updated serial to the state file\r\n\r\nThe question is really which bit takes a long time. Is there a reason that step 2. couldn't be be delayed or 4. write to an intermediate file first?","I think the main constraint as I understand it is that the following cannot both be true at once:\r\n\r\n1. The file's identity (in whatever sense the OS tracks it for locking purposes) remains constant throughout the whole write operation.\r\n2. The observable content of the file is valid throughout the whole write operation.\r\n\r\nThe typical way to achieve 2 on Unix is to make a new file and then rename it over the old one, but that invalidates 1. As far as I remember, even _that_ wasn't possible on Windows where holding the lock _forces_ the file's identity to stay constant.\r\n\r\nCurrently Terraform is prioritizing 1 at the expense of 2, which in the worst case means that Terraform can potentially be interrupted while the content of the file is not valid.\r\n\r\nThe comment in the file is reflecting that we wanted to find a way to achieve both at once. If we can find a design that does so without breaking any existing guarantees (due to the 1.x Compatibility Promises) then we would like to adopt it, and the question would then just be how to mitigate the risk of making that change.\r\n\r\nOne interesting constraint in the mix here is that in particularly-awkward cases the two systems competing to acquire a lock might not be running the same version of Terraform CLI, and in particular one might be running the _current_ implementation which expects the lock to be held on the same file containing the state data. This constraint has tended to be the upset that's always got in the way of my first idea in this area, which is to use a separate file (whose content is irrelevant) as the holder of the lock, so that other files around it can then change identity without affecting the locked file.\r\n\r\nMy brain is in a very different place than this right now and so I'm afraid I don't have any more context ready to share off the top of my head than what I've shared already, but if you have any specific design ideas then one of my colleagues on the team can hopefully participate in a more detailed discussion about them!\r\n\r\n","Thanks @apparentlymart that makes sense.\r\n\r\nI did a quick and dirty profiling using some extra logging after every statement to see where the time was actually being spent. It might be that there could be a good improvement just by moving the Truncate later.\r\n\r\nSplitting the `Write` call into three parts, the times were roughly\r\n```\r\nManaging backup: ~7ms (after the first one)\r\n<-- File truncated here -->\r\nChecking state: ~134ms\r\nWriting state: ~54ms\r\nTotal: ~195ms\r\n```\r\n\r\nThe main thing was the encoding in `statefile.writeStateV4` takes ~50ms on my machine with a large-ish state. Between the call to `Truncate` this gets called three times (two during the check, once for the real write) which means ~150ms out of the ~188ms that the file is empty is just processing the state.\r\n\r\nMoving the `Truncate` method to just before the final `Write` call would reduce \"risky\" time from 188ms to 54ms or 96% to 27%.\r\nThat seems a reasonably small change for a big effect? It doesn't solve the problem but it might make it much rarer. I can put a PR up for this if it seems something worth doing.\r\n\r\nIn the _ideal_ case, if we could move the `Truncate` to right before the [final Write](https:\/\/github.com\/hashicorp\/terraform\/blob\/f7149c216d08ca048d70fd75a70711d8f26b86de\/internal\/states\/statefile\/version4.go#L436C15-L436C15) in `writeStateV4` we could get that time down to ~3ms or ~2% but I can't see a simple way to do that while using the `io.Writer` interface. It would require splitting the function into \"encode\" and \"write\" which would be a fairly large change.\r\n\r\nAnyway, I'm going to look into using a remote backend as suggested which should avoid this for us at least.","Thanks for that extra context, @MJohnson459!\r\n\r\nI wonder if @jbardin has some further thoughts here, since I think he's the one that's spent most time in this part of the system in the past.\r\n","I agree that moving the truncate call to after the marshal check would be a safe way to lessen the time spent with no state when there is a lot of data, though that's not really a fix for the problem.\r\n\r\nOne thing we might be able to do is create platform-specific methods for the `Filesystem` type, so that `writeState` on POSIX systems can do an atomic write, but we would need to do some research to see how the locks interact. Linux locks are only advisory locally, but could be enforced by a remote system. NFSv4 was the notable use case before, but since this was all designed CIFS mounts added mandatory locks in Linux5.4."],"labels":["bug","new","v1.6"]},{"title":"Read state lock info and lock holder info from environment variables","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\nThe defult `LockInfo` object created by terraform duirng state locking does not use the `Info` field present in `LockInfo` object which is useful for storing metadata. Neither does the it effectively set the `Who` field. Currently the `Who` field default to `user@host`. Which isn't useful when the lock is being created from inside a docker image or in a CI\/CD pipeline. In those cases, the user might find it useful if the `Who` field was instead configurable. \r\n\r\nThis PR adds the feature of reading `Info` and `Who` fields for the `LockInfo` object from the `TF_LOCK_METADATA` and `TF_LOCK_OWNER_ID` environment variables if present from the `statemgr.NewLockInfo` method which is used as the default `LockInfo` factory in all backends in this repo. If the environment variables are not present, the code defaults to the old behavior and thus wouldn't change anything.   \r\n\r\nAs a rationale for the usages of this feature, consider the case where terraform plans are ran from a CI\/CD pipeline like in Jenkins. One might want to know which build of which job caused the plan to lock. Then setting `TF_LOCK_OWNER_ID=${JOB_NAME}-${JOB_NUMBER}` will resolve the issue.\r\n  \r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/23910\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/26928\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### ENHANCEMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n- User can add custom information about state lock if they want by adding the information in `TF_LOCK_METADATA` environment variable.\r\n- User can add customize the who field of `LockInfo` by exporting the `TF_LOCK_OWNER_ID` environment variable.\r\n- These features will only be present in backends that do not write those two fields themselves i.e, uses the default LockInfo factory as is i.e the PR only enhances the behavior of the default LockInfo generation process.\r\n- If those environment variables are not present the behavior will remain unchanged i.e the `Info` field won't be present, and the `Who` field will default to `user@host` i.e this PR only adds on top of existing features are keeps it backward compatible. \r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=34265) <br\/>All committers have signed the CLA.","Thanks for this submission! I'll raise it in triage. ","Hi @hsiam261, the summary of the feedback from triage is that there is an internal conversation around changing a big feature adjacent to this pull request, and reviewing\/accepting this now would make that change more difficult. The bad news is that discussion will likely take months \/ quarters to resolve, and so you are unlikely to see much movement on this PR in that time frame. \r\n\r\nAssuming we did review this in the future, there is one piece of feedback to note. We've been moving away from directly accessing environment variables from inside Terraform's \"guts\", because that makes it much harder to keep all of the env vars in mind under future maintenance and requires us to do undesirable things in tests to fake the environment variables. However, I do not recommend making any major changes at this time (to plumb the values through multiple layers of code to avoid using environment variables) since it is uncertain at this time whether we will even move forward with this PR.  \r\n\r\nThanks for this submission, and I am sorry we cannot give you a definitive answer today on this PR. If you want to check in in a few months, we might have more to share; if you chose to close it we understand. Thanks again!"],"labels":["enhancement"]},{"title":"Terraform Test: add JUNIT output","body":"### Terraform Version\n\n```shell\nterraform version\r\nTerraform v1.6.3\r\non darwin_arm64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.6.4. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Use Cases\n\nTools like `tfsec` and `tflint` can output their test results as junit xml, which can then eg. be published and shown in the tests tab of azure devops pipeline runs. \n\n### Attempted Solutions\n\nn\/a it's not in the documentation\n\n### Proposal\n\nAdd the functionality to export test results (or at least on test failure) as JUNIT XML files, so this can be used in eg. Azure Devops to show result in the tests tab, like tfsec \/ tflint. \n\n### References\n\n_No response_","comments":["Thanks for this feedback, @rvdouderaa!\r\n\r\nEarlier iterations of the `terraform test` experiment did have some JUnit XML support, but we found two challenges along the way:\r\n\r\n1. The JUnit XML format was originally tailored for JUnit itself, and it's close enough to be useful for various other test frameworks in other programming languages that us similar programming paradigms (object-oriented \/ imperative), but it's not so clear how best to map Terraform's quite different concepts onto its model.\r\n2. JUnit XML is not actually formally specified anywhere and so supporting it seems to be a matter of just finding and studying all implementations of it to learn what subset of the format is reasonable to use, how to use that subset so that the test result output is actually useful, and working around bugs and quirks in the implementations.\r\n\r\nWe also got very, very little feedback on that part of the experiment and so we didn't feel confident enough in our guesses at problem 1 to warrant spending all the time working through problem 2.\r\n\r\nHowever, now that you're here with a request specifically _for_ this, we can potentially make some progress on problem 1!\r\n\r\nTo help with that, I wonder if you'd be willing to create and share a practical example of a JUnit XML file reporting the results of a realistic `terraform test` run you've done where there were some interesting failures to report, and\/or where the success case produces something useful for you in the Azure DevOps UI.\r\n\r\nThat would help by illustrating one possible mapping from Terraform's concepts to the JUnit concepts that is at least useful for Azure DevOps in particular, and hopefully also useful in some other similar test orchestrators that support JUnit XML. The idea here would be to figure out a mapping that is _useful in practice_ with JUnit XML parser implementations that are in real use, as opposed to a mapping which is theoretically plausible but causes the result to be less useful in real existing test reporting software.\r\n\r\nThanks again!\r\n\r\n","Thanks for the response @apparentlymart\r\n\r\nI found the following page, which describes the format\r\nhttps:\/\/github.com\/testmoapp\/junitxml \/ https:\/\/windyroad.com.au\/dl\/Open%20Source\/JUnit.xsd\r\n\r\nMaybe no official documentation, however the Apache Ant implementations seems to be the defacto standard.\r\n\r\nAs for use cases. We want to add Terraform Test to our CI\/CD pipelines as we are using `tfsec` and `tflint`. Results should be exported to a readable format by Azure Devops, so test results can be published and pipeline then failed. This would need a feature to continue when he step fails, so the error can be handled by the nexts step. \r\n\r\nIt doesn't really matter on which test it would fail, the tests are there for a reason. \r\n\r\nFor example:\r\n```hcl\r\n\r\nrun \"input_validation\" {\r\n    command = plan\r\n\r\n    variables {\r\n        name = \"asdfaslkdjfkasjdflkasjdf!@$@$\"\r\n        resource_group_name = \"core-services-rg\"\r\n    }\r\n\r\n    expect_failures = [\r\n        var.name\r\n    ]\r\n}\r\n```\r\n\r\nThis name should not be accepted by the module (only a-z and 0-9) so the input should fail. If not, something is wrong with the modules validation check and we should not be able to publish the module. And we would it to be displayed like this `tfsec` error\r\n\r\n<img width=\"1210\" alt=\"image\" src=\"https:\/\/github.com\/hashicorp\/terraform\/assets\/27759759\/a603ea69-648a-48df-8a23-819e98ae1989\">\r\n\r\nHope this makes our usecase(s) clear. ","Addition:\r\n\r\nThis are the test formats supported by Azure Devops:\r\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/devops\/pipelines\/tasks\/reference\/publish-test-results-v2?view=azure-pipelines&tabs=trx%2Ctrxattachments%2Cyaml\r\n\r\nJunit scheme:\r\nhttps:\/\/github.com\/windyroad\/JUnit-Schema\/blob\/master\/JUnit.xsd","Thanks for that extra context, @rvdouderaa!\r\n\r\nIf you're able to do so, it would help to see an example of exactly what JUnit XML `tfsec` and `tflint` are creating for you, since presumably in your case mimicking how _those_ tools use JUnit XML would give a result that fits well with how you're already using it with those other tools.\r\n\r\nOf course if you can't share it then we ought to be able to test with those tools ourselves at some point to find out, but seeing an example from your already-running system will make it easier to quickly see if mimicking how those tools use the format is likely to be a viable strategy for `terraform test`.\r\n\r\nIf you _would_ like to share those files, I'd suggest creating a [GitHub Gist](https:\/\/gist.github.com\/) with the two (or more) files inside it and linking to it here, just because GitHub comments don't work very well for sharing longer code examples.\r\n\r\n\r\nThanks!\r\n","@apparentlymart \r\nI created a gist with a `tfsec` and a `tflint` xml output\r\nhttps:\/\/gist.github.com\/rvdouderaa\/40821f63aa1407279a3e29292f34ce0c","Thanks for sharing those, @rvdouderaa.\r\n\r\nIt seems that both of these tools have made the decision that the file containing the problem should be treated as the \"test class name\" in JUnit terms. However, it also seems from your screenshot of Azure DevOps that it doesn't actually pay attention to that at all; I don't see the filenames appearing anywhere in the UI.\r\n\r\nIt also seems that tfsec elected to use \"tfsec\" as the name of the entire suite, while tflint did not name the test suite at all.\r\n\r\nFor naming the test cases themselves, tflint used some sort of identifier `terraform_typed_variables` -- I guess that's the machine-readable name for one of its lint rules? -- while tfsec seems to have just chosen a human-readable description of the problem itself (duplicating the text from the failure message inside) rather than of what was being tested.\r\n\r\nFor `terraform test` we have some additional concepts that would need to be mapped onto the JUnit XML model:\r\n\r\n- The name of the test scenario (the `.tftest.hcl` file) that was being evaluated.\r\n- The name of the test step (the label of the `run` block) that was being evaluated.\r\n- The address of each checkable object, when reporting the outcomes of preconditions, postconditions, variable validation rules, and `check` blocks.\r\n\r\nGiven that, I suppose one possible way to map it would be:\r\n\r\n- The overall file is rooted with a `testsuites` element, so that the report can include multiple suites.\r\n- Each test scenario is a separate `testsuite` element, named after the basename of the `.tftest.hcl` file it came from.\r\n- Inside each suite, each `run` block is a `testcase`, whose name is the label in the `run` block header. `class` would not be present at all, since Terraform doesn't have anything analogous to \"classes\" in JUnit, and the tool you are using seems to ignore it anyway.\r\n- If a test step encounters an error or warning, the usual diagnostics rendering (with color codes omitted) would be placed in a `system-err` element beneath that test step's `testcase`.\r\n- If at least one checkable object fails any of its checks, the `testcase` contains a `failure` element whose body contains some textual representation of all of the failures.\r\n\r\n    (It isn't clear to me whether multiple `failure` elements are supported; ideally there'd be a separate `failure` for each checkable object that failed so that each one can be presented separately, but I've assumed here that tools would only accept one since I can't see any example of multiple failures in the docs you linked.)\r\n\r\nDoes that seem plausible to you as a way to populate the JUnit XML format based on a `terraform test` outcome?\r\n\r\n","@apparentlymart you can see the testsuite name in the Azure Devops screenshot (first line with a cross, `tfsec (1\/1)`\r\nThat's the only reference. In the screenshot above, there were no `tflint` findings. \r\n\r\nThe proposed solutions seems plausible. However I tried this with multiple findings in tflint, and it creates 2 seperate `testcase` entries... I updated the gist with an example.","looking forward for this functionality to be enabled. When can we expect this to be live?","@mefarazahmad There is no commitment. I would recommend following the PR: https:\/\/github.com\/hashicorp\/terraform\/pull\/34291","Hi all,\r\n\r\nToday's alpha release of Terraform CLI includes an early experimental option to generate JUnit XML output.\r\n\r\nFor those who are interested in seeing this move forward, it would be much appreciated if you could [share feedback in the community forum topic about the experiment](https:\/\/discuss.hashicorp.com\/t\/request-for-feedback-on-terraform-test-junit-xml-output\/62307).\r\n\r\nI must forewarn that I'm currently focused primarily on a different project and so this JUnit XML experiment is a bit of a \"background task\" for me and so I might be slow to respond, but I do intend to collect up all the feedback and act on it later.\r\n\r\nThanks!\r\n\r\n---\r\n\r\n(We use the community forum for this sort of early feedback, rather than discussion here in GitHub, because the Discourse forum does at least have some support for tracking which comments are in reply to which other comments, whereas unstructured GitHub issue discussions tend to just become a confusing pile! We'll transition back to discussing in GitHub once it seems clearer what design we're going to move forward with and we become more concerned with the details of getting it implemented \"for real\".)\r\n\r\n","Hi again!\r\n\r\nAfter some discussion in the community forum topic I linked earlier, there were some conclusions I wanted to bring back in here to inform subsequent rounds of experiment:\r\n\r\n- The way we\u2019re describing the test scenarios (each separate `.tftest.hcl` file) doesn\u2019t seem to match what these tools are expecting: `testsuite` names didn't appear anywhere in the UI of either Azure DevOps or GitLab CI\/CD. It seems like we should try moving the test scenario name into the \u201cclassname\u201d attribute instead, to see if that makes it visible to these tools.\r\n\r\n- Test time durations are effectively mandatory in this format, because tools assume that if they are absent then the test took zero seconds to run, rather than (what we had hoped for) treating it as \u201ctest duration unknown\u201d.\r\n\r\n    This one is trickier because the test harness doesn\u2019t currently measure total duration of tech test step and scenario at all, so we\u2019ll need to add that to the test harness itself before we could include that information in the JUnit XML output.\r\n\r\nThanks to everyone for sharing their feedback and screenshots!\r\n\r\nI'm going to be away from this issue for at least a little while since my attention is needed elsewhere, but hopefully the above will be useful for either future-me or someone else working on a second round of experiment soon.\r\n"],"labels":["enhancement","new","terraform test"]},{"title":"console unable to inspect module resources","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.4\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/random v3.5.1\r\n\r\n# Also same results in 1.5.6.\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n###  \/main.tf\r\nmodule \"good\" {\r\n  source = \".\/modules\"\r\n}\r\n\r\nresource \"random_string\" \"root\" {\r\n  length  = 2\r\n  special = false\r\n  keepers = {\r\n    ts = timestamp()\r\n  }\r\n}\r\n\r\n### modules\/main.tf\r\nresource \"random_string\" \"this\" {\r\n  length  = 2\r\n  special = false\r\n  keepers = {\r\n    ts = timestamp()\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/staranto\/896d17262a2058273b246522facfb1f1\r\n\r\n### Expected Behavior\r\n\r\nI'd expect `terraform console` to allow me to inspect state resources created in modules.  If this isn't the case, perhaps the [console docs](https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/console) needs changed?  In particular, this statement --\r\n\r\n> You can use it to test interpolations before using them in configurations and to interact with any values currently saved in state.\r\n\r\n### Actual Behavior\r\n\r\nError.  See below.\r\n\r\n### Steps to Reproduce\r\n\r\n```\r\n# Clean.  No init or state.\r\n> ls --tree\r\ndrwxrwxr-x   - 16 Nov 06:12 .\r\ndrwxrwxr-x   - 16 Nov 06:10 \u251c\u2500\u2500 modules\r\n.rw-rw-r-- 108 16 Nov 06:10 \u2502  \u2514\u2500\u2500 main.tf\r\n.rw-rw-r-- 150 16 Nov 06:08 \u2514\u2500\u2500 main.tf\r\n\r\n> terraform init\r\n\r\n> terraform apply --auto-apply\r\n...\r\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\r\n\r\n> terraform state list\r\nrandom_string.root\r\nmodule.good.random_string.this\r\n\r\n# This works as expected\r\n> echo 'random_string.root' | terraform console\r\n{\r\n  \"id\" = \"D6\"\r\n  \"keepers\" = tomap({\r\n    \"ts\" = \"2023-11-16T11:16:27Z\"\r\n  })\r\n  <snip>\r\n}\r\n\r\n# This fails.  Does console not support inspecting\r\n# resources in modules?\r\necho 'module.good.random_string.this' | terraform console\r\n\u2577\r\n\u2502 Error: Unsupported attribute\r\n\u2502 \r\n\u2502   on <console-input> line 1:\r\n\u2502   (source code not available)\r\n\u2502 \r\n\u2502 This object does not have an attribute named \"random_string\".\r\n\r\n# Terraform recognizes that there is a module.good\r\n> echo 'module.good' | terraform console\r\n{}\r\n> echo 'module.goodXXX' | terraform console\r\n\u2577\r\n\u2502 Error: Reference to undeclared module\r\n\u2502 \r\n\u2502   on <console-input> line 1:\r\n\u2502   (source code not available)\r\n\u2502 \r\n\u2502 No module call named \"goodXX\" is declared in the root module. Did you mean \"good\"?\r\n```\r\n\r\n\r\n\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @staranto,\r\n\r\nThanks for filing the issue! That sentence should probably indicate that the \"state\" which you have access to is scoped to the root module. The expression syntax cannot traverse modules, and only has access to child module outputs. For any future additions to the language which might allow expressions with a different scope, you can follow #31861.\r\n\r\nThanks!","At some point in the past `terraform console` did allow specifying a different module to use as the evaluation scope via a command line option. I don't remember off the top of my head why we ended up needing to remove that, but I suspect it has something to do with the changes we made to shrink state snapshots by no longer saving information about named value objects like local values and output values except for those in the root module.\r\n\r\nI suspect we could probably find _some_ way to restore that capability at least in part, possibly with a few caveats about certain data appearing as unknown values when evaluating in a non-root module. I think a next step here would be to try a prototype to relearn what the constraints are here; we might even find that the constraints today are not as severe as they were at the time, since we've made changes to our expression evaluator in the meantime.\r\n\r\nConcretely, the sort of thing I mean is:\r\n\r\n```\r\nterraform console -scope=module.foo\r\n```\r\n\r\n...which would mean that any expressions evaluated in that console session would be evaluated in the scope of the given module instance, instead of the root module.\r\n\r\nI think the original name for this option was \"module\" rather than \"scope\", but this more general name will leave the door open for e.g. allowing resource instances as scopes later, making `each.key`, etc available as they would be inside that resource block.\r\n\r\nWith all of that said, I would also guess that we wouldn't be able to prioritise this right now since everyone is already busy with other things. Voting \ud83d\udc4d on the original issue comment could influence that prioritization, by illustrating that lots of people would appreciate this new (or rather, restored) capability.\r\n\r\n","Thank you @jbardin and @apparentlymart for your quick and thorough responses!  I've up voted the original issue.","As a work around, you can view the sub-module resources in the state file. (I'm using Terraform v1.7.4)\r\n\r\nIf you've never seen the state file before, resources aren't named wholly as you'd expect and are split as such:\r\n\r\n`{\r\n      \"module\": \"module.networking.module.vpc\",\r\n      \"mode\": \"managed\",\r\n      \"type\": \"aws_route_table_association\",\r\n      \"name\": \"private\",\r\n`\r\n\r\nSo you'll have to make use of a find utility to find your resource based on it's module or type or name if unique.\r\n\r\nI agree this should be a feature - If it's saved in the same state file, why not have this ability for sub-modules, if it's available for modules."],"labels":["bug","documentation","console","v1.6"]},{"title":"Allow referring to a `check` resource to use as a feature flag","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.3\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\n`data` sources return an error when checking for something that doesn't exist.\r\n\r\nWith the new `check` resource added, it'd be great if we could use it to wrap `data` source that could fail and use the result of the check as a boolean for ` count` attribute for example.\r\n\r\nIn the case of an AWS IAM service role, you may end up in a situation where the role might already exist (AWS create the roles \"for you\" when accessing\/enabling certain services through the UI) or not (never use the service).\r\n\r\nWriting Terraform config for this use case becomes painful afaik and solutions are limited:\r\n- nuke the IAM service role that may have been created so that it just works\u2122\ufe0f\r\n- do a bunch of `null_resource` to script a way around the problem\r\n- import by hand so that Terraform don't try to create the role again\r\n\r\n### Attempted Solutions\r\n\r\nUse a Terraform `null_resource` resource with `local-exec`:\r\n\r\n```bash\r\n#!\/bin\/bash\r\n\r\nROLE_NAME=\"AWSServiceRoleForAutoScaling\"\r\nOUTPUT_FILE=\"role_existence.txt\"\r\n\r\nif aws iam get-role --role-name $ROLE_NAME 2>\/dev\/null; then\r\n  echo \"true\" > $OUTPUT_FILE\r\nelse\r\n  echo \"false\" > $OUTPUT_FILE\r\nfi\r\n```\r\n\r\n```hcl\r\nresource \"null_resource\" \"check_role_existence\" {\r\n  provisioner \"local-exec\" {\r\n    command = \"path\/to\/script\/above.sh\"\r\n    interpreter = [\"bash\", \"-c\"]\r\n  }\r\n\r\n  triggers = {\r\n    always_run = \"${timestamp()}\"\r\n  }\r\n}\r\n```\r\n\r\n```hcl\r\ndata \"local_file\" \"role_existence\" {\r\n  depends_on = [null_resource.check_role_existence]\r\n  filename   = \"${path.module}\/role_existence.txt\"\r\n}\r\n```\r\n\r\n```hcl\r\nresource \"aws_iam_service_linked_role\" \"autoscaling\" {\r\n  count = data.local_file.role_existence.content == \"true\" ? 0 : 1\r\n\r\n  aws_service_name = \"autoscaling.amazonaws.com\"\r\n  description      = \"Default Service-Linked Role enables access to AWS Services and Resources used or managed by Auto Scaling.\"\r\n}\r\n```\r\n\r\n2. Import the IAM role resource so that Terraform ignores it?\r\n\r\n### Proposal\r\n\r\nIt'd be amazing if we could simply just refer to the `check` resource with some kind of \"return\" parameter to dictate the behavior:\r\n\r\n\r\n```hcl\r\nresource \"aws_iam_service_linked_role\" \"AWSServiceRoleForAutoScaling\" {\r\n  count = check.role_AWSServiceRoleForAutoScaling_already_exists ? 0 : 1\r\n  aws_service_name = \"autoscaling.amazonaws.com\"\r\n  description      = \"Default Service-Linked Role enables access to AWS Services and Resources used or managed by Auto Scaling.\"\r\n}\r\n\r\ncheck \"role_AWSServiceRoleForAutoScaling_already_exists\" {\r\n  data \"aws_iam_role\" \"AWSServiceRoleForAutoScaling\" {\r\n    name = \"AWSServiceRoleForAutoScaling\"\r\n  }\r\n\r\n  assert {\r\n    condition = data.aws_iam_role.AWSServiceRoleForAutoScaling.name == \"AWSServiceRoleForAutoScaling\"\r\n    error_message = \"${data.aws_iam_role.AWSServiceRoleForAutoScaling.name} already exist.\"\r\n    return = \"true\"\r\n  }\r\n}\r\n```\r\n\r\n### References\r\n\r\n- #34208","comments":["@4n3w saved the ~day~ night:\r\n\r\n```hcl\r\nlocals {\r\n  role_exists = can(data.aws_iam_role.AWSServiceRoleForAutoScaling.name)\r\n}\r\n\r\ndata \"aws_iam_role\" \"AWSServiceRoleForAutoScaling\" {\r\n  name = \"AWSServiceRoleForAutoScaling\"\r\n}\r\n\r\nresource \"aws_iam_service_linked_role\" \"AWSServiceRoleForAutoScaling\" {\r\n  count = local.role_exists ? 0 : 1\r\n  aws_service_name = \"autoscaling.amazonaws.com\"\r\n  description      = \"Default Service-Linked Role enables access to AWS Services and Resources used or managed by Auto Scaling.\"\r\n}\r\n```\r\n\r\nIt's not 100% the same behavior but probably can close as `can`\/`try` help with that AFAIK. If a maintainer can confirm that this is the way to go, I'll take it \ud83d\ude47 ","Hi @scalp42,\r\n\r\nThe primary purpose of `check` blocks -- what distinguishes them from preconditions and postconditions -- is that they never block forward progress and instead just act as an additional signal about whether the system is functioning as intended.\r\n\r\nThat means that in practice nothing can refer to a check block, because by definition checks must always be evaluated only after everything else has been dealt with. In a sense, a check block implicitly depends on everything else in the configuration and so if something else refers to it then that would produce a dependency cycle.\r\n\r\nGiven that, I don't think the specific solution you've proposed is viable, but I'd like to learn more about your use-case regardless of the specific proposed solution, because we might be able to find another way to get there using another language feature.\r\n\r\n---\r\n\r\nHowever, it seems like your goal might be \"create this if it doesn't already exist\", in which case that is _intentionally_ not allowed because Terraform is a desired state system and so you must tell it what it is supposed to be managing or else its behavior would be unpredictable.\r\n\r\nFor example, if we were to implement exactly what you proposed (the dependency ordering issue notwithstanding) then on the first run Terraform would find that the object doesn't exist yet and therefore see your declaration that it should exist, but then on the next run Terraform would find that the object already exists and therefore conclude that you want zero instances of the resource. Terraform would see that you currently have one instance of that resource, and so would propose to destroy it to converge with the new desired state. Then the third run would propose to create it again, and so on.\r\n\r\nInstead, your configuration should state whether the object is already expected to exist or not, and should therefore fail if that expectation isn't met.\r\n","@apparentlymart thanks for the detailed answer. \r\n\r\nUnfortunately, `can` won't work here because the `data` source triggers first. I just went with the `import` resource for now (which doesn't support interpolation unfortunately). \r\n\r\nWhile I understand the core philosophy of Terraform, I sometimes wish some of the choices could be left to the user. That being said, a lot of progress has been made (like `can`, `import` etc) and reflecting back on the earlier years of Terraform, a lot of flexibility was added so I understand. I just wish we could have a more of it at times.\r\n\r\nThanks again to you @apparentlymart and a special mention to @ewbankkit! Your contributions over the years are truly valued and appreciated \u2764\ufe0f \r\n\r\nWe should probably consider closing this issue."],"labels":["enhancement","new"]},{"title":"Unclear error message when mixing up \"data\" and \"resource\" blocks","body":"### Terraform Version\r\n\r\n```shell\r\nI already upgraded couple of time, so I am not sure which version I used when reported the issue first time.\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```diff\r\n-data \"aws_route53_zone\" \"base_domain\" {\r\n+resource \"aws_route53_zone\" \"base_domain\" {\r\n  name = \"staging.example.com\"\r\n\r\n  delegation_set_id = aws_route53_delegation_set.ns_set.id\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\nNA\r\n\r\n### Expected Behavior\r\n\r\n### Description\r\n\r\n![image](https:\/\/github.com\/hashicorp\/terraform-provider-aws\/assets\/139780495\/b35ff937-812d-4390-a96b-4cee43391847)\r\n\r\nit is not clear here that I should replace `data` with `resource`:\r\n```diff\r\n-data \"aws_route53_zone\" \"base_domain\" {\r\n+resource \"aws_route53_zone\" \"base_domain\" {\r\n  name = \"staging.example.com\"\r\n\r\n  delegation_set_id = aws_route53_delegation_set.ns_set.id\r\n}\r\n```\r\n\r\nI expect something like `you can not use delegation_set_id with \"data\"`\r\n\r\n### Actual Behavior\r\n\r\nIt is not clear what error means.\r\n\r\n### Steps to Reproduce\r\n\r\nUse specified type of resource.\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/33278","comments":["Hi @EugenKon! Thanks for this feedback.\r\n\r\nI think what you've suggested here would, in practice, be implemented like this:\r\n\r\nIf decoding the body of a `resource` block against the provider's schema for that managed resource type fails with any error, check whether there's a data resource type of the same name in the provider. If so, try decoding the block contents with the schema of _that_ resource type. If that succeeds, it's possible that the author intended to use the data resource instead of the managed resource.\r\n\r\nIn that case then, Terraform could potentially add an extra paragraph to the end of the error message:\r\n\r\n```\r\nDid you intend to use the \"aws_route53_zone\" data source? If so, change\r\nthis declaration from a \"resource\" block to a \"data\" block.\r\n```\r\n\r\nThe same logic could potentially work with the two resource modes swapped too: if it's a `data` block that fails, try looking for a managed resource type of the same name and try decoding with that.\r\n\r\nThere are lots of different specific error messages that could result from selecting the wrong resource type, which is why I framed the above as behavior implemented in response to _any_ error related to the body of the block. It's also possible that variations of this mistake could lead to multiple error messages, in which case I suppose we'd need to annotate all of them with the extra paragraph if the heuristic matches.\r\n\r\nWhile considering this, we might also consider another variation: if there's another resource type in the provider whose name has a small [edit distance](https:\/\/en.wikipedia.org\/wiki\/Edit_distance) from the one the author specified, we could try decoding against that one and if successful produce an error message suggesting the other resource type. As far as Terraform itself is concerned, the mistake of selecting a managed resource type instead of a data resource type is essentially the same as choosing the wrong resource type within a particular resource mode.\r\n"],"labels":["enhancement","new"]},{"title":"Terraform Test: Add ability to restrict parallelism when creating resources","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.3\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI have been converting some modules from my home-grown E2E testing pattern to using the new `terraform test` framework. It's been great but one issue I'm running into is that there is no ability to set the `-parallelism=<N>` flag when using the `apply` command. This is generally not a problem but there are certain AWS resources that run into problems when this is not set to 1. Specifically, certain AWS ECS resources as outlined https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/9777\r\n\r\n### Attempted Solutions\r\n\r\nTried to set the following envvar: `TF_CLI_ARGS_apply=\"-parallelism=1\"`. Confirmed that the `-parallelism` argument is not supported by the `test` command.\r\n\r\n### Proposal\r\n\r\nAdd a `-parallelism=<N>` flag to the `test` command that operates in the same way it does for the `apply` command.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new","terraform test"]},{"title":"Docs: map function","body":"### Terraform Version\n\n```shell\n$ terraform version\r\nTerraform v1.5.2\r\non linux_amd64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.6.3. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/meta-arguments\/for_each\n\n### What is the docs issue?\n\nThe page contains this example:\r\n\r\n```\r\nvariable \"vpcs\" {\r\n  type = map(object({\r\n    cidr_block = string\r\n  }))\r\n}\r\n```\r\n\r\nIt appears to be using a function called `map`.  Which leads to:  \r\n\r\n https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/map\r\n\r\n\"The map function is no longer available. Prior to Terraform v0.12...\"\r\n\r\n\n\n### Proposal\n\nShould the example be rewritten without `map`? (It is also referencing `object`,  does that need to be updated also?  not sure.) Thanks.\n\n### References\n\n_No response_","comments":["Hi @sdarwin! Thanks for starting this discussion.\r\n\r\nLearning how you interpreted this documentation is helpful, because it does highlight that these different pieces of documentation are not connected well, which has led you to a conclusion which isn't quite right. That's not your fault of course... it's the documentation's job to lead you to a correct conclusion, and it didn't here.\r\n\r\nI'm going to share some context here about how these different features are related, and I'm sharing it mostly in the hope it's helpful to someone who will revise the docs in future, but I hope this is also helpful to you by clarifying what these docs were _intending_ to communicate so you can make whatever decision you were aiming to make based on this documentation.\r\n\r\nSome missing backstory here is that in Terraform v0.11 and earlier there was no dedicated expression syntax for describing a map at all, and so there was a `map` function which took a sequence of alternating key and value pairs and returned a map: `map(\"foo\", foo_value, \"bar\", bar_value)`. Terraform v0.12 introduced the braces-based syntax for writing an object value, and then a later release introduced [the `tomap` function](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/tomap) which allows succinctly converting an object value into a map value.\r\n\r\nThe `map` function was then deprecated _in part_ because it overlapped confusingly with the language feature quoted in the issue text above: the `map(...)` syntax for declaring a map _type_, rather than a map _value_. In contexts where Terraform expects a type constraint, `map(x)` means that a map with elements of type x is required.\r\n\r\nA type constraint expression is a separate idea to a value expression, similar to how in some statically-typed programming languages there's a separate grammar for writing types than writing values. Despite both using the name `map`, the `map(...)` type constraint syntax is completely independent of the deprecated `map` function. The `map` function was deprecated and then later removed in part _because_ it's confusing to use the same word for two different meanings.\r\n\r\nSo with all of that said, the documentation quoted in this issue is correct, but it assumes that the reader is already familiar with [input variable declarations](https:\/\/developer.hashicorp.com\/terraform\/language\/values\/variables#declaring-an-input-variable) and their [type constraints](https:\/\/developer.hashicorp.com\/terraform\/language\/values\/variables#type-constraints), despite not linking to any information about those.\r\n\r\nGiven all that, it would perhaps be better to give a little more context about what the different parts of that `for_each` example represent and link to further information about the other language features used in that example. We could also consider at this point entirely removing that documentation page for the `map` function, since the function it refers to has been absent for some time now, and those using much older versions of Terraform can refer to [the archived documentation from those older versions](https:\/\/developer.hashicorp.com\/terraform\/language\/v1.1.x\/functions\/map).\r\n\r\nSimilar feedback could be made about [the `list` function](https:\/\/developer.hashicorp.com\/terraform\/language\/v1.1.x\/functions\/list), which has a similar history.\r\n","That makes sense! In the context of type, map() is a type constraint.  \r\n\r\nI had searched on google: terraform map, the top result was for the map function, which was deprecated, replaced by `tomap` so I thought the docs should also say `tomap`.     Yes, maybe it would help to remove that map() page entirely.    You are welcome to close this issue any time.  \r\n"],"labels":["documentation","new"]},{"title":"add parameter to alltrue() to fail when empty","body":"### Terraform Version\n\n```shell\nlatest\n```\n\n\n### Use Cases\n\n```hcl\r\nalltrue([ for _,v : v.value if v.othervalue > 0 ])\r\n```\r\n\r\nwith the introduction of terraform test, I have several cases coming up when I would like to do the above and ensure that both\r\n\r\n1. all values are true\r\n2. the test should fail if no results are returned which can happen if there was a problem with the condition inside of the for loop. if that were to happen the test would still pass which is obviously problematic.\r\n\r\nInterestingly `anytrue()` returns false on an empty list which I think is the correct behavior for both functions.\n\n### Attempted Solutions\n\nI think the most reasonable solution right now is a separate assertion duplicating the condition and checking for length of 0.\n\n### Proposal\n\nProbably can't make `anytrue` return false on an empty list due to backwards compatibility.\r\n\r\nAdding a new parameter would help - `alltrue([], failifempty)`\r\n\r\nAlternatively the introduction of an `allfalse` function that fails when empty could help and someone could invert the condition, but I think it's cognitively more complex.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Allow slice function to take step","body":"### Terraform Version\r\n\r\n```shell\r\nv1.5.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nDesired behavior:\r\n\r\n```terraform\r\n> slice(range(1,11),0,10,2)\r\ntolist([\r\n  1,\r\n  3,\r\n  5,\r\n  7,\r\n  9,\r\n])\r\n```\r\n\r\n`slice` should allow a third argument for step","comments":["Hi @paololazzari! Thanks for sharing this proposal.\r\n\r\nCan you share a real example of something you'd use this for? We're aiming to soon add support for extensibility of the set of functions through providers so that not everything needs to be built in, and so we're limiting built-in functions only to very broadly-useful functions moving forward, and so it would help to know if your goal is something that others are likely to also want to achieve, or if there are potentially many smaller use-cases for this building block.\r\n\r\nIn the meantime, you might be able to achieve a similar result by writing an expression that includes some use of [`chunklist`](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/chunklist), which allows breaking a list up into fixed-size chunks and so is a similar sort of operation to slicing with a step. (This is a good example of a very specialized function that's rarely used and so probably wouldn't make the cut as a built-in function if proposed today, but nonetheless it was early enough to now be protected by compatibility promises.)\r\n","Hi @paololazzari, If you want a solution in the meantime, the desired indexes can be generated via the `range` function already, allowing you to get the result via a `for` expression:\r\n\r\n```\r\nlocals {\r\n  collection = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n  stepped = [ for i in range(0, length(local.collection), 2) : local.collection[i] ]\r\n  # [1, 3, 5, 7, 9]\r\n}\r\n```\r\n"],"labels":["enhancement","new"]},{"title":"1-TO-1 dependency on multi-instance resources","body":"### Terraform Version\n\n```shell\nTerraform v1.6.3\r\non linux_amd64\n```\n\n\n### Use Cases\n\nUsing this simple test-case:\r\n\r\n```\r\nresource \"local_file\" \"a\" {\r\n  count = 3\r\n\r\n  filename = \"\/tmp\/a-${count.index}.txt\"\r\n  content = \"Risorsa A locale creata da Terraform ${count.index}\"\r\n}\r\n\r\nresource \"local_file\" \"b\" {\r\n  count = length(local_file.a)\r\n\r\n  depends_on = [local_file.a]\r\n\r\n  filename = \"\/tmp\/b-${count.index}.txt\"\r\n  content = \"Risorsa B locale creata da Terraform ${count.index}\"\r\n}\r\n```\r\n\r\nThis is the output of `terraform apply`:\r\n\r\n```\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\r\n  + create\r\n\r\nTerraform will perform the following actions:\r\n\r\n  # local_file.a[0] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 0\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-0.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.a[1] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 1\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-1.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.a[2] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 2\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-2.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[0] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 0\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-0.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[1] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 1\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-1.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[2] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 2\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-2.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\nPlan: 6 to add, 0 to change, 0 to destroy.\r\n```\r\n\r\nAs you can see, terraform will create first all  \"local_file\" \"a\" resource and after all  \"local_file\" \"b\" resource.\r\n\r\nIt could be nice to have a sort of 1-TO-1 dependency on multi-instance resources, so terraform will create alternately each instance of   \"local_file\" \"a\" and   \"local_file\" \"b\":\r\n\r\n\r\n```\r\nTerraform will perform the following actions:\r\n\r\n  # local_file.a[0] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 0\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-0.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[0] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 0\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-0.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.a[1] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 1\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-1.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[1] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 1\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-1.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.a[2] will be created\r\n  + resource \"local_file\" \"a\" {\r\n      + content              = \"Risorsa A locale creata da Terraform 2\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/a-2.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.b[2] will be created\r\n  + resource \"local_file\" \"b\" {\r\n      + content              = \"Risorsa B locale creata da Terraform 2\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"\/tmp\/b-2.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\nPlan: 6 to add, 0 to change, 0 to destroy.\r\n```\r\n\r\nIt could be nice if this should be done with:\r\n\r\n`depends_on = [local_file.a[count.index]]`\r\n\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\nPlease, implements dependency based on count.index\r\n\r\n`depends_on = [local_file.a[count.index]]`\r\n\r\nand modify apply order in this way:\r\n```\r\n\r\nlocal_file.a[0]: Creating...\r\nlocal_file.a[0]: Creation complete after 0s [id=aa3f8214673fe48dbffa00c0bf482aef7258d0cb]\r\nlocal_file.b[0]: Creating...\r\nlocal_file.b[0]: Creation complete after 0s [id=c0a5b2ca321691f4205075661e9ac56087f003e4]\r\nlocal_file.a[1]: Creating...\r\nlocal_file.a[1]: Creation complete after 0s [id=fa8b2f2df2d31ce98db8e4dbb92d18e35679d73e]\r\nlocal_file.b[1]: Creating...\r\nlocal_file.b[1]: Creation complete after 0s [id=1609ce6a76506ccfb235f9f484b409d07d3e4795]\r\nlocal_file.a[2]: Creating...\r\nlocal_file.a[2]: Creation complete after 0s [id=865afe722281f06b5d8290312a8def9feb0415f1]\r\nlocal_file.b[2]: Creating...\r\nlocal_file.b[2]: Creation complete after 0s [id=58e049f0c9d26ecc474a03c683de8ceb1c971fbe]\r\n```\r\n\r\n\n\n### References\n\n_No response_","comments":["The new feature should also manage the destroy operation in this way:\r\n\r\n`terraform destroy --target=local_file.a[2]`\r\n\r\n```\r\nlocal_file.b[2]: Destroying... [id=58e049f0c9d26ecc474a03c683de8ceb1c971fbe]\r\nlocal_file.b[2]: Destruction complete after 0s\r\nlocal_file.a[2]: Destroying... [id=865afe722281f06b5d8290312a8def9feb0415f1]\r\nlocal_file.a[2]: Destruction complete after 0s\r\n\r\nDestroy complete! Resources: 2 destroyed.\r\n```\r\n\r\nThis should destroy only local_file.a[2] and (due the depends_on) local_file.b[2], leaving all other instances of local_file.a and local_file.b untouched.","Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hi @marcofortina,\r\n\r\nThanks for filing the issue. Though a slightly different request, the underlying restrictions here are the same as in #27188, #28330, and #30841. Terraform was designed to track dependencies between whole resources, and because the graph must be built from configuration before there are any individual instances, only connections between whole resource blocks can be made at that point. Reconstructing how these dependencies are handled would need to be part of a very large redesign of the core of Terraform."],"labels":["enhancement","core"]},{"title":"Support `for_each` and \/ or `dynamic` blocks for `check` ","body":"### Terraform Version\r\n\r\n```shell\r\nlatest\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nObjects are often used with for_each logic to create multiple versions of a resource.  In order to validate the variables a similar construct is required for the check block.  Currently check does not support either for_each, count or dynamic so more complex interpolations are required to create a single check for multiple counts of a resource.\r\n\r\n### Attempted Solutions\r\n\r\n```\r\nlocals {\r\n  my_object = {\r\n    one = {\r\n      big   = 10\r\n      small = 7\r\n    },\r\n    two = {\r\n      big   = 7\r\n      small = 10\r\n    }\r\n  }\r\n  big   = 10\r\n  small = 7\r\n}\r\n\r\ncheck \"greaterthan_simple\" {\r\n  assert {\r\n    condition     = local.big > local.small\r\n    error_message = \"big is not greater than small\"\r\n  }\r\n}\r\n\r\ncheck \"greaterthan_complex\" {\r\n  assert {\r\n    condition     = alltrue([for key, val in local.my_object : val.big > val.small])\r\n    error_message = format(\"big is not greater than small for these keys: %s\", join(\", \", [for key, val in local.my_object : key if val.big <= val.small]))\r\n  }\r\n}\r\n\r\n\/\/ Would be nice if you could do this in a for_each loop\r\ncheck \"greaterthan_foreach\" {\r\n  for_each = local.my_object\r\n  assert {\r\n    condition     = each.value.big > each.value.small\r\n    error_message = format(\"big is not greater than small in key: %s\", each.key)\r\n  }\r\n}\r\n\r\n\/\/ Alternatively supporting dynamic would also unlock similar functionality\r\ncheck \"greaterthan_dynamic\" {\r\n  dynamic \"assert\" {\r\n    for_each      = local.my_object\r\n    content {\r\n      condition       = assert.value.big > assert.value.small\r\n      error_message = format(\"big is not greater than small in key: %s\", assert.key)\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n### Proposal\r\n\r\nAdd support for for_each and \/ or dynamic blocks to simplify the logic required to use a check block.  The attempted solution gives a simple tf file and example implementations.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Thanks for sharing this use-case, @davepattie.\r\n\r\nWe were also chatting in another place about this and so I just wanted to note here an example of a different design I shared that I've been considering, since I too have been frustrated by writing conditions for collection-typed values where I want to treat each element as a separate entity for validation.\r\n\r\n```hcl\r\n  assert {\r\n    error_messages = [\r\n      for k, v in local.my_objects :\r\n      \"Element ${format(\"%q\", k)}: big attribute must be greater than small attribute.\"\r\n      if v.value.big <= v.value.small\r\n    ]\r\n  }\r\n```\r\n\r\nI've shown this as an `assert` block to match with the examples you shared, but whatever we do here I would expect to support it in the same way for variable validation rules, preconditions, and postconditions too, since they are all intended to support similar patterns.\r\n\r\nThe way I imagine this working is that:\r\n- `error_messages` is mutually-exclusive with the `condition` and `error_message` pair that these blocks currently require. Both would be valid, but each condition block would have either a list of error messages _or_ a single condition and associated error message.\r\n- The `error_messages` expression must return something that can convert to `list(string)`.\r\n- Each element of the list becomes a separate error message, making it possible to generate a separate error message for each element of a collection rather than just a single error message which must somehow incorporate a list of all of the incorrect element keys.\r\n- If the list has zero elements, there are no errors, and therefore that's the same as having a `condition` argument that is set to `true`: the check passes.\r\n\r\nMost of the syntax in my example above is not new: it's just a [`for` expression](https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/for) that produces a sequence of strings. The only truly new part is the `error_messages` argument itself, and it would accept _any_ valid Terraform expression that produces something that convert to a list of strings, meaning this feature could scale to more complicated situations when needed.\r\n","Is there a way to use a check resource as a boolean for a `count`?\r\n\r\nSomething like this:\r\n\r\n```hcl\r\nresource \"aws_iam_service_linked_role\" \"AWSServiceRoleForAutoScaling\" {\r\n  count = check.role_AWSServiceRoleForAutoScaling_already_exists ? 0 : 1\r\n  aws_service_name = \"autoscaling.amazonaws.com\"\r\n  description      = \"Default Service-Linked Role enables access to AWS Services and Resources used or managed by Auto Scaling.\"\r\n}\r\n\r\ncheck \"role_AWSServiceRoleForAutoScaling_already_exists\" {\r\n  data \"aws_iam_role\" \"AWSServiceRoleForAutoScaling\" {\r\n    name = \"AWSServiceRoleForAutoScaling\"\r\n  }\r\n\r\n  assert {\r\n    condition = data.aws_iam_role.AWSServiceRoleForAutoScaling.name == \"AWSServiceRoleForAutoScaling\"\r\n    error_message = \"${data.aws_iam_role.AWSServiceRoleForAutoScaling.name} doesn't exist yet.\"\r\n  }\r\n}\r\n```\r\n\r\nThen if we could use `for_each`, we could be checking for multiple \"conditions\" which would in turn make it even easier."],"labels":["enhancement","new"]},{"title":"No provider config schema available message for builtin provider should not be labelled ERROR in output","body":"### Terraform Version\r\n\r\n```shell\r\n1.6.3\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\ndata \"terraform_remote_state\" \"this\" {\r\n  backend = \"local\"\r\n    config = {\r\n    path = \"\/path\/to\/pre-existing\/terraform.tfstate\"\r\n  }\r\n}\r\n\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\n2023-11-08T14:34:04.205+0200 [DEBUG] ProviderTransformer: \"data.terraform_remote_state.this\" (*terraform.NodeValidatableResource) needs provider[\"terraform.io\/builtin\/terraform\"]\r\n**2023-11-08T14:34:04.205+0200 [ERROR] AttachSchemaTransformer: No provider config schema available for provider[\"terraform.io\/builtin\/terraform\"]**\r\n2023-11-08T14:34:04.210+0200 [DEBUG] ReferenceTransformer: \"data.terraform_remote_state.this\" references: []\r\n\r\n\r\n### Expected Behavior\r\n\r\n**[ERROR] AttachSchemaTransformer: No provider config schema available for provider[\"terraform.io\/builtin\/terraform\"]**\r\n\r\nThe above message should:\r\n\r\n1. Be clearer. There is a provider - it is just built-in and not explicitly defined.\r\n2. The **[ERROR]** label should be **[DEBUG]**. If it is left as **[ERROR]**, it makes it much more difficult for a log filter to actually differentiate it from real errors.\r\n\r\n### Actual Behavior\r\n\r\n1. There is a misleading error message.\r\n2. The message is erroneously labelled as **[ERROR]**.\r\n\r\n### Steps to Reproduce\r\n1. `export TF_LOG=DEBUG`\r\n2. `terraform init`\r\n3. `terraform plan`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @thewileyriley,\r\n\r\nThanks for filing the issue! The log message is correct in most cases, except it does produce an erroneous message for the `terraform` provider. Since the log output is targeted at the developers (the \"error\" would usually be in the code), it hasn't yet been handled as a special case. It's probably more correct to just make sure the internal provider has an empty schema even though it's not used.","@jbardin \r\nThanks for your feedback. What is an issue for me is that I am using an ADO Terraform task https:\/\/github.com\/microsoft\/azure-pipelines-terraform\/tree\/main\/Tasks\/TerraformTask\/TerraformTaskV4 which filters the debug log and prints anything with the **[ERROR]** label in the Terraform log to stdout in red. It is not something the bothers me particularly, however - it does raise questions from other people that use the pipeline and it also makes the output look less clean.\r\n\r\nIs it possible to change the label so we can keep the log output clean?","The label is probably not going to change, since it would be an error in most cases, but we can probably avoid this situation with the terraform provider in a future release. I'm not sure what the reasoning for the log output in that task is, any errors (or even warnings) which are actionable by a user will be output in the normal diagnostics. The logs are targeted at developers, however if a user is having a problem the `warn` level is often most useful as it will display certain provider problems which could result in confusing behaviors. ","Same issue here but in my case using a \"terraform_data\" resource.\r\n\r\nCode:\r\n\r\n`resource \"terraform_data\" \"product_version\" {\r\n  input = var.product_version\r\n}`\r\n\r\nError:\r\n\r\n`2024-02-14T12:18:42.213Z [ERROR] AttachSchemaTransformer: No provider config schema available for provider[\"terraform.io\/builtin\/terraform\"]`\r\n\r\n"],"labels":["bug","core","v1.6"]},{"title":"A request of ucloud object storage backend.","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.3\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWe are the maintenance team of [terraform ucloud provider project]( https:\/\/github.com\/ucloud\/terraform-provider-ucloud). Our customers want to use our object storage product as a backend storage.\r\n\r\n### Attempted Solutions\r\n\r\nWe attempted to use S3 backend through standard S3 protocol, but the state locking and consistency checking are supported via [Dynamo DB](https:\/\/aws.amazon.com\/dynamodb\/).\r\n\r\n### Proposal\r\n\r\nWe want to implement a new backend type with our object storage.\r\n\r\n### References\r\n\r\nOur object storage product: [US3](https:\/\/docs.ucloud.cn\/ufile\/README)","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. \r\n\r\nPlease read [Contributing.md](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md#state-storage-backends) regarding new state storage backends. In short, it is unlikely that we will add new backends in the medium-term future. If this is a first-party request, it could help to have a partnership discussion with HashiCorp. You can find out more about partnership at https:\/\/www.hashicorp.com\/partners. I will also pass this along to our product manager. \r\n\r\nThanks again for this request! "],"labels":["enhancement","new-backend","new"]},{"title":"Error: Failed to get existing workspaces: containers.Client#ListBlobs: Failure sending request: StatusCode=0 -- Original Error: Get \"https:\/\/storage.blob.core.windows.net\/container?comp=list&prefix=terraform.tfstateenv%3A&restype=container\": EOF","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.1 on windows_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```hcl\r\nterraform {\r\n  required_providers {\r\n    azurerm = {\r\n      source = \"hashicorp\/azurerm\"\r\n      version = \"3.79.0\"\r\n    }\r\n  }\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"rg_name\"\r\n    storage_account_name = \"storage\"\r\n    container_name            = \"container\"\r\n    key                                = \"terraform.tfstate\"\r\n  }\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\n```\r\n2023-11-07T14:52:25.778Z [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2023-11-07T14:52:25.778Z [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2023-11-07T14:52:25.778Z [TRACE] statemgr.Filesystem: read nil snapshot\r\n2023-11-07T14:52:25.778Z [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2023-11-07T14:52:25.784Z [DEBUG] New state was assigned lineage \"xxxxxxx-xxxxx-xxxx-xxxx-xxxxxxxx\"\r\n2023-11-07T14:52:25.784Z [DEBUG] Azure Backend Request: \r\nGET \/container?comp=list&prefix=terraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: storage.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.6.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Tue, 07 Nov 2023 14:52:25 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n2023-11-07T14:54:36.671Z [DEBUG] Request to https:\/\/storage.blob.core.windows.net\/container?comp=list&prefix=terraform.tfstateenv%3A&restype=container completed with no response\r\n2023-11-07T14:55:06.674Z [DEBUG] Azure Backend Request: \r\nGET \/container?comp=list&prefix=terraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: storage.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.6.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Tue, 07 Nov 2023 14:52:25 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n```\r\n\r\n### Expected Behavior\r\n\r\nHi everyone,\r\n\r\nI kindly ask for your support with the problem I am currently experiencing. Basically I need to configure the backend in my terraform code but currently from the client where I'm trying it's not possible and I get the error I indicated.\r\nThe client is an agent pool used for a pipeline on Azure Devops, so I take advantage of the pre-built modules that are already present.\r\n\r\n\r\n### Actual Behavior\r\n\r\nI have this issue during the terraform init process:\r\n\r\n```\r\nterraform.exe init -backend-config=storage_account_name=storage -backend-config=container_name=container -backend-config=key=terraform.tfstate -backend-config=resource_group_name=rg -backend-config=subscription_id=xxxxxx-xxxx-xxxxx-xxxxxx-xxxxxxx -backend-config=tenant_id=xxxxx-xxxxx-xxxx-xxxxx-xxxxxx -backend-config=client_id=*** -backend-config=client_secret=***\r\n\r\nError: Failed to get existing workspaces: containers.Client#ListBlobs: Failure sending request: StatusCode=0 -- Original Error: Get \"https:\/\/storage.blob.core.windows.net\/container?comp=list&prefix=terraform.tfstateenv%3A&restype=container\": EOF\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init -backend-config`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["I just came across this error. Here are some things I noticed\/tried while trying to fix the issue. Hopefully this helps someone.\r\n\r\nMy deployment is also running in Devops. I have found that the same debug message that rod2198 had also appears for me on terraform init, plan, workspace, and apply (probably any command that needs to access the remote storage). From watching the logs I can see that it hangs for a very long time on these requests. Sometimes it retries and succeeds with a 200 response but even if it gets through each command, the time it takes is upward of 30 mins. If it gets \"completed with no response\" enough times then the command fails with the same error that rod2198 got. I have found that this issue gets progressively worse the more times you attempt to run the pipeline\/commands. My pipeline is running on a self hosted Linux agent.\r\n\r\nI found that this issue does not happen if the commands are run locally or if not using remote storage. I have checked that the Azure CLI token has not expired, I have checked that the DNS for the storage account resolves. I have checked the firewall and storage account for failures or denies (there are none). The service principle we use to authenticate has the correct permissions to access the storage account.\r\n\r\nThis issue varies in severity. sometimes it only returns \"completed with no response\" once and then the rest of the pipeline succeeds without issue and other times every command hangs and timesout until eventually it hits the max retries and errors.","I have the same issue in terraform 1.7.1, same checks that @Perks-of-Being-a-Cauliflower mentioned are done.\r\n```\r\nRun terraform init -backend-config=\"key=portal\/prod\/portal-infra\/terraform.tfstate\" -reconfigure\r\n2024-01-25T10:23:45.425Z [INFO]  Terraform version: 1.7.1\r\n2024-01-25T10:23:45.426Z [DEBUG] using github.com\/hashicorp\/go-tfe v1.41.0\r\n2024-01-25T10:23:45.426Z [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2024-01-25T10:23:45.426Z [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2024-01-25T10:23:45.426Z [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2024-01-25T10:23:45.426Z [INFO]  Go runtime version: go1.21.5\r\n2024-01-25T10:23:45.426Z [INFO]  CLI args: []string{\"terraform\", \"init\", \"-backend-config=key=portal\/prod\/portal-infra\/terraform.tfstate\", \"-reconfigure\"}\r\n2024-01-25T10:23:45.426Z [DEBUG] Attempting to open CLI config file: \/home\/docker\/.terraformrc\r\n2024-01-25T10:23:45.426Z [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2024-01-25T10:23:45.426Z [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-01-25T10:23:45.426Z [DEBUG] ignoring non-existing provider search directory \/home\/docker\/.terraform.d\/plugins\r\n2024-01-25T10:23:45.426Z [DEBUG] ignoring non-existing provider search directory \/home\/docker\/.local\/share\/terraform\/plugins\r\n2024-01-25T10:23:45.426Z [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2024-01-25T10:23:45.426Z [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2024-01-25T10:23:45.432Z [INFO]  CLI command args: []string{\"init\", \"-backend-config=key=portal\/prod\/portal-infra\/terraform.tfstate\", \"-reconfigure\"}\r\n\r\nInitializing the backend...\r\n2024-01-25T10:23:45.468Z [DEBUG] New state was assigned lineage \"3c1ca212-5cf4-17be-3ccd-e2aa328fc412\"\r\n2024-01-25T10:23:45.469Z [DEBUG] checking for provisioner in \".\"\r\n2024-01-25T10:23:45.470Z [DEBUG] checking for provisioner in \"\/home\/docker\/actions-runner\/_work\/_temp\/c27432b6-08db-46cd-923a-9c0bad6b620a\"\r\n2024-01-25T10:23:45.471Z [INFO]  Testing if Service Principal \/ Client Certificate is applicable for Authentication..\r\n2024-01-25T10:23:45.471Z [INFO]  Testing if Multi Tenant Service Principal \/ Client Secret is applicable for Authentication..\r\n2024-01-25T10:23:45.471Z [INFO]  Testing if Service Principal \/ Client Secret is applicable for Authentication..\r\n2024-01-25T10:23:45.471Z [INFO]  Testing if OIDC is applicable for Authentication..\r\n2024-01-25T10:23:45.471Z [INFO]  Using OIDC for Authentication\r\n2024-01-25T10:23:45.471Z [INFO]  Getting OAuth config for endpoint https:\/\/login.microsoftonline.com\/ with  tenant ***\r\n2024-01-25T10:23:45.473Z [DEBUG] Obtaining an MSAL \/ Microsoft Graph token for Resource Manager..\r\n2024-01-25T10:23:45.475Z [DEBUG] New state was assigned lineage \"17814ded-cbf7-e921-8333-aa8b0e533eb3\"\r\n2024-01-25T10:23:45.475Z [DEBUG] Building the Container Client from an Access Token (using user credentials)\r\n2024-01-25T10:23:46.239Z [DEBUG] Azure Backend Request: \r\nPOST \/subscriptions\/***\/resourceGroups\/rg-iasp-euw-terraformBackend\/providers\/Microsoft.Storage\/storageAccounts\/<storageaccountname>\/listKeys?api-version=2021-01-01 HTTP\/1.1\r\nHost: management.azure.com\r\nUser-Agent: HashiCorp Terraform\/1.7.1 (+https:\/\/www.terraform.io)\r\nContent-Length: 0\r\nX-Ms-Authorization-Auxiliary: \r\nAccept-Encoding: gzip\r\n2024-01-25T10:23:46.444Z [DEBUG] Azure Backend Response for https:\/\/management.azure.com\/subscriptions\/***\/resourceGroups\/rg-iasp-euw-terraformBackend\/providers\/Microsoft.Storage\/storageAccounts\/<storageaccountname>\/listKeys?api-version=2021-01-01: \r\nHTTP\/2.0 200 OK\r\nContent-Length: 288\r\nCache-Control: no-cache\r\nContent-Type: application\/json\r\nDate: Thu, 25 Jan 2024 10:23:45 GMT\r\nExpires: -1\r\nPragma: no-cache\r\nServer: Microsoft-Azure-Storage-Resource-Provider\/1.0,Microsoft-HTTPAPI\/2.0 Microsoft-HTTPAPI\/2.0\r\nStrict-Transport-Security: max-age=31536000; includeSubDomains\r\nX-Content-Type-Options: nosniff\r\nX-Ms-Correlation-Request-Id: b4547ff4-c8aa-46b1-826c-5ab999fdaaf7\r\nX-Ms-Ratelimit-Remaining-Subscription-Resource-Requests: 11999\r\nX-Ms-Request-Id: 0a1f3c40-66f5-40f5-8172-0e639393bacf\r\nX-Ms-Routing-Request-Id: WESTEUROPE:20240125T102346Z:b4547ff4-c8aa-46b1-826c-5ab999fdaaf7\r\n\r\n{\"keys\":[{\"keyName\":\"key1\",\"value\":\"****************\",\"permissions\":\"FULL\"},{\"keyName\":\"key2\",\"value\":\"****************\",\"permissions\":\"FULL\"}]}\r\n2024-01-25T10:23:46.445Z [DEBUG] Azure Backend Request: \r\nGET \/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: <storageaccountname>.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.7.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Thu, 25 Jan 2024 10:23:46 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n2024-01-25T10:39:37.968Z [DEBUG] Request to https:\/\/<storageaccountname>.blob.core.windows.net\/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container completed with no response\r\n2024-01-25T10:40:07.971Z [DEBUG] Azure Backend Request: \r\nGET \/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: <storageaccountname>.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.7.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Thu, 25 Jan 2024 10:23:46 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n2024-01-25T10:56:01.010Z [DEBUG] Request to https:\/\/<storageaccountname>.blob.core.windows.net\/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container completed with no response\r\n2024-01-25T10:57:01.016Z [DEBUG] Azure Backend Request: \r\nGET \/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: <storageaccountname>.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.7.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Thu, 25 Jan 2024 10:23:46 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n2024-01-25T11:12:52.720Z [DEBUG] Request to https:\/\/<storageaccountname>.blob.core.windows.net\/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container completed with no response\r\n2024-01-25T11:14:52.721Z [DEBUG] Azure Backend Request: \r\nGET \/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container HTTP\/1.1\r\nHost: <storageaccountname>.blob.core.windows.net\r\nUser-Agent: HashiCorp Terraform\/1.7.1 (+https:\/\/www.terraform.io)\r\nContent-Type: application\/xml; charset=utf-8\r\nX-Ms-Date: Thu, 25 Jan 2024 10:23:46 GMT\r\nX-Ms-Version: 2018-11-09\r\nAccept-Encoding: gzip\r\n2024-01-25T11:30:45.872Z [DEBUG] Request to https:\/\/<storageaccountname>.blob.core.windows.net\/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container completed with no response\r\nInitializing modules...\r\n2024-01-25T11:34:45.879Z [DEBUG] Module installer: begin RG\r\n2024-01-25T11:34:45.880Z [DEBUG] Module installer: RG installed at modules\/rg\r\n2024-01-25T11:34:45.880Z [DEBUG] Module installer: begin subnet_app\r\n- RG in modules\/rg\r\n2024-01-25T11:34:45.883Z [DEBUG] Module installer: subnet_app installed at modules\/subnet\r\n- subnet_app in modules\/subnet\r\n\u2577\r\n\u2502 Error: Failed to get existing workspaces: containers.Client#ListBlobs: Failure sending request: StatusCode=0 -- Original Error: Get \"https:\/\/<storageaccountname>.blob.core.windows.net\/tfstate?comp=list&prefix=portal%2Fprod%2Fportal-infra%2Fterraform.tfstateenv%3A&restype=container\": read tcp 10.233.134.164:47904->20.157.146.104:443: read: connection timed out\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n\r\nError: Process completed with exit code 1.\r\n```","Guys i solved the issue!\r\n\r\nWe solved the issue, basically it was necessary to configure the peering between Devops agent VM Vnet and the storage private endpoint Vnet and modify the NSG associated with the subnet to allow 443 traffic on the container.\r\n\r\nI hope that this can help you."],"labels":["bug","backend\/azure","new","v1.6"]},{"title":"Invalid dynamic iterator breaks variable scope","body":"### Terraform Version\n\n```shell\nTerraform v1.6.3\n```\n\n\n### Terraform Configuration Files\n\n```\r\nresource \"digitalocean_firewall\" \"challenge_firewall\" {\r\n  name = \"${var.challenge_name}-firewall\"\r\n  droplet_ids = [digitalocean_droplet.challenge_droplet.id]\r\n\r\n  dynamic \"inbound_rule\" {\r\n    for_each = var.ingress_rules\r\n    iterator = \"rules\"\r\n    content {\r\n      protocol = rules.value[\"protocol\"]\r\n      port_range = rules.value[\"port_range\"]\r\n      source_addresses = rules.value[\"source_addresses\"]\r\n    }\r\n  }\r\n}\r\n```\r\nThe variable ingress_rules looks like this\r\n\r\n```\r\nvariable \"ingress_rules\" {\r\n    description = \"The ingress rule for the challenges\"\r\n    type = map(object({\r\n        protocol = string\r\n        port_range = string\r\n        source_addresses = list(string)\r\n    }))\r\n    default = {\r\n      \"HTTP\" = {\r\n        protocol = \"tcp\",\r\n        port_range = \"80\",\r\n        source_addresses = [\"0.0.0.0\/0\", \"::\/0\"]\r\n      },\r\n      \"SSH\" = {\r\n        protocol = \"tcp\",\r\n        port_range = \"22\",\r\n        source_addresses = [\"0.0.0.0\/0\", \"::\/0\"]\r\n      },\r\n      \"ICMP\" = {\r\n        protocol = \"icmp\",\r\n        port_range = \"1-65535\",\r\n        source_addresses = [\"0.0.0.0\/0\", \"::\/0\"]\r\n      }\r\n    }\r\n}\r\n```\n\n### Debug Output\n\nTerraform v1.6.3\r\non linux_amd64\r\nInitializing plugins and modules...\r\n\u2577\r\n\u2502 Error: Unknown variable\r\n\u2502 \r\n\u2502   on resources.tf line 37, in resource \"digitalocean_firewall\" \"challenge_firewall\":\r\n\u2502   37:     for_each = var.ingress_rules ? [1] : []\r\n\u2502 \r\n\u2502 There is no variable named \"var\".\r\n\u2575\r\nOperation failed: failed running terraform plan (exit 1)\n\n### Expected Behavior\n\nThe dynamic block should be able to run smoothly without any problems\n\n### Actual Behavior\n\nThe terraform crashed giving the above error which is really ambigous\n\n### Steps to Reproduce\n\n1. `terraform init`\r\n2. `terraform plan`\n\n### Additional Context\n\nSo I am trying to create multiple firewall rules for a machine, and to do that I am using a dynamic block so that the rules are easily customizable and not fixed. However, when I run the snippet, I get the error `There is no variable named \"var\".`. Obviously, I have searched for any clues to solve the error, but nothing I tried worked so far. The error is really ambigous and not helpful. Please help\n\n### References\n\n_No response_","comments":["Hi @karimhreda,\r\n\r\nThanks for filing the issue! It seems a diagnostic message is being lost here and replaced with a broken reference to `var`. Normally you should see something like `Error: Invalid expression` for your iterator value, which is declaring a new name to reference locally, and should not be enclosed in quotes. The following configuration works as expected:\r\n\r\n```\r\n  dynamic \"inbound_rule\" {\r\n    for_each = var.ingress_rules\r\n    iterator = rules\r\n    content {\r\n      protocol = rules.value[\"protocol\"]\r\n      port_range = rules.value[\"port_range\"]\r\n      source_addresses = rules.value[\"source_addresses\"]\r\n    }\r\n  }\r\n  ```\r\n"],"labels":["bug","config","confirmed","v1.6"]},{"title":"-refresh=false doesn't work with terraform \"cloud\"","body":"### Terraform Version\n\n```shell\nTerraform v1.6.2\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.22.0\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.6.3. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nterraform {\r\n  cloud {\r\n    organization = \"my-organization\"\r\n    hostname = \"tfe.example.com\" # terraform enterprise endpoint\r\n    workspaces {\r\n      name = \"my-workspace\"\r\n    }\r\n  }\r\n}\r\n```\r\n\n\n### Debug Output\n\nN\/A\n\n### Expected Behavior\n\nSupplying the `-refresh=false` should disable refreshing even if using terraform cloud or terraform enterprise.\n\n### Actual Behavior\n\nAs far as I can tell supplying `-refresh=false` on the command line  does nothing if using terraform enterprise or terraform cloud with  a `terraform.cloud` configuration. At least if VCS is set up for the workspace.\r\n\r\nI still see output saying that the state for all resources is being refreshed.\n\n### Steps to Reproduce\n\n1. Set up terraform with terraform cloud or enterprise\r\n2. Run a plan or apply with the `-refresh=false` flag\r\n3. Watch as that flag has no effect\n\n### Additional Context\n\nThe reason that I want to use `-refresh=false` is that when I am developing new configuration I sometimes want to run speculative plans to see the actual changes my code will produce. Needing to wait for the state to be refreshed for  all resources slows that down, which makes iteration slower. I'd like to speed up the process so that I can iterate more quickly. when actually making a pull request, doing a full refresh is fine.\r\n\r\nAs a sidenote, it is also annoying that I can't use the TF_LOG environment variable to get more verbose logs, or set an environment variable for a single speculative run. If I need to get debug logs, I have to to the settings of the workspace, set the TF_LOG environment variable, do the run, then go back and set it back, which is rather annoying.  \n\n### References\n\n_No response_","comments":["Thanks for reporting this, @tmccombs.\r\n\r\nIt does seem that there's code in the Cloud integration to pass that option on to the Terraform Cloud API:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/10f4567fcc284cc649d939705a8b2a70c339b951\/internal\/cloud\/backend_plan.go#L219-L225\r\n\r\nThe `Refresh` argument here is the documented way to disable refreshing when creating a run with the Terraform Cloud API, so at first glance it seems like the code in this repository is upholding its part of the contract.\r\n\r\nI can also see in the Terraform Cloud Agent codebase (whose source code isn't public) some logic that adds the `-refresh=false` option to the command it's running when Terraform Cloud commands the agent job to \"skip refresh\".\r\n\r\nTherefore it isn't clear to me exactly what's missing here, but there's a lot of other Terraform Cloud control plane code between the two \"ends\" I am familiar with and so I think we'll need to await someone with more familiarity with Terraform Cloud to understand exactly what's going on here.\r\n\r\n----\r\n\r\nIf you _have_ been able to capture some `TF_LOG=trace` output using the technique you described, it might be helpful to see the \"CLI args:\" log line from the early part of that output, which should look something like this:\r\n\r\n```\r\n2023-11-06T17:54:20.256-0500 [INFO]  CLI args: []string{\"terraform\", \"plan\"}\r\n```\r\n\r\nIn your case I'd hope to see at least one more argument `\"-refresh=false\"`. If that's not present then it would confirm that this flag is getting \"lost\" somewhere in the chain between your local Terraform CLI and the remote one running in Terraform Cloud Agent.\r\n\r\n"],"labels":["bug","new","cloud","v1.6"]},{"title":"Terraform Test: Add ability to continue running tests on failure","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.3\r\non darwin_arm64\r\n```\r\n\r\n### Use Cases\r\n\r\nI am hoping to reduce the development cycle for writing tests against \"heavy\" resources that take a long time to create and destroy.\r\n\r\nMy specific case is writing a series of tests against a module that creates AWS Elasticache Redis clusters. The Elasticache service is very flexible in how to configure these clusters and I have (so far) four separate `run` blocks for various configurations using the `apply` target. AWS is very slow in provisioning these resources (25 minutes!) so an error in the first test will abort the run and not surface errors in any subsequent tests.\r\n\r\nI will note that this may become obsolete if\/when #34180 is closed.\r\n\r\n### Attempted Solutions\r\n\r\nI've worked around this by simply commenting out all but the single test I'm working on, ensuring it works, and then moving on to the next test in isolation.\r\n\r\n### Proposal\r\n\r\nAdd `-continue-on-failure` CLI flag that will always run each test regardless of failures.\r\n\r\n### References\r\n\r\n- #34180","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hey @richid, the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me oismail@hashicorp.com and we can schedule a time to chat!"],"labels":["enhancement","new","terraform test"]},{"title":"Terraform Test: add the ability to run tests in parallel","body":"### Terraform Version\n\n```shell\nTerraform v1.6.3\n```\n\n\n### Use Cases\n\nIn order to speed up the testing process, we would like to have the ability to run all the tests in parallel (which are not depended on each other) \n\n### Attempted Solutions\n\nWe tried to split our test into multiple testing files, but it seems the test still runs one by one and not in parallel \n\n### Proposal\n\nAdd the ability to run test in parallel\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Hey @ron-matsliah, the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me oismail@hashicorp.com and we can schedule a time to chat!"],"labels":["enhancement","new","terraform test"]},{"title":"honor openssh settings from user ssh config file","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.6\r\non windows_amd64\r\n+ provider registry.terraform.io\/hashicorp\/local v2.4.0\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.1\r\n+ provider registry.terraform.io\/oracle\/oci v5.18.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n...terraform config...\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\n2023-11-01T18:48:26.536+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.536+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.537+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.537+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.538+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.539+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.539+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.540+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.541+0100 [WARN]  unexpected data: registry.terraform.io\/oracle\/oci:stdout=\"in privateKeyPath section\"\r\n2023-11-01T18:48:26.542+0100 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=\"rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n2023-11-01T18:48:26.574+0100 [DEBUG] provider: plugin process exited: path=.terraform\/providers\/registry.terraform.io\/oracle\/oci\/5.18.0\/windows_amd64\/terraform-provider-oci_v5.18.0.exe pid=10980\r\n2023-11-01T18:48:26.574+0100 [DEBUG] provider: plugin exited\r\n2023-11-01T18:48:26.578+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=72312d47-45dc-64ee-d44d-e1a8ef4803f8 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=triggers tf_resource_type=null_resource tf_rpc=ValidateResourceTypeConfig timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.578+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=ValidateResourceTypeConfig tf_req_id=14cc2888-699b-294b-91cf-07eb92b6c82b tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.578+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=id tf_req_id=9dd5cd36-2ecb-5b31-f774-8f35688628a5 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.579+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_attribute_path=id tf_req_id=9dd5cd36-2ecb-5b31-f774-8f35688628a5 tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=ValidateResourceTypeConfig timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.580+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=14cc2888-699b-294b-91cf-07eb92b6c82b tf_resource_type=null_resource tf_attribute_path=id @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.580+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_resource_type=null_resource tf_rpc=ValidateResourceTypeConfig @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_attribute_path=triggers tf_req_id=72312d47-45dc-64ee-d44d-e1a8ef4803f8 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.581+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_req_id=9dd5cd36-2ecb-5b31-f774-8f35688628a5 tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null @module=sdk.framework tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.581+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @module=sdk.framework tf_attribute_path=triggers tf_req_id=14cc2888-699b-294b-91cf-07eb92b6c82b tf_resource_type=null_resource tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.581+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @module=sdk.framework tf_req_id=9dd5cd36-2ecb-5b31-f774-8f35688628a5 tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.581+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=14cc2888-699b-294b-91cf-07eb92b6c82b tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.582+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_req_id=72312d47-45dc-64ee-d44d-e1a8ef4803f8 tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=ValidateResourceTypeConfig timestamp=2023-11-01T18:48:26.578+0100\r\n2023-11-01T18:48:26.582+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=ValidateResourceTypeConfig @module=sdk.framework tf_req_id=72312d47-45dc-64ee-d44d-e1a8ef4803f8 timestamp=2023-11-01T18:48:26.579+0100\r\n2023-11-01T18:48:26.585+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: marking computed attribute that is null in the config as unknown: tf_attribute_path=AttributeName(\"id\") tf_resource_type=null_resource tf_rpc=PlanResourceChange tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_planresourcechange.go:304 @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.585+0100\r\n2023-11-01T18:48:26.585+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: marking computed attribute that is null in the config as unknown: @module=sdk.framework tf_attribute_path=AttributeName(\"id\") tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_planresourcechange.go:304 tf_resource_type=null_resource tf_rpc=PlanResourceChange tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.585+0100\r\n2023-11-01T18:48:26.586+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: marking computed attribute that is null in the config as unknown: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_planresourcechange.go:304 tf_attribute_path=AttributeName(\"id\") tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @module=sdk.framework tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.585+0100\r\n2023-11-01T18:48:26.586+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null @module=sdk.framework tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.587+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 @module=sdk.framework tf_attribute_path= tf_resource_type=null_resource tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.587+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_rpc=PlanResourceChange @module=sdk.framework tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.588+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @module=sdk.framework tf_attribute_path= tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.589+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_resource_type=null_resource tf_rpc=PlanResourceChange tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.589+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.589+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework tf_attribute_path= tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.590+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework tf_rpc=PlanResourceChange tf_attribute_path= timestamp=2023-11-01T18:48:26.586+0100\r\n2023-11-01T18:48:26.590+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework tf_attribute_path= @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 timestamp=2023-11-01T18:48:26.587+0100\r\n2023-11-01T18:48:26.590+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange @module=sdk.framework tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 timestamp=2023-11-01T18:48:26.587+0100\r\n2023-11-01T18:48:26.591+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_attribute_path= tf_provider_addr=registry.terraform.io\/hashicorp\/null timestamp=2023-11-01T18:48:26.587+0100\r\n2023-11-01T18:48:26.591+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" @module=sdk.framework tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.587+0100        \r\n2023-11-01T18:48:26.591+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource tf_attribute_path= timestamp=2023-11-01T18:48:26.588+0100\r\n2023-11-01T18:48:26.592+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_rpc=PlanResourceChange @module=sdk.framework description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.588+0100        \r\n2023-11-01T18:48:26.592+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: @module=sdk.framework tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 timestamp=2023-11-01T18:48:26.588+0100\r\n2023-11-01T18:48:26.592+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: @module=sdk.framework tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" tf_resource_type=null_resource tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.588+0100\r\n2023-11-01T18:48:26.593+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: tf_rpc=PlanResourceChange tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 @module=sdk.framework description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" tf_attribute_path=triggers timestamp=2023-11-01T18:48:26.588+0100        \r\n2023-11-01T18:48:26.593+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: description=\"If the value of this attribute changes, Terraform will destroy and recreate the resource.\" tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange @module=sdk.framework timestamp=2023-11-01T18:48:26.588+0100\r\n2023-11-01T18:48:26.594+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_attribute_path=triggers tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.594+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @module=sdk.framework tf_attribute_path=triggers @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 tf_resource_type=null_resource tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.594+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_resource_type=null_resource tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 @module=sdk.framework timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.594+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_rpc=PlanResourceChange tf_attribute_path=triggers tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource tf_provider_addr=registry.terraform.io\/hashicorp\/null @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 @module=sdk.framework timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.595+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_resource_type=null_resource @module=sdk.framework tf_attribute_path=triggers tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.595+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @module=sdk.framework tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 timestamp=2023-11-01T18:48:26.591+0100\r\n2023-11-01T18:48:26.595+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_resource_type=null_resource tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.596+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @module=sdk.framework tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_attribute_path=triggers tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.596+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource tf_rpc=PlanResourceChange tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.596+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=triggers timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.597+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource tf_attribute_path=triggers tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 @module=sdk.framework timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.597+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 tf_attribute_path=triggers tf_rpc=PlanResourceChange @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.597+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.598+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @module=sdk.framework tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.594+0100\r\n2023-11-01T18:48:26.598+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_attribute_path=triggers tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @module=sdk.framework timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.599+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework tf_attribute_path=triggers tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.599+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: @module=sdk.framework description=\"Once set, the value of this attribute in state will not change.\" tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.599+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:78 @module=sdk.framework tf_attribute_path=triggers tf_rpc=PlanResourceChange tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.600+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null @module=sdk.framework tf_attribute_path=triggers tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_resource_type=null_resource tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_value.go:80 timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.600+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 @module=sdk.framework description=\"Once set, the value of this attribute in state will not change.\" tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.600+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: tf_attribute_path=id tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework description=\"Once set, the value of this attribute in state will not change.\" timestamp=2023-11-01T18:48:26.595+0100\r\n2023-11-01T18:48:26.601+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined AttributePlanModifier: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:52 description=\"Once set, the value of this attribute in state will not change.\" tf_attribute_path=id tf_resource_type=null_resource tf_rpc=PlanResourceChange @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea timestamp=2023-11-01T18:48:26.597+0100\r\n2023-11-01T18:48:26.601+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @module=sdk.framework tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.597+0100\r\n2023-11-01T18:48:26.601+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: @module=sdk.framework tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 description=\"Once set, the value of this attribute in state will not change.\" tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.597+0100\r\n2023-11-01T18:48:26.602+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange tf_resource_type=null_resource @module=sdk.framework tf_req_id=810fef26-cd51-112f-3955-a8b7cc458322 timestamp=2023-11-01T18:48:26.597+0100\r\n2023-11-01T18:48:26.602+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined AttributePlanModifier: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/attribute_plan_modification.go:60 @module=sdk.framework tf_attribute_path=id tf_resource_type=null_resource description=\"Once set, the value of this attribute in state will not change.\" tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.597+0100\r\n2023-11-01T18:48:26.602+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_rpc=PlanResourceChange tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.600+0100\r\n2023-11-01T18:48:26.603+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @module=sdk.framework timestamp=2023-11-01T18:48:26.600+0100\r\nnull_resource.cluster_configuration[1]: Creating...\r\n2023-11-01T18:48:26.603+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 tf_req_id=7a0f6811-dc74-b55d-dac6-13722d6c2dea tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.600+0100\r\nnull_resource.cluster_configuration[2]: Creating...\r\nnull_resource.cluster_configuration[0]: Creating...\r\n2023-11-01T18:48:26.603+0100 [INFO]  Starting apply for null_resource.cluster_configuration[1]\r\n2023-11-01T18:48:26.603+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=2948134a-f3b3-3a1e-bfbc-7bcaa795af20 tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_rpc=PlanResourceChange timestamp=2023-11-01T18:48:26.600+0100\r\n2023-11-01T18:48:26.604+0100 [INFO]  Starting apply for null_resource.cluster_configuration[2]\r\n2023-11-01T18:48:26.604+0100 [INFO]  Starting apply for null_resource.cluster_configuration[0]\r\n2023-11-01T18:48:26.604+0100 [DEBUG] null_resource.cluster_configuration[2]: applying the planned Create change\r\n2023-11-01T18:48:26.604+0100 [DEBUG] null_resource.cluster_configuration[1]: applying the planned Create change\r\n2023-11-01T18:48:26.604+0100 [DEBUG] null_resource.cluster_configuration[0]: applying the planned Create change\r\n2023-11-01T18:48:26.605+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Resource Create: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=0c6721df-565a-6084-480e-808182e417f1 tf_resource_type=null_resource tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:96 @module=sdk.framework timestamp=2023-11-01T18:48:26.605+0100\r\n2023-11-01T18:48:26.606+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Resource Create: tf_req_id=13f7b86b-8426-1857-fab1-49bfddcd6f56 tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:96 @module=sdk.framework timestamp=2023-11-01T18:48:26.605+0100\r\n2023-11-01T18:48:26.607+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Resource Create: @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=f784e027-e3a4-464f-8d02-f79865438cef tf_resource_type=null_resource tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:96 timestamp=2023-11-01T18:48:26.605+0100\r\n2023-11-01T18:48:26.608+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 tf_attribute_path=id @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=0c6721df-565a-6084-480e-808182e417f1 tf_resource_type=null_resource tf_rpc=ApplyResourceChange timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.609+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: tf_req_id=0c6721df-565a-6084-480e-808182e417f1 tf_resource_type=null_resource tf_rpc=ApplyResourceChange tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_attribute_path=id @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 @module=sdk.framework timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.609+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=f784e027-e3a4-464f-8d02-f79865438cef tf_resource_type=null_resource @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 @module=sdk.framework tf_attribute_path=id tf_rpc=ApplyResourceChange timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.609+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Calling provider defined Type Validate: tf_rpc=ApplyResourceChange @module=sdk.framework tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_resource_type=null_resource tf_req_id=13f7b86b-8426-1857-fab1-49bfddcd6f56 @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:71 tf_attribute_path=id timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.610+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Resource Create: tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=0c6721df-565a-6084-480e-808182e417f1 tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:98 @module=sdk.framework tf_resource_type=null_resource timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.611+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 @module=sdk.framework tf_attribute_path=id tf_resource_type=null_resource tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=f784e027-e3a4-464f-8d02-f79865438cef tf_rpc=ApplyResourceChange timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.611+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Type Validate: @module=sdk.framework tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=13f7b86b-8426-1857-fab1-49bfddcd6f56 tf_resource_type=null_resource tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwschemadata\/data_set_at_path.go:73 timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.611+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Resource Create: @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:98 tf_attribute_path=id tf_provider_addr=registry.terraform.io\/hashicorp\/null tf_req_id=f784e027-e3a4-464f-8d02-f79865438cef tf_resource_type=null_resource tf_rpc=ApplyResourceChange @module=sdk.framework timestamp=2023-11-01T18:48:26.608+0100\r\nnull_resource.cluster_configuration[1]: Provisioning with 'file'...\r\n2023-11-01T18:48:26.611+0100 [DEBUG] provider.terraform-provider-null_v3.2.1_x5.exe: Called provider defined Resource Create: tf_req_id=13f7b86b-8426-1857-fab1-49bfddcd6f56 tf_provider_addr=registry.terraform.io\/hashicorp\/null @module=sdk.framework tf_attribute_path=id tf_resource_type=null_resource tf_rpc=ApplyResourceChange @caller=github.com\/hashicorp\/terraform-plugin-framework@v0.16.0\/internal\/fwserver\/server_createresource.go:98 timestamp=2023-11-01T18:48:26.608+0100\r\n2023-11-01T18:48:26.612+0100 [DEBUG] Connecting to 10.14.8.76:22 for SSH\r\nnull_resource.cluster_configuration[2]: Provisioning with 'file'...\r\nnull_resource.cluster_configuration[0]: Provisioning with 'file'...\r\n2023-11-01T18:48:26.612+0100 [DEBUG] Connecting to 10.14.0.201:22 for SSH\r\n2023-11-01T18:48:26.612+0100 [DEBUG] Connecting to 10.14.9.146:22 for SSH\r\nnull_resource.cluster_configuration[2]: Still creating... [10s elapsed]\r\nnull_resource.cluster_configuration[0]: Still creating... [10s elapsed]\r\nnull_resource.cluster_configuration[1]: Still creating... [10s elapsed]\r\n2023-11-01T18:48:41.616+0100 [ERROR] connection error: dial tcp 10.14.9.146:22: i\/o timeout\r\n2023-11-01T18:48:41.617+0100 [WARN]  retryable error: dial tcp 10.14.9.146:22: i\/o timeout\r\n2023-11-01T18:48:41.617+0100 [INFO]  sleeping for 1s\r\n2023-11-01T18:48:41.617+0100 [ERROR] connection error: dial tcp 10.14.8.76:22: i\/o timeout\r\n2023-11-01T18:48:41.618+0100 [WARN]  retryable error: dial tcp 10.14.8.76:22: i\/o timeout\r\n2023-11-01T18:48:41.618+0100 [ERROR] connection error: dial tcp 10.14.0.201:22: i\/o timeout\r\n2023-11-01T18:48:41.618+0100 [WARN]  retryable error: dial tcp 10.14.0.201:22: i\/o timeout\r\n2023-11-01T18:48:41.618+0100 [INFO]  sleeping for 1s\r\n2023-11-01T18:48:41.618+0100 [INFO]  sleeping for 1s\r\n2023-11-01T18:48:42.628+0100 [DEBUG] Connecting to 10.14.0.201:22 for SSH\r\n2023-11-01T18:48:42.629+0100 [DEBUG] Connecting to 10.14.9.146:22 for SSH\r\n2023-11-01T18:48:42.629+0100 [DEBUG] Connecting to 10.14.8.76:22 for SSH\r\nStopping operation...\r\n2023-11-01T18:48:44.858+0100 [WARN]  terraform: Stop called, initiating interrupt sequence\r\n2023-11-01T18:48:44.859+0100 [WARN]  terraform: run context exists, stopping\r\n2023-11-01T18:48:44.859+0100 [INFO]  terraform: waiting for graceful stop to complete\r\n\r\nInterrupt received.\r\nPlease wait for Terraform to exit or data loss may occur.\r\nGracefully shutting down...\r\n\r\n2023-11-01T18:48:44.860+0100 [WARN]  Errors while provisioning null_resource.cluster_configuration[0] with \"file\", so aborting\r\n2023-11-01T18:48:44.860+0100 [WARN]  Errors while provisioning null_resource.cluster_configuration[2] with \"file\", so aborting\r\n2023-11-01T18:48:44.860+0100 [WARN]  Errors while provisioning null_resource.cluster_configuration[1] with \"file\", so aborting\r\n2023-11-01T18:48:44.876+0100 [ERROR] vertex \"null_resource.cluster_configuration[0]\" error: file provisioner error\r\n2023-11-01T18:48:44.876+0100 [ERROR] vertex \"null_resource.cluster_configuration[0]\" error: execution halted\r\n2023-11-01T18:48:44.877+0100 [ERROR] vertex \"null_resource.cluster_configuration[0]\" error: execution halted\r\n2023-11-01T18:48:44.892+0100 [ERROR] vertex \"null_resource.cluster_configuration[1]\" error: file provisioner error\r\n2023-11-01T18:48:44.892+0100 [ERROR] vertex \"null_resource.cluster_configuration[1]\" error: execution halted\r\n2023-11-01T18:48:44.893+0100 [ERROR] vertex \"null_resource.cluster_configuration[1]\" error: execution halted\r\n2023-11-01T18:48:44.908+0100 [ERROR] vertex \"null_resource.cluster_configuration[2]\" error: file provisioner error\r\n2023-11-01T18:48:44.908+0100 [ERROR] vertex \"null_resource.cluster_configuration[2]\" error: execution halted\r\n2023-11-01T18:48:44.908+0100 [ERROR] vertex \"null_resource.cluster_configuration[2]\" error: execution halted\r\n2023-11-01T18:48:44.908+0100 [WARN]  terraform: stop complete\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: execution halted\r\n\u2502\r\n\u2502\r\n\u2575\r\n\u2577\r\n\u2502 Error: file provisioner error\r\n\u2502\r\n\u2502   with null_resource.cluster_configuration[1],\r\n\u2502   on job.tf line 57, in resource \"null_resource\" \"cluster_configuration\":\r\n\u2502   57:     provisioner \"file\" {\r\n\u2502\r\n\u2502 interrupted - last error: dial tcp 10.14.8.76:22: i\/o timeout\r\n\u2575\r\n\u2577\r\n\u2502 Error: file provisioner error\r\n\u2502\r\n\u2502   with null_resource.cluster_configuration[0],\r\n\u2502   on job.tf line 57, in resource \"null_resource\" \"cluster_configuration\":\r\n\u2502   57:     provisioner \"file\" {\r\n\u2502\r\n\u2502 interrupted - last error: dial tcp 10.14.9.146:22: i\/o timeout\r\n\u2575\r\n\u2577\r\n\u2502 Error: file provisioner error\r\n\u2502\r\n\u2502   with null_resource.cluster_configuration[2],\r\n\u2502   on job.tf line 57, in resource \"null_resource\" \"cluster_configuration\":\r\n\u2502   57:     provisioner \"file\" {\r\n\u2502\r\n\u2502 interrupted - last error: dial tcp 10.14.0.201:22: i\/o timeout\r\n\u2575\r\n2023-11-01T18:48:44.945+0100 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=\"rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n2023-11-01T18:48:44.957+0100 [DEBUG] provider: plugin process exited: path=.terraform\/providers\/registry.terraform.io\/hashicorp\/null\/3.2.1\/windows_amd64\/terraform-provider-null_v3.2.1_x5.exe pid=36404\r\n2023-11-01T18:48:44.958+0100 [DEBUG] provider: plugin exited\r\n```\r\n### Expected Behavior\r\n\r\nI expected terraform to use ~\/.ssh\/config file so it can use a bastion host configured there\r\n\r\n### Actual Behavior\r\n\r\nTerraform ignored the ~\/.ssh\/config file and tried to connect to the IPs (compute nodes on OCI) without using the bastion defined in ~\/.ssh\/config\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nI cannot give you a complete code as it involves OCI and sensitive data but, in a nutshell, my code:\r\n1. Create Compute nodes on OCI\r\n2. Has couple template files which I fill with information from the newly created compute nodes\r\n3. It tries to copy the files to the compute nodes which are behind a bastion server\r\n\r\ncompute -> bastion -> VM\r\n\r\nMy ssh to the VM is working, so I expected terraform to use the same connection.\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @JulienAndonov,\r\n\r\nThanks for filing the issue. Terraform does not use openssh for the the ssh connection, so does not natively read the openssh configuration files. The provisioner connection is typically configured via the [`connection` block](https:\/\/developer.hashicorp.com\/terraform\/language\/resources\/provisioners\/connection) when specifying the provisioner settings, including a bastion host.\r\n\r\nSince Terraform is working as designed, I'll convert this to an enhancement request so it can be evaluated for future development. Note however that [provisioners are not the preferred method for host configuration](https:\/\/developer.hashicorp.com\/terraform\/language\/resources\/provisioners\/syntax#provisioners-are-a-last-resort), so major changes here are not likely in the near term, and automatically reading the file may not be possible when it may break existing workflows where the file is intended to be ignored."],"labels":["enhancement","provisioners"]},{"title":"Feature Request: Allow remote removal of state files (useful for CI pipelines)","body":"### Use Cases\r\n\r\nTerraform running in CI provisioning transient resources\r\n\r\n### Attempted Solutions\r\n\r\nRemoving state file with third party api after all resources have been destroyed. This works, but it's not ideal.\r\n\r\n### Proposal\r\n\r\nTerraform should have a mechanism to delete a statefile held remotely by the backend. The justifications for not doing this do not address the proposed use case.\r\n\r\nUser bob pushes up code to repo Foo. Foo has a pipeline that uses Terraform. There is no notion of transient provisions in Terraform. The pipeline should be transient. Upon completion all resources and the remote state file should be deleted.\r\n\r\n**This does not mean Terraform must not use a state file in provisioning and later deleting resources, on the contrary that's totally fine; but terraform should not force a statefile to exist after `terraform destroy` has been run.**  Gitlab and other third party back ends track state files. Statefiles without resources still exist, [this is by design](https:\/\/github.com\/hashicorp\/terraform\/issues\/10474#issuecomment-264318061) (further covered in 10474):\r\n\r\n> This isn't possible because there needs to be some database of some sort to map Terraform config <=> real world. For some providers like AWS you could theoretically use something like AWS tags (early versions of Terraform actually had no state file and did this). We quickly ran into problems: not all resources support tags.\r\n\r\nHowever, this does not address why you need \"AWS tags\" or a mapping between two worlds, when you fundamentally want to destroy that mapping and start fresh later, and have already successfully executed a `terraform destroy`.\r\n\r\nBy extending the backend to allow cleanup, we remove the need for others to learn proprietary Terraform-state-management-APIs, such as those by [Gitlab](https:\/\/docs.gitlab.com\/ee\/user\/infrastructure\/iac\/terraform_state.html#remove-a-state-file-by-using-the-api) and we make easier cross-backend migrations (moving from GitLab to AWS or Azure for example)\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @EvanCarroll! Thanks for sharing this feature request.\r\n\r\nThinking about Terraform's current functionality, I think the mechanism closest to what you've described is the ability to create and delete workspaces within an existing backend. [`terraform workspace delete`](https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/workspace\/delete) is an existing operation that is implemented by most backends literally as deleting a state snapshot from the remote system, because the set of available workspaces is itself implemented by listing the objects in the remote system.\r\n\r\nOf course, today most backends don't allow deleting their special \"default\" workspace which is assume to always exist, and so there isn't any way to reach _zero_ remote state snapshots using just Terraform commands. We could in principle allow deleting the default workspace, but many existing backends assume that it always exists without actually storing a representation of it in the remote system. For example, the `s3` backend will announce that `default` exists even if there's not yet anything at all stored in the nominated S3 bucket, and will create the first state snapshot only at the first apply.\r\n\r\nWith all of that said then, I think there's a design question here about what exactly deleting a state snapshot means in Terraform's model. `terraform workspace delete` is a potential vehicle for it, but it isn't a perfect fit as currently designed. We could also add something new for it, but we'd still need to think about how that should interact with the existing functionality of `terraform workspace delete`.\r\n\r\nThanks again!\r\n","All good thoughts. So long as deleting the default workspace ends up effectively running code like this,\r\n\r\n    curl --header \"Private-Token: <your_access_token>\" --request DELETE \"https:\/\/gitlab.example.com\/api\/v4\/projects\/<your_project_id>\/terraform\/state\/<your_state_name>\"\r\n\r\nGitlab doesn't expose the buckets to us, so we rely on the API. I would just like to see that code moved to the backend transport, a desirable API would be somethig like\r\n\r\n    terraform destroy --destroy-workspace"],"labels":["enhancement","new"]},{"title":"Upgrading from v0.13.7 to 0.14.11","body":"### Terraform Version\r\n\r\n```shell\r\n0.14.11\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nterraform {\r\n  required_version = \">= 0.14\"\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n    }\r\n    archive = {\r\n      source = \"hashicorp\/archive\"\r\n    }\r\n    external = {\r\n      source = \"hashicorp\/external\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\npanic: value is marked, so must be unmarked first\r\n\r\ngoroutine 21444 [running]:\r\ngithub.com\/zclconf\/go-cty\/cty.Value.assertUnmarked(...)\r\n\t\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.8.0\/cty\/marks.go:123\r\ngithub.com\/zclconf\/go-cty\/cty.Value.LengthInt(0x2cebc40, 0xc012e04980, 0x25b4900, 0xc00654ea00, 0xc005085400)\r\n\t\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.8.0\/cty\/value_ops.go:1045 +0x4f\r\ngithub.com\/hashicorp\/terraform\/plans\/objchange.proposedNewObject(0xc0017a8c60, 0x2cebc00, 0xc00e63fe00, 0x248e380, 0x3e01300, 0x2cebc00, 0xc012e04990, 0x24bc420, 0xc014f83d70, 0x82, ...)\r\n\t\/home\/circleci\/project\/project\/plans\/objchange\/objchange.go:241 +0x18a5\r\ngithub.com\/hashicorp\/terraform\/plans\/objchange.PlannedDataResourceObject(0xc0017a8c60, 0x2cebc00, 0xc012e04990, 0x24bc420, 0xc014f83d70, 0x1, 0x0, 0x0, 0x0)\r\n\t\/home\/circleci\/project\/project\/plans\/objchange\/objchange.go:64 +0x89\r\ngithub.com\/hashicorp\/terraform\/terraform.(*evalReadDataPlan).Eval(0xc0087b7a90, 0x2d278e0, 0xc00fdaa1a0, 0x0, 0x0, 0x0, 0x0)\r\n\t\/home\/circleci\/project\/project\/terraform\/eval_read_data_plan.go:70 +0x952\r\ngithub.com\/hashicorp\/terraform\/terraform.(*NodePlannableResourceInstance).dataResourceExecute(0xc0123921f0, 0x2d278e0, 0xc00fdaa1a0, 0x0, 0x380000)\r\n\t\/home\/circleci\/project\/project\/terraform\/node_resource_plan_instance.go:87 +0x4b4\r\ngithub.com\/hashicorp\/terraform\/terraform.(*NodePlannableResourceInstance).Execute(0xc0123921f0, 0x2d278e0, 0xc00fdaa1a0, 0xc00e808002, 0x250b240, 0x2825d00)\r\n\t\/home\/circleci\/project\/project\/terraform\/node_resource_plan_instance.go:41 +0x11f\r\ngithub.com\/hashicorp\/terraform\/terraform.(*ContextGraphWalker).Execute(0xc017700680, 0x2d278e0, 0xc00fdaa1a0, 0x7fb952edc450, 0xc0123921f0, 0x0, 0x0, 0x0)\r\n\t\/home\/circleci\/project\/project\/terraform\/graph_walk_context.go:127 +0xbc\r\ngithub.com\/hashicorp\/terraform\/terraform.(*Graph).walk.func1(0x2825d00, 0xc0123921f0, 0x0, 0x0, 0x0)\r\n\t\/home\/circleci\/project\/project\/terraform\/graph.go:59 +0x962\r\ngithub.com\/hashicorp\/terraform\/dag.(*Walker).walkVertex(0xc004bd60c0, 0x2825d00, 0xc0123921f0, 0xc00e65fac0)\r\n\t\/home\/circleci\/project\/project\/dag\/walk.go:387 +0x375\r\ncreated by github.com\/hashicorp\/terraform\/dag.(*Walker).Update\r\n\t\/home\/circleci\/project\/project\/dag\/walk.go:309 +0x1246\r\n\r\n\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nA crash log has been placed at \"crash.log\" relative to your current\r\nworking directory. It would be immensely helpful if you could please\r\nreport the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version. That\r\ninformation is available on the first line of crash.log. You can also\r\nget it by running 'terraform --version' on the command line.\r\n\r\nSECURITY WARNING: the \"crash.log\" file that was created may contain \r\nsensitive information that must be redacted before it is safe to share \r\non the issue tracker.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\nFailed running command \"cd '\/terraform' && envdir \/env terraform show -json \/terraform\/terraform.tfplan\" (exit 1)\r\nOutput: Terraform couldn't read the given file as a state or plan file.\r\nThe errors while attempting to read the file as each format are\r\nshown below.\r\n\r\nState read error: Error loading statefile: open \/terraform\/terraform.tfplan: no such file or directory\r\n\r\nPlan read error: open \/terraform\/terraform.tfplan: no such file or directory\r\n```\r\n\r\n### Expected Behavior\r\n\r\nApply should be completed successfully with v0.14.11\r\n\r\n### Actual Behavior\r\n\r\nTerraform crash\r\n\r\n### Steps to Reproduce\r\n\r\nterraform apply -no-color -auto-approve > named_pipe 2>&1\r\n\r\n### Additional Context\r\n\r\nThe upgrade worked successfully for multiple environments with any issue. Started seeing the crash failures out of nowhere. No code changes involved.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @tipirnenichowdary! I'm sorry this didn't work as expected.\r\n\r\nThe documented upgrade process includes an intermediate step of upgrading from v0.13 to v0.14 primarily because that extra step helps confirm that the v0.13 upgrade process was completed successfully before moving on. However, if you've already been successfully using Terraform v0.13 for some time you may be able to successfully skip the v0.14 series and upgrade directly to the latest v1.0.x release, which at the time of writing is v1.0.11.\r\n\r\nCan you try that and see if it improves the situation? I'm hoping that the bug you encountered was fixed in a later release.\r\n\r\nIf you are able to successfully upgrade to v1.0.11 then from there you should be able to upgrade to the very latest v1.x minor release, since everything in that major series is subject to [compatibility promises](https:\/\/developer.hashicorp.com\/terraform\/language\/v1-compatibility-promises). (There are some pragmatic exceptions, but they are only rarely relevant.)\r\n\r\nThe v0.14 series is well past its period of routine maintenance now, so it's unlikely to see any new releases unless they are for very significant security reasons, or similar. So I think we will need to find a way to get you past this error despite the bug, rather than fixing the bug. (And as noted above, I hope it's already been fixed in a later release anyway.)\r\n","Thank you @apparentlymart. I will upgrade directly to 1.0.11"],"labels":["bug","waiting-response","new","v0.14"]},{"title":"Terraform test: fuzz testing \/ fuzzing","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.2\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWhile `terraform test` feature introduces [integration \/ unit testing](https:\/\/developer.hashicorp.com\/terraform\/language\/tests#integration-or-unit-testing), I was wondering how it could go further and helps validating a configuration or more specifically a module, by testing **several combinations** of **input variables**, in order to validate the config \/ module **quality** and **behavior**, especially after a huge evolution (major update or refactoring). As I'm not a dev, I'm not sure wether it would be called [acceptance](https:\/\/en.wikipedia.org\/wiki\/Software_testing#Acceptance_testing), [smoke](https:\/\/en.wikipedia.org\/wiki\/Smoke_testing_(software)) or [non-regression](https:\/\/en.wikipedia.org\/wiki\/Software_testing#Regression_testing) testing.\r\n\r\n### Attempted Solutions\r\n\r\n.\r\n\r\n### Proposal\r\n\r\nMain ideas would be twofold:\r\n1. Load some file or generate values following input variables constraints for \"positive\" testing, or generate them randomly and expect tests failures when forbidden values are used.\r\n2. Since it would look like some kind of brute force testing \ud83d\ude05, set up an upper value in order to limit the number of tested combinations against the target API.\r\n\r\nThese functions may be already be available separately (`.tfvars` loading, using several [`random` resources](https:\/\/registry.terraform.io\/providers\/hashicorp\/random\/latest\/docs)), but I don't think the second part exists, unless some `count` or `for_each` logic is authorized on `run` blocks. And for the first part, I think giving examples on how to use `random` ressources to fuel tests would be of great interest.\r\n\r\n\r\n\r\nPS: thx @omarismail for our brainstorming session \ud83d\ude09\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @DamienCarpy! Thanks for sharing this feedback.\r\n\r\nWhen I visualize what I understood from your proposal, I think it seems to be in the area of \"property-based testing\", which is a broad term for testing strategies that test arbitrary input against a set of rules (\"properties\") that are expected to always hold. This is in contrast to \"example-based testing\", which is likewise a broad term for all of the strategies that involve hand-writing example input and its expected output, which is what today's `terraform test` does.\r\n\r\nI suppose we could say that typical \"fuzzing\" is a specific case of property-based testing where the property being tested is resilience to arbitrary user input, and often specifically testing whether a program can handle arbitrary input without crashing or performing unsafe memory access.\r\n\r\nWith all of that said then, one specific way I could imagine designing this is to have Terraform generate random correctly-typed garbage to feed to a module's input variables, and to quickly discard any input that doesn't pass the variables' validation rules to create a corpus of inputs that the module should theoretically accept. That alone would permit a basic form of testing where the property is \"can be applied without error\", helping to judge whether the input variable validation rules are sufficiently specified.\r\n\r\nI'm not sure where I'd take that idea next, though. One answer could be to use it to then generate \"equivalence tests\" where the system remembers what the desired state and possibly the actual final state looked like, and then fix those as test cases that will fail if anything changes in future. But I'm skeptical that would be useful since it would presumably just fail as soon as you change _anything_ about the module. It would also, I think, require the use of mock providers to ensure that the responses to particular input stay consistent, since many resource types intentionally produce different-but-equivalent results, such as including generated ids that are not predictable during the plan phase.\r\n\r\nAm I thinking in the right direction for what you proposed? Do you have any further ideas based on what I've described here? I think this seems like something that might be best to start experimenting with in a separate tool outside of Terraform that generates `.tftest.hcl` files and then runs `terraform test` with them, since in that case it should be easier to iterate quickly and learn which capabilities seem broadly useful vs. what is hard to do effectively in practice.\r\n\r\n\r\n\r\n","Hi @apparentlymart, thanks for your detailed reply. I share your skepticism about the \"equivalence testing\" direction you described, I also think it would inevitably fail. As for the main idea of generating random input values, at first I also thought of using an external program to do that, but I wanted to push the idea of using Terraform to the limit first. I agree experimenting is now the way to go \ud83d\udc4d\ud83c\udffb"],"labels":["enhancement","new","terraform test"]},{"title":"terraform is creating `~\/.terraform.d` directory on macOS","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.7\r\non darwin_arm64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.6.2. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nna\r\n\r\n### Debug Output\r\n\r\n$ tree ~\/.terraform.d\/\r\n\/Users\/jasonkarns\/.terraform.d\/\r\n\u251c\u2500\u2500 checkpoint_cache\r\n\u2514\u2500\u2500 checkpoint_signature\r\n\r\n1 directory, 2 files\r\n\r\n\r\n### Expected Behavior\r\n\r\nDirectory should not exist\r\n\r\n### Actual Behavior\r\n\r\nDirectory is being created.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n`terraform fmt`\r\n\r\n-> creates ~\/.terraform.d\/checkpoint_signature\r\n\r\n`terraform --version`\r\n\r\n-> creates ~\/.terraform.d\/checkpoint_cache\r\n\r\n### Additional Context\r\n\r\nThis is a reopen of https:\/\/github.com\/hashicorp\/terraform\/issues\/33289 which was closed presuming it was a dupe of https:\/\/github.com\/hashicorp\/terraform\/issues\/15389. But 15389 is limited in scope to linux.\r\n\r\nPer the comments in 33289, this directory should not be created on macOS. Supposedly, they should be following Apple's guidelines and using Application Support. (Personally, I disagree and believe terraform should be following xdg as darwin is unix-like and developers configure their mac's similarly across both macos, bsd, and linux for portable configurations. Regardless, this is still a bug because whether the files should be in XDG vs ~\/Library, in no case should they be in ~\/.terraform.d.\r\n\r\n### References\r\n\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/15389\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/33289","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Source code not available error when importing EC2 instance (This character is not used within the language.)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.2\r\non windows_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.22.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n```\r\nimport {\r\n  to = aws_instance.name\r\n  id = \"i-xxxxxxxxxxxx\"\r\n}\r\n```\r\n### Debug Output\r\n\r\nwill provide if needed.\r\n\r\n### Expected Behavior\r\n\r\nShould be able to create generated.tf\r\n\r\n### Actual Behavior\r\n\r\nNot able to create generated.tf\r\n```\r\nterraform plan -generate-config-out=\"generated.tf\"\r\naws_instance.name: Preparing import... [id=i-xxxxxxxxxxxxxx]\r\naws_instance.name: Refreshing state... [id=i-xxxxxxxxxxxxxx]\r\n\r\nPlanning failed. Terraform encountered an error while generating this plan.\r\n\r\n\u2577\r\n\u2502 Error: Invalid character\r\n\u2502\r\n\u2502   on generated.tf line 33:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 This character is not used within the language.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid expression\r\n\u2502\r\n\u2502   on generated.tf line 33:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 Expected the start of an expression, but found an invalid expression token.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid character\r\n\u2502\r\n\u2502   on generated.tf line 43:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 This character is not used within the language.\r\n```\r\n### Steps to Reproduce\r\n```\r\nterraform init\r\nterraform plan -generate-config-out=generated.tf\r\n```\r\n### Additional Context\r\n\r\nThe EC2 instance I am trying for was migrated from bare metal to AWS using AWS migration service.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @prahastakumar! Sorry this didn't work as expected.\r\n\r\nCould you also share the content of the `generated.tf` file? It seems like Terraform has generated something invalid in that file, and so it would be helpful to see exact what was generated to understand what's causing the problem.\r\n\r\nThanks!\r\n","I think this is probably coming from before `generated.tf` can actually be written since the plan could not complete. Maybe we can find a way to hook in there and dump the raw config to a file in the face of an error? I'm not sure what could be going wrong, but maybe `hclwrite.TokensForValue` (or its callers) has a bug with escaped newlines or quotes.\r\n","@apparentlymart It's not generating `generated.tf` file.\r\n\r\n@jbardin in logs I am seeing below error lines\r\n\r\n ```\r\n[ERROR] vertex \"aws_instance.name\" error: Invalid character\r\n[ERROR] vertex \"aws_instance.name\" error: Invalid character\r\n[ERROR] vertex \"aws_instance.name\" error: Invalid expression\r\n[TRACE] vertex \"aws_instance.name\": visit complete, with errors\r\n[TRACE] dag\/walk: upstream of \"root\" errored, so skipping\r\n[TRACE] vertex \"aws_instance.name (expand)\": dynamic subgraph encountered errors: Invalid character,Invalid character,Invalid expression\r\n[ERROR] vertex \"aws_instance.name (expand)\" error: Invalid character\r\n[ERROR] vertex \"aws_instance.name (expand)\" error: Invalid character\r\n[ERROR] vertex \"aws_instance.name (expand)\" error: Invalid expression\r\n[TRACE] vertex \"aws_instance.name (expand)\": visit complete, with errors\r\n```","any update on this?","Hi @prahastakumar,\r\n\r\nUnfortunately the log lines there don't add any more useful information. Your resource's particular state is somehow causing terraform to try and generate an invalid configuration, but without the state it's hard to guess what that could be.\r\n\r\nIf your only goal is to get this one resource imported, then I would suggest just creating the config manually. If you add a minimal configuration for the resource, the plan will indicate where it differs from the remote state, and what you need to add to get no changes. As a bonus, that process may show what is triggering the invalid configuration data.","I think the first step here is to get more detail printed in diags for generated config.\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Invalid character\r\n\u2502\r\n\u2502   on generated.tf line 33:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 This character is not used within the language.\r\n\u2575\r\n```\r\n\r\nThe error, `This character is not used within the language`, comes from `hclsyntax`, and should be accompanied by a sourced snippet. Since generated config is not processed through the main loader, the snippet is missing.\r\n\r\nThe diags could also do with one more wrapping line stating that these particular planning errors were due to config generation."],"labels":["bug","new","waiting for reproduction","plannable-import","v1.6"]},{"title":"Update constraints in dependency lock file when running `terraform init -upgrade` even if installed versions are unchanged","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.7\r\non darwin_arm64\r\n\r\nand also:\r\n\r\nTerraform v1.7.0-dev\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nIt's not 100% clear whether this is a bug or would be considered an enhancement. Given that the comments for `depsfile.Locks.Equal()`:\r\n> Equal explicitly _does not_ consider the equality of version constraints in the saved locks, because those are saved only as hints to help the UI explain what's changed between runs, and are never used as part of dependency installation decisions.\r\n\r\nsuggest that this is intentional, enhancement seems most appropriate.\r\n\r\nThe end goal here is to ensure that the dependency lock file accurately and completely reflects currently installed providers and their constraints.\r\n\r\nAs it stands, when the latest version of a provider is installed in compliance with a version constraint, and that constraint is then updated (whilst the installed provider version is still the latest version and is still compliant with the updated constraint), running `terraform init -upgrade` will not update the `constraints` property for the provider in `.terraform.lock.hcl`. When asking `init` to do an `-upgrade` to bring the dependency lock file up to date with all of the latest plugin versions, it seems reasonable to expect that it would also update any plugin version constraints which have changed, even if this didn't result in any changes to installed versions.\r\n\r\nTake the following `.terraform.lock.hcl` file, for example:\r\n```\r\nprovider \"registry.terraform.io\/hashicorp\/aws\" {\r\n  version     = \"5.23.0\"\r\n  constraints = \"~> 5.0\"\r\n  hashes = [\r\n    ...\r\n  ]\r\n}\r\n```\r\ngenerated from the following terraform config:\r\n```\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.0\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIf the terraform config is updated to:\r\n```\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.20\"\r\n    }\r\n  }\r\n}\r\n```\r\nthen, at the moment, running `terraform init -upgrade` will make no changes to the dependency lock file, however it seems reasonable that it should be updated to:\r\n```\r\nprovider \"registry.terraform.io\/hashicorp\/aws\" {\r\n  version     = \"5.23.0\"\r\n  constraints = \"~> 5.20\"\r\n  hashes = [\r\n    ...\r\n  ]\r\n}\r\n```\r\nto match the changed version constraints.\r\n\r\n### Attempted Solutions\r\n\r\nIn the trivial example above simply deleting the lock file and running `terraform init` would give the desired result.\r\n\r\n### Proposal\r\n\r\nUpdate `terraform init -upgrade` such that it will update the constraints in the dependency lock file even if the installed versions haven't changed.\r\n\r\nSome initial scoping suggests the following approach:\r\n* update `providercache.Installer.EnsureProviderVersions()` to add an additional check on requested version constraints vs lockfile version constraints if the provider is already installed and the hashes match (~line 360 of `internal\/providercache\/installer.go`)\r\n  * if the constraints have changed we could either not break out of the loop and 'install' the latest version (which would have misleading output about installing a version that's already installed); or\r\n  * simply call `locks.SetProvider()` with the new constraints, emit a new event (producing different output) and break out of the loop as per current behaviour\r\n* update `depsfile.Locks.Equal()` to consider equality of version constraints\r\n* add `getproviders.VersionConstraints.Equal()` method to enable comparison of version constraints (this could be avoided by comparing stringified versions via `getproviders.VersionConstraintsString`, but that's ugly)\r\n\r\nHappy to raise a PR for this.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Support use of computed values in import blocks IDs","body":"### Terraform Version\n\n```shell\nTerraform v1.6.2\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nI have an EC2 instance which I would like to optionally restore from a backup AMI via Terraform. This AMI contains snapshots of the instance's EBS volumes (these volumes are also managed in my Terraform configuration as their own resources), so when an instance is built with it, it builds the EBS volumes as well. The downside of this is that Terraform has no way of knowing about these volumes created by the AMI restore, so they won't be tracked in the state. So the only way to do it I can see is this:\r\n1. Run a Terraform apply with a variable such as restore_from_backup set to true, which tells Terraform to use the backup AMI to rebuild the server, and not to build the EBS volumes\r\n2. Manually import the EBS volumes to the state\r\n3. Apply Terraform again with restore_from_backup set to false, to make sure the tags etc. on the volumes are correct.\r\n\r\nHaving to manually import the EBS volumes and run two applies is not ideal. I'd like to be able to do all of this with a single apply.\n\n### Attempted Solutions\n\nThe first thing I tried was to use the ebs_block_device blocks inline on the aws_instance resource, rather than separate ebs_volume resources. Terraform recognised the restored EBS volumes in this case, but it always wanted to destroy and recreate them, along with the server itself, even when the parameters on the ebs_block_device blocks in the code matched the ones in the state. So that didn't work.\r\n\r\nMy second attempt involved using Terraform's new import blocks. The relevant code looks like this:\r\n\r\n```hcl\r\nlocals {\r\n  ebs_volumes = {\r\n    \"gocd-server\" = {\r\n      device_name = \"\/dev\/sdh\"\r\n      size        = var.gocd_server_volume_size\r\n    }\r\n    \"gocd-artifacts\" = {\r\n      device_name = \"\/dev\/sdg\"\r\n      size        = var.gocd_artifacts_volume_size\r\n    }\r\n  }\r\n}\r\n\r\ndata \"aws_ebs_volume\" \"restored_volumes\" {\r\n  for_each = var.restore_from_backup_enabled ? local.ebs_volumes : {}\r\n  depends_on = [aws_instance.gocd_server]\r\n  most_recent = true\r\n\r\n  filter {\r\n    name   = \"snapshot-id\"\r\n    values = [\r\n        for volume in data.aws_ami.gocd_ami.block_device_mappings : \r\n        volume.ebs.snapshot_id if volume.device_name == each.value.device_name\r\n    ]\r\n  }\r\n}\r\n\r\n# import blocks don't support for_each yet so having to hardcode the two volume imports\r\n# for_each will be added in v1.7: https:\/\/github.com\/hashicorp\/terraform\/pull\/33932\r\nimport {\r\n  to = aws_ebs_volume.persistent_volumes[\"gocd-server\"]\r\n  id = data.aws_ebs_volume.restored_volumes[\"gocd-server\"].id\r\n}\r\nimport {\r\n  to = aws_ebs_volume.persistent_volumes[\"gocd-artifacts\"]\r\n  id = data.aws_ebs_volume.restored_volumes[\"gocd-artifacts\"].id\r\n}\r\n\r\nresource \"aws_ebs_volume\" \"persistent_volumes\" {\r\n  for_each = local.ebs_volumes\r\n\r\n  availability_zone = data.aws_subnet.ec2_private_subnet.availability_zone\r\n  encrypted         = true\r\n  kms_key_id        = module.kms.kmskey_arn\r\n  size              = each.value.size\r\n  tags = merge(local.tags, {\r\n    Name = each.key\r\n  })\r\n}\r\n```\r\n\r\nSo I'm using the snapshot IDs from my AMI's data source as inputs to an EBS volume data source to find the volumes built from these snapshot IDs. I've used a depends_on for the EBS volume data source to delay its creation until after the EC2 instance is created, since this is when the EBS volumes associated with the AMI are restored from snapshots. I've then added import blocks to import each of these using the EBS volume data source to retrieve the volume IDs. \r\n\r\nThe code I've provided won't work currently for the case where var.restore_from_backup_enabled == false, since the EBS volume data sources won't be created, but I can get around that when for_each for the imports is released. I think the logic should work for the case where var.restore_from_backup_enabled == true though, but when I try it, Terraform complains that the IDs for the imports are not known at plan time:\r\n\r\n```\r\n\u2502 Error: Invalid import id argument\r\n\u2502 \r\n\u2502   on ebs.tf line 34, in import:\r\n\u2502   34:   id = data.aws_ebs_volume.restored_volumes[\"gocd-server\"].id\r\n\u2502 \r\n\u2502 The import block \"id\" argument depends on resource attributes that cannot\r\n\u2502 be determined until apply, so Terraform cannot plan to import this\r\n\u2502 resource.\r\n\u2575\r\n\u2577\r\n\u2502 Error: Invalid import id argument\r\n\u2502 \r\n\u2502   on ebs.tf line 38, in import:\r\n\u2502   38:   id = data.aws_ebs_volume.restored_volumes[\"gocd-artifacts\"].id\r\n\u2502 \r\n\u2502 The import block \"id\" argument depends on resource attributes that cannot\r\n\u2502 be determined until apply, so Terraform cannot plan to import this\r\n\u2502 resource.\r\n```\r\n\r\n\r\n\r\n\n\n### Proposal\n\nIt should be possible for import blocks to use values only computed at apply time in their inputs, similar to how many resources do it. The plan output would show this:\r\n```\r\nid = (known after apply)\r\n```\r\n\r\nAnd then the import would be performed at apply time, after the value is computed.\r\n\r\nWithout this feature, import blocks are of limited usefulness, since you have to know the IDs of what you're importing ahead of running Terraform. This is an unfortunate limitation when Terraform is quite capable of finding these IDs for you from data sources.\n\n### References\n\n_No response_","comments":["Hi @Rwwn,\r\n\r\nThanks for filing the issue. You cannot import a resource if the import data is unknown, because Terraform needs to have the imported first resource in order to complete the plan. If what you are looking for is a method for handling multiple phases of apply to deal with unknown values, we are tracking the general problem space under #30937.\r\n\r\nYour attempt with the data source to lookup the import data is the right approach, but from what I can see here it's only prevented by the `depends_on = [aws_instance.gocd_server]`, which is why the id is unknown (if there's any change planned to `aws_instance.gocd_server`). If the restored volumes already exist to be imported, there should be no reason for the explicit dependency to prevent the data source from being read. Does your configuration actually require the `depends_on` for some other reason?\r\n","Hi @jbardin, thanks for the response. What I'm trying to achieve relies on the depends_on being there, because the volumes are only created during the Terraform run - specifically, the creation of the EC2 from the AMI is what creates the EBS volumes I want to import. The AMI has several block devices associated with it, which result in volumes being created from snapshots when an instance is created using the AMI - but these won't be present in the state. \r\n\r\nSo in short, all I have at the start of the plan is the AMI and the snapshots; the volumes get created during apply, and I need to delay the data source and import blocks from running until the volumes are created, hence the depends_on. Sorry if that wasn't clear, I understand it's a bit convoluted. I might be barking up the wrong tree with this solution, so I'm happy to hear any other workarounds as well.\r\n\r\nOn reflection, I suppose the problem with my solution is similar to the one with using computed values for for_each and count given in the issue you linked at https:\/\/github.com\/hashicorp\/terraform\/issues\/30937 - if the volume ID for the import isn't known until apply, what is Terraform supposed to do when planning the aws_ebs_volume resources? It makes the plan conditional on the outcome of the apply, so I see the problem. If this was allowed, you'd have situations where the plan could not predict what Terraform would do to the EBS volumes on the apply, which is probably not desired. I may have to resort to importing these volumes manually after the apply after all, which is a bit of a shame.\r\n\r\nThe comment here https:\/\/github.com\/hashicorp\/terraform\/issues\/30937#issuecomment-1154173481 describing lazy applies sounds like it would solve my use case. Whether allowing this is a good idea or not is another question.","Thanks @Rwwn, that's helpful information. The fact that this is attempting to import something which is implicitly created during the same run is not something the current `import` process can handle. This is also close to other issues providers want to work on where an existing resource could be \"adopted\" during creation, or be able to import or create depending on whether something exists. That would give the provider the tools to communicate to the user what the actions on the resource will be during apply (a provider could technically make this work now in the existing resource lifecycle, but it would not be clear in the plan what is happening, and we tend to discourage breaking the strict lifecycle of resources since it can lead to other side effects).","Just in case anyone else comes across this, I've got a workaround using Terragrunt [post hooks](https:\/\/terragrunt.gruntwork.io\/docs\/features\/hooks\/) to run Terraform output and import commands in. It works well, but of course it requires an external tool in the shape of Terragrunt. I couldn't find any way to do it in Terraform natively, outside of writing a script to do multiple Terraform invocations.","@jbardin I ran into a similar issue with the vpc_route_table's default local route that is created. Do you know of any way in tf 1.7 to be able to manage these routes within the initial plan\/apply?\r\n\r\nThe below code would throw the following error if doing it from a clean apply.\r\n\r\n```\r\n\u2502 Error: Invalid import id argument\r\n\u2502 \r\n\u2502   on vpc.tf line 224, in import:\r\n\u2502  224:   id       = \"${aws_route_table.public[each.key].id}_${local.vpc_cidr}\"\r\n\u2502 \r\n\u2502 The import block \"id\" argument depends on resource attributes that cannot be determined until apply, so Terraform cannot plan to import this resource.\r\n```\r\n\r\nSnippet of code:\r\n```tf\r\n################################################################################\r\n# Publi\u0441 Subnets\r\n################################################################################\r\n\r\nresource \"aws_subnet\" \"public\" {\r\n  #checkov:skip=CKV_AWS_130:Public subnet needs public ips\r\n  for_each = toset(local.azs)\r\n\r\n  availability_zone       = each.key\r\n  cidr_block              = local.public_subnet_cidr[index(local.azs, each.key)]\r\n  map_public_ip_on_launch = true\r\n  vpc_id                  = aws_vpc.this.id\r\n\r\n  tags = merge(\r\n    { \"Name\" = \"${local.name}-public-${each.key}\" },\r\n    local.tags,\r\n    local.public_subnet_tags\r\n  )\r\n}\r\n\r\n# Public Route Table\r\nresource \"aws_route_table\" \"public\" {\r\n  for_each = toset(local.azs)\r\n\r\n  vpc_id = aws_vpc.this.id\r\n\r\n  tags = merge(\r\n    { \"Name\" = \"${local.name}-public-rt-${each.key}\" },\r\n    local.tags,\r\n  )\r\n}\r\n\r\nimport {\r\n  for_each = toset(local.azs)\r\n  to       = aws_route.public_to_firewall[each.key]\r\n  id       = \"${aws_route_table.public[each.key].id}_${local.vpc_cidr}\"\r\n}\r\n\r\n\r\nresource \"aws_route\" \"public_to_firewall\" {\r\n  for_each = toset(local.azs)\r\n\r\n  route_table_id         = aws_route_table.public[each.key].id\r\n  destination_cidr_block = local.vpc_cidr\r\n  vpc_endpoint_id        = tomap(local.az_to_endpoint)[each.key]\r\n\r\n  timeouts {\r\n    create = \"5m\"\r\n  }\r\n}\r\n\r\n```","@kclinden,\r\n\r\nNo, there's no way to do this with a single operation right now. Import must happen during the planning phase, but the resources you want to import don't exist until after apply is complete. ","> @kclinden,\r\n> \r\n> No, there's no way to do this with a single operation right now. Import must happen during the planning phase, but the resources you want to import don't exist until after apply is complete.\r\n\r\nYea in that case i think my only option is to manage the routes as part of the route table. That won't throw the same error."],"labels":["enhancement","import","unknown-values"]},{"title":"Optional skipping of `import` blocks during Terraform test plan operations","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.0 on windows_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI have a terraform configuration that includes declarative import blocks. For this configuration, I have unit tests (tests with command = plan) to assert that plan time values are calculated correctly. I run these tests against a dummy environment since I don't care about creating actual resources. However, my tests always fail because the dummy environment does not contain the resources that should be imported:\r\n\r\nError: Cannot import non-existent remote object\r\nWhile attempting to import an existing object to \"azurerm_subnet.gateway\", the provider detected that no object exists with the given id. Only pre-existing objects can be imported; check that the id  is correct and that it is associated with the provider's configured region or endpoint, or use \"terraform apply\" to create a new remote object for this resource.\r\n\r\n### Attempted Solutions\r\n\r\nInclude the imports in the list of expected failures in my tests\r\n\r\n### Proposal\r\n\r\n```hcl\r\n# in a *.tftest.hcl file\r\nexpect_failures = [\r\n   import.azurerm_subnet.gateway # ignore this specific resource\r\n]\r\n\r\n# even better\r\nexpect_failures = [\r\n   import # or import.* =>  ignore all imports\r\n]\r\n```\r\n\r\n### References\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/issues\/33633 suggests adding a **fail_on_missing** flag to import blocks. This feature would solve my problem, it serves however a different use case. I would have to modify my import blocks (the actual terraform configuration), instead of the tests.\r\n","comments":["Hi @Taha-cmd, thanks for filing this request.\r\n\r\nI wonder if you could expand on your use case more. Are you aiming to test a module you've built, or some standalone configuration for a service? Why do you require `import` blocks to be present in your configuration? What exactly do you want to test about the `import` blocks?\r\n\r\nI don't think simply allowing `import` blocks to be referenced from `expect_failures` will achieve what you want it to. The `expect_failures` block only affects the output of the test, it doesn't affect the behaviour of the Terraform operation. If a condition referenced in `expect_failures` fails during a Terraform operation, then the overall operation will still fail. It's just the test framework acknowledges the reason for the failure, and marks the test as passing despite that. Expecting a failure from an import block would mean the overall operation would still fail at the same time it does now, just that the testing framework would consider that a successful test.\r\n\r\nIt doesn't seem right to me that you'd want to use `import` blocks in tests at all. The `import` block is generally a one-time thing, that should be used to onboard already existing infrastructure into your Terraform state on a single use-basis, as if you were onboarding an existing service onto Terraform and you didn't want to recreate anything. Once a resource has been imported, you can remove the `import` block and Terraform will continue to manage the resource even without the `import` block being present.\r\n\r\nYour use-case here, as I understand it, suggests that you have some configuration that requires someone to create infrastructure outside of Terraform, just so that it can then be imported and managed by Terraform later. In this situation, why can't you just create the resources in Terraform to begin with, and then nothing needs to be imported? Alternatively, If the configuration you want to test has already imported the resources in your main state, then you can just take the `import` blocks away, and the testing framework will create the resources that were imported for your main configuration when it executes anyway.\r\n\r\nAs a further alternative, if you really do require the `import` blocks in your configuration, you could create a setup step in your test file that creates the resources that are to be imported before your main configuration executes.","Hi @liamcervante, thanks for the detailed response. Let me elaborate on our use case a bit more.\r\n\r\n* Our infrastructure was initially created manually through clickops and now we want to use Terraform to retrospectively automate it. Recreating the infrastructure with Terraform is not possible, because that would cause downtime for our customers (hence the required imports).\r\n\r\n* We have a very strict backend configuration that doesn't allow access to any \"human\" user. It is protected by firewall rules and RBAC permissions. Only the CD pipeline is allowed to interact with the state. (Hence config-driven imports instead of cli commands)\r\n\r\n* The terraform configuration I mentioned in the original post contains these imports. The same configuration also contains dynamically calculated values and string interpolations that are known at plan time. We want to write tests to ensure that all calculations are done correctly. This is why we use tests with **command = plan** and run assertions against the plan.\r\n\r\n* Since our tests are \"unit tests\" only and they only operate on the plan, we point them to a dummy environment that does not contain any infrastructure. However, since the configuration contains **import blocks**, terraform will try to plan the **imports**, which will fail, because the resources don't exist in the real world. I am not trying to test the **imports**. I want to ignore the **imports** in my tests. This is where I think that putting imports in the **expect_failures** section would solve my problem\r\n\r\n* I know that imprting resources is a one time thing and we can delete the import blocks later on. But we must use config-driven imports (see bullet point 2) in the CD pipeline. However, the configuration won't reach the CD pipeline if it fails in the CI pipeline, which runs the tests. This is basically a bootstrapping problem for brownfield situations. To solve it, I would have to disable the tests in the CI pipeline before deploying a terraform configuration for the first time and then enable them again afterwards. This makes the \"onboarding\" experience very tedious and error prone","> As a further alternative, if you really do require the import blocks in your configuration, you could create a setup step in your test file that creates the resources that are to be imported before your main configuration executes.\r\n\r\nThis not feasible for us, because the resources in question are considered heavyweight and will require 45-60 mins to create. We can't afford running this setup just to assert that plan-time values are calculated correctly.","Hi @Taha-cmd, thanks for the additional context.\r\n\r\nI think adding `import` blocks into the `expect_failures` block isn't the right thing for us to do at a language level. The behaviour of the `expect_failures` block doesn't currently modify the execution of Terraform in any way, and I think your use-case would require that. Skipping parts of the configuration is a different action to expecting parts of the configuration to fail, and treating that as a success. We'll have to think more about the right way to do this so I'll leave this open and get some eyes on it from our product team.\r\n\r\nIn the meantime, have you tried using the `target` command in the run blocks? This would allow you to only plan the resources you want to test and potentially skip any resources that require importing. It might be that your import targets are referenced by the resources you want to test, which does mean this might not work.\r\n\r\nAnother upcoming alternative, we also plan to add support for mocking resources and data sources in the testing framework soon (hopefully 1.7), this would allow you to define a fake resource for your import targets within the test.","@liamcervante \r\n> I think adding `import` blocks into the `expect_failures` block isn't the right thing for us to do at a language level. The behaviour of the `expect_failures` block doesn't currently modify the execution of Terraform in any way, and I think your use-case would require that. Skipping parts of the configuration is a different action to expecting parts of the configuration to fail, and treating that as a success. We'll have to think more about the right way to do this so I'll leave this open and get some eyes on it from our product team.\r\n\r\nSo, we also have an interest in allowing imports to ignore missing resources. Our company has implemented generic terraform modules for our cloud deployment that allows us to provide a yaml definition of our azure deployments at runtime. Terraform consumes this yaml and creates the infrastructure for us, injecting certain automated configuration. Because the declaration occurs in our yaml definition and not strictly the terraform, importing is not a one and done. The same terraform code could be used to create any number of deployments, some of which will have existing resources we would want to import into our state as part of migration. Then our DevOps CI pipelines can easily spit out json\/yaml to provision or manage resources without having to strictly write terraform.\r\n\r\nThe desired behavior would be to \"import if exists, create if it doesn't\" which appears to be a commonly requested feature from what I've seen. Honestly, I think this could be abstracted to a more general error handling meta-argument. Something along the lines of...\r\n\r\n``` \r\ndata \"azurerm_linux_virtual_machine\" \"frontend_host\" {\r\n   ...\r\n   \r\n   ignore_errors [\r\n      \"missing_resource\", ...\r\n   ]\r\n}\r\n\r\nimport {\r\n   to = azurerm_linux_virtual_machine.frontend_host\r\n   id = ...\r\n\r\n   ignore_errors [\r\n      \"missing_resource\", ...\r\n   ]\r\n}\r\n\r\nresource\"azurerm_linux_virtual_machine\" \"frontend_host\" {\r\n   ...\r\n   \r\n   ignore_errors [\r\n      \"*\", ...\r\n   ]\r\n}\r\n```\r\n\r\nWhere a 'caught' error by ignore_error meta-argument would result in a null datasource\/resource instead of failed terraform operations. Maybe couple with a warning that the operation failed so the possible problem is still visible. An implementation like this would create a LOT of functionality. A couple use cases that occur to me off the top of my head\r\n- Import if exists, create if doesn't. Supports terraform migration or local backends where you may have a previous run and not have access to the state file.\r\n- Implementing control flow logic to recover from failures or handle missing datasources\r\n     **ex:** _\"Grab data about my cloud loadbalancer but, if it doesn't exist (datasource triggered a missing resource error), provision out a baremetal loadbalancer deployment.\"_\r\n\r\nI'm fully aware that it introduces some complexity into troubleshooting but I'd argue that is complexity\/risk the developer should be given a choice to adopt in exchange for more flexibility.","@AMEvers, the possibility of \"import if exists, create if it doesn't\" is separate from the `terraform test` functionality being discussed here. That is currently possible given the availability of cooperating data sources, and may become easier with future enhancements. See also #33465 and #26558"],"labels":["enhancement","terraform test"]},{"title":"Starting a local variable with digits throws an incorrect error message","body":"### Terraform Version\n\n```shell\n% terraform version\r\nTerraform v1.6.2\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nlocals {\r\n  2023_test = \"foobar\"\r\n}\r\n```\n\n### Debug Output\n\n(Ignoring the request to use a Gist instead of pasting here because the testcase is so trivial that the debug output isn't actually that long.)\r\n\r\n```\r\n% TF_LOG=trace terraform plan\r\n2023-10-26T20:25:19.051-0500 [INFO]  Terraform version: 1.6.2\r\n2023-10-26T20:25:19.051-0500 [DEBUG] using github.com\/hashicorp\/go-tfe v1.36.0\r\n2023-10-26T20:25:19.051-0500 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2023-10-26T20:25:19.051-0500 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2023-10-26T20:25:19.051-0500 [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2023-10-26T20:25:19.051-0500 [INFO]  Go runtime version: go1.21.1\r\n2023-10-26T20:25:19.051-0500 [INFO]  CLI args: []string{\"terraform\", \"plan\"}\r\n2023-10-26T20:25:19.051-0500 [TRACE] Stdout is a terminal of width 80\r\n2023-10-26T20:25:19.051-0500 [TRACE] Stderr is a terminal of width 80\r\n2023-10-26T20:25:19.051-0500 [TRACE] Stdin is a terminal\r\n2023-10-26T20:25:19.051-0500 [DEBUG] Attempting to open CLI config file: \/var\/home\/alex\/.terraformrc\r\n2023-10-26T20:25:19.051-0500 [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/var\/home\/alex\/.terraform.d\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/var\/home\/alex\/.local\/share\/terraform\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/var\/home\/alex\/.local\/share\/flatpak\/exports\/share\/terraform\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/var\/lib\/flatpak\/exports\/share\/terraform\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2023-10-26T20:25:19.051-0500 [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2023-10-26T20:25:19.052-0500 [INFO]  CLI command args: []string{\"plan\"}\r\n\u2577\r\n\u2502 Error: Argument or block definition required\r\n\u2502 \r\n\u2502   on test.tf line 2, in locals:\r\n\u2502    2:   2023_test = \"foobar\"\r\n\u2502 \r\n\u2502 An argument or block definition is required here.\r\n\u2575\r\n```\n\n### Expected Behavior\n\nI should have seen something similar to this error:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Invalid resource name\r\n\u2502 \r\n\u2502   on dns.tf line 209, in resource \"aws_route53_record\" \"2023-ephemeral-delegation-a\":\r\n\u2502  209: resource \"aws_route53_record\" \"2023-ephemeral-delegation-a\" {\r\n\u2502 \r\n\u2502 A name must start with a letter or underscore and may contain only letters, digits, underscores, and dashes.\r\n\u2575\r\n```\n\n### Actual Behavior\n\nI saw an error about an argument or block definition being required, even though AFAICT that was provided.\n\n### Steps to Reproduce\n\n1. Create `test.tf` with the repro file contents\r\n2. `terraform plan`\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @strugee,\r\n\r\nIn this situation Terraform is encountering a number and noticing that a number cannot possibly be the start of \"an argument or block definition\". Unfortunately with syntax errors Terraform often has to do the best it can with partial information, because before there's even a complete parse tree any assumptions Terraform is making about context are only guesses.\r\n\r\nWith that said, given that a number is never valid in this particular context I imagine it would be possible to add a special case to the parser for when it encounters a number here. I don't think it would be appropriate to use an error exactly like the other one you shared about resource names -- Terraform is _far more certain_ that the string there is intended to be a resource name, because main parsing was already complete -- but we could perhaps compromise by adding an extra sentence to the detail part of the error message:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Argument or block definition required\r\n\u2502 \r\n\u2502   on test.tf line 2, in locals:\r\n\u2502    2:   2023_test = \"foobar\"\r\n\u2502 \r\n\u2502 An argument or block definition is required here. Argument names and\r\n\u2502 block types cannot begin with a digit.\r\n\u2575\r\n```\r\n\r\nThanks for reporting this! The code that's handling this actually lives upstream in the HCL repository, so I'm going to label this as \"upstream\" but I think we should still use this issue to represent the problem since it's most likely to be the Terraform team working on this change.\r\n"],"labels":["enhancement","upstream","config"]},{"title":"Terraform 1.6.x - SignatureDoesNotMatch Error - S3 Backend (GetObject)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.0\r\n\r\non darwin_arm64\r\n\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.21.0\r\n\r\n+ provider registry.terraform.io\/hashicorp\/external v2.3.1\r\n\r\n+ provider registry.terraform.io\/hashicorp\/time v0.9.1\r\n\r\n+ provider registry.terraform.io\/venafi\/venafi v0.16.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```\r\nterraform {\r\n  backend \"s3\" {\r\n    skip_region_validation = true\r\n    encrypt                = true\r\n    dynamodb_table         = \"tfstate\"\r\n    bucket                 = \"xxx-eu-west-1-terraform\"\r\n    region                 = \"eu-west-1\"\r\n    key                    = \"terraform.tfstate\"\r\n    profile                = \"saml\"\r\n    #    access_key = \"xxx\"\r\n    #    secret_key = \"xxx\"\r\n    #    token = \"xxx\"\r\n\r\n    #    assume_role = {\r\n    #      role_arn = \"arn:aws:iam::[REDACTED]:role\/jenkins\"\r\n    #      external_id = \"opbk\"\r\n    #    }\r\n  }\r\n}\r\n\r\nterraform {\r\n  required_version = \"~> 1.6.0\"\r\n\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.0\"\r\n    }\r\n    venafi = {\r\n      source  = \"venafi\/venafi\"\r\n      version = \"0.16.0\"\r\n    }\r\n    external = {\r\n      source  = \"hashicorp\/external\"\r\n      version = \"~> 2.2\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\nInitializing the backend...\r\n2023-10-19T13:56:11.140+0100 [DEBUG] backend-s3.aws-base: Resolving credentials provider: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.140+0100 [DEBUG] backend-s3.aws-base: Using profile: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate tf_aws.profile=saml tf_aws.profile.source=provider\r\n2023-10-19T13:56:11.140+0100 [DEBUG] backend-s3.aws-base: Loading profile: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate tf_aws.profile=saml\r\n2023-10-19T13:56:11.140+0100 [DEBUG] backend-s3.aws-base: Setting profile: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate tf_aws.profile=saml tf_aws.profile.source=provider\r\n2023-10-19T13:56:11.141+0100 [DEBUG] backend-s3.aws-base: Loading configuration: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.143+0100 [DEBUG] backend-s3.aws-base: Retrieving credentials: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.143+0100 [INFO]  backend-s3.aws-base: Retrieved credentials: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate tf_aws.credentials_source=\"SharedConfigCredentials: \/Users\/[REDACTED]\/.aws\/credentials\"\r\n2023-10-19T13:56:11.143+0100 [DEBUG] backend-s3.aws-base: Loading configuration: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.146+0100 [DEBUG] backend-s3.aws-base: Retrieving caller identity from STS: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.147+0100 [DEBUG] backend-s3.aws-base: HTTP Request Sent: aws.operation=GetCallerIdentity aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=STS tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.method=POST http.url=https:\/\/sts.eu-west-1.amazonaws.com\/ http.request_content_length=43 http.request.header.x_amz_security_token=\"*****\" http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.header.x_amz_date=20231019T125611Z http.request.header.content_type=application\/x-www-form-urlencoded http.request.header.amz_sdk_invocation_id=4028fcfd-14f0-4f09-8055-331c1785a2b2 net.peer.name=sts.eu-west-1.amazonaws.com http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/sts#1.21.5\" http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/sts\/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=*****\"\r\n  http.request.body=\r\n  | Action=GetCallerIdentity&Version=2011-06-15\r\n  \r\n2023-10-19T13:56:11.607+0100 [DEBUG] backend-s3.aws-base: HTTP Response Received: aws.operation=GetCallerIdentity aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=STS tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.response.header.content_type=text\/xml http.response.header.date=\"Thu, 19 Oct 2023 12:56:11 GMT\" http.response.header.proxy_connection=Keep-Alive http.response.header.connection=Keep-Alive\r\n  http.response.body=\r\n  | <GetCallerIdentityResponse xmlns=\"https:\/\/sts.amazonaws.com\/doc\/2011-06-15\/\">\r\n  |   <GetCallerIdentityResult>\r\n  |     <Arn>arn:aws:sts::[REDACTED]:assumed-role\/[REDACTED]\/[REDACTED]<\/Arn>\r\n  |     <UserId>AROA*************PSOA:[REDACTED]<\/UserId>\r\n  |     <Account>[REDACTED]<\/Account>\r\n  |   <\/GetCallerIdentityResult>\r\n  |   <ResponseMetadata>\r\n  |     <RequestId>266e2059-c950-4e0c-b46b-b55ade6bef6f<\/RequestId>\r\n  |   <\/ResponseMetadata>\r\n  | <\/GetCallerIdentityResponse>\r\n   http.duration=460 http.response.header.x_amzn_requestid=266e2059-c950-4e0c-b46b-b55ade6bef6f http.response.header.cache_control=proxy-revalidate http.status_code=200 http.response_content_length=439\r\n2023-10-19T13:56:11.608+0100 [INFO]  backend-s3.aws-base: Retrieved caller identity from STS: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.608+0100 [DEBUG] backend-s3.aws-base: Retrieving caller identity from STS: tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:56:11.608+0100 [DEBUG] backend-s3.aws-base: HTTP Request Sent: aws.operation=GetCallerIdentity aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=STS tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.method=POST net.peer.name=sts.eu-west-1.amazonaws.com http.request.header.content_type=application\/x-www-form-urlencoded http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.header.x_amz_date=20231019T125611Z\r\n  http.request.body=\r\n  | Action=GetCallerIdentity&Version=2011-06-15\r\n   http.url=https:\/\/sts.eu-west-1.amazonaws.com\/ http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/sts#1.21.5\" http.request_content_length=43 http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/sts\/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date;x-amz-security-token, Signature=*****\" http.request.header.x_amz_security_token=\"*****\" http.request.header.amz_sdk_invocation_id=97a6a188-0dec-4d50-9d10-319b6127d938\r\n2023-10-19T13:56:11.671+0100 [DEBUG] backend-s3.aws-base: HTTP Response Received: aws.operation=GetCallerIdentity aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=STS tf_backend.operation=Configure tf_backend.req_id=e36c1bc5-a9e8-a929-6236-ec2628af7de8 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.response.header.proxy_connection=Keep-Alive http.response.header.connection=Keep-Alive http.response.header.x_amzn_requestid=5095e69f-2def-4a86-b0aa-622643fecbe1 http.response.header.date=\"Thu, 19 Oct 2023 12:56:11 GMT\"\r\n  http.response.body=\r\n  | <GetCallerIdentityResponse xmlns=\"https:\/\/sts.amazonaws.com\/doc\/2011-06-15\/\">\r\n  |   <GetCallerIdentityResult>\r\n  |     <Arn>arn:aws:sts::[REDACTED]:assumed-role\/[REDACTED]\/[REDACTED]<\/Arn>\r\n  |     <UserId>AROA*************PSOA:[REDACTED]<\/UserId>\r\n  |     <Account>[REDACTED]<\/Account>\r\n  |   <\/GetCallerIdentityResult>\r\n  |   <ResponseMetadata>\r\n  |     <RequestId>5095e69f-2def-4a86-b0aa-622643fecbe1<\/RequestId>\r\n  |   <\/ResponseMetadata>\r\n  | <\/GetCallerIdentityResponse>\r\n   http.duration=52 http.status_code=200 http.response_content_length=439 http.response.header.cache_control=proxy-revalidate http.response.header.content_type=text\/xml\r\n...\r\n2023-10-19T13:58:04.762+0100 [INFO]  backend\/local: starting Apply operation\r\n2023-10-19T13:58:04.767+0100 [DEBUG] backend-s3: HTTP Request Sent: aws.operation=ListObjectsV2 aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Workspaces tf_backend.req_id=efd44fde-c0a5-a6f7-8d86-fac47232aacc tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.workspace-prefix=env:\/ http.request.header.x_amz_content_sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 http.method=GET http.url=\"https:\/\/ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com\/?list-type=2&max-keys=1000&prefix=env%3A%2F\" net.peer.name=ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/s3#1.38.5\" http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/s3\/aws4_request, SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;x-amz-content-sha256;x-amz-date;x-amz-security-token, Signature=*****\" http.request.header.accept_encoding=identity http.request.header.x_amz_security_token=\"*****\" http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.header.x_amz_date=20231019T125804Z http.request.header.amz_sdk_invocation_id=cf1a5ee3-e8ea-4faa-8eaf-ba9547215401 http.request.body=\"\"\r\n2023-10-19T13:58:05.132+0100 [DEBUG] backend-s3: HTTP Response Received: aws.operation=ListObjectsV2 aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Workspaces tf_backend.req_id=efd44fde-c0a5-a6f7-8d86-fac47232aacc tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.workspace-prefix=env:\/ http.duration=364 http.response.header.x_amz_bucket_region=eu-west-1 http.status_code=200 http.response.header.x_amz_id_2=sZjjoTx6Z2iAMbvhQyIkFxjA9NQkCatocObVZ4ZNi5Cuv\/OCC25paf5QA4QVz3Qu6lnTs1jY9fo= http.response.header.age=0 http.response.header.x_amz_request_id=XKPW9DHXZDS3MSFZ http.response.header.cache_control=proxy-revalidate http.response.header.server=AmazonS3 http.response.header.proxy_connection=Keep-Alive http.response.header.content_type=application\/xml http.response.header.date=\"Thu, 19 Oct 2023 12:58:06 GMT\" http.response.header.connection=Keep-Alive\r\n  http.response.body=\r\n  | <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n  | <ListBucketResult xmlns=\"http:\/\/s3.amazonaws.com\/doc\/2006-03-01\/\"><Name>ob-[REDACTED]-eu-west-1-terraform<\/Name><Prefix>env:\/<\/Prefix><KeyCount>0<\/KeyCount><MaxKeys>1000<\/MaxKeys><IsTruncated>false<\/IsTruncated><\/ListBucketResult>\r\n  \r\n2023-10-19T13:58:05.134+0100 [INFO]  backend-s3: Locking remote state: tf_backend.lock.id=55746556-14c1-c611-0b56-12e210a1d7b3 tf_backend.lock.info=\"\" tf_backend.lock.operation=OperationTypeApply tf_backend.lock.path=ob-[REDACTED]-eu-west-1-terraform\/terraform.tfstate tf_backend.lock.version=1.6.0 tf_backend.lock.who=[REDACTED] tf_backend.operation=Lock tf_backend.req_id=da082e9c-d5a6-aa83-6101-0cb213335053 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:58:05.137+0100 [DEBUG] backend-s3: HTTP Request Sent: aws.operation=PutItem aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=DynamoDB tf_backend.lock.id=55746556-14c1-c611-0b56-12e210a1d7b3 tf_backend.lock.info=\"\" tf_backend.lock.operation=OperationTypeApply tf_backend.lock.path=ob-[REDACTED]-eu-west-1-terraform\/terraform.tfstate tf_backend.lock.version=1.6.0 tf_backend.lock.who=[REDACTED] tf_backend.operation=Lock tf_backend.req_id=da082e9c-d5a6-aa83-6101-0cb213335053 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.request.header.x_amz_date=20231019T125805Z http.request_content_length=473 http.request.header.accept_encoding=identity http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/dynamodb#1.21.1\" http.request.header.x_amz_security_token=\"*****\" http.request.header.amz_sdk_invocation_id=b6443168-801f-4e75-8ec4-b7ad8524fef2 http.method=POST http.url=https:\/\/dynamodb.eu-west-1.amazonaws.com\/\r\n  http.request.body=\r\n  | {\"ConditionExpression\":\"attribute_not_exists(LockID)\",\"Item\":{\"LockID\":{\"S\":\"ob-[REDACTED]-eu-west-1-terraform\/terraform.tfstate\"},\"Info\":{\"S\":\"{\\\"ID\\\":\\\"55746556-14c1-c611-0b56-12e210a1d7b3\\\",\\\"Operation\\\":\\\"OperationTypeApply\\\",\\\"Info\\\":\\\"\\\",\\\"Who\\\":\\\"[REDACTED]\\\",\\\"Version\\\":\\\"1.6.0\\\",\\\"Created\\\":\\\"2023-10-19T12:58:05.133785Z\\\",\\\"Path\\\":\\\"ob-[REDACTED]-eu-west-1-terraform\/terraform.tfstate\\\"}\"}},\"TableName\":\"tfstate\"}\r\n   net.peer.name=dynamodb.eu-west-1.amazonaws.com http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/dynamodb\/aws4_request, SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;content-length;content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=*****\" http.request.header.content_type=application\/x-amz-json-1.0 http.request.header.x_amz_target=DynamoDB_20120810.PutItem http.request.header.amz_sdk_request=\"attempt=1; max=5\"\r\nAcquiring state lock. This may take a few moments...\r\n2023-10-19T13:58:05.608+0100 [DEBUG] backend-s3: HTTP Response Received: aws.operation=PutItem aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=DynamoDB tf_backend.lock.id=55746556-14c1-c611-0b56-12e210a1d7b3 tf_backend.lock.info=\"\" tf_backend.lock.operation=OperationTypeApply tf_backend.lock.path=ob-[REDACTED]-eu-west-1-terraform\/terraform.tfstate tf_backend.lock.version=1.6.0 tf_backend.lock.who=[REDACTED] tf_backend.operation=Lock tf_backend.req_id=da082e9c-d5a6-aa83-6101-0cb213335053 tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.duration=470 http.status_code=200 http.response.header.connection=Keep-Alive http.response.header.server=Server http.response.header.content_type=application\/x-amz-json-1.0 http.response.header.x_amzn_requestid=AKE7BV97AC409M5LCIRHUPS0I3VV4KQNSO5AEMVJF66Q9ASUAAJG\r\n  http.response.body=\r\n  | {}\r\n   http.response_content_length=2 http.response.header.proxy_connection=Keep-Alive http.response.header.x_amz_crc32=2745614147 http.response.header.date=\"Thu, 19 Oct 2023 12:58:05 GMT\" http.response.header.cache_control=proxy-revalidate\r\n2023-10-19T13:58:05.608+0100 [INFO]  backend-s3: Downloading remote state: tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n2023-10-19T13:58:05.608+0100 [DEBUG] backend-s3: HTTP Request Sent: aws.operation=HeadObject aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.request.header.amz_sdk_invocation_id=d2ab369f-c7d0-4820-b839-723030056884 http.request.body=\"\" net.peer.name=ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com http.request.header.x_amz_security_token=\"*****\" http.request.header.x_amz_content_sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 http.request.header.accept_encoding=identity http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.header.x_amz_date=20231019T125805Z http.method=HEAD http.url=https:\/\/ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com\/terraform.tfstate http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/s3#1.38.5\" http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/s3\/aws4_request, SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;x-amz-content-sha256;x-amz-date;x-amz-security-token, Signature=*****\"\r\n2023-10-19T13:58:05.675+0100 [DEBUG] backend-s3: HTTP Response Received: aws.operation=HeadObject aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.response.header.x_amz_request_id=XKPGJW5QCDDJA0TG http.response.header.content_type=application\/json http.status_code=200 http.response.header.date=\"Thu, 19 Oct 2023 12:58:06 GMT\" http.response.header.accept_ranges=bytes http.response.header.last_modified=\"Wed, 18 Oct 2023 13:43:56 GMT\" http.response.header.x_amz_server_side_encryption=AES256 http.response.header.proxy_connection=Keep-Alive http.response.header.server=AmazonS3 http.response.body=\"\" http.duration=66 http.response_content_length=78153 http.response.header.connection=Keep-Alive http.response.header.x_amz_id_2=\"v2SyhwwkLc8bqlz4U1dctQcY0qVKUL4VCeNfhKOuNw4+oVaBgoJoSQfYHk05qUYFxAxTtDq3IJI=\" http.response.header.x_amz_version_id=WUYrxTVdg0eQFhSYVpZxp3BNVlhTxlAQ http.response.header.etag=\"\\\"e005e0206157a4ee4466d20f821bc318\\\"\" http.response.header.cache_control=proxy-revalidate\r\n2023-10-19T13:58:05.675+0100 [DEBUG] backend-s3: HTTP Request Sent: aws.operation=GetObject aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.request.body=\"\" net.peer.name=ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=ASIA************VBYM\/20231019\/eu-west-1\/s3\/aws4_request, SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;range;x-amz-content-sha256;x-amz-date;x-amz-security-token, Signature=*****\" http.request.header.x_amz_security_token=\"*****\" http.request.header.accept_encoding=identity http.url=https:\/\/ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com\/terraform.tfstate?x-id=GetObject http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.6.0 (+https:\/\/www.terraform.io) aws-sdk-go-v2\/1.21.0 os\/macos lang\/go#1.21.1 md\/GOOS#darwin md\/GOARCH#arm64 api\/s3#1.38.5 ft\/s3-transfer\" http.request.header.amz_sdk_request=\"attempt=1; max=5\" http.request.header.range=bytes=0-5242879 http.request.header.x_amz_content_sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 http.method=GET http.request.header.amz_sdk_invocation_id=b109984a-408a-44b9-a9e8-b49c46aa3385 http.request.header.x_amz_date=20231019T125805Z\r\n2023-10-19T13:58:05.766+0100 [DEBUG] backend-s3: HTTP Response Received: aws.operation=GetObject aws.region=eu-west-1 aws.sdk=aws-sdk-go-v2 aws.service=S3 tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate http.response.header.proxy_connection=Keep-Alive http.response.header.connection=Keep-Alive http.duration=60 http.response.header.date=\"Thu, 19 Oct 2023 12:58:05 GMT\" http.response.header.server=AmazonS3 http.response.header.cache_control=proxy-revalidate http.response.header.x_amz_request_id=XKPYFG87BPS57CX7 http.status_code=403 http.response.header.x_amz_id_2=\"U7iKvrG1tQoSygW7mehKxqLHT3yCsZgYuuaoAVmNMpSAz8b7WS41++NWCrdVYzwigSZYAio7ruY=\" http.response.header.age=0 http.response.header.content_type=application\/xml\r\n  http.response.body=\r\n  | <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n  | <Error><Code>SignatureDoesNotMatch<\/Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.<\/Message><AWSAccessKeyId>ASIA************VBYM<\/AWSAccessKeyId><StringToSign>AWS4-HMAC-SHA256\r\n  | 20231019T125805Z\r\n  | 20231019\/eu-west-1\/s3\/aws4_request\r\n  | e712eb376d5c8966e49476bdf5ed859ca5410507254bf9c107b1e63a6c25cc67<\/StringToSign><SignatureProvided>89073fd6d1cb5d4add1720b954f1c7bc1d87fcfa829dfedebc2a7a2317269c16<\/SignatureProvided><StringToSignBytes>41 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 0a 32 30 32 33 31 30 31 39 54 31 32 35 38 30 35 5a 0a 32 30 32 33 31 30 31 39 2f 65 75 2d 77 65 73 74 2d 31 2f 73 33 2f 61 77 73 34 5f 72 65 71 75 65 73 74 0a 65 37 31 32 65 62 33 37 36 64 35 63 38 39 36 36 65 34 39 34 37 36 62 64 66 35 65 64 38 35 39 63 61 35 34 31 30 35 30 37 32 35 34 62 66 39 63 31 30 37 62 31 65 36 33 61 36 63 32 35 63 63 36 37<\/StringToSignBytes><CanonicalRequest>GET\r\n  | \/terraform.tfstate\r\n  | x-id=GetObject\r\n  | accept-encoding:identity\r\n  | amz-sdk-invocation-id:b109984a-408a-44b9-a9e8-b49c46aa3385\r\n  | amz-sdk-request:attempt=1; max=5\r\n  | host:ob-[REDACTED]-eu-west-1-terraform.s3.eu-west-1.amazonaws.com\r\n  | range:\r\n  | x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\n  | x-amz-date:20231019T125805Z\r\n  | x-amz-security-token:IQoJb3JpZ2luX2VjEKX\/\/\/\/\/\/\/\/\/\/wEaCWV1LXdlc3QtMSJIMEYCIQDVAJjWItEqp1aJipucGZkqndGkHvMKEF6z6ngbe6ddoQIhAK9F7LrbAc09NppzxLvYhc736xkghnBXE9ul+i4JUabKKvACCL7\/\/\/\/\/\/\/\/\/\/wEQAxoMNjU4MDg0ODg5NjExIgxOEurbb7lKF2jiYXEqxAKoDC\/tApUtseKWZymLr8PHDKmpcJoyZMqYp0mGhRoNBC7rs2iLgMKGcBOMEnFNrLTFvulE\/KtTSkPUle7JoIaqBZ155UJlB0IQS6EoKucwte0Tnft6uQXRmK2X7seQC\/oya7fNJgB\/PLfxqRIgnVqomOmDrX3JQOL\/l0aiCr+doCafMh73JZqyVeEakMqEIZq1naugVnFD8iIZh4p519XJrU7DIr50C4fq8VpPsjBkDE9FVmg4XIvGt3\/jw\/CI5MocT9I+i55GQSIAO2QbI3mHgNcDY2NCuj6j0qPngw2Cl9LZGr8VszH25g9Lf+bujiKFIWsT+Lcdd6Qy2KVaN4HJV1kHnWhGC\/nMrGvSXpBg3AJFmojiuFi+PXOnt5wTXxOpY1hgj1uvuq+5uIszY676dGAbSNIiOxOTMIy4Tqt4tVTD4BYwv8fEqQY6mgF5eLs7tNbWs++1LMqex7wzCrq1NSk8sMvogGk1cZdlV7mCzV2eRGLHfNYVY32xTJopr3bHFFh6f3w9k8kANDwW29y0qXsPAMiqoAJIK7FIumIzOBOUPECeRtwaOv4ILIj6zoA8wEvhSOmryOxkBZb9eH9mkR6DkOzBDg1xdGS05FyH\/mTixrsdmvcBNxDSlHCVpxYScjv5I+hB\r\n  | \r\n  | accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;range;x-amz-content-sha256;x-amz-date;x-amz-security-token\r\n  | e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855<\/CanonicalRequest><CanonicalRequestBytes>47 45 54 0a 2f 70 68 69 6c 2d 6e 67 69 6e 78 2d 61 63 63 6f 75 6e 74 2e 74 66 73 74 61 74 65 0a 78 2d 69 64 3d 47 65 74 4f 62 6a 65 63 74 0a 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 3a 69 64 65 6e 74 69 74 79 0a 61 6d 7a 2d 73 64 6b 2d 69 6e 76 6f 63 61 74 69 6f 6e 2d 69 64 3a 62 31 30 39 39 38 34 61 2d 34 30 38 61 2d 34 34 62 39 2d 61 39 65 38 2d 62 34 39 63 34 36 61 61 33 33 38 35 0a 61 6d 7a 2d 73 64 6b 2d 72 65 71 75 65 73 74 3a 61 74 74 65 6d 70 74 3d 31 3b 20 6d 61 78 3d 35 0a 68 6f 73 74 3a 6f 62 2d 36 35 38 30 38 34 38 38 39 36 31 31 2d 65 75 2d 77 65 73 74 2d 31 2d 74 65 72 72 61 66 6f 72 6d 2e 73 33 2e 65 75 2d 77 65 73 74 2d 31 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 0a 72 61 6e 67 65 3a 0a 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3a 65 33 62 30 63 34 34 32 39 38 66 63 31 63 31 34 39 61 66 62 66 34 63 38 39 39 36 66 62 39 32 34 32 37 61 65 34 31 65 34 36 34 39 62 39 33 34 63 61 34 39 35 39 39 31 62 37 38 35 32 62 38 35 35 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 32 30 32 33 31 30 31 39 54 31 32 35 38 30 35 5a 0a 78 2d 61 6d 7a 2d 73 65 63 75 72 69 74 79 2d 74 6f 6b 65 6e 3a 49 51 6f 4a 62 33 4a 70 5a 32 6c 75 58 32 56 6a 45 4b 58 2f 2f 2f 2f 2f 2f 2f 2f 2f 2f 77 45 61 43 57 56 31 4c 58 64 6c 63 33 51 74 4d 53 4a 49 4d 45 59 43 49 51 44 56 41 4a 6a 57 49 74 45 71 70 31 61 4a 69 70 75 63 47 5a 6b 71 6e 64 47 6b 48 76 4d 4b 45 46 36 7a 36 6e 67 62 65 36 64 64 6f 51 49 68 41 4b 39 46 37 4c 72 62 41 63 30 39 4e 70 70 7a 78 4c 76 59 68 63 37 33 36 78 6b 67 68 6e 42 58 45 39 75 6c 2b 69 34 4a 55 61 62 4b 4b 76 41 43 43 4c 37 2f 2f 2f 2f 2f 2f 2f 2f 2f 2f 77 45 51 41 78 6f 4d 4e 6a 55 34 4d 44 67 30 4f 44 67 35 4e 6a 45 78 49 67 78 4f 45 75 72 62 62 37 6c 4b 46 32 6a 69 59 58 45 71 78 41 4b 6f 44 43 2f 74 41 70 55 74 73 65 4b 57 5a 79 6d 4c 72 38 50 48 44 4b 6d 70 63 4a 6f 79 5a 4d 71 59 70 30 6d 47 68 52 6f 4e 42 43 37 72 73 32 69 4c 67 4d 4b 47 63 42 4f 4d 45 6e 46 4e 72 4c 54 46 76 75 6c 45 2f 4b 74 54 53 6b 50 55 6c 65 37 4a 6f 49 61 71 42 5a 31 35 35 55 4a 6c 42 30 49 51 53 36 45 6f 4b 75 63 77 74 65 30 54 6e 66 74 36 75 51 58 52 6d 4b 32 58 37 73 65 51 43 2f 6f 79 61 37 66 4e 4a 67 42 2f 50 4c 66 78 71 52 49 67 6e 56 71 6f 6d 4f 6d 44 72 58 33 4a 51 4f 4c 2f 6c 30 61 69 43 72 2b 64 6f 43 61 66 4d 68 37 33 4a 5a 71 79 56 65 45 61 6b 4d 71 45 49 5a 71 31 6e 61 75 67 56 6e 46 44 38 69 49 5a 68 34 70 35 31 39 58 4a 72 55 37 44 49 72 35 30 43 34 66 71 38 56 70 50 73 6a 42 6b 44 45 39 46 56 6d 67 34 58 49 76 47 74 33 2f 6a 77 2f 43 49 35 4d 6f 63 54 39 49 2b 69 35 35 47 51 53 49 41 4f 32 51 62 49 33 6d 48 67 4e 63 44 59 32 4e 43 75 6a 36 6a 30 71 50 6e 67 77 32 43 6c 39 4c 5a 47 72 38 56 73 7a 48 32 35 67 39 4c 66 2b 62 75 6a 69 4b 46 49 57 73 54 2b 4c 63 64 64 36 51 79 32 4b 56 61 4e 34 48 4a 56 31 6b 48 6e 57 68 47 43 2f 6e 4d 72 47 76 53 58 70 42 67 33 41 4a 46 6d 6f 6a 69 75 46 69 2b 50 58 4f 6e 74 35 77 54 58 78 4f 70 59 31 68 67 6a 31 75 76 75 71 2b 35 75 49 73 7a 59 36 37 36 64 47 41 62 53 4e 49 69 4f 78 4f 54 4d 49 79 34 54 71 74 34 74 56 54 44 34 42 59 77 76 38 66 45 71 51 59 36 6d 67 46 35 65 4c 73 37 74 4e 62 57 73 2b 2b 31 4c 4d 71 65 78 37 77 7a 43 72 71 31 4e 53 6b 38 73 4d 76 6f 67 47 6b 31 63 5a 64 6c 56 37 6d 43 7a 56 32 65 52 47 4c 48 66 4e 59 56 59 33 32 78 54 4a 6f 70 72 33 62 48 46 46 68 36 66 33 77 39 6b 38 6b 41 4e 44 77 57 32 39 79 30 71 58 73 50 41 4d 69 71 6f 41 4a 49 4b 37 46 49 75 6d 49 7a 4f 42 4f 55 50 45 43 65 52 74 77 61 4f 76 34 49 4c 49 6a 36 7a 6f 41 38 77 45 76 68 53 4f 6d 72 79 4f 78 6b 42 5a 62 39 65 48 39 6d 6b 52 36 44 6b 4f 7a 42 44 67 31 78 64 47 53 30 35 46 79 48 2f 6d 54 69 78 72 73 64 6d 76 63 42 4e 78 44 53 6c 48 43 56 70 78 59 53 63 6a 76 35 49 2b 68 42 0a 0a 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 3b 61 6d 7a 2d 73 64 6b 2d 69 6e 76 6f 63 61 74 69 6f 6e 2d 69 64 3b 61 6d 7a 2d 73 64 6b 2d 72 65 71 75 65 73 74 3b 68 6f 73 74 3b 72 61 6e 67 65 3b 78 2d 61 6d 7a 2d 63 6f 6e 74 65 6e 74 2d 73 68 61 32 35 36 3b 78 2d 61 6d 7a 2d 64 61 74 65 3b 78 2d 61 6d 7a 2d 73 65 63 75 72 69 74 79 2d 74 6f 6b 65 6e 0a 65 33 62 30 63 34 34 32 39 38 66 63 31 63 31 34 39 61 66 62 66 34 63 38 39 39 36 66 62 39 32 34 32 37 61 65 34 31 65 34 36 34 39 62 39 33 34 63 61 34 39 35 39 39 31 62 37 38 35 32 62 38 35 35<\/CanonicalRequestBytes><RequestId>XKPYFG87BPS57CX7<\/RequestId><HostId>U7iKvrG1tQoSygW7mehKxqLHT3yCsZgYuuaoAVmNMpSAz8b7WS41++NWCrdVYzwigSZYAio7ruY=<\/HostId><\/Error>\r\n  | [truncated...]\r\n  \r\n2023-10-19T13:58:05.772+0100 [DEBUG] backend-s3: request failed with unretryable error https response error StatusCode: 403, RequestID: XKPYFG87BPS57CX7, HostID: U7iKvrG1tQoSygW7mehKxqLHT3yCsZgYuuaoAVmNMpSAz8b7WS41++NWCrdVYzwigSZYAio7ruY=, api error SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method.: tf_backend.operation=Get tf_backend.req_id=63c46ab0-c213-4f8d-1c9b-8b5a0ac6520b tf_backend.s3.bucket=ob-[REDACTED]-eu-west-1-terraform tf_backend.s3.path=terraform.tfstate\r\n```\r\n\r\n### Expected Behavior\r\n\r\nTerraform should be able to refresh the state and initialise properly.\r\n\r\n### Actual Behavior\r\n\r\nTerraform attempts to refresh the state (GetObject) and a SignatureDoesNotMatch error is thrown.\r\n\r\n### Steps to Reproduce\r\n\r\n1. terraform init (after first apply)\r\n\r\n### Additional Context\r\n\r\nTerraform 1.6.0 is not able to refresh an existing state file after first run for a new state held in an s3 backend.\r\n\r\nThe first run is able to perform the ListBucketsV2, HeadObject and PutObject operations to init and apply. On subsequent runs, the ListBucketsV2 and HeadObject operations are successful, but the GetObject operation fails (HeadObject returns that an object with the key name is present) with a SignatureDoesNotMatch error.\r\n\r\nThis behaviour is **only** observed when running Terraform locally. When the same execution is made via a jenkins pipeline, no issue occurs. The only difference between the two is that our jenkins pipeline gets session credentials via an assume-role CLI call and exports them to environment variables, whereas locally we are using a federated AD role to gain session credentials.\r\n\r\nI have tried numerous configurations for the s3 backend:\r\n\r\n-  assume role config\r\n-  hardcoding the keys\/token on the backend config block (see above)\r\n- setting\/unsetting the related environment variables (AWS_PROFILE, AWS_SESSION_TOKEN etc.) to no avail.\r\n- changing profile config in ~\/.aws\/credentials\r\n- have also tried with 1.6.1 and 1.6.2, behaviour is the same\r\n\r\nMy only summary is that the signature for the GetObject request is not being created properly. From debug logs i can see the session credentials are received correctly whichever way I configure the backend, so I don't believe the issue is in this area. It's just that GetObject call that Terraform doesn't like!\r\n\r\nNOTE: The exact same configuration works perfectly if I switch back to either version 1.3.1 or 1.5.3 (these are the only other pre-1.6.x versions available to us).\r\n\r\nAny help would be greatly appreciated.\r\n\r\n### References\r\n\r\n_No response_","comments":["I believe I may be encountering a similar issue\r\n\r\n```\r\nTerraform v1.5.7\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.24.0\r\n+ provider registry.terraform.io\/hashicorp\/external v2.3.1\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.1\r\n```\r\n\r\nI am listing objects in a bucket and then getting details about each:\r\n```\r\ndata \"aws_s3_objects\" \"ova_objects\" {\r\n  bucket = var.src_bucket\r\n  prefix = var.prefix\r\n}\r\n\r\ndata \"aws_s3_object\" \"object_details\" {\r\n  for_each = toset(data.aws_s3_objects.ova_objects.keys)\r\n\r\n  bucket = var.src_bucket\r\n  key = each.value\r\n}\r\n```\r\n\r\nThe `aws_s3_objects` works fine and all but one of the `aws_s3_object` requests works. But one errors with this:\r\n```\r\n Error: downloading S3 Bucket ([redacted]-us-west-2-[redacted]) Object (<object-prefix>\/encrypt_password.txt): operation error S3: GetObject, https response error StatusCode: 403, RequestID: ***************, HostID: *************, api error SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method.\r\n```\r\n\r\nI've researched other causes of this problem like clock sync and credentials issues. I've ruled those out because all my other requests succeed. I have other prefixes that have similar files in them. The requests also fail on the `encrypt_password.txt` file in those key prefixes as well.\r\n\r\nIs it possible there is a bug in the signature generation?","Thanks for replying @josh-keller - I\u2019ve still not found a solution after countless hours troubleshooting.\r\n\r\nI just updated my initial comment to mention that the exact same configuration works perfectly with previous versions 1.3.1 and 1.5.3, so it seems this could either be a bug or some behavioural change based off the same configuration with 1.6.x.","@lambbuster I have tried on 1.3.1 and 1.5.3 and it doesn't seem to work. Wondering if this could be in the AWS provider. What version of that are you using with those earlier versions of Terraform?","Actually, I have narrowed this down. My request works with `v5.16.2` of the AWS provider but not `v5.17.0`. So there appears to be a breaking change in `v5.17.0`. @lambbuster can you confirm?","I see in the `v5.17.0` [CHANGELOG](https:\/\/github.com\/hashicorp\/terraform-provider-aws\/releases\/tag\/v5.17.0) that the AWS SDK for Go was [upgraded to v2](https:\/\/github.com\/hashicorp\/terraform-provider-aws\/pull\/33358). So I'm guessing it's likely that there is a bug upstream with v2 of the SDK.\r\nI've opened an issue on the provider: https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/34351","@josh-keller - for the older TF versions (1.3.1\/1.5.3) we had the constraint set to use the latest version 4 release (~> 4.0), so 4.67.0.\r\n\r\nI\u2019ve just tried using 1.6.0 (and 1.6.3!) with 5.16.2 of the AWS provider and still get the same error :( \r\n\r\nThanks for helping with the troubleshooting!","> I\u2019ve just tried using 1.6.0 (and 1.6.3!) with 5.16.2 of the AWS provider and still get the same error :(\r\n\r\nInteresting, maybe it's not the Go SDK v2. Or maybe there are two different issues going on. Anyway, I have a version that works for me at the moment and need to move forward with that for now.","We have performed verification with #34243 and fix mentioning this Issue does not resolve it.\r\n\r\nThis issue is caused by PRs #33669 #33843 - which is switch from s3 client to s3 transfer manager. As s3 transfer manager is using HTTP range header and also includes it in AWS SigV4 generation so when proxy drop this header, AWS cannot correctly verify signature\r\n\r\nIn above samples we can see\r\n`SignedHeaders=accept-encoding;amz-sdk-invocation-id;amz-sdk-request;host;range;x-amz-content-sha256;x-amz-date;x-amz-security-token, \r\nOutbound: http.request.header.range=bytes=0-5242879 \r\nAWS Reponse to SigV4 verification failure : range (empty) \r\n`\r\n\r\nIssue will occur for everyone that uses proxy that does not support HTTP Header \"Range\" Reference: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Headers\/Range\r\n\r\nProxies can have 3 behaviors:\r\na) does not support Range : when they see such header they drop it before outbound traffic request\r\nb) support Range thru caching : from client perspective range is supported but proxy is dropping it for outbound traffic taking full response and on its own provide range capability thru caching of outbound response\r\nc) support fully Range - passing it outbound as sent by client\r\n\r\nThis change was not part of 1.6.0-beta1 which still worked correctly with proxies not supporting range.\r\n\r\nThere are couple of options how we can approach this problem:\r\na) there will be a change to exclude or opt-out of using range header in SigV4 by s3 transfer manager\r\nb) revert back to s3 client from s3 transfer manager\r\nb) there will be no plan to change and fix this means breaking change and information in documentation that remote s3 backend will not work with proxies not supporting Range Headers","@Echeoss \r\nDid you find solution for this ? ","> @Echeoss Did you find solution for this ?\r\n\r\nHi. We switched for testing to local tfstate file. On remote execution we do not have proxy so issue does not exists. "],"labels":["bug","backend\/s3","new"]},{"title":"Confusing extra quoting in import error message for \"object not found\"","body":"(Originally discussed in [a Reddit post](https:\/\/www.reddit.com\/r\/Terraform\/comments\/17cel7k\/having_trouble_running_an_import_with_windows\/).)\r\n\r\nIt seems that when Terraform asks a provider to import something and the provider reports \"not found\", Terraform is producing an error message which includes the resource address in quoted form, rather than in the typical \"naked\" form:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Cannot import non-existent remote object\r\n\u2502\r\n\u2502 While attempting to import an existing object to \"azurerm_pim_eligible_role_assignment.role-vdi-vmadminpim[\\\"Team3\\\"]\", the provider detected that no object exists with the given id. Only pre-existing      \r\n\u2502 objects can be imported; check that the id is correct and that it is associated with the provider's configured region or endpoint, or use \"terraform apply\" to create a new remote object for this resource.  \r\n\u2575\r\n```\r\n\r\nResource instance addresses should not be presented in quotes, because they aren't written in quotes in the configuration and because (as shown here) they sometimes contain quotes themselves, making the result misleading for someone who isn't familiar with Terraform's internals. (In the original question, this made both me and the person who saw it originally think the problem was caused by the escaping on the command line, since unfortunately the command line quote-escaping conventions are the same on Windows.)\r\n\r\nI'm guessing this is really just an accidental use of `%q` instead of `%s` in a `fmt.Sprintf` call, in which case it's hopefully easy to fix so that the error message won't lead folks to try to solve the wrong problem in future.\r\n","comments":[],"labels":["bug","core","confirmed","v1.5","v1.6"]},{"title":"Feature: Update before destroy","body":"### Terraform Version\n\n```shell\n1.5.4\n```\n\n\n### Use Cases\n\nDisabling termination protection, setting a force delete flag, or other change that is necessary to allow Terraform to destroy the resource.\n\n### Attempted Solutions\n\nThe current solution requires two steps: updating the resource through Terraform, then destroying it.  In order to make this possible without a code change, or in the case of a child module, a variable must be exposed at the root level that allows the caller to override the value when calling `terraform destroy`.  This forces the caller to be made aware of this implementation behavior and requirement, as opposed to allowing the child module writer to expose a simpler interface.\n\n### Proposal\n\nInside a resource's `lifecycle` block, add an object attribute called `update_before_destroy` that allows specifying any attributes within the resource, and the value each should take in an update that occurs just prior to the resource being destroyed.\r\n\r\n### Example\r\n\r\nThis EC2 instance has API termination enabled, which normally prevents the instance from being terminated:\r\n```hcl\r\nresource \"aws_instance\" \"this\" {\r\n  disable_api_termination = true\r\n  ...\r\n}\r\n```\r\nBy adding the `update_before_destroy` lifecycle attribute, the instance can be terminated when Terraform performs a destroy operation on it (but in general, cannot be terminated through the API):\r\n```hcl\r\nresource \"aws_instance\" \"this\" {\r\n  disable_api_termination = true\r\n  ...\r\n\r\n  lifecycle {\r\n    update_before_destroy = {\r\n      disable_api_termination = false\r\n    }\r\n  }\r\n}\r\n```\r\nThe plan output would include the fact that this attribute is being changed prior to destroy:\r\n```\r\n# aws_instance.this will be updated in-place, then destroyed\r\n  ~ resource \"aws_instance\" \"this\" {\r\n        id                      = \"i-01234567\"\r\n      ~ disable_api_termination = true -> false\r\n        # (43 unchanged attributes hidden)\r\n    }\r\n# followed by:\r\n  - resource \"aws_instance\" \"this\" {\r\n      - id                      = \"i-01234567\" -> null\r\n      - image_id                = \"ami-012345678901234567\" -> null\r\n      - instance_type           = \"t3.small\" -> null\r\n      - disable_api_termination = false -> null\r\n      ...\r\n    }\r\n```\r\n### Benefits\r\n\r\n1. Reduces the number of times and ways the caller must invoke Terraform.\r\n2. Allows better module behavior encapsulation.\n\n### References\n\n_No response_","comments":["Thanks for sharing this use-case, @bondsb!\r\n\r\nI think this idea is interesting but also gives me pause, because when discussing provider features like this `disable_api_termination` argument I've commonly heard that folks want it _because_ it requires two steps to destroy the object, so that they get an extra level of protection against mistakes.\r\n\r\nWhile I do understand that what you proposed is opt-in and therefore would not prevent folks from continuing to use it in the current way, it does seem to raise the question: if you want to be able to destroy this object in only one step, why not leave `disable_api_termination` set to `false` in the first place? What problem is that argument solving for you if not to make it harder to destroy the object?\r\n\r\nThanks!\r\n","Great question, @apparentlymart.  My use case is when a team wants termination to be performed by a Terraform pipeline without hassle (no extra pipelines to change the resources first, no extra code changes), but still want to be protected when someone is using another means such as the cloud management console.","Thanks for that additional context, @bondsb!\r\n\r\nSo I guess my new understanding of the underlying use-case is something like: I want to reduce the risk that something is accidentally deleted outside of Terraform, while still being able to destroy it with Terraform in only a single step.\r\n\r\n(I'm stating that as just a naked use-case without an associated design proposal only because that sort of framing tends to invite other possible solutions to the same problem and also makes it easier to understand exactly what benefit a proposal is hoping to achieve, and how broad an impact it's likely to have.)\r\n\r\n---\r\n\r\nI think with today's Terraform I would recommend that somebody with your stated use-case set things up so that by default only Terraform's credentials have access to delete things, and the credentials used by operators for ad-hoc API console access do not.\r\n\r\nI would probably still have an \"escape hatch\" where those folks are allowed to assume a more privileged role to be able to do disaster recovery. In that case, escalating to the higher privilege level is an explicit action someone must take, similar to the explicit action of turning off the \"disable API termination\" setting.\r\n\r\nI don't mean that answer to say that we therefore won't add anything new to the Terraform language to give other options here, but I'm sharing this alternative both in case it's useful to those who find this issue in the meantime and because any new feature proposal should typically be evaluated against solutions that are already available, and considered in the context of those solutions.\r\n\r\nThanks again!\r\n","As I think about the technical details of the proposal as written, it seems to me that this requires a new sort of evaluation mode for the contents of `update_before_destroy` which is, as usual, driven by the provider's schema for the resource type, but has some different requirements:\r\n\r\n- Anything left unset should presumably retain its value from the prior state, rather than being set to `null` as would be true in the normal `resource` block contents.\r\n- For resource types that have nested blocks as part of their schema, we'd presumably need to figure out what the rules are for deciding whether a block of the same type in the `update_before_destroy` section represents a change to an existing block of that type, represents an entirely new block of that type, or is intended to entirely replace all of the existing blocks of that type. This is a similar ambiguity that causes problems for the [override files](https:\/\/developer.hashicorp.com\/terraform\/language\/files\/override) mechanism, and is a big part of why we tend to recommend against using them except as a last resort and only in very simple cases.\r\n\r\nI don't think that's insurmountable, but it does add some extra things we'd need to figure out in the details.\r\n\r\n---\r\n\r\nSeparately, the fact that resource type schemas are allowed to include nested blocks means that we'd need to make `update_before_destroy` itself be a block rather than a normal argument, because an expression cannot return blocks. That's only marginally different, though:\r\n\r\n```hcl\r\nresource \"aws_instance\" \"this\" {\r\n  disable_api_termination = true\r\n  ...\r\n\r\n  lifecycle {\r\n    update_before_destroy {\r\n      disable_api_termination = false\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n","If this turns out to be a pretty narrow use-case that only applies to a small number of resource types in a small number of providers (presumably, only those for APIs that have some form of server-side \"deletion prevention\" like this) then we might end up deciding that this is something that individual providers should offer as part of their implementation, rather than a core language feature.\r\n\r\nFor example, the `hashicorp\/aws` provider could potentially offer yet another boolean argument which specifies that the provider should itself re-enable \"API termination\" before trying to stop the instance. I don't have a good idea for the name of it so I'm just going to invent a terrible one as a placeholder for the sake of example:\r\n\r\n```hcl\r\nresource \"aws_instance\" \"example\" {\r\n  # ...\r\n\r\n  disable_api_termination            = true\r\n  enable_api_termination_for_destroy = true\r\n}\r\n```\r\n\r\nI would imagine the provider then checking in its destroy implementation whether `enable_api_termination_for_destroy` is set, and if so it would first make the API call to turn off whatever underlying API setting corresponds to `disable_api_termination`, and only if that succeeds then to try to destroy the instance.\r\n\r\nThis would still require two steps to destroy if you didn't already have `enable_api_termination_for_destroy` set at the last time you made a change to this resource instance, but if you plan ahead and set both options then you'd get the behavior you were looking for, with the provider taking care of it internally based on its knowledge of the EC2 API.\r\n\r\nAnother optional addition here would be for the provider to return an error when it's asked to _plan_ a destroy, if `disable_api_termination` is set but `enable_api_termination_for_destroy` is not. Then the error message could give directly-actionable advice to set `enable_api_termination_for_destroy` or unset `disable_api_termination` first as a separate step, rather than creating a plan that will definitely fail -- probably with a lower-quality error message -- once applied. Being able to return higher-quality error messages that directly refer to EC2 features would be one key advantage of making this a provider-level concern rather than a language-level concern.\r\n\r\nAs far as I know, there's nothing in the language or provider protocol design preventing a provider from implementing this behavior today, though of course provider devs might have their own reasons for not offering it.\r\n"],"labels":["enhancement","new"]},{"title":"Allow null value outputs to be present in json output","body":"### Terraform Version\n\n```shell\nTerraform v1.6.1\n```\n\n\n### Use Cases\n\nWhen using the json output of terraform as config for other systems, it might be a good idea to also give the option to include the null values inside the output json. This way you can be sure that certain keys will always exist.\n\n### Attempted Solutions\n\nThe current terraform output command with the json option does not output values that have a null value.\n\n### Proposal\n\nIt would be nice if either by default the output json always includes null values. It might also be an option for the terraform output command like this\r\n\r\n`terraform output --json --allow-null`\r\n\r\n\n\n### References\n\n_No response_","comments":["Hi @jorenvh1,\r\n\r\nThanks for filing the issue. This unfortunately is a little more difficult than just displaying a null value. Throughout Terraform `null` is used to indicate the absence of a value, and for top-level objects, triggers their removal from state. If you inspect the state file, you will see that there is no output at all if the value was `null`, so there is nothing for Terraform to display in that case. \r\n\r\nIn most cases if you are decoding json, there is usually a way to handle missing keys in the same manner as keys with a `null` value, given of course that you know what keys to expect. Some other workarounds may be to handle the `-json` output from the CLI plan itself which has information about all actions being performed, or inspect the stored plan which does store `null` output values, as that is captured before they are applied to the state.\r\n\r\nThanks!\r\n\r\n"],"labels":["enhancement","core"]},{"title":"`terraform fmt` won't fix interpolation inside map","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.1\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nvariable \"var\" {}\r\n\r\nlocals {\r\n  bar = \"${var.var}\"\r\n\r\n  foo = {\r\n    foo_bar = \"${var.var}\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\n```shell\r\n$ terraform fmt -write=false -diff\r\ntest.tf\r\n--- old\/test.tf\r\n+++ new\/test.tf\r\n@@ -1,7 +1,7 @@\r\n variable \"var\" {}\r\n\r\n locals {\r\n-  bar = \"${var.var}\"\r\n+  bar = var.var\r\n\r\n   foo = {\r\n     bar = \"${var.var}\"\r\n```\r\n\r\n### Expected Behavior\r\n\r\nInterpolations for `bar` and `foo_bar` fixed.\r\n\r\n### Actual Behavior\r\n\r\nInterpolation fixed for `bar` only.\r\n\r\n### Steps to Reproduce\r\n\r\n`terraform fmt`\r\n","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","cli","fmt"]},{"title":"Terraform Test: add ability to skip teardown","body":"### Terraform Version\n\n```shell\n~>1.6.0\n```\n\n\n### Use Cases\n\nWhen writing integration tests for resources which have a long creation time (ex: k8s clusters, database clusters, etc.), the feedback cycle when your test fails can be excruciatingly long.  It would be nice if we could skip the teardown process and continue writing our tests against these resources.\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\nAdd a CLI flag to `terraform test` to skip the teardown\/destroy process and\/or add an attributes to skip the teardown on the `run` block.\r\nExamples\r\n`terraform test --skip-teardown`\r\n\r\n```hcl\r\nrun \"integration\" {\r\n  skip-teardown = true\r\n  ...\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Hi @yohanb! Thanks for this suggestion.\r\n\r\nAre you imagining that Terraform would just totally ignore the objects that were created, and expect you to clean them up manually in the remote system? Do you have any concern that a subsequent run of the test would fail due to trying to create new objects that conflict with objects that were produced on the previous run?\r\n\r\nThanks again!\r\n","> Hi @yohanb! Thanks for this suggestion.\r\n> \r\n> Are you imagining that Terraform would just totally ignore the objects that were created, and expect you to clean them up manually in the remote system? Do you have any concern that a subsequent run of the test would fail due to trying to create new objects that conflict with objects that were produced on the previous run?\r\n> \r\n> Thanks again!\r\n\r\nHi @apparentlymart !  Thanks for the quick feedback.\r\nI was thinking that it could optionally keep the state of what it had created so it doesn't have to recreate them everytime.  In this case, it would be the responsibility of the developer to cleanup resources and avoid conflicts.","Thanks for that extra context, @yohanb!\r\n\r\nWhat you've described is something that was possible in the original experimental form of `terraform test` where a test case was really just a normal module and the test harness was literally just applying and then destroying each one in turn. In that case, it was possible to skip the test harness while iterating on one particular test -- just using the test module with the normal plan and apply commands and only use the test harness to automate running the tests in bulk once they are written.\r\n\r\nThe new `.tftest.hcl` language in the final design is important for testing more elaborate situations like changes made over time, but in return it makes it a little harder to essentially halt the process partway through and then do normal-ish Terraform workflow things with that interim state before proceeding.\r\n\r\nThe first thing that comes to my mind thinking about this problem is something like Git's \"interactive rebase\" mechanism, where you can effectively halt a rebase partway through and then run normal git commands to manipulate things before you proceed, or to bail out and clean up the intermediate state of things and return to normal git usage. I'm sure there are other ways to solve this too, but I mention that only to see if it resonates with you.\r\n\r\nIn the meantime though, since the original testing design was essentially just a convention for laying out normal Terraform root modules to use for testing, it's still possible to work that way even though `terraform test` doesn't inherently understand it anymore. You can bridge the two techniques by using [the optional ability to specify a separate module for a test step](https:\/\/developer.hashicorp.com\/terraform\/language\/tests#modules). If you make a directory containing a separate Terraform root module that calls the module you want to test, you can use normal Terraform workflow commands like init, plan, apply directly in that directory while you are iterating but then wire that module in to one of your `.tftest.hcl` files so that it will also run through the automated test harness. This more-or-less recreates the model from the experimental `terraform test`, allowing you to treat the test case as a normal Terraform configuration in situations where that's more convenient.\r\n\r\n\r\n\r\n\r\n","@apparentlymart thanks, appreciate the feedback.\r\nI was thinking more the sense of iterating the test cases.  For example, you want to assert something on a module which takes 20+ minutes to setup and then your test fails for whatever reason.  The test will automatically tear it down and you have to start over to fix your assertions.\r\nI understand the workaround you propose where you can create a module to indirectly call your root module and iterate that way but you can effectively do that without the test framework right?\r\n","Yes, I was suggesting to do it without the test harness while you are developing\/iterating and to use the test harness only for running all the tests at once after you are finished developing to make sure they all still work under the specific sequence of operations described in the test configuration files.\r\n\r\nIn making that suggestion I'm thinking that a test configuration file is effectively just automating a sequence of plan and apply commands interspersed with arbitrary checks. In situations where the automation is inconvenient, you can run the same operations manually.\r\n\r\nOne way I imagine making this more integrated in future is to help automate the setup of such an environment. You might specify a single test configuration file and a single step within that file, and then Terraform would:\r\n\r\n1. Execute all of the test steps prior to the chosen one in the same file non-interactively just as would happen in a normal test run.\r\n2. Once the specified step is reached, write the current in-memory-only state to a special new filename under the `.terraform` directory. The presence of a file at that new location tells Terraform CLI that it should run in a special mode where operations occur against the transient test state instead of whatever normal workspace is selected.\r\n3. At this point Terraform exits and returns you to your shell prompt. Any normal Terraform commands you run, like `terraform plan` or `terraform apply`, will work against the transient test state that was created in the previous step. You can iterate as much as you like and do anything to that state that you could normally do to a workspace state.\r\n4. Once you're finished, you run another test-specific command to exit this testing mode. At this point you could have at least the following options for how to proceed:\r\n    - Continue running the remaining test steps non-interactively to completion, including the normal destroy at the end.\r\n    - Run only the currently-active test step and then halt again at the next one still in the transient test state mode, but now with the effects of another step applied to the state.\r\n    - Destroy everything that's currently existing and halt completely. You might do this if you've made such a large change to the test state that the remaining steps no longer make sense to run, for example.\r\n    - Possibly it might make sense to \"upgrade\" the transient test case into a normal named workspace in the configured backend, if you decide for some reason that the test infrastructure is now \"real\" infrastructure in some sense. (Maybe now a long-lived development environment?)\r\n\r\nMost of what I described above is already possible to do manually by explicitly creating a separate root module to iterate in, so my ideas above are intended to \"pave the cowpath\" by having the system do those steps itself and to be able to reuse the main module directory (in a special new mode) instead of having to make a new working directory to develop in.\r\n\r\nI'm sure there are other ways to do stuff like this, but as I mentioned before this is inspired (at a very high level) by the \"git rebase\" model, where your work tree temporarily switches into a different mode where you can run various normal git commands to do open-ended, self-directed operations and then either continue rebasing subsequent commits or otherwise bail out and return to the \"normal\" mode.\r\n","I stumbled across another use-case for introducing a **skip-teardown** attribute. \r\nI think there are cases where it makes sense to create resources in run modules that are dependent on the resources of the main module.\r\n\r\nAt the moment it [is documented](https:\/\/developer.hashicorp.com\/terraform\/language\/tests#modules-cleanup) that this is not possible and leads to errors in the destroy process.\r\n\r\nMy use case would be as follows:\r\n\r\nFirst, a vault container should be created with a setup\/run module.\r\nIn the second step, the module to be tested should be applied to create policies and other resources in the vault.\r\nAfter that, a test module with a token should create resources in the vault based on the new policy. This is to verify that the created policies work as planned.\r\n\r\nHowever, since the resources created for the test depend in part on resources created in the main module, the destroy inevitably fails.\r\n\r\nSince the created resources were only created in a temporary vault container, the destroy error is actually irrelevant. The resources would not necessarily have to be deleted, or the error could be ignored. I have not yet found a way to do either.\r\n\r\nThis case is a bit different from the one discussed before, but it could be an argument to make the destroy of run blocks optional.\r\n\r\nI think that this problem could be generalizable with classes of \"disposable resources\". For example, it would also apply to objects in a k8s cluster that is only created in the main module.\r\n\r\nBy the way, as a quick feedback, I really like the test feature itself. Once you've internalized a few basic ideas, you can build some pretty cool things with it. Thanks for your work.","Hey @yohanb (and others in this issue), the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me oismail@hashicorp.com and we can schedule a time to chat!","> Hey @yohanb (and others in this issue), the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me [oismail@hashicorp.com](mailto:oismail@hashicorp.com) and we can schedule a time to chat!\r\n\r\nHi @omarismail !\r\nThanks for the opportunity.  Will do!"],"labels":["enhancement","new","terraform test"]},{"title":"Allow validating terraform provider configuration in `required_providers` block","body":"### Terraform Version\n\n```shell\n1.6.1\n```\n\n\n### Use Cases\n\nThe terraform aws provider proposes a `default_tags` configuration that sets tags for every resources created.\r\nWhen writing a module, we used to force tags to be passed as variable and now we want to leverage that feature instead and we still want to ensure tags are always passed.\r\n\r\nThe problem is that I now need to rely on the module user to properly configure their provider. \r\n\r\nI'd like to have a way to tell, from the module, that I require such and such provider configuration. That way, the module can fail early if the configuration is not provided. The `required_providers` block seems to be the logical place for that.\n\n### Attempted Solutions\n\nNA\n\n### Proposal\n\nThe validation could re-use the same validation as with variables, so for the usecase above I'd imagine something like this:\r\n\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    mycloud = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.0\"\r\n      validation {\r\n        condition     = length(config.default_tags) > 0 && lookup(config.default_tags.tags, \"a_mandatory_key\", null) != null\r\n        error_message = \"AWS provider must have default_tags set and contain the 'a_mandatory_key' tag.\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI replaced the `var` keyword with `config` here to distinguish the two, but the syntax is otherwise the same.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"terraform init: duplicate diags being printed","body":"Duplicate diagnostics are being printed when `terraform init` fails while loading config. Not sure exactly which cases yet, discovered while fixing https:\/\/github.com\/hashicorp\/terraform\/issues\/34041.\r\n\r\n### Version\r\nThe bug seems to have been introduced in v1.5.5. Not present in v1.5.4.\r\n\r\n### Config\r\n\r\n```hcl\r\nmoved {}\r\n```\r\n\r\n### Steps taken\r\n\r\n1. `terraform init`\r\n\r\n### Expected behaviour\r\n\r\n```\r\n\u222e terraform1_5_4 init\r\n\r\nInitializing the backend...\r\nThere are some problems with the configuration, described below.\r\n\r\nThe Terraform configuration must be valid before initialization so that\r\nTerraform can determine which modules and providers need to be installed.\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"from\" is required, but no definition was found.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"to\" is required, but no definition was found.\r\n\u2575\r\n```\r\n\r\n### Actual behaviour\r\n```\r\n \u222e terraform1_5_5 init\r\n\r\nInitializing the backend...\r\nTerraform encountered problems during initialisation, including problems\r\nwith the configuration, described below.\r\n\r\nThe Terraform configuration must be valid before initialization so that\r\nTerraform can determine which modules and providers need to be installed.\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"from\" is required, but no definition was found.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"to\" is required, but no definition was found.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"from\" is required, but no definition was found.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Missing required argument\r\n\u2502 \r\n\u2502   on main.tf line 8, in moved:\r\n\u2502    8: moved {}\r\n\u2502 \r\n\u2502 The argument \"to\" is required, but no definition was found.\r\n\u2575\r\n```\r\n","comments":["I believe the root cause behind this is contained somewhere in https:\/\/github.com\/hashicorp\/terraform\/pull\/33628 and may even be described in https:\/\/github.com\/hashicorp\/terraform\/pull\/33628\/files#r1286524862 specifically.\r\n\r\nI can confirm that I can reduce it back to 2 diagnostics when reverting the two commits:\r\n\r\n```\r\ngit revert 9655c2f\r\ngit revert 24d06a4\r\n```\r\n"],"labels":["bug","v1.5"]},{"title":"Missing validation for incorrectly quoted moved addresses","body":"### Terraform Version\n\n```shell\n1.5.3\n```\n\n\n### Terraform Configuration Files\n\nI can't tell whats relevant, and our configuration is uhm, sizeable. Also internal.\n\n### Debug Output\n\n.\n\n### Expected Behavior\n\nTerraform should not have crashed.\n\n### Actual Behavior\n\n```\r\nrunning \"\/atlantis\/bin\/terraform1.5.3 init -input=false -upgrade\" in \"\/atlantis\/repos\/cognitedata\/terraform\/8272\/default\/cdf\/greenfield\": exit status 11\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nPlease report the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version, the stack trace\r\nshown below, and any additional information which may help replicate the issue.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: invalid memory address or nil pointer dereference\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n\t\/opt\/hostedtoolcache\/go\/1.20.0\/x64\/src\/runtime\/debug\/stack.go:24 +0x65\r\nruntime\/debug.PrintStack()\r\n\t\/opt\/hostedtoolcache\/go\/1.20.0\/x64\/src\/runtime\/debug\/stack.go:16 +0x19\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/logging\/panic.go:58 +0x153\r\npanic({0x242c520, 0x41dc420})\r\n\t\/opt\/hostedtoolcache\/go\/1.20.0\/x64\/src\/runtime\/panic.go:890 +0x263\r\ngithub.com\/hashicorp\/terraform\/internal\/addrs.(*MoveEndpoint).String(...)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/addrs\/move_endpoint.go:54\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.(*Module).appendFile(0xc000b01d40, 0xc000164c00)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/module.go:443 +0x3dd8\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.NewModule({0xc00013e3e0, 0x4, 0x0?}, {0x0, 0x0, 0x122f616?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/module.go:154 +0x475\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.(*Parser).LoadConfigDir(0xc000154000?, {0x2db13b8, 0x1})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/parser_config_dir.go:45 +0x265\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*Meta).loadSingleModule(0xc000064234?, {0xc000064234?, 0xc0001541a4?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/meta_config.go:71 +0x9b\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*InitCommand).Run(0xc000154000, {0xc0000500a0, 0x2, 0x2})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/init.go:161 +0xe8b\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc00013c280)\r\n\t\/home\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5f8\r\nmain.realMain()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:318 +0x1729\r\nmain.main()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:61 +0x19\r\n\r\n```\n\n### Steps to Reproduce\n\nterraform1.5.3 init -input=false -upgrade\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @rbtcollins,\r\n\r\nThanks for filing the issue! Can you verify that the crash still happens in a current release version, to make sure it hasn't already been patched? I'm not sure what would trigger this yet, but my first guess us that there is a moved block with an invalid `to` or `from` address that somehow slips through.\r\n\r\nThanks!","```\r\nrunning \"\/atlantis\/bin\/terraform1.6.1 init -input=false -upgrade\" in \"\/atlantis\/repos\/....\": exit status 11\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nPlease report the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version, the stack trace\r\nshown below, and any additional information which may help replicate the issue.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: invalid memory address or nil pointer dereference\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:24 +0x5e\r\nruntime\/debug.PrintStack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:16 +0x13\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/logging\/panic.go:58 +0x13b\r\npanic({0x2cd7d40?, 0x50ab790?})\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/panic.go:920 +0x270\r\ngithub.com\/hashicorp\/terraform\/internal\/addrs.(*MoveEndpoint).String(...)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/addrs\/move_endpoint.go:54\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.(*Module).appendFile(0xc0005771e0, 0xc000574180)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/module.go:456 +0x3d75\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.NewModule({0xc0002e1f00, 0x5, 0x1?}, {0x0, 0x0, 0x1?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/module.go:167 +0x4d5\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.NewModuleWithTests(...)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/module.go:100\r\ngithub.com\/hashicorp\/terraform\/internal\/configs.(*Parser).LoadConfigDirWithTests(0xc000996e00?, {0x38bf420, 0x1}, {0x317b3ae?, 0x41?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/configs\/parser_config_dir.go:73 +0x3a5\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*Meta).loadSingleModuleWithTests(0xc000062234?, {0xc000062234?, 0xc0009a9c20?}, {0x317b3ae, 0x5})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/meta_config.go:108 +0xac\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*InitCommand).Run(0xc000996e00, {0xc0000500a0, 0x2, 0x2})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/init.go:176 +0x125a\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc0002fc3c0)\r\n\t\/home\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5b8\r\nmain.realMain()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:339 +0x1eab\r\nmain.main()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:64 +0x13\r\n```","Yes, I have a move block. I'm refactoring a configuration from:\r\n\r\nroot module \r\n+- childone\r\n+- childtwo\r\n\r\nto \r\nroot module\r\n+ unify\r\n  + childone with count=0\/1 conditional\r\n  + childtwo\r\n\r\nin order to remove highly duplicated code in our fleet - we have many root modules that are 99% identical code (providers, marshalling outputs of childone to childtwo).\r\n\r\n\r\n```\r\nmoved {\r\n  from = \"module.childone\"\r\n  to   = \"module.unify.module.childone[0]\"\r\n}\r\nmoved {\r\n  from = \"module.childtwo\"\r\n  to   = \"module.unify.module.childtwo\"\r\n}\r\n```","Just finding my way at the moment,\r\n\r\n```\r\n=> 421:         m.Moved = append(m.Moved, file.Moved...)\r\n   422:\r\n   423:         for _, i := range file.Import {\r\n   424:                 for _, mi := range m.Import {\r\n   425:                         if i.To.Equal(mi.To) {\r\n   426:                                 diags = append(diags, &hcl.Diagnostic{\r\n(dlv) p file.Moved\r\n[]*github.com\/hashicorp\/terraform\/internal\/configs.Moved len: 2, cap: 2, [\r\n        *{\r\n                From: *github.com\/hashicorp\/terraform\/internal\/addrs.MoveEndpoint nil,\r\n                To: *github.com\/hashicorp\/terraform\/internal\/addrs.MoveEndpoint nil,\r\n                DeclRange: (*\"github.com\/hashicorp\/hcl\/v2.Range\")(0xc0000c3230),},\r\n        *{\r\n                From: *github.com\/hashicorp\/terraform\/internal\/addrs.MoveEndpoint nil,\r\n                To: *github.com\/hashicorp\/terraform\/internal\/addrs.MoveEndpoint nil,\r\n                DeclRange: (*\"github.com\/hashicorp\/hcl\/v2.Range\")(0xc0000c3280),},\r\n]\r\n```\r\n\r\nThis seems very wrong - it is as far as I can tell the right file - it has the module invocation in it; the code looks like that in my previous comment, but has endpoint values of nil","The problem was the quotation marks around the resource addresses. Removing them prevented the crash. I can't offer a patch - nothing ideological, just the overheads of a CLA for a one-off don't add up; I would suggest that both the parser extracting addresses and the appendFile method that is consuming the slice of Moved's should be validating the datastructure's basic properies. E.g. pre conditions or other such assertions (since golang doesn't permit encoding this in the type system).\r\n","No worries @rbtcollins, thanks for adding the configuration example!","The panic occurs when there is an `import` block and a `moved` block without valid `to` and `from` addresses in the same module. \r\n\r\nRepro:\r\n```hcl\r\nmoved {\r\n  to = null_resource.foo\r\n  from = \"null_resource.bar\"\r\n}\r\n\r\nimport {}\r\n```\r\n\r\nMinimal repro:\r\n```hcl\r\nmoved {}\r\nimport {}\r\n```\r\n\r\nThe cross-validation of the `import` block and `moved` block args happens whether or not there were errors parsing the args, hence panic."],"labels":["bug","config","confirmed","v1.5"]},{"title":"Support for UUIDv7 (adding function uuidv7)","body":"### Terraform Version\n\n```shell\n1.6.x\n```\n\n\n### Use Cases\n\nAs 1.6.x, UUID formats v4 and v5 are supported. \r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/functions\/uuid\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/functions\/uuidv5\r\n\r\nSome time ago, new UUID formats were launched by IETF and, one of them (UUID v7) has some very interesting properties that make it a need in many scenarios where distributed generation of quasi-sequential numbers is desired (up to 50ns resolution + 48 random bits)\r\nCurrently generating them requires launching external commands.\r\nThis request is to add support for UUID v7.\n\n### Attempted Solutions\n\nLaunching external commands\n\n### Proposal\n\nAdd uuidv7 function.\n\n### References\n\nSome supporting material:\r\n* https:\/\/datatracker.ietf.org\/doc\/html\/draft-peabody-dispatch-new-uuid-format#name-uuid-version-7\r\n* https:\/\/buildkite.com\/blog\/goodbye-integers-hello-uuids","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Question: why aren't you using [ULID](https:\/\/github.com\/ulid\/spec) that already does that? "],"labels":["enhancement","functions","new"]},{"title":"add functions ifnull, ifempty, ifnullorempty, isempty, isnullorempty","body":"### Terraform Version\r\n\r\n```shell\r\n1.6.x\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nThe terraform language is quite rich and very powerful.\r\nHowever, for those accustomed to other similar languages, some useful functions could be added to make development life easier, and without breaking any kind of existing code. It does also allow the use of some development idioms that are currently out of bounds.\r\n\r\n* `ifnull(expression, expression-to-return-if-null)` -> expression\r\n* `ifempty(expression, expression-to-return-if-empty)` -> expression\r\n* `ifnullorempty(expression, expression-to-return-if-null-or-empty)` -> expression\r\n* `isempty(expression)` -> `true` if is \"\", [], {}\r\n* `isnullorempty(expression)` -> `true` if is null, \"\", [], {}\r\n\r\n### Attempted Solutions\r\n\r\nThere are workarounds for them, but they are quite verbose, force expressions to be repeated (eg, when using ?: to do a ifnull) and\/or are harder to manage\r\n\r\n### Proposal\r\n\r\nAdd the following functions to the library:\r\n* `ifnull(expression, expression-to-return-if-null)` -> expression\r\n* `ifempty(expression, expression-to-return-if-empty)` -> expression\r\n* `ifnullorempty(expression, expression-to-return-if-null-or-empty)` -> expression\r\n* `isempty(expression)` -> `true` if is \"\", [], {}\r\n* `isnullorempty(expression)` -> `true` if is null, \"\", [], {}\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for these feature requests, @joaocc.\r\n\r\nSome of these overlap with existing functions or language features, though not perfectly in all cases:\r\n\r\n- [`coalesce`](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/coalesce) is already essentially your proposed `ifnullorempty`, except that it supports any number of arguments and just returns the first one that isn't null or an empty string. (It doesn't treat any other kind of \"empty\" as absent, so it's not a perfect analogy.)\r\n\r\n    I would prefer if `coalesce` were more like your proposed `ifnull` -- only skipping null values, not empty strings -- since that's closer to the SQL function that this was inspired by, but sadly `coalesce` inherited this \"empty string is the same as null\" assumption from Terraform v0.11 and earlier and so we're now stuck with it for backward compatibility.\r\n- `isempty(e)` seems to be equivalent to `length(e) != 0`, and so doesn't seem to be necessary.\r\n\r\nIn general I'm not a fan of features that would encourage treating `null` and \"empty\" as the same, since that seems likely to confuse the meanings of those concepts. `null` in Terraform represents the total absense of a value, which is intentionally a distinct idea from a non-null value that happens to be \"empty\" if such a concept is available for its type. This special meaning of `null` is important because it's how Terraform represents the value of a totally-unset argument, allowing default values to be used instead, whereas \"empty values\" should not be treated in that way.\r\n\r\nThere are historical inconsistencies like the `coalesce` function I mentioned above (which treats `\"\"` the same as `null`) and the default treatment of input variables (when they don't have `nullable = false` set), but both of those design warts are things we'd like to fix in a future [language edition](https:\/\/log.martinatkins.me\/2021\/09\/25\/future-of-the-terraform-language\/#terraform-language-editions) and doing so will become harder if we introduce more features that persist that ambiguity.\r\n\r\nGiven all that, my own version of this proposal would be:\r\n\r\n* In a future language edition, make `coalesce` only skip null arguments, and treat `\"\"` as a normal value to be returned.\r\n\r\n    That would mean that `coalesce(foo, \"bar\")` would be equivalent to what you proposed as `ifnull(foo, \"bar\")`, but it would also be possible to write `coalesce(foo, bar, baz, \"fallback\")` to take the first one that isn't null, just like the SQL function of the same name.\r\n* Consider a new function which takes a value of any type and returns `null` if it's an empty string, empty collection, empty object, or empty tuple. This would be an \"escape hatch\" for when working with legacy or strange modules that use non-null empty values to represent \"absent\", and would then avoid the need for special additional functions to work with \"empty\" because you'd be able to combine the null-oriented ones with this new function.\r\n\r\n    I don't know yet what I'd call such a function. It's a little similar to our existing [`one`](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/one) function except that it would work for more types and for collections\/structures with more than one element. `emptyasnull` seems like a plausible placeholder name for discussion:\r\n\r\n    - `coalesce(emptyasnull(foo), \"bar\")`, assuming that we'd also redefined `coalesce` to only skip null, would be the same as your proposed `ifnullorempty(foo, \"bar\")`.\r\n\r\n    - `emptyasnull(foo) == null` would be the same as your proposed `isnullorempty(foo)`.\r\n\r\n    This is of course not as readable\/convenient as having specialized functions that handle both cases, but for me that's intentional to help communicate that using non-null values to represent absence is an unusual thing to do, and thus to encourage using `null` to represent the absence of a value in all cases.\r\n\r\nWith that said, we are also planning to introduce the ability for providers plugins to contribute their own functions to the Terraform language in a future Terraform version, and so at that point it would be possible in principle to write a provider that offers additional combinations, if you really want them. For the core language we try to focus on a single \"main\" usage pattern and then offer composable building blocks (as few as possible) to deal with situations that diverge from that main usage pattern, but provider developers will be able to make different design tradeoffs.\r\n"],"labels":["enhancement","functions","new"]},{"title":"Terraform Test: Strict mode to fail on warnings","body":"### Terraform Version\n\n```shell\nTerraform v1.6.0\n```\n\n\n### Use Cases\n\nMake `terraform test` fail if any warnings are produced.\n\n### Attempted Solutions\n\nNone\n\n### Proposal\n\n- Add new flag [here](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/command\/arguments\/test.go#L12)\r\n- Propagate new value through to TestFileRunner\r\n- Update plan diagnostics check [here](https:\/\/github.com\/hashicorp\/terraform\/blob\/edb8ea4baa43421a73d7ba50fed957e6be599b45\/internal\/backend\/local\/test.go#L427).\r\n- Update apply diagnostics check [here](https:\/\/github.com\/hashicorp\/terraform\/blob\/edb8ea4baa43421a73d7ba50fed957e6be599b45\/internal\/backend\/local\/test.go#L452). \r\n\r\n*Will need additional internal work to integrate this through to TFC*\n\n### References\n\n- https:\/\/discuss.hashicorp.com\/t\/request-for-testing-terraform-test\/56254\/12","comments":[],"labels":["enhancement","terraform test"]},{"title":"Add `ls`\/`list`, `rm`\/`remove`\/`delete`, and `mv`\/`move`\/`rename` aliases to `state`","body":"I was once again bitten by not remember if it was `state ls` or `state list`, so I went hunting to see if anyone else ran into this.\r\n\r\nAnswer is yes, and there was already an issue open on it (with a duplicate):\r\n* open issue: https:\/\/github.com\/hashicorp\/terraform\/issues\/20998\r\n* duplicate issue: https:\/\/github.com\/hashicorp\/terraform\/issues\/31688\r\n\r\nThis felt like a pretty superficial change code-wise, so I thought I'd give it a shot. (Also, no comments on the open bug since 2019, so obvs not a lot of other interest?)\r\n\r\n## Target Release\r\n\r\nIndifferent; this is purely a cosmetic \/ mnemonics enhancement. It should also be back-port-able to even truly ancient versions of Terraform, as it's not touching the inner logic at all.\r\n\r\n## Draft CHANGELOG entry\r\n\r\n### ENHANCEMENTS\r\n\r\n- Allow both English and Unix names for various `state` subcommands:\r\n  - `state ls` == `state list`\r\n  - `state mv` == `state move` == `state rename`\r\n  - `state rm` == `state remove` == `state delete`  \r\n","comments":["\n[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/not_signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33999)\n\n\nThank you for your submission! We require that all contributors sign our Contributor License Agreement (\"CLA\") before we can accept the contribution. [Read and sign the agreement](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33999)\n\n[Learn more about why HashiCorp requires a CLA and what the CLA includes](https:\/\/www.hashicorp.com\/cla)\n\n<sub>Have you signed the CLA already but the status is still pending? [Recheck it](https:\/\/cla.hashicorp.com\/check\/hashicorp\/terraform?pullRequest=33999).<\/sub>","Uhg, sorry, thought I was doing a PR against my own fork, didn't mean to add noise!","Hi @ajf-firstup, thanks for working on this! I have a memory of reviewing a similar pull request, but I cannot find it at the moment. Generally the maintainers have resisted adding these types of aliases, however I will bring this to triage and see if I can get a definitive position to share. Also, reminder to sign the CLA before this is accepted for review \/ ready to review. Thanks!","> Hi @ajf-firstup, thanks for working on this! I have a memory of reviewing a similar pull request, but I cannot find it at the moment.\r\n\r\nThe concept seems to have popped up pretty regularly, so wouldn't surprise me if someone has already given it a try.\r\n\r\n> Generally the maintainers have resisted adding these types of aliases,\r\n\r\nDid they share why?\r\n\r\n(I can understand the argument of \"can't change it once it's there\", but this is an addition, not a removal. Which could then open the floodgates to dozens of aliases, but that seems unlikely -- most of the other CLI commands are already full words, not abbreviations. So I'm curious.)\r\n\r\n> however I will bring this to triage and see if I can get a definitive position to share. \r\n\r\nThanks!\r\n\r\n> Also, reminder to sign the CLA before this is accepted for review \/ ready to review. Thanks!\r\n\r\nAck'd.\r\n","Hi @ajf-firstup, apologies for the late reply. The response in triage was that generally speaking, these commands are little-used and the team would prefer to deprecate them. As such, it does not make sense to add more surface area to the code support and documentation that would be needed to add aliases. \r\n\r\nThat said, it is also recognized that this is more of a product decision, so I have added it to the list of pull requests to triage with the product manager when he is back from paternity leave. I believe he should be back in early January, to set an expectation on that. Thanks for your understanding on that, and for your submission!"],"labels":["enhancement"]},{"title":"kubernets backend has error when secret_suffix ends with -<number> ","body":"### Terraform Version\r\n\r\n```shell\r\n1.6.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n  backend \"kubernetes\" {\r\n    secret_suffix     = \"sbx-390\"\r\n    namespace         = \"terraform\"\r\n  }\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: index out of range [390] with length 1\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:24 +0x5e\r\nruntime\/debug.PrintStack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:16 +0x13\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/logging\/panic.go:58 +0x13b\r\npanic({0x2fb0420?, 0xc000e7d7d0?})\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/panic.go:920 +0x270\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).getSecrets(0xc000603740)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:109 +0x6c5\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).Get(0x60?)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:54 +0x25\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).refreshState(0xc0006037a0)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:138 +0x28\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).RefreshState(0xc00083f020?)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:1[31](https:\/\/github.com\/turo\/dev-sbx-deployments\/actions\/runs\/6411568781\/job\/17407278639?pr=390#step:9:32) +0x79\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*Backend).StateMgr(0x38c9540?, {0x317d2c5?, 0x7?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/backend_state.go:89 +0x70\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/local.(*Local).StateMgr(0xc0008f16c0?, {0x317d2c5?, 0xc000d48140?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/local\/backend.go:245 +0x382\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*InitCommand).Run(0xc0008f16c0, {0xc000050070, 0x0, 0x0})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/init.go:222 +0x1470\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc000743900)\r\n\t\/home\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5b8\r\nmain.realMain()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:[33](https:\/\/github.com\/turo\/dev-sbx-deployments\/actions\/runs\/6411568781\/job\/17407278639?pr=390#step:9:34)9 +0x1eab\r\nmain.main()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:64 +0x13\r\nError: Terraform exited with code 11.\r\nError: Process completed with exit code 1.\r\n```\r\n\r\n### Expected Behavior\r\n\r\nI would expect that the new feature allowing for larger state files through using multiple secrets would take the secret suffix into account before deciding that it is a multi-segment state file.\r\n\r\n### Actual Behavior\r\n\r\nWe see the following error.  It appears that it is reading in the -390 from the secret suffix part of the name and using that as an index for an array access.\r\n\r\n```\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: index out of range [390] with length 1\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:24 +0x5e\r\nruntime\/debug.PrintStack()\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:16 +0x13\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/logging\/panic.go:58 +0x13b\r\npanic({0x2fb0420?, 0xc000e7d7d0?})\r\n\t\/opt\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/panic.go:920 +0x270\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).getSecrets(0xc000603740)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:109 +0x6c5\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).Get(0x60?)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:54 +0x25\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).refreshState(0xc0006037a0)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:138 +0x28\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).RefreshState(0xc00083f020?)\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:1[31](https:\/\/github.com\/turo\/dev-sbx-deployments\/actions\/runs\/6411568781\/job\/17407278639?pr=390#step:9:32) +0x79\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*Backend).StateMgr(0x38c9540?, {0x317d2c5?, 0x7?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/backend_state.go:89 +0x70\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/local.(*Local).StateMgr(0xc0008f16c0?, {0x317d2c5?, 0xc000d48140?})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/backend\/local\/backend.go:245 +0x382\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*InitCommand).Run(0xc0008f16c0, {0xc000050070, 0x0, 0x0})\r\n\t\/home\/runner\/work\/terraform\/terraform\/internal\/command\/init.go:222 +0x1470\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc000743900)\r\n\t\/home\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5b8\r\nmain.realMain()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:[33](https:\/\/github.com\/turo\/dev-sbx-deployments\/actions\/runs\/6411568781\/job\/17407278639?pr=390#step:9:34)9 +0x1eab\r\nmain.main()\r\n\t\/home\/runner\/work\/terraform\/terraform\/main.go:64 +0x13\r\nError: Terraform exited with code 11.\r\nError: Process completed with exit code 1.\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. run terraform 1.6 \r\n2. configure k8s access -- I used docker for desktop local k8s for this\r\n3. create namespace `terraform` -- `kubectl create ns terraform`\r\n4. use the following config file\r\n```\r\nterraform {\r\n  required_version = \">= 1.1.0, < 2.0.0\"\r\n\r\n  required_providers {\r\n    kubernetes = {\r\n      source  = \"hashicorp\/kubernetes\"\r\n      version = \">=2.12.1\"\r\n    }\r\n  }\r\n\r\n  backend \"kubernetes\" {\r\n    secret_suffix     = \"sbx-390\"\r\n    config_path      = \"~\/.kube\/config\"\r\n    namespace         = \"terraform\"\r\n  }\r\n}\r\n\r\nprovider \"kubernetes\" {\r\n}\r\n```\r\n5. run `terraform init`\r\n\r\nSee error:\r\n```\r\n\u276f terraform init\r\n\r\nInitializing the backend...\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nPlease report the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version, the stack trace\r\nshown below, and any additional information which may help replicate the issue.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: index out of range [390] with length 1\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n        \/Users\/runner\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:24 +0x5e\r\nruntime\/debug.PrintStack()\r\n        \/Users\/runner\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/debug\/stack.go:16 +0x13\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/logging\/panic.go:58 +0x13b\r\npanic({0x3bbc880?, 0xc000530450?})\r\n        \/Users\/runner\/hostedtoolcache\/go\/1.21.1\/x64\/src\/runtime\/panic.go:920 +0x270\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).getSecrets(0xc000a4afc0)\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:109 +0x6c5\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/remote-state\/kubernetes.(*RemoteClient).Get(0x3d93953?)\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/backend\/remote-state\/kubernetes\/client.go:54 +0x25\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).refreshState(0xc000a4b020)\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:138 +0x28\r\ngithub.com\/hashicorp\/terraform\/internal\/states\/remote.(*State).RefreshState(0xc0005cc380?)\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/states\/remote\/state.go:131 +0x79\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*InitCommand).Run(0xc0005cc380, {0xc0001ca010, 0x0, 0x0})\r\n        \/Users\/runner\/work\/terraform\/terraform\/internal\/command\/init.go:228 +0x150c\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc00063ac80)\r\n        \/Users\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5b8\r\nmain.realMain()\r\n        \/Users\/runner\/work\/terraform\/terraform\/main.go:339 +0x1eab\r\nmain.main()\r\n        \/Users\/runner\/work\/terraform\/terraform\/main.go:64 +0x13\r\n```\r\n\r\n### Additional Context\r\n\r\nIt appears to be a direct result of this feature:\r\n\r\n> backend\/kubernetes: The Kubernetes backend is no longer limited to storing states below 1MiB in size, and can now scale by splitting state across multiple secrets. (https:\/\/github.com\/hashicorp\/terraform\/pull\/29678)\r\n\r\n### References\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/pull\/29678","comments":["FYI k8s backend [CODEOWNERS](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/CODEOWNERS) @jrhouston @alexsomesan. Thanks!"],"labels":["bug","backend\/k8s","new"]},{"title":"A method to mock data sources for testing","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\r\non windows_amd64\r\n+ provider registry.terraform.io\/hashicorp\/azurerm v3.69.0\r\n+ provider registry.terraform.io\/hashicorp\/helm v2.8.0\r\n+ provider registry.terraform.io\/hashicorp\/kubernetes v2.16.1\r\n+ provider registry.terraform.io\/hashicorp\/random v3.5.1\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.7. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Use Cases\n\nWe would like to be able to optimistically validate and plan modules that use the `terraform_remote_state data` source when the remote state may not contain some outputs yet. Once the plans look good we then want to apply the stacks in order _without_ using any default values.\r\n\r\nAdd `new_output` to A, then validate and plan A\r\nAdd usage of `A.new_output` to B, then validate and plan B (using mock\/default values for `A.new_output`)\r\nIf all looks good, apply A, then apply B (NOT using mock\/default values for `A.new_output`)\n\n### Attempted Solutions\n\nWe've tried specifying [defaults](https:\/\/registry.terraform.io\/providers\/hashicorp\/terraform\/latest\/docs\/data-sources\/remote_state#defaults), but haven't found a way to tell terraform that they should only be used for `validate` and `plan` operations and NOT be used for `apply` operations.\n\n### Proposal\n\nAdd an optional argument to the `terraform_remote_state` data source called something like `defaults_operations`.\r\n- If not provided it defaults to all operations.\r\n- If provided, it is a list of operations like `[\"validate\", \"plan\"]`.\r\n\r\nDefaults are only used when terraform is running the operations specified by that field.\n\n### References\n\n- #12316\r\n- #8001","comments":["Hi @mpkuth,\r\n\r\nCan you give an example configuration and steps showing what you are trying to do? It's not clear what it would mean for a data source to do something only during `plan` and _not_ during `apply`. What happens in `apply` is just the execution of what was recorded in the plan, and in most cases a data source is not read at all during apply.","Hello @jbardin!\r\n\r\nWe'd like to write something like:\r\n\r\nModule A\r\n```\r\noutput \"a\" {\r\n  value = \"real\"\r\n}\r\n\r\noutput \"b\" {\r\n   value = \"real\"\r\n}\r\n\r\noutput \"c\" {\r\n  value = 1\r\n}\r\n\r\noutput \"d\" {\r\n  value = [\"real\"]\r\n}\r\n```\r\n\r\nModule B\r\n```\r\ndata \"terraform_remote_state\" \"module_a\" {\r\n  ...\r\n  \r\n  defaults = {\r\n    a = \"mock\"\r\n    b = \"mock\"\r\n    c = -1\r\n    d = [\"mock\"]\r\n  }\r\n\r\n  defaults_operations = [\"validate\", \"plan\"]\r\n}\r\n```\r\n\r\nExample 1: We want to deploy a new environment that includes both Module A and Module B. We'd like to be able to validate and plan all of the modules in that environment before starting to apply anything, but we cannot right now because validation and planning of Module B will fail because there is no remote state for Module A yet.\r\n\r\nExample 2: We want to deploy a change to an existing environment that adds a new output \"e\" to Module A and uses it in Module B. We'd like to be able to validate and plan the changes for both modules before starting to apply anything, but we cannot right now because validation and planning of Module B will fail because the new output \"e\" is not in the remote state for Module A yet.\r\n\r\nWe think this is the only missing piece preventing us from moving away from a similar feature in terragrunt (generally preferring built-in capabilities of terraform where possible): https:\/\/terragrunt.gruntwork.io\/docs\/features\/execute-terraform-commands-on-multiple-modules-at-once\/#unapplied-dependency-and-mock-outputs. That also explains the use case in detail.\r\n\r\nAs I think about this more the `defaults` field could\/should remain a separate concept and so the proposal would be to add a new `mocks` field and a correlated `mock_operations` field that controls when they are used (which defaults to none of them).\r\n\r\nAt the risk of complicating the request, it would be interesting to explore the options for _just_ having a `mock_outputs` field that takes a list of operations (which defaults to none of them) and then includes a well-known indicator like \"(mock remote state value)\" in the plan output (similar to the existing \"(known after apply)\" indicator). Or something like that which achieves the same thing but doesn't require setting mock outputs for every output manually, which is one of the main gripes we have with the terragrunt implementation of this feature.\r\n\r\n> What happens in apply is just the execution of what was recorded in the plan, and in most cases a data source is not read at all during apply.\r\n\r\nThere could be a flag in the plan that says \"I was generated using mock values and cannot be applied\" and the \"apply\" operation just checks that and errors out with a helpful message if it is set? Or checks for the well-known \"(mock remote state value)\" in the plan and fails if it finds it if the feature is implemented that way. I think that would handle the case of a applying a pre-saved plan as well.","Thanks for the extra information @mpkuth! I think there's some confusion in terminology here, which makes the scope harder to pin down. In Terraform, \"validation\" is entirely offline, so validation cannot fail because of the data in the remote state, because the remote state is never read. It may also not be clear that `terraform_remote_state` is a data source just like any other, and follows the same lifecycle rules as every other data source. Other than validating its configuration, only action it has is `ReadDataSource`, which usually happens during plan, but can also be deferred to apply if necessitated by the configuration.\r\n\r\nSince there doesn't appear to be anything which `terraform_remote_state` could do to solve the problem here on its own, I think we could better classify this as a request for testing or mocking of resources in general. \r\n\r\nAs for workaround, I'd have to take some time to study what terragrunt is doing there, but it seems like you could test your plans with some configuration overrides and fake remote state staged specifically for the tests.\r\n","Thanks for the quick responses and information about validation. I must have misconfigured something else when trying this out in our project. After your comment I implemented an independent set of modules with the minimum configuration to demonstrate the use case and confirmed that does work in these cases.\r\n\r\n**module-a**\r\n\r\nconfig.tf\r\n```\r\nterraform {\r\n  backend \"s3\" {\r\n    bucket = \"kuth\"\r\n    key    = \"remote-state-test\/module-a\"\r\n    region = \"us-west-2\"\r\n  }\r\n}\r\n```\r\n\r\noutputs.tf\r\n```\r\noutput \"foo\" {\r\n  value = \"bar\"\r\n}\r\n```\r\n\r\n**module-b**\r\n\r\nconfig.tf\r\n```\r\nterraform {\r\n  backend \"s3\" {\r\n    bucket = \"kuth\"\r\n    key    = \"remote-state-test\/module-b\"\r\n    region = \"us-west-2\"\r\n  }\r\n\r\n  required_providers {\r\n    local = {\r\n      source  = \"hashicorp\/local\"\r\n      version = \"2.4.0\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nmain.tf\r\n```\r\ndata \"terraform_remote_state\" \"module_a\" {\r\n  backend = \"s3\"\r\n  config = {\r\n    bucket = \"kuth\"\r\n    key    = \"remote-state-test\/module-a\"\r\n    region = \"us-west-2\"\r\n  }\r\n}\r\n\r\nresource \"local_file\" \"test\" {\r\n  filename = \"test.txt\"\r\n  content  = \"TEST\"\r\n}\r\n\r\nresource \"local_file\" \"foo\" {\r\n  filename = \"foo.txt\"\r\n  content  = data.terraform_remote_state.module_a.outputs.foo\r\n}\r\n```\r\n\r\noutputs.tf\r\n```\r\noutput \"foo\" {\r\n  value = data.terraform_remote_state.module_a.outputs.foo\r\n}\r\n```\r\n\r\n---\r\n\r\nCase 1: Planning before the module that we depend on has been deployed at all\r\n\r\n```\r\n$ terraform plan\r\ndata.terraform_remote_state.module_a: Reading...\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\r\n  + create\r\n\r\nTerraform planned the following actions, but then encountered a problem:\r\n\r\n  # local_file.test will be created\r\n  + resource \"local_file\" \"test\" {\r\n      + content              = \"TEST\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"test.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\nPlan: 1 to add, 0 to change, 0 to destroy.\r\n\u2577\r\n\u2502 Error: Unable to find remote state\r\n\u2502\r\n\u2502   with data.terraform_remote_state.module_a,\r\n\u2502   on main.tf line 1, in data \"terraform_remote_state\" \"module_a\":\r\n\u2502    1: data \"terraform_remote_state\" \"module_a\" {\r\n\u2502\r\n\u2502 No stored state was found for the given workspace in the given backend.\r\n```\r\n\r\nWe'd like to be able to see what the plan would look like if the missing values were known.\r\n\r\nEven if we add the following to the `terraform_remote_state` data source we still see the same error.\r\n```\r\n  defaults = {\r\n    foo = \"missing\"\r\n  }\r\n```\r\n\r\n---\r\n\r\nCase 2: The dependency has already been deployed but we're adding a new output to it for use in the dependent module.\r\n\r\n```\r\n$ terraform plan\r\ndata.terraform_remote_state.module_a: Reading...\r\ndata.terraform_remote_state.module_a: Read complete after 1s\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\r\n  + create\r\n\r\nTerraform planned the following actions, but then encountered a problem:\r\n\r\n  # local_file.test will be created\r\n  + resource \"local_file\" \"test\" {\r\n      + content              = \"TEST\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"test.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\nPlan: 1 to add, 0 to change, 0 to destroy.\r\n\u2577\r\n\u2502 Error: Unsupported attribute\r\n\u2502\r\n\u2502   on main.tf line 17, in resource \"local_file\" \"foo\":\r\n\u2502   17:   content  = data.terraform_remote_state.module_a.outputs.foo\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 data.terraform_remote_state.module_a.outputs is object with no attributes\r\n\u2502\r\n\u2502 This object does not have an attribute named \"foo\".\r\n\u2575\r\n\u2577\r\n\u2502 Error: Unsupported attribute\r\n\u2502\r\n\u2502   on outputs.tf line 2, in output \"foo\":\r\n\u2502    2:   value = data.terraform_remote_state.module_a.outputs.foo\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 data.terraform_remote_state.module_a.outputs is object with no attributes\r\n\u2502\r\n\u2502 This object does not have an attribute named \"foo\".\r\n\u2575\r\n```\r\n\r\nIn this case, adding the following to the `terraform_remote_state` data source will allow us to see what we want (a complete plan with dummy values for missing remote state outputs).\r\n```\r\n  defaults = {\r\n    foo = \"missing\"\r\n  }\r\n```\r\n```\r\n$ terraform plan\r\ndata.terraform_remote_state.module_a: Reading...\r\ndata.terraform_remote_state.module_a: Read complete after 1s\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\r\n  + create\r\n\r\nTerraform will perform the following actions:\r\n\r\n  # local_file.foo will be created\r\n  + resource \"local_file\" \"foo\" {\r\n      + content              = \"missing\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"foo.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\n  # local_file.test will be created\r\n  + resource \"local_file\" \"test\" {\r\n      + content              = \"TEST\"\r\n      + content_base64sha256 = (known after apply)\r\n      + content_base64sha512 = (known after apply)\r\n      + content_md5          = (known after apply)\r\n      + content_sha1         = (known after apply)\r\n      + content_sha256       = (known after apply)\r\n      + content_sha512       = (known after apply)\r\n      + directory_permission = \"0777\"\r\n      + file_permission      = \"0777\"\r\n      + filename             = \"test.txt\"\r\n      + id                   = (known after apply)\r\n    }\r\n\r\nPlan: 2 to add, 0 to change, 0 to destroy.\r\n\r\nChanges to Outputs:\r\n  + foo = \"missing\"\r\n```\r\n\r\nHowever, as far as I know we cannot prevent applying a plan that was generated using any missing values. We'd prefer an option that results in an error if that is attempted. If this pre-planning\/dry-run ability was implemented as part of a \"test\" plan and terraform wouldn't let you apply \"test\" plans I think that would meet the need. But I do still wonder if it could just be a flag on the `terragrunt_remote_state` data source that lets the plan complete with the default values but then raises an error that makes the plan invalid and uses the \"Terraform planned the following actions, but then encountered a problem:\" output?\r\n\r\nAnyway, I appreciate the time and consideration. Just wanted to confirm that I do see the validation behavior you mentioned and provide a better example of what we're hoping to do natively. We can continue to use terragrunt for this until if\/when something similar makes its way into terraform.","@mpkuth have you looked at the new test framework that includes mocking? Does that capability satisfy your need here?"],"labels":["enhancement","terraform test"]},{"title":"Provide an automated way of using a default list of provider platforms when lock files get created\/updated","body":"### Terraform Version\n\n```shell\nTerraform v1.3.8\r\non darwin_amd64\n```\n\n\n### Use Cases\n\nWhen you have multiple platforms which are not your own in a shared environment it is quite annoying to constantly remember after doing work that your provider lock file has been altered and you need to also remember the correct incantation to add other processors and other OS's back to your lock file.\n\n### Attempted Solutions\n\nThere are lots of work arounds like making your own pre-commit hook, making a meanly named shell alias, a CI based solution, etc. However, rather than a bunch of people re-inventing the wheel something like a config\/preferences file in your home directory or something similar seems a lot easier.\n\n### Proposal\n\nAs mentioned above, my initial idea would probably a file in your home dir, but anything similar where you set it once and forget about it would be just fine.\n\n### References\n\nI couldn't find any, though lots of people mention a plugins directory. I don't think that's related, but I don't know how the provider lock system works.","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","Had a similar thought occur to me after searching my shell history for the correct `terraform providers lock` incantation I used last time I added a provider to the current terraform repo I'm working in.\r\n\r\n> my initial idea would probably a file in your home dir\r\n\r\nI think a per-workspace setting would be more appropriate, I might have to collaborate on different repos which run on different architectures\/OS's separately and would need different lockfiles generating.\r\n\r\nI wonder if this could be added to the `terraform {}` configuration block like we have `required_version` for terraform. Would keep it scoped to the workspace, and keeps it in source control too.\r\n\r\nPossibly allowing for falling back to `TERRAFORM_PROVIDER_LOCK_PLATFORMS` for the generic \"configure my machine for all repos\" use case?\r\n\r\n```hcl\r\nterraform {\r\n  provider_lock_platforms = [\r\n    \"darwin_arm64\",\r\n    \"linux_amd64\",\r\n  ]\r\n}\r\n```"],"labels":["enhancement","new"]},{"title":"Terraform plans to destroy existing resources even though configuration .tf file exists","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.5\r\n+ provider registry.terraform.io\/hashicorp\/aws v4.67.0\r\n+ provider registry.terraform.io\/hashicorp\/template v2.2.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nBelow is a sample .tf which simply creates EBS volume and instance. The contents of the .tf file won't matter for this issue as much as presence of file itself.\r\n\r\n```\r\nlocals {\r\n  instance_type     = \"t3.2xlarge\"\r\n  instance_ami      = \"ami-02a89cbcded39bf14\"\r\n  instance_key_pair = \"stage\"\r\n}\r\n\r\n\/\/ Create EBS Volume\r\nresource \"aws_ebs_volume\" \"test_ebs_vol\" {\r\n  availability_zone = \"us-east-1f\"\r\n  size              = 60\r\n  type              = \"gp2\"\r\n  encrypted         = \"true\"\r\n  tags = {\r\n    Name = \"test\"\r\n  }\r\n}\r\n\r\n\/\/ Create Instance\r\nresource \"aws_instance\" \"test_instance\" {\r\n  ami                    = local.instance_ami\r\n  instance_type          = local.instance_type\r\n  subnet_id              = \"${module.network.out_subnet_public_ids}\" [0]\r\n  source_dest_check      = \"true\"\r\n  iam_instance_profile   = \"aws-elasticbeanstalk-ec2-role\"\r\n  vpc_security_group_ids = [module.securitygroups.out_security_groups[\"FeatureEnvironmentsEc2\"].id]\r\n  key_name               = local.instance_key_pair\r\n  ebs_optimized          = \"true\"\r\n  root_block_device {\r\n    volume_size = 120\r\n  }\r\n  tags = {\r\n    Name = \"test\"\r\n  }\r\n  depends_on = [aws_ebs_volume.ease_fe_ebs_vol]\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nTF plan output which plans to destroy existing EBS volume resource defined in `test.tf`\r\n```\r\n# aws_ebs_volume.test_ebs_vol will be destroyed\r\n# (because aws_ebs_volume.test_ebs_vol is not in configuration)\r\n- resource \"aws_ebs_volume\" \"test_ebs_vol\" {\r\n    - type             = \"gp2\" -> null\r\n    ...\r\n  }\r\n```\r\n\r\n### Expected Behavior\r\n\r\n1. As `test.tf` exists locally in same directory (along with other local .tf files) where TF is being run.\r\n2. Expect TF to include `test.tf` just like every other file when calculating TF plan and applying it. \r\n\r\n### Actual Behavior\r\n\r\n1. Even though `test.tf` exists locally TF simply doesn't see it.\r\n2. Hence upon running TF plan we see that TF plans to destroy all resources defined in `test.tf`.\r\n3. Interestingly this is only happening for that 1 file, rest all resources defined in same directory are intact.\r\n\r\n### Steps to Reproduce\r\n\r\n1. In our TF initialized dir, create a `.tf` file with any sample config.\r\n2. Create resources with `terraform apply`.\r\n3. Subsequent `terraform plan` should show that it plans to destroy resources even though that file still exists.\r\n\r\n### Additional Context\r\n\r\nPlease note following:\r\n1. Files hierarchy is as follows, just standard setup.\r\n```\r\n\u251c\u2500\u2500 .terraform\/\r\n\u251c\u2500\u2500 .terraform.lock.hcl\r\n\u251c\u2500\u2500 provider.tf\r\n\u251c\u2500\u2500 backend.tf\r\n\u251c\u2500\u2500 test.tf\r\n\u2514\u2500\u2500 other.tf\r\n```\r\n2. All resources in the `test.tf` file were originally created via TF.\r\n3. `terraform state show` still shows the existing correct resources.\r\n4. Verified entire dir configuration is valid with `terraform validate`.\r\n5. Even tried refreshing remote state with `terraform refresh`.\r\n6. Everything was operating as expected until few days back when all of sudden one TF plan showed that it planned to destroy resources even though that particular file had no config changes.\r\n7. Solutions tried so far:\r\n- renaming the `test.tf` file.\r\n- dropping resources from state and manually importing them back.\r\n- upgrading TF and aws provider versions.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @UdhavPawar, thanks for filing this!\r\n\r\nIs this just the `aws_ebs_volume` resource that is affected? Other resources within `test.tf` do not show up as to-be-destroyed? You mentioned the content of the file doesn't matter, but I see you didn't include the `aws_instance` in the plan output as also being destroyed?\r\n\r\nAnd for clarification, none of your attempted solutions made any difference? The plan output was exactly the same?\r\n\r\nThanks!","> Hi @UdhavPawar, thanks for filing this!\r\n> \r\n> Is this just the `aws_ebs_volume` resource that is affected? Other resources within `test.tf` do not show up as to-be-destroyed? You mentioned the content of the file doesn't matter, but I see you didn't include the `aws_instance` in the plan output as also being destroyed?\r\n> \r\n> And for clarification, none of your attempted solutions made any difference? The plan output was exactly the same?\r\n> \r\n> Thanks!\r\n\r\nHey @liamcervante thanks for quick response. Yes, none of the mentioned attempts worked and correct, just to keep things simple took EBS volume as example but all resources in that file are affected. In production version of `test.tf`, I've volume, instance, public ALB, target groups and attaching instance to target groups (113 total resources all planned for deletion).","Thanks for the extra context. I haven't been able to replicate this yet. \r\n\r\nThe fact that changing the filename doesn't fix the issue makes me think there must be something in that config file that is \r\ncausing this, but I can't think what.\r\n\r\nSome more questions:\r\n\r\n\r\n  - Is it just AWS resources you have in that file that is being ignored?\r\n    - If so, if you add another resource in there from another provider, something simple like a time_sleep or a local_file, does that make any difference?\r\n  - If you distribute the resources in the bad file between the known good files, does that cause the other files to be ignored or do the resources start working?\r\n  - What's the environment this is is executing in? Is it happening locally, in Terraform Cloud or some other CI\/CD pipeline?\r\n\r\n> Everything was operating as expected until few days back when all of sudden one TF plan showed that it planned to destroy resources even though that particular file had no config changes.\r\n\r\n - Was there anything else to prompt this change? Like updating Terraform to v1.4.5. \r\n\r\nAnd just to cover all bases, you don't have a .terraformignore file or a .gitignore file that might be messing with things?\r\n\r\nThanks!","Those are some good Qs @liamcervante \r\n\r\n1. Yes that file has all aws resources. Added time_sleep for 5 secs but still same.\r\n```\r\nInitializing provider plugins...\r\n- Finding latest version of hashicorp\/time...\r\n- Reusing previous version of hashicorp\/template from the dependency lock file\r\n- Reusing previous version of hashicorp\/aws from the dependency lock file\r\n- Installing hashicorp\/time v0.9.1...\r\n- Installed hashicorp\/time v0.9.1 (signed by HashiCorp)\r\n- Using previously-installed hashicorp\/template v2.2.0\r\n- Using previously-installed hashicorp\/aws v4.67.0\r\n```\r\nResource definition in test.tf:\r\n```\r\nresource \"time_sleep\" \"wait_5_seconds\" {\r\n  create_duration = \"5s\"\r\n}\r\n```\r\nTF plan:\r\n```\r\n# time_sleep.wait_5_seconds will be created\r\n+ resource \"time_sleep\" \"wait_5_seconds\" {\r\n  + create_duration = \"5s\"\r\n  + id              = (known after apply)\r\n}\r\n```\r\nInterestingly, TF plans to add the new resource defined in same `test.tf` which confirms that TF sees the file, initially from symptom of TF planning to delete everything in that file, it gave me the impression that file is somehow invisible to TF.\r\n\r\n2. Tried distributing resource definitions inside `test.tf` to other `.tf` files (eg. `dynamodb.tf`) in same dir, but terraform validate failed with `Duplicate resource configuration`\r\n```\r\n\u2502 Error: Duplicate resource \"aws_lb_target_group_attachment\" configuration\r\n\u2502 \r\n\u2502   on test.tf line 171:\r\n\u2502  171: resource \"aws_lb_target_group_attachment\" \"test_tg_attachment_custom_ports\" {\r\n\u2502 \r\n\u2502 A aws_lb_target_group_attachment resource named \"test_tg_attachment_custom_ports\" was already declared at dynamodb.tf:166,1-76. Resource names must be unique per type in each module.\r\n```\r\n3. TF runs on self managed CI server (no TF Cloud) with remote state in S3. But could repro the issue locally as well.\r\n4. Nope, no changes whatsoever. It was running as expected few days back and all of sudden drift was detected.\r\n5. Yep no `.terraformignore` or `.gitignore` on CI server or local.","Thanks for the additional context. Given that the `time_sleep` doesn't exhibit the same behaviour as the `aws` resources, I wonder if there's something happening in the AWS provider to cause this. Or maybe, we could be seeing two separate bugs one marking the resources for destruction when it shouldn't and then Terraform adding in the wrong explanation as it doesn't understand why the resources are being destroyed.\r\n\r\nI've still be unable to replicate this locally. I'll see if anyone in the AWS team here has seen anything similar.\r\n\r\nIn the meantime, I did notice that in your configuration you had `depends_on = [aws_ebs_volume.ease_fe_ebs_vol]` in your `test_instance` resource definition. Is that referencing something from another file? Has anything happened to the referenced resource?","Sorry, one more follow up.\r\n\r\n> Tried distributing resource definitions inside test.tf to other .tf files (eg. dynamodb.tf) in same dir, but terraform validate failed with Duplicate resource configuration\r\n\r\nDid you also remove the definition from `test.tf` file? If not, could you try that? It would deal with the duplicate definition issue, and then we'd see if the behaviour of the actual resource is the same regardless of the file that we're in. I think Terraform is still detecting the file, but then marking the resources to be deleted for another reason and then potentially mislabelling that reason. ","Hi @UdhavPawar,\r\n\r\nI think the next step here would be to have a complete reproduction case, which can be run from scratch to others can verify the setup. This means including source and all commands exactly as executed. If you can't replicate this in a fresh environment (which may provide some clue itself as to what the difference might be), the full trace log from the plan would be the first place to continue investigation.\r\n\r\nThanks! ","@liamcervante \r\n1. No depends on is for resources in same .tf.\r\n2. Yes did drop definitions from `test.tf` when we added them to other working `.tf` files.\r\n\r\n@jbardin sounds like a plan. Only command I'm running is `terraform plan`. Apart from plan with `TF_LOG = DEBUG` and actual `test.tf` file, what other info do we need?\r\n\r\nThank You. ","@UdhavPawar, for complete information we would want to log `TF_LOG_CORE=trace`. Given the very strange behavior described here something is different from the way everyone else runs Terraform, so any and every detail you can provide would be helpful. It's best to package it up as a completely self-contained and scripted example that can reproduce the issue from a clean environment.","> @UdhavPawar, for complete information we would want to log `TF_LOG_CORE=trace`. Given the very strange behavior described here something is different from the way everyone else runs Terraform, so any and every detail you can provide would be helpful. It's best to package it up as a completely self-contained and scripted example that can reproduce the issue from a clean environment.\r\n\r\n@jbardin Looking at the symptoms, my suspicion is something is messed up with state for resources in `test.tf`. Given state file has sensitive data, I'm not entirely sure how can we repro the issue because it's not really the configuration itself in .tf from my understanding.\r\n\r\nPls lmk if you have an approach in mind to repro this issue, more than happy to execute it. ","Hello, this might be due to a reference to an already existing tfstate key in your terraform backend. So terraform tries to read the resources from that backend instead of creating a new one. Because the resources aren't the current tf files, terraform assumes you want to destroy or replace them. \nIf you copied the providers, make sure to change the key to the name of your current project.\nYou can also try to make a backend config file when you make a new tf project. This way you can define macros that add the name of the current project to the config file.\nThis is because variables aren't allowed when defining providers. "],"labels":["bug","waiting for reproduction","v1.4"]},{"title":"Cannot use SAS token with backend provider","body":"### Is there an existing issue for this?\n\n- [X] I have searched the existing issues\n\n### Community Note\n\n<!--- Please keep this note for the community --->\r\n\r\n* Please vote on this issue by adding a :thumbsup: [reaction](https:\/\/blog.github.com\/2016-03-10-add-reactions-to-pull-requests-issues-and-comments\/) to the original issue to help the community and maintainers prioritize this request\r\n* Please do not leave \"+1\" or \"me too\" comments, they generate extra noise for issue followers and do not help prioritize the request\r\n* If you are interested in working on this issue or have submitted a pull request, please leave a comment and review the [contribution guide](https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/blob\/main\/contributing\/README.md) to help.\r\n\r\n<!--- Thank you for keeping this note for the community --->\r\n\n\n### Terraform Version\n\n1.5.7\n\n### AzureRM Provider Version\n\n>3.74.0\n\n### Affected Resource(s)\/Data Source(s)\n\nbackend \"azurerm\n\n### Terraform Configuration Files\n\n```hcl\nterraform {\r\n  required_version = \">= 1.5.7\"\r\n\r\n  required_providers {\r\n    azurerm = {\r\n      version = \"~>3.74.0\"\r\n      source  = \"hashicorp\/azurerm\"\r\n    }\r\n  }\r\n\r\n  # Populated by command line arguments when running terraform init\r\n  backend \"azurerm\" {}\r\n}\r\n\r\nprovider \"azurerm\" {\r\n  features {}\r\n}\n```\n\n\n### Debug Output\/Panic Output\n\n```shell\nPlanning failed. Terraform encountered an error while generating this plan.\r\n\u2577\r\n\u2502 Error: unable to build authorizer for Resource Manager API: could not configure AzureCli Authorizer: obtaining tenant ID: running Azure CLI: exit status 1: ERROR: Please run 'az login' to setup account.\r\n\u2502 \r\n\u2502   with provider[\"registry.terraform.io\/hashicorp\/azurerm\"],\r\n\u2502   on providers.tf line 16, in provider \"azurerm\":\r\n\u2502   16: provider \"azurerm\" {\r\n\u2502 \r\n\u2575\n```\n\n\n### Expected Behaviour\n\nA valid plan\n\n### Actual Behaviour\n\nError indicating that the backend cannot connect to azure correctly\n\n### Steps to Reproduce\n\n\r\n    terraform init \\\r\n        -backend-config=\"subscription_id=${azure_subscription_id}\" \\\r\n        -backend-config=\"storage_account_name=xxx\" \\\r\n        -backend-config=\"container_name=tf-state-azure-infrastructure\" \\\r\n        -backend-config=\"key=azure-infrastructure.tfstate\" \\\r\n        -backend-config=\"sas_token=${azure_storage_account_sas_token}\"\r\n\r\nterraform plan -out=tfplan\n\n### Important Factoids\n\n_No response_\n\n### References\n\n_No response_","comments":["@antgustech Thank you for taking the time to open this issue. Issues related to the azurerm backend belong in the terraform core repo because the code for the azurerm backend is housed in this terraform core repo, so I have transferred this issue over for you. ","Hi @rcskosir, the error here is from a provider, not from the backend. I think the problem however is a misunderstanding of the configuration, backends are not providers, and configuring one does not configure the other. The provider must have its own configuration or be able to locate it from the running environment. "],"labels":["bug","question","backend\/azure"]},{"title":"Unnecessary iterating through \"env:\/\" S3 bucket contents when initialising without a workspace","body":"### Terraform Version\n\n```shell\nTerraform v1.5.7\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.17.0\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.1\r\n+ provider registry.terraform.io\/mongey\/kafka v0.5.4\n```\n\n\n### Terraform Configuration Files\n\n\tterraform init \\\r\n\t-backend-config=\"bucket=${BACKEND_BUCKET}\" \\\r\n\t-backend-config=\"key=${BACKEND_KEY}\" \\\r\n\t-backend-config=\"region=${AWS_DEFAULT_REGION}\" \\\r\n\t-reconfigure\n\n### Debug Output\n\nRunning terraform init:\r\n\r\n```\r\n2023-09-28T23:27:53.073Z [DEBUG] [aws-sdk-go] DEBUG: Request s3\/ListObjects Details:\r\n---[ REQUEST POST-SIGN ]-----------------------------\r\nGET \/base-xxx-dev-au-tfstate.iress.online?max-keys=1000&prefix=env%3A%2F HTTP\/1.1\r\n\r\n2023-09-28T23:27:53.808Z [DEBUG] [aws-sdk-go] DEBUG: Response s3\/ListObjects Details:\r\n---[ RESPONSE ]--------------------------------------\r\nHTTP\/1.1 200 OK\r\n```\r\n(Returns first 1000 entries)\r\n\r\n```\r\n2023-09-28T23:27:53.817Z [DEBUG] [aws-sdk-go] DEBUG: Request s3\/ListObjects Details:\r\n---[ REQUEST POST-SIGN ]-----------------------------\r\nGET \/base-xxx-dev-au-tfstate.iress.online?marker=env%3A%2FIPS_ci_486013ef_1818%2Fips-ecs-prechecks%2Fterraform.tfstate&max-keys=1000&prefix=env%3A%2F HTTP\/1.1\r\n```\r\n\r\n(Returns next 1000 entries etc until no more)\r\n\r\nOutput: \r\n```\r\nSuccessfully configured the backend \"s3\"! Terraform will automatically\r\nuse this backend unless the backend configuration changes.\r\n```\r\n\r\nThere is a message that pertains to the backend at this point: `2023-09-28T23:33:50.611Z [TRACE] Meta.Backend: backend *s3.Backend does not support operations, so wrapping it in a local backend`\r\n\r\n**Then repeats the entire process again and runs ListObjects over 1000s of bucket objects where `prefix=env:\/`**\r\n\r\nFinally makes a request for the specific bucket object containing my state;\r\n\r\n```\r\n2023-09-28T23:33:50.927Z [DEBUG] [aws-sdk-go] DEBUG: Request s3\/GetObject Details:\r\n---[ REQUEST POST-SIGN ]-----------------------------\r\nGET \/base-xxx-dev-au-tfstate.iress.online\/tf-module-msk-kafka-management\/tf-module-msk-kafka-management.tfstate HTTP\/1.1\r\n```\r\n\r\nAnd continues on as expected:\r\n`Initializing modules...`\r\n\n\n### Expected Behavior\n\nI am using the S3 backend for remote statefile storage and not using a specific workspace.  I expect that TF will use the provided bucket name name and key to directly access my statefile.\r\n\r\nThe docs say:\r\n\r\n[workspace_key_prefix](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/s3#workspace_key_prefix) - (Optional) Prefix applied to the state path inside the bucket. This is only relevant when using a non-default workspace. Defaults to env:.\r\n\r\nAs I am using the default workspace, I expect the prefix to not be applied.\n\n### Actual Behavior\n\nDebug logging indicates that before my statefile is read, TF iterates through all the objects in the `env:\/` path via `s3\/ListObjects`.  As this is a developer sandbox account, there are 10s of thousands of objects here (mostly empty as things have been undeployed and TF provides no easy way to completely cleanup state).  \r\nIt is not clear from the logs what TF is looking for - once the paginated iteration is complete it just moves on with no message to indicate it found\/didn't find what it was looking for.\r\n\r\nAt this point it says `Successfully configured the backend \"s3\"!` but then iterates through all the objects again.\r\n\r\nOnce the iteration is complete, `s3\/GetObject` is called for the first time on my desired state file.\r\n\r\nWhy is this a problem?  Because it is extremely slow and bandwidth intensive, which is particularly noticeable when trying to work when tethered to a mobile device.  It only gets slower over time as deployments (like CI branch verification) grow.\n\n### Steps to Reproduce\n\n```\r\n\tterraform init \\\r\n\t-backend-config=\"bucket=${BACKEND_BUCKET}\" \\\r\n\t-backend-config=\"key=${BACKEND_KEY}\" \\\r\n\t-backend-config=\"region=${AWS_DEFAULT_REGION}\" \\\r\n\t-reconfigure\r\n```\n\n### Additional Context\n\nI don't really want to use workspaces, but I discovered a workaround.  By simply adding a custom workspace prefix, I can force it to not look at `env:\/` and use a new path which has nothing in it:\r\n\r\n```\r\n\t-backend-config=\"workspace_key_prefix=${KEY_PREFIX}\"\r\n```\r\n\r\n`KEY_PREFIX` can be anything as it isn't ultimately used (my statefile is written and read from its expected location in the root of the bucket).  The [docs](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/s3#s3-state-storage) suggest that the `workspace_key_prefix` is only used when using a non-default workspace, which seems to be technically true except for the iterating through the objects in the `workspace_key_prefix` path, the output of which seems to be ignored.\r\nI can safely change this prefix even for projects with existing state.\r\n\r\nOn my home broadband connection it take `init` from 1m05s down to 10s.  \r\n\n\n### References\n\n_No response_","comments":["Similar to this ancient issue: https:\/\/github.com\/hashicorp\/terraform\/issues\/22492 except for me it is not manifesting in \"access denied\" because I have full access in the sandbox environment","Thanks for reporting this, @patrickherrera.\r\n\r\nI agree that this behavior seems non-ideal but since the what your described is the S3 backend working as designed it seems like any improvement here will involve changing its design, and so I'm going to reclassify this as an enhancement because that's how we represent changes that need technical design work vs. just fixing a defect in the implementation to make it better implement the intended design.\r\n\r\nI believe the backend behaves in this way because it wants to know _if_ you are using non-default workspaces, by checking if any are present in your bucket. Setting the workspace prefix to something that won't match any keys at all seems like a plausible workaround in that case, since you are effectively telling the backend to look for non-default workspaces in a location where you will never place any.\r\n\r\nI'm not an S3 backend expert so I will leave the details of this for someone else to ponder, but I do have some initial questions that came to my mind while I was thinking about the issue as reported, which I'll share here just in case they are useful to someone working on this later:\r\n- Is it actually even necessary to enumerate all of the workspaces just for initialization? I have a suspicion that this is happening as a side-effect of something else Terraform wants to do during init, such as finding out which providers are used across all of the workspace states, or similar. If that is true then it might be unavoidable, but I think worth checking that first.\r\n- Assuming that we _do_ need to enumerate all workspaces during init, is there a more efficient way to do it than scanning the entire bucket contents? I think I remember it being implemented this way because S3's own API only supported filtering by prefix, but I'm not sure. Maybe there is some other filterable characteristic that we could offer in addition to prefix matching that might be more appropriate for buckets containing lots of unrelated objects.\r\n- If all else fails, perhaps it would be plausible to have a setting which tells the backend to just assume that no non-default workspaces exist, and thus skip this extra prefix-based enumeration altogether. This wouldn't be my first preference for a solution, but it seems like it could be pragmatic for those who want to put state in a large bucket with lots of unrelated objects and ensure that each configuration has exactly one explicitly-specified S3 object key.\r\n\r\n","When `terraform init` is called with `-reconfigure`, among other flags, the list of workspaces is requested from whichever backend is configured. See https:\/\/github.com\/hashicorp\/terraform\/blob\/2664c062c92c8ba043c1c66ff586243317f975d4\/internal\/command\/meta_backend.go#L218C1-L218C1, for example.\r\n\r\nThe backend lists the workspaces again during configuration to see if the workspace exists. This could be replaced with a simple check for the object itself.\r\n\r\nIn the meantime, you can use `terraform workspace list` and `terraform workspace delete` to remove unused workspaces","Thanks all for looking at this.  I might try scripting something up but not sure of the best approach since there are many dozens of projects owned by different teams (many of which are long abandoned with empty state files left behind).  I can probably just delete S3 files manually based on some criteria, as long as I delete any DynamoDB hashes at the same time.\r\nAnd going forward I'm happy to use a project-specific `workspace_key_prefix`"],"labels":["enhancement","backend\/s3"]},{"title":"Kubernetes state backend uses private label keys","body":"### Terraform Version\r\n\r\n```shell\r\nv1.5.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nn\/a\r\n\r\n### Debug Output\r\n\r\nn\/a\r\n\r\n### Expected Behavior\r\n\r\nWhen storing state into Kubernetes, Terraform (and clones) should use label and annotation keys prefixed with a DNS domain.\r\n\r\n\r\n\r\nPlausible example:\r\n```yaml\r\n...\r\n   metadata:\r\n     labels:\r\n        app.kubernetes.io\/managed-by: terraform\r\n        state.terraform.io\/used-for-state-storage: \"true\"\r\n        state.terraform.io\/name-suffix: tfstate\r\n        state.terraform.io\/workspace-name: default\r\n```\r\n\r\n### Actual Behavior\r\n\r\nActual example:\r\n```yaml\r\n...\r\n   metadata:\r\n     labels:\r\n        app.kubernetes.io\/managed-by: terraform\r\n        tfstate: \"true\"\r\n        tfstateSecretSuffix: tfstate\r\n        tfstateWorkspace: default\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n- Set up a Kubernetes cluster\r\n- Set up Terraform to store state into that cluster\r\n- `apply` some infrastructure code\r\n- `kubectl get secrets --all-namespaces`\r\n\r\n\r\n### Additional Context\r\n\r\nTo learn about Kubernetes' conventions for labels (and annotations), see https:\/\/github.com\/kubernetes\/community\/blob\/586e54c54a4225612e8a2e956a0e36509604b548\/contributors\/devel\/sig-architecture\/api-conventions.md#label-selector-and-annotation-conventions\r\n\r\n### References\r\n\r\n_No response_","comments":["Using a private label may conflict with a cluster operator's policy; for example, they might have an allow list of private labels. Obviously it's feasible to fix this by forking Terraform to make a binary that complies with that policy, but using a prefixed key in the official upstream code is a better approach.","An FYI ping to official codeowners @jrhouston @alexsomesan. "],"labels":["bug","backend\/k8s","new"]},{"title":"Terraform Import does not work with Terraform Cloud Varsets","body":"TLDR; The variable set feature for Terraform Cloud has not been properly integrated with Terraform CLI local operations like `terraform import`, meaning that this command fails saying an input \"is not set, and has no default value\" if that input is required for your application and also would be provided by the remote variable set.\r\n\r\n### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.7\r\non darwin_arm64\r\n+ provider registry.terraform.io\/auth0\/auth0 v1.0.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nThe `versions.tf` file:\r\n```terraform\r\nterraform {\r\n  required_version = \">= 1.4.6\"\r\n\r\n  cloud {\r\n    organization = \"<ORG_NAME>\"\r\n    workspaces {\r\n      tags = [\"auth0\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n(note that in the actual project it's not ORG_NAME but rather our actual org name)\r\n\r\nThe `variables.tf` file:\r\n```terraform\r\nvariable \"AUTH0_DOMAIN\" {}\r\nvariable \"AUTH0_CLIENT_ID\" {}\r\nvariable \"AUTH0_CLIENT_SECRET\" {}\r\nvariable \"API_DOMAIN\" {}\r\nvariable \"FRONTEND_DOMAIN\" {}\r\n```\r\n\r\nThe `main.tf` is not relevant other than that it declares resources like an `auth0_client`\r\n\r\n### Debug Output\r\n\r\nThe relevant line is right before the failure:\r\n```\r\n2023-09-27T11:29:45.856+0100 [TRACE] cloud: retrieving variables from workspace staging-auth0\/<ORG_NAME> (ws-<REDACTED>)\r\n```\r\n\r\n(I've redacted the values in the `<>` because they might expose our settings more precisely.)\r\n\r\n### Expected Behavior\r\n\r\nRunning `terraform import auth0_client.frontend <CLIENT_ID>` should import that client.\r\n\r\n### Actual Behavior\r\n\r\nThe process fails with the error:\r\n\r\n```\r\nWarning: Value for var.AUTH0_CLIENT_SECRET unavailable\r\n\u2502 \r\n\u2502 The value of variable \"AUTH0_CLIENT_SECRET\" is marked as sensitive in the remote workspace. This operation always runs locally, so the value for that variable is not\r\n\u2502 available.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: No value for required variable\r\n\u2502 \r\n\u2502   on \/Users\/zack\/code\/infrastructure\/applications\/auth0\/variables.tf line 25:\r\n\u2502   25: variable \"API_DOMAIN\" {}\r\n\u2502 \r\n\u2502 The root module input variable \"API_DOMAIN\" is not set, and has no default value. Use a -var or -var-file command line argument to provide a value for this variable.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: No value for required variable\r\n\u2502 \r\n\u2502   on \/Users\/zack\/code\/infrastructure\/applications\/auth0\/variables.tf line 27:\r\n\u2502   27: variable \"FRONTEND_DOMAIN\" {}\r\n\u2502 \r\n\u2502 The root module input variable \"FRONTEND_DOMAIN\" is not set, and has no default value. Use a -var or -var-file command line argument to provide a value for this variable.\r\n\u2575\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create your terraform workspace in terraform cloud.\r\n2. Add it to a Terraform Cloud Project - see https:\/\/www.hashicorp.com\/blog\/terraform-cloud-adds-projects-to-organize-workspaces-at-scale\r\n3. Provide some variables via the variable set feature, and some variables via the workspace's own variables page.\r\n5. Run a `terraform import ...` command. It should fail.\r\n\r\nThe fact that the import command is unable to download private variables is already documented in #26494, so that is not the new issue here. **The new issue here is** that the `import` command cannot access non-sensitive variables provided by variable sets.\r\n\r\n### Additional Context\r\n\r\nNote that the `terraform plan` command is able to access all variables and so it does not fail. Additionally, note that the `terraform import` command only throws an error for missing variables for those provided by the variable set, so it is able to access variables provided at the workspace level.\r\n\r\nThis is how I narrowed the issue down to be that `terraform import` specifically cannot access variables defined by the variable sets feature in terraform cloud.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for reporting! Further investigation indicates that this problem is not specific to project varsets, but is a gap in support for variable sets as a whole when looking up variables for local operations - that is, the `cloud` integration only looks for non-sensitive variables defined locally within the workspace and ignores variable sets entirely, whether or not they're scoped to a specific project. We'll triage this with the appropriate Terraform Cloud team.\r\n\r\nIn the meantime, you may find [the `import` block](https:\/\/developer.hashicorp.com\/terraform\/language\/v1.5.x\/import) that was added in Terraform 1.5 useful as a way to move forward. Unlike the standalone `import` command, `import` blocks run remotely in TFC as part of the plan and apply workflow, so they have full access to all credentials and variables, including those provided as variable sets.\r\n"],"labels":["bug","new","cloud"]},{"title":"Terraform S3 Backend Customisation - DynamoDB Free Option","body":"\r\nDescription:\r\n\r\nOverview\r\nThis PR introduces a significant enhancement to Terraform's S3 backend configuration. The objective is to provide a DynamoDB-free alternative for state file locking, making our infrastructure management more flexible and cost-efficient.\r\n\r\nChanges Made\r\nConfiguration Update: I have modified the Terraform backend *s3* to include a new *lock_storage_type* option in the s3 backend configuration block. Users can now choose between two options:\r\n\r\n\"DynamoDB\" (the default): The traditional DynamoDB-based state locking mechanism.\r\n\"S3Bucket\": The new DynamoDB-free alternative for state locking.\r\nUsage:\r\n\r\nTo implement this change, users simply need to set lock_storage_type = \"S3Bucket\" in their Terraform configuration. This will enable state locking using the S3 bucket itself, without the need for a DynamoDB table.\r\nCompatibility: We have ensured that this customisation is fully compatible with AWS S3 buckets and is backward compatible.\r\n\r\nGitHub Commit: The corresponding code changes can be found in this [GitHub commit](https:\/\/github.com\/ravinitp\/terraform\/commit\/883b454ff34decf9335eccf81114b9493820f369).\r\n\r\nHow to Use\r\nFor detailed information on how to implement this customisation, please refer to the official Terraform documentation, where I have added a dedicated section explaining the new lock_storage_type option and providing usage examples.\r\nexample\r\n```\r\nterraform {\r\n\r\n    backend \"s3\" {\r\n      bucket = \"terraform-backend-ravi\"\r\n      region = \"ap-south-1\"\r\n      skip_region_validation = true\r\n      skip_credentials_validation = true\r\n      force_path_style = true\r\n      key = \"terraform.tfstate\"\r\n      lock_storage_type = \"S3Bucket\"\r\n      access_key = \"<access_key>\"\r\n      secret_key = \"<Secrete_Key>\"\r\n    }\r\n}\r\n```\r\n\r\nFeedback\r\nI encourage terraform community and users to try out this feature and provide feedback. Your insights and suggestions are highly valuable to me and will help me further refine and improve this customisation.\r\n\r\nConclusion\r\nThis PR represents a significant step in making Terraform's state management more flexible and cost-effective. I look forward to your feedback and the positive impact this change will have on our infrastructure provisioning and management processes.\r\n\r\nPlease review the code and provide any feedback or suggestions to enhance its clarity and completeness.","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33941) <br\/>All committers have signed the CLA.","Thanks for this submission. I have notified the appropriate team. To set expectations, S3 backend PRs are only irregularly reviewed. The team is just wrapping up a big upgrade to the latest AWS SDK. Thanks again for the submission!","Thanks for working on this, @ravinitp!\r\n\r\nThe AWS provider team at HashiCorp is also responsible for maintaining this backend, so I will leave the detailed review for them, but I remember that back in https:\/\/github.com\/hashicorp\/terraform\/issues\/27070 we'd concluded that S3's consistency model wasn't sufficient for race-free locking and so I expect we'll want to either see that something has changed in S3 since that research or that the conclusion was based on an incorrect premise.\r\n\r\nIf you have any information you referred to that suggested that locking in S3 alone would be safe, I expect that would help the future reviewers in considering this change.\r\n\r\nFor what it's worth, the note that was quoted in the closing comment in the other issue still seems to be present in the documentation today, and there's some worked examples below where one indicates that there's no guaranteed behavior if one client reads while another client's write is still in progress. That leads me to believe that S3's consistency model is still not strong enough to be used as a locking mechanism, but I've not researched it in detail.\r\n\r\nThanks again!\r\n","> Thanks for working on this, @ravinitp!\r\n> \r\n> The AWS provider team at HashiCorp is also responsible for maintaining this backend, so I will leave the detailed review for them, but I remember that back in #27070 we'd concluded that S3's consistency model wasn't sufficient for race-free locking and so I expect we'll want to either see that something has changed in S3 since that research or that the conclusion was based on an incorrect premise.\r\n> \r\n> If you have any information you referred to that suggested that locking in S3 alone would be safe, I expect that would help the future reviewers in considering this change.\r\n> \r\n> For what it's worth, the note that was quoted in the closing comment in the other issue still seems to be present in the documentation today, and there's some worked examples below where one indicates that there's no guaranteed behavior if one client reads while another client's write is still in progress. That leads me to believe that S3's consistency model is still not strong enough to be used as a locking mechanism, but I've not researched it in detail.\r\n> \r\n> Thanks again!\r\n\r\nThanks @apparentlymart for you valuable time spent for code review. I think the lock mechanism used in this PR is not atomic. Therefore marking this PR to draft.\r\n\r\nAlthough I did a POC for terraform state lock using S3 alone and tested with concurrent 100 threads and It seems to be great fit. But I want this [POC](https:\/\/github.com\/ravinitp\/s3-object-lock-demo\/pull\/1) to get reviewed by AWS team.\r\nhttps:\/\/github.com\/ravinitp\/s3-object-lock-demo\/pull\/1\r\n\r\nI will get back once I get approval on this.\r\n\r\nThanks again :D \r\n"],"labels":["enhancement","backend\/s3"]},{"title":"Feature\/ks3 backend","body":"Hello, \r\n\r\nThis PR provides a new backend `ks3` that terraform can store state remotely in KingsoftCloud KS3 with locking.\r\n\r\n#33876 \r\n\r\n### NEW FEATURES\r\n","comments":["## Test Pass\r\n--- PASS: TestLockDurationParse (0.00s)\r\n    --- PASS: TestLockDurationParse\/hour (0.00s)\r\n    --- PASS: TestLockDurationParse\/minute (0.00s)\r\n    --- PASS: TestLockDurationParse\/unlimited (0.00s)\r\n    --- PASS: TestLockDurationParse\/ignore (0.00s)\r\n    --- PASS: TestLockDurationParse\/0_minute (0.00s)\r\n    --- PASS: TestLockDurationParse\/second (0.00s)\r\n--- PASS: TestBackendStateFile (0.00s)\r\n    --- PASS: TestBackendStateFile\/default (0.00s)\r\n    --- PASS: TestBackendStateFile\/ws-development (0.00s)\r\n    --- PASS: TestBackendStateFile\/prefix-dev (0.00s)\r\n--- PASS: TestRemoteClientWithEncryption (1.68s)\r\n--- PASS: TestRemoteClientWithPrefix (1.76s)\r\n--- PASS: TestRemoteClient (2.07s)\r\n--- PASS: TestRemoteLocks (4.10s)\r\n--- PASS: TestBackendWithPrefix (6.67s)\r\n--- PASS: TestBackend (8.75s)\r\n--- PASS: TestDeadLockRelease (27.16s)\r\n    --- PASS: TestDeadLockRelease\/duration_3_minute_not_required (2.96s)\r\n    --- PASS: TestDeadLockRelease\/unlimited_with_3_minutes (1.96s)\r\n    --- PASS: TestDeadLockRelease\/duration_3_hours_required (2.72s)\r\n    --- PASS: TestDeadLockRelease\/unlimited_with_3_hours (1.84s)\r\n    --- PASS: TestDeadLockRelease\/duration_3_hours (2.66s)\r\n    --- PASS: TestDeadLockRelease\/duration_3_minute_required (1.86s)\r\n    --- PASS: TestDeadLockRelease\/duration_0_minute (2.02s)\r\n    --- PASS: TestDeadLockRelease\/ignore_old_lock_with_3_minutes (2.87s)\r\n    --- PASS: TestDeadLockRelease\/ignore_old_lock_with_3_hours (8.27s)\r\n","Thanks for this submission. "],"labels":["enhancement","new-backend"]},{"title":"list resource addresses on `plan` (`terraform plan -summary`)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.7\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nfrom plan, easily get what would be added\/changed\/deleted \r\n\r\nthe current plan output:\r\n- is too long to get which resources would be added\/changed\/deleted.\r\n- has summary just each number of them, not the addresses(name), at the end.\r\n\r\nhttps:\/\/www.reddit.com\/r\/Terraform\/comments\/10m7jdd\/show_only_the_resources_that_will_be_changed_in\/\r\npeople have same requirements i think\r\n\r\n\r\n### Attempted Solutions\r\n\r\n![image](https:\/\/raw.githubusercontent.com\/flavono123\/terraform-plan-summary\/main\/assets\/demo.gif)\r\n\r\n\r\nhttps:\/\/github.com\/flavono123\/terraform-plan-summary\r\n\r\n### Proposal\r\n\r\n\ud83d\udc46  can i contribute the above feature, `-summary` (or with another proper name) flag to `plan`?\r\ni'm also open for any feedbacks :) \r\n\r\n### References\r\n\r\ncannot found","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","I really like that feature!! I usually work with a lot of cloud resources and it's tedious watching all of them."],"labels":["enhancement","new"]},{"title":"`terraform providers mirror` option to only update network mirror indexes, and not download anything new","body":"### Terraform Version\n\n```shell\nTerraform v1.5.7\n```\n\n\n### Use Cases\n\nhi!\r\nWhen I use `terraform providers mirror` command to generate json configuration of providers, I find it seems unable to download providers from local file systems. When I use `terraform init`, it can download fast but not in `terraform providers mirror`. \n\n### Attempted Solutions\n\nCan `terraform providers mirror` command download from local-file-system?\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Excuse me, is there any other way to automatically generate the `index.json` and `version.json` corresponding to the provider in addition to the `terraform providers mirror` command?","Hi @shanye997,\r\n\r\nThe `terraform providers mirror` command is intended for retrieving provider packages from their origin registry and constructing a local mirror directory based on what the origin registry reports. It intentionally ignores any custom installation method configuration because we assume that the person running this command is probably using it to update their configured mirror to match the origin registry, and so it would be annoying to have to temporarily unconfigure the mirror setting in the CLI configuration just to run this command to update that same mirror.\r\n\r\nIf I understand your question correctly it seems like you are trying to use one mirror to generate another. Specifically, I think you'd like to use a filesystem mirror to generate the metadata files needed for using the directory as a network mirror. Is that right?\r\n\r\n","Hi @apparentlymart ,\r\n\r\nIt is right!  I have the filesystem mirrors and I want to generate the network mirror based on them. Is there any way to generate the metadata files needed for the network mirror?","Hi @shanye997,\r\n\r\nI don't think there's any built-in command to achieve exactly that today. With today's Terraform I think you would either need to generate the index files manually yourself or use `terraform providers mirror` and accept that it will download some provider packages as part of its work.\r\n\r\nIt seems possible in principle to offer an option on `terraform providers mirror` to tell it to _only_ generate or update indexes for whatever provider packages are already available in the mirror directory. In that case it would not modify the packages in the mirror directory at all but would scan the directory to see which packages are already present, and then regenerate all of the index files to match.\r\n\r\nThat would then enable an alternative workflow for building a mirror:\r\n- Download the provider packages you want using whatever tools you might normally use to download zip files over the internet. Arrange them into the \"packed layout\" as documented for a filesystem mirror.\r\n- Run the new index-generating operation against that directory. It would search for files matching the bundled mirror layout and then generate the JSON index files based on what it found.\r\n- Repeat this process each time you want to mirror a new package, adding the new .zip files to the existing mirror directory and then re-running the index generator to add the new packages to the index.\r\n\r\nThis compromise is recognizing that generating the index files requires calculating package checksums in a way that is Terraform-specific and therefore hard to achieve outside of Terraform, whereas downloading .zip files and placing them into a directory structure is a more standard operation that's relatively easy to achieve outside of Terraform using typical scripting techniques.\r\n\r\nWould something like what I described above help you meet your goals? Thanks!\r\n","Hi @apparentlymart ,\r\nThe workflow you described can solve my problem perfectly. The current implementation of network mirror requires zip files and json index files. If terraform can provide a command to generate json index files based on zip files, it will greatly facilitate my building the network mirror based on the downloaded package. The `terraform providers mirror` command downloads the zip files every time it is executed. This will cause a lot of network overhead for me and is also very dependent on network conditions. Thank you for your reply and solution\uff01","Thanks, @shanye997!\r\n\r\nI've updated the labels and summary of this issue to reflect what we just discussed.\r\n\r\nThis issue will still be subject to our usual prioritization process and so I can't promise work on this in the near future, but one of the ways we prioritize is to use the number of :+1: reactions on the original issue as a proxy for potential impact, so if anyone else finds this issue and would find what I described above useful please add a reaction to the original issue comment (_not_ to this reply!) to register your interest.\r\n","Thanks a lot, @apparentlymart !\r\nThank you for your patient reply and discussion, I am looking forward to this new feature !"],"labels":["enhancement","cli","providers"]},{"title":"Consider adding enum or sum types ","body":"### Terraform Version\n\n```shell\n1.5.7\n```\n\n\n### Use Cases\n\nThere would be many use cases in which an enum type may come to help.\r\nRight now, every time one wants an enum-like behavior, if one is lucky enough that the variable is a string, one has to come up with a regex validation rule, otherwise one has to come up with more complex (and arguably less readable than an enum definition) validation rules.\r\n\r\nMoreover, with an enum type, it should become easier to implement the feature of having a variable module source (something similar to what is asked by https:\/\/github.com\/hashicorp\/terraform\/issues\/25587 or https:\/\/github.com\/hashicorp\/terraform\/issues\/33793). In fact, in principle, if you restrict the source field to `string` or `enum`, you can always know beforehand all the possible values for the field, and install all the dependencies beforehand, then at run-time only choose which one to use.\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\n```hcl\r\n# first proposal\r\nvariable \"my_string_enum\" {\r\n  type        = enum(string, \"string1\", \"string2\", \"string3\")\r\n  description = \"enum of strings\"\r\n}\r\n\r\nvariable \"my_object_enum\" {\r\n  type = enum(object({\r\n    id   = number\r\n    name = string\r\n  }), object({\r\n    id   = 1\r\n    name = \"first\"\r\n  }), object({\r\n    id   = 2\r\n    name = \"second\"\r\n  }))\r\n  description = \"enum of objects\"\r\n}\r\n\r\n# second proposal\r\n# exploiting the default field\r\nvariable \"my_object_enum\" {\r\n  type = enum(object({\r\n    id   = number\r\n    name = string\r\n  }))\r\n  description = \"enum of objects\"\r\n  default     = [\r\n    {\r\n      id   = 1\r\n      name = \"one\"\r\n    },\r\n    {\r\n      id   = 2\r\n      name = \"two\"\r\n    },\r\n  ]\r\n}\r\n```\n\n### References\n\n- #25587 \r\n- #33793 ","comments":["Thanks for sharing this proposal, @lorenzofelletti.\r\n\r\nI remember that _somewhere_ in our big heap of issues there are suggestions for two related but not equivalent features:\r\n\r\n- Something like [tagged unions](https:\/\/en.wikipedia.org\/wiki\/Tagged_union) (sum types) which somehow allows declaring that a variable can match of any one of a set of type constraints.\r\n- A similar fixed set of valid values but without requiring that they are all of the same type, possibly mixed with the previous point so that it would be possible to say something like \"can be any object of this object type _or_ the fixed string `\"something\"`.\r\n\r\nIn order to keep the language coherent I think we'd want to choose which subset of these goals we actually want to meet and design a single feature that solves everything in that subset; there's enough overlap of use-cases between these different approaches that it would likely be confusing to have a separate language feature for each of them.\r\n\r\nI wasn't able to quickly find the other issues I'm thinking of above. Hopefully I or someone else will find them in future triage walks and will be able to link them all together retroactively.\r\n\r\n---\r\n\r\nIn the past I've prototyped adopting some of the ideas from [CUE](https:\/\/cuelang.org\/), whose design offers the insight that it's possible to unify the concept of types with the concept of values by defining a concrete value like `\"hello\"` as a subtype of `string`.\r\n\r\nUnder that model, we could imagine saying that a variable can have the type `string` to permit any string, or could have a type like `\"foo\" || \"bar\"` to permit only exactly those two strings, or even more complicated interactions like my example above of `\"something\" || object({foo = string})`.\r\n\r\nThat way of thinking seems to neatly solve all of the above variations under a single model, but of course does so with a higher level of complexity than solving just one problem in isolation, such as the enum proposal in this issue.\r\n\r\nI was prototyping those in the context of my personal project `cty` that Terraform is built around, rather than within Terraform itself, and so that was my own personal work rather than work I did while wearing my \"Terraform Team at HashiCorp\" hat. I suspect that to be successful it would be better to design this at the Terraform and HCL layer and then see what supporting help those might need from `cty`, if anything.\r\n\r\n---\r\n\r\nI assume the intent of the \"upstream\" label here was to represent that the type constraint expression syntax belongs to HCL rather than directly to Terraform. However, the design of that part of HCL tends to be primarily influenced by Terraform's needs, and so I expect it would end up being a Terraform contributor that ultimately figures out what we want to do here and drives it to implementation; there are no separate HCL maintainers to drive this independently.\r\n","Thank you for your insightful reply, @apparentlymart.\r\n\r\nI really like the idea from CUE of using `\"foo\" || \"bar\"` , the syntax too seems much more convenient than the one I proposed.\r\nI also like the possibility of \"mixing up with types\" like in your example (`\"something\" || object({foo = string})`), although I think a convenient syntax should be provided to handle \"choosing\" the variant in this case.\r\n\r\nIf the possible values are all of the same type, then the problem does not arise, but if `var.my_var` can be either a string or an object, then we may need a way to discriminate between them, and a special syntax for it may be useful in order to keep the code as readable as possible.","The problem of \"choosing\" is indeed a part I glossed over here. Languages which have that sort of type system tend to also have something like \"pattern-matching\" syntax for concisely evaluating different expressions based on what kind of value was provided, and I agree that designing something in that area is an important part of designing this should we decide to make this more like sum-types and less like \"fixed set of values of a single type\".\r\n\r\nI don't have any sufficient concrete proposal for that yet, but an analogy to help frame what we're talking about might be [Rust's `match` expressions](https:\/\/doc.rust-lang.org\/reference\/expressions\/match-expr.html), which allow concisely taking different evaluation paths based on a value and destructuring complex values to more conveniently use their constituent parts.\r\n\r\n---\r\n\r\nLooking back at my previous (long) reply I realize that I forgot to say that the language today does have a way to express that sort of constraint as validating rule, rather than as a type constraint:\r\n\r\n```hcl\r\nvariable \"my_string_enum\" {\r\n  type        = string\r\n  description = \"enum of strings\"\r\n\r\n  validation {\r\n    condition = contains([\"string1\", \"string2\", \"string3\"], var.my_string_enum)\r\n    error_message = \"Must be either \\\"string1\\\", \\\"string2\\\", or \\\"string3\\\".\"\r\n  }\r\n}\r\n```\r\n\r\nThis approach does _functionally_ work today, but of course there are some ergonomic annoyances with it:\r\n\r\n- It's harder to automatically extract documentation saying which values are acceptable, since the `condition` is a dynamic expression that can be written in various different ways, rather than a static constraint.\r\n- It's considerably harder to use this pattern when the \"enum\" is for a nested value inside a collection or structural type; it tends to require using [`alltrue`](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/alltrue) with `for` expressions, or other similar patterns that are functionally equivalent but hard to write and maintain.\r\n- With `validation` as currently designed, there's no way to share the set of allowed values between `condition` and `error_message`, so we typically end up having to write the set of valid values twice so that the error message will be actionable. (There is a more focused proposal somewhere for something like allowing `locals` blocks inside `validation` blocks for tightly-scoped locals that can be shared across both `condition` and `error_message`, but that's a far blunter solution than treating it as a type-system-shaped problem.)\r\n\r\nI would recommend that anyone with this need should try to follow the pattern I showed above with today's Terraform. Ergonomic concerns aside, it does _functionally_ meet the use-case as stated. However, I don't find this answer satisfying and would like to continue researching alternatives like those we've been discussing in this issue.\r\n","Taking inspiration from Rust's pattern-matching to implement this feature would be a great thing, in my opinion.\r\n\r\nI have been thinking about potential implementations lately, and here are my thoughts on how it could be achieved.\r\n\r\nFirst, I think that a way to distinguish and reference each possible variant in an easy and readable way is needed. Something like this to be clear:\r\n```hcl\r\n\/\/ variable difinition proposal\r\nvariable \"sum_type\" {\r\n  \/\/ each branch is given a unique key\r\n  type  = a_string: \"STRING1\" || a_number: number || an_obj: object({ field = string })\r\n  description = \"desc\"\r\n}\r\n\r\n\/\/ variable difinition proposal 2\r\nvariable \"sum_type\" {\r\n  type = union({\r\n    \/\/ each branch is given a unique key\r\n    a_string = \"STRING1\"\r\n    an_obj   = object({\r\n      field = string\r\n    })\r\n    a_number = number\r\n  })\r\n  description = \"desc\"\r\n}\r\n```\r\n\r\nMoreover, I think that a crucial point should be on which level we want the match to occur.\r\n\r\nFor example, there are at least two different levels in which a hypothetical match operator could fit:\r\n- As a right-hand side construct, not much dissimilar to a function that maps each \u201cbranch\u201d to a value\r\n- As a resource\/module-level block, which could allow for potentially more complex scenarios.\r\n\r\nIn the example below, a proposal for a `match` construct it is introduced as a right-hand side expression:\r\n\r\n```hcl\r\n\/\/ proposal for match as right-hand side expression\r\nresource \"a_resource\" \"my_res\" {\r\n  a_field = match(var.sum_type, {\r\n    \/\/ i use the key for branching\r\n    a_string => var.sum_type\r\n    an_obj   => var.sum_type.field\r\n  })\r\n\r\n  another_field = match(var.sum_type, {\r\n    a_string => var.sum_type\r\n    \/\/ and a convenient else to unify the logic for all remaining branches\r\n    else   => \"\"\r\n  })\r\n}\r\n```\r\n\r\nBut, I can already imagine cases in which such a `match` construct will just clutter the code (e.g. when you need to repeat the same match logic for many fields of a resource).\r\n\r\nThus, here is a proposal for adding it as a module\/resource-level block:\r\n\r\n```hcl\r\n\/\/ proposal for match as a module\/resource-level block\r\nresource \"a_resource\" \"my_res\" {\r\n  name = var.res_name\r\n\r\n  match(var.sum_type) {\r\n    \/\/ for each branch you define which variables to pass\r\n    a_string {\r\n      \/\/ a self\/this keyword would make it even more readable\r\n      a_field = var.sum_type\r\n    }\r\n    an_obj {\r\n      a_field       = var.sum_type.field1\r\n      another_field = var.sum_type.field1\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis second approach would, in my opinion, cover a bigger area for which such a construct may be needed, without hindering readability.\r\n\r\nOne may also think of a convenient way to check the actual \u201cvariant\u201d (useful in constructs like `if`s, and to selectively create resources e.g. with `count`). Something like:\r\n```hcl\r\ncount = variant(var.sum_type, a_string) ? 0 : 1\r\n```"],"labels":["enhancement","upstream","config"]},{"title":"`element types must all match for conversion` when using matching type but from module output","body":"### Terraform Version\n\n```shell\n\u1405 terraform -version\r\nTerraform v1.5.7\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n## main.tf\r\n\r\n```terraform\r\nlocals {\r\n  things = [\r\n    {\r\n      thing = {}\r\n    },\r\n    {\r\n      thing = {} # works\r\n      # thing = module.data.thing # yields: element types must all match for conversion\r\n    }\r\n  ]\r\n}\r\n\r\nmodule \"data\" {\r\n  source = \".\/data\"\r\n}\r\n\r\nmodule \"things\" {\r\n  source = \".\/things\"\r\n  in     = local.things\r\n}\r\n\r\noutput \"things\" {\r\n  value = module.things\r\n}\r\n\r\noutput \"comparison\" {\r\n  value = {\r\n    from_local  = local.things[1].thing\r\n    from_module = module.data.thing\r\n  }\r\n}\r\n```\r\n\r\n## data\/main.tf\r\n\r\n```terraform\r\noutput \"thing\" {\r\n  value = {}\r\n}\r\n```\r\n\r\n## things\/main.tf\r\n\r\n```terraform\r\nvariable \"in\" {\r\n  type = list(any)\r\n}\r\n\r\noutput \"out\" {\r\n  value = var.in\r\n}\r\n```\n\n### Debug Output\n\n[debug output](https:\/\/gist.github.com\/theherk\/f750fec15f2f9c556f768a549f71e378)\n\n### Expected Behavior\n\nI have tried my best to distill the issue down to a minimal reproducible issue. Essentially, when I'm passing a variable to a module that has a type of `list(any)` the types of the contained values have to match; completely normal. If I define the value locally, great. However, if I use a value given as an output in another module, terraform complains that the types don't match.\n\n### Actual Behavior\n\nAs I illustrate in the example, terraform complains that the types don't match, but if you look at the planned output for `comparison`, they sure look like the same type to me.\r\n\r\n```\r\n\u1405 TF_LOG=trace TF_LOG_PATH=tf-trace-(date +%FT%T+01).log terraform plan -lock=false\r\n\r\nChanges to Outputs:\r\n  + comparison = {\r\n      + from_local  = {}\r\n      + from_module = {}\r\n    }\r\n  + things     = {\r\n      + out = [\r\n          + {\r\n              + thing = {}\r\n            },\r\n          + {\r\n              + thing = {}\r\n            },\r\n        ]\r\n    }\r\n```\n\n### Steps to Reproduce\n\n1. Create these files as given.\r\n2. `terraform init`\r\n3. `terraform plan`\r\n4. Comment L7.\r\n5. Uncomment L8.\r\n6. Observe the new error.\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for the example @theherk! I'm not sure how this hasn't cropped up before, but it is a bit of an odd case. The failure is happening during validation, where runtime values are all considered as unknown, so the list is being created with an empty object and a dynamic value, which can't convert to a single type. While the static module output in this example makes the type obvious, in most cases the output type is not going to be known during validation, so deriving a correctly typed value may not be possible, and will probably require output type declarations as well.\r\n\r\nIn the meantime, using a more precise type for the module input usually will help with the type inference problems. You usually don't want something like `list(any)`, because while that allows the list to contain a single element type which is unknown, it makes the type inference more difficult because that single type could literally be \"anything\".","In some cases, there are uses for `any`. This, I believe is one of them, though I admit that isn't clear in the contrived example. I don't understand what is unknown though, at least in this case. The output is a concrete empty map; nothing dynamic as you point out. So, I'm still unclear on why validation can't be done when the value _can_ be known.\r\n\r\nAre output type declarations forthcoming? That is an interesting development. Thank you.","Validation is done using unknown values wherever possible, because we are statically validating the configuration, and want to cover all possible inputs and values for variables and resources. Yes, the output is static here, and an exception could probably be added to the configuration to record the type in this particular case; but that is rare in practice, and does not work if the module is expanded or the type is inferred from other dynamic values.\r\n\r\nBeing a contrived example it's hard to say what may be optimal, but using `list(any)` is the what sets up the problem. Using that type constraint gives you very little extra assurance since the element type is still unknown and types are invariant, so it may be more useful if you simply use `any` instead if you can't declare a static type for the value. \r\n\r\nOutput type declarations are something being considered, but there is no set timeline as of yet."],"labels":["bug","config","explained","v1.5"]},{"title":"Skip ValidateResourceConfig when the resource is only being destroyed","body":"### Terraform Version\n\n```shell\nTerraform v1.5.1\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\n...terraform config...\r\n```\r\n\n\n### Debug Output\n\n\u2502 Error: versioning_configuration.status cannot be updated from 'Enabled' to 'Disabled'\n\n### Expected Behavior\n\nWhen destroy is called terraform should not perform any validation and just delete target objects\n\n### Actual Behavior\n\nChecks are done as it seems\n\n### Steps to Reproduce\n\n1. Create an S3 bucket link aws_s3_bucket_versioning resource to it\r\n2. Modify aws_s3_bucket_versioning (only code)\r\n3. Run terraform destroy\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @vladimirtiukhtin,\r\n\r\nThanks for filing the issue. This is currently expected, since there are elements of the configuration which do come into play even during a destroy operation. We also need to take into account that providers expect all resources to be read before they are destroyed, which currently requires a normal plan to be executed internally before the destroy plan is constructed.\r\n\r\nBecause the validation and destroy plan happen in different phases of execution it makes the change a bit more difficult, but I'm going to update this to an enhancement request to see if that change can be made at all.\r\n\r\nThanks!"],"labels":["enhancement","core"]},{"title":"Use self or this in validation rules to reference variable","body":"### Terraform Version\r\n\r\n```shell\r\n1.5.7\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWhen writing validation rules for a variable, one is forced to reference the variable through `var.my_var` which seems a little redundant, since the validation rule can only be written inside a variable block.\r\n\r\n### Attempted Solutions\r\n\r\nN\/A\r\n\r\n### Proposal\r\n\r\nIn my opinion, the code would be more readable if we had the possibility to reference the variable through a keyword like `self` or `this`.\r\n\r\nSo, for example something like:\r\n```hcl\r\nvariable \"my_var\" {\r\n  type        = string\r\n  description = \"An informative desc\"\r\n  validation {\r\n    condition     = can(regex(\"^[a-z-_]+\", self))\r\n    error_message = \"Variable does not conform to regex [a-z-_]+\"\r\n  }\r\n}\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Inconsistent behavior for `s3` backend with named `profile` and `credential_process`","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.5\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nterraform {\r\n  required_version = \"~> 1.5\"\r\n\r\n  backend \"s3\" {\r\n    region                  = \"us-east-1\"\r\n    bucket                  = \"my-state-bucket\"\r\n    key                     = \"path\/to\/terraform.tfstate\"\r\n    encrypt                 = true\r\n    dynamodb_table          = \"terraform-lock-table\"\r\n    profile                 = \"local-backend\"\r\n    shared_credentials_file = \"\/path\/to\/local\/credentials\"\r\n  }\r\n\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.0\"\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n```bash\r\n# ~\/.aws\/credentials\r\n\r\n[global-backend]\r\ncredential_process = \/usr\/local\/bin\/custom-credentials-process\r\n```\r\n\r\n```bash\r\n# \/path\/to\/local\/credentials\r\n\r\n[local-backend]\r\ncredential_process = \/usr\/local\/bin\/custom-credentials-process\r\n```\r\n\r\n### Debug Output\r\n\r\n```\r\n2023-09-14T10:44:35.140-0600 [INFO]  Terraform version: 1.5.5\r\n2023-09-14T10:44:35.140-0600 [DEBUG] using github.com\/hashicorp\/go-tfe v1.26.0\r\n2023-09-14T10:44:35.140-0600 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.16.2\r\n2023-09-14T10:44:35.140-0600 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.0\r\n2023-09-14T10:44:35.140-0600 [DEBUG] using github.com\/zclconf\/go-cty v1.12.2\r\n2023-09-14T10:44:35.140-0600 [INFO]  Go runtime version: go1.20.7\r\n2023-09-14T10:44:35.140-0600 [INFO]  CLI args: []string{\"\/opt\/homebrew\/Cellar\/tfenv\/3.0.0\/versions\/1.5.5\/terraform\", \"init\"}\r\n2023-09-14T10:44:35.140-0600 [TRACE] Stdout is a terminal of width 320\r\n2023-09-14T10:44:35.140-0600 [TRACE] Stderr is a terminal of width 320\r\n2023-09-14T10:44:35.140-0600 [TRACE] Stdin is a terminal\r\n2023-09-14T10:44:35.140-0600 [DEBUG] Attempting to open CLI config file: \/Users\/jcarlson\/.terraformrc\r\n2023-09-14T10:44:35.140-0600 [INFO]  Loading CLI configuration from \/Users\/jcarlson\/.terraformrc\r\n2023-09-14T10:44:35.141-0600 [INFO]  Loading CLI configuration from \/Users\/jcarlson\/.terraform.d\/credentials.tfrc.json\r\n2023-09-14T10:44:35.142-0600 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2023-09-14T10:44:35.142-0600 [DEBUG] ignoring non-existing provider search directory \/Users\/jcarlson\/.terraform.d\/plugins\r\n2023-09-14T10:44:35.142-0600 [DEBUG] ignoring non-existing provider search directory \/Users\/jcarlson\/Library\/Application Support\/io.terraform\/plugins\r\n2023-09-14T10:44:35.142-0600 [DEBUG] ignoring non-existing provider search directory \/Library\/Application Support\/io.terraform\/plugins\r\n2023-09-14T10:44:35.143-0600 [INFO]  CLI command args: []string{\"init\"}\r\n\r\nInitializing the backend...\r\n2023-09-14T10:44:35.145-0600 [TRACE] Meta.Backend: built configuration for \"s3\" backend with hash value 1229001865\r\n2023-09-14T10:44:35.145-0600 [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2023-09-14T10:44:35.145-0600 [DEBUG] New state was assigned lineage \"9a942f48-8f96-d3af-74bc-4cd559c01f43\"\r\n2023-09-14T10:44:35.145-0600 [TRACE] Meta.Backend: moving from default local state only to \"s3\" backend\r\n2023-09-14T10:44:35.145-0600 [DEBUG] checking for provisioner in \".\"\r\n2023-09-14T10:44:35.145-0600 [DEBUG] checking for provisioner in \"\/opt\/homebrew\/Cellar\/tfenv\/3.0.0\/versions\/1.5.5\"\r\n2023-09-14T10:44:35.145-0600 [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2023-09-14T10:44:35.145-0600 [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2023-09-14T10:44:35.145-0600 [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2023-09-14T10:44:35.145-0600 [TRACE] statemgr.Filesystem: read nil snapshot\r\n2023-09-14T10:44:35.145-0600 [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2023-09-14T10:44:35.147-0600 [INFO]  Attempting to use session-derived credentials\r\n\u2577\r\n\u2502 Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\r\n\u2502\r\n\u2502 Please see https:\/\/www.terraform.io\/docs\/language\/settings\/backends\/s3.html\r\n\u2502 for more information about providing credentials.\r\n\u2502\r\n\u2502 Error: NoCredentialProviders: no valid providers in chain. Deprecated.\r\n\u2502 \tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors\r\n\u2502\r\n\u2502\r\n\u2502\r\n\u2575\r\n```\r\n\r\n### Expected Behavior\r\n\r\nTerraform should respect the `credential_process` specified in `\/path\/to\/local\/credentials` to generate AWS credentials on-demand.\r\n\r\n### Actual Behavior\r\n\r\nTerraform is unable to locate AWS credentials.\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n\r\n### Additional Context\r\n\r\nAccording to the [AWS documentation](https:\/\/docs.aws.amazon.com\/sdkref\/latest\/guide\/feature-process-credentials.html), the `credential_process` configuration option is supported only in a shared `config` file and not a shared `credentials` file. However, in the sample code above, replacing \r\n\r\n```\r\n    profile                 = \"local-backend\"\r\n    shared_credentials_file = \"\/path\/to\/local\/credentials\"\r\n```\r\n\r\nwith\r\n\r\n```\r\n    profile = \"global-backend\"\r\n```\r\n\r\ndoes work as expected. \r\n\r\nThe behavior here is inconsistent. When using a global shared credentials file at the default location `~\/.aws\/credentials` the `credential_process` option works as expected, but when using a custom shared credentials file, such as `\/path\/to\/custom\/credentials`, it does not work.\r\n\r\nIdeally, the `s3` backend configuration block would support `shared_config_files` and `shared_credential_files` in the same manner that the `aws` provider [supports both](https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs#argument-reference).\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this report. It might be worth checking if `1.6.0-beta2` still has this issue, it may have been fixed due to updating the AWS SDK dependency. ","Unfortunately, our internal engineering leadership has advised us to hold at Terraform `< 1.6.0` for now due to the license terms changing. So I'll have to stick to 1.5.x.","OK! To set expectations correctly, further bug fixes for the S3 backend in 1.5.x are highly unlikely. This would have been the case in any event, but I acknowledge that the license change does change the context of that message. "],"labels":["bug","backend\/s3","new","v1.5"]},{"title":"Support for filtering details of plan command output by add, change or destroy actions","body":"### Terraform Version\n\n```shell\n1.5.6-1\n```\n\n\n### Use Cases\n\nLong output.\n\n### Attempted Solutions\n\ndriftctl tool\n\n### Proposal\n\nIt would be super helpful some options in the plan command where we can filter only by changes, additions or deletions of resources, I mean, an option that allows filtering the terraform plan output with details according to our event or action we want to get more details.\r\n\r\nSometimes the amount of changes is big and the output only show changes according to your screen where the only option we have is to see the number next to the plan output with events like change, add or destroy but, it would be helpful to get a list of those 3 actions, not all in one or nothing.\r\n\r\nE.g.:\r\n**Plan:** 60 to add, 15 to change, 4 to destroy.\r\n\r\nI'd like to know what we can do in these scenarios, whether we want to validate changes or resources to be deleted. I scroll up but I can't see enough details -- I think I can see the last 100 lines.\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["enhancement","new"]},{"title":"Allow provider schemas to be larger than 64MiB","body":"### Terraform Version\n\n```shell\nTerraform v1.7.0-dev\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\n# does not apply\r\n```\r\n\n\n### Debug Output\n\n```\r\nPlugin error: The plugin returned an\r\nunexpected error from plugin6.(*GRPCProvider).GetProviderSchema: rpc error:\r\ncode = ResourceExhausted desc = grpc: received message larger than max\r\n(68485182 vs. 67108864)..\r\n```\n\n### Expected Behavior\n\nProvider schema should be readable even if it is larger than 64mb\n\n### Actual Behavior\n\nTerraform throws a plugin error in case a provider returns a schema that is larger than 64mb.\n\n### Steps to Reproduce\n\n1. Clone https:\/\/github.com\/metio\/terraform-provider-k8s\/tree\/development\r\n2. Run `make docs` in the cloned repo\n\n### Additional Context\n\nThe limit was previously raised to 64mb and some comment indicated that in the future, a fine-grained API should be provided to fetch individual resource schemas (see linked tickets below). \n\n### References\n\n- https:\/\/github.com\/hashicorp\/terraform\/pull\/20906\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/20879","comments":["Thanks for this feedback, @sebhoss.\r\n\r\nI've retitled this issue to more directly represent the request that is implied by your issue. As you noted, it seems likely that the solution here will be to design some new protocol for schema that allows gradually requesting only relevant parts of the schema rather than retrieving it all as a single response, since just continuing to grow the upper limit on the grpc response is going to eventually hit other limits that won't be so easy to change.\r\n\r\nA possible design we could consider for a future Terraform release:\r\n- Initial schema request only returns all of the available resource type names and does not yet describe their individual schemas. This will allow Terraform to quickly determine whether all of the resource types mentioned in the configuration are available for the provider.\r\n- Subsequent requests can name one or more specific resource types to retrieve detailed schema for. Terraform would then ask only for information about resource types that appear either in the configuration or prior state.\r\n\r\nA design challenge with that approach is that some other parts of Terraform currently expect to be able to fetch the entire schema once and then cache it for future use for all operations using the same version of that provider. Some of those callers don't have access to the state at the time of doing their work, and so would not be able to predict the full set of resource types that might appear in later work. Therefore we'd need to first find some solution that allows those other systems to retrieve schema piecemeal as described, which might be challenging since some downstream schema consumers are not able to run Terraform providers directly themselves, either for performance or security reasons.\r\n\r\nI'm going to relabel this as an enhancement because Terraform is currently behaving as designed and this issue is a request to change its design to remove an intentionally-imposed limit. We use the \"bug\" label only for defects that can be corrected by changing Terraform to better meet the original design, whereas this request seems to require some new design work and therefore must be treated as an enhancement in our development process.\r\n\r\nThanks again!\r\n","hey @apparentlymart, thanks for the detailed response. I fully agree with everything you said! If there is anything I can do to help with this, please let me know!"],"labels":["enhancement","providers","providers\/protocol"]},{"title":"backend\/ks3 Add Kingsoft Cloud backend ks3 with locking","body":"### Terraform Version\r\n\r\n```shell\r\n\u276f terraform version\r\nTerraform v1.4.6\r\non darwin_arm64\r\n+ provider registry.terraform.io\/kingsoftcloud\/ksyun v1.10.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\n```hcl\r\n  backend \"ks3\" {\r\n    bucket                      = \"tf-backend\"\r\n    key                         = \"terraform.tfstate\"\r\n    region                      = \"cn-beijing-6\"\r\n    endpoint                    = \"ks3-cn-beijing.ksyuncs.com\"\r\n    workspace_key_prefix        = \"backend-dev\/test\"\r\n    access_key                  = \"\"\r\n    secret_key                  = \"\"\r\n  }\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nHi there,\r\n\r\nWe want to add a new backend ks3 that terraform can store state remotely in KingsoftCloud Ks3 with locking.\r\n\r\n#### About KingsoftCloud\r\nKingsoft Cloud provides a stable, safe and efficient cloud compute service. It is the well-known Cloud Provider in China.\r\n\r\n#### About KingsoftCloud Ks3\r\nKingsoft Cloud Standard Storage Service (KS3) is a highly secure and reliable distributed cloud storage service that provides large storage capacity at a low cost. \r\n\r\n### Proposal\r\n\r\n**However**, I learned from [Contributing File](https:\/\/github.com\/hashicorp\/terraform\/blob\/58bd0f94607c12e56b0f572ee4e973e22a6a3595\/.github\/CONTRIBUTING.md) that can`t merge PRs with a new backend. And, now, is it still valid? \r\nPerhaps, is there another way to help me to add the new backend?\r\nThanks! \ud83d\ude04 \r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this request! I have notified our product manager. ","> Thanks for this request! I have notified our product manager.\r\n\r\nHi @crw , it's been a few weeks since this PR submission, any new news? \r\nI'm looking forward to your reply and thanks.","No update right now. I apologize, I should have previously linked to our [Contributing.md](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md#state-storage-backends) document, specifically:\r\n\r\n> The Terraform team is not merging PRs for new state storage backends. Our priority regarding state storage backends is to find maintainers for existing backends and remove those backends without maintainers.\r\n\r\nThis is the starting point for backends right now. If Kingsoft had a partner relationship with HashiCorp, this would be something they could discuss and possibly we would support it. \r\n\r\nI will let you know if this changes in the future. Thanks!","> No update right now. I apologize, I should have previously linked to our [Contributing.md](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md#state-storage-backends) document, specifically:\r\n> \r\n> > The Terraform team is not merging PRs for new state storage backends. Our priority regarding state storage backends is to find maintainers for existing backends and remove those backends without maintainers.\r\n> \r\n> This is the starting point for backends right now. If Kingsoft had a partner relationship with HashiCorp, this would be something they could discuss and possibly we would support it.\r\n> \r\n> I will let you know if this changes in the future. Thanks!\r\n\r\nHow can we establish a partner relationship with HashiCorp?","Thanks for asking, I should have included this! [Terraform Integration Program](https:\/\/developer.hashicorp.com\/terraform\/docs\/partnerships), although you may want to skip straight to the bottom (https:\/\/developer.hashicorp.com\/terraform\/docs\/partnerships#terraform-enterprise-integrations)"],"labels":["enhancement","new-backend","new"]},{"title":"Support conditionally auto loading tfvars on workspace","body":"Automatically loads variable definitions files ending with .auto.\\<workspace\\>.tfvars and .auto.\\<workspace\\>.tfvars.json. \\<workspace\\> is the name of the active workspace.\r\n\r\nFixes #15966\r\n\r\n## Target Release\r\n1.6.x\r\n\r\n### ENHANCEMENTS\r\n- Support conditionally auto loading tfvars on workspace  \r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33873) <br\/>All committers have signed the CLA.","Thanks for this submission, I have added it to the triage list."],"labels":["enhancement"]},{"title":"templatefile option to leave undefined variables as literal interpolation syntax, instead of an error","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.0\r\n```\r\n\r\n```bash\r\n$ cat script.sh\r\n#!\/bin\/bash\r\ns=\"bar\"\r\n\r\necho \"${var}\"\r\necho \"${s}\"\r\n```\r\n\r\n```bash\r\n$ cat main.tf\r\ndata \"template_file\" \"demo\" {\r\n   template = file(\"script.sh\")\r\n   vars = {\r\n     var = \"foo\"\r\n   }\r\n}\r\n\r\noutput \"demo\" {\r\n    value = data.template_file.demo.rendered\r\n}\r\n```\r\n\r\n```bash\r\n$ terraform apply\r\n...\r\n> \u2502 Error: failed to render : <template_file>:5,9-10: Unknown variable; There is no variable named \"s\".\r\n...\r\n```\r\n\r\n\r\n\r\n### Use Cases\r\n\r\nI can get this to work by changing `\"${s}\"` to `\"$${s}\"`, but I think this is unreasonable.\r\n\r\n### Proposal\r\n\r\ntemplate_file should attempt to evaluate only specified variables.\r\n","comments":["Hi @paololazzari,\r\n\r\nYou are describing a behavior of the deprecated `hashicorp\/template` provider rather than Terraform itself, but the same behavior would apply to the built-in `templatefile` function that replaced the `template_file` data source, so let's consider this as feedback for that function instead.\r\n\r\nWith that said: what you have here is a tradeoff of convenience vs. robustness. The current behavior of Terraform template rendering prioritizes robustness by raising an explicit error if you write your template incorrectly. It seems like you would instead like Terraform to just guess that any invalid interpolation sequence is intended to be taken literally, which I can see would be more convenient but would also give no feedback if someone spelled a variable name incorrectly when writing their template.\r\n\r\nIt's typical when you embed one language inside another that you will need to use escaping for any elements that are meaningful to both languages, so that each layer can interpret its input in its own language. Terraform's template language achieves that using the `$${` sequence that you've classified as \"unreasonable\".\r\n\r\nI think there are arguments on both sides here -- convenience can _sometimes_ trump robustness\/correctness -- but Terraform typically errs on the side of robustness because mistakes with infrastructure objects are often costly. We consider that the need to be explicit about your intentions by adding an additional character is worth it in order to get feedback about problems earlier, vs. potentially only once expensive infrastructure has already been incorrectly deployed.\r\n\r\nBased on those tradeoffs, I don't expect that we would change how `templatefile` behaves -- and indeed, how Terraform templates work in general, since `templatefile` is really just the same string template language used in `.tf` files. It is possible today to write a provider that embeds some other template engine that makes different tradeoffs that better suit your preferences, and so if you feel strongly about this then you might consider that possibility.\r\n\r\nI do wonder though if it would be feasible for the `templatefile` function to give better feedback in this situation, if we can devise a sufficient heuristic to guess if a template result seems like it's trying to be a shell script. As long as we could define a heuristic that has minimal false positives, we could potentially make the error message include a hint about escaping the `${` sequence if it's intended to be consumed by the underlying shell instead of by Terraform itself.\r\n","Hi @apparentlymart ,\r\n\r\n> It seems like you would instead like Terraform to just guess that any invalid interpolation sequence is intended to be taken literally\r\n\r\nNo, I am not saying this. I am suggesting that Terraform should only interpolate variables which have been defined. It should not error out for variables that were not defined.\r\n\r\n\r\n","Why do you change the title of issues opened by users? The new title you proposed is completely different than what I had. ","> > It seems like you would instead like Terraform to just guess that any invalid interpolation sequence is intended to be taken literally\r\n> \r\n> No, I am not saying this. I am suggesting that Terraform should only interpolate variables which have been defined. It should not error out for variables that were not defined.\r\n\r\nIt may seem like a different goal here, but internally these are just a rephrasing of the same functionality intended to clarify the situation.\r\n\r\n> Why do you change the title of issues opened by users? The new title you proposed is completely different than what I had.\r\n\r\nSorry if that was surprising, but we do often will rewrite a request like this to highlight a part of the idea which is more likely to result in a change. The interpolation behavior change as originally proposed is something which has been considered and rejected in the past, and does not align with the product goals, so is much less likely to be considered at all.","> The interpolation behavior change as originally proposed is something which has been considered and rejected in the past, and does not align with the product goals, so is much less likely to be considered at all.\r\n\r\nHow about having `templatefile` accept a `strict` argument? If provided, then all variables are validated, otherwise only those specified. \r\nImagine I have a huge shell script, but I only want to pass one variable from terraform. Surely that's a valid, common use case?\r\n","Part of the problem here is that [string templates](https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/strings#string-templates) are not shell syntax, and only overlap in a few minor cases, so conceptually it's not correct to assume that unassigned variables correlate to text that should remain as shell syntax (plus Terraform does not know what comprises the shell syntax in question). We need to account for the fact that simple string substitution is only part of the possible cases which will be encountered; variables can appear in any interpolation (`${...}`) sequence which can contain arbitrarily complex expressions, as well as formatting directives (`%{...}`). There would need to be a well-defined solution for handling all those cases where variables appear at any level of nesting and for all syntax, but the only definitive way to know when an error should be raised is if the user defines the variables or escapes the sequences in question.\r\n\r\nThis amounts to hiding errors from invalid interpolation sequences, and while that might be convenient at first, it means that later changes around the template which cause errors can no longer be detected, and may not show the problems it's until long after the changes has been committed.","Okay, let's make this issue be about the possibility of enabling a more lax mode that just ignores errors.\r\n\r\nIt isn't entirely clear how to achieve that from an implementation standpoint since the template has already been parsed long before Terraform knows what variables are available -- parsing and evaluation are separate steps with different information available each -- but this issue can represent the use-case for now and leave the technical design for a later time.\r\n\r\n---\r\n\r\nIn the meantime my suggestion for the situation you mentioned -- a shell script that only needs one variable interpolated by Terraform -- would be to separate the interpolated part from the literal part, such as:\r\n\r\n```\r\n<-EOT\r\n#!\/bin\/bash\r\nsome_var='${replace(var.example, \"'\", \"'\\\\''\")}'\r\n${file(\"${path.module}\/script.sh\")}\r\nEOT\r\n```\r\n\r\nThe above inserts the script using `file` instead of `templatefile` and so will just take the entire script literally. The extra chatter above defines a shell variable `some_var` which the literal script can then refer to using normal shell interpolation syntax.\r\n\r\nA while back I wrote [a specialized provider for doing the above more systematically](https:\/\/github.com\/apparentlymart\/terraform-provider-bash), but unfortunately I was unable to get it published to the Terraform Registry due to an inscrutable message about my GPG signature being in an incorrect format. I'll probably make another attempt once it becomes possible for providers to contribute functions into the Terraform language, since that would make for a more convenient way to access that provider's abstraction.\r\n\r\n","I faced quite similar challenge as @paololazzari.\r\nI have Grafana dashboards as .json files, where I need to update \"UID\" of datasource according to env where I deploy them.\r\nTerraform replace worked perfectly, as Grafana dashboard can contain Prometheus  syntax that looks like interpolation syntax. With replace I could add placeholder string that will be different from standard interpolation syntax ${}.\r\nHere is an example:\r\n\r\n```\r\nresource \"grafana_dashboard\" \"SRE\" {\r\n  for_each = {for item in var.dashboards_files_paths: index(var.dashboards_files_paths, item) => item }\r\n  config_json = replace(file(each.value), \"__thanos_uid__\", data.grafana_data_source.Thanos_from_name.uid )\r\n  folder = grafana_folder.SRE_Dashboards.id\r\n}\r\n```"],"labels":["enhancement","new"]},{"title":"Gunzipbase64","body":"add terraform function gunzipbase64 as opposite of base64gzip similar to\r\n\r\n- base64decode opposite of base64encode\r\n- textdecodebase64 opposite of textencodebase64\r\n- jsondecode opposite of jsonencode\r\n- yamldecode opposite of yamlencode\r\n\r\n<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #22568 \r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\nterraform function gunzipbase64 can be used as opposite of base64gzip, e.g.\r\n\r\n- write base64gzip content to any storage, read it and decode it with gunzipbase64\r\n","comments":["Thanks for this submission!","> Thanks for this submission!\r\n\r\nI appreciate your feedback. I like to ask for review.","Hey @l-with , the team is working on ways to make functions extensible so anyone can add their own functions. Would love to share some thoughts with you. Mind emailing me and we can get a conversation started? oismail@hashicorp.com. Thanks!","Hi @omarismail is there any chances to get such function in terraform core functions ? "],"labels":["enhancement","functions"]},{"title":"Terraform Build command","body":"### Terraform Version\n\n```shell\nTerraform v1.5.1\r\non windows_386\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.7. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Use Cases\n\nFor most types of non infrastructure code, the idea in a CI pipeline is Build Once, Deploy Many. \r\nThe first step you download all the files you need from package repositories and bundle them up and then move that through the environments.\r\nFor Terraform (and other IAC tools ) this doesn't seem to be standard.  \r\nEvery environment, which has a different VM, we download a fresh copy of the provider and modules from their sources. \r\nThis means we Build Many, Deploy Many which isn't the best.  \r\nI think we can encourage and support a better pattern.\r\n\r\n\n\n### Attempted Solutions\n\nExisting solutions are just using zip in a pipeline. \n\n### Proposal\n\nI am suggesting a terraform build command that can be passed a command to pull down all the providers and modules and then zip them up.  \r\n\r\nRun \"terraform build \"  it will init without backend to pull down all the providers, modules etc.  \r\nThen zip them into one file so you can upload it in your CI system as a pipeline artefact to be pulled down through the environments.\r\n\r\nThis would be especially useful in older enterprises that perhaps have days of lag in between deploying dev and prod, to help reduce the risk of supply chain corruption and incorrect versions between the environment stages.\n\n### References\n\nI couldn't see any","comments":["Hi @gabrielmccoll,\r\n\r\nThanks for filing the request. If I understand correctly, you are looking for a method to initialize Terraform without interacting with the backend configuration. That would normally be done by using `terraform init -backend=false` -- does that not do what you are looking for in this case?\r\n\r\nThanks!","Hi there! I wasn't. I was trying to offer an idea around making terraform produce a distributable package of dependency's so that it can be deployed many from one build ","This sounds like the old [terraform-bundle](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/tools\/terraform-bundle\/README.md) tool.\r\n\r\nI can see why it would be desirable to have an easy way to obtain a zip of all the providers and modules in a CI system that does not let you persist artefacts between stages. I'm not sure exactly whether that's the situation @gabrielmccoll is describing - if you're able to persist a zip artefact between stages, why not the whole `.terraform` directory?","So in my case each environment is on a segregated private network and therefore each build machine would be different. The zip would be uploaded to the pipeline system and pull down the whole thing. \nJust trying to encourage a better practice you know \nHopefully I'm making sense ","Could you use Packer\/Vagrant to create some base OS images of your VMs that include the provider\/packages you want, then roll out that?","> Could you use Packer\/Vagrant to create some base OS images of your VMs that include the provider\/packages you want, then roll out that?\n\nHeya thanks for the ideas ! \nYeah I mean I already have a method of doing it via the Azure Devops Pipeline and extra steps. \nI guess I just thought it seemed like a way of making IAC closer to just C if you had inbuilt dependency bundling. \n\nIf people aren't into it that's okay though.   ","The idea seems fine, it just overlaps with existing Hashicorp tools, in my opinion, based on the description given.\r\n\r\nSimilarly, you could build Docker images with all packages you need, and deploy those using terraform modules such as aws fargate \/ azure containers \/ kubernetes + helm \/ nomad providers ","We use BitBucket pipelines to run the plan and apply processes. To help with this, we use BitBucket Pipeline caches. We use cache keys to ensure the version of the content we want cached is associated with the file that controls which versions.\r\n\r\nThis is by no means a perfect setup, but it is working well for us.\r\n\r\n```yaml\r\ndefinitions:\r\n  caches:\r\n    terraform-plugins:\r\n      key:\r\n        files:\r\n          - .terraform.lock.hcl\r\n          - versions.tf\r\n      path: \/root\/.terraform.d\/plugin-cache\r\n    terraform-local-plugins: .terraform\/plugins\r\n    terraform-local-providers:\r\n      key:\r\n        files:\r\n          - .terraform.lock.hcl\r\n          - versions.tf\r\n      path: .terraform\/providers\r\n    terraform-modules: .terraform\/modules\r\n    tflint-plugins:\r\n      key:\r\n        files:\r\n          - .tflint.hcl\r\n          - .pre-commit-config.yaml\r\n      path: \/root\/.tflint.d\/plugins\r\n```\r\n\r\nSo, every time we run the pipeline, we see ...\r\n```\r\nTerraform .tfvars files\r\n-----------------------\r\n - terraform.tfvars\r\n \r\nInitializing the backend...\r\nSuccessfully configured the backend \"s3\"! Terraform will automatically\r\nuse this backend unless the backend configuration changes.\r\nInitializing modules...\r\nInitializing provider plugins...\r\n- Reusing previous version of hashicorp\/aws from the dependency lock file\r\n- Reusing previous version of cloudflare\/cloudflare from the dependency lock file\r\n- Reusing previous version of integrations\/github from the dependency lock file\r\n- Reusing previous version of hashicorp\/random from the dependency lock file\r\n- Reusing previous version of hashicorp\/archive from the dependency lock file\r\n- Using previously-installed hashicorp\/aws v4.64.0\r\n- Using previously-installed cloudflare\/cloudflare v4.4.0\r\n- Using previously-installed integrations\/github v5.23.0\r\n- Using previously-installed hashicorp\/random v3.5.1\r\n- Using previously-installed hashicorp\/archive v2.3.0\r\nTerraform has been successfully initialized!\r\n```\r\n\r\nFrom what I understand, there is no downloading and all modules are retained at the version that we are asking for.\r\n\r\nAdmittedly, BitBucket clears the cache weekly (and you can do it manually).\r\n\r\nIt's a little better, I think, than nothing."],"labels":["enhancement","new"]},{"title":"Support double hop in the provisioner connection settings for winrm","body":"### Terraform Version\n\n```shell\n1.5.7\n```\n\n\n### Use Cases\n\nWhen running some PowerShell commands on a Windows server, it may need double hop.\r\n\r\nFor example:\r\nTerraform connects to a Windows domain member server to run \"Get-AdComputer\" to retrieve a server's AD information.\n\n### Attempted Solutions\n\nThe current Winrm connection only supports NTLM or basic. They don't support double hop.\n\n### Proposal\n\nSupport Kerberos and CredSSP\n\n### References\n\n_No response_","comments":["Thanks for this request! To set expectations, provisioners are not actively maintained so not likely (though not impossible) to be updated with new features. Still, thanks for posting this request!"],"labels":["enhancement","provisioner\/winrm","new"]},{"title":"Support Cloudflare r2 for storing Terraform state","body":"### Terraform Version\n\n```shell\n1.5.6\n```\n\n\n### Use Cases\n\nCloudflare r2 storage is S3 API compatible, but the `s3` backend for storing Terraform state seems to be only compatible with Amazon AWS.\r\n\r\nhttps:\/\/developers.cloudflare.com\/r2\/api\/s3\/api\/\n\n### Attempted Solutions\n\nTried using the S3 but it's not compatible. Thought changing the `endpoint` would do it, but the region field is compulsory. When adding in a random region it fails to authenticate with the Cloudflare endpoint. \n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["This seems to work:\r\n\r\n```\r\n  backend \"s3\" {\r\n    bucket         = \"bucket-name\"\r\n    key            = \"tfstate\/terraform.tfstate\"\r\n    endpoint       = \"xxx\"\r\n    access_key     = \"xxx\"\r\n    secret_key     = \"xxx\"\r\n    skip_credentials_validation = true\r\n    skip_region_validation = true\r\n    region         = \"us-east-1\"\r\n  }\r\n```\r\n\r\nYet if I remove `region         = \"us-east-1\"` it prompts for a region:\r\n\r\n```\r\nInitializing the backend...\r\nregion\r\n  AWS region of the S3 Bucket and DynamoDB Table (if used).\r\n\r\n  Enter a value:\r\n```\r\n\r\nI had expected `skip_region_validation = true` to override that. ","Thanks for this request! ","Didn't find a working solution for Terraform 1.6+","I am having the same problem, i also added\r\n```\r\nskip_requesting_account_id  = true\r\nskip_metadata_api_check     = true\r\n```\r\n\r\nBut now I get:\r\n```\r\n\u2502 Error: Failed to save state\r\n\u2502 \r\n\u2502 Error saving state: failed to upload state: operation error S3: PutObject,\r\n\u2502 https response error StatusCode: 501, RequestID: , HostID: , api error\r\n\u2502 NotImplemented: STREAMING-UNSIGNED-PAYLOAD-TRAILER not implemented\r\n```\r\n\r\nI could not find a workaround ","Terraform 1.6.x included an upgrade to the AWS SDK v2, (see the [release notes](https:\/\/github.com\/hashicorp\/terraform\/releases\/tag\/v1.6.0)), and 1.6.3 will add a feature that should help with third-party, S3-\"compatible\" object stores. It may be worth re-testing with 1.6.3, although I do not specifically recognize the error specified above. ","I can confirm the error mentioned above (STREAMING-UNSIGNED-PAYLOAD-TRAILER) appears with many others s3 compatible providers, such as Oracle Cloud buckets for example.\r\nState file and s3 cant really be used anymore at the moment.. will test out with 1.6.3 as soon as its out","> State file and s3 cant really be used anymore at the moment\r\n\r\nTo clarify for any future reader of this issue, AWS S3 can be used as a state file backend now. \"S3-compatible\" services from non-AWS vendors appear to be having trouble keeping up with compatibility, but hopefully the fixes made to the S3 backend by our AWS provider team to workaround these compatibility issues will help alleviate the issue. ","> > State file and s3 cant really be used anymore at the moment\r\n> \r\n> To clarify for any future reader of this issue, AWS S3 can be used as a state file backend now. \"S3-compatible\" services from non-AWS vendors appear to be having trouble keeping up with compatibility, but hopefully the fixes made to the S3 backend by our AWS provider team to workaround these compatibility issues will help alleviate the issue.\r\n\r\nDoesn't look like the changes have stopped the issue\r\n\r\n```\r\n\u2502 Error saving state: failed to upload state: operation error S3: PutObject,\r\n\u2502 https response error StatusCode: 501, RequestID: , HostID: , api error\r\n\u2502 NotImplemented: STREAMING-UNSIGNED-PAYLOAD-TRAILER not implemented\r\n```\r\n\r\nVersion 1.6.5","I am also keen to see a resolution on this","+1 for resolution","I was able to use a Cloudflare R2 bucket as a s3 backend with terraform `1.6.6` today.\r\n\r\nIn order to solve the `NotImplemented: STREAMING-UNSIGNED-PAYLOAD-TRAILER` error I needed to add `skip_s3_checksum = true`:\r\n\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    cloudflare = {\r\n      source  = \"cloudflare\/cloudflare\"\r\n      version = \"~> 4.0\"\r\n    }\r\n  }\r\n\r\n  backend \"s3\" {\r\n    bucket = \"terraform-state\"\r\n    key    = \"project_name\/terraform.tfstate\"\r\n    endpoints = { s3 = \"https:\/\/xxxxx.r2.cloudflarestorage.com\" }\r\n    region = \"us-east-1\"\r\n\r\n    access_key = \"xxxx\"\r\n    secret_key = \"xxxxx\"\r\n    skip_credentials_validation = true\r\n    skip_region_validation = true\r\n    skip_requesting_account_id  = true\r\n    skip_metadata_api_check     = true\r\n    skip_s3_checksum = true\r\n  }\r\n}\r\n```","reference: https:\/\/github.com\/hashicorp\/terraform\/pull\/34127","> I was able to use a Cloudflare R2 bucket as a s3 backend with terraform `1.6.6` today.\r\n> \r\n> In order to solve the `NotImplemented: STREAMING-UNSIGNED-PAYLOAD-TRAILER` error I needed to add `skip_s3_checksum = true`:\r\n> \r\n> ```terraform\r\n> terraform {\r\n>   required_providers {\r\n>     cloudflare = {\r\n>       source  = \"cloudflare\/cloudflare\"\r\n>       version = \"~> 4.0\"\r\n>     }\r\n>   }\r\n> \r\n>   backend \"s3\" {\r\n>     bucket = \"terraform-state\"\r\n>     key    = \"project_name\/terraform.tfstate\"\r\n>     endpoints = { s3 = \"https:\/\/xxxxx.r2.cloudflarestorage.com\" }\r\n>     region = \"us-east-1\"\r\n> \r\n>     access_key = \"xxxx\"\r\n>     secret_key = \"xxxxx\"\r\n>     skip_credentials_validation = true\r\n>     skip_region_validation = true\r\n>     skip_requesting_account_id  = true\r\n>     skip_metadata_api_check     = true\r\n>     skip_s3_checksum = true\r\n>   }\r\n> }\r\n> ```\r\n\r\nThanks! that has worked for me on v1.6.5. I have been able to init, plan and apply.","> I was able to use a Cloudflare R2 bucket as a s3 backend with terraform `1.6.6` today.\r\n> \r\n> In order to solve the `NotImplemented: STREAMING-UNSIGNED-PAYLOAD-TRAILER` error I needed to add `skip_s3_checksum = true`:\r\n> \r\n> ```terraform\r\n> terraform {\r\n>   required_providers {\r\n>     cloudflare = {\r\n>       source  = \"cloudflare\/cloudflare\"\r\n>       version = \"~> 4.0\"\r\n>     }\r\n>   }\r\n> \r\n>   backend \"s3\" {\r\n>     bucket = \"terraform-state\"\r\n>     key    = \"project_name\/terraform.tfstate\"\r\n>     endpoints = { s3 = \"https:\/\/xxxxx.r2.cloudflarestorage.com\" }\r\n>     region = \"us-east-1\"\r\n> \r\n>     access_key = \"xxxx\"\r\n>     secret_key = \"xxxxx\"\r\n>     skip_credentials_validation = true\r\n>     skip_region_validation = true\r\n>     skip_requesting_account_id  = true\r\n>     skip_metadata_api_check     = true\r\n>     skip_s3_checksum = true\r\n>   }\r\n> }\r\n> ```\r\n\r\nWorks for me too. \r\n\r\nWould have thought skip_region_validation would mean we wouldn\u2019t need the us-east entry though, so still potentially a bug that could confuse people? Or is that doing something different? ","[terraform version 1.6.6]\r\nUnfortunately this workaround doesn't seem to work when wanting to provide the key via environmental variables i.e.\r\n\r\n```\r\nterraform {\r\n  required_version = \"1.6.6\"\r\n\r\n  required_providers {\r\n    cloudflare = {\r\n      source  = \"cloudflare\/cloudflare\"\r\n      version = \"~> 4\"\r\n    }\r\n  }\r\n}\r\n\r\ndata \"terraform_remote_state\" \"state\" {\r\n  backend = \"s3\"\r\n\r\n  config = {\r\n    bucket    = \"terraform-state\"\r\n    key       = \"project_name\/terraform.tfstate\"\r\n    endpoints = { s3 = \"https:\/\/xxxx.r2.cloudflarestorage.com\" }\r\n    region    = \"us-east-1\"\r\n\r\n    access_key                  = \"${var.CLOUDFLARE_S3_ACCESS_KEY_ID}\"\r\n    secret_key                  = \"${var.CLOUDFLARE_S3_SECRET_ACCESS_KEY}\"\r\n    skip_credentials_validation = true\r\n    skip_region_validation      = true\r\n    skip_requesting_account_id  = true\r\n    skip_metadata_api_check     = true\r\n    skip_s3_checksum            = true\r\n  }\r\n}\r\n```\r\n\r\nIt's returning this as an error during the `terraform plan`:\r\n\r\n```\r\n Error: Unable to access object \"terraform.tfstate\" in S3 bucket \"terraform\": operation error S3: HeadObject, https response error StatusCode: 400, RequestID: , HostID: , api error BadRequest: Bad Request\r\n\u2502```\r\n\r\nHas anyone else encountered this issue and found a solution?","> doesn't seem to work when wanting to provide the key via environmental variables\r\n\r\nhttps:\/\/stackoverflow.com\/a\/59491777","@missinglink It's not an issue with using the values from environment variables, it is successfuly authenticating to the S3 API.\r\nThere's an issue where the `HeadObject` API call in the terraform plan doesn't succeed. \r\nSometimes the above error isn't thrown, as if the remote state file has been fetched but it has no content. All of the resources need to be created in the plan results even though they exist and are present in the state file.","Fixed my issue. I misunderstood what the `terraform_remote_state` did. \r\nWhat I needed was this solution https:\/\/github.com\/hashicorp\/terraform\/issues\/13022#issuecomment-294262392","I am under the impression that as of the 1.6.6 release, the original issue scenario now is supported or have work-arounds. Is anyone still experiencing the original issue as described? If not I'll close this issue. Thanks!","@crw it certainly doesn't work 'out of the box' as there is significant configuration required.\r\n\r\nOnce this issue is closed where will the knowledge gained in this thread live?","Good question. I doubt we would document the nuances of configuration for every 3rd party S3-compatible vendor on the primary S3 backend page. Is there any existing documentation on this on the Cloudflare side? A quick google search turns up this user-editable page: https:\/\/developers.cloudflare.com\/r2\/examples\/terraform\/ -- my gut feeling is that this would be the best place for this documentation.  ","Following confirmed to work on Terraform `v1.7.4`\r\n```tf\r\n  backend \"s3\" {\r\n    bucket = \"whatever\"\r\n    key    = \"key\/goes\/here\/terraform.tfstate\"\r\n\r\n    region                      = \"auto\"\r\n    skip_credentials_validation = true\r\n    skip_metadata_api_check     = true\r\n    skip_region_validation      = true\r\n    skip_requesting_account_id  = true\r\n    skip_s3_checksum            = true\r\n    use_path_style              = true\r\n    # endpoint                  = AWS_ENDPOINT_URL_S3\r\n\r\n    \/*\r\n      ENVIRONMENT VARIABLES\r\n      ---------------------\r\n      AWS_ACCESS_KEY_ID     - R2 token\r\n      AWS_SECRET_ACCESS_KEY - R2 secret\r\n      AWS_ENDPOINT_URL_S3   - R2 location: https:\/\/ACCOUNT_ID.r2.cloudflarestorage.com\r\n    *\/\r\n  }"],"labels":["enhancement","documentation","backend\/s3","new"]},{"title":"backend\/s3: Add support for specifying an external credentials process directly","body":"### Terraform Version\n\n```shell\nTarget version: 1.6.x\n```\n\n\n### Use Cases\n\n\r\nThe backend can currently use an [external credentials process](https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-configure-sourcing-external.html) from shared config files with a named profile. Support should be added to directly set it on the backend configuration block.\r\n\r\n### Potential Configuration\r\n\r\n```terraform\r\nbackend \"s3\" {\r\n  # ...\r\n  credential_process {\r\n    command = \"...\"        # Required. Can be the full command or the executable name\r\n    args    = [\"...\", ...] # Optional. Can contain arguments for the executable\r\n  }\r\n}\r\n```\r\n\r\nNeeds investigation to see if environment variables can be passed as well.\r\n\r\nThe credentials process can be used along with `assume_role` but cannot be used with directly passed credentials, `profile`, or `assume_role_with_web_identity`.\r\n\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\n```terraform\r\nbackend \"s3\" {\r\n  # ...\r\n  credential_process {\r\n    command = \"...\"        # Required. Can be the full command or the executable name\r\n    args    = [\"...\", ...] # Optional. Can contain arguments for the executable\r\n  }\r\n}\r\n```\r\n\r\nNeeds investigation to see if environment variables can be passed as well.\r\n\r\nThe credentials process can be used along with `assume_role` but cannot be used with directly passed credentials, `profile`, or `assume_role_with_web_identity`.\r\n\r\nThis is related to https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/24885, and should be implemented at the same time. Will depend on hashicorp\/aws-sdk-go-base#291\n\n### References\n\n_No response_","comments":["One thing we should consider here:\r\n\r\nThis seems like it would allow a root module to execute arbitrary code controlled by the module author during `terraform init`.\r\n\r\nI'm not sure if that actually is a problem in practice. Here is some thinking aloud that will hopefully help decide:\r\n- The ability to execute external code _at all_ is not new here. It has technically been possible to ship a credentials configuration file along with the module and refer to it, and that file could specify a credentials helper program. Therefore we might argue this is an extension of what's already possible, only making it more convenient to use correctly.\r\n- It will remain possible to use `terraform init -backend=false` to get dependencies installed without running any backend code, so there's still a path for those who want to insert an extra step of scanning their dependencies before giving them any opportunity to run.\r\n- This is only under the control of the root module, so it cannot be exploited by a maliciously-modified third-party module. We typically assume that the user trusts their own root module because Terraform does not install that one itself.\r\n\r\nMy initial instinct based on the above is that this is not a problem, but I'm curious on your take, or if you can see any mistaken assumptions I'm making.\r\n","Fwiw, the particular use case I had in mind is that it is common for a `credential_process` utility to accept a `role_arn`, or various components thereof (account ID, role name, etc), as an argument to the command. The utility then uses that information to retrieve temporary credentials for the desired role.\r\n\r\nOur current workaround is to specify an aws-cli profile in the provider config, where that profile contains the `credential_process`, and to have a separate profile for every possible `role_arn`. By supporting the argument directly in the provider, it becomes possible to use some interpolation available to terraform at run time to construct the command. For our use case, that would allow us to eliminate all those different profiles.\r\n"],"labels":["enhancement","backend\/s3","new"]},{"title":"AzureRM backend OIDC authentication hangs during terraform init stage within Github workflow","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n```\r\nterraform {\r\n  required_providers {\r\n    azurerm = {\r\n      source = \"hashicorp\/azurerm\"\r\n      version = \"3.71.0\"\r\n    }\r\n  }\r\n  backend \"azurerm\" {}\r\n}\r\n\r\nprovider \"azurerm\" {\r\n  features {}\r\n}\r\n\r\ndata \"azurerm_resource_group\" \"rg\" {\r\n  name = \"rg-example-1\"\r\n}\r\n```\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/delikvent\/caf426f7c34553adce018b25b314c6c8\r\n\r\n### Expected Behavior\r\n\r\nterraform init stage within github workflow should fail if no matching federated identity record is found for presented assertion.\r\n\r\n### Actual Behavior\r\n\r\nterraform init stage within github workflow hangs (seemingly forever).\r\n\r\n### Steps to Reproduce\r\n\r\nterraform init \\\r\n-backend-config=subscription_id=REDACTED \\\r\n-backend-config=resource_group_name=REDACTED \\\r\n-backend-config=storage_account_name=REDACTED \\\r\n-backend-config=container_name=REDACTED \\\r\n-backend-config=key=REDACTED \\\r\n-input=false\r\n\r\n### Additional Context\r\n\r\nshell:\r\n    bash\r\n\r\nenvironment_variables:\r\n    ARM_USE_OIDC=TRUE\r\n    ARM_CLIENT_ID=REDACTED\r\n    ARM_TENANT_ID=REDACTED\r\n\r\ngithub_workflow_permissions:\r\n    id-token: write\r\n    contents: read\r\n\r\nadditional_info:\r\n    When performing authentication via \"az login\" instead of \"terraform\" core binary we can see the expected error message:\r\n    <<- ERROR MESSAGE START ->>\r\n        AADSTS70021: No matching federated identity record found for presented assertion.\r\n        Assertion Issuer: \"https:\/\/token.actions.githubusercontent.com\"\r\n        Assertion Subject: REDACTED\r\n        Assertion Audience: \"api:\/\/AzureADTokenExchange\"\r\n    <<- ERROR MESSAGE END ->>\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this bug report! \r\n\r\nIf you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","I'm having the same issue on v1.6.3\r\n\r\nhere is trace\r\n```\r\n2023-11-02T20:01:53.475Z [INFO]  Terraform version: 1.6.3\r\n2023-11-02T20:01:53.475Z [DEBUG] using github.com\/hashicorp\/go-tfe v1.36.0\r\n2023-11-02T20:01:53.475Z [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2023-11-02T20:01:53.475Z [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2023-11-02T20:01:53.475Z [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2023-11-02T20:01:53.475Z [INFO]  Go runtime version: go1.21.3\r\n2023-11-02T20:01:53.475Z [INFO]  CLI args: []string{\"terraform\", \"init\", \"-backend-config=backend.tfvars\"}\r\n2023-11-02T20:01:53.475Z [TRACE] Stdout is a terminal of width 188\r\n2023-11-02T20:01:53.475Z [TRACE] Stderr is a terminal of width 188\r\n2023-11-02T20:01:53.475Z [TRACE] Stdin is a terminal\r\n2023-11-02T20:01:53.475Z [DEBUG] Attempting to open CLI config file: \/home\/vsts\/.terraformrc\r\n2023-11-02T20:01:53.475Z [DEBUG] File doesn't exist, but doesn't need to. Ignoring.\r\n2023-11-02T20:01:53.475Z [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2023-11-02T20:01:53.475Z [DEBUG] ignoring non-existing provider search directory \/home\/vsts\/.terraform.d\/plugins\r\n2023-11-02T20:01:53.475Z [DEBUG] ignoring non-existing provider search directory \/home\/vsts\/.local\/share\/terraform\/plugins\r\n2023-11-02T20:01:53.475Z [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2023-11-02T20:01:53.476Z [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2023-11-02T20:01:53.476Z [DEBUG] ignoring non-existing provider search directory \/var\/lib\/snapd\/desktop\/terraform\/plugins\r\n2023-11-02T20:01:53.476Z [INFO]  CLI command args: []string{\"init\", \"-backend-config=backend.tfvars\"}\r\n\r\nInitializing the backend...\r\n2023-11-02T20:01:53.480Z [TRACE] Meta.Backend: merging -backend-config=... CLI overrides into backend configuration\r\n2023-11-02T20:01:53.480Z [TRACE] Meta.Backend: built configuration for \"azurerm\" backend with hash value 3206449086\r\n2023-11-02T20:01:53.480Z [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2023-11-02T20:01:53.480Z [DEBUG] New state was assigned lineage \"5a4553ea-f389-4894-9f87-e7e7cbc99335\"\r\n2023-11-02T20:01:53.480Z [TRACE] Meta.Backend: moving from default local state only to \"azurerm\" backend\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-11-02T20:01:53.482Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-11-02T20:01:53.482Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-11-02T20:01:53.482Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-11-02T20:01:53.482Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-11-02T20:01:53.482Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.482Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-11-02T20:01:53.483Z [DEBUG] checking for provisioner in \".\"\r\n2023-11-02T20:01:53.483Z [DEBUG] checking for provisioner in \"\/opt\/hostedtoolcache\/terraform\/1.6.3\/x64\"\r\n2023-11-02T20:01:53.483Z [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2023-11-02T20:01:53.483Z [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2023-11-02T20:01:53.483Z [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2023-11-02T20:01:53.483Z [TRACE] statemgr.Filesystem: read nil snapshot\r\n2023-11-02T20:01:53.483Z [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2023-11-02T20:01:53.484Z [INFO]  Testing if Service Principal \/ Client Certificate is applicable for Authentication..\r\n2023-11-02T20:01:53.484Z [INFO]  Testing if Multi Tenant Service Principal \/ Client Secret is applicable for Authentication..\r\n2023-11-02T20:01:53.484Z [INFO]  Testing if Service Principal \/ Client Secret is applicable for Authentication..\r\n2023-11-02T20:01:53.484Z [INFO]  Testing if OIDC is applicable for Authentication..\r\n2023-11-02T20:01:53.484Z [INFO]  Using OIDC for Authentication\r\n2023-11-02T20:01:53.484Z [INFO]  Getting OAuth config for endpoint https:\/\/login.microsoftonline.com\/ with  tenant 5a4553ea-f389-4894-9f87-e7e7cbc99335\r\n2023-11-02T20:01:53.484Z [DEBUG] Obtaining an MSAL \/ Microsoft Graph token for Resource Manager..\r\n2023-11-02T20:01:53.484Z [DEBUG] New state was assigned lineage \"5a4553ea-f389-4894-9f87-e7e7cbc99335\"\r\n2023-11-02T20:01:53.484Z [DEBUG] Building the Container Client from an Access Token (using user credentials)\r\n```","Found this issue, the setup is a bit finicky. What worked for me is the following:\r\n\r\nin the backend config you need:\r\n```\r\nsubscription_id              =  \"00000-0000-0000-0000-00000000\" # subscription with state storage account\r\nresource_group_name   = \"state_rg_name\"\r\nstorage_account_name = \"state_container\"\r\ncontainer_name            = \"tfstate\"\r\nkey                                = \"my_awesome_state.tfstate\"\r\nclient_id                        = \"00000-0000-0000-0000-00000000\" # client_id that is setup with oidc and privileges to the state\r\nuse_oidc                       = true\r\ntenant_id                      = \"00000-0000-0000-0000-00000000\" \r\n```\r\nDon't set ANY ARM_** environment variables (The only one I have is ARM_TENANT_ID, but this can also be included in the backend file as above.\r\n\r\nWith this the backend seems to initialize fine trough OIDC now.\r\nAnd subsequent provider blocks in the plan can be used with different client_id sourced from variables or ENV variables.\r\n\r\nWhich looks like this in GH actions and the provider config:\r\n```\r\n# Github actions config\r\n\r\n      - name: Terraform Plan\r\n        if: inputs.tf_action == 'plan'\r\n        id: plan\r\n        run: terraform plan -var-file=<(cat ..\/vars\/${{ env.tenant }}\/*.tfvars) -out=tfplan\r\n        working-directory: ${{ env.plan }}\/plan\r\n        env:\r\n          ARM_TENANT_ID: ${{ vars.ARM_TENANT_ID }}\r\n          TF_VAR_client_id_1: ${{ vars.ID1 }}\r\n          TF_VAR_client_id_2: ${{ vars.ID2 }}\r\n          TF_VAR_client_id_3: ${{ vars.ID3 }}\r\n\r\n\r\n# Provider config          \r\nprovider \"azurerm\" {\r\n  alias           = \"ID1\"\r\n  use_oidc        = true\r\n  subscription_id = var.target_subscription_id\r\n  client_id       = var.client_id_1\r\n  features {}\r\n  skip_provider_registration = true\r\n}\r\n\r\nprovider \"azurerm\" {\r\n  alias           = \"ID2\"\r\n  use_oidc        = true\r\n  subscription_id = var.target_subscription_id\r\n  client_id       = var.client_id_2\r\n  features {}\r\n  skip_provider_registration = true\r\n}\r\n\r\nprovider \"azuread\" {\r\n  alias     = \"ID3\"\r\n  use_oidc  = true\r\n  client_id = var.client_id_3\r\n}\r\n```\r\n"],"labels":["bug","backend\/azure","new"]},{"title":"Update README.md","body":"Updated with grammatical updates: This is my first PR, and I am looking forward to contributing more.\r\n\r\n<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES | UPGRADE NOTES | ENHANCEMENTS | BUG FIXES | EXPERIMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n-  \r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33816) <br\/>All committers have signed the CLA.","Thanks for the submission! I've put it on the docs team triage queue. Since these changes deal with style, the docs team has the final say on these types of changes. Thanks again!","Is this  issue solved ?\r\n","i want to resolve this issue.","Hi @Yashrajput7232, the original author (@mesrikanthreddy) would need to accept the changes proposed by @trujillo-adam. If you want to open a PR that makes the original changes and also incorporates these recommendations, I can get it re-reviewed and we can close this one. Thanks!"],"labels":["waiting-response","documentation","tw-reviewed"]},{"title":"backend\/s3: `assume_role_with_web_identity` ignores related environment variables","body":"### Terraform Version\r\n\r\n```shell\r\nv1.6.0-beta1\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nterraform {\r\n  backend \"s3\" {\r\n    bucket = \"jb-test\"\r\n    key    = \"path\/to\/statefiles\"\r\n    region = \"us-east-1\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/pull\/31276#issuecomment-1703530574\r\n\r\n### Expected Behavior\r\n\r\nThe `assume_role_with_web_identity` block should respect the corresponding AWS environment variables.\r\n\r\n### Actual Behavior\r\n\r\nRole assumption flow fails.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Set the `AWS_ROLE_ARN` and `AWS_WEB_IDENTITY_TOKEN_FILE` environment variables.\r\n2. `terraform init`\r\n3. Observe failure\r\n\r\n### Additional Context\r\n\r\nOriginally reported in this comment: https:\/\/github.com\/hashicorp\/terraform\/pull\/31276#issuecomment-1703530574\r\n\r\n### References\r\n\r\nRelates #31276 \r\nRelates #33730","comments":["@manobi this should actually work, since the AWS SDK for Go handles the environment variables. I've created a branch that (re-)enables API request logging for the authentication flow at https:\/\/github.com\/hashicorp\/terraform\/tree\/s3\/log-base.\r\n\r\nCan you try again using that branch and with the environment variable `TF_LOG` set to `DEBUG` and share the log, please?\r\n\r\nCan you also please share your backend configuration, please?","I would love to help with this test, but I only have this trust relationship for my CI\/CD runners.\n\nI can't try it until it's published as a docker image on docker hub or something like it.","The logging update should be released in `v1.6.0-beta2`","Have it already being released to public docker registries?","No, `v1.6.0-beta2` has not yet been built and released at all, thus it is not in Dockerhub. See also: https:\/\/hub.docker.com\/r\/hashicorp\/terraform\/tags","Hi @manobi, `v1.6.0-beta3` is now on Dockerhub","Hi @manobi. Now that v1.6 has been released, are you still seeing this problem? If so, can you please share your backend configuration and a debug log generated by setting the environment variable `TF_LOG` to `DEBUG` ","I'm seeing a similar issue with v1.6.5, however the steps to reproduce are a bit different (set `AWS_WEB_IDENTITY_TOKEN_FILE` env var, set [`role_arn`](https:\/\/developer.hashicorp.com\/terraform\/language\/settings\/backends\/s3#role_arn-2) in backend config). \r\n\r\n```\r\nInitializing the backend...\r\n\u2577\r\n\u2502 Error: Missing Required Value\r\n\u2502 \r\n\u2502   on main.tf line 9, in terraform:\r\n\u2502    9:   backend \"s3\" {\r\n\u2502 \r\n\u2502 Exactly one of web_identity_token, web_identity_token_file must be set.\r\n\u2575\r\n```\r\n\r\nPerhaps I'm barking up the wrong tree and this is a SDK issue, similar to: https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/27019.\r\n"],"labels":["bug","backend\/s3"]},{"title":"A function for selecting one of a set of options (or a default) based on another value or on arbitrary predicates","body":"### Terraform Version\r\n\r\n```shell\r\n1.5.6\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nImagine having a variable holding the environment, e.g. `var.env` which could be either `dev`, `staging` or `prod`.\r\nThe use case would be to set a value depending on the variable, e.g.:\r\n\r\n```terraform\r\nvalue = case(var.env == \"dev\", \"foo\", var.env == \"staging\", \"bar\", var.env == \"prod\", \"foobar\", \"default\")\r\n```\r\n\r\n- if `var.env` was equal to `dev`, value would be `foo`\r\n- if `var.env` was equal to `staging`, value would be `bar`\r\n- if `var.env` was equal to `prod`, value would be `foobar`\r\n- if `var.env` was anything else, value would be `default`\r\n\r\n### Attempted Solutions\r\n\r\nN.A.\r\n\r\n### Proposal\r\n\r\nSee above","comments":["Thanks for this suggestion, @paololazzari.\r\n\r\nThis makes me think of another idea discussed somewhere else, but I wasn't able to find a reference quickly so I'll just write an example of what I remember in here:\r\n\r\n```hcl\r\nselect(var.env, {\r\n  \"dev\"     = \"foo\"\r\n  \"staging\" = \"bar\"\r\n  \"prod\"    = \"prod\"\r\n  default   = \"default\"\r\n})\r\n```\r\n\r\nThe above is essentially the same as what you described but with some different punctuation details. This particular form of the idea would likely require some custom expression syntax evaluation behavior similar to how we implement `try` and `func`, because that second argument ought to be a fixed piece of syntax rather than a normal object value so that it can:\r\n\r\n1. Support non-string values in the \"key\" position, for situations where the first argument is of a different type.\r\n2. Treat `default` in the \"key\" position as a special keyword instead of just the string `\"default\"`. To match literally `\"default\"` would require writing it in quotes.\r\n\r\nNeither of these is ideal from a syntax perspective so I imagine more design iteration would be helpful here. Your proposal does allow arbitrary expressions in each case but for the specific use-case you shared that seems to be more of an annoyance than a help since all of your cases are comparisons against the same variable anyway. The `select` syntax I showed above could, if implemented using custom argument evaluation as I mentioned, also be defined as having boolean expressions on the left hand side of that pseudo-mapping, but that diverges far more from the typical meaning of this syntax -- and makes this single-variable-matching situation far more verbose again:\r\n\r\n```hcl\r\nselect({\r\n  (var.env == \"dev\")     = \"foo\"\r\n  (var.env == \"staging\") = \"bar\"\r\n  (var.env == \"prod\")    = \"prod\"\r\n  default                = \"default\"\r\n})\r\n```\r\n\r\n\r\nIn today's Terraform the typical way to express something like this is via a lookup into a map or object. That of course only supports matching strings, because map keys and object attributes are always strings, but it does fit with your example:\r\n\r\n```hcl\r\nlookup({\r\n  \"dev\"     = \"foo\"\r\n  \"staging\" = \"bar\"\r\n  \"prod\"    = \"foobar\"\r\n}, var.env, \"default\")\r\n```\r\n\r\nI would suggest that most recent example as a concise way to get something similar to what you shared done today, assuming that your real example does fit the pattern of choosing a different result based on a fixed set of strings.\r\n"],"labels":["enhancement","functions","new"]},{"title":"Terraform breaks other programs within the same console on Windows","body":"### Terraform Version\r\n\r\n```shell\r\n\ufeffTerraform v1.5.4\r\non windows_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nN\/A. Simply running the CLI will cause this issue.\r\n\r\n### Debug Output\r\n\r\nN\/A. Simply running the CLI will cause this issue.\r\n\r\n### Expected Behavior\r\n\r\nRunning the terraform CLI shouldn't break other programs in the same console on Windows.\r\n\r\n### Actual Behavior\r\n\r\nThere was work done in https:\/\/github.com\/hashicorp\/terraform\/issues\/18242#issuecomment-759846280 about 2.5 years ago to re-do how Terraform handles reading\/writing to the console on Windows so that things like colors would work with Unicode characters, etc.\r\n\r\nWe recently upgraded our CLI to 1.5.4 -- a version that includes the previously mentioned fixes and our deployment pipeline is now failing. Our deployment pipeline builds our application and also runs Terraform to initialize, validate, plan, and apply changes.\r\n\r\nWe have a part of our application that launches a separate process, writes to its standard input, and reads back the standard output. We have tests that confirm this always works. Since upgrading the CLI, our tests for launching this process are now failing. We eventually tracked it down that the process was not processing the standard input we were sending. We couldn't reproduce this when running our tests in Visual Studio, couldn't reproduce when we ran _just our tests_ via console runners, but could reproduce this when doing our full deployment. And once our deployment failed, we couldn't get our tests to run via console runners again -- everything about the console seemed broken!\r\n\r\nThe crux of this problem is that Terraform [is changing the code page of the console](https:\/\/github.com\/hashicorp\/terraform\/blob\/35baa30f36c040aab800bd980d54d2723476aafb\/internal\/terminal\/impl_windows.go#L38) to UTF-8 in an attempt to use a single change to have a large impact. This is a bad idea on Windows because:\r\n\r\n1. It's changing the user's console, not just a local change to the Terraform process.\r\n2. UTF-8 code page is broken on Windows, which causes a problem of affecting other processes after the Terraform CLI has exited. The code page shouldn't need to be set to input\/output Unicode in a console. See [this StackOverflow post](https:\/\/stackoverflow.com\/questions\/388490\/how-can-i-use-unicode-characters-on-the-windows-command-line\/388500#388500).\r\n\r\nFor example, opening a new PowerShell console and running these commands:\r\n\r\n```\r\n$ chcp\r\nActive code page: 437\r\n\r\n$ terraform --version\r\nTerraform v1.5.4\r\non windows_amd64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.6. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n\r\n$ chcp\r\nActive code page: 65001\r\n```\r\n\r\nThe default code page for me on Windows using the English United States locale is CP-437. You can see that Terraform changes it to 65001 (UTF-8). \r\n\r\nThis can affect other processes launched from the console after Terraform has run. For example:\r\n\r\n**Example C program:**\r\n\r\n`target.c`\r\n\r\n```c\r\n#include <windows.h>\r\n#include <stdio.h>\r\n#include <fcntl.h>\r\n#include <io.h>\r\n#include <stdint.h>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    \/\/ Set \"stdin\" to binary mode.\r\n    int modeResult = _setmode(_fileno(stdin), _O_BINARY);\r\n    if (modeResult == -1)\r\n    {\r\n        return NULL;\r\n    }\r\n\r\n    FILE *f = stdin;\r\n\r\n    int i;\r\n\r\n    if (f == NULL)\r\n    {\r\n        fprintf(stderr, \"Failed to set stdin mode.\\n\");\r\n        return -1;\r\n    }\r\n\r\n    printf(\"Successfully set stdin to binary mode\\n\");\r\n\r\n    char header1 = getc(f);\r\n    char header2 = getc(f);\r\n    char header3 = getc(f);\r\n\r\n    if (\r\n        header1 != 'A' ||\r\n        header2 != 'B' ||\r\n        header3 != 'C')\r\n    {\r\n        fprintf(stderr, \"Invalid file header: \");\r\n        fprintf(stderr, \"(decimal=%d, hex=%x, char=%c)\", header1, header1, header1);\r\n        fprintf(stderr, \"(decimal=%d, hex=%x, char=%c)\", header2, header2, header2);\r\n        fprintf(stderr, \"(decimal=%d, hex=%x, char=%c)\", header3, header3, header3);\r\n        fprintf(stderr, \".\\n\");\r\n        return -1;\r\n    }\r\n\r\n    printf(\"The file header is correct! :)\");\r\n    return 0;\r\n}\r\n```\r\n\r\nNOTE: We actually use third-party binaries in practice, but this is a small example of how to reproduce the bug.\r\n\r\nCompile the above:\r\n\r\n```\r\n$ cl .\/target.c\r\n```\r\n\r\nExample C# source program:\r\n\r\n`Source.cs`\r\n\r\n```\r\ninternal class Program\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        var targetExePath = args[0];\r\n\r\n        var sourceData = Encoding.UTF8.GetBytes(\"ABC\");\r\n\r\n        var processStartInfo = new ProcessStartInfo(targetExePath);\r\n\r\n        processStartInfo.RedirectStandardInput = true;\r\n        processStartInfo.RedirectStandardOutput = true;\r\n        processStartInfo.RedirectStandardError = true;\r\n        processStartInfo.UseShellExecute = false;\r\n        processStartInfo.CreateNoWindow = true;\r\n\r\n        using (var process = new Process())\r\n        {\r\n            process.StartInfo = processStartInfo;\r\n\r\n            process.Start();\r\n\r\n            Console.WriteLine($\"Standard input code page = {process.StandardInput.Encoding.CodePage}\");\r\n\r\n            process.StandardInput.BaseStream.Write(sourceData, 0, sourceData.Length);\r\n\r\n            process.StandardInput.Close();\r\n\r\n            var stdout = process.StandardOutput.ReadToEnd();\r\n            var stderr = process.StandardError.ReadToEnd();\r\n\r\n            var hasExited = process.HasExited;\r\n\r\n            if (hasExited)\r\n            {\r\n                var exitCode = process.ExitCode;\r\n                if (exitCode != 0)\r\n                {\r\n                    Console.WriteLine($\"Failed to exit successfully (ExitCode={process.ExitCode}).\\nStandard Error:\\n{stderr}\");\r\n                }\r\n            }\r\n            else\r\n            {\r\n                Console.WriteLine(\"Failed to cleanly exit after writing to stdin (manually killing).\");\r\n                process.Kill();\r\n            }\r\n\r\n            Console.WriteLine($\"Standard Output:\\n{stdout}\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nCompile the above using .NET Framework 4.8 (not .NET Core\/.NET 6, I believe this issue is worked around).\r\n\r\nTest:\r\n\r\n```\r\n$ chcp 437\r\n\r\n$ Source.exe target.exe\r\nStandard input code page = 437\r\nStandard Output:\r\nSuccessfully set stdin to binary mode\r\nThe file header is correct! :)\r\n\r\n$ terraform --version\r\nTerraform v1.5.4\r\non windows_amd64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.6. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n\r\n$ chcp\r\nActive code page: 65001\r\n\r\n$ Source.exe target.exe\r\nStandard input code page = 65001\r\nFailed to exit successfully (ExitCode=-1).\r\nStandard Error:\r\nInvalid file header: (decimal=-17, hex=ffffffef, char=\ufffd)(decimal=-69, hex=ffffffbb, char=\ufffd)(decimal=-65, hex=ffffffbf, char=\ufffd).\r\n\r\nStandard Output:\r\nSuccessfully set stdin to binary mode\r\n```\r\n\r\nThis has the same output on Windows 10 and Windows 11.\r\n\r\nI personally believe that it shouldn't be Terraform's responsibility to change configuration about the current console, especially the code page (as I mentioned, it's typically misinformed\/bad advice). The gist for a fix is that Terraform should most likely be using Window's Console APIs instead of File APIs for reading\/writing text to the console. Here's how Python's CLI handles this: https:\/\/github.com\/python\/cpython\/blob\/5141b1ebe07ad54279e0770b4704eaf76f24951d\/Modules\/_io\/winconsoleio.c\r\n\r\nAs a temporary work around\/fast fix, Terraform could set the code page and then reset it once the CLI exits?\r\n\r\n### Steps to Reproduce\r\n\r\n1. `chcp 437`\r\n2. `terraform --version`\r\n3. `chcp` -- prints `65001`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this feature request! The level of detail and code inspection is very appreciated.\r\n\r\nIf you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","@TheCloudlessSky What version of PowerShell are you using?","@kmoe v5 but I've also confirmed just with cmd and PowerShell 7.3.","Unfortunately it seems I didn't retain good enough notes about everything I was referring to when making the change that caused this problem, but we made these changes in response to recommendations from Microsoft and so expected that they were good advice. I'm sorry that turns out not to be true. :confounded: \r\n\r\n---\r\n\r\nOne reference I do still have a link to from my notes is [Classic Console APIs versus Virtual Terminal Sequences](https:\/\/learn.microsoft.com\/en-us\/windows\/console\/classic-vs-vt). That document is primarily concerned with the _other_ big thing we changed at the same time -- activating virtual terminal processing instead of using the legacy console API -- but it does still briefly touch on UTF-8 support:\r\n\r\n> ## Unicode\r\n>\r\n> UTF-8 is the accepted encoding for Unicode data across almost all modern platforms, as it strikes the right balance between portability, storage size and processing time. However, Windows historically chose UTF-16 as its primary encoding for Unicode data. Support for UTF-8 is increasing in Windows and use of these Unicode formats does not preclude the usage of other encodings.\r\n>\r\n> The *Windows Console* platform has supported and will continue to support all existing code pages and encodings. Use UTF-16 for maximum compatibility across Windows versions and perform algorithmic translation with UTF-8 if necessary. Increased support of UTF-8 is in progress for the console system.\r\n>\r\n> UTF-16 support in the console can be utilized with no additional configuration via the _W_ variant of all console APIs and is a more likely choice for applications already well versed in UTF-16 through communication with the `wchar_t` and _W_ variant of other Microsoft and Windows platform functions and products.\r\n>\r\n> UTF-8 support in the console can be utilized via the A variant of Console APIs against console handles after setting the codepage to 65001 or `CP_UTF8` with the `SetConsoleOutputCP` and `SetConsoleCP` methods, as appropriate. Setting the code pages in advance is only necessary if the machine has not chosen \"Use Unicode UTF-8 for worldwide language support\" in the settings for Non-Unicode applications in the Region section of the Control Panel.\r\n\r\nFor this section in particular, I will concede that we don't seem to be following this recommendation fully, although I'm also not sure this section _is_ making a single clear recommendation, as opposed to just gesturing vaguely at a handful of different options.\r\n\r\nSpecifically, we are using `SetConsoleOutputCP` to change the console's \"legacy codepage\", but I believe all subsequent writes to the console are effectively using the _W_ variants of the file I\/O APIs, although we're doing that only indirectly through the Windows implementation of Go's `os` package.\r\n\r\nBased on what I remember about these parts of the Windows API (my memory is spotty, since it's been a long while since I did _real_ Windows API dev), only the _A_ variants of the Win32 API functions are concerned with legacy codepages, and so it's quite possible that we don't actually need to change the legacy code page of the console now that modern Windows versions have a fully-unicode-aware text buffer. Older versions of Windows -- pre-Windows 10 -- could only retain in the console buffer characters from the currently-selected legacy codepage, but this documentation suggests that was fixed as part of all of the recent console modernization work.\r\n\r\nI think the best next step here then would be to try removing the `SetConsoleOutputCP` call altogether and then encourage Terraform to write some astral plane characters like emoji to its output, and see if my hunch is correct that the modernized text buffer will retain the emoji characters even with the console codepage set to 437.\r\n\r\nThe following should test it, I think:\r\n\r\n1. Comment out the call to change the codepage in the Terraform code (linked from the initial issue comment above) and recompile.\r\n2. Make sure you are running in a console with its legacy codepage set to 437, by running `chcp`.\r\n3. Write a Terraform configuration that includes an intentionally-failing `check`, or `precondition`, or similar whose `error_message` includes an emoji character or other astral plane character.\r\n4. Run `terraform plan` and inspect the rendered diagnostic. If the astral plane character appears correctly in the terminal, this test is successful and therefore we should be able to safely remove the `SetConsoleOutputCP` call.\r\n\r\nI'd note that this will probably still cause Terraform to leave the terminal in Virtual Terminal Processing mode after it exits, which is not ideal, but hopefully still less troublesome than changing the console's legacy codepage.\r\n\r\nI don't have a functioning Windows system handy to test this on right now, so I can't try this myself immediately, but hopefully the above is useful to someone who picks up this issue to work on in the future.\r\n"],"labels":["bug","windows","new"]},{"title":"Allow capturing and passing module source as a variable","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.2\r\n\r\nThis is irrelevant because this issue is like 10 years old now.\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nCan be a solution for people who want variable support in module sources.\r\n\r\n### Attempted Solutions\r\n\r\nVariables in modules sources - was rejected because modules need to be installed as dependencies before they can be used. This makes sense.\r\n\r\n### Proposal\r\n\r\nAllow doing this:\r\n\r\n- In the module `qq`\r\n\r\n  ```hcl\r\n  variable \"qwe\" {\r\n    type = module_source\r\n  }\r\n\r\n  module \"abc\" {\r\n    source = var.qwe\r\n  }\r\n  ```\r\n\r\n- Outside of the module:\r\n\r\n  ```hcl\r\n  module_source \"qwe\" {\r\n    source = \".\/qwe\" # could also be git: or any other source\r\n    # only source is allowed, and no instance of the module is created here - just the module source is loaded and made available for other variables\r\n  }\r\n\r\n  module \"qq\" {\r\n    source = \".\/qq\"\r\n  \r\n    abc = module_source.qwe\r\n    # other parameters that \"qq\" mod accepts\r\n  }\r\n  ```\r\n\r\nThe version of `qwe` is locked and specified in the outside module.\r\n\r\n### References\r\n\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/1439\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/25587","comments":["Thanks for this feature request! This issue looks fairly close to https:\/\/github.com\/hashicorp\/terraform\/issues\/25587 -- do you think this is a use case of that issue, or an alternate implementation? \r\n\r\nIf you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","This is exactly in relation to that issue, but this is an alternative approach that is actually doable, in contrast to the approach that people want at #25587 and similar issues (there are severe architectural issues that have been blockers for like 10 years now and there is no solution in sight).\r\n\r\n---\r\n\r\nIf we had #25587 implemented - we would not need this, but if we don't have #25587 this is much more practical. I suspect the only way to actually implement #25587 is to do what I suggest here, but it is not clear just yet.\r\nWe can have both this and #25587 (i.e. one does not supersede\/replace the other) as the niche use cases are different, but for what I need to do in my more general use case both this and #25587 would be sufficient (there is no way #25587 will be implemented though since it is the same as the eight-year-old #1439).","Adding my voice here. We use Terraform Cloud extensively across multiple teams and have shared modules in github repositories. Version tracking is almost impossible except manually.. and it is not just the base code but all previous Terraform Cloud workspaces that were deployed have to be version-pinned per module.\r\n\r\nTerraform cloud also does not allow a pre-init script, and the pre_plan key does not work for this. There's no clear mechanism between github and Terraform Cloud where scripts can be added to replace these versions before terraform runs. \r\n\r\nAny mechanism that allows a module source to be dynamically specified in terraform code will be welcome.\r\n\r\nA mechanism that can substitute variables (or environment variables) in the git path or ref will be particularly helpful.\r\n"],"labels":["enhancement","new"]},{"title":"`terraform test`: dump the complete state file when a destroy operation fails.","body":"### Terraform Version\r\n\r\n```shell\r\nterraform 1.6\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI'm interested in using the `test` command, in the upcoming terraform 1.6 release. We currently have a similar workflow using the [`tftest` pytest library](https:\/\/github.com\/GoogleCloudPlatform\/terraform-python-testing-helper). One thing we've found useful is that tftest leaves the state behind when the destroy action fails. This gives us the opportunity to investigate what happened, update the config or permissions if needed, and re-execute the destroy to cleanup any lingering resources.\r\n\r\nCurrently, the `test` command maintains state only in memory. I am curious what this means for destroy-time errors. Do the resources just remain lingering? Is there any way to inspect the failure and cleanup?\r\n\r\n### Attempted Solutions\r\n\r\nn\/a\r\n\r\n### Proposal\r\n\r\nPerhaps on error, write the state to a file?\r\n\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @lorengordon, thanks for the feedback.\r\n\r\nCurrently, `terraform test` will provide a set of diagnostics that describes any state left behind if the destroy action fails. I do like the idea of potentially writing out a file. I will investigate this, and see how practical it is.\r\n\r\nThanks again!","I've considered this a bit further and I will say that we won't see any changes here for the launch of `v1.6`. The test command at the moment doesn't have the ability to recover or use state files, so printing out the state file won't really give additional information over the diagnostics that are already produced. It's also not straightforward how Terraform should handle state files from alternate modules and\/or registry modules and this needs more thought. We'd want to do this properly instead of trying to rush something in place for `v1.6`.\r\n\r\nI did quickly implement #33809 which means the test command will return a non-zero return code if the destroy operation fails, making it easier to detect as a fail case.\r\n\r\nI will leave this issue open though, as we are investigating approaches to custom state management within test files and it may be that with fine-grained state management we can actually use any dumped state files to make the tidy up process easier. Potentially, we'll see this arriving in Terraform `v1.7`.","I suppose I was figuring I'd be able to use the state file locally, with just `terraform destroy`. But there are probably some details of how `test` works that I haven't considered.","Yeah, I think that works for the very simple case. But it gets complicated with edge cases as the test command could be tearing down infrastructure that it has created based on other modules that are held in the registry, terraform test could be executing within a CI pipeline or in TFC, there could already be state files in the local directory already.\r\n\r\nI really like this idea as a feature, but I'd want to do it properly and realistically that means giving it a full development cycle, and as a bonus that means we can make it work with the other state operations we've had as suggestions which just means it'll be even better when it does launch. All changes we make have to be backwards\/forwards compatible and I wouldn't want to lock this out of future integrations because we just rushed and solved the simple case.","Oh yes, I don't think it's that urgent for us. I honestly wasn't expecting an implementation of this request for the 1.6 release. Sorry if I wasn't clear on that. Really appreciate the work and attention you've put into the question. Thanks again!","Will there be an intention on this, when you have to manually destroy ~30 aws resources terraform test stop being easy to use","Hi @albertorm95, `terraform test` should already be attempting to destroy the resources it creates - is there a particular reason that this isn't working for you?\r\n\r\nI'd imagined what we implemented here would be about making a rare edge case easier to recover from rather than something that would be used as part of a regular test cycle. Ideally, Terraform would successfully clean up on every test execution rather than it failing and dumping the state file every time.\r\n\r\nMaybe you could post your use case into the [forums](https:\/\/discuss.hashicorp.com\/c\/terraform-core\/27), and we could try and get the destroy operation to execute successfully?","Hello! @liamcervante \r\n\r\nWhat about an scenario where the test is executed on a remote system and for any reason the system failed before destroying or in the destroy process, those resources will be there without a state that represent them\r\n\r\nIf there is state file on the test we will able to reference it and execute a manual destroy \ud83e\udd14","Hi @albertorm95, I agree that there are valid use cases for this. It's a question of priority for us internally, so I was curious if you had a use case for why you need the state file regularly or if we could work to ensure that the destroy operation simply doesn't fail for you?\r\n\r\nI'll try and explain why I view this as a lower priority than you perhaps do. As far as I'm aware, the state file would only be required in rare cases when the destroy operation fails - regardless of the environment that Terraform is executing in. If the destroy operation is failing regularly enough that the `terraform test` command becomes unusable, I'd rather focus on improving the reliability of the destroy operation. I don't think that expecting users to execute a manual `terraform destroy` regularly is a particularly valid solution to an unreliable clean up operation. It does make the required manual cleanup easier but ideally we'd avoid the manual cleanup altogether. \r\n\r\n","I see, I agree with you on this as a lower priority, this happened to me with and AWS error:\r\n\r\n```\r\nTerraform encountered an error destroying resources created while executing tests\/test.tftest.hcl\/apply_foo.\r\n\u2577\r\n\u2502 Error: waiting for ECS Service (arn:aws:ecs:eu-central-1:1234567:service\/foo-foo-test-fra\/foo-master-test) delete: RequestError: send request failed\r\n\u2502 caused by: Post \"https:\/\/ecs.eu-central-1.amazonaws.com\/\": read tcp IP:PORT->ANOTHERIP:ANOTHERPORT: read: can't assign requested address\r\n\u2502 \r\n\u2502 \r\n\u2575\r\n\r\nTerraform left the following resources in state after executing tests\/test.tftest.hcl\/apply_foo, and they need to be cleaned up manually:\r\n  - aws_acm_certificate.foo\r\n  - aws_cloudwatch_log_group.foo[\"centralized\"]\r\n  - aws_cloudwatch_log_group.foo[\"master\"]\r\n  - aws_cloudwatch_log_group.foo[\"remotestate\"]\r\n  - aws_cloudwatch_log_group.foo[\"simple\"]\r\n  - aws_ecs_cluster.foo\r\n  - aws_ecs_service.foo[\"master\"]\r\n  - aws_ecs_task_definition.foo[\"centralized\"]\r\n  - aws_ecs_task_definition.foo[\"master\"]\r\n  - aws_ecs_task_definition.foo[\"remotestate\"]\r\n  - aws_ecs_task_definition.foo[\"simple\"]\r\n  - aws_iam_role.foo[\"centralized\"]\r\n  - aws_iam_role.foo[\"master\"]\r\n  - aws_iam_role.foo[\"remotestate\"]\r\n  - aws_iam_role.foo[\"simple\"]\r\n  - aws_lb_listener_rule.foo[\"centralized\"]\r\n  - aws_lb_listener_rule.foo[\"master\"]\r\n  - aws_lb_listener_rule.foo[\"remotestate\"]\r\n  - aws_lb_listener_rule.foo[\"simple\"]\r\n  - aws_lb_target_group.foo[\"centralized\"]\r\n  - aws_lb_target_group.foo[\"master\"]\r\n  - aws_lb_target_group.foo[\"remotestate\"]\r\n  - aws_lb_target_group.foo[\"simple\"]\r\n  - aws_secretsmanager_secret.foo[\"centralized\"]\r\n  - aws_secretsmanager_secret.foo[\"master\"]\r\n  - aws_secretsmanager_secret.foo[\"remotestate\"]\r\n  - aws_secretsmanager_secret.foo[\"simple\"]\r\n  - random_id.suffix\r\n  - module.foo_kms_key.aws_kms_alias.this\r\n  - module.foo_kms_key.aws_kms_key.this\r\n  - module.foo_lb.aws_lb.lb\r\n  - module.foo_lb.aws_lb_listener.https_forward\r\n  - module.foo_lb.aws_lb_target_group.target_group\r\n  - module.foo_lb_sg.aws_security_group.sg\r\n  - module.foo_sg.aws_security_group.sg\r\ntests\/test.tftest.hcl... fail\r\n```\r\n\r\n\r\nWhat about in this list append the id of the resource:\r\nmodule.foo_sg.aws_security_group.sg : ID\r\n\r\nthat will make it easier for the user to find and destroy the resource, WDYT?","Thanks for the context @albertorm95! \r\n\r\nA quick note on the ID idea, which we did experiment with. Unfortunately, the idea of an `id` attribute is not constant throughout the Terraform provider ecosystem. Some cloud providers do assign a unique global identifier to their resources, and then some Terraform providers do choose to expose that ID but it's not consistent. For example, the AWS provider uses the `arn` attribute as the global unique identifier for many resources and doesn't expose anything called `id`. Terraform core (where the test command operates) doesn't know which attribute a given provider is using as an `id` so it's not aware of what attribute it should provide. We have had talks internally about creating some kind of global `id` attribute that Terraform can use where appropriate, so it maybe we can use that when\/if it becomes available.\r\n\r\nFor your use case, would you mind posting into the forums? That way I can dig into it with you there without sending notifications to everyone subscribed to this Github issue. I think any discussion about the format of the output, or potential improvements should stay in this ticket. But for specific use cases it's better to move into the forums.\r\n\r\nThanks!","Hey @lorengordon (and others in this issue), the Terraform team is doing research into this problem, and I'd love to chat to learn more. Please reach out to me oismail@hashicorp.com and we can schedule a time to chat!"],"labels":["enhancement","terraform test"]},{"title":"S3 backend configuration page can be more clear","body":"### Terraform Version\n\n```shell\nv1.5.x\n```\n\n\n### Affected Pages\n\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/website\/docs\/language\/settings\/backends\/s3.mdx\n\n### What is the docs issue?\n\nThis page comes under \"backends\". The name of the file and the first headline are \"S3\". Fine\r\nThere is a series of headlines after S3. They are at the same level. Example Configuration, S3 bucket permissions, DynamoDB table permissions\r\nBy now, you have an idea the page is trying to tell you how to use the S3 backend, which is good\r\nThen comes Data Source Configuration\r\nIt says the use of this is to access information from \"other terraform configurations\". My issue is this -- is this section really needed in this page? IMO it can come under \"See also\", as this is not the essence of how you use S3 backend. Or is it?\r\nTo me the section is very confusing. Suddenly it is talking about some strange root module. Here the documentation loses direction, because new questions arise - why do I need information about some mysterious unknown \"other terraform configuration\"? What are the data items I need to implement my S3 backend? Can I please not use this \"other\" configuration, as I have enough on my plate already? Why is dependency on some assumed other external entity? This is too much!\r\nOTOH if there is no real dependency, then please don't have it in this place, as the same level of heading as \"S3\". Please Step your headings. Let me know what the relationship is between the title and content. Allow me to see if something can be skipped, and what portions exactly. Because as you go on reading, there is no point at which this Data Source Configuration seems to end.\r\nFor the above reason, the page does not help\n\n### Proposal\n\nMove Data Source Configuration to the end or to another page, if it is not essential to making the S3 backend work\r\nUse hyperlinks to the references made to other features, such as \"data source\", \"root module\", \"outputs\", \"nested modules\" etc\r\nIs \"Configuration\" part of \"Data Source Configuration\"? If not I guess this is where Data Source Configuration section ends. But having just talked about Data Source Configuration the page is now talking about Configuration. I am lost as to what configuration is it now, and what about the configuration that was set out to discuss - the s3 backend configuration?\n\n### References\n\n_No response_","comments":["Thanks for this docs feedback!","Hey, I am interested in working on this issue. I would like to reformat the page so that there is another section for \"more about configuration. Then add the data configuration section. Should I begin making changes?","@teelrabbit Good question. Let me see if I can get someone to comment on the intentions behind the current page structure. You could start work on a PR, but I don't want you to have to throw away work if there is some reason things are as they are today. ","Hey, sounds good. I'll wait until you get some more details and then proceeded accordingly. Thanks for responding \ud83e\udee1","Hey @krlinus and @teelrabbit \ud83d\udc4b I'm the technical community manager for the Terraform AWS Provider, which also covers the S3 backend. I took a bit of time to review the current page structure, and have some follow-ups.\r\n\r\nThe page currently follows a pattern that matches how the other backend pages are loosely structured, with three main H2 headings:\r\n\r\n- **Example Configuration:** a basic example showing how to use the backend with Terraform\r\n- **Data Source Configuration:** a basic example showing how to use the backend with the [`terraform_remote_state` data source](https:\/\/developer.hashicorp.com\/terraform\/language\/state\/remote-state-data)\r\n- **Configuration Variables:** documentation of all configuration arguments, and what they mean or do\r\n\r\nAside from the headers being titled slightly differently, and some pages (including S3) having additional sub-headings beneath these, the formatting is relatively consistent. \r\n\r\nGiven how intertwined they are, I'm not sure it would make sense to move the reference for `terraform_remote_state` to a separate area in the documentation, so from my perspective, the content on the page is all relevant, and should be kept.\r\n\r\nWith the information above, does anyone still feel that the document should be reworked, or could be clarified in some way?","i would like to work on that issue\r\n","Hi @Yashrajput7232, per https:\/\/github.com\/hashicorp\/terraform\/issues\/33785#issuecomment-1850797146 we are looking for feedback from the original commenters as to whether there is work to do here. Thanks for your interest. ","Hi @justinretzolk thanks for your response. It's been a 2-3 months since I opened this issue. In this time, I have been able to make the s3 state storage work, however, I have not used the locking feature\r\nFrom your comment I understand you are following a consistent pattern for all the pages because that is what you think fit\r\nAs a user trying to master the product, I am not really expecting or asking for the same headings to be present in all the pages. True, it makes your work easier and I am not saying you should not make your work easier. So what can be done about this page?\r\n\r\nHaving moved on a bit and not having the benefit of being a first timer, my feelings regarding this are not the same or with the same intensity.\r\n\r\nI suggest reviewing the content and putting in a bit more contextual glue so someone reading this is guided properly, even if they land on this page directly and not aware of what \"data source configuration\" in the context of this topic means, or that you have the explanation elsewhere if they care to read from the start\r\n\r\nCurrently this section starts like \"To make use of the S3 remote state in another configuration\" where the title says \"Data source configuration\". What is being referred to as \"another configuration\" here?\r\nIn retrospect, it turns out what it really means to say is another terraform project (using configuration as slang for it). If you said so it would be way clearer, and I would know to skip that section because I am still working on the very first one\r\n"],"labels":["documentation","backend\/s3","new"]},{"title":"Provider Aliases Error Handling Creates Misleading Errors","body":"### Terraform Version\n\n```shell\n1.5.6\n```\n\n\n### Terraform Configuration Files\n\nmain.tf\r\n```terraform\r\nterraform {\r\n  required_version = \">=1.5.6,<1.6\"\r\n  required_providers {\r\n    snowflake = {\r\n      source  = \"snowflake-labs\/snowflake\"\r\n      version = \">=0.69.0,<=0.70\"\r\n    }\r\n  }\r\n}\r\n\r\nprovider \"snowflake\" {\r\n  alias   = \"account\"\r\n  account = \"mock_account\"\r\n  role    = \"SYSADMIN\"\r\n}\r\nprovider \"snowflake\" {\r\n  alias   = \"sys\"\r\n  account = \"mock_account\"\r\n  role    = \"SYSADMIN\"\r\n}\r\nprovider \"snowflake\" {\r\n  alias   = \"security\"\r\n  account = \"mock_account\"\r\n  role    = \"SECURITYADMIN\"\r\n}\r\n\r\nmodule \"module_a_impl\" {\r\n  source = \".\/module_a\"\r\n  providers = {\r\n    snowflake.security=snowflake.security\r\n    snowflake.sys=snowflake.sys\r\n    snowflake.account=snowflake.account\r\n  }\r\n  roles = var.roles\r\n}\r\n\r\nvariable \"roles\" {\r\n  type = list(string)\r\n}\r\n```\r\n\r\nmodule_a\/main.tf\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    snowflake = {\r\n      source                = \"snowflake-labs\/snowflake\"\r\n      configuration_aliases = [snowflake.security, snowflake.sys, snowflake.account]\r\n    }\r\n  }\r\n}\r\n\r\nresource \"snowflake_database\" \"mock_database\" {\r\n  name = \"MOCK_DATABASE\"\r\n  provider = snowflake.sys\r\n}\r\n\r\nresource \"snowflake_role\" \"mock_role\" {\r\n  name = \"MOCK_ROLE\"\r\n  provider = snowflake.security\r\n  # Do something wrong.\r\n  lifecycle {\r\n    repalce_triggered_by=\"0\"\r\n  }\r\n}\r\n\r\nmodule \"module_b_impl\" {\r\n  source = \"..\/module_b\"\r\n  providers = {\r\n    snowflake.security=snowflake.security\r\n    snowflake.sys=snowflake.sys\r\n    snowflake.account=snowflake.account\r\n  }\r\n  roles = var.roles\r\n}\r\n\r\nvariable \"roles\" {\r\n  type = list(string)\r\n}\r\n```\r\n\r\nmodule_b\/main.tf\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    snowflake = {\r\n      source                = \"snowflake-labs\/snowflake\"\r\n      configuration_aliases = [snowflake.security, snowflake.sys, snowflake.account]\r\n    }\r\n  }\r\n}\r\n\r\nresource \"snowflake_database\" \"mock_database\" {\r\n  name = \"MOCK_DATABASE\"\r\n  provider = snowflake.sys\r\n}\r\n\r\nresource \"snowflake_role\" \"mock_role\" {\r\n  name = \"MOCK_ROLE\"\r\n  provider = snowflake.security\r\n}\r\n\r\nvariable \"roles\" {\r\n  type = list(string)\r\n}\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/DustinMoriarty\/b2c68ab3352435182fb4b369cebf68ec\n\n### Expected Behavior\n\nI expect to see the most relevant error about the actual cause of the problem: \r\n\r\nError: Unsupported argument\r\n\r\n  on module_a\/main.tf line 20, in resource \"snowflake_role\" \"mock_role\":\r\n  20:     repalce_triggered_by=\"0\"\r\n\r\nAn argument named \"repalce_triggered_by\" is not expected here. Did you mean\r\n\"replace_triggered_by\"?\r\n\r\n\n\n### Actual Behavior\n\nThe error is buried in many misleading errors about provider type mismatch. The providers are just fine and the command works without errors once I address the actual problem related to the lifecycle block. \n\n### Steps to Reproduce\n\nterraform init\n\n### Additional Context\n\nThis problem is amplified as the terraform configuration get's more complex. With more modules, more providers and more nesting of modules the actual cause is buried in a very large number of misleading errors. I kept the example simple to make it easy to reproduce. However, if you add more modules and providers you will see how bad it can be for a user experience. I consider myself to be a pretty advanced terraform user. However, this problem has been popping up for us periodically for months before I finally understood that the providers have absolutely nothing to do with these errors. Our less experienced terraform users would have zero chance of figuring out the true cause of these errors. \n\n### References\n\n_No response_","comments":["Thanks for this feedback!"],"labels":["bug","new"]},{"title":"Custom Search Engine support in chrome or other modern browser","body":"### Terraform Version\n\n```shell\nnon-version specific\n```\n\n\n### Affected Pages\n\n_No response_\n\n### What is the docs issue?\n\nOften times I google \"azurerm_some_resource_name\" hoping to get a registry.terraform.io hit on the first page.\r\nunfortunately, sometimes the result doesn't show up on the first page... sometimes not even on the first 3 pages of search results.\r\n\r\nWhen searching in chrome, I want to use registry.terraform.io as a specific search engine\r\nheres a tutorial on how to setup a search engine: \r\nhttps:\/\/www.wired.com\/2013\/09\/h2-chrome-omnibox\/\r\n\r\nOther sites that support this are youtube, twitter, soundcloud etc...\r\n\r\nOnce You go into a provider specific page, there is a filter on the sidebar, to filter resources. but the search bar at the top stays at a top level view. and searches across multiple providers etc... I would like a provider specific keyword search.\r\n\r\ne.g. for azurerm if i type 'virtual' I would like to see results of a few virtual_machine resources.\n\n### Proposal\n\nAdd support for provider specific search, to enter a GET request with a ?q=<searchterm> parameter, returning a page of results, instead of just a filter widget.\n\n### References\n\n_No response_","comments":["Hi @cschar,\r\n\r\nIf I'm understanding your request correctly, I think you are asking for a URL template on `registry.terraform.io` that is compatible with the custom search capabilities in popular browsers.\r\n\r\nIf so, I think such a template already exists: `https:\/\/registry.terraform.io\/search\/providers?q=%s`. This returns all of the providers matching a given query, and is accessible from the UI by selecting \"See all\" from the \"Providers\" header in the search box's pop-out panel.\r\n\r\nDoes that achieve what you were hoping for, or is there something missing? Thanks!\r\n","This would be helpful!"],"labels":["waiting-response","documentation","registry","new"]},{"title":"Named resource groups\/namespaces","body":"### Terraform Version\r\n\r\n```shell\r\n1.5\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nEnabling grouping related elements in modules together under a given namespace to allow simplifying names.\r\n\r\nMany cases exist where we have some kind of repeated IaC, but the overhead of using modules for them increases the complexity by needing variables and outputs to be redeclared.\r\n\r\nAn example would be setting up an AWS VPC. You might need to declare VPC endpoints for various AWS services, each needing a security group, security group rules, and other possible configurations like IAM policies. This quickly becomes very hard to separate concerns where the configuration varies slightly each time.\r\n\r\nHaving the ability to group resources into namespaces would make this easier to read since IDEs could collapse\/fold entire namespaces at a time, and all resources under the namespace are enforced to have a consistent base name.\r\n\r\n### Attempted Solutions\r\n\r\nRight now, we can use modules to physically separate groups of resources but this introduces a lot of repeated code by having to redeclare variables and outputs.\r\n\r\nWe could also just use more complex names but this becomes harder to read and is difficult to enforce consistently.\r\n\r\n### Proposal\r\n\r\nAllow using an HCL named block at the top level of a Terraform source file to declare a namespace.\r\n\r\n```terraform\r\nnamespace \"stubs\" {\r\n}\r\n```\r\n\r\nThe namespace can then hold any modules, datasources, resources, etc that relate to the namespace.\r\n\r\nResources within the namespace can refer to each other using their normal names like in existing Terraform code. This can lead to them taking more generalised names that are clearer to their relative module. Take this theoretical example:\r\n\r\n```terraform\r\n\/**\r\n * Resources related to the creation of a stub API server.\r\n *\/\r\nnamespace \"wiremock_stub\" {\r\n  resource \"wiremock_image\" \"image\" {\r\n    version = \"1.2.3\"\r\n  }\r\n\r\n  resource \"wiremock_server\" \"server\" {\r\n    image         = wiremock_image.image.id\r\n    stub_mappings = wiremock_mappings.mappings.json\r\n  }\r\n\r\n  data \"wiremock_mappings\" \"mappings\" {\r\n    ...\r\n  }\r\n}\r\n```\r\n\r\nResources can be interacted with outside their namespace using their fully qualified name.\r\n\r\n```terraform\r\nnamespace \"wiremock_stub\" {\r\n  ...\r\n\r\n  resource \"wiremock_server\" \"this\" {\r\n    ...\r\n  }\r\n\r\n  ...\r\n}\r\n\r\nresource \"something_else\" \"whatever\" {\r\n  stub_server_endpoint = wiremock_stub.wiremock_server.this.url\r\n}\r\n```\r\n\r\nThis could even be extended further to allow features such as those that I suggested in https:\/\/github.com\/hashicorp\/terraform\/issues\/33750 to be considered, like allowing blocks to be optional in bulk.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @ascopes! Thanks for this proposal.\r\n\r\nThe way I understand what you are proposing is that a namespace is essentially an \"inline module\", with a closure-like capability to capture symbols from the surrounding lexical scope.\r\n\r\nThis does seem like a plausible idea in principle, but challenging to design in the details:\r\n- Terraform currently has no situations where one scope can inherit from another like this. I expect it would require an entirely new design for expression evaluation to be able to take into account there being objects from multiple namespaces in scope at the same time.\r\n- Because of the `resource_type.resource_name` syntax for referring to a resource, it isn't generally possible for us to add new kinds of top-level symbol without breaking compatibility with providers that might have conflicting resource types. However, we could potentially mitigate that by calling your namespaces \"inline modules\" and reusing the `module.foo` syntax to refer to them, since that's already reserved as a special case. (This means it would not be possible to have both a normal module and an inline module of the same name in the same scope, but that seems fine to me.)\r\n- Terraform's graph builder currently only supports each node providing symbols to a single module scope, so that subsystem would need considerable rethink to allow an object in one module to directly depend on another without an intermediate \"bridging object\" like an input variable or output value. (Or maybe this would work by quietly inserting anonymous extra graph nodes that bridge across that gap, as a hidden implementation detail.)\r\n- All resources do still need to have a unique address within the configuration for tracking across runs in the state, so we'd need to figure out what those would look like. Again, if we consider these as \"inline modules\" then we can potentially reuse the existing addressing scheme for objects inside modules.\r\n\r\nWith all of that said, my best idea for how to do this right now would be to treat a \"namespace\" as a funny kind of module which gets different treatment in the graph builder and expression evaluator so that the propagation of symbols between scopes happens automatically via hidden proxy nodes that serve a similar role as input variables and output values by transferring dependencies between the separate module namespaces.\r\n\r\nThat's far easier said than done, so I think this would require considerably more design work before we could say whether it's feasible and whether it would meet the use-cases you presented, but we will leave this issue open to remember the use-case and initial ideas.\r\n\r\nThanks again!\r\n"],"labels":["enhancement","new"]},{"title":"a way to initialize modules and providers while ignoring stored backend configuration","body":"### Terraform Version\n\n```shell\nTerraform v1.5.4\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\n```terraform {\r\n  required_version = \">= 1.5.4\"\r\n  backend \"s3\" {\r\n    encrypt = true\r\n  }\r\n\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 5.11.0\"\r\n    }\r\n  }\r\n}\r\n```\r\n\n\n### Debug Output\n\nSnippit of relavant debug logs\r\n```\r\n2023-08-23T10:03:43.584-0400 [INFO]  CLI command args: []string{\"init\", \"-reconfigure\", \"-backend=false\"}\r\n2023-08-23T10:03:43.586-0400 [TRACE] Preserving existing state lineage \"f25705b8-23d5-ed76-22a5-9ae636479294\"\r\n2023-08-23T10:03:43.586-0400 [TRACE] Preserving existing state lineage \"f25705b8-23d5-ed76-22a5-9ae636479294\"\r\n2023-08-23T10:03:43.586-0400 [TRACE] Meta.Backend: working directory was previously initialized for \"s3\" backend\r\n2023-08-23T10:03:43.587-0400 [INFO]  AWS Auth provider used: \"SharedCredentialsProvider\"\r\n2023-08-23T10:03:43.587-0400 [DEBUG] Trying to get account information via sts:GetCallerIdentity\r\n2023-08-23T10:03:43.587-0400 [DEBUG] [aws-sdk-go] DEBUG: Request sts\/GetCallerIdentity Details:\r\n```\n\n### Expected Behavior\n\nThe documentation states that\r\n  -reconfigure            Reconfigure a backend, ignoring any saved\r\n                          configuration.\r\n-backend=false          Disable backend or Terraform Cloud initialization\r\n                          for this configuration and use what was previously\r\n                          initialized instead.\r\n\r\nI would expect that when running the init command with both of those flags, it would ignore the existing saved configuration and re-initialize the root module with no backend \n\n### Actual Behavior\n\nThe command attempts to connect to the backend. If the credentials are expired it fails. \n\n### Steps to Reproduce\n\n1. terraform init -reconfigure -backend=false\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @aaronmell,\r\n\r\nThanks for filing the issue. Can you explain what the end goal is here? The `-reconfigure` option is when you want to reconfigure the stored backend configuration without using that stored configuration at all. The `-backend=false` option is used when you want to fetch providers and modules without [re]initializing the backend in the first place. Terraform cannot ignore the backend configuration entirely, what is the result you expect here?\r\n\r\nThanks!","OK, I think I was confused by the addition of `-reconfigure` here, the goal seems to be to ignore the stored configuration. The key part of the `-backend=false` help text is \"use what was previously initialized instead\". This was not meant to be combined with `-reconfigure`, but I could see how the combined help text of those options could be read that way.\r\n\r\nThe exiting behavior is working as designed, but we can look into allowing this combination as well, though perhaps another way since the word \"reconfigure\" does imply that reconfiguration would happen rather than ignoring it altogether.","So the intent here was I am writing a pre-commit hook that runs terraform validate. I managed to solve it another way by just checking if a .terraform folder already exists in the root module, and if it does skip running init. ","I too was expecting `terraform init -reconfigure -backend=false` would do the equivalent of:\r\n```\r\nrm .terraform\/terraform.tfstate && terraform init -backend=false\r\n```\r\n\r\nie: reinitialise terraform without a backend. This is useful when there is an existing backend and it requires authentication, but we don't want to `plan` or `apply` we just want to `validate` (and so don't need auth). ","Related to https:\/\/github.com\/hashicorp\/terraform\/issues\/29625","Hi all,\r\n\r\nA typical way to run validate in a scenario where backend credentials are unavailable is:\r\n\r\n```\r\nterraform init -backend=false\r\nterraform validate\r\n```\r\n\r\nThe `-backend=false` option instructs Terraform to skip all of the backend-related parts of `terraform init`. That's okay for `terraform validate` because it doesn't use the backend anyway; it's a config-only operation.\r\n\r\nIf you exclude `-reconfigure` as I've shown above, does that get the behavior you need or is something missing?\r\n","If the backend has previously been initialised, then \r\n```\r\nterraform init -backend=false\r\n```\r\nwill attempt to auth using the existing backend, rather than ignore it.\r\n\r\nThis is most obvious when using an AWS backend when AWS SSO creds have expired, eg:\r\n\r\n1. authenticate to AWS SSO\r\n1. `terraform init` - initialise with AWS backend\r\n1. `rm ~\/.aws\/sso\/cache` - remove SSO session token\r\n1.   ```\r\n      \u276f terraform init -backend=false\r\n      Initializing modules...\r\n      \u2577\r\n      \u2502 Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\r\n      \u2502 \r\n      \u2502 Please see https:\/\/www.terraform.io\/docs\/language\/settings\/backends\/s3.html\r\n      \u2502 for more information about providing credentials.\r\n      \u2502 \r\n      \u2502 Error: SSOProviderInvalidToken: the SSO session has expired or is invalid\r\n      ```\r\n     \r\n\r\n\r\n","Thanks for that existing context @tekumara. I hadn't understood that you are trying to initialize a directory where the backend was previously initialized already.\r\n\r\nIndeed, that isn't something Terraform supports today, although I must admit I'm not sure if that omission was an intentional design decision or just an accidental consequence of the implementation; the `-backend=false` option was intended for situations like running `terraform validate` in a CI system that _never_ initializes the backend, rather than for \"de-initializing\" an already-initialized backend in an existing working directory.\r\n\r\nOne avenue we could potentially explore here is to change `-backend=false` so that it skips _all_ actions related to the backend, even if there's already an initialized backend in the working directory. I don't know yet what the consequences would be of doing that; we'd need to go explore the code and understand why the backend verification is happening in this codepath in the first place.\r\n\r\nI _suspect_ what's going on is that `terraform init` is trying to retrieve the state to see if there are any additional required providers there that aren't captured in the current configuration. If so, deciding that `-backend=false` means to ignore the backend entirely then implies that `-backend=false` also means to ignore any state-only provider dependencies, which would be fine if the goal is only to run `terraform validate` since that doesn't rely on information from the state anyway.\r\n\r\nThe combination of both `-reconfigure` and `-backend=false` at the same time doesn't really make sense, because we can't both reconfigure the backend and skip configuring the backend at the same time. I think we should consider making that combination an error that explains why that combination of options isn't supported, although we'd first need to convince ourselves that it's unlikely that anyone would be depending on the current emergent behavior of that combination for some real purpose or else we'd be breaking the Terraform v1.x compatibility promises.\r\n\r\n(If you _do_ want to achieve this \"forget what you know about the backend\" behavior with today's Terraform, one way to do that would be to delete the `.terraform\/terraform.tfstate` file where Terraform tracks which backend is initialized. However, the existence and purpose of that file are an implementation detail rather than a promised interface, so just deleting that file won't necessarily be sufficient in future Terraform versions if the implementation of the backend model were to change.)\r\n"],"labels":["enhancement","cli"]},{"title":"Make cli options 'json' and 'no-color' global options","body":"### Terraform Version\n\n```shell\nTerraform v1.5.5\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nWorking with the cli programmatically, some commands accept these options, others don't, and it seems somewhat arbitrary.\n\n### Attempted Solutions\n\nMapping to every command individually, but have to maintain if each command supports it or not.\n\n### Proposal\n\nMake the terraform cli options 'json' and 'no-color' global options (i.e. before the subcommand).\n\n### References\n\n_No response_","comments":["Thanks for this request! It appears to be a duplicate of https:\/\/github.com\/hashicorp\/terraform\/issues\/23708. If you agree, I will close this issue. Please let me know! Thanks. ","@crw - yes for `no-color`, but for a different reason, but #23708 doesn't include a global `json` option."],"labels":["enhancement","new"]},{"title":"Set custom location of '${HOME}\/.terraform.d' directory for either Checkpoint client or Terraform CLI","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.5\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nTo enhance flexibility and maintain a clean `$HOME` directory, it would be beneficial to have the ability to set a custom directory for either the Checkpoint service or the Terraform CLI configuration directory (not the file) through an environmental variable, similar to the approach used for `TF_CLI_CONFIG_FILE`. \ud83d\ude09\r\n\r\n### Attempted Solutions\r\n\r\nPlease correct me if I'm wrong, but configuring Terraform [environmental variables for CLI config](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/environment-variables) does not affect the location where the Checkpoint client generates its files during the execution of the `terraform init` command. These files are consistently generated in the following [hard coded locations](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/command\/cliconfig\/config_unix.go#L31):\r\n\r\n1. `${HOME}\/.terraform.d\/checkpoint_cache`\r\n2. `${HOME}\/.terraform.d\/checkpoint_signature`\r\n\r\nExemplary configuration attempt:\r\n```\r\nexport XDG_CONFIG_HOME=\"${HOME}\/.config\"\r\nexport XDG_DATA_HOME=\"${HOME}\/.local\/share\"\r\nexport TF_CLI_CONFIG_FILE=\"${XDG_CONFIG_HOME}\/terraform\/terraformrc\"\r\nexport TF_PLUGIN_CACHE_DIR=\"${XDG_DATA_HOME}\"\r\n```\r\n\r\n### Proposal\r\n\r\n1. **Implement the feature to terraform CLI**\r\nOverriding of [Checkpoint configuration](https:\/\/github.com\/hashicorp\/terraform\/blob\/114f876f815b9edbce890246f315932546dd63d2\/checkpoint.go#L26C1-L26C5) through environmental variables, similar to what is done in the [`LoadConfig` function](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/command\/cliconfig\/cliconfig.go#L105) that calls the [`cliConfigFileOverride` function](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/internal\/command\/cliconfig\/cliconfig.go#L127C5-L127C26), I suggest the use of the name `TF_CLI_CONFIG_DIR`. It's possible that this could eventually replace `TF_CLI_CONFIG_FILE`, but I am not fully aware of all the consequences of such a change, except for some obvious considerations such as maintaining CLI backward compatibility. Alternatively, a more specific name related solely to the Checkpoint service could also be considered (point `2.`)\r\n\r\n2. **Implement the feature to go-checkpoint client**\r\nSince the Checkpoint client is not exclusive to Terraform but is also used in other products like Packer, Vagrant, and more, it might be more appropriate to propose a similar improvement as mentioned in the previous point directly to the [go-checkpoint repository](https:\/\/github.com\/hashicorp\/go-checkpoint).\r\n\r\nI would be enthusiastic about implementing such an improvement, but I would appreciate input from the community to determine which approach is more reasonable (if any).\r\n\r\n### References\r\n\r\n_No response_","comments":["See also: #15389","@apparentlymart thank you for the reference, I may have overlooked it initially. After thorough research encompassing related issues and closed PRs, it appears that there were several primary challenges that impeded the implementation of this particular feature:\r\n\r\n1. A lower priority in comparison to other features being developed concurrently\r\n2. Varying conventions across different Unix-like operating systems, with XDG base directories not being widely adopted on macOS\r\n3. The need to maintain backward compatibility was a significant concern\r\n4. Cache-ish and config-ish nature of files that are being stored in `${HOME}\/.terraform.d`\r\n\r\nHowever, I have not come across any evidence of substantial progress towards enabling a custom path for `${HOME}\/.terraform.d` beyond extensive discussions. Do you have any suggestions on how we can advance this topic?\r\n","Hi @kmezynski! Sorry for the terseness of my previous reply. I was intending that as a note to others on our team for context from the previous discussion, not as criticism that you didn't find the other issue.\r\n","This would probably be beneficial to builds that run on platforms like Jenkins where multiple jobs may run on the same node and thus inherit the same environment concurrently."],"labels":["enhancement","new"]},{"title":"Restrict terraform plugins(providers\/modules) from downloading modules from unverified sources","body":"### Terraform Version\n\n```shell\nNA\n```\n\n\n### Use Cases\n\nWe cater to various internal developers across different teams, making it challenging to oversee their internet downloads. While we can confine downloads to the terraform registry source, the `HashiCorp Terraform registry` often redirects to GitHub for certain plugins. Consequently, we lack precise control over external access limitations. \r\n\r\nOne approach is to rely on `tf-plan` output to detect external access calls by modules or data sources. However, this approach might be too late, especially for data sources where code executes during tf-plan.\n\n### Attempted Solutions\n\nWrote a custom wrapper on top of terraform that parse every *.tf files and decide whether to allow this module\/provider or not by matching it with our allowlist. But when request goes to hashicorp-registry and hashicorp registry downloads the modules from github, this defeats our purpose.\r\n\r\nTerraform will download the module named \"module\" from the \"hashicorp\/example\" namespace on the Terraform Registry.\r\n\r\nHowever, it's worth noting that some modules from the Terraform Registry might have dependencies or plugins that are hosted on other platforms like GitHub. In such cases, Terraform will also download those dependencies from their respective sources.\n\n### Proposal\n\n I propose implementing an allowlist or trusted module list. \r\nThis would enable the Terraform binary to adhere to such a list no matter how deeper in the dependency chain the plugin is used.\n\n### References\n\nNA","comments":["Hi @sahilsk,\r\n\r\nFrom what you've written I have the impression that you are primarily concerned about controlling which provider plugins can be used, and that you are only worried about _modules_ because depending on a model can potentially introduce an indirect dependency on another provider.\r\n\r\nIf that is true, I think [the CLI configuration file](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/config-file)'s [provider installation settings](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/config-file#provider-installation) already provide a potential solution.\r\n\r\nAlthough that mechanism is primarily intended for customizing where providers get installed from rather than controlling what gets installed, it is possible to write a provider installation configuration that only specifies how to install a subset of the available providers. Any provider that doesn't match a provider installation rule will immediately fail installation, because there would be no installation method configured for it.\r\n\r\nFor example, if you were to write the following in your CLI configuration then Terraform would be unable to install any provider that doesn't belong to the \"hashicorp\" namespace on the public Terraform Registry:\r\n\r\n```hcl\r\nprovider_installation {\r\n  direct {\r\n    include = [\r\n      \"registry.terraform.io\/hashicorp\/*\",\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nIf the HashiCorp namespace (which contains only Official provider plugins) is not sufficiently constrained then you can instead enumerate a fixed set of exact providers that are allowed:\r\n\r\n```hcl\r\nprovider_installation {\r\n  direct {\r\n    include = [\r\n      \"registry.terraform.io\/hashicorp\/aws\",\r\n      \"registry.terraform.io\/hashicorp\/null\",\r\n      \"registry.terraform.io\/hashicorp\/tls\",\r\n      # ...\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nThere is no comparable mechanism for overriding how _modules_ are installed because module source addresses are typically direct physical locations, but if you constrain which providers are allowed as I showed above then it will effectively block the use of any modules that use providers outside of those allowed, because `terraform init` will fail to install those additional providers.\r\n","@apparentlymart  Thanks for sharing this information. \r\nThis does help restricting one of the possible gap. but the core of the problem still persist and unless we have a way of restricting sources used in `modules` this security gap persist.\r\n\r\nIf i may request , please let me know what you think could be done in CASE-3\r\n\r\n## Case 1: Using source directly\r\n\r\n```\r\n# downloading directly .i.e `terraform-get`\r\nmodule \"malicious-module\" {\r\n    source = \"github.com\/malicious-module.xx\"\r\n}\r\n```\r\n\r\nSolution in this case is simple: scan root module *.tf files and extract all source starting with \"github.com\/xxx\"\r\n\r\n## Case 2: Using module indirectly via dependency\r\n\r\nIn case scanning all the child modules would take ages. So, what we can do is scan the  \"module.json\" file generated at tf-init stage. Downside: Malicious module already being downloaded in the `infrastructure`, though not executed which is a fare compromise for speed\r\n\r\n```\r\nmodule \"awesome-module\" {\r\n   module \"awesome-module-child-1\" {\r\n     module \"awesome-module-child-n\" {\r\n            ......\r\n           source = \"github.com\/malicious-module.xx\"\r\n}\r\n```\r\n\r\n## Case 3: hashicorp registry \r\n\r\nNow thing complicates when hashicorp registry is involved. in this case module is downloaded from registry, however, registry(behind the back) can download the module from anywhere: github\/bitbucket etc, which we won't know ???\r\n\r\n\r\n```\r\n# downloaded via registry or via terraform-get\r\nmodule \"awesome-module\" {\r\n    source = \"hashicorp\/malicious-module-sugar-coated.xx\" -> github.com\/malicious-module\r\n}\r\n```\r\n\r\n\r\n","As currently implemented, modules from the `registry.terraform.io` registry are _always_ from GitHub repositories, and the relationship between the source address and the underlying GitHub repository is systematic.\r\n\r\nFor example, `hashicorp\/dir\/template` is a shorthand for `registry.terraform.io\/hashicorp\/dir\/template`, and corresponds to the GitHub repository `https:\/\/github.com\/hashicorp\/terraform-template-dir`.\r\n\r\nThe general rule is that `registry.terraform.io\/NAMESPACE\/NAME\/TARGETSYSTEM` maps to `https:\/\/github.com\/NAMESPACE\/terraform-TARGETSYSTEM-NAME`.\r\n\r\nI will concede that this is something that _could_ potentially change in future, but I think it's very unlikely to change in the foreseeable future, because the namespace of Terraform Registry is tightly coupled to GitHub's namespace and it would be a big project to change that.\r\n\r\nUsing that information, perhaps you can adapt your approach for Case 2 to also support Case 3. Either you could prohibit using registry addresses altogether -- and always use the corresponding GitHub URL directly -- or you could reject entries from `modules.json` that refer to registry URLs not on your allowlist.\r\n\r\n---\r\n\r\nI forgot to mention in my previous comment that we already have #29362 open representing the possibility of custom installation methods for modules, with similar capabilities to the `provider_installation` block.\r\n\r\nDo you think that something similar to the `provider_installation` block for the module installer would be an acceptable solution? If so, would it be sufficient for it to work only for module registry addresses (where we'd be able to follow a similar wildcard matching strategy as for provider source addresses), assuming that you could solve non-registry addresses as you described in case 2?\r\n","@apparentlymart  Thank you for taking time to address my concern. I REALLY APPRECIATE IT.\r\n\r\nAs far as i can see, it(`provider_installation`) indeed sounds like a promising solution. \r\n\r\nWe thought about blocking hashicorp registry entirely and keep github.com only but added friction hampered developer efficiency. But other way around: blocking all github.com source but hashicorp ones sounds more promising.\r\n\r\nWith that being mentioned, I'd like to introduce a few additional points of concerns while we are it :\r\n\r\n1. Version whitelist\/blacklist: AWS recently introduced their new 5.x provider, which unfortunately lacks backward compatibility. It's important that we establish a method for excluding version 5.x from our system . ( `provider_installation` should have helped here) . Additionally, in cases where problematic packages are identified, a mechanism should be in place to prevent their integration.\r\n\r\n2. Exclusive use of checksums\/signed packages: This is particularly relevant for modules sourced from platforms like GitHub (indirectly through HashiCorp). Many teams typically don't require third-party modules unless they're working on significant projects like EKS\/Kubernetes. However, in situations where they are necessary, implementing a process involving checksums or digital signatures would enable us to compare downloaded modules against verified standards. This step will guarantee that only recognized and validated modules become part of our enterprise infrastructure.\r\n\r\n-----\r\n\r\nOn #29362  I've added our solution to the problem raised there and a suggestion to improve it further \r\n\r\n\r\n\r\n","@apparentlymart  any thoughts? is `module_installation` in the pipeline already? I am willing to offer help(coding\/testing) if it helps expedite it,\r\n\r\n\r\n"],"labels":["enhancement","new"]},{"title":"Allow for \"reversible\" refactoring operations","body":"### Brief\r\n\r\nAs a developer and engineer, I would like to be able to rollback Terraform refactoring changes, by running older versions of my IaC configuration. This would allow me to prevent downtime from resources destroying\/recreating when reverting `import` and `moved` blocks that are not present in older versions of the code.\r\n\r\n### Use Cases\r\n\r\nBusinesses that operate on periodic release windows rather than continuous integration\/deployments often are required to operate all changes such that operations have a clear back-out plan.\r\n\r\nThe existing `moved` blocks are fantastic for refactoring previously untidy and inconsistently named resources, as well as refactoring code in, out, and between modules, etc.\r\n\r\nThe problem comes when you need to rollback a release. Since `moved` operations do not \"remember\" their previous name, older versions of the code must also be modified with `moved` blocks to revert changes, otherwise production-critical resources can get destroyed.\r\n\r\n### Attempted Solutions\r\n\r\nOptions such as running apply operations for code outside regular release windows is not possible, and means that Terraform refactoring changes often have to be accompanied by other unrelated changes that may need backing out if deployment issues to production-critical systems occur.\r\n\r\nRewriting previous versions of Terraform code to use `moved` blocks with the `from` and `to` attributes reversed is always possible, but requires maintaining a second version of the code. This is easily missable and causes problems when using tags to mark releases.\r\n\r\n### Proposal\r\n\r\nAdd an optional attribute to `moved` blocks that allows them to retain their previous identifiers in the state somewhere.\r\n\r\nSuppose my v2023.1 release has this configuration:\r\n\r\n```terraform\r\nresource \"aws_lambda_function\" \"some-function\" {   \/\/ bad naming, eek!\r\n  vpc_config {\r\n    ...\r\n  }\r\n}\r\n```\r\n\r\nI want to refactor our codebase to use consistent naming so I can begin to utilise tools such as tflint to enforce good practises in my team.\r\n\r\nLet's say I change my code in v2023.2 to be the following:\r\n\r\n```terraform\r\nmoved {\r\n  from = aws_lambda_function.some-function\r\n  to   = aws_lambda_function.shopping_cart_api\r\n}\r\n\r\nresource \"aws_lambda_function\" \"shopping_cart_api\" {\r\n  vpc_config {\r\n    ...\r\n  }\r\n}\r\n```\r\n\r\nThis works perfectly when I release v2023.2. Unfortunately I am required to provide the ability to roll back any changes to v2023.1 if something fails in this release due to enterprise business requirements I have no control over.\r\n\r\nIf I were to try and re-run the v2023.1 configuration, then the `moved` block will no longer be specified. This will result in the lambda being destroyed and recreated due to the resource identifier change.\r\n\r\nThe problem becomes more of an issue in this scenario because AWS Lambda ENIs take up to an hour to deregister. In this time, any security group still remains partially attached to old ENIs, so the build will take up to an hour to start recreating the old Lambda after the new one has destroyed. This results in a downtime of up to an hour in the worst case.\r\n\r\n---\r\n\r\nMy proposal is to introduce a new attribute to the `moved` block that is `false` by default to prevent breaking existing configurations:\r\n\r\n```terraform\r\nmoved {\r\n  from       = aws_lambda_function.some-function\r\n  to         = aws_lambda_function.shopping_cart_api\r\n  reversible = true\r\n}\r\n```\r\n\r\nWhen this `reversible` attribute is `true`, I would expect a mapping in the Terraform state to be made that looks something along these lines:\r\n\r\n```\r\n{\r\n  \"reversible_refactoring_mapping\": {\r\n    \"aws_lambda_function.some-function\": \"aws_lambda_function.shopping_cart_api\",\r\n    \"aws_lambda_function.shopping_cart_api\": \"aws_lambda_function.some-function\"\r\n  }\r\n}\r\n```\r\n\r\nWhen I try to run my v2023.1 code on top of config already used for v2023.2, I would expect Terraform to perform these steps:\r\n\r\n1. Observe `aws_lambda_function.shopping_cart_api` no longer exists.\r\n2. Observe `aws_lambda_function.shopping_cart_api` was a previous refactor candidate and has been marked as reversible.\r\n3. Observe `aws_lambda_function.shopping_cart_api` maps to `aws_lambda_function.some-function` internally in the `reversible_refactoring_mapping`.\r\n4. Observe `aws_lambda_function.some-function` is managed by the current Terraform source code.\r\n5. Remove `aws_lambda_function.shopping_cart_api` from the state and reimport it as `aws_lambda_function.some-function` as the first step in applying.\r\n6. Proceed to perform the apply after reimporting the resource under the old name.\r\n\r\nBy doing this, no downtime would be created, and `moved` blocks would be able to manage source-based rollbacks.\r\n\r\n---\r\n\r\nThis could also be extended to the `import` block to allow rollbacks to \"unimport\" resources if the `import` is removed along with the resource declaration that is being imported to.\r\n\r\n```terraform\r\nimport {\r\n  to         = aws_lambda_function.payment_api\r\n  id         = \"PaymentApi\"\r\n  reversible = true\r\n}\r\n\r\nresource \"aws_lambda_function\" \"payment_api\" {\r\n  ...\r\n}\r\n```\r\n\r\nIf I ran an apply for this and then attempted to rollback to an older version, then the `import` block and the `resource` would both be removed. The `reversible` attribute could then instruct Terraform to \"rm\" from the state first before deleting the resource from the plan, thus allowing reverting the import of externally-managed resources without destroying them or needing manual commandline intervention.","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!","This would be an excellent addition to the `moved` directive. Working with Azure APIM Management it takes over an hour to provision sometimes. Not being able to easily roll-back from a `moved` directive is an issue for resources like this."],"labels":["enhancement","new","moved_blocks","plannable-import"]},{"title":"backend\/s3: Allow specifying S3 Object using ARN","body":"### Terraform Version\n\n```shell\nTargeting v1.6\n```\n\n\n### Use Cases\n\nThe S3 object is currently specified using a combination of the `bucket`, `key`, and `region` parameters. This could also be specified using an ARN\n\n### Attempted Solutions\n\nN\/A\n\n### Proposal\n\n_No response_\n\n### References\n\n* #29254","comments":["Some tools in the S3 ecosystem also use a pseudo-URL syntax like `s3:\/\/bucket\/path`, for identifying an S3 object without also having to name the account that owns it or the region where it's stored (since the S3 bucket namespace is global). I think I've also seen that supported by some of AWS's own tools too, but I'm not 100% sure.\r\n\r\nDo you think it would be useful and possible to support that too? Since we require that the bucket already exists before using it with the S3 backend anyway, I imagine that _in principle_ we could automatically pick up the account and region by looking up the bucket, but of course that doesn't mean that it would be _wise_ to implement it that way. I wonder what the tradeoffs are, and how they compare to other tools that use this pseudo-URL syntax.\r\n","That would be a good idea. We could implicitly require that the user is signed in to the correct account. The region can be determined from the bucket, and the AWS SDK for Go v2 can override the region on a per-client or even per-call basis."],"labels":["enhancement","backend\/s3","new"]},{"title":"expand language around the use of `null` in various assignments","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.1\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nImagine a module with\r\n```\r\nvar \"test\" {\r\n  default = \"test\"\r\n}\r\n```\r\nthen a root using this module\r\n```\r\nmodule \"test\" {\r\n  test = var.environment == \"production\" ? \"tset\" : null\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\nN\/A\r\n\r\n### Expected Behavior\r\n\r\nNull behaves as documented \"null is used like a parameter was not supplied at all\", so the \"test\" variable inside the module gets assigned its default value\r\n\r\n### Actual Behavior\r\n\r\nVariable test is assigned \"null\" literally\r\n\r\n### Steps to Reproduce\r\n\r\nProvided above\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @vladimirtiukhtin,\r\n\r\nFor compatibility with earlier version of Terraform, assigning `null` to an input variable can override the default. If you want to prevent that, you can use the [`nullable`](https:\/\/developer.hashicorp.com\/terraform\/language\/values\/variables#disallowing-null-input-values) argument.\r\n\r\nWe use GitHub issues for tracking bugs and enhancements, rather than for questions. While we can sometimes help with certain simple problems here, it's better to use [the community forum](https:\/\/discuss.hashicorp.com\/c\/terraform-core) where there are more people ready to help.\r\n\r\nThanks!","Documentation here https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/types clearly stands\r\n```\r\nnull: a value that represents absence or omission. If you set an argument of a resource to null, Terraform behaves as though you had completely omitted it \u2014 it will use the argument's default value if it has one, or raise an error if the argument is mandatory. null is most useful in conditional expressions, so you can dynamically omit an argument if a condition isn't met.\r\n```\r\nAccording to the documentation terraform must use default value for the module. It is not happening which makes it a bug either in terraform itself or documentation","Thanks @vladimirtiukhtin. That section was carefully worded to specify \"If you set an argument of a _resource_ to null\", because resources were at that time the only place where `null` could be considered the same as unset. The language there could probably be expanded a bit to cover the `null` behavior for other cases like optional object attributes and non-nullable input variables."],"labels":["enhancement","documentation"]},{"title":"Give better feedback when two \"moved\" blocks target the same resource instance address","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.5\r\non windows_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n[code to reproduce issue](https:\/\/github.com\/KenSpur\/terraform-ambiguous-move-statements)\r\n\r\n\r\n### Debug Output\r\n\r\n Should be easy to recreate using the provided code .\r\n\r\n\u2502 Error: Ambiguous move statements\r\n\u2502\r\n\u2502   on module\\main.tf line 23:\r\n\u2502   23: moved {\r\n\u2502\r\n\u2502 A statement at main.tf:19,1 declared that azurerm_resource_group.rg moved to module.resource_group.azurerm_resource_group.moved, but this statement instead declares that module.resource_group.azurerm_resource_group.main moved there.\r\n\u2502\r\n\u2502 Each resource can have moved from only one source resource.\r\n\r\n### Expected Behavior\r\n\r\nThe `azurerm_resource_group` should be successfully moved from the root module to the child module without any errors. This process should not hinder the consumption of child modules that make use of the `moved` statement.\r\n\r\n### Actual Behavior\r\n\r\nReceived the following error:\r\n\r\n\u2502 Error: Ambiguous move statements\r\n\u2502 \r\n\u2502   on module\\main.tf line 18:\r\n\u2502   18: moved {\r\n\u2502\r\n\u2502 A statement at main.tf:19,1 declared that azurerm_resource_group.rg moved to module.resource_group.azurerm_resource_group.moved, but this statement instead declares that module.resource_group.azurerm_resource_group.main moved there.\r\n\u2502\r\n\u2502 Each resource can have moved from only one source resource.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1.`terraform init`\r\n2.`terraform apply`\r\n\r\n### Additional Context\r\n\r\nWhile the provided example uses a local module, the same issue arises when consuming externally hosted modules.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @KenSpur! Thanks for reporting this.\r\n\r\nBased on your code example, I think Terraform is working as intended here. Terraform requires an unambiguous graph of moves so that the validity of the move statements can be decided entirely by the configuration, and conversely so that it isn't possible to get yourself into a trap where you can't proceed because the state contains two resource instances that have apparently moved to the same address.\r\n\r\nYou can get a similar effect to what you described here by exploiting the \"chaining rule\" for moves: if the `to` of one move statement matches the `from` of another then Terraform will bind a resource instance from either of the `from` addresses to the final `to` address. In your case, that would mean changing the `main.tf` move statement to the following:\r\n\r\n```hcl\r\nmoved {\r\n  from = azurerm_resource_group.rg\r\n  to   = module.resource_group.azurerm_resource_group.main\r\n}\r\n```\r\n\r\nSuch a declaration, combined with the existing `moved` block in the child module, creates several possible situations, handled in the following preference order:\r\n\r\n1. State has no objects bound to any of the addresses, in which case both of the `moved` blocks are inert.\r\n2. State already has an object bound to `module.resource_group.azurerm_resource_group.moved`, in which case both of the `moved` blocks are inert.\r\n3. State already has an object bound to `module.resource_group.azurerm_resource_group.main`, in which case the root module `moved` block is inert and the existing object will be rebound to `module.resource_group.azurerm_resource_group.main`.\r\n4. State already has an object bound to top-level `azurerm_resource_group.rg`, in which case both `moved` blocks are active and that object gets rebound to `module.resource_group.azurerm_resource_group.moved`\r\n\r\nAs a general rule, if the source address for an _inert_ `moved` block has an object bound to it then that object will be planned for destruction. That is Terraform's last resort for dealing with an unresolvable situation, and so chaining rule allows you to describe which of multiple conflicting objects should be kept. If you _don't_ have multiple conflicting objects then the single object will end up bound to the final address either way.\r\n\r\nSince Terraform was working as designed here, I'm going to relabel this as an enhancement. My initial idea for what this enhancement represents is to make this error message give a direct suggestion about using the chaining rule -- that is, a more concise version of what I described above -- and also to consider adding a section about this rarer situation to [the refactoring documentation](https:\/\/developer.hashicorp.com\/terraform\/language\/modules\/develop\/refactoring).\r\n\r\nThanks again!\r\n","Hey @apparentlymart,\r\n\r\nThank you for the detailed response. I appreciate the insight into Terraform's design considerations regarding the moved statement.\r\n\r\nFrom the scenarios you outlined, I gather that chaining is an integral part of managing the state transitions effectively. With this in mind, would it be accurate to infer that the best practice, especially when refactoring or working with modules, is to always bind to the initial from address in a move chain?","In your particular situation there is only one possible valid chain, because the root module can declare that an object has moved into its child but the child cannot declare that the object has moved from its parent.\r\n\r\nIf this were all happening in the _same_ module then you as the author would need to make a decision about which object should \"win\" in the edge case where there are multiple objects bound to addresses in the chain. For example:\r\n\r\n```hcl\r\nmoved {\r\n  from = null_resource.a\r\n  to   = null_resource.b\r\n}\r\n\r\nmoved {\r\n  from = null_resource.b\r\n  to   = null_resource.c\r\n}\r\n\r\nresource \"null_resource\" \"c\" {\r\n}\r\n```\r\n\r\nIn the above situation, if the prior state has objects bound to both `null_resource.a` _and_ `null_resource.b` then per the chaining rule it will be the object that was bound to `b` that will get moved to `c` and preserved; the object bound to `a` will be proposed for deletion, because there is no existing vacant address for it to be rebound to.\r\n\r\nThis mechanism is primarily intended to deal with situations where a module has been refactored multiple times and so existing users of the module could potentially be upgrading over multiple refactors at once and therefore need Terraform to follow the chain to reach the module author's intended end state. Things get a little more tricky when refactoring within a module interacts with refactoring _between_ modules, as is true in your case, but the same rule applies: the object that's nearest to the end of the chain will be preserved, and any others earlier in the chain will be proposed for destroy.\r\n\r\nIn practice multiple objects in the chain is rare when using Terraform in a conventional way, but the `moved` mechanism is designed to still produce an unambiguous result even when the input is \"strange\", which can be caused by less common operations such as `terraform state mv` that were performed earlier.\r\n\r\n","@apparentlymart \r\n\r\nThank you for the detailed explanation. Based on your guidance, I've updated my code using the null_resource to create a working example. I appreciate your assistance in clarifying this for me!"],"labels":["enhancement","config","refactoring"]},{"title":"Support Provider Differences in Modules","body":"### Terraform Version\n\n```shell\nTerraform v1.5.1\n```\n\n\n### Use Cases\n\nWe have a library of common modules that are used by hundreds of deployments. We are also in a regulated environment, so change control is key, and therefore we pin all of our deployments to a specific version of Terraform and Providers.  We run into a problem often where we need to expose a new property, or a new resource type, but in order to do so we need to force every deployment to upgrade providers, whether they need the new feature or not, otherwise their pipelines will fail.  \r\n\r\nThe request is for terraform to be able to support differences in provider versions to better support sharing of common modules.\n\n### Attempted Solutions\n\nWe tried doing this with a dynamic block, but it seems like the property is still sent to the provider even if the content is null.\r\n\r\n```\r\nvariable \"vnet_encryption_mode\" {\r\n  description = \"Setting this flag enables VNET Encryption. Allowed values are AllowUnencrypted or DropUnencrypted.\"\r\n  type        = string\r\n  default     = null\r\n}\r\n\r\nresource \"azurerm_virtual_network\" \"this\" {\r\n  address_space       = 10.0.0.0\/24\r\n  location            = \"eastus2\"\r\n  name                = \"myvnet\"\r\n  resource_group_name = \"my-rg\"\r\n  dynamic \"encryption\" {\r\n    for_each = toset([var.vnet_encryption_mode])\r\n    content = each.key == null ? null : {enforcement = each.key}\r\n  }\r\n\r\n  tags = var.tags\r\n}\r\n```\r\n\r\nThat resulted in this:\r\n```\r\n\u2502 Error: Unsupported block type\r\n\u2502 \r\n\u2502   on .terraform\/modules\/common\/network\/main.tf line 198, in resource \"azurerm_virtual_network\" \"this\":\r\n\u2502  198:   dynamic \"encryption\" {\r\n\u2502 \r\n\u2502 Blocks of type \"encryption\" are not expected here.\r\n\r\n```\n\n### Proposal\n\nThis would need to work for full resource types, and also properties within resources.\r\n\r\nFor resources, I would propose introducing an \"init_condition\", where if the init condition isn't met, that resource isn't even exposed to the provider.\r\n```\r\nresource \"azurerm_custom_ip_prefix\" \"this\" {\r\n  name                = \"example-CustomIPPrefix\"\r\n  location            = azurerm_resource_group.example.location\r\n  resource_group_name = azurerm_resource_group.example.name\r\n\r\n  cidr  = \"1.2.3.4\/22\"\r\n  zones = [\"1\", \"2\", \"3\"]\r\n\r\n  commissioning_enabled = true\r\n\r\n  roa_validity_end_date         = \"2099-12-12\"\r\n  wan_validation_signed_message = \"signed message for WAN validation\"\r\n\r\n  tags = {\r\n    env = \"test\"\r\n  }\r\n  lifecycle {\r\n    init_condition {\r\n      condition = azurerm.default.version >= 3.68.0\r\n    }\r\n  }\r\n\r\n  tags = var.tags\r\n}\r\n\r\nresource \"azurerm_virtual_network\" \"this\" {\r\n  address_space       = 10.0.0.0\/24\r\n  location            = \"eastus2\"\r\n  name                = \"myvnet\"\r\n  resource_group_name = \"my-rg\"\r\n  encryption {\r\n      enforcement = var.vnet_encryption_mode\r\n  }\r\n  lifecycle {\r\n    init_condition {\r\n      condition = azurerm.default.version < 3.68.0\r\n    }\r\n  }\r\n\r\n  tags = var.tags\r\n}\r\n```\r\n\r\nFor property differences, introduce a new lifecycle option called ignore_property which would be evaluated before the plan.   This would support some data structure where you could have multiple properties, and different conditions for each of them. \r\n\r\nExample:\r\n```\r\nvariable \"vnet_encryption_mode\" {\r\n  description = \"Setting this flag enables VNET Encryption. Allowed values are AllowUnencrypted or DropUnencrypted.\"\r\n  type        = string\r\n  default     = null\r\n}\r\n\r\nresource \"azurerm_virtual_network\" \"this\" {\r\n  address_space       = 10.0.0.0\/24\r\n  location            = \"eastus2\"\r\n  name                = \"myvnet\"\r\n  resource_group_name = \"my-rg\"\r\n  encryption {\r\n    enforcement = var.vnet_encryption_mode\r\n  }\r\n  lifecycle {\r\n    ignore_properties = {\r\n      [\r\n        property_path = \"encryption\"\r\n        condition     = provider.azurerm.default.version >= 3.68.0\r\n      }\r\n    ]\r\n  }\r\n\r\n  tags = var.tags\r\n}\r\n```\r\nIn this example the encryption property would only be exposed to the provider if the provider version is greater or equal to 3.68.0.\n\n### References\n\n_No response_","comments":["Hi @NYRangers30,\r\n\r\nThanks for filing the request. I don't think this is something which is technically possible given the architecture of Terraform. Terraform requires the provider schema to decode the configuration, which means that the configuration must match the schema for each resource and having an errant `encryption` block in the body is going to fail to decode. We would also have to contend with other schema changes besides the addition of a block, like the addition and removal of block attributes, changes to attribute types, and the data upgrade paths for those schema changes.\r\n\r\nThe usual method for dealing with changes like this in module configuration is similar to how the software is versioned, you would have multiple stable branches which can diverge as necessary to account for the corresponding changes in the provider. In most cases it's usually better to track the provider directly, and require upgrading the provider and module in conjunction, since the improvements to the provider also come with associated bug fixes and required features."],"labels":["enhancement","new"]},{"title":"Modules in Package Sub-directories space optimization","body":"### Terraform Version\n\n```shell\n1.5.1\n```\n\n\n### Use Cases\n\nI'm using module in package sub-directory feature because of we create one git repository to many terraform modules.\r\nWe expect that only sub-directory specified in URL will stored on local disk, actually space used is number of module importer multiply by respository size. \r\nWe think that module relative import must be managed from developer and is not responsibility of import function. \n\n### Attempted Solutions\n\nAFAIK not solution with current implementation\n\n### Proposal\n\nDownload the entire repository but store on local disk only requested folder\n\n### References\n\nSimilar to [8078](https:\/\/github.com\/hashicorp\/terraform\/issues\/8078)\r\nRelated to [28299](https:\/\/github.com\/hashicorp\/terraform\/issues\/28299)","comments":["Thanks for this request! For others who would like to support this request, please use the \ud83d\udc4d reaction on the original description to \"upvote\" this request. Thanks again!"],"labels":["enhancement","new"]},{"title":"New Function keylistmap()","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.4\r\non darwin_arm64\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.5. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\n### Problem to be solved\r\n\r\nIt is not possible to `for_each` over a `list(map)` unless a map comprehension is written which is slower and more tedious to write than `for_each = local.map_object`.\r\n\r\n### Overview\r\n\r\n`for_each` requires a map object as it unpacks to at least `each.key` and optionally includes `each.value`\r\n\r\nGenerally this is done with the following:\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = { x = 1, y = 2 }\r\n  \r\n  name = each.value\r\n}\r\n```\r\n\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = toset([1, 2])\r\n  \r\n  name = each.key\r\n}\r\n```\r\n\r\n`for_each` often loops over a long `map` object somewhere in `locals{}`. These object `keys` can become harder to read at a glance with the rest of the object as the map object gets longer. \r\n\r\n```hcl\r\nlocals {\r\n  eventbridge_map = {\r\n    update_available = {\r\n      description = \"blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Available\"]\r\n          severity    = [\"Informational\"]\r\n          description = [{ prefix = \"Service software update\" }]\r\n        }\r\n      }\r\n    }\r\n    update_failed = {\r\n      description = \"blah blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Failed\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Installation of service software update\" }]\r\n        }\r\n      }\r\n    }\r\n    update_required = {\r\n      description = \"blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software update [\" }]\r\n        }\r\n      }\r\n    }\r\n    update_restarted = {\r\n      description = \"blah blah blahh\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software restarted [\" }]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI believe that it is easier and quicker to read the equivalent `list(map)` as the indentation is more consistent. Further, most modern editors will also render the value of the `name` attribute as a different color which makes it stand out more compared to the `update_*` map keys in the previous example, which are the same color as the other map keys. While not exactly the same idea, this makes me think of Mat Ryer's [argument of line of sight (Medium.com -- Code: Align the happy path to the left edge](https:\/\/medium.com\/@matryer\/line-of-sight-in-code-186dd7cdea88) \r\n\r\n```hcl\r\nlocals {\r\n  eventbridge_list_map = [\r\n    {\r\n      name        = \"update_available\"\r\n      description = \"blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Available\"]\r\n          severity    = [\"Informational\"]\r\n          description = [{ prefix = \"Service software update\" }]\r\n        }\r\n      }\r\n    },\r\n    {\r\n      name        = \"update_failed\"\r\n      description = \"blah blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Failed\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Installation of service software update\" }]\r\n        }\r\n      }\r\n    },\r\n    {\r\n      name        = \"update_required\"\r\n      description = \"blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software required [\" }]\r\n        }\r\n      }\r\n    },\r\n    {\r\n      name        = \"update_restarted\"\r\n      description = \"blah blah blahh\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software restarted [\" }]\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nConsidering the above example where `local.eventbridge_list_map` is a `list(map)`, the only way to use that object in `for_each` is with a map compression.\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = { for v in local.eventbridge_list_map :  v.name => v }\r\n  \r\n  name = each.value.name\r\n}\r\n```\r\n\r\nHowever, using the `list(map)` here is more tedious to write and harder to read than using `local.eventbridge_map` despite having the same outcome\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = local.eventbridge_map  \r\n\r\n  name = each.value.name\r\n}\r\n```\r\n\r\n### Proposal\r\n\r\nCreate a new function `keylistmap()` which has the following interface:\r\n\r\n`keylistmap(key, listmap)`\r\n\r\n- **key**: string name of an attribute that is present inside of every top level `map` in a `list(map)`.\r\n- **listmap**: list of maps to be transformed.\r\n\r\nInvoking  `keylistmap` with the `local.eventbridge_list_map` object would have the following behavior:\r\n\r\n```hcl\r\n$ terraform console\r\n> keylistmap(\"name\", local.eventbridge_list_map)\r\n{\r\n    update_available = {\r\n      name        = \"update_available\"\r\n      description = \"blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Available\"]\r\n          severity    = [\"Informational\"]\r\n          description = [{ prefix = \"Service software update\" }]\r\n        }\r\n      }\r\n    },\r\n    update_failed = {\r\n      name        = \"update_failed\"\r\n      description = \"blah blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Failed\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Installation of service software update\" }]\r\n        }\r\n      }\r\n    },\r\n    update_required = {\r\n      name        = \"update_required\"\r\n      description = \"blah blah\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software required [\" }]\r\n        }\r\n      }\r\n    },\r\n    update_restarted = {\r\n      name        = \"update_restarted\"\r\n      description = \"blah blah blahh\"\r\n      event_pattern = {\r\n        detail = {\r\n          status      = [\"Required\"]\r\n          severity    = [\"High\"]\r\n          description = [{ prefix = \"Service software restarted [\" }]\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\nThis would have the benefit of turning the map comprehension example...\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = { for v in local.eventbridge_list_map :  v.name => v }\r\n  \r\n  name = each.value.name\r\n}\r\n```\r\n\r\nInto something more native:\r\n\r\n\r\n```hcl\r\nresource \"foo\" \"this\" {\r\n  for_each = keylistmap(\"name\", local.eventbridge_list_map)\r\n  \r\n  name = each.value.name\r\n}\r\n```\r\n\r\nThis aims to maximize readability in the iterable through utilizing `list(map)s` as well as in the `for_each` statement by via an ergonomic alternative to a map comprehension.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thank you for your continued interest in this issue. \r\n\r\n[Terraform version 1.8](https:\/\/github.com\/hashicorp\/terraform\/blob\/v1.8.0-beta1\/CHANGELOG.md) launches with support of provider-defined functions. It is now possible to implement your own functions! We would love to see this implemented as a provider-defined function. \r\n\r\nPlease see the [provider-defined functions documentation](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework\/functions) to learn how to implement functions in your providers. If you are new to provider development, learn how to [create a new provider](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework) with the Terraform Plugin Framework. If you have any questions, please visit the [Terraform Plugin Development category in our official forum](https:\/\/discuss.hashicorp.com\/c\/terraform-providers\/tf-plugin-sdk\/43). \r\n\r\nWe hope this feature unblocks future function development and provides more flexibility for the Terraform community. Thank you for your continued support of Terraform!"],"labels":["enhancement","functions","new"]},{"title":"Azurerm backend: set lease duration","body":"### Terraform Version\n\n```shell\nTerraform v1.5.4\r\non windows_amd64\n```\n\n\n### Use Cases\n\nWhen an azurerm backend is in use, it leverages the Azure Storage lease feature to hold the lock. This is good - it avoids having to build any other state locking mechanism. At the end of the operation, the lease is dropped & the next workflow can continue. \r\n\r\nWhen it does this, the lease for the lock is set to an *infinite* length. This means that if anything goes wrong in terraform (panic) or the agent running the pipeline (agent death\/ agent timeout exceeded) the lock remains in existence for ever. At this point, Terraform cannot function until someone uses Azure itself to break the lease.\n\n### Attempted Solutions\n\nThis is a rare edge case to hit (we are only seeing this because our build agents went on the fritz). \r\nI don't see any obvious way to set the lease - and if the assumption is \"my pipeline agent is broken\" then I can't even work around it by scripting a final \"Always\" task!\n\n### Proposal\n\nWhen you set the provider, you should be able to specify the lock duration in Terraform syntax. Then users should be encouraged to set this to 50% amount longer than their CI\/CD build tooling timeouts. This means that the lease is of a finite length, customisable by the user.\r\n\r\nEvery \"lock_renewal_interval\", if Terraform is still running, the lease should be renewed (which resets the lease duration). This covers:\r\n\r\n- As long as Terraform is still running, it will constantly refresh the lock. This means no running pipeline can be interrupted. \r\n- If for any reason, Terraform crashes without removing the lock, it will expire in the expiry time frame after the last successful renewal. \r\n\r\nThis means that any catastrophic failure to keep the lock managed will be released in lock_duration  \r\n\r\n```\r\nterraform {\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"StorageAccount-ResourceGroup\"\r\n    storage_account_name = \"abcd1234\"\r\n    container_name            = \"tfstate\"\r\n    key                                = \"prod.terraform.tfstate\"\r\n    lock_duration                = \"60m\" #defaults to 50 minutes\r\n    lock_renewal_interval    = \"5m\"   #defaults to 5 minutes\r\n  }\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Another scenario where it can fail to release the lease is if the pipeline is cancelled. I think this scenario could be worked around by having an always\/cancelled task, but I don't think it should have to.\r\n\r\nNote, the maximum lease length (other than infinite) is 60 seconds, so maybe rather than have users set duration\/renewal this issue could be solved by taking a 60 second lease, then having some backgroud code that keeps renewing the lease until the tf work is finished."],"labels":["enhancement","backend\/azure"]},{"title":"[Enhancement]: allow generating splitted iam role inline policy when importing an `aws_iam_role` using an `import` block","body":"### Description\r\n\r\nWhen you use an `import` block to import an IAM Role through its `name` attribute, Terraform will generate an `aws_iam_role` resource definition with an `inline_policy` argument. This may not necessarily reflect the result wanted by the user. Maybe the user would prefer that Terraform generates an `aws_iam_role` without the `inline_policy` argument; they may prefer that Terraform generates an IAM Role inline policy as a separate `aws_iam_role_policy` resource linked to the `aws_iam_role` instead.\r\n\r\n### Affected Resource(s) and\/or Data Source(s)\r\n\r\n* aws_iam_role\r\n\r\n### Potential Terraform Configuration\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_\r\n\r\n### Would you like to implement a fix?\r\n\r\nNone","comments":["# Community Note\n\n**Voting for Prioritization**\n\n* Please vote on this issue by adding a \ud83d\udc4d [reaction](https:\/\/blog.github.com\/2016-03-10-add-reactions-to-pull-requests-issues-and-comments\/) to the original post to help the community and maintainers prioritize this request.\n* Please see our [prioritization guide](https:\/\/hashicorp.github.io\/terraform-provider-aws\/prioritization\/) for information on how we prioritize.\n* Please **do not** leave \"+1\" or other comments that do not add relevant new information or questions, they generate extra noise for issue followers and do not help prioritize the request.\n\n**Volunteering to Work on This Issue**\n\n* If you are interested in working on this issue, please leave a comment.\n* If this would be your first contribution, please review the [contribution guide](https:\/\/hashicorp.github.io\/terraform-provider-aws\/).","Hey @garysassano \ud83d\udc4b Thank you for taking the time to raise this! It sounds like you might be referencing the ability to generate configuration using the new `-generate-config-out=` flag, is that correct?","@justinretzolk  Yes, that's correct.","Thanks for clarifying @garysassano! This would need to be an enhancement at the Terraform Core level, so I'll transfer your issue over to that repository to get it in front of the appropriate eyes."],"labels":["enhancement","plannable-import"]},{"title":"Setup and Implement Read tutorial - Wrong filename reference in code block","body":"### Terraform Version\r\n\r\n```shell\r\nNot applicable\r\n```\r\n\r\n\r\n### Affected Pages\r\n\r\nhttps:\/\/developer.hashicorp.com\/terraform\/tutorials\/providers\/provider-setup#add-data-source-to-provider\r\n\r\n### What is the docs issue?\r\n\r\nThe code snippet's filename seems to be wrong as the description and code suggests `hashicups\/provider.go` instead of `hashicups\/data_source_coffee.go`\r\n\r\n### Proposal\r\n\r\nChange the filename to `hashicups\/provider.go` in the section's codeblock\r\n\r\n\r\n\r\n### References\r\n\r\nNone","comments":["Can I work on this issue, please. Could you please assign it to me. Thanks!","as far as I know, these documentation repos are not open to PRs. \r\nlet's wait for the team to respond. ","Thanks for this submission! The tutorials are not publicly editable, no. Thanks again!"],"labels":["documentation","new"]},{"title":"`TF_WORKSPACE` environment variable seems partially ignored","body":"### Terraform Version\n\n```shell\nTerraform v1.3.9\r\non windows_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source = \"hashicorp\/aws\"\r\n    }\r\n  }\r\n  cloud {\r\n    hostname     = \"app.terraform.io\"\r\n    organization = \"My-Org\"\r\n  }\r\n}\r\n```\r\n\n\n### Debug Output\n\n```pwsh\r\nInitializing Terraform Cloud...\r\n\u2577\r\n\u2502 Error: Invalid workspaces configuration\r\n\u2502\r\n\u2502 Missing workspace mapping strategy. Either workspace \"tags\" or \"name\" is\r\n\u2502 required.\r\n\u2502\r\n\u2502 The 'workspaces' block configures how Terraform CLI maps its workspaces for\r\n\u2502 this single\r\n\u2502 configuration to workspaces within a Terraform Cloud organization. Two\r\n\u2502 strategies are available:\r\n\u2502\r\n\u2502 tags - A set of tags used to select remote Terraform Cloud\r\n\u2502 workspaces to be used for this single\r\n\u2502 configuration. New workspaces will automatically be tagged with these tag\r\n\u2502 values. Generally, this\r\n\u2502 is the primary and recommended strategy to use.  This option conflicts with\r\n\u2502 \"name\".\r\n\u2502\r\n\u2502 name - The name of a single Terraform Cloud workspace to be\r\n\u2502 used with this configuration.\r\n\u2502 When configured, only the specified workspace can be used. This option\r\n\u2502 conflicts with \"tags\".\r\n\u2575\r\n\r\nOperation failed: failed running terraform init (exit 1)\r\n```\n\n### Expected Behavior\n\nI am trying to set the workspace name dynamically. As I understand it the only option to do this is via the [`TF_WORKSPACE` environment variable](https:\/\/developer.hashicorp.com\/terraform\/cli\/config\/environment-variables#tf_workspace). So I am setting that prior to my terraform command.\r\n\r\n```pwsh\r\n$env:TF_WORKSPACE=\"My-Workspace\"\r\necho $env:TF_WORKSPACE\r\nterraform -chdir=deploy\/bucket plan\r\n```\r\n\r\nI would expect this to execute correctly against the Terraform Cloud workspace that I have named in the variable.\n\n### Actual Behavior\n\nIt does partially work as it does start against the correct workspace. However, it shows the above error. Which seems to be telling me to do what I have already done. So I am confused. If I do not set the environment variable I get the following error:\r\n\r\n```pwsh\r\n\u2502 Error: Invalid workspaces configuration\r\n\u2502\r\n\u2502 Missing workspace mapping strategy. Either workspace \"tags\" or \"name\" is required.\r\n\u2502\r\n\u2502 The 'workspaces' block configures how Terraform CLI maps its workspaces for this single\r\n\u2502 configuration to workspaces within a Terraform Cloud organization. Two strategies are available:\r\n\u2502\r\n\u2502 [bold]tags[reset] - A set of tags used to select remote Terraform Cloud workspaces to be used for this single\r\n\u2502 configuration. New workspaces will automatically be tagged with these tag values. Generally, this\r\n\u2502 is the primary and recommended strategy to use.  This option conflicts with \"name\".\r\n\u2502\r\n\u2502 [bold]name[reset] - The name of a single Terraform Cloud workspace to be used with this configuration.\r\n\u2502 When configured, only the specified workspace can be used. This option conflicts with \"tags\".\r\n```\r\n\r\nSo I must be on the right track, but I cannot see what I have done wrong.\n\n### Steps to Reproduce\n\n```pwsh\r\n$env:TF_WORKSPACE=\"My-Workspace\"\r\necho $env:TF_WORKSPACE\r\nterraform -chdir=deploy\/bucket plan\r\n```\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":[],"labels":["bug","new","cloud"]},{"title":"Plannable Import: Don't fail, but create resource, if not exist","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.2\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nIn our CI\/CD system, I want to import resources, that eventually already had been created. If they do not exist, terraform could safely create them using the existing configuration.\r\n\r\nMy concrete example:\r\nI have some (AWS) Lambda functions, that had been created by terraform. When being executed for the first time, the functions will create a LogGroup with the function name in CloudWatch. Unfortunately, the default config for these LogGroups doesn't fit our needs (e.g. no log retention being set). When I add the LogGroup to the terraform configuration, applying will fail in most (but not all!) cases, because it tries to create the LogGroup with the existing name.\r\n\r\n### Attempted Solutions\r\n\r\nIn similar situations, we added some commands before doing the \"apply\" and imported the resources using the CLI import command or just deleted the resource. The new `import` block would be a game changer for us...\r\n\r\nThanks to CDKTF, as a workaround we can make the \"import\" block optional and check the existence of the resource using AWS API.\r\n\r\n### Proposal\r\n\r\nI see to possible ways to tackle this:\r\n\r\n1. Flag in the CLI, which allows to ignore \"Cannot import non-existent remote object\" errors\r\n2. Optional property in the \"import\" block, which tells terraform how to proceed, when resource does not exist.\r\n\r\nOption 2 feels best for me, because the behavior can be configured individually for each resource\/import. The config could look like:\r\n```hcl\r\nimport {\r\n   id              = \"\/aws\/lambda\/function-name\"\r\n   to              = aws_cloudwatch_log_group.lambda_log_group\r\n   fail_on_missing = false # optional, default: \"true\"\r\n}\r\n```\r\nMore \"positiv\" sounding proposal by @acdha:\r\n```hcl\r\nimport {\r\n   id                  = \"\/aws\/lambda\/function-name\"\r\n   to                  = aws_cloudwatch_log_group.lambda_log_group\r\n   create_when_missing = true # optional, default: \"false\"\r\n}\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["This is definitely the more common use case for us. We reuse a lot of our terraform code. \r\n- In a new instance mode we would like the resources to be created\r\n- In recovery mode we probably need to import the existing resources into the state\r\n\r\nHaving something like `fail_on_missing` is great for us.\r\n\r\nAlso possibly related to https:\/\/github.com\/hashicorp\/terraform\/issues\/33624 where we want to possibly switch off the `import` blocks.","This would definitely help by allowing to predefine imports in modules we provide to our devs. We're mostly migrating existing infrastructure, and handling terraform imports outside the CI pipeline is a time waster right now.","I thought I'd be able to workaround this by using `terraform import`  to determine which import blocks I could generate, but there's no dry-run feature available, per this closed issue: https:\/\/github.com\/hashicorp\/terraform\/issues\/25713","This patch https:\/\/github.com\/hashicorp\/terraform\/commit\/350f9b86349531c2687d771b9e26e922cdfd2d19 works, though it assumes any import error should be ignored:\r\n```diff\r\ndiff --git a\/internal\/terraform\/node_resource_plan_instance.go b\/internal\/terraform\/node_resource_plan_instance.go\r\nindex a4bbaa5359..561faa7371 100644\r\n--- a\/internal\/terraform\/node_resource_plan_instance.go\r\n+++ b\/internal\/terraform\/node_resource_plan_instance.go\r\n@@ -165,7 +165,11 @@ func (n *NodePlannableResourceInstance) managedResourceExecute(ctx EvalContext)\r\n        \/\/ If the resource is to be imported, we now ask the provider for an Import\r\n        \/\/ and a Refresh, and save the resulting state to instanceRefreshState.\r\n        if importing {\r\n-               instanceRefreshState, diags = n.importState(ctx, addr, importId, provider, providerSchema)\r\n+               var importDiags tfdiags.Diagnostics\r\n+               instanceRefreshState, importDiags = n.importState(ctx, addr, importId, provider, providerSchema)\r\n+               if importDiags.HasErrors() {\r\n+                       importing = false\r\n+               }\r\n        } else {\r\n                var readDiags tfdiags.Diagnostics\r\n                instanceRefreshState, readDiags = n.readResourceInstanceState(ctx, addr)\r\n```\r\n\r\nLooks like ignoring `Cannot import non-existent remote object` errors might not be sufficient, as some providers just error out on some resources when attempting an import (ie. GCP project IAM bindings):\r\n```\r\n\u2502 Error: Cannot find binding for \"project \\\"gcp-project\\\"\" with role \"roles\/...\", member \"serviceAccount:...@gcp-project.iam.gserviceaccount.com\", and condition title \"\"\r\n```","We have a very similar use-case: our AWS account configuration is managed using Terraform. `import` blocks provide a good way to handle our existing accounts but that won't work for a new account unless you run the configuration as of an old config version first and then let it upgrade, or do something kludgy like deleting the imports first. I think that `fail_on_missing` proposal would be great but I'm wondering whether the name should be something more along the lines of `create_when_missing` to focus on the desired outcome.","> We have a very similar use-case: our AWS account configuration is managed using Terraform. `import` blocks provide a good way to handle our existing accounts but that won't work for a new account unless you run the configuration as of an old config version first and then let it upgrade, or do something kludgy like deleting the imports first. I think that `fail_on_missing` proposal would be great but I'm wondering whether the name should be something more along the lines of `create_when_missing` to focus on the desired outcome.\r\n\r\nI disagree the `create_when_missing` is needed.  Remember that you're attaching the import block to a resource. the import block needs to just be ignored if the `fail_on_missing` flag is false, allowing the terraform script to gracefully continue and plan to Create a new resource instead of failing the entire script.","Something like this would be hugely helpful. We are forced to use local backends for our terraform and have had cases where our state file was lost. Being able to have something equivalent to \"import if exists else create\" would be amazing. Given @antoinedeschenes comment about only catching missing resources error being insufficient, I would suggest something along the lines of `allow_failure` or `ignore_error`.  Either that or ignore the fact that not all providers return proper errors and encourage them to align with standards.","> I disagree the `create_when_missing` is needed. Remember that you're attaching the import block to a resource. the import block needs to just be ignored if the `fail_on_missing` flag is false, allowing the terraform script to gracefully continue and plan to Create a new resource instead of failing the entire script.\r\n\r\nThe property name doesn't have to decribe the behavior in the code.\r\nSomething like `create_when_missing` feels more \"natural\" regarding the \"Developer Experience\". In my opinion (!), it might make sense, to have this as default behavior after some time... But that would be a breaking change \ud83d\ude04 \r\n\r\n> [...] I would suggest something along the lines of `allow_failure` or `ignore_error`. Either that or ignore the fact that not all providers return proper errors and encourage them to align with standards.\r\n\r\nThis sounds too generic... What about errors e.g. regarding permissions?\r\n\r\nWould be great, if there is already something in place, that is able to search the given ID using the providers implementation. Catching an import error (and deciding to throw it or not) feels not right to me and could lead into unexpected behaviors for some providers.\r\nThe workflow should look like:\r\n* Resource is already in state -> Import directive ignored (current behavior)\r\n* Resource is NOT in state\r\n  * `create_when_missing = true` -> **Search**, if given ID exist (new behavior)\r\n    * If YES: **Import**\r\n    * If NO: **Create** new resource\r\n  * `create_when_missing = false` -> **Import** (current behavior)\r\n\r\nThis would cause one additional API call during the import, but only once!","@jbardin are you able to share a timeframe when this might be included?\r\n\r\nAnd in the meantime, are there any recommended patterns for workarounds?  \r\n\r\nI'm also hitting the issue with cloud watch log groups that are automatically created by other AWS services.  I need to:\r\n\r\n1. Reference them in the same terraform apply that creates a related lambda function for example.  In this case terraform throws an error because the log group hasn't been created yet\r\n2. Update existing log groups that have already been created by another AWS service","@kevinkuszyk, No there is no timeline yet, but it does not sound like it would fit your use case. The import action and planning decisions all need to happen during the plan, so if there are log groups created implicitly by other resources, you're still going to end up creating conflicting log groups during apply. What you're describing is a much more complex feature which requires support from providers and a new protocol to handle. It's something we're aware of, but not directly related to import.","This issue is critical to what I'm trying to build. I have an immutable kubernetes secret which I want to import if it exists, but if not I wish to have it created. I'm relying on the `ignore_changes` lifecycle option to maintain the immutability of the secret which works great for my use case, however if the secret doesn't exist I get the `Cannot import non-existent remote object` error.","I also would love to have a create_if_not_exists option on the import block, or something similar.\r\nWe manage some resource types that are persisted on external DBs, and therefore survive when we rebuild our environments. However, we often create them with TF directly: having to deploy the config twice, first with the import block commented-out, and then the final one, is annoying."],"labels":["enhancement","plannable-import"]},{"title":"Add CIDR collapse functionality","body":"Adding a CIDR collapse function to terraform for squashing network ranges to avoid use of external data sources (often not encouraged by security policies)\r\n\r\nTakes in a list of cidr ranges as a string list and returns a list of superset string values.\r\n\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33629) <br\/>All committers have signed the CLA.","@zachsdd could you add docs for new function similar to https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/website\/docs\/language\/functions\/cidrnetmask.mdx","### \ud83d\udcc4 Content Checks\n\nUpdated: Wed, 30 Aug 2023 19:29:41 GMT\n\n<details><summary>Found 1 error(s)<\/summary>\n\n#### `docs\/language\/functions\/cidrcollapse.mdx`\n\n| Position | Description | Rule |\n|---|---|---|\n| `1:1-1:1` | This file is not present in the nav data file at data\/language-nav-data.json. Either add a path that maps to this file in the nav data or remove the file. If you want the page to exist but not be linked in the navigation, add a `hidden` property to the associated nav node. | `no-unlinked-pages` |\n\n<\/details>\n<!-- Sticky Pull Request Commentcontent-conformance -->","Thanks for this submission! Ultimately, this function will need to be built as a plugin function provider, as opposed to a function built into Terraform. As such it is waiting for the implementation of plugin function providers. Please see:\r\n* https:\/\/github.com\/hashicorp\/terraform\/issues\/2771\r\n* https:\/\/github.com\/hashicorp\/terraform\/issues\/27696 \r\n\r\nThere are a number of other CIDR-related function PRs, probably the ideal goal would be to wrap those together into one CIDR function provider. The organization for such an effort remains to be seen. Thanks again for your submission!\r\n"],"labels":["enhancement","functions"]},{"title":"Moved block is not mentioned on documentation page about moving resources","body":"### Terraform Version\n\n```shell\n1.5.4\n```\n\n\n### Affected Pages\n\n- https:\/\/developer.hashicorp.com\/terraform\/cli\/state\/move\r\n- https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/state\/mv\n\n### What is the docs issue?\n\nIt does not mention `moved` blocks at all or link to them as a possible or the preferred way for doing state moves, it only mentions the CLI commands\n\n### Proposal\n\nThese pages should talk about the existence of `moved` blocks and issue a recommendation for when `moved` blocks should be used vs the CLI commands.\n\n### References\n\n_No response_","comments":["Thanks for pointing this out. I will raise it with the docs team. ","For the first link, it does have a section on the topic:\r\n> For most cases we recommend using [the Terraform language's refactoring features](https:\/\/developer.hashicorp.com\/terraform\/language\/modules\/develop\/refactoring) to document in your module exactly how the resource names have changed over time. Terraform reacts to this information automatically during planning, so users of your module do not need to take any unusual extra steps.\r\n> \r\n> Hands On: Try the [Use Configuration to Move Resources](https:\/\/developer.hashicorp.com\/terraform\/tutorials\/configuration-language\/move-config) tutorial.\r\n\r\nDo you think the language here is too oblique?  ","Ah, yeah that's a bit oblique. Also if you're scanning the doc, it would be a lot clearer if it was more around this section\r\n<img width=\"772\" alt=\"image\" src=\"https:\/\/github.com\/hashicorp\/terraform\/assets\/67290882\/9b8c189b-4103-4b71-bbe3-0bbd441e3e5b\">\r\nsince right now it's really buried in the overview of what it means to move a resource\/why you might want to and before that section I screenshotted where it comes across as being \"here is the possible ways to do the move\". So some kind of refinement to break it out into really clear sections of \r\n\r\n1. Overview - What does it mean to move resources\/why might you want to?\r\n2. Methods - that breaks out here are the 4 different ways of doing moves and why you might want to choose each","Thanks @joe-a-t, that is what I figured (and I agree), but I didn't want to assume. "],"labels":["documentation","new"]},{"title":"Don't validate lifecycle precondition on change","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.5\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWe would like to have the ability to **NOT** validate a `precondition` on resource **change**, but only validate it on resource **create**.\r\n\r\nFor example, consider creating an EC2 instance:\r\n\r\n```hcl\r\nresource \"aws_instance\" \"this\" {\r\n  # instance configuration...\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition     = var.operating_system == \"windows\" || var.join_domain == false || (var.operating_system == \"linux\" && var.join_domain == true && can(regex(\"^t2.\", var.instance_type)))\r\n      error_message = \"If deploying a Linux instance that should be joined to the domain, you must choose an instance type of the family \\\"t2\\\" since there is a bug when creating the DNS record for other instance types\"\r\n    }\r\n}\r\n```\r\n\r\nIn this case, we want to ensure that new Linux EC2 instances will be created with an instance family type of `t2`, but after their creation, it's safe to upgrade them to other families (like `t3`) so we don't want the `precondition` to fail the plan.\r\n\r\n### Attempted Solutions\r\n\r\nNo real solution attempted.\r\n\r\n### Proposal\r\n\r\nAdd an attribute under the `precondition` block to mention on which stages it should validate the condition.\r\n\r\nFor example:\r\n\r\n```hcl\r\nlifecycle {\r\n  precondition {\r\n    condition     = var.operating_system == \"windows\" || var.join_domain == false || (var.operating_system == \"linux\" && var.join_domain == true && can(regex(\"^t2.\", var.instance_type)))\r\n    error_message = \"If deploying a Linux instance that should be joined to the domain, you must choose an instance type of the family \\\"t2\\\" since there is a bug when creating the DNS record for other instance types\"\r\n    stages        = [\"create\"]  # Add something like this\r\n  }\r\n}\r\n```\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this request!"],"labels":["enhancement","new"]},{"title":".terraform.lock.hcl contains hashes for manifest.json files.","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\r\non darwin_arm64\n```\n\n\n### Terraform Configuration Files\n\nnot relevant, pretty much any workspace that includes hashicorp providers.\n\n### Debug Output\n\nnot relevant\n\n### Expected Behavior\n\nThe `.terraform.lock.hcl` file should only contain hashes of actual providers binaries.\n\n### Actual Behavior\n\nThe `zh:` hash for the manifest.json files are included.\r\n\r\n```\r\nprovider \"registry.terraform.io\/hashicorp\/consul\" {\r\n   version     = \"2.18.0\"\r\n   constraints = \"2.18.0\"\r\n   hashes = [\r\n     \"h1:ABHNF3vYoRej5vLiiKR\/XYgjhjubsjlVROssc555Vj8=\",\r\n     \"h1:GIbc9IimARunBuk+GdxgUX0cnZ2WwFW0IZMyT8E88B8=\",\r\n     \"h1:fpHbgFUF+uyPCG+RLVJbTjpBHR8EZOxZkCh4e6VB7tY=\",\r\n     \"zh:155f6899c8f1e0162169e4d09f426ea9a9738d74129f0ffc54b7570941c49cd4\",\r\n     \"zh:241ba909b387c349845b0371321af45ccbb00290edbe1ea0861ad8732951f718\",\r\n     \"zh:612f52be1fb5dd507b8064c67785313a531faad35873cba53f998f3766473335\",\r\n     \"zh:67fbd695381b9c5db83ffeeaec1bfdbcc477ccb1e9de1caff76a0186d4c85908\",\r\n     \"zh:7f88b151c9690a6addccbffe8484f0257344ef55424a9efb025dfddd052a4dc6\",\r\n     \"zh:8d954f3ffeb72b6c18cc5ae8c3189bb3a8cb66967b2106c7d0163009c12bba15\",\r\n     \"zh:913774c7eabc6e9078a1bd00347cc539b19a6f6b45dacbd21454dffdc9f4ae43\",\r\n     \"zh:9517558883f994649695643a6208079ed0445aaa0ac2dee69f88cb044d21c6c9\",\r\n     \"zh:a0211f596e35bd1b8d4bb9cda321cb1555a427a8d3f6724fe09893168fac9b7e\",\r\n     \"zh:a70eaa0a88f677f901855a4ab908ddcf961e4afaf3a0147b08faaead57b4fe07\",\r\n     \"zh:b02f5cd94ab236d988cfec531c56c199e3087803f1b2908a1a2b6da8a57b3751\",\r\n     \"zh:f3d3efac504c9484a025beb919d22b290aa6dbff256f6e86c1f8ce7817e077e5\",\r\n   ]\r\n }\r\n```\r\nIn this case the last hash is from https:\/\/github.com\/hashicorp\/terraform-provider-consul\/releases\/download\/v2.18.0\/terraform-provider-consul_2.18.0_SHA256SUMS\r\n\r\n```\r\n155f6899c8f1e0162169e4d09f426ea9a9738d74129f0ffc54b7570941c49cd4  terraform-provider-consul_2.18.0_freebsd_386.zip\r\n241ba909b387c349845b0371321af45ccbb00290edbe1ea0861ad8732951f718  terraform-provider-consul_2.18.0_darwin_amd64.zip\r\n612f52be1fb5dd507b8064c67785313a531faad35873cba53f998f3766473335  terraform-provider-consul_2.18.0_linux_arm.zip\r\n67fbd695381b9c5db83ffeeaec1bfdbcc477ccb1e9de1caff76a0186d4c85908  terraform-provider-consul_2.18.0_freebsd_amd64.zip\r\n7f88b151c9690a6addccbffe8484f0257344ef55424a9efb025dfddd052a4dc6  terraform-provider-consul_2.18.0_darwin_arm64.zip\r\n8d954f3ffeb72b6c18cc5ae8c3189bb3a8cb66967b2106c7d0163009c12bba15  terraform-provider-consul_2.18.0_linux_386.zip\r\n913774c7eabc6e9078a1bd00347cc539b19a6f6b45dacbd21454dffdc9f4ae43  terraform-provider-consul_2.18.0_freebsd_arm.zip\r\n9517558883f994649695643a6208079ed0445aaa0ac2dee69f88cb044d21c6c9  terraform-provider-consul_2.18.0_windows_amd64.zip\r\na0211f596e35bd1b8d4bb9cda321cb1555a427a8d3f6724fe09893168fac9b7e  terraform-provider-consul_2.18.0_linux_arm64.zip\r\na70eaa0a88f677f901855a4ab908ddcf961e4afaf3a0147b08faaead57b4fe07  terraform-provider-consul_2.18.0_linux_amd64.zip\r\nb02f5cd94ab236d988cfec531c56c199e3087803f1b2908a1a2b6da8a57b3751  terraform-provider-consul_2.18.0_windows_386.zip\r\nf3d3efac504c9484a025beb919d22b290aa6dbff256f6e86c1f8ce7817e077e5  terraform-provider-consul_2.18.0_manifest.json\r\n```\r\n\n\n### Steps to Reproduce\n\n`terraform init`\n\n### Additional Context\n\nThis is not a very serious bug, but the fact is that the lock is referencing hash of files are not invalid. We noticed that while reviewing an update of our providers and that for hashicorp providers there was one `zh:` line that remained constant, but not for third party providers.\r\n\r\nSomething that `terraform` could do when parsing the SHA256SUM files is to filter out the non zip files. If it turns out that the manifest.json checksum are actually intentionally provided and used, then ignore this report.\n\n### References\n\n_No response_","comments":["Hi @sodul,\r\n\r\nTerraform does not make any assumptions about what the filenames in your SHA256SUMS file might mean, or try to guess which ones are really packages and which ones aren't. Instead, Terraform expects that you will only include valid package checksums in that file, and exclude everything else.\r\n\r\nGiven that, I'm now wondering how you have ended up with a manifest file listed in your package checksums. Were you following some documentation that suggested you should do that? If so, I think we should correct that documentation so that the generated checksums file will be correct, rather than changing Terraform's behavior.\r\n\r\nThe filenames listed in the checksums file are not meaningful to Terraform and to start making assumptions about how they are named now would be a potential breaking change for existing published providers that might use different filenames (since the specific filenames chosen have been, so far, only a private detail chosen by the provider package build process).\r\n","@apparentlymart we do not have any custom logic for the checksum, we just call `terraform providers lock` to generate our `.terraform.lock.hcl` files.\r\n\r\nI think the issue is that for the Hashicorp providers the `*_SHA256SUMS` file generated during the release include the `_manifest.json` file, and the registry blindly provides this back to `terraform` itself. Then terraform just trusts whatever the registry is claiming, which is fine I guess. Possibly the bug is in the registry where it could filter out the checksum for files that are not the actual provider binaries.\r\n\r\nWe have noticed this pattern with official Hashicorp providers, third party providers do not have that issue.\r\n\r\nBTW, since we only care about a limited number of platforms it would be quite nice if `terraform providers lock` could avoid including the `zh:` hashes for all the non relevant platforms, that would take care of that extraneous `_manifest.json` checksum and limit storage and diff bloat.\r\n","Ahh, sorry for the misunderstanding. I thought you were reporting that a provider _you_ created was being treated in this way, but I see now that you were referring to `hashicorp\/consul` as a concrete example, and that wasn't just a placeholder to avoid disclosing the true name of the provider.\r\n\r\nIn that case, I think the problem here is that the release process for the official providers seems to be incorrectly including the manifest file in the set of files it considers when calculating the `SHA256SUMS` file. The fix in that case would be to change the provider release process to fix this problem, although that is not something we'd change in  _this_ codebase because the release processes for the official providers belong to their own codebases. Since this slight misbehavior seems common across them all, I assume they all have some shared dependency that we could change to fix this for all of them at once, at least moving forward with new releases.\r\n\r\nThanks for reporting this, and sorry for the misunderstanding!\r\n\r\n---\r\n\r\nI do also acknowledge your addendum about only recording a subset of the checksums. Let's keep that a separate concern because it would require changing the provider registry protocol to explicitly distinguish between checksums for each target platform, rather than just reporting them all together with no indication of which is which.\r\n\r\n(While as humans we can infer that the filename containing `linux_amd64` is probably a package for `linux_amd64`, those names are not fixed as part of the protocol and so to depend on them would itself be a protocol change.)\r\n\r\nThe existing issue #27264 is already covering an improvement to the checksum management that would require changing the registry protocol -- to be able to return more than just `zh:`-type checksums -- so I think it's probably best to treat this as an additional requirement to consider when the registry team is ready to pick that up, so we can make just one protocol change that deals with both of these concerns at once.\r\n","I have filed https:\/\/github.com\/hashicorp\/ghaction-terraform-provider-release\/issues\/81 at the request of the team that takes care of releasing our providers, or setting the conventions for releasing them to be more specific."],"labels":["bug","new"]},{"title":"Invalid AWS Region: eu-central-2 coming from Terraform CLI","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nbackend \"s3\" {\r\n# Replace this with your bucket name!\r\nbucket = \"example\"\r\nkey = \"example\/tfstate\"\r\nregion = \"eu-central-2\"\r\n\r\n# Replace this with your DynamoDB table name!\r\ndynamodb_table = \"example\"\r\nencrypt        = true\r\n\r\n}\r\n```\r\n\n\n### Debug Output\n\nUpgrading modules...\r\n- ec2-ami in modules\/ec2-ami\r\n- s3 in modules\/s3\r\n- vpc in modules\/vpc\r\n\r\nInitializing the backend...\r\nregion\r\n  AWS region of the S3 Bucket and DynamoDB Table (if used).\r\n\r\n  Enter a value: eu-central-2\r\n\r\n\u2577\r\n\u2502 Error: Invalid AWS Region: eu-central-2\r\n\u2502 \r\n\u2502 \r\n\u2575\n\n### Expected Behavior\n\nno error message without region_skip_validation\n\n### Actual Behavior\n\nError: Invalid AWS Region: eu-central-2\n\n### Steps to Reproduce\n\nterraform init\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @dosilyoun,\r\n\r\nThe set of valid regions is encoded in the AWS SDK that Terraform uses to interact with S3, and so this region might be available in a newer version of the SDK.\r\n\r\nHave you tried with the latest version of Terraform to see if the SDK is updated to know about this region? At the time I'm writing this comment the latest release is v1.5.3.\r\n\r\nIf even the latest version does not support this region then we may need to upgrade to a newer version of the AWS SDK to get support for it, and then make a new Terraform release with the updated SDK.\r\n\r\nIn the meantime you can configure the backend to assume all regions are valid using the option you already found.\r\n\r\nThanks!","Per https:\/\/github.com\/hashicorp\/terraform\/pull\/33469#issuecomment-1626158408, there is a project to upgrade the aws sdk version underway. Just FYI. ","@apparentlymart  for your responses and thank you @crw  for sharing the update on upgrade progress.\r\n\r\nFor extra clarity let me add more details here:\r\n\r\n\r\nProblem is not with with the providers. (It could be with providers if your root module is using those regions, but that's the concern of aws team to resolve). \r\n\r\nAs far as terraform is concerned issue is with backend backed by `s3`.\r\n\r\nDuring the terraform `init` stage it initialize aws s3 backed using aws-sdk-go and there the support of zurich and other regions was found missing across all terraform releases including 1.6.x at the time of writing this comment.\r\n\r\nSupport for zuich was added in aws-sdk-go@v1.44.35 and for Melbourne in aws-sdk-go@v1.44.85.  \r\nI custom compiled the terraform binary with these updated sdk to confirm and it does work\r\n\r\n\r\nOne thing i would like to advise aws folks is to be more verbose about such major release. Nowhere in the changelog it's mentioned that zurich support is added: https:\/\/github.com\/aws\/aws-sdk-go\/pull\/4619\r\n\r\n\r\n","Discussion is going on https:\/\/github.com\/hashicorp\/terraform\/pull\/33607 ","> Problem is not with with the providers. \r\n\r\nThe provider teams maintain the backends in their respective domains. The AWS team maintains the S3 backend, Azure team maintains the Azure backend, etc. Just FYI.  "],"labels":["bug","backend\/s3"]},{"title":"Reject or warn about additional attributes during object type conversion","body":"### Terraform Version\n\n```shell\n1.5.1\n```\n\n\n### Terraform Configuration Files\n\n```go\r\ntype Things struct {\r\n\tA     types.Bool `tfsdk:\"A\"`\r\n\tB types.Bool `tfsdk:\"B\"`\r\n}\r\n\r\ntype SomeResource struct {\r\n   Things things\r\n}\r\n\r\n<snip>\r\n\"things\": schema.SingleNestedAttribute{\r\n\t\t\t\tAttributes: map[string]schema.Attribute{\r\n\t\t\t\t\t\"B\": schema.BoolAttribute{\r\n\t\t\t\t\t\tDefault:  booldefault.StaticBool(false),\r\n\t\t\t\t\t\tOptional: true,\r\n\t\t\t\t\t\tComputed: true,\r\n\t\t\t\t\t},\r\n\t\t\t\t\t\"B\": schema.BoolAttribute{\r\n\t\t\t\t\t\tDefault:  booldefault.StaticBool(false),\r\n\t\t\t\t\t\tOptional: true,\r\n\t\t\t\t\t\tComputed: true,\r\n\t\t\t\t\t},\r\n}\r\nrequired: true,\r\n}\r\n```\r\n\r\n\r\n\r\n```terraform\r\nresource \"some_resource\" \"some_resource_name\" {\r\n\r\n  things = {\r\n     A = true\r\n     should_fail_to_validate = \"but doesn't\"\r\n  }\r\n}\r\n\r\n```\r\n\n\n### Debug Output\n\nn\/a\n\n### Expected Behavior\n\n`terraform validate` should fail with a message that an unknown key is present.\n\n### Actual Behavior\n\n`terraform validate\/plan\/apply` all ran succesfully without errors or warning.\n\n### Steps to Reproduce\n\n1. Include the above model and schema in a provider.\r\n2. `terraform validate`.\n\n### Additional Context\n\nIt was suggested by @bflad to open an issue in this tracker instead, including some notes about what is being passed: https:\/\/github.com\/hashicorp\/terraform-plugin-framework\/issues\/805#issuecomment-1648521484\r\n\n\n### References\n\nThere appear to be a number of related issues already open:\r\n\r\n- #28696\r\n- #28727\r\n- #29204\r\n- #32015","comments":["Hi @asaba-hashi,\r\n\r\nIn the Terraform language an object type constraint accepts any value that has _at least_ the attributes specified. Any additional attributes are discarded during type conversion.\r\n\r\nTherefore this seems like Terraform working as designed, similar to what would happen if a module author declared an input variable of an object type. This is a design tradeoff of the type system that is intended to allow the producer and consumer of an argument to evolve independently:\r\n\r\n- the consumer can _accept more_ in later releases by adding new optional attributes\r\n- the producer can _provide more_ in later releases as long as it preserves all of the attributes that were previously provided\r\n\r\nAlthough the ability to \"provide more\" is not important when the object is written out literally using an object constructor as you showed here, it is important to reduce coupling when the entire object is being provided from further away, such as in another module or as an output attribute of another resource type.\r\n\r\nIf you would prefer a more rigid interpretation then I think you will need to declare a nested block type called `thing`. Nested blocks can be more rigid because all of the arguments must be written out literally; it's not syntactically possible to just assign an entire object value in that case, so the author of the block decides explicitly which arguments to include and thus Terraform can safely reject unexpected arguments without it causing problems when the source object grows to include new attributes later.\r\n\r\nIf this is a request to change Terraform's type system to reject attributes that are not declared rather than just discarding them during conversion then unfortunately I don't see a path to get there within the Terraform v1.x Compatibility Promises, even if we were to decide that the compatibility considerations I described above were not important."," > If you would prefer a more rigid interpretation then I think you will need to declare a nested block type called `thing`. \r\n\r\n> ...unfortunately I don't see a path to get there within the Terraform v1.x Compatibility Promises...\r\n\r\nThanks for the thorough explanation!\r\n\r\nAs a new framework based provider, I only chose a nested attributes because of the following note in the docs, so I've opened up a doc issue for the framework in the meantime:\r\n\r\n> Use [nested attributes](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework\/handling-data\/attributes#nested-attributes) for new schema implementations. Block support is mainly for migrating prior SDK-based providers.","It sounds like there is some potential conflation of concerns because any particular schema definition recommendations will cause provider developers to try to choose an appropriate practitioner experience for them. When is it more \"correct\" for a provider developer to choose blocks over attributes? Nested blocks, while having the strict attribute name validation, are inherently more difficult to work with as a practitioner since they require explicit per attribute re-mapping and potential usage of [dynamic block expressions](https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/dynamic-blocks). My understanding of the _intention_ of adding nested attributes into schema definitions was to simplify the practitioner experience in this regard, where the behaviors mentioned by @apparentlymart are actually a benefit in many cases.","Indeed, unfortunately it is the fact that blocks are \"more difficult to work with\" that makes it reasonable to do the stricter validation we've historically done for them: the arguments inside a block must _always_ be written out individually and explicitly, which means we know that any extraneous arguments were put there directly by the author of the block. In return for the more onerous requirements at the definition site we get more information about author intent and so can justify stricter error checking.\r\n\r\nOn the other hand, arguments of an object type add some more flexibility because now it's possible to dynamically generate an entire object, or who wholesale assign an object from one resource to another, and various other situations where the exact set of attributes is not explicitly specified in the source code immediately at the assignment site. That means we take this different approach so that the source of the object and the location that it's assigned to will not be so tightly coupled. Without this compromise, it would be a potential breaking change to add an attribute to any existing module output value or resource type schema, which would make it challenging to evolve providers and modules to meet new requirements. (The idea here is similar to how in JSON-based API design it's common to ask the recipient of a JSON document to ignore properties they don't recognize, rather than immediately returning an error, so that there is a clear path to backward-compatible API evolution.)\r\n\r\nThese two design approaches have different tradeoffs, and so I don't think it's really possible to rule that one is unambiguously \"better\" than the other. Instead, like most API design decisions, one needs to understand the benefits and consequences of each approach and decide which one best suits the problem. I think it's fair to say that the value-based approach offers more flexibility and is therefore probably a reasonable default choice where the answer is unclear, but I wouldn't assert that it's universally better to use that over blocks.\r\n\r\nFor example, [in my old experimental testing provider](https:\/\/registry.terraform.io\/providers\/apparentlymart\/testing\/latest\/docs\/data-sources\/testing_assertions) I decided to use nested blocks to describe each assertion because there is little reason to be generating a set of test assertions dynamically, and the block structure allows Terraform to give better feedback for incorrect usage in return for the relative rigidity. But I would not claim that tradeoff as universal; for most real infrastructure objects (as opposed to utility providers) it seems more important to be flexible in allowing dynamic definitions, even though that generates slightly less helpful feedback for incorrect usage.\r\n\r\n\r\n\r\n\r\n"],"labels":["enhancement","config"]},{"title":"Fix error message for Import block with blank ID","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\nThese changes will add a friendly error message when an `import` block `id` is empty string `\"\"`.\r\n\r\n<b>Example Code<\/b>\r\n\r\n```\r\nimport {\r\n  to = null_resource.foo\r\n  id = \"\"\r\n}\r\n```\r\n\r\n<b>Error Message Before<\/b>\r\n\r\n```\r\n$ terraform plan                                                                                                                                                           \ue0b2 \u2714 \ue0b2 22s \uf252 \r\n\r\nPlanning failed. Terraform encountered an error while generating this plan.\r\n\r\n\u2577\r\n\u2502 Error: Resource has no configuration\r\n\u2502 \r\n\u2502 Terraform attempted to process a resource at null_resource.foo that has no configuration. This is a bug in Terraform; please report it!\r\n\u2575\r\n\r\n```\r\n\r\n<b>Error Message After<\/b>\r\n\r\n```\r\n$ terraform plan \r\n\r\n\u2502 Error: Import ID cannot be blank\r\n\u2502 \r\n\u2502   on temp.tf line 3, in import:\r\n\u2502    3:   id = \"\"\r\n\u2502 \r\n\u2502 The id argument of the import block must be a non-empty string. Please see the resource documentation for how to find the import ID.\r\n\u2575\r\n\r\n```\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes https:\/\/github.com\/hashicorp\/terraform\/issues\/33505\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33558) <br\/>All committers have signed the CLA.","Thanks for this submission! I've assigned it to @kmoe for review. ","@GOTAM672 Looks like this is almost there, just need to fix the failing unit test.","Done :)","Hello @kmoe, I believe I have addressed the issues with the test cases. Would you kindly review them once more, please?","@GOTAM672 The Code Consistency Checks are still failing - take a look. It's an easy fix.\r\n\r\nPlease also re-target this PR at the v1.5 branch. This will go in as a bug fix, since the import ID was changed to an `hcl.Expression` on `main` for the v1.6 release.\r\n\r\nThanks!","@GOTAM672 Are you still interested in getting this PR over the line? Let me know, and no worries if not. Thanks!"],"labels":["bug","waiting-response"]},{"title":"Update lower.mdx(removed russian language)","body":"Removed russian language from examples.\r\n\r\nIf that's not russian language, PR can be closed","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33557) <br\/>All committers have signed the CLA.","Google Translate detects it as Kazakh, though it is the same in Russian. I will refer this to the docs team for their review. "],"labels":["documentation","tw-reviewed"]},{"title":"Bump word-wrap from 1.2.3 to 1.2.4 in \/website","body":"Bumps [word-wrap](https:\/\/github.com\/jonschlinkert\/word-wrap) from 1.2.3 to 1.2.4.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/releases\">word-wrap's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>1.2.4<\/h2>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Remove default indent by <a href=\"https:\/\/github.com\/mohd-akram\"><code>@\u200bmohd-akram<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/24\">jonschlinkert\/word-wrap#24<\/a><\/li>\n<li>\ud83d\udd12fix: CVE 2023 26115 (2) by <a href=\"https:\/\/github.com\/OlafConijn\"><code>@\u200bOlafConijn<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/41\">jonschlinkert\/word-wrap#41<\/a><\/li>\n<li>:lock: fix: CVE-2023-26115 by <a href=\"https:\/\/github.com\/aashutoshrathi\"><code>@\u200baashutoshrathi<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/33\">jonschlinkert\/word-wrap#33<\/a><\/li>\n<li>chore: publish workflow by <a href=\"https:\/\/github.com\/OlafConijn\"><code>@\u200bOlafConijn<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/42\">jonschlinkert\/word-wrap#42<\/a><\/li>\n<\/ul>\n<h2>New Contributors<\/h2>\n<ul>\n<li><a href=\"https:\/\/github.com\/mohd-akram\"><code>@\u200bmohd-akram<\/code><\/a> made their first contribution in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/24\">jonschlinkert\/word-wrap#24<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/OlafConijn\"><code>@\u200bOlafConijn<\/code><\/a> made their first contribution in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/41\">jonschlinkert\/word-wrap#41<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aashutoshrathi\"><code>@\u200baashutoshrathi<\/code><\/a> made their first contribution in <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/pull\/33\">jonschlinkert\/word-wrap#33<\/a><\/li>\n<\/ul>\n<p><strong>Full Changelog<\/strong>: <a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/compare\/1.2.3...1.2.4\">https:\/\/github.com\/jonschlinkert\/word-wrap\/compare\/1.2.3...1.2.4<\/a><\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/f64b188c7261d26b99e1e2075d6b12f21798e83a\"><code>f64b188<\/code><\/a> run verb to generate README<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/03ea08256ba0c8e8b02b1b304f0f5bd2b1863207\"><code>03ea082<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/issues\/42\">#42<\/a> from jonschlinkert\/chore\/publish-workflow<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/420dce9a2412b21881202b73a3c34f0edc53cb2e\"><code>420dce9<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/jonschlinkert\/word-wrap\/issues\/41\">#41<\/a> from jonschlinkert\/fix\/CVE-2023-26115-2<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/bfa694edf55bb84ff84512f13da6d68bf7593f06\"><code>bfa694e<\/code><\/a> Update .github\/workflows\/publish.yml<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/ace0b3c78f81aaf43040bab3bc91d3c5546d3fd2\"><code>ace0b3c<\/code><\/a> chore: bump version to 1.2.4<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/6fd727594676f3e1b196b08a320908bec2f4ca02\"><code>6fd7275<\/code><\/a> chore: add publish workflow<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/30d6daf60fce429f5f559252fa86ee78200652c4\"><code>30d6daf<\/code><\/a> chore: fix test<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/655929cabea6299dddf3b4a21fc3713fca701b48\"><code>655929c<\/code><\/a> chore: remove package-lock<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/49e08bbc32a84da5d79e6b7e0fa74ff6217f6d81\"><code>49e08bb<\/code><\/a> chore: added an additional testcase<\/li>\n<li><a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/commit\/9f626935f3fac6ec0f3c4b26baea4eb9740d9645\"><code>9f62693<\/code><\/a> fix: cve 2023-26115<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/jonschlinkert\/word-wrap\/compare\/1.2.3...1.2.4\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=word-wrap&package-manager=npm_and_yarn&previous-version=1.2.3&new-version=1.2.4)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https:\/\/github.com\/hashicorp\/terraform\/network\/alerts).\n\n<\/details>\n\n> **Note**\n> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.\n","comments":[],"labels":["dependencies","javascript"]},{"title":"grpc client panic with nil resource schema","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.3\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nNA\r\n\r\n### Debug Output\r\n\r\nNA\r\n\r\n### Expected Behavior\r\n\r\nDocs should have been generated\r\n\r\n### Actual Behavior\r\n\r\ngo version 1.20.6\r\n```\r\n~\/work\/src\/github.com\/juju\/terraform-provider-juju$ go generate .\/...\r\nrendering website for provider \"terraform-provider-juju\" (as \"terraform-provider-juju\")\r\ncopying any existing content to tmp dir\r\nexporting schema from Terraform\r\ncompiling provider \"juju\"\r\nusing Terraform CLI binary from PATH if available, otherwise downloading latest Terraform CLI binary\r\nrunning terraform init\r\ngetting provider schema\r\nError executing command: unable to generate website: exit status 11\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nPlease report the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version, the stack trace\r\nshown below, and any additional information which may help replicate the issue.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: invalid memory address or nil pointer dereference\r\ngoroutine 1 [running]:\r\nruntime\/debug.Stack()\r\n\t\/snap\/go\/current\/src\/runtime\/debug\/stack.go:24 +0x65\r\nruntime\/debug.PrintStack()\r\n\t\/snap\/go\/current\/src\/runtime\/debug\/stack.go:16 +0x19\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/logging\/panic.go:58 +0x153\r\npanic({0x2440000, 0x41eadf0})\r\n\t\/snap\/go\/current\/src\/runtime\/panic.go:890 +0x263\r\ngithub.com\/hashicorp\/terraform\/internal\/plugin\/convert.ProtoToConfigSchema(0x0)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/plugin\/convert\/schema.go:107 +0x52\r\ngithub.com\/hashicorp\/terraform\/internal\/plugin\/convert.ProtoToProviderSchema(...)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/plugin\/convert\/schema.go:95\r\ngithub.com\/hashicorp\/terraform\/internal\/plugin.(*GRPCProvider).GetProviderSchema(0xc000c80630)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/plugin\/grpc_provider.go:129 +0x5af\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*contextPlugins).ProviderSchema(0xc000b0d530, {{0xc00041e020, 0x4}, {0x27ff34e, 0x9}, {0x2821029, 0x15}})\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/terraform\/context_plugins.go:100 +0x30b\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.loadProviderSchemas.func1({{0xc00041e020, 0x4}, {0x27ff34e, 0x9}, {0x2821029, 0x15}})\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/terraform\/schemas.go:112 +0x13f\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.loadProviderSchemas(0xc0005b2410?, 0x26686e0?, 0x0, 0x0?)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/terraform\/schemas.go:133 +0xf6\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.loadSchemas(0x0?, 0x0?, 0x0?)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/terraform\/schemas.go:93 +0xa5\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*Context).Schemas(0x0?, 0x0?, 0x0?)\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/terraform\/context.go:158 +0x2a\r\ngithub.com\/hashicorp\/terraform\/internal\/command.(*ProvidersSchemaCommand).Run(0xc0002edc00, {0xc0000500d0?, 0xffffffffffffffff?, 0x0?})\r\n\t\/build\/terraform\/parts\/terraform\/build\/internal\/command\/providers_schema.go:103 +0x8fe\r\ngithub.com\/mitchellh\/cli.(*CLI).Run(0xc000a88b40)\r\n\t\/root\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5f8\r\nmain.realMain()\r\n\t\/build\/terraform\/parts\/terraform\/build\/main.go:318 +0x1729\r\nmain.main()\r\n\t\/build\/terraform\/parts\/terraform\/build\/main.go:61 +0x19\r\n\r\n\r\nexit status 1\r\nmain.go:26: running \"go\": exit status 1\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n`go generate .\/...`\r\n\r\nFrom this PR https:\/\/github.com\/juju\/terraform-provider-juju\/pull\/248, though the PR might change over time. Hopefully the nil pointer panic trace is sufficient. We will be looking to see what is wrong in the PR code to produce this.\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this report!","Getting similar error - \r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nTerraform crashed! This is always indicative of a bug within Terraform.\r\nPlease report the crash with Terraform[1] so that we can fix this.\r\n\r\nWhen reporting bugs, please include your terraform version, the stack trace\r\nshown below, and any additional information which may help replicate the issue.\r\n\r\n[1]: https:\/\/github.com\/hashicorp\/terraform\/issues\r\n\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!! TERRAFORM CRASH !!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n\r\nruntime error: invalid memory address or nil pointer dereference\r\ngoroutine 30 [running]:\r\nruntime\/debug.Stack()\r\n\t\/usr\/local\/go\/src\/runtime\/debug\/stack.go:24 +0x65\r\nruntime\/debug.PrintStack()\r\n\t\/usr\/local\/go\/src\/runtime\/debug\/stack.go:16 +0x19\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n\t\/home\/circleci\/project\/project\/internal\/logging\/panic.go:55 +0x153\r\npanic({0x23441a0, 0x406fc90})\r\n\t\/usr\/local\/go\/src\/runtime\/panic.go:884 +0x212\r\ngithub.com\/hashicorp\/terraform\/internal\/plans.(*Plan).CanApply(0x0?)\r\n\t\/home\/circleci\/project\/project\/internal\/plans\/plan.go:100 +0x[18](https:\/\/github.com\/VIDADiagnostics\/portal-infrastructure\/actions\/runs\/5724268892\/job\/15510431798?pr=505#step:10:19)\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/local.(*Local).opPlan(0xc001673040, {0x2ccf8f8, 0xc000e84780}, {0x2ccf8f8, 0xc000e847c0}, 0xc0014d01[20](https:\/\/github.com\/VIDADiagnostics\/portal-infrastructure\/actions\/runs\/5724268892\/job\/15510431798?pr=505#step:10:21), 0xc000e84740)\r\n\t\/home\/circleci\/project\/project\/internal\/backend\/local\/backend_plan.go:104 +0x690\r\ngithub.com\/hashicorp\/terraform\/internal\/backend\/local.(*Local).Operation.func1()\r\n\t\/home\/circleci\/project\/project\/internal\/backend\/local\/backend.go:3[23](https:\/\/github.com\/VIDADiagnostics\/portal-infrastructure\/actions\/runs\/5724268892\/job\/15510431798?pr=505#step:10:24) +0xc3\r\ncreated by github.com\/hashicorp\/terraform\/internal\/backend\/local.(*Local).Operation\r\n\t\/home\/circleci\/project\/project\/internal\/backend\/local\/backend.go:[31](https:\/\/github.com\/VIDADiagnostics\/portal-infrastructure\/actions\/runs\/5724268892\/job\/15510431798?pr=505#step:10:32)6 +0x[44](https:\/\/github.com\/VIDADiagnostics\/portal-infrastructure\/actions\/runs\/5724268892\/job\/15510431798?pr=505#step:10:45)a\r\nError: Terraform exited with code 11.\r\nError: Process completed with exit code 1.","Hi @hmlanigan, thanks for filing this! Based on the stack trace the crash is originating [here](https:\/\/github.com\/hashicorp\/terraform\/blob\/v1.5.3\/internal\/plugin\/convert\/schema.go#L107). This suggests that your provider was passing in the schema as `nil`, and then when Terraform tries reads from it we get our crash. It seems like an easy fix would be for us to just check for `nil` before processing that.\r\n\r\nWhat I'm curious about is how it's actually possible for Terraform to be receiving a `nil` schema - did you ever manage to figure out what was causing the crash from within your PR?","Hi @lbhattvida, I think your stack trace is a bit different to the one provided initially. Did this also trigger while generating the docs for a provider? If not, it might be best for you to file that as a separate issue and provide the configuration you were planning or applying, or the commands you ran to trigger it. Thanks!","@liamcervante the code is working now and not panicking. Unfortunately I'm not sure exactly what the change to resolve it was. We've been on a steep learning curve on how terraform works in general while migration from using the SDK to the Framework.","Okay, thanks! I'm concerned we have a bug in the provider SDK that is causing it to send invalid schema. With that in mind, I'm tempted to not fix this right now and hope we can reproduce it again another time. This would allow us to both fix up the SDK and Terraform itself. If we just made Terraform more robust, we might not discover the issue in the SDK."],"labels":["bug","core","waiting for reproduction","v1.5"]},{"title":"Terraform outputs sensitive information when using import block as comment","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nimport {\r\n  to = aws_iam_access_key.access_key_1\r\n  id = \"AK...\" # Key value omitted\r\n}\r\n```\r\n\r\nWill output \r\n\r\n```terraform\r\n# generated_resources.tf\r\n# __generated__ by Terraform from \"AK...\"\r\nresource \"aws_iam_access_key\" \"access_key\" {\r\n  pgp_key = null\r\n  status  = \"Active\"\r\n  user    = \"...\"\r\n}\r\n```\r\n\r\n\n\n### Debug Output\n\nN\/A\n\n### Expected Behavior\n\nTerraform maybe should hash the value instead of plain text. While  having only the AWS Access Key ID without the Secret Access Key is not as dangerous as having both, but it is still essential to treat the Access Key ID as sensitive information and take measures to protect it from unauthorized access. \n\n### Actual Behavior\n\nterraform creates the resources and leaks the access key ids while the import blocks can be moved.\n\n### Steps to Reproduce\n\nterraform plan -generate-config-out=generated_resources.tf\n\n### Additional Context\n\n_No response_\n\n### References\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/import\/generating-configuration","comments":["Thanks, I agree we should be conservative here and not write the import id to the comment in case it's sensitive.","This does seem to be a rather different idea of \"sensitive\" than how we usually think of it.\r\n\r\nKnowing an access key ID alone does not give access to anything new, but knowing it might be useful for cross-referencing with other information outside of this scope. But by that argument, _all_ ids are sensitive because they can be used for cross-referencing.\r\n\r\nI don't mean to say that we shouldn't just delete this comment from the generated code, since the need for it seems pretty marginal anyway. But I think we should be careful to be clear about what we mean when we say \"sensitive\"; elsewhere in Terraform it typically means a credential granting some access to something. IAM distinguishes between the access key ID and its secret specifically so that it's possible to talk about a specific access key without disclosing its secret token.\r\n\r\nIf we broaden \"sensitive\" to mean \"anything that any individual might prefer not to disclose\" then Terraform would not be able to generate any configuration at all, and so I think as part of this change (along with removing the unnecessary comment as suggested) we might wish to make sure the docs about config generation spend some words discussing the possibility that the generated files might contain information that should be edited out by the user before committing the changes, since Terraform doesn't have enough information to reliably guess what might be problematic to include in any specific situation.\r\n\r\n","Yes, \"sensitive\" is not accurate here. It is sufficient that the comment is unnecessary.","That said, this becomes a bigger problem if #33228 is implemented, as then it may be possible for an import ID to be the result of interpolating a sensitive value, in which case it is definitely inappropriate to write the literal value to the generated file.\r\n\r\n\r\nIn the case described in this issue, the value written to the generated file is, by definition, already in configuration, since it must be a literal string inside the `import` block.","We currently prohibit using a sensitive map (in the sense defined by the language runtime) in resource and module `for_each` so that we can avoid having resource instance addresses that we can't show in the UI, since that would make it very hard to properly review the proposed plan or updated state.\r\n\r\nThat might be sufficient precedent to also prohibit using a sensitive string as the `id` for import, although if we can avoid actually showing the ID string in the UI anywhere (aside from showing the real resource attributes that just happen to be derived from it, where our UI code already respects sensitivity) then that's less of a concern.\r\n","The import block also outputs the import ID in output as where the resource is being imported from and as the Id of the resource in the plan before apply. This means for cases like the venafi_certificate resource where the import ID contains the password for the private key that sensitive information gets exposed. This occurs even if the sensitive function is used on the value assigned to the id field of the import block.\r\n\r\nThe terraform import command outputs the import ID as well.\r\n\r\nThis means that there is no way to import resources such as venafi_certificate without exposing sensitive information. It would good if the ID of the import block respected the sensitive function.\r\n\r\nHere is the doco on the import ID venafi_certificate:\r\nhttps:\/\/registry.terraform.io\/providers\/Venafi\/venafi\/latest\/docs\/resources\/venafi_certificate#import"],"labels":["enhancement","plannable-import","v1.5"]},{"title":"Impossible to use Private Modules Registry if it uses url subpath","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform version: 1.5.3\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```hcl\r\nmodule \"my_module_name\" {\r\n  source = \"**server_url**\/gitlab\/**project**\/**name**\/**system**\"\r\n  version = \"0.0.1\"\r\n}\r\n```\r\n\r\n### Debug Output\r\n\r\n```\r\n2023-07-18T13:28:21.912Z [TRACE] HTTP client GET request to https:\/\/**server_url**\/.well-known\/terraform.json\r\n\u2502 Error: Invalid registry module source address\r\n\u2502\r\n\u2502   on main.tf line 62, in module \"my_module_name\":\r\n\u2502   62:   source = \"**server_url**\/gitlab\/**project**\/**name**\/**system**\"\r\n\u2502\r\n\u2502 Failed to parse module registry address: a module registry source address must have either three or four\r\n\u2502 slash-separated components.\r\n\u2502\r\n\u2502 Terraform assumed that you intended a module registry source address because you also set the argument \"version\",\r\n\u2502 which applies only to registry modules.\r\n```\r\n\r\n### Expected Behavior\r\n\r\nAbility to configure a proper server endpoint, so:\r\n1) metadata can be properly downloaded from\r\nhttps:\/\/**server_url**\/gitlab\/.well-known\/terraform.json\r\ninstead of\r\nhttps:\/\/**server_url**\/.well-known\/terraform.json\r\n\r\n2) parsing of the module url works properly\r\n\r\n### Actual Behavior\r\n\r\n1) [TRACE] HTTP client GET request to https:\/\/***\/.well-known\/terraform.json\r\n2) \u2502 Failed to parse module registry address: a module registry source address must have either three or four\r\n\u2502 slash-separated components.\r\n\r\n### Steps to Reproduce\r\n\r\nterraform init\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @MrImpossibru,\r\n\r\nThe behavior you've described here is how Terraform is intended to behave: a module registry source address for a third-party registry _must_ have exactly four parts: hostname, namespace, module name, and target system.\r\n\r\nTherefore you should remove the extra \"gitlab\" portion from your source address, because it is invalid.\r\n\r\nIf your registry implementation requires extra path elements in the underlying API URL then you should include those in the `\/.well-known\/terraform.json` file, and then Terraform will use the base URL you specified there.\r\n\r\n[The `.well-known` path convention](https:\/\/en.wikipedia.org\/wiki\/Well-known_URI) is specified in [RFC 8615](https:\/\/datatracker.ietf.org\/doc\/html\/rfc8615) and is required to be at the root of the URL space for the specified hostname. It is invalid to place it at any other path.\r\n","Hi @apparentlymart ,\nGitlab server was deployed with a subpath: **server_url**\/gitlab\/, so there's no such possibility to avoid that subpath.","Hi @MrImpossibru,\r\n\r\nIf you have GitLab deployed at a location other than the root of the server then GitLab will not be able to serve the `\/.well-known\/terraform.json` document automatically. In that case, you will need to configure your web server separately to serve a suitable `\/.well-known\/terraform.json` document which includes information equivalent to what GitLab would have reported.\r\n\r\nBased on what is reported in [the `gitlab.com` discovery document](https:\/\/gitlab.com\/.well-known\/terraform.json) I would guess that your `\/.well-known\/terraform.json` URL should return a JSON document like this:\r\n\r\n```json\r\n{\"modules.v1\":\"\/gitlab\/api\/v4\/packages\/terraform\/modules\/v1\/\"}\r\n```\r\n\r\nNotice that I included your `\/gitlab\/` prefix in the URL, so that when Terraform performs service discovery for this hostname it will then find the URL of the GitLab module registry API. This means that you only need to manually implement the service discovery JSON document, and you do not need to re-implement any parts of GitLab's registry API endpoints.\r\n\r\n(I don't know if self-hosted GitLab uses the same API endpoints as `gitlab.com`, so I would suggest first retrieving the `terraform.json` document from your own internal GitLab server to learn whether the API locations are the same or different for self-hosted GitLab.)\r\n","Hi @apparentlymart ,\n\nAnyway parsing of the path will fail because of \"too many segments\".\n\nBest regards,\nNikolai","Hi @MrImpossibru,\r\n\r\nCorrectly implementing the service discovery protocol as I described will allow you to use valid module source addresses, without your invalid extra \"gitlab\" segment.\r\n\r\nModule registry addresses are not URLs. If you wish to run a private module registry then you must implement the relevant protocols correctly and use the module source address syntax as documented.\r\n\r\nAlternatively, you can just specify Git repository sources directly and ignore the module registry protocol. The module registry is just an abstraction to hide the physical location of a particular module package, but if that abstraction is inconvenient for you then you can ignore it and specify the physical location directly.\r\n"],"labels":["bug","new"]},{"title":"Support for running terraform in time-constrained execution environments","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.2\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nOur end goal is to be able to apply terraform changes without the need for long-running threads. \r\n\r\nWe have an infrastructure deployment mechanism which accepts HCL from customers and runs `apply` on their behalf to affect the modeled changes. This way, the client does not need to have access to the underlying resources and does not need to actually run terraform themselves.\r\n\r\nCurrently, doing this requires that our server-side infrastructure be capable of having arbitrarily long-running processes in order to `apply` an HCL change with an unpredictable number of updates. If we want to run this server-side process on Lambda we are limited to the max runtime of 15 minutes, and we hog provisioned concurrency while it's running. We also have no guarantee that the thread or lambda running the apply process won't be terminated without completing successfully.\r\n\r\nGiven these challenges, we'd like to find a way to run apply without needing to block until its full completion.\r\n\r\n### Attempted Solutions\r\n\r\nI've made a dumb approach to solving this problem here: https:\/\/github.com\/gravitylow\/terraform\/commit\/71f03352f25fcd636f9968835ca7b5c30482d895\r\n\r\nWhich involves just halting the graph walk after 1 destroy\/update\/create has been found. When this patch is applied, the operator can progressively apply changes with multiple invocations of the CLI, where each invocation mutates the next resource in line and then returns. An example with a simple configuration with just an AWS VPC and Subnet and two outputs can be seen here: https:\/\/gist.github.com\/gravitylow\/3d05850c9eb75f886394e83ff1d0cf05\r\n\r\n### Proposal\r\n\r\nI'd like to propose adding a new CLI `apply` option `-max-changes` which can allow users to invoke this behavior by setting a limit of actions to apply in a single invocation.\r\n\r\nIn practice, my usecase would probably only ever be setting this to `-max-changes 1` so maybe an alternative could be to make a boolean flag such as `-single-change` or something with a better name.\r\n\r\nAnother more complex but potentially useful option would be to set a max running time, for example `-max-duration 10` to set a 10-second maximum. Once the maximum duration has been reached, the CLI will return after completing the current resource. This could allow multiple invocations to try and target the same runtime so you have a more consistent throughput by working on either one slow resource or several quicker resources dynamically. I think the complexity of this probably isn't worth the value but just for completeness sake :)\r\n\r\nI'm willing to implement this if we come to a consensus on the approach.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @gravitylow! Thanks for sharing this use-case.\r\n\r\nI think if we were to add something like this I would expect it to be a [planning option](https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/plan#planning-options) rather than an apply-time argument, because then Terraform would be able to be explicit about what will and will not be included when the plan is applied. Otherwise the operator would need to use their own intuition to infer which subset of the proposed operations would actually get applied.\r\n\r\nWith that said, I'm not sure that your proposed solution fully solves the problem as stated. If you have a hard time limit of 15 minutes then presumably a complete solution would require Terraform to somehow predict that a particular operation would take longer than the time limit and so skip starting it in the first place. In particular note that there are real examples of resource types that can take 30 minutes or more to complete applying a change, and so even just applying one change can potentially take longer than 15 minutes.\r\n\r\nMy instinct then is to use this issue to represent the general problem of running Terraform in an environment with a rigid execution time deadline -- which is not currently supported at all -- with the goal of evaluating whether supporting that situation is viable at all in the general case (where provider operations themselves have no time limit), and if so how best to represent that situation within the Terraform workflow.\r\n\r\nIn the meantime, I would recommend against running Terraform in environments like AWS Lambda. Today's Terraform expects to be able to run to completion without interruption in the normal case (exceptional system failures notwithstanding), although it does respond to a `SIGINT` signal by asking providers to safely abort any operations in progress if possible and not scheduling any additional work, and so a potential _workaround_ in today's Terraform is to send the Terraform CLI process a `SIGINT` signal after 10 minutes and then hope that the providers are able to abort their operations in progress in the remaining 5 minute deadline. However, it's up to each provider to define what \"safely abort\" means, and so I cannot guarantee that all of the resource types you will using will be able to abort their work within that deadline.\r\n\r\nThanks again!\r\n","@apparentlymart Thanks for your quick and detailed response. Ack on the possible `SIGINT` workaround.\r\n\r\nI agree with your thoughts about the core problem here and have renamed the issue accordingly.\r\n\r\nI'll add that in my particular (probably relatively unique) use-case, we are in full control of the providers in-use in our environment and can accept a solution which requires providers to be well-behaved, either in responding to a `SIGINT` or in some other newly-developed mechanism which helps establish a rigid deadline for execution. Perhaps that helps scope down the effort to deliver an \"experimental\" feature which providers can opt-in to to support this use-case.\r\n\r\nIf not, or for the more general use-case, please let me know if you have recommendations for next steps on my part, as someone who is motivated to help deliver this functionality but is relatively unfamiliar with the scoping\/researching process for a new behavior like this.","Thinking more about this a bit -- I do think the value of limiting the number of changes to be applied can still be quite useful in this sort of architecture, for the purpose of load-balancing terraform work across multiple concurrent configuration changes being applied. Although you're right that it does not conclusively solve the problem of running on Lambda, for example (but it does make running into issues there less likely):\r\n\r\nConsider a scenario where we have a \"terraform worker\" to which an expensive configuration with many resources is submitted to apply, which takes ~10 minutes to complete in full. Directly after the expensive configuration is picked up by the worker, an inexpensive configuration with only one resource which only takes 10 seconds to apply is queued for execution by the worker. This lightweight configuration must remain completed untouched for the ten minutes the worker is tied up applying the expensive configuration.\r\n\r\nCompare this with if we apply the `-max-changes=1` flag to the worker. The worker will pick up only the first resource from the expensive configuration and apply it, with an execution duration of < 10 minutes. It will then return to find more work to do, and (assuming there is a good \"fairness\" algorithm for selecting the next work to do) can select the single resource from the inexpensive template to complete it before returning to the expensive work from the original. \r\n\r\nWe can obviously help address this problem by scaling up our worker count and having concurrent runs going, but worker-1 will still be completely tied up and unable to do work until its configuration is fully applied, and if a single additional expensive template comes along we get into the same situation. Adding a max resource limit helps add more opportunities to \"take a break\" and re-evaluate what we should be doing and thus hopefully load-balance our output better.","Interesting question. As @apparentlymart says, choosing to run Terraform on something other than AWS Lambda will eliminate this class of problems.\r\n\r\nThe crux is here:\r\n> Consider a scenario where we have a \"terraform worker\" to which an expensive configuration with many resources is submitted to apply, which takes ~10 minutes to complete in full. Directly after the expensive configuration is picked up by the worker, an inexpensive configuration with only one resource which only takes 10 seconds to apply is queued for execution by the worker.\r\n\r\nTerraform has no way of knowing which configurations will be expensive, and which cheap, to apply. Providers also have no way of knowing in general. \r\nA specific provider could be built with [timeouts](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework\/resources\/timeouts) set in every resource schema. Then we must consider retries, behaviour on interrupt as discussed above, additions to the plugin protocol, and finally, the behaviour of the runner in all of these error cases, the outer system to which all these errors must eventually be propagated and handled. It's at least a rather complex solution, which makes me inclined to look for a simpler way to solve or avoid the problem."],"labels":["enhancement","new"]},{"title":"Add a `create_namespace` attribute to the Kubernetes backend configuration","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nHere are some use cases that the issue addresses:\r\n\r\n- Experiment faster (no need to configure a namespace, or ask someone to do it)\r\n- Avoid errors during the development\/experiment due to the use of a namespace that does not exist\r\n- Easy clean-up (when removing the namespace, the state is reset. Could be helpful when using Terraform to deploy only Kubernetes resources in the namespace)\n\n### Attempted Solutions\n\nHere are some solutions I've used to achieve this behavior actually:\r\n\r\n- Wrap the `terraform init` command with a script or whatever else that calls `kubectl create namespace my-ns` before calling Terraform.\r\n- Use a tool like [Terragrunt](https:\/\/terragrunt.gruntwork.io\/) that chains two modules (one for creating the namespace, and another to deploy the infrastructure)\n\n### Proposal\n\nWe can simply add a new configuration key into the Kubernetes backend configuration: `create_namespace`. This key will be a boolean that determines whether Terraform should create the namespace if it does not exist. Below is an example of the expected configuration : \r\n\r\n```hcl\r\nterraform {\r\n  backend \"kubernetes\" {\r\n    # other settings required...\r\n    namespace        = \"my-state-namespace\"\r\n    create_namespace = true # The namespace will be created automatically if it does not exist\r\n  }\r\n}\r\n```\r\n\r\nBy default, if not provided, the `create_namespace` value is `false` for sure to avoid any breaking changes. If set to `true`, during the init operation, the backend is responsible for ensuring the namespace is available to avoid any Terraform error.\r\n\r\nI've already a working proof of concept for this enhancement in my local terraform repository. If this enhancement is accepted by the core team, I'll open a PR for it.\n\n### References\n\nI didn't find any references for such behavior. Correct me if i'm wrong!","comments":["Thanks for this request. [CODEOWNERS](https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/CODEOWNERS) shows @jrhouston @alexsomesan as being the owners of the kubernetes backend, so just an FYI that this issue was filed. Thanks!"],"labels":["enhancement","backend\/k8s"]},{"title":"Impenetrable Constraints - More detail required from terraform contraint errors","body":"### Terraform Version\n\n```shell\nTerraform v1.5.3\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nHaving multiple conflicting constraints scattered across multiple terraform modules without an indication of whom is emitting the constraint makes diagnosing and fixing incredibly hard...\r\n\r\nExample:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Failed to query available provider packages\r\n\u2502 \r\n\u2502 Could not retrieve the list of available versions for provider\r\n\u2502 hashicorp\/aws: no available releases match the given constraints >= 4.0.0,\r\n\u2502 >= 4.2.0, >= 4.3.0, >= 4.8.0, < 5.0.0, ~> 5.7\r\n\u2575\r\n```\n\n### Attempted Solutions\n\nNone\n\n### Proposal\n\nReport in the error the source of each constraint such that one can resolve and upgrade the affected modules.\r\n\r\nPossible output style:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Failed to query available provider packages\r\n\u2502 \r\n\u2502 Could not retrieve the list of available versions for provider\r\n\u2502 hashicorp\/aws: no available releases match the given constraints >= 4.0.0,\r\n\u2502 >= 4.2.0 [in module.x.], >= 4.3.0 [in module.y], >= 4.8.0 [in module.z], < 5.0.0 [in required_version], ~> 5.7 [in hashicorp\/aws provider]\r\n\u2575\r\n```\r\nSomething along these lines. Resolving these otherwise is quite painful and can take many hours of digging through module dependencies of module dependencies.\n\n### References\n\n_No response_","comments":["Hi @richardj-bsquare! Thanks for this enhancement request.\r\n\r\nIn the meantime you might find it useful to know that you can run `terraform providers` to see Terraform summarize the provider dependencies across modules.\r\n\r\nSome of Terraform's error messages about provider installation include a suggestion to run that command for more information, but apparently not this one. Adding that hint here might be a good compromise to solving this issue, since I expect that the subsystem that's creating this error is dealing with the already-merged constraint set and so might not be able to produce more detail directly itself, whereas the providers command has access to the full configuration.\r\n"],"labels":["enhancement","new"]},{"title":"Problem with caching of vendor plugins in Terraform 1.4.0 and later versions","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.0\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n```\r\n# Dockerfile\r\nFROM jenkins\/agent:alpine\r\nUSER root\r\nENV PYTHONUNBUFFERED=1\r\nENV TF_PLUGIN_CACHE_DIR=\"\/tf_cache\"\r\nENV TF_CLI_CONFIG_FILE=\"\/tf_cache\/.terraformrc\"\r\n...\r\nRUN mkdir \/tf_cache && \\\r\n    chown -R jenkins:jenkins \/tf_cache && \\\r\n    chmod 775 \/tf_cache && \\\r\n    chmod 775 \/bin\/* \r\n\r\nCOPY .terraformrc \/tf_cache\/.terraformrc\r\n\r\nVOLUME [\"\/tf_cache\"]\r\n```\r\n\r\n```\r\n# .terraformrc\r\nplugin_cache_dir = \"\/tf_cache\"\r\ndisable_checkpoint = true\r\nplugin_cache_may_break_dependency_lock_file = true\r\n```\r\n\r\n### Debug Output\r\n```\r\n2023-07-10T13:14:44.009Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/kubernetes v2.21.1 from https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_linux_amd64.zip\r\n2023-07-10T13:14:53.340Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/hashicorp\/kubernetes v2.21.1 from existing cache \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n- Installed hashicorp\/kubernetes v2.21.1 (signed by HashiCorp)\r\n\r\n+ terraform init -reconfigure\r\n2023-07-10T13:14:43.413Z [INFO]  Terraform version: 1.4.0\r\n2023-07-10T13:14:43.413Z [DEBUG] using github.com\/hashicorp\/go-tfe v1.18.0\r\n2023-07-10T13:14:43.413Z [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.16.0\r\n2023-07-10T13:14:43.413Z [DEBUG] using github.com\/hashicorp\/terraform-config-inspect v0.0.0-20210209133302-4fd17a0faac2\r\n2023-07-10T13:14:43.413Z [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.0\r\n2023-07-10T13:14:43.413Z [DEBUG] using github.com\/zclconf\/go-cty v1.12.1\r\n2023-07-10T13:14:43.413Z [INFO]  Go runtime version: go1.19.6\r\n2023-07-10T13:14:43.413Z [INFO]  CLI args: []string{\"terraform\", \"init\", \"-reconfigure\"}\r\n2023-07-10T13:14:43.413Z [TRACE] Stdout is not a terminal\r\n2023-07-10T13:14:43.413Z [TRACE] Stderr is not a terminal\r\n2023-07-10T13:14:43.413Z [TRACE] Stdin is not a terminal\r\n2023-07-10T13:14:43.413Z [DEBUG] Attempting to open CLI config file: \/tf_cache\/.terraformrc\r\n2023-07-10T13:14:43.413Z [DEBUG] Not reading CLI config directory because config location is overridden by environment variable\r\n2023-07-10T13:14:43.413Z [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2023-07-10T13:14:43.413Z [DEBUG] ignoring non-existing provider search directory \/root\/.terraform.d\/plugins\r\n2023-07-10T13:14:43.413Z [DEBUG] ignoring non-existing provider search directory \/root\/.local\/share\/terraform\/plugins\r\n2023-07-10T13:14:43.413Z [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2023-07-10T13:14:43.413Z [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2023-07-10T13:14:43.414Z [INFO]  CLI command args: []string{\"init\", \"-reconfigure\"}\r\n\r\nInitializing the backend...\r\n2023-07-10T13:14:43.418Z [TRACE] Meta.Backend: built configuration for \"kubernetes\" backend with hash value 3266516096\r\n2023-07-10T13:14:43.418Z [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2023-07-10T13:14:43.418Z [DEBUG] New state was assigned lineage \"5d3830a1-9e0c-0aac-5879-5dda8f288e4e\"\r\n2023-07-10T13:14:43.418Z [TRACE] Meta.Backend: moving from default local state only to \"kubernetes\" backend\r\n2023-07-10T13:14:43.419Z [DEBUG] checking for provisioner in \".\"\r\n2023-07-10T13:14:43.421Z [DEBUG] checking for provisioner in \"\/bin\"\r\n2023-07-10T13:14:43.421Z [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2023-07-10T13:14:43.421Z [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2023-07-10T13:14:43.421Z [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2023-07-10T13:14:43.421Z [TRACE] statemgr.Filesystem: read nil snapshot\r\n2023-07-10T13:14:43.421Z [TRACE] Meta.Backend: ignoring local \"default\" workspace because its state is empty\r\n2023-07-10T13:14:43.422Z [DEBUG] Using kubeconfig: \/root\/.kube\/config\r\n2023-07-10T13:14:43.425Z [INFO]  Successfully initialized config\r\n2023-07-10T13:14:43.426Z [DEBUG] New state was assigned lineage \"34810b53-3e0f-cdf2-1d23-5b9407ad069b\"\r\n2023-07-10T13:14:43.482Z [TRACE] Meta.selectWorkspace: the currently selected workspace is present in the configured backend (default)\r\n\r\n2023-07-10T13:15:05.532Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/helm v2.10.1 from https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_linux_amd64.zip\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/hashicorp\/helm v2.10.1 from existing cache \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n- Installed hashicorp\/helm v2.10.1 (signed by HashiCorp)\r\n\r\n2023-07-10T13:15:05.121Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/rancher\/rancher2 v3.0.2 from https:\/\/releases.hashicorp.com\/terraform-provider-rancher2\/3.0.2\/terraform-provider-rancher2_3.0.2_linux_amd64.zip\r\n2023-07-10T13:15:05.122Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/rancher\/rancher2 v3.0.2 from existing cache \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n- Installed rancher\/rancher2 v3.0.2 (signed by a HashiCorp partner, key ID 2EEB0F9AD44A135C)\r\n\r\nInitializing provider plugins...\r\n- Finding latest version of hashicorp\/kubernetes...\r\n2023-07-10T13:14:43.550Z [DEBUG] Service discovery for registry.terraform.io at https:\/\/registry.terraform.io\/.well-known\/terraform.json\r\n2023-07-10T13:14:43.550Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/.well-known\/terraform.json\r\n2023-07-10T13:14:43.854Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/kubernetes\/versions\r\n2023-07-10T13:14:43.854Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/kubernetes\/versions\r\n- Finding latest version of rancher\/rancher2...\r\n2023-07-10T13:14:43.913Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/rancher\/rancher2\/versions\r\n2023-07-10T13:14:43.913Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/rancher\/rancher2\/versions\r\n- Finding latest version of hashicorp\/helm...\r\n2023-07-10T13:14:43.963Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/helm\/versions\r\n2023-07-10T13:14:43.963Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/helm\/versions\r\n2023-07-10T13:14:44.008Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-07-10T13:14:44.008Z [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-07-10T13:14:44.008Z [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2023-07-10T13:14:44.008Z [TRACE] providercache.fillMetaCache: scanning directory \/tf_cache\r\n2023-07-10T13:14:44.008Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/helm v2.10.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:14:44.008Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:14:44.009Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:14:44.009Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/helm 2.10.1\r\n2023-07-10T13:14:44.009Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:14:44.009Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n2023-07-10T13:14:44.009Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/kubernetes\/2.21.1\/download\/linux\/amd64\r\n2023-07-10T13:14:44.009Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/kubernetes\/2.21.1\/download\/linux\/amd64\r\n2023-07-10T13:14:44.057Z [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_SHA256SUMS\r\n2023-07-10T13:14:44.057Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_SHA256SUMS\r\n2023-07-10T13:14:44.268Z [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_SHA256SUMS.72D7468F.sig\r\n2023-07-10T13:14:44.268Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_SHA256SUMS.72D7468F.sig\r\n- Installing hashicorp\/kubernetes v2.21.1...\r\n2023-07-10T13:14:44.286Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/kubernetes v2.21.1 from https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_linux_amd64.zip\r\n2023-07-10T13:14:44.286Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-kubernetes\/2.21.1\/terraform-provider-kubernetes_2.21.1_linux_amd64.zip\r\n2023-07-10T13:14:52.765Z [DEBUG] Provider signed by 34365D9472D7468F HashiCorp Security (hashicorp.com\/security) <security@hashicorp.com>\r\n2023-07-10T13:14:53.337Z [TRACE] providercache.fillMetaCache: scanning directory \/tf_cache\r\n2023-07-10T13:14:53.338Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/helm v2.10.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:14:53.338Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:14:53.338Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:14:53.338Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/helm 2.10.1\r\n2023-07-10T13:14:53.338Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:14:53.338Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n2023-07-10T13:14:53.338Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/hashicorp\/kubernetes v2.21.1 from existing cache \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:14:53.340Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-07-10T13:14:53.340Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:14:53.340Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n- Installed hashicorp\/kubernetes v2.21.1 (signed by HashiCorp)\r\n2023-07-10T13:14:53.586Z [TRACE] providercache.fillMetaCache: using cached result from previous scan of .terraform\/providers\r\n2023-07-10T13:14:53.586Z [TRACE] providercache.fillMetaCache: using cached result from previous scan of \/tf_cache\r\n2023-07-10T13:14:53.586Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/rancher\/rancher2\/3.0.2\/download\/linux\/amd64\r\n2023-07-10T13:14:53.586Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/rancher\/rancher2\/3.0.2\/download\/linux\/amd64\r\n2023-07-10T13:14:53.632Z [DEBUG] GET https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_SHA256SUMS\r\n2023-07-10T13:14:53.632Z [TRACE] HTTP client GET request to https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_SHA256SUMS\r\n2023-07-10T13:14:53.987Z [TRACE] HTTP client GET request to https:\/\/objects.githubusercontent.com\/github-production-release-asset-2e65be\/186468979\/5bc12e80-1f67-4957-913d-fe03d45ede77?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230710%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230710T131453Z&X-Amz-Expires=300&X-Amz-Signature=7163325c625ffa9fc09cea6d10ef1f263a65881514a682adbaa165d1c62a80fc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=186468979&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-rancher2_3.0.2_SHA256SUMS&response-content-type=application%2Foctet-stream\r\n2023-07-10T13:14:54.224Z [DEBUG] GET https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_SHA256SUMS.sig\r\n2023-07-10T13:14:54.224Z [TRACE] HTTP client GET request to https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_SHA256SUMS.sig\r\n2023-07-10T13:14:54.276Z [TRACE] HTTP client GET request to https:\/\/objects.githubusercontent.com\/github-production-release-asset-2e65be\/186468979\/02ddf9de-1a88-4c64-b672-8a5a6e69550f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230710%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230710T131343Z&X-Amz-Expires=300&X-Amz-Signature=04ba69ff060c9bde7f3de367e1bab9b403d7106ce95897a9465cabfcc9570eae&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=186468979&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-rancher2_3.0.2_SHA256SUMS.sig&response-content-type=application%2Foctet-stream\r\n- Installing rancher\/rancher2 v3.0.2...\r\n2023-07-10T13:14:54.623Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/rancher\/rancher2 v3.0.2 from https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_linux_amd64.zip\r\n2023-07-10T13:14:54.623Z [TRACE] HTTP client GET request to https:\/\/github.com\/rancher\/terraform-provider-rancher2\/releases\/download\/v3.0.2\/terraform-provider-rancher2_3.0.2_linux_amd64.zip\r\n2023-07-10T13:14:54.954Z [TRACE] HTTP client GET request to https:\/\/objects.githubusercontent.com\/github-production-release-asset-2e65be\/186468979\/535d6035-7760-4483-a9a8-0a8dd8109265?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230710%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230710T131454Z&X-Amz-Expires=300&X-Amz-Signature=19fb424d0563b8067d93ae6da1575711193ad51a39823262189d3cb488f9d1f3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=186468979&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-rancher2_3.0.2_linux_amd64.zip&response-content-type=application%2Foctet-stream\r\n2023-07-10T13:15:04.416Z [DEBUG] Provider signed by 2EEB0F9AD44A135C Rancher terraform (Rancher GPG key for signing terraform providers and modules) <terraform-gpg@rancher.com>\r\n2023-07-10T13:15:05.118Z [TRACE] providercache.fillMetaCache: scanning directory \/tf_cache\r\n2023-07-10T13:15:05.118Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/helm v2.10.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:15:05.119Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:15:05.119Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:15:05.119Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/helm 2.10.1\r\n2023-07-10T13:15:05.119Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:15:05.119Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n2023-07-10T13:15:05.119Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/rancher\/rancher2 v3.0.2 from existing cache \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:15:05.121Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-07-10T13:15:05.121Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:15:05.122Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:15:05.122Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:15:05.122Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n- Installed rancher\/rancher2 v3.0.2 (signed by a HashiCorp partner, key ID 2EEB0F9AD44A135C)\r\n2023-07-10T13:15:05.405Z [TRACE] providercache.fillMetaCache: using cached result from previous scan of .terraform\/providers\r\n2023-07-10T13:15:05.405Z [TRACE] providercache.fillMetaCache: using cached result from previous scan of \/tf_cache\r\n2023-07-10T13:15:05.405Z [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/helm\/2.10.1\/download\/linux\/amd64\r\n2023-07-10T13:15:05.405Z [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/helm\/2.10.1\/download\/linux\/amd64\r\n2023-07-10T13:15:05.465Z [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_SHA256SUMS\r\n2023-07-10T13:15:05.465Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_SHA256SUMS\r\n2023-07-10T13:15:05.514Z [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_SHA256SUMS.72D7468F.sig\r\n2023-07-10T13:15:05.514Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_SHA256SUMS.72D7468F.sig\r\n- Installing hashicorp\/helm v2.10.1...\r\n2023-07-10T13:15:05.532Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/helm v2.10.1 from https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_linux_amd64.zip\r\n2023-07-10T13:15:05.532Z [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_linux_amd64.zip\r\n2023-07-10T13:15:16.548Z [DEBUG] Provider signed by 34365D9472D7468F HashiCorp Security (hashicorp.com\/security) <security@hashicorp.com>\r\n2023-07-10T13:15:17.123Z [TRACE] providercache.fillMetaCache: scanning directory \/tf_cache\r\n2023-07-10T13:15:17.123Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/helm v2.10.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:15:17.124Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:15:17.124Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/helm 2.10.1\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.fillMetaCache: including \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/hashicorp\/helm v2.10.1 from existing cache \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:15:17.125Z [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2023-07-10T13:15:17.125Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/helm v2.10.1 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n2023-07-10T13:15:17.125Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/kubernetes v2.21.1 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64\r\n2023-07-10T13:15:17.125Z [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/rancher\/rancher2 v3.0.2 for linux_amd64 at .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n2023-07-10T13:15:17.125Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/helm 2.10.1\r\n2023-07-10T13:15:17.125Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/hashicorp\/kubernetes\/2.21.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/kubernetes 2.21.1\r\n2023-07-10T13:15:17.125Z [TRACE] providercache.fillMetaCache: including .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 as a candidate package for registry.terraform.io\/rancher\/rancher2 3.0.2\r\n- Installed hashicorp\/helm v2.10.1 (signed by HashiCorp)\r\n```\r\n\r\n### Expected Behavior\r\n\r\nI was expecting Terraform to use cached vendor plugins instead of downloading them again.\r\n\r\n### Actual Behavior\r\n\r\nTerraform is downloading again the plugins from the providers, even though they are already present in the cache and apparently it detects them, if I am making a mistake please let me know.\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n2. `terraform apply`\r\n\r\n### Additional Context\r\n\r\nI am using Jenkins 2.235.5 and I have configured an agent template with a Terraform image. This image already includes a directory and volume \"tf_cache\", as well as the environment variables: TF_PLUGIN_CACHE_DIR=\"\/tf_cache\" and ENV TF_CLI_CONFIG_FILE=\"\/tf_cache\/.terraformrc\". Additionally, I copy a file named .terraformrc to \"tf_cache\" which contains the following settings:\r\n\r\n```\r\nplugin_cache_dir = \"\/tf_cache\".\r\ndisable_checkpoint = true\r\nplugin_cache_may_break_dependency_lock_file = true\r\n```\r\n\r\nThese settings are intended to set the plugin cache directory of the provider plugins to \/tf_cache. This setting works as expected up to Terraform version 1.3.9. However, starting with version 1.4.0, Terraform re-downloads vendor plugins, even though they are already present in the cache.\r\n\r\nThe \"tf_cache\" volume is mounted on the server where Jenkins is running. Although versions after 1.4.0 of Terraform persist the providers in \"tf_cache\", they do not use them and instead redownload them. This results in inefficient and undesirable behavior.\r\n\r\n### References\r\n\r\nI am not aware of any other problems or pull requests that should be linked here.","comments":["Thanks for this request! It may be a duplicate of https:\/\/github.com\/hashicorp\/terraform\/issues\/32901.","Hi @BrianTuduri! Sorry for this strange behavior and thanks for reporting it.\r\n\r\nI've been trying to make sense of the series of messages in the trace log you shared, but there are some oddities that I can't explain and I wonder if you have some additional context that might help me understand what's going on.\r\n\r\nThe main thing that confuses me is that I see log lines about installing provider packages from the cache before Terraform actually reached the point of \"Initializing provider plugins...\" and thus before it should know which versions it's supposed to be using:\r\n\r\n```\r\n2023-07-10T13:15:05.532Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/helm v2.10.1 from https:\/\/releases.hashicorp.com\/terraform-provider-helm\/2.10.1\/terraform-provider-helm_2.10.1_linux_amd64.zip\r\n2023-07-10T13:15:17.124Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/hashicorp\/helm v2.10.1 from existing cache \/tf_cache\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/hashicorp\/helm\/2.10.1\/linux_amd64\r\n- Installed hashicorp\/helm v2.10.1 (signed by HashiCorp)\r\n\r\n2023-07-10T13:15:05.121Z [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/rancher\/rancher2 v3.0.2 from https:\/\/releases.hashicorp.com\/terraform-provider-rancher2\/3.0.2\/terraform-provider-rancher2_3.0.2_linux_amd64.zip\r\n2023-07-10T13:15:05.122Z [TRACE] providercache.Dir.LinkFromOtherCache: linking registry.terraform.io\/rancher\/rancher2 v3.0.2 from existing cache \/tf_cache\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64 to .terraform\/providers\/registry.terraform.io\/rancher\/rancher2\/3.0.2\/linux_amd64\r\n- Installed rancher\/rancher2 v3.0.2 (signed by a HashiCorp partner, key ID 2EEB0F9AD44A135C)\r\n\r\nInitializing provider plugins...\r\n- Finding latest version of hashicorp\/kubernetes...\r\n2023-07-10T13:14:43.550Z [DEBUG] Service discovery for registry.terraform.io at https:\/\/registry.terraform.io\/.well-known\/terraform.json\r\n[etc]\r\n```\r\n\r\nHowever, I notice that the timestamp on the final log line is `13:14:43.55` while the earlier one was `13:15:05.532`, so it seems like perhaps these log lines have somehow been misordered by the system that you used to collect them?\r\n\r\nIf I try to slot those lines into the place where their timestamps suggest they should go then the sequence of events does seem plausible, so if I assume that this was just a log aggregation quirk then indeed it does seem like Terraform is somehow concluding that the items in the cache are not valid, but unfortunately the logs here don't seem to give any indication as to why, so I think this might take some additional debugging.\r\n\r\nI see in your CLI configuration you enabled `plugin_cache_may_break_dependency_lock_file = true`, and the code which handles that special exception is written to generate a log line acknowledging it:\r\n\r\n```\r\n[WARN] plugin_cache_may_break_dependency_lock_file: Using global cache dir package for %s v%s even though it doesn't match this configuration's dependency lock file\r\n```\r\n\r\nThere seems to be no such message in the logs you shared, which (assuming that there are no log lines missing from what you shared) suggests that the `plugin_cache_may_break_dependency_lock_file` was not active for some reason.\r\n\r\nThe log lines we _can_ see seem consistent with how Terraform would behave if that setting were not in effect and if the cache directory checksums were unverifyable, so it seems like Terraform is for some reason not honoring that setting, as opposed to the relevant log lines just being absent in your trace log.\r\n\r\nSo of course this raises the question of why exactly Terraform cannot \"see\" the `plugin_cache_may_break_dependency_lock_file` setting.\r\n\r\nAlthough I don't yet have a direct explanation for why this would be true, I have a hypothesis that for some reason that setting might be ineffective when the CLI configuration location is overridden using the `TF_CLI_CONFIG_FILE` environment variable. Of course that's not the _intended_ behavior, but setting that environment variable does alter some of the configuration loading behavior (since it's loading exactly one file rather than scanning for possibly many files in different directories), and so there might be a bug in how that different situation gets handled.\r\n\r\nIs it possible for you to temporarily arrange for that CLI configuration file to be in one of the standard search locations, so you can see if the incorrect behavior persists when you _don't_ set `TF_CLI_CONFIG_FILE`? Thanks!\r\n\r\n\r\n","Even I faced similar issue and I spent lot of time in fixing this issue by trying different possibilities. Finally here is what I did as workaround for the issue in terraform version v1.5.3.\r\n\r\nFirst make sure all the downloaded plugins from terraform repo are available at `$HOME\/.terraform.d\/tf-cache`\r\n\r\nAdd below `.terraformrc` config file to home directory.\r\n\r\n```\r\ncat <<EOF > $HOME\/.terraformrc\r\nprovider_installation {\r\n  filesystem_mirror {\r\n    path=\"${HOME}\/.terraform.d\/tf-cache\/\"\r\n    include=[\"registry.terraform.io\/hashicorp\/*\"]\r\n  }\r\n  direct {\r\n    exclude=[\"registry.terraform.io\/hashicorp\/*\"]\r\n  }\r\n}\r\nplugin_cache_dir=\"${HOME}\/.terraform.d\/tf-cache\/\"\r\ndisable_checkpoint=true\r\nEOF\r\n```\r\nWhile running terraform init use `--plugin-dir` option, it started working. \r\n\r\n`terraform init -backend-config=terraform-state.tfvars --plugin-dir \"$HOME\/.terraform.d\/tf-cache\/\"`\r\n\r\nHope this helps.","I have a similar issue with a fresh new environment setup to run terraform. I have following config:\r\n\r\n```shell\r\nplugin_cache_dir   = \"$HOME\/.terraform.d\/plugin-cache\"\r\n```\r\n\r\nEverytime when I run `terraform init`, it downloads the provider again even though they exist in the cache.\r\n\r\nThe log looks like below:\r\n\r\n<details>\r\n<summary>Log<\/summary>\r\n<pre>\r\n2024-03-07T13:38:34.267+0800 [INFO]  Terraform version: 1.7.4\r\n2024-03-07T13:38:34.267+0800 [DEBUG] using github.com\/hashicorp\/go-tfe v1.41.0\r\n2024-03-07T13:38:34.267+0800 [DEBUG] using github.com\/hashicorp\/hcl\/v2 v2.19.1\r\n2024-03-07T13:38:34.267+0800 [DEBUG] using github.com\/hashicorp\/terraform-svchost v0.1.1\r\n2024-03-07T13:38:34.267+0800 [DEBUG] using github.com\/zclconf\/go-cty v1.14.1\r\n2024-03-07T13:38:34.267+0800 [INFO]  Go runtime version: go1.21.5\r\n2024-03-07T13:38:34.267+0800 [INFO]  CLI args: []string{\"terraform\", \"init\"}\r\n2024-03-07T13:38:34.267+0800 [TRACE] Stdout is a terminal of width 198\r\n2024-03-07T13:38:34.267+0800 [TRACE] Stderr is a terminal of width 198\r\n2024-03-07T13:38:34.267+0800 [TRACE] Stdin is a terminal\r\n2024-03-07T13:38:34.267+0800 [DEBUG] Attempting to open CLI config file: \/home\/magodo\/.terraformrc\r\n2024-03-07T13:38:34.267+0800 [INFO]  Loading CLI configuration from \/home\/magodo\/.terraformrc\r\n2024-03-07T13:38:34.267+0800 [DEBUG] ignoring non-existing provider search directory terraform.d\/plugins\r\n2024-03-07T13:38:34.267+0800 [DEBUG] ignoring non-existing provider search directory \/home\/magodo\/.terraform.d\/plugins\r\n2024-03-07T13:38:34.267+0800 [DEBUG] ignoring non-existing provider search directory \/home\/magodo\/.local\/share\/terraform\/plugins\r\n2024-03-07T13:38:34.267+0800 [DEBUG] ignoring non-existing provider search directory \/usr\/local\/share\/terraform\/plugins\r\n2024-03-07T13:38:34.267+0800 [DEBUG] ignoring non-existing provider search directory \/usr\/share\/terraform\/plugins\r\n2024-03-07T13:38:34.268+0800 [INFO]  CLI command args: []string{\"init\"}\r\n\r\nInitializing the backend...\r\n2024-03-07T13:38:34.269+0800 [TRACE] Meta.Backend: no config given or present on disk, so returning nil config\r\n2024-03-07T13:38:34.269+0800 [TRACE] Meta.Backend: backend has not previously been initialized in this working directory\r\n2024-03-07T13:38:34.269+0800 [DEBUG] New state was assigned lineage \"faa16da8-6f83-086e-e859-5bb56fd36ed8\"\r\n2024-03-07T13:38:34.269+0800 [TRACE] Meta.Backend: using default local state only (no backend configuration, and no existing initialized backend)\r\n2024-03-07T13:38:34.269+0800 [TRACE] Meta.Backend: instantiated backend of type <nil>\r\n2024-03-07T13:38:34.269+0800 [DEBUG] checking for provisioner in \".\"\r\n2024-03-07T13:38:34.270+0800 [DEBUG] checking for provisioner in \"\/usr\/bin\"\r\n2024-03-07T13:38:34.270+0800 [TRACE] Meta.Backend: backend <nil> does not support operations, so wrapping it in a local backend\r\n2024-03-07T13:38:34.270+0800 [TRACE] backend\/local: state manager for workspace \"default\" will:\r\n - read initial snapshot from terraform.tfstate\r\n - write new snapshots to terraform.tfstate\r\n - create any backup at terraform.tfstate.backup\r\n2024-03-07T13:38:34.270+0800 [TRACE] statemgr.Filesystem: reading initial snapshot from terraform.tfstate\r\n2024-03-07T13:38:34.270+0800 [TRACE] statemgr.Filesystem: snapshot file has nil snapshot, but that's okay\r\n2024-03-07T13:38:34.270+0800 [TRACE] statemgr.Filesystem: read nil snapshot\r\n\r\nInitializing provider plugins...\r\n- Finding latest version of hashicorp\/azurerm...\r\n2024-03-07T13:38:34.270+0800 [DEBUG] Service discovery for registry.terraform.io at https:\/\/registry.terraform.io\/.well-known\/terraform.json\r\n2024-03-07T13:38:34.270+0800 [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/.well-known\/terraform.json\r\n2024-03-07T13:38:34.564+0800 [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/azurerm\/versions\r\n2024-03-07T13:38:34.564+0800 [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/azurerm\/versions\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: scanning directory .terraform\/providers\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: failed to resolve symlinks for .terraform\/providers: lstat .terraform: no such file or directory\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: error while scanning directory .terraform\/providers: cannot search .terraform\/providers: lstat .terraform\/providers: no such file or directory\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: scanning directory \/home\/magodo\/.terraform.d\/plugin-cache\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/azuread v2.38.0 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/azuread\/2.38.0\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/azurerm v3.94.0 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/azurerm\/3.94.0\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/time v0.9.1 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/time\/0.9.1\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/tls v4.0.4 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/tls\/4.0.4\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/hashicorp\/tls v4.0.5 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/tls\/4.0.5\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/vancluever\/acme v2.20.2 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/vancluever\/acme\/2.20.2\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] getproviders.SearchLocalDirectory: found registry.terraform.io\/vancluever\/acme v2.7.1 for linux_amd64 at \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/vancluever\/acme\/2.7.1\/linux_amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/azuread\/2.38.0\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/azuread 2.38.0\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/azurerm\/3.94.0\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/azurerm 3.94.0\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/time\/0.9.1\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/time 0.9.1\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/tls\/4.0.4\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/tls 4.0.4\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/hashicorp\/tls\/4.0.5\/linux_amd64 as a candidate package for registry.terraform.io\/hashicorp\/tls 4.0.5\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/vancluever\/acme\/2.7.1\/linux_amd64 as a candidate package for registry.terraform.io\/vancluever\/acme 2.7.1\r\n2024-03-07T13:38:34.837+0800 [TRACE] providercache.fillMetaCache: including \/home\/magodo\/.terraform.d\/plugin-cache\/registry.terraform.io\/vancluever\/acme\/2.20.2\/linux_amd64 as a candidate package for registry.terraform.io\/vancluever\/acme 2.20.2\r\n2024-03-07T13:38:34.837+0800 [DEBUG] GET https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/azurerm\/3.94.0\/download\/linux\/amd64\r\n2024-03-07T13:38:34.837+0800 [TRACE] HTTP client GET request to https:\/\/registry.terraform.io\/v1\/providers\/hashicorp\/azurerm\/3.94.0\/download\/linux\/amd64\r\n2024-03-07T13:38:35.136+0800 [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_SHA256SUMS\r\n2024-03-07T13:38:35.136+0800 [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_SHA256SUMS\r\n2024-03-07T13:38:35.544+0800 [DEBUG] GET https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_SHA256SUMS.72D7468F.sig\r\n2024-03-07T13:38:35.545+0800 [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_SHA256SUMS.72D7468F.sig\r\n- Installing hashicorp\/azurerm v3.94.0...\r\n2024-03-07T13:38:35.636+0800 [TRACE] providercache.Dir.InstallPackage: installing registry.terraform.io\/hashicorp\/azurerm v3.94.0 from https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_linux_amd64.zip\r\n2024-03-07T13:38:35.636+0800 [TRACE] HTTP client GET request to https:\/\/releases.hashicorp.com\/terraform-provider-azurerm\/3.94.0\/terraform-provider-azurerm_3.94.0_linux_amd64.zip\r\n<\/pre>\r\n<\/details>\r\n\r\nOnly when I enable `plugin_cache_may_break_dependency_lock_file`, it stopps the re-downloading. I suspect this is also unexpected?"],"labels":["bug","new"]},{"title":"Terraform registry module examples get misidentified as submodules","body":"### Terraform Version\n\n```shell\nany\n```\n\n\n### Terraform Configuration Files\n\nnone\n\n### Debug Output\n\nnone\n\n### Expected Behavior\n\nThe code of the example and relevant information should be shown.\n\n### Actual Behavior\n\nThe example is treated as a submodule, resulting in showing wrong information.\n\n### Steps to Reproduce\n\nGo to any module in terraform registry and view any example.\r\nFor example:\r\n- https:\/\/registry.terraform.io\/modules\/Azure\/vnet\/azurerm\/latest\/examples\/complete\r\n  - > This is a submodule used internally by Azure \/ vnet \/ azurerm . Using this submodule on its own is not recommended.\r\n- https:\/\/registry.terraform.io\/modules\/terraform-aws-modules\/rds\/aws\/latest\/examples\/complete-mysql\r\n  - README is shown but the code of the example is nowhere to be found\n\n### Additional Context\n\n[Standard Module Structure](https:\/\/developer.hashicorp.com\/terraform\/language\/modules\/develop\/structure) states that module examples should be in the `examples` directory, and that README for code examples is optional.\n\n### References\n\n_No response_","comments":["Thanks for the request! Although we do not typically track requests for the Registry in this repository, I have notified the appropriate team and will leave this open for the time being to be triaged. Thanks again!","Thanks! Could you point me to the proper repository for Registry issues? I was unable to find it, so even though I saw that none of the issue templates in this repository fit for Registry issues, I posted it here. ","There is no public-facing repository for Registry issues, unfortunately. Feedback is channeled through issues filed in other public-facing repos or the [Discuss forum](https:\/\/discuss.hashicorp.com\/). I could not find a clear category in the Discuss forum for feedback, but maybe try the [Terraform Provider](https:\/\/discuss.hashicorp.com\/c\/terraform-providers\/31) category. Thanks!"],"labels":["bug","registry","new"]},{"title":"build(deps): bump tough-cookie from 4.1.2 to 4.1.3 in \/website","body":"Bumps [tough-cookie](https:\/\/github.com\/salesforce\/tough-cookie) from 4.1.2 to 4.1.3.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/salesforce\/tough-cookie\/releases\">tough-cookie's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>4.1.3<\/h2>\n<p>Security fix for Prototype Pollution discovery in <a href=\"https:\/\/redirect.github.com\/salesforce\/tough-cookie\/issues\/282\">#282<\/a>. This is a minor release, although output from the <code>inspect<\/code> utility is affected by this change, we felt this change was important enough to be pushed into the next patch.<\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/salesforce\/tough-cookie\/commit\/4ff4d29f6cefd279a412b8d62a21142ebd410b36\"><code>4ff4d29<\/code><\/a> 4.1.3 release preparation, update the package and lib\/version to 4.1.3. (<a href=\"https:\/\/redirect.github.com\/salesforce\/tough-cookie\/issues\/284\">#284<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/salesforce\/tough-cookie\/commit\/12d474791bb856004e858fdb1c47b7608d09cf6e\"><code>12d4747<\/code><\/a> Prevent prototype pollution in cookie memstore (<a href=\"https:\/\/redirect.github.com\/salesforce\/tough-cookie\/issues\/283\">#283<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/salesforce\/tough-cookie\/commit\/f06b72d1d447f33dfa6222c0a3c0c5e063558248\"><code>f06b72d<\/code><\/a> Fix documentation for store.findCookies, missing allowSpecialUseDomain proper...<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/salesforce\/tough-cookie\/compare\/v4.1.2...v4.1.3\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=tough-cookie&package-manager=npm_and_yarn&previous-version=4.1.2&new-version=4.1.3)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https:\/\/github.com\/hashicorp\/terraform\/network\/alerts).\n\n<\/details>\n\n> **Note**\n> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.\n","comments":[],"labels":["dependencies","javascript"]},{"title":"Config validation errors not citing the expected configuration attribute\/line","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non darwin_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\ndata \"path_example\" \"e\" {\r\n  number_list = [\r\n    {\r\n      number       = \"3\"  # <= Invalid config because `must_be_even` is true\r\n      must_be_even = true\r\n    }\r\n  ]\r\n}\r\n```\r\n\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/chrismarget\/9f872fc55af45b164acfef42db7ad963\r\n\r\nOf particular interest here, the validation error includes the path to the problem attribute:\r\n\r\n```\r\nResponse contains error diagnostic: <snip> diagnostic_attribute=AttributeName(\"number_list\").ElementKeyInt(0).AttributeName(\"number\")\r\n```\r\n\n\n### Expected Behavior\n\nI expected the user-facing error message to use the attribute's path seen in the debug output to cite the specific configuration line with the problem `number` attribute.\r\n\r\nSomething like this, I guess? I'm not sure exactly what the config file citation should look like.\r\n\r\n```text\r\n\u2577\r\n\u2502 Error: invalid attribute combination\r\n\u2502 \r\n\u2502   with data.path_example.e,\r\n\u2502   on main.tf line 4, in data \"path_example\" \"e\":\r\n\u2502    4:       number = \"3\"\r\n\u2502 \r\n\u2502 number must be even when must_be_even is true\r\n\u2575\r\n```\n\n### Actual Behavior\n\nThe error message citation refers to the entire resource block, not the specific problem attribute `numbers[0].number`:\r\n\r\n```text\r\n\u2577\r\n\u2502 Error: invalid attribute combination\r\n\u2502 \r\n\u2502   with data.path_example.e,\r\n\u2502   on main.tf line 1, in data \"path_example\" \"e\":\r\n\u2502    1: data \"path_example\" \"e\" {\r\n\u2502 \r\n\u2502 number must be even when must_be_even is true\r\n\u2575\r\n```\r\n\r\nShortening the diagnostic path to just `AttributeName(\"number_list\").ElementKeyInt(0)` gets us much closer to the mark:\r\n\r\n```text\r\n\u2577\r\n\u2502 Error: invalid attribute combination\r\n\u2502 \r\n\u2502   with data.path_example.e,\r\n\u2502   on main.tf line 3, in data \"path_example\" \"e\":\r\n\u2502    3:     {\r\n\u2502    4:       number = \"3\"\r\n\u2502    5:       must_be_even = true\r\n\u2502    6:     }\r\n\u2502 \r\n\u2502 number must be even when must_be_even is true\r\n\u2575\r\n```\r\n\r\nI can get the error to point to the top of the list element, but I can't seem to get a citation for `number_list[0].number`.\r\n\n\n### Steps to Reproduce\n\nThere's a skeleton provider which demonstrates the problem [here](https:\/\/github.com\/chrismarget\/terraform-provider-path).\r\n\r\n* Compile it.\r\n* Tell your terraform about it (dev_overrides?).\r\n* cd test\r\n* terraform plan\n\n### Additional Context\n\n_No response_\n\n### References\n\nThe issue was previously discussed [here](https:\/\/discuss.hashicorp.com\/t\/framework-v1-3-2-pathsteps-in-addattributeerror-not-producing-the-results-i-expect\/55823\/3) on the Hashicorp discuss server.","comments":["Hi @chrismarget,\r\n\r\nThis looks more like the linked framework issue, rather than one in the Terraform CLI. Since we don't have any indication that the diagnostics are not working in core, I'm going to close this out for now and we can investigate the issue from the framework side.","@jbardin we are sending over what I personally thought would be a correct diagnostic attribute paths for these situations with nested attributes. It seems like the plan renderer is not correctly aligning the configuration source. Are we supposed to be doing something different on our side of the protocol?","Hmm, let's setup a test case to see where the info is getting misaligned, because normal diagnostics do track the attribute location correctly.","I'm not too familiar with the various passing around of source config, but I'm able to see what I think is unexpected behavior if I update the test configuration in `internal\/tfdiags\/TestAttributeValue` with this at the end:\r\n\r\n```hcl\r\nnested_list = [\r\n  {\r\n    nested_str = \"test\"\r\n  }\r\n]\r\nnested_map = {\r\n  testkey = {\r\n    nested_str = \"test\"\r\n  }\r\n}\r\n```\r\n\r\nAnd two new unit test cases:\r\n\r\n```go\r\n\t\t{\r\n\t\t\tAttributeValue(\r\n\t\t\t\tError,\r\n\t\t\t\t\"nested_list[0].nested_str\",\r\n\t\t\t\t\"detail\",\r\n\t\t\t\tcty.Path{\r\n\t\t\t\t\tcty.GetAttrStep{Name: \"nested_list\"},\r\n\t\t\t\t\tcty.IndexStep{Key: cty.NumberIntVal(0)},\r\n\t\t\t\t\tcty.GetAttrStep{Name: \"nested_str\"},\r\n\t\t\t\t},\r\n\t\t\t),\r\n\t\t\t&SourceRange{\r\n\t\t\t\tFilename: \"test.tf\",\r\n\t\t\t\tStart:    SourcePos{Line: 35, Column: 5, Byte: 459},\r\n\t\t\t\tEnd:      SourcePos{Line: 35, Column: 24, Byte: 478},\r\n\t\t\t},\r\n\t\t},\r\n\t\t{\r\n\t\t\tAttributeValue(\r\n\t\t\t\tError,\r\n\t\t\t\t\"nested_map[\\\"testkey\\\"].nested_str\",\r\n\t\t\t\t\"detail\",\r\n\t\t\t\tcty.Path{\r\n\t\t\t\t\tcty.GetAttrStep{Name: \"nested_map\"},\r\n\t\t\t\t\tcty.IndexStep{Key: cty.StringVal(\"testkey\")},\r\n\t\t\t\t\tcty.GetAttrStep{Name: \"nested_str\"},\r\n\t\t\t\t},\r\n\t\t\t),\r\n\t\t\t&SourceRange{\r\n\t\t\t\tFilename: \"test.tf\",\r\n\t\t\t\tStart:    SourcePos{Line: 40, Column: 5, Byte: 518},\r\n\t\t\t\tEnd:      SourcePos{Line: 40, Column: 24, Byte: 537},\r\n\t\t\t},\r\n\t\t},\r\n```\r\n\r\nWhich shows it zero'ing out the contextual information:\r\n\r\n```\r\n--- FAIL: TestAttributeValue (0.00s)\r\n    --- FAIL: TestAttributeValue\/18:{_nested_list[0].nested_str_detail} (0.00s)\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Line: 1 != 35\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Column: 1 != 5\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Byte: 0 != 459\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Line: 1 != 35\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Column: 1 != 24\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Byte: 0 != 478\r\n    --- FAIL: TestAttributeValue\/19:{_nested_map[\"testkey\"].nested_str_detail} (0.00s)\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Line: 1 != 40\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Column: 1 != 5\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: Start.Byte: 0 != 518\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Line: 1 != 40\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Column: 1 != 24\r\n        \/Users\/bflad\/src\/github.com\/hashicorp\/terraform\/internal\/tfdiags\/contextual_test.go:603: End.Byte: 0 != 537\r\n```\r\n","Thanks @bflad, the diagnostics on the wire do look correct, so this seems to be something related to nested attributes. We can continue and dig into the diagnostic internals here.","The heuristic for guessing a relevant configuration element for a given attribute path was written in a legacy SDK world and so IIRC it assumes that multiple levels of traversal are always through nested blocks, and not through attributes. I think the original goal there was to return the first attribute it encountered.\r\n\r\nTraversing inside of an attribute is a harder proposition because it requires digging into the arbitrarily-complex expression syntax rather than just dealing with the fixed nested block structure. This is one of the downsides of our later adoption of structural attribute types: it's value-oriented rather than syntax-oriented and so syntax-based analysis won't be possible in general.\r\n\r\nHowever, I think we should at least be able to identify the whole of `number_list` as the cause here in the worst case, and we clearly aren't even managing that yet, probably because the analyzer was written to assume that `number_list` must be a nested block type here, since that would have been true prior to structural attribute types.\r\n\r\nWe might also be able to teach the analyzer how to dig in to `hclsyntax` expression nodes to see if it can find an attribute inside an object constructor to highlight, but that will be a harder problem than just finding the first matching structural argument.\r\n\r\n","I can confirm this does not work as expected. Here are some details in my use case: https:\/\/github.com\/hashicorp\/terraform-plugin-framework\/issues\/795\r\n\r\nLooking forward for updates","Looking into the code I can see that indeed the heuristic does currently assume that any attribute path returned by a provider will refer to what _HCL_ considers to be an argument -- a key-value pair written directly inside a block -- but it does have some basic support for traversing one level of indexing under that, which I guess is there to deal with the oddities of [attributes as blocks mode](https:\/\/developer.hashicorp.com\/terraform\/language\/attr-as-blocks) where Terraform tolerates using block syntax for an attribute for historical reasons. That's why `AttributeName(\"number_list\").ElementKeyInt(0)` did something somewhat sensible.\r\n\r\nI think essentially this heuristic needs to be totally rewritten to accommodate the more general case of structural attributes. Currently it assumes that all traversal (except for the final index special case) is through nested blocks, but it will instead need to probe for either a nested block _or_ an attribute for any intermediate `AttributeName`, and then divert into either analyzing a nested body or into the syntax of an expression depending on which it finds.\r\n\r\nThe current implementation of that final-indexing case is doing some shallow static analysis:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/19a885074860d85da0050aa17698ce0af255e78f\/internal\/tfdiags\/contextual.go#L305-L314\r\n\r\nUsing `hcl.ExprList` here is a bit tricky because it's asking the underlying HCL implementation (either native syntax or JSON) to analyze in static mode, which causes the JSON syntax to evaluate its values a little differently. But I think the fallback to just reporting the entire expression when that analysis fails makes that okay as a best-effort heuristic... it should be able to traverse deeper as long as we keep finding object and tuple constructor expressions, but will need to give up if it encounters anything else, such as a function call.\r\n\r\nI imagine this expression analyzer could be generalized to produce a good answer for the situation this issue is talking about, but it won't be trivial.\r\n","Thank you @apparentlymart, @bflad, @jbardin!\r\n\r\nSeeing this team come together to discuss my bug report is a great feeling. I really appreciate you all and your attention to this issue.\r\n\r\nGiven the apparent complexity of dealing with the underlying issue, I'd be satisfied for medium (or long!) term with either of these:\r\n\r\n* \"at least be able to identify the whole of `number_list` as the cause here in the worst case,\" as mentioned by @apparentlymart\r\n* some clear advice about just how deep I should be pointing the `path.Path` in various nesting scenarios so that I can reliably write less-specific errors which render correctly\r\n\r\nThere's some discussion around that last point (legacy SDK vs. framework, attribute nesting vs. blocks) here, but I don't have enough of a grasp of the historical context to turn that discussion into actionable advice writing errors about deeply-nested attribute paths.\r\n\r\nedit: Once again, I demonstrate my inability to manage multiple GitHub accounts with any semblance of consistency."],"labels":["bug","core"]},{"title":"Unpack TF plugins in a more atomic way.","body":"The original implementation of plugin cache handling unpacks plugin\r\narchives in a way that can result in race conditions when multiple\r\nterraform processes are accessing the same plugin cache. Since the\r\narchives are being decompressed in chunks and output files are written\r\ndirectly to the cache, we observed following manifestations of the issue:\r\n\r\n- `text file busy` errors if other terraform processes are already\r\n  running the plugin and another process tries to replace it.\r\n- various plugin checksum errors triggered likely by simultaneous checksumming\r\n  and rewriting the file.\r\n\r\nThis PR changes the zip archives with plugins are handled:\r\n\r\n1. Instead of writing directly to the target directory,\r\n   `installFromLocalArchive` is now writing to a temporary staging\r\n   directory prefixed with`.temp` that resides in the plugin cache dir\r\n   to ensure this is on the same filesystem.\r\n2. After unpacking, the directory structure of the staging directory is\r\n   replicated in the `targetDir`. The directories are created as needed\r\n   and any files are moved using `os.Rename`. After this, the staging\r\n   directory is removed.\r\n3. Since the temporary staging directories can be picked up by\r\n   `SearchLocalDirectory` and make it fail in the situation when they're\r\n   removed during directory traversal, the function has been modified to\r\n   skip any entry that starts with dot.\r\n\r\nSigned-off-by: Milan Plzik <milan.plzik@grafana.com>\r\n\r\n<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #31964\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### BUG FIXES\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n-  Concurrent write access to plugin cache directory has been fixed.\r\n\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33479) <br\/>All committers have signed the CLA.","Thanks for this submission, I will raise it in triage. ","Oh please this would make me so happy, thanks","Thanks for the reminder, and thanks @mplzik again for the submission. \r\n\r\nThe review from triage is that this change would require more due diligence. One issue with this implementation is that it relies on `os.rename` being atomic, [which it is not](https:\/\/pkg.go.dev\/os#Rename). This may make the concurrency issue worse on different platforms.\r\n\r\nWe also noticed that the documentation should call out more clearly that the plugin cache is not concurrency-safe.\r\n\r\nI'll leave this PR open for discussion for the moment, but will likely close it after a time as the feeling was it would be better to redesign a concurrency-safe plugin cache, which would require more discussion and prioritization. \r\n\r\nThanks again for this submission! ","@crw I definitely agree that this is not solution to all the problems -- hence my comment `\/\/ On supported platforms, this should perform atomic replacement of the file.`, with `supported` meaning platforms where this is supported for `os.Rename`. Speaking of this, would hiding this change behind a (default off) flag would make this PR more acceptable? I understand that implementing a really atomic cache would be preferred, but likely also entails much bigger engineering time investment (and it hasn't been implemented for a longer time, suggesting it might not be trivial).\r\n\r\nLet me also do a bit of an argument in favor of merging this PR from the risk assessment side.\r\n\r\nThe golang documentation is not going too far on describing `os.Rename` non-atomicity, but https:\/\/stackoverflow.com\/questions\/30385225\/is-there-an-os-independent-way-to-atomically-overwrite-a-file goes a bit farther in describing the platform-specific behavior. Specifically (assuming files on the same volume):\r\n- On Linux, this operations should be atomic (as much as the underlying filesystems can guarantee), translating into Renameat syscall\r\n- On Windows, the golang implementation uses [MoveFileEx](https:\/\/github.com\/golang\/go\/blob\/74caf475e3b31a463637bfe3cbdb317674f0930f\/src\/internal\/syscall\/windows\/syscall_windows.go#L318C9-L318C19), which according to information available is trying to do atomic replace if applicable, resorting to delete\/copy operation if this is not supported. (see https:\/\/social.msdn.microsoft.com\/Forums\/windowsdesktop\/en-US\/449bb49d-8acc-48dc-a46f-0760ceddbfc3\/movefileexmovefilereplaceexisting-ntfs-same-volume-atomic, Doug's answer)\r\n- On MacOS, this translates to `Rename` syscall which is supposed to be atomic.\r\n\r\nI see two main differences:\r\n- The renaming approach uses more disk space temporarily, so some real-world setups that are on almost full disks might be tipped to exceeding available disk space. This might, however, happen also with increasing size of the providers.\r\n- In the windows case, the file might be deleted (and thus stop existing) for a short period of time if atomic rename is not supported. I believe this will trigger mostly the same issues that are manifesting now (although I can't provide more data to prove that -- our infrastructure is not using any Windows machines to run Terraform).\r\n","Hi. Just some 0.02\u20ac from userland. We (and many others using terraform directly or via wrappers - in my case terragrunt) are having a terrible and very painfull time due to the concurrency issues, stopping us from moving ahead, and causing uncertainty in processes which should be quite trivial.\r\nI would tend to agree with @mplzik in that, even if this is not 100% bulletprof, the fact that it is supposed to work much better on linux and (from what I understood) on Windows, means that a huge number of users should have their experience improved, and get back to using terraform in a productive and streamlined manner.\r\nIf in the future there is an \"even better way\" (TM) to improve the cache in more complex scenarios (like network file systems), then it would be also very welcomed, of course. But in the meantime, the benefits would already be rolled out.\r\nThanks","I'm failing to understand the rationale behind blocking a small, simple change that would fix the problem for the vast majority of users in favor of holding out for an unknown, unplanned bulletproof solution that (likely) no one is working on.  I'm finding it difficult to believe this could have been fixed for months already and yet it just sits here getting stale.\r\n\r\nYou can always revert the PR when you implement the really badass perfect plugin cache that you're now committed to implementing.  And if you're not committed to it, why block this?  Can we at least get a timeline of when you expect to release the real fix?","First, apologies to @mplzik for the late response on this, the team appreciates your thoughtful response on this PR. The team is still concerned about edge cases with this solution. Speaking based on my own observations of how the maintainer team considers PRs, I think this is a difficult section of code to change from outside due to plugin cache management being in the critical path for every user.\r\n\r\n@brandon-fryslie, to answer your question, the rationale is that the primary path to run Terraform is in sequence, not in parallel. The plugin cache is not concurrency safe. You are correct to assume no one is working on a more robust solution, as making the plugin cache safe for concurrent execution of Terraform is not currently a priority for the team. I'll re-raise this PR with our product manager to see if concurrency-safety for the plugin cache is something we can prioritize. Given that Terraform, as a whole, is not meant to be run in parallel, this likely has considerations beyond the plugin cache. However, it is always possible we will need to prioritize this as we add new features going forward. \r\n\r\nThanks for your feedback and continued interest in this feature. ","> the primary path to run Terraform is in sequence, not in parallel.\r\n\r\nJust to chime in - running Terraform in parallel (for different projects that use the same modules) is a huge part of our CI\/CD workflow."],"labels":["enhancement"]},{"title":"Allow import blocks within child modules","body":"Currently, `import` blocks may only appear in the root module. Should we allow them to appear in child modules?\r\n\r\nRelated: https:\/\/github.com\/hashicorp\/terraform\/issues\/33376","comments":["Appearing within child modules should be fine as it would only really apply to local modules and it would make passing providers to the import block easier or not needed","i would appreciate the option to specify either a fully qualified namespace, or a local\/relative namespace for import block within modules.","This is a huge blocker for moving to providers like the AWS Provider v6.x which has announced breaking changed to aws_ssm_parameter which requires import of changed variables.\r\n\r\n```\r\n Warning: Argument is deprecated\r\nwith module.this.aws_ssm_parameter.tfe_api_url\r\non .terraform\/modules\/this\/ssm.tf line 17, in resource \"aws_ssm_parameter\" \"tfe_api_url\":\r\n\r\n  overwrite = true\r\n\r\nthis attribute has been deprecated\r\n```\r\n","Adding my 2 cents, this is also a major issue for us. We've been writing a lot of modules that deploy cloud infrastructure and modules being unable to import within a child module is incredibly burdensome. ","Hey @AMEvers @arnvid @dylan-shipwell, I'd love to learn more here about your workflows and use cases. Could you email me at oismail@hashicorp.com and we can schedule time to chat?","> Hey I'd love to learn more here about your workflows and use cases. Could you email me at [oismail@hashicorp.com](mailto:oismail@hashicorp.com) and we can schedule time to chat?\r\n\r\nYou can reach me on slack if you ping Robert G.\r\n","i also have a use case.\r\n\r\ni use the tailscale terraform provider, and it requires the `tailscale_acl` resource is imported before it can be used - https:\/\/registry.terraform.io\/providers\/tailscale\/tailscale\/latest\/docs\/resources\/acl\r\n\r\nI have a module that wraps that resource, and i need to import the `tailscale_acl` in that module.\r\n","Is there any planning to implement this feature?","I would also like to be able to use it within child modules, I hadn't realised it was only for the root module and have spent the last few weeks implementing imports into a number of modules so we can update them and move away from legacy code since some of our modules date back to terraform 0.12 before we had an internal standard set and such.\r\n\r\nWe also have other scenarios that would be solved with the ability to use them specifically for the following two scenarios\r\n\r\nChanging Security Groups\/Rules without it deleting and re-creating, this has been an annoying blocker for a lot of people within the Company.\r\n\r\nDatadog Monitors for example we not too long ago had the Data Dog monitors all moved into a single company wide environment isntead of a different one for each aws environment\r\n\r\nA lot of builds stopped working and had to have monitors manually deleted so they could be recreated by the module.","off topic, work around\r\nin case it helps anyone\r\n<details>\r\n\r\ni don't really believe hcl `import` will be a successor to proper state migrations. (even if they reverse their opinion on child import statements being illegal)\r\n\r\nthere aren't any great terraform migration frameworks out there that i know of. there is https:\/\/github.com\/minamijoyo\/tfmigrate but it struggles to accomplish obvious things in a lot of ways similar to terraform native `import` does. \r\n\r\n\r\nfor now, i'm sticking to manual migration script files\r\nthese generally read like this\r\n```\r\n#!\/bin\/sh -eux\r\n# terraform\/myproject\/migrations\/0001-some-state-migration.sh\r\n: \"${TERRAFORM_BIN:=terraform}\" # use \"terraform\" from $PATH or $TERRAFORM_BIN if specified\r\n\r\ndo_forward=1\r\nwhile [[ $# -gt 0 ]]; do\r\n  case $1 in\r\n    --reverse) do_forward=0;;\r\n    *) echo \"Unknown option $1\"; exit 1;;\r\n  esac\r\n  shift\r\ndone\r\n\r\nfn_tf_forward() {\r\n    $TERRAFORM_BIN import foo.bar.baz my_uid\r\n}\r\n\r\nfn_tf_backward() {\r\n    $TERRAFORM_BIN state rm foo.bar.baz my_uid\r\n}\r\n\r\nif [ $do_forward -eq 1 ]; then\r\n    fn_tf_forward; \r\nelse\r\n    fn_tf_backward;\r\nfi\r\n```\r\n\r\nthe most annoying thing about this low tech solution is that there is no persistent database of which migration files have been completed. i just remember with my mind right now.\r\n\r\n`hcl import` is nice because it shows a plan of what should happen if both the import step and apply step are happy, this is a great feature, and it remembers if an import is complete.\r\n\r\ntfmigrate is nice because it remembers which migrations have been applied.\r\n\r\nthe nice thing about manual migration scripts is that you can write them in any language you like, meaning i have access to **for loops, and if this then that logic control, and string format\/templates**, not to mention service provider apis like cloud vendors. very helpful when i have 1000 users of a module that aren't all homogeneous but are patterned. given that its also software, i could use these to generate correct `import` hcl blocks, and i have considered doing this, the most annoying part of this to me, is that the import hcl files must be in the same folder as the top level terraform.tf\/main.tf, so i usually move them to a `mgirations\/` subfolder after they have been applied to not clutter the project. scripts that I run can live anywhere so i prefer that slightly.\r\n","I also have a scenario where this feature would be very useful.\r\nWe have some child modules that dynamically crates (or not) and manages (or not) several resources depending on the environment they are deployed. This is controlled in the root module by passing some flags.\r\nSome of these resources also need to be imported, and at the moment the only way to make this work is to have several complicated import blocks in the root module, where we use the for_each construct to replicate the flag-controlled configuration accordingly.\r\nIf we could push the import in the child module, our configuration would be way simpler, easier to maintain and less prone to errors.\r\n\r\nOf course, the ID of the imported resource would be an input variable for the child module, while at the moment import blocks only accept literal strings. I can imagine this being a challenge, but it should be doable to relax such constraint."],"labels":["enhancement","plannable-import"]},{"title":"More intelligent JSON diffing capabailities","body":"### Terraform Version\n\n```shell\nTerraform v1.5.0\r\non linux_amd64\r\n+ provider registry.terraform.io\/akamai\/akamai v4.1.0\r\n+ provider registry.terraform.io\/hashicorp\/tfe v0.45.0\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.2. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\n```\n\n\n### Use Cases\n\nSome providers take complicate json policy documents as inputs. Terraform will try to detect if strings are JSON, and if so, use a use a JSON diffing algorithm. Often, this works great, however sometimes relatively small changes in the JSON can result in massive diffs. I haven't (yet) been able to determine exactly what scenarios cause this, but I recently ran into a case where a dozen lines of changes, all at the leaf level of deeply nested object, out of about a thousand lines of json resulted in basically everything in the document being marked as replaced, and added.\r\n\r\nThis makes reviewing the diffs for such changes very difficult.\n\n### Attempted Solutions\n\nIt is possible to work around this by getting the json output of a plan (for example using `terraform show -json` or the API for terraform cloud or enterprise), extracting the before and after values (using jq or similar), and then using another json diffing tool (for example https:\/\/www.npmjs.com\/package\/json-diff) to obtain a more useful diff.\n\n### Proposal\n\nImprove the json diffing formatter to show more useful diffs for changes to complicated JSON  documents. \r\n\r\nPossibly make use of an existing go library for json diffs.\n\n### References\n\n_No response_","comments":["Thanks, sounds sensible. Hopefully we'll find an example of this sooner or later so we can implement an appropriate fix."],"labels":["enhancement","plan-renderer"]},{"title":"Procedural resource discovery and management","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.1\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/http v3.4.0\r\n+ provider registry.terraform.io\/newrelic\/newrelic v3.25.0\r\n+ provider registry.terraform.io\/opsgenie\/opsgenie v0.6.26\r\n\r\nYour version of Terraform is out of date! The latest version\r\nis 1.5.2. You can update by downloading from https:\/\/www.terraform.io\/downloads.html\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nTo understand the use cases, it's important to understand a little about the implementation here: I have a couple of applications that procedurally generate *.auto.tfvars.json files by querying our CMDB prior to terraform being run in CI pipelines.\r\n\r\n# Procedurally claim and manage resources\r\nResources can be imported into CMDBs with discovery and other methods, these resources will already exist. In this use case, we not only discover these resources, but, where applicable, manage them too.\r\n\r\n# Build onto existing solutions at bulk\r\nThis use case fits best into those occasions where terraform is being used to manage CI's rather than traditional resources, such as Teams in OpsGenie or sub-accounts in New Relic. For the purposes of enabling people within an organisation to self-service, we want to allow a senior members of staff to create a team or service in OpsGenie, or a sub account in New Relic - safe in the knowledge that the new resource will be picked up by terraform and have everything set up for them such as integration between New Relic and OpsGenie, Service Incident rules in OpsGenie, workloads for each of their services in New Relic and so on.\r\n\r\n### Attempted Solutions\r\n\r\n> For simplicity, I'll just include Services in OpsGenie as the same principle applies to everything else.  \r\n# Step-by-step\r\n## 1. Pre execution\r\nBefore terraform is run, another application is called that queries our CMDB, cleans up the result and writes the output to *.auto.tfvars.json files in the root of the terraform project.\r\nExample:\r\n```\r\n{\r\n  \"services\": [\r\n    {\r\n      \"attributes\": {\r\n        \"Key\": \"SVC-1\",\r\n        \"Name\": \"Test Service 1\",\r\n        \"Created\": \"___\",\r\n        \"Updated\": \"___\",\r\n        \"Description\": \"This is a test service\",\r\n        \"Tier\": \"Tier 3\",\r\n        \"Service_ID\": \"___\",\r\n        \"Revision\": \"___\",\r\n        \"Service_Owners\": {\r\n          \"opsgenieTeam\": {\r\n            \"id\": \"___\",\r\n            \"name\": \"___\"\r\n          }\r\n        }\r\n      },\r\n      \"id\": \"1\",\r\n      \"label\": \"Test Service 1\",\r\n      \"name\": \"Test Service 1\",\r\n      \"objectKey\": \"SVC-1\",\r\n      \"objectTypeId\": \"1\",\r\n      \"objectTypeName\": \"Service\",\r\n      \"workspaceId\": \"___\"\r\n    }\r\n  ]\r\n}\r\n```\r\n## 2. The variable structure\r\nThis response is interpreted by Terraform as a variable like so:\r\n```\r\nvariable \"services\" {\r\n  description = \"The Services to create.\"\r\n  type = list(\r\n    object(\r\n      {\r\n        attributes = object(\r\n          {\r\n            Key            = string\r\n            Name           = string\r\n            Created        = string\r\n            Updated        = string\r\n            Description    = optional(string)\r\n            Tier           = optional(string)\r\n            Service_ID     = optional(string)\r\n            Revision       = optional(string)\r\n            Service_Owners = optional(\r\n              object(\r\n                {\r\n                  opsgenieTeam = object(\r\n                    {\r\n                      id   = string\r\n                      name = string\r\n                    }\r\n                  )\r\n                }\r\n              )\r\n            )\r\n          }\r\n        )\r\n        id             = string\r\n        label          = string\r\n        name           = string\r\n        objectKey      = string\r\n        objectTypeId   = string\r\n        objectTypeName = string\r\n        workspaceId    = string\r\n      }\r\n    )\r\n  )\r\n}\r\n```\r\n## 3. The root module\r\nThe root module calls the OpsGenie Service submodule for each service object like so:\r\n```\r\n# Create the OpsGenie Services and necessary additional components\r\n# Only include services that have an owner configured to avoid errors.\r\nmodule \"opsgenie_service\" {\r\n  for_each = {\r\n    for service in var.services : service.id => {\r\n      id          = service.attributes.Service_ID\r\n      name        = service.attributes.Name\r\n      description = service.attributes.Description\r\n      team_id     = service.attributes.Service_Owners.opsgenieTeam.id\r\n    } if lookup(service.attributes, \"Service_Owners\", null) != null\r\n  }\r\n\r\n  source = \".\/modules\/opsgenie_service\"\r\n\r\n  id          = each.value.id\r\n  name        = each.value.name\r\n  description = each.value.description\r\n  team_id     = each.value.team_id\r\n}\r\n```\r\n## 4. Import and manage the service object\r\nThe OpsGenie Service sub module _should_ import each service and provision services that are already created like so:\r\n```\r\nimport {\r\n  to = opsgenie_service.this\r\n  id = var.id\r\n}\r\n\r\nresource \"opsgenie_service\" \"this\" {\r\n  name    = var.name\r\n  team_id = var.team_id\r\n}\r\n\r\n# Do more stuff with this service...\r\n```\r\n# Issues encountered\r\n## 1. Can't import to non-root module\r\nAn import block cannot be run as part of non-root module. While this can be worked around by running the import in the root module before calling the submodule. It's messy and it would be better for the import to be contained within the same module that will be managing that resource. Additionally, since import blocks don't support for_each calls this doesn't account for the inherent proceduralism in this implementation, whereas calling an import from a submodule would do.\r\n```\r\n\u2502 Error: Invalid import configuration\r\n\u2502 \r\n\u2502   on modules\/opsgenie_service\/main.tf line 10:\r\n\u2502   10: import {\r\n\u2502 \r\n\u2502 An import block was detected in \"module.opsgenie_service\". Import blocks are only allowed in the root module.\r\n```\r\n## 2. Variables not allowed\r\nVariables are not allowed in import blocks, giving the following error:\r\n```\r\n\u2502 Error: Variables not allowed\r\n\u2502 \r\n\u2502   on modules\/opsgenie_service\/main.tf line 12, in import:\r\n\u2502   12:   id = var.id\r\n\u2502 \r\n\u2502 Variables may not be used here.\r\n```\r\n## 3. Value for import must be known\r\nLikely the result of using a variable to declare the import ID. However, the value here is known as it is a static value given by the json input generated in step 1.\r\n```\r\n\u2502 Error: Unsuitable value type\r\n\u2502 \r\n\u2502   on modules\/opsgenie_service\/main.tf line 12, in import:\r\n\u2502   12:   id = var.id\r\n\u2502 \r\n\u2502 Unsuitable value: value must be known\r\n```\r\n\r\n### Proposal\r\n\r\n# Allow import blocks in sub modules\r\nIn cases where the ID is a known, static value - it should be possible to allow imports to be run in a sub-module. Even sub modules that are called in a for_each loop. Given all the information needed to complete a plan exists. This would be an awesome first step towards enabling proceduralism in terraform runs.\r\n# Allow variables as import IDs\r\nIn cases where the variable is a static and known value, it should be allowed to be used in an import block. Edge cases whereby a variable can be modified between runs - it should be ok to destroy the previously imported resource and replace it with the newly import version. This is how the codebase remains declarative and where the input value changes, it should be treated as a declaration of intent. No different to manually writing out the ID in your codebase. However, allowing variables as IDs for import promotes good coding practices by not including potentially sensitive information in your codebase.\r\n# Pre-execution queries\r\nThis is a potential and hypothetical solution for future discussion. It would be really nice if something similar to a http data block can be marked in such a way that it is not run a second time during the apply stage. Subsequently, getting rid of the need to use external applications to generate json input.\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for the detailed write-up. It's helpful to understand the use case in detail.\r\n\r\nThe issue tracker should contain one issue per issue. We tend to think of an \"issue\" as a problem statement, rather than a proposed solution, but the boundary is often permeable. \r\n\r\nIn this case, \"Allow variables as import IDs\" is covered by https:\/\/github.com\/hashicorp\/terraform\/issues\/33228. I've created https:\/\/github.com\/hashicorp\/terraform\/issues\/33474 for \"Allow import blocks in sub modules\", as this is also an independent feature.\r\n\r\nSubtracting those, what remains is an interesting problem that slightly exceeds the limits of what Terraform currently considers its responsibility. Import blocks are currently the outermost interface that we offer for coupling with other infra management software, since Terraform is not a program that _monitors_, or _picks up from_ a CMDB.\r\n\r\ncc @omarismail\r\n\r\n>It would be really nice if something similar to a http data block can be marked in such a way that it is not run a second time during the apply stage.\r\n\r\nWhat problems are caused by evaluating data sources during the apply stage?\r\n","Hi @kmoe, thanks for getting back! :)\r\n\r\nAgreed, so communication with a CMDB is less of a Terraform specific request and more of a design consideration for Terraforms potential as a component in a system like the one I describe above. At the moment it's not possible to use data sources in Terraform to dynamically provision resources as data sources aren't static since they are executed once during planning and again during execution. I can whip together an example if you thank that would be helpful? The error you would get would say something along the lines of resource identifiers must be static.\r\n\r\nUltimately, this specific use case can be resolved by #33228 and #33474 so long as the dependency remains on an external application generating .auto.tfvars.json files prior to each terraform run.\r\n\r\nAllowing data sources to be marked in such a way that indicates they should only be run during planning and not during execution means that the data retreived can be used to dynamically provision resources and eliminates the need for an external application generating the .auto.tfvars.json files. This combined with the previous two issues you've helpfully separated out, should place Terraform in a good position to acommodate incorporation into a discovery pipeline, without directly identifying it is a core responsibility of the product.","Another way to solve this could potentially be by introducing different variable providers. Such as indicating that a variable is a http variable:\r\n```\r\nvariable \"http\" \"opsgenie_services\" {\r\n  description = \"The OpsGenie services to import and manage\"\r\n  type = list(\r\n    object(\r\n      {\r\n        name = string\r\n        id = string\r\n      }\r\n    )\r\n  )\r\n}\r\n````\r\nIn this approach, variables as we currently know them might be called \"local\" variables. For example:\r\n````\r\nvariable \"local\" \"opsgenie_team\" {\r\n  description = \"The name of the OpsGenie Team that will manage provisioned resources.\"\r\n  type = string\r\n}\r\n````\r\nYour TF Vars file will look a little different too:\r\n````\r\nhttp {\r\n  opsgenie_services = {\r\n    url = \"https:\/\/api.eu.opsgenie.com\/v1\/services\"\r\n    method = \"get\"\r\n    headers = {\r\n      Accept = \"application\/json\"\r\n      Authorization = \"Basic XXXXXXXX...\"\r\n    }\r\n  }\r\n}\r\n\r\nlocal {\r\n  opsgenie_team = \"Some Team\"\r\n}\r\n````\r\n\r\n","Hi @Froazin! As others have said, thanks for sharing these use-cases.\r\n\r\nYou have mentioned a few times in your comments so far a problem of data sources being read twice, with one read during planning and one read during apply.\r\n\r\nThat doesn't match my understanding of Terraform's current behavior. The intended behavior is that Terraform reads a data source during planning if possible, _or_ during apply if necessary. \"If possible\" is covering a few different rules here, but the general idea is that Terraform will wait until the apply phase if it seems like the result of the data source might be changed by other actions proposed in the plan \r\n\r\nI'm not sure how crucial a part this plays in the use-case you are describing, but if you are seeing Terraform read a data source in both the plan and the apply phase and that's causing problems for your use of import then I'd like to learn more about what you have observed; reading the data both during plan and apply sounds like a bug.\r\n","Would it be possible to add a fourth item referenced in the mentioned issue #33537? Specifically, creating a resource if the id in an import doesn't exist.\r\n\r\nAnd finally, a fifth - support for for_each for imports. This would require the second item as well.\r\n\r\nThanks! ","Hi @etaham,\r\n\r\n\"Import if it exists and create it otherwise\" is intentionally not supported and is unlikely to be supported in future because in general it is not safe. We require the operator to explicitly decide between creating and importing because when importing it's the operator's responsible to ensure that each object gets bound to no more than one resource address across all of your Terraform configurations.\r\n\r\nIf you would like to discuss a situation where that safety problem would not apply, please open a new issue to describe your use-case in more detail. Thanks!\r\n\r\n\r\n"],"labels":["enhancement","plannable-import"]},{"title":"Update azurerm.mdx for subscription_id parameter in Azure CLI","body":"Issue> Remote state listing keys requires az default subscription set #29425\r\n\r\nThe subscription_id parameter is used to specify the Azure subscription ID where the storage account is located. When using Azure CLI-based authentication, this parameter can be set to the desired subscription ID, ensuring Terraform uses that subscription instead of the default one set in the Azure CLI context.\r\n\r\nThis would assist users in understanding how to avoid potential issues when Terraform attempts to retrieve keys from the default Azure CLI subscription, which might not have the necessary keys if they are located in a different subscription.\r\n\r\n<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES | UPGRADE NOTES | ENHANCEMENTS | BUG FIXES | EXPERIMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n-  \r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33461) <br\/>All committers have signed the CLA."],"labels":["documentation","backend\/azure"]},{"title":"[Enhancement]: Pass resource address as resource attribute","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non darwin_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.6.1\r\n+ provider registry.terraform.io\/hashicorp\/tfe v0.45.0\n```\n\n\n### Use Cases\n\nTerraform can be a confusing place for many engineers, especially those who have never used Terraform, or who use Terraform only once or twice a year.\r\n\r\nFor the purposes of automating some sorts of deployment processes, it can also be useful to try and provide information to those systems based directly on the configuration. It'd be convenient to be able to configure that directly from Terraform.\n\n### Attempted Solutions\n\nIt's maybe possible to configure such deployment systems out-of-band, but if you have a highly distributed approach to workspace management, and hundreds of workspaces, keeping everything in sync becomes fairly difficult.\n\n### Proposal\n\nA couple options:\r\n\r\n* provide a new value, similar to `path.module`, that provides the address of the current resource - only available from the context of the data source\/resource itself. for example, `self.resource_address`\r\n* provide a function or a common attribute on resources that provides the resource address\r\n\r\nIn my particular case, I'd like to be able to set a tag on an `aws_lambda_function` that corresponds to the resource address that manages the function.\n\n### References\n\n_No response_","comments":["Thanks for this request! ","Hi @skeggse! Indeed, thanks for describing this use-case.\r\n\r\nHistorically we've intentionally avoided exposing Terraform's own tracking addresses as data because they are intended only for Terraform's own namespacing and so from the perspective of the remote system they should be considered an invisible implementation detail, rather than actually part of the desired state.\r\n\r\nBroadly speaking the problem is that this would allow a module's declared desired state to vary based on how it's called, which would then defeat mechanisms like config-driven move (`moved` blocks) which aim to allow reorganizing the structure of a Terraform configuration while not actually changing the effectiveness remote system state in any way.\r\n\r\nPersonally the way I've thought about this sort of problem in the past is that I'd prefer to be able to take an identifier from the remote system and ask _Terraform_ where it is managed, so that the responsibility for maintaining that mapping remains in Terraform's domain rather than also living in the remote system itself. There is not yet a full solution to that in Terraform -- it requires some custom tooling that has special logic for each resource type, which is not really practical -- but it could be possible in principle for Terraform providers to give Terraform more information that would allow it to, for example, take an ID from a remote system and return all of the resource instance addresses that are associated with that ID.\r\n\r\nIf there were a feature like that in Terraform itself, would that be a reasonable alternative way to meet this need for answering the question \"given this thing I can see in the remote system, where is it managed in Terraform?\"\r\n\r\n\r\n\r\n","> avoided exposing Terraform's own tracking addresses as data because they are intended only for Terraform's own namespacing and so from the perspective of the remote system they should be considered an invisible implementation detail, rather than actually part of the desired state\r\n\r\nI have some comments here, but perhaps before I go and misinterpret your comment - can you clarify what kind of data you mean when you say \"own tracking addresses as data?\"\r\n\r\n> If there were a feature like that in Terraform itself, would that be a reasonable alternative way to meet this need for answering the question \"given this thing I can see in the remote system, where is it managed in Terraform?\"\r\n\r\nProvided I can get a single answer from an API in less than a second, sure! If I have to do a bunch of pagination and filtering client-side, that wouldn't work. However, given that Terraform Cloud does not have a good machine-to-machine authorization story (for its API), and AWS _does_, I'd strongly prefer to have that information just available to the context that's already authorized to AWS, and already describing relevant resources.\r\n\r\n> defeat mechanisms like config-driven move (moved blocks) which aim to allow reorganizing the structure of a Terraform configuration while not actually changing the effectiveness remote system state in any way.\r\n\r\nI get that this is perhaps an intent behind `moved` blocks, but I personally have no problem with the movement of a resource resulting in an update to a resource's tags. Tags are enough disjoint from the resource itself that this feels intuitive and reasonable to me."],"labels":["enhancement","new"]},{"title":"Terraform provider version constraint resolution for NOT prerelease","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non windows_amd64\n```\n\n\n### Terraform Configuration Files\n\nTerraform is able to select a pre-release provider for a module even if it's explicitly denied using a version constraint.\r\nThis means that if any one module enforces a pre-release version and other modules use pessimistic or greater than version constraints, after all the constraints are AND'ed together, a pre-release version will be selected.\r\n\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    myprovider= {\r\n      source  = \"mydomain.com\/namespace\/myprovider\"\r\n      version = \"~>2.0, !=2.0.1-alpha1, !=2.0.1-alpha2\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\noutput\r\n```\n\n### Debug Output\n\n```\r\n$ terraform init -upgrade\r\n\r\nInitializing the backend...\r\nUpgrading modules...\r\n- my-module in ..\\..\\modules\\terraform-myprovider-my-module\r\n\r\nInitializing provider plugins...\r\n- Finding mydomain.com\/namespace\/myprovider versions matching \"\"~> 2.0, !=2.0.1-alpha1, != 2.0.1-alpha2, 2.0.1-alpha2\"...\r\n- Installing mydomain.com\/namespace\/myprovider v2.0.1-alpha2...\r\n- Installed mydomain.com\/namespace\/myprovider v2.0.1-alpha2 (unauthenticated)\r\n\r\nTerraform has created a lock file .terraform.lock.hcl to record the provider\r\nselections it made above. Include this file in your version control repository\r\nso that Terraform can guarantee to make the same selections by default when\r\nyou run \"terraform init\" in the future.\r\n```\n\n### Expected Behavior\n\nWe expected that we should be able to deny a specific pre-release version of a provider in the provider version constraint in our modules. In the case where we combine a module that explicitly requires a pre-release provider and another module that explicitly denies this provider, an error is appropriate and expected.\r\n\r\nThe documentation is ambiguous in this regard as the != \"not equals\" operator is an \"exact\" operator but is not listed here:\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/version-constraints\r\n- A prerelease version is a version number that contains a suffix introduced by a dash, like 1.2.0-beta. A prerelease version can be selected only by an exact version constraint (the = operator or no operator). Prerelease versions do not match inexact operators such as >=, ~>, etc.\n\n### Actual Behavior\n\nTerraform selects the pre-release provider despite the constraint to the contrary.\n\n### Steps to Reproduce\n\n1. Have a pre-release provider version available (in this case 2.0.1-alpha1)\r\n2. Define a provider constraint like:\r\n```\r\nversion = \"2.0.1-alpha1, !=2.0.1-alpha1\"\r\n```\r\n3. Expect an error as constraint not resolvable.\r\n\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for reporting this @jskirde.\r\n\r\nThere is logic in Terraform's version resolution to ignore prerelease versions unless they are named exactly, so writing just `~> 2.0` without the exclusions should prevent selecting these two alpha releases.\r\n\r\nI wonder then if the logic for that special rule isn't taking into account the negation. It works by searching the version constraints for any exact version references that are prereleases, and so perhaps it's incorrectly classifying these negated terms as \"exact\", since they do specify an exact version even though it's negated.\r\n\r\nThis version handle belongs to an upstream library rather than to Terraform itself, but I happen to be the maintainer of that library so I'll be able to arrange for a fix to be released so far Terraform can use it, if the problem does turn out to be there.\r\n","I tried quickly adding a test case to the upstream [apparentlymart\/go-versions](https:\/\/pkg.go.dev\/github.com\/apparentlymart\/go-versions) that resembles this situation, with both a lower bound and a negation:\r\n\r\n```go\r\n\t\t{\r\n\t\t\tMustMakeSet(MeetingConstraintsStringRuby(\">= 2.0.0, != 2.0.0-beta1, != 2.0.0-beta2\")),\r\n\t\t\tMustParseVersion(\"2.0.0-beta2\"),\r\n\t\t\tfalse,\r\n\t\t},\r\n```\r\n\r\nThis table-based test asks whether the version given in the second field is matched by the constraints given in the first field. In this case I would expect the answer to be `false` both because the `2.0.0-beta2` is a prerelease and should therefore be excluded unless explicitly selected _and_ because it's been explicitly excluded.\r\n\r\nThis test passed, so unfortunately I think that disproves my theory that this might just be a flaw in the logic for deciding whether a particular prerelease version is \"explicitly selected\". Something else seems to be going on here.\r\n","Looking back again at what you reported, I see this in the `terraform init` output:\r\n\r\n```\r\nFinding mydomain.com\/namespace\/myprovider versions matching \"~> 2.0, !=2.0.1-alpha1, != 2.0.1-alpha2, 2.0.1-alpha2\"...\r\n```\r\n\r\nIn particular, notice that this final merged constraint contains both `!= 2.0.1-alpha2` _and_ `2.0.1-alpha2`. I assume that the extra `2.0.1-alpha2` clause has come from another module in your configuration, not included in the question.\r\n\r\nAs you noted at the end, Terraform should've treated this as a contradiction and either matched nothing at all (because a version can't be both 2.0.1-alpha2 and _not_ 2.0.1-alpha2 at the same time) or returned an error during version constraint parsing saying that there's a contradiction.\r\n\r\nIt seems like the bug is that the positive `2.0.1-alpha2` constraint effectively superseded the negative `!= 2.0.1-alpha2`. I'll try a test case for _that_ and see where that leads.\r\n","Okay! I was able to reproduce this variant in the upstream test suite:\r\n\r\n```go\r\n\t\t{\r\n\t\t\tMustMakeSet(MeetingConstraintsStringRuby(\">= 2.0.0, != 2.0.0-beta1, != 2.0.0-beta2, 2.0.0-beta2\")),\r\n\t\t\tMustParseVersion(\"2.0.0-beta2\"),\r\n\t\t\tfalse,\r\n\t\t},\r\n```\r\n\r\nThis test fails because the version set membership test returns `true` instead of `false`, claiming that `2.0.0-beta2` is included by these version constraints.\r\n\r\n","It seems like the minimum reproduction case here is:\r\n\r\n* A positive and negative constraint for the same exact version, thereby creating a contradiction.\r\n* The specified version _must_ be a prerelease.\r\n\r\nIf either of the above don't hold then the test passes. Here's the revised test case:\r\n\r\n```go\r\n\t\t{\r\n\t\t\tMustMakeSet(MeetingConstraintsStringRuby(\"!= 2.0.0-beta1, 2.0.0-beta1\")),\r\n\t\t\tMustParseVersion(\"2.0.0-beta1\"),\r\n\t\t\tfalse, \/\/ the constraint is contradictory, so includes nothing\r\n\t\t},\r\n```\r\n\r\n```\r\n--- FAIL: TestSetHas (0.00s)\r\n    --- FAIL: TestSetHas\/versions.Union(versions.Only(versions.MustParseVersion(\"2.0.0-beta1\")),_versions.Intersection(versions.Released,_(versions.All).Subtract(versions.Only(versions.MustParseVersion(\"2.0.0-beta1\"))),_versions.Only(versions.MustParseVersion(\"2.0.0-beta1\")))) (0.00s)\r\n        set_test.go:348: wrong result\r\n            set:     versions.Union(versions.Only(versions.MustParseVersion(\"2.0.0-beta1\")), versions.Intersection(versions.Released, (versions.All).Subtract(versions.Only(versions.MustParseVersion(\"2.0.0-beta1\"))), versions.Only(versions.MustParseVersion(\"2.0.0-beta1\"))))\r\n            version: versions.MustParseVersion(\"2.0.0-beta1\")\r\n            got:     true\r\n            want:    false\r\n```\r\n\r\nFor readability, here's a more concise view of how the library is interpreting this version constraint:\r\n\r\n```\r\nunion(\r\n  2.0.0-beta1,\r\n  intersection(\r\n    non-prerelease versions,\r\n    all possible versions - 2.0.0-beta1,\r\n  )\r\n)\r\n```\r\n\r\nThat interpretation seems flawed: the outer `union` combines a set containing only 2.0.0-beta1 with a set containing everything _except_ that version, thereby effectively reproducing a set with all versions allowed again. This particular combination of a negation and a prerelease seems to require special handling beyond just set operations, so that the negation can overrule the outermost union.\r\n\r\nWith that said, I am also kinda curious as to why it chose \"union\" here since I would have intuitively expected these version sets to be combined with \"intersection\", since the comma operator in this syntax means \"AND\", not \"OR\". Perhaps the root cause is there.\r\n\r\n","Further poking confirms that it does seem to be the special case for prerelease versions that's somehow causing the problem, because there's already a test in the parsing code for this trivial contradiction case:\r\n\r\n```\r\n\t\t{\r\n\t\t\t`1.0.0, != 1.0.0`, \/\/ degenerate empty set\r\n\t\t\tIntersection(\r\n\t\t\t\tReleased,\r\n\t\t\t\tOnly(MustParseVersion(`1.0.0`)),\r\n\t\t\t\tAll.Subtract(Only(MustParseVersion(`1.0.0`))),\r\n\t\t\t),\r\n\t\t},\r\n```\r\n\r\nThis behaves as I would've expected, producing an empty set by applying intersection to both \"1.0.0\" and \"not 1.0.0\", which have no versions in common.\r\n\r\nThe equivalent of this with prereleases makes a similar odd result as in the previous comment:\r\n\r\n```go\r\n`1.0.0-beta1, != 1.0.0-beta1`\r\n\r\nversions.Union(\r\n    versions.Only(versions.MustParseVersion(\"1.0.0-beta1\")),\r\n    versions.Intersection(\r\n        versions.Released,\r\n        versions.Only(versions.MustParseVersion(\"1.0.0-beta1\")),\r\n        (versions.All).Subtract(versions.Only(versions.MustParseVersion(\"1.0.0-beta1\"))),\r\n    )\r\n)\r\n```\r\n\r\nSo it seems like the logic that tries to carve out the exception for allowing prereleases while lowering the constraint into its constituent operators is where the problem lies here.\r\n\r\nSince this bug is with what is arguably an edge case (it's rare for version constraints to contain negations at all, let alone ones that are prereleases _and_ are contradicted) and the correct behavior here would've been a failure anyway (so this isn't blocking any valid configuration from working) I'm going to leave this context here for now in the hope of addressing this in future, but I unfortunately have to divert to other planned work for today.\r\n\r\nThanks for reporting this, and sorry for the incorrect behavior.\r\n"],"labels":["bug","upstream","confirmed","v1.5"]},{"title":"`merge()` in for expression ignores optional attributes from object variable","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.2\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n# foo.auto.tfvars\r\nsetting_one = \"hello\"\r\nsetting_two = \"world\"\r\n\r\nconfiguration = {\r\n    type_one = {\r\n        mandatory_prop = \"foo\"\r\n    }\r\n    type_two = {\r\n        mandatory_prop = \"bar\"\r\n    }\r\n    type_three = {\r\n        mandatory_prop = \"baz\"\r\n    }\r\n}\r\n```\r\n\r\n```terraform\r\n# main.tf\r\nvariable \"setting_one\" {}\r\nvariable \"setting_two\" {}\r\n\r\nvariable \"configuration\" {\r\n    type = map(object({\r\n        mandatory_prop = string\r\n        opt_one = optional(string)\r\n        opt_two = optional(string)\r\n    }))\r\n}\r\n\r\nlocals {\r\n    # Exact same value than .tfvars file\r\n    configuration = {\r\n        type_one = {\r\n            mandatory_prop = \"foo\"\r\n        }\r\n        type_two = {\r\n            mandatory_prop = \"bar\"\r\n        }\r\n        type_three = {\r\n            mandatory_prop = \"baz\"\r\n        }\r\n    }\r\n}\r\n\r\noutput \"from_local\" {\r\n    value = { for k, v in local.configuration : k => merge({\r\n        opt_one = var.setting_one\r\n        opt_two = var.setting_two\r\n    }, v)}\r\n}\r\n\r\noutput \"from_var\" {\r\n    value = { for k, v in var.configuration : k => merge({\r\n        opt_one = var.setting_one\r\n        opt_two = var.setting_two\r\n    }, v)}\r\n}\r\n\r\n```\r\n\r\n### Debug Output\r\n\r\nN\/A\r\n\r\n### Expected Behavior\r\n\r\nBoth outputs (`from_var` and `from_local`) should be identical:\r\n\r\n```\r\n{\r\n  \"type_one\" = {\r\n    \"mandatory_prop\" = \"foo\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n  \"type_three\" = {\r\n    \"mandatory_prop\" = \"baz\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n  \"type_two\" = {\r\n    \"mandatory_prop\" = \"bar\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n}\r\n```\r\n\r\n### Actual Behavior\r\n\r\nThe `from_var` output doesn't set any of the optional properties, defaulting to `tostring(null)`:\r\n\r\n```\r\nOutputs:\r\n\r\nfrom_local = {\r\n  \"type_one\" = {\r\n    \"mandatory_prop\" = \"foo\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n  \"type_three\" = {\r\n    \"mandatory_prop\" = \"baz\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n  \"type_two\" = {\r\n    \"mandatory_prop\" = \"bar\"\r\n    \"opt_one\" = \"hello\"\r\n    \"opt_two\" = \"world\"\r\n  }\r\n}\r\nfrom_var = {\r\n  \"type_one\" = {\r\n    \"mandatory_prop\" = \"foo\"\r\n    \"opt_one\" = tostring(null)\r\n    \"opt_two\" = tostring(null)\r\n  }\r\n  \"type_three\" = {\r\n    \"mandatory_prop\" = \"baz\"\r\n    \"opt_one\" = tostring(null)\r\n    \"opt_two\" = tostring(null)\r\n  }\r\n  \"type_two\" = {\r\n    \"mandatory_prop\" = \"bar\"\r\n    \"opt_one\" = tostring(null)\r\n    \"opt_two\" = tostring(null)\r\n  }\r\n}\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n`terraform init`\r\n`terraform apply`\r\n\r\n### Additional Context\r\n\r\nThis doesn't seem to happen outside of a for expression:\r\n\r\n```terraform\r\nvariable \"min\" {\r\n    type = object({\r\n        mandatory = string\r\n        opt = optional(string)\r\n    })\r\n\r\n    default = {\r\n        mandatory = \"bar\"\r\n    }\r\n}\r\n```\r\n\r\n`terraform console <<< 'merge(var.min, {opt = \"foo\"})'` works as expected, returning:\r\n\r\n```\r\n{\r\n  \"mandatory\" = \"bar\"\r\n  \"opt\" = \"foo\"\r\n}\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @dsiguero,\r\n\r\nI think the difference between your two cases here is the argument order. In your first example with a `for` expression you passed the variable value in the second argument, and so the `null` values for the unassigned attributes overrode the attributes of the same name in the first object. In the second example you wrote the variable first and then the literal object, so your fixed `opt` overrode the `null`.\r\n\r\nI think what you've observed here is that `merge` is considering whether the attributes are present, regardless of whether they are `null`. That's because `merge` was originally built for maps and so its support for merging objects takes a \"map-like\" viewpoint where it's the presence of the keys that matter. But in an object value all of the attributes specified in the type must always be present (or else the value wouldn't be of the given type) so when trying to substitute default values dynamically you would need to use a different strategy that checks for null values rather than for whether the attribute is present at all.\r\n\r\nOne way to do this would be using the `coalesce` function which takes one or more arguments and returns the first one that isn't null.\r\n\r\nThe `merge` function is working as designed here, and cannot change to behave how you wanted because that would be a breaking change. For that reason I'm going to change this into an enhancement request for a convenient way to \"merge\" two or more objects of the same type in a way that uses `null` as the representation of \"not set\".\r\n\r\nThis requirement is essentially what the `defaults` function that was experimental in earlier versions of Terraform did, but that exact design was too confusing (measured by how many people had to ask for help to understand how to use it) and so it didn't graduate to being stable. I think this issue represents the possibility of finding a new variant of that which is clearer how to use correctly.\r\n\r\nIn the meantime I recommend using `coalesce`, but that does mean you will need to handle the default for each attribute separately because it's the attributes that are null, not the object itself.\r\n\r\nThanks for sharing this feedback!"],"labels":["enhancement","new"]},{"title":"\"create_before_destroy = false\" could return error or warning if the resource is forced to be create_before_destroy due to its dependencies","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.2\r\non darwin_amd64\r\n+ provider registry.terraform.io\/hashicorp\/archive v2.4.0\r\n+ provider registry.terraform.io\/hashicorp\/google v4.69.1\r\n+ provider registry.terraform.io\/hashicorp\/google-beta v4.69.1\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```\r\nresource \"google_storage_bucket\" \"gcf_source_bucket\" {\r\n  name     = \"moloco-rmp-data-cloud-function-source\"\r\n  location = \"US\"\r\n  uniform_bucket_level_access = true\r\n}\r\n\r\nlocals {\r\n  backends = {\r\n    gcs = \"http_to_gcs\"\r\n  }\r\n}\r\n\r\ndata \"archive_file\" \"zipped_sources\" {\r\n  for_each = local.backends\r\n\r\n  type        = \"zip\"\r\n  source_dir  = \"${path.module}\/cloud_function_source\/${each.value}\"\r\n  output_path = \"${path.module}\/cloud_function_source\/zipped\/${each.value}.zip\"\r\n}\r\n\r\nresource \"google_storage_bucket_object\" \"function_sources\" {\r\n  for_each = local.backends\r\n\r\n  name   = \"user-data-deletion-service-${each.key}.zip\"\r\n  bucket = google_storage_bucket.gcf_source_bucket.name\r\n  source = \"${path.module}\/cloud_function_source\/zipped\/${each.value}.zip\"\r\n\r\n  depends_on = [data.archive_file.zipped_sources]\r\n  lifecycle {\r\n    create_before_destroy = false\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nN\/A (no error is present)\r\n\r\n### Expected Behavior\r\n\r\nAfter changes are made to files in `${path.module}\/cloud_function_source\/http_to_gcs\/*`, and `terraform apply` is run, there should be a GCS object present in the GCS bucket, which is the archived version of the most up-to-date files in `${path.module}\/cloud_function_source\/http_to_gcs\/`.\r\n\r\n### Actual Behavior\r\n\r\nThe GCS bucket is empty.\r\n\r\nFrom the log of `terraform apply`, it looks like terraform creates a new resource first, and then destroys the old one. Since the name of the resources are the same, after `terraform apply` completes, no object is present in the GCS bucket.\r\n\r\nDeleting before creating should be the default behavior of terraform, rather than creating before deleting happening here. However even if I explicitly set `create_before_destroy = false`, the same still happens.\r\n\r\nOutput of `terraform apply`:\r\n```Terraform will perform the following actions:\r\n\r\n  # module.user_data_deletion_service.module.backend.google_storage_bucket_object.function_sources[\"gcs\"] must be replaced\r\n+\/- resource \"google_storage_bucket_object\" \"function_sources\" {\r\n      ~ content_type     = \"application\/zip\" -> (known after apply)\r\n      ~ crc32c           = \"ihU0xA==\" -> (known after apply)\r\n      ~ detect_md5hash   = \"zlDoiSZOwbi+f5UiDcy1qw==\" -> \"different hash\" # forces replacement\r\n      - event_based_hold = false -> null\r\n      ~ id               = \"moloco-rmp-data-cloud-function-source-user-data-deletion-service-gcs.zip\" -> (known after apply)\r\n      + kms_key_name     = (known after apply)\r\n      ~ md5hash          = \"zlDoiSZOwbi+f5UiDcy1qw==\" -> (known after apply)\r\n      ~ media_link       = \"https:\/\/storage.googleapis.com\/download\/storage\/v1\/b\/moloco-rmp-data-cloud-function-source\/o\/user-data-deletion-service-gcs.zip?generation=1688072204957963&alt=media\" -> (known after apply)\r\n      - metadata         = {} -> null\r\n        name             = \"user-data-deletion-service-gcs.zip\"\r\n      ~ output_name      = \"user-data-deletion-service-gcs.zip\" -> (known after apply)\r\n      ~ self_link        = \"https:\/\/www.googleapis.com\/storage\/v1\/b\/moloco-rmp-data-cloud-function-source\/o\/user-data-deletion-service-gcs.zip\" -> (known after apply)\r\n      ~ storage_class    = \"STANDARD\" -> (known after apply)\r\n      - temporary_hold   = false -> null\r\n        # (2 unchanged attributes hidden)\r\n    }\r\n\r\nPlan: 1 to add, 0 to change, 1 to destroy.\r\n\r\nDo you want to perform these actions in workspace \"prod\"?\r\n  Terraform will perform the actions described above.\r\n  Only 'yes' will be accepted to approve.\r\n\r\n  Enter a value: yes\r\n\r\nmodule.user_data_deletion_service.module.backend.google_storage_bucket_object.function_sources[\"gcs\"]: Creating...\r\nmodule.user_data_deletion_service.module.backend.google_storage_bucket_object.function_sources[\"gcs\"]: Creation complete after 1s [id=moloco-rmp-data-cloud-function-source-user-data-deletion-service-gcs.zip]\r\nmodule.user_data_deletion_service.module.backend.google_storage_bucket_object.function_sources[\"gcs\"] (deposed object 92d626d9): Destroying... [id=moloco-rmp-data-cloud-function-source-user-data-deletion-service-gcs.zip]\r\nmodule.user_data_deletion_service.module.backend.google_storage_bucket_object.function_sources[\"gcs\"]: Destruction complete after 0s\r\n\r\nApply complete! Resources: 1 added, 0 changed, 1 destroyed.\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. `terraform init`\r\n2. `terraform apply` (to create a GCS object)\r\n3. Change something under `${path.module}\/cloud_function_source\/http_to_gcs\/`\r\n4. `terraform apply`\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @Yiranwu,\r\n\r\nDestroy before create _is_ the default behavior, but enabling create before destroy for one resource requires implicitly enabling it for everything else in the same dependency chain because otherwise there can be no correct order to perform all of the operations. I expect the the ordering for this resource is being \"infected\" by a non-default ordering somewhere in what this resource depends on or is depended on by.\r\n\r\nCan you confirm whether there's any `create_before_destroy` somewhere else in the dependency chain for this resource? Thanks!","> Hi @Yiranwu,\r\n> \r\n> Destroy before create _is_ the default behavior, but enabling create before destroy for one resource requires implicitly enabling it for everything else in the same dependency chain because otherwise there can be no correct order to perform all of the operations. I expect the the ordering for this resource is being \"infected\" by a non-default ordering somewhere in what this resource depends on or is depended on by.\r\n> \r\n> Can you confirm whether there's any `create_before_destroy` somewhere else in the dependency chain for this resource? Thanks!\r\n\r\nThe resource being created & deleted here is `google_storage_bucket_object.function_sources`, which only depends on `data.archive_file.zipped_sources`. The `zipped_sources` is constructed from local files, so no further-up dependencies at this point.\r\n\r\nSo, I am not aware of any resources that `google_storage_bucket_object.function_sources` may inherit `create_before_destroy = true` from.","Thanks for confirming!\r\n\r\nAnd just for completeness, is there anything downstream from this resource that depends on the bucket object? The upstream and downstream dependencies can both potentially force using create before destroy.","> Hi @Yiranwu,\r\n> \r\n> Destroy before create _is_ the default behavior, but enabling create before destroy for one resource requires implicitly enabling it for everything else in the same dependency chain because otherwise there can be no correct order to perform all of the operations. I expect the the ordering for this resource is being \"infected\" by a non-default ordering somewhere in what this resource depends on or is depended on by.\r\n> \r\n> Can you confirm whether there's any `create_before_destroy` somewhere else in the dependency chain for this resource? Thanks!\r\n\r\nThanks for reminding! I missed the point that downstream configurations may also play a role.\r\n\r\nYes, there is indeed a downstream resource with `create_before_destroy = true`. That explains the _actual behavior_ of terraform.\r\n\r\nA small suggestion: It may be helpful to throw an error when people try to define resources with `create_before_destroy = false`, while it has to be handled with `create_before_destroy = true` due to constraints in its dependency tree.","Thanks for confirmin, @Yiranwu.\r\n\r\nAnd indeed, making `create_before_destroy = false` produce at least a warning saying that it was effectively ignored was something I was thinking about while reading your issue and I agree that seems like a worthwhile improvement. I think we didn't consider it before just because it's very rare to set `create_before_destroy = false`, but using it as a way to assert that a particular resource _must not_ use `create_before_destroy` (because its configuration doesn't make sense for that sequence of actions) does seem like a reasonable thing to do with it.\r\n\r\nWith that in mind, I'm going to reopen this issue and transform it into an enhancement request for that behavior. Thanks!\r\n"],"labels":["enhancement","core","config"]},{"title":"Extremely flaky applies","body":"### Terraform Version\n\n```shell\nTerraform v1.5.2\r\non darwin_amd64\r\n+ provider registry.terraform.io\/cloudflare\/cloudflare v4.9.0\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.5.0\r\n+ provider registry.terraform.io\/hashicorp\/null v3.2.1\r\n+ provider registry.terraform.io\/hashicorp\/random v3.5.1\n```\n\n\n### Terraform Configuration Files\n\n(not sure what the relevant parts would be)\n\n### Debug Output\n\n```\r\n...\r\n2023-06-29T21:29:21.426+0300 [DEBUG] provider.terraform-provider-aws_v5.5.0_x5: HTTP Response Received: http.response.header.date=\"Thu, 29 Jun 2023 18:29:22 GMT\" http.response.header.x_amz_id_2=2CpuCCr2TA2v6A\/7FDNQYmB6C5KQE8StY2kJYAxoxLkWRiucKuk6DuOvkjTkCCXH9VpH52xJi6M= @module=aws aws.region=eu-west-1 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=0f138c8a-9891-138d-c848-947408734784 aws.sdk=aws-sdk-go aws.service=S3 http.status_code=200 tf_mux_provider=*schema.GRPCProviderServer @caller=github.com\/hashicorp\/aws-sdk-go-base\/v2\/awsv1shim\/v2@v2.0.0-beta.31\/logger.go:144 http.duration=58 http.response.body=\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Vers***************tion xmlns=\"http:\/\/s3.amazonaws.com\/doc\/2006-03-01\/\"><Status>Enabled<\/Status><MfaDelete>Disabled<\/MfaDelete><\/Ver****************tion>\r\n\" http.response.header.x_amz_request_id=3F0XK0DJ7JJB25SS tf_rpc=ReadResource aws.operation=GetBucketVersioning http.response.header.server=AmazonS3 tf_resource_type=aws_s3_bucket_versioning timestamp=2023-06-29T21:29:21.425+0300\r\n2023-06-29T21:29:21.427+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: [TRACE] Waiting 100ms before next try\r\n2023-06-29T21:29:21.530+0300 [DEBUG] provider.terraform-provider-aws_v5.5.0_x5: HTTP Request Sent: @caller=github.com\/hashicorp\/aws-sdk-go-base\/v2\/awsv1shim\/v2@v2.0.0-beta.31\/logger.go:96 http.request.body= http.request.header.authorization=\"AWS4-HMAC-SHA256 Credential=AKIA************M7AQ********9\/eu-west-1\/s3\/aws4_request, Sign**********host;x-amz-content-sha256;x-amz-date, Signature=*****\" http.user_agent=\"APN\/1.0 HashiCorp\/1.0 Terraform\/1.5.2 (+https:\/\/www.terraform.io) terraform-provider-aws\/5.5.0 (+https:\/\/registry.terraform.io\/providers\/hashicorp\/aws) aws-sdk-go\/1.44.286 (go1.20.5; darwin; amd64)\" tf_mux_provider=*schema.GRPCProviderServer tf_resource_type=aws_s3_bucket_versioning @module=aws aws.service=S3 http.flavor=1.1 http.request.header.x_amz_content_sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 http.url=https:\/\/my-infra-terraform-state.s3.eu-west-1.amazonaws.com\/?versioning= tf_req_id=0f138c8a-9891-138d-c848-947408734784 aws.operation=GetBucketVersioning aws.region=eu-west-1 http.request.header.x_amz_date=20230629T182921Z tf_provider_addr=registry.terraform.io\/hashicorp\/aws aws.sdk=aws-sdk-go http.method=GET net.peer.name=my-infra-terraform-state.s3.eu-west-1.amazonaws.com tf_rpc=ReadResource timestamp=2023-06-29T21:29:21.529+0300\r\n2023-06-29T21:29:21.589+0300 [DEBUG] provider.terraform-provider-aws_v5.5.0_x5: HTTP Response Received: tf_rpc=ReadResource @module=aws http.duration=58 http.response.header.server=AmazonS3 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_mux_provider=*schema.GRPCProviderServer tf_resource_type=aws_s3_bucket_versioning aws.region=eu-west-1 http.response.header.date=\"Thu, 29 Jun 2023 18:29:22 GMT\" http.response.header.x_amz_id_2=NNniObbVckHe9GU3ioE75wrlieFRiq0ZsJRCWtP+bXlfc92cX8DCijc2etwt+YjeRcXj+gC9yrQ= http.status_code=200 @caller=github.com\/hashicorp\/aws-sdk-go-base\/v2\/awsv1shim\/v2@v2.0.0-beta.31\/logger.go:144 tf_req_id=0f138c8a-9891-138d-c848-947408734784 aws.operation=GetBucketVersioning aws.sdk=aws-sdk-go aws.service=S3 http.response.body=\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Vers***************tion xmlns=\"http:\/\/s3.amazonaws.com\/doc\/2006-03-01\/\"><Status>Enabled<\/Status><MfaDelete>Disabled<\/MfaDelete><\/Ver****************tion>\r\n\" http.response.header.x_amz_request_id=3F0SZD2TA703P8YB timestamp=2023-06-29T21:29:21.588+0300\r\n2023-06-29T21:29:21.591+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Called downstream: @module=sdk.helper_schema tf_mux_provider=*schema.GRPCProviderServer tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_rpc=ReadResource @caller=github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema\/resource.go:1016 tf_req_id=0f138c8a-9891-138d-c848-947408734784 tf_resource_type=aws_s3_bucket_versioning timestamp=2023-06-29T21:29:21.590+0300\r\n2023-06-29T21:29:21.591+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Received downstream response: tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=0f138c8a-9891-138d-c848-947408734784 @module=sdk.proto tf_proto_version=5.3 diagnostic_error_count=0 diagnostic_warning_count=0 tf_req_duration_ms=1392 tf_resource_type=aws_s3_bucket_versioning tf_rpc=ReadResource @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/internal\/tf5serverlogging\/downstream_request.go:37 timestamp=2023-06-29T21:29:21.590+0300\r\n2023-06-29T21:29:21.591+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Served request: tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=0f138c8a-9891-138d-c848-947408734784 tf_resource_type=aws_s3_bucket_versioning @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/tf5server\/server.go:761 @module=sdk.proto tf_proto_version=5.3 tf_rpc=ReadResource timestamp=2023-06-29T21:29:21.591+0300\r\n2023-06-29T21:29:21.592+0300 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState to refreshState for aws_s3_bucket_versioning.terraform_state\r\n2023-06-29T21:29:21.592+0300 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState: writing state object for aws_s3_bucket_versioning.terraform_state\r\n2023-06-29T21:29:21.595+0300 [TRACE] Re-validating config for \"aws_s3_bucket_versioning.terraform_state\"\r\n2023-06-29T21:29:21.595+0300 [TRACE] GRPCProvider: ValidateResourceConfig\r\n2023-06-29T21:29:21.596+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Received request: @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/tf5server\/server.go:679 tf_proto_version=5.3 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 tf_resource_type=aws_s3_bucket_versioning tf_rpc=ValidateResourceTypeConfig @module=sdk.proto timestamp=2023-06-29T21:29:21.596+0300\r\n2023-06-29T21:29:21.596+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Sending request downstream: @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/internal\/tf5serverlogging\/downstream_request.go:17 tf_proto_version=5.3 tf_rpc=ValidateResourceTypeConfig tf_resource_type=aws_s3_bucket_versioning @module=sdk.proto tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 timestamp=2023-06-29T21:29:21.596+0300\r\n2023-06-29T21:29:21.596+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: calling downstream server: @caller=github.com\/hashicorp\/terraform-plugin-mux@v0.10.0\/internal\/logging\/mux.go:16 @module=sdk.mux tf_mux_provider=*schema.GRPCProviderServer tf_rpc=ValidateResourceTypeConfig timestamp=2023-06-29T21:29:21.596+0300\r\n2023-06-29T21:29:21.596+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Calling downstream: @module=sdk.helper_schema tf_mux_provider=*schema.GRPCProviderServer tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_resource_type=aws_s3_bucket_versioning tf_rpc=ValidateResourceTypeConfig @caller=github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema\/grpc_provider.go:245 tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 timestamp=2023-06-29T21:29:21.596+0300\r\n2023-06-29T21:29:21.598+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Called downstream: @caller=github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema\/grpc_provider.go:247 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 tf_resource_type=aws_s3_bucket_versioning tf_rpc=ValidateResourceTypeConfig @module=sdk.helper_schema tf_mux_provider=*schema.GRPCProviderServer timestamp=2023-06-29T21:29:21.598+0300\r\n2023-06-29T21:29:21.598+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Received downstream response: @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/internal\/tf5serverlogging\/downstream_request.go:37 diagnostic_error_count=0 tf_req_duration_ms=2 tf_rpc=ValidateResourceTypeConfig tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 tf_resource_type=aws_s3_bucket_versioning @module=sdk.proto diagnostic_warning_count=0 tf_proto_version=5.3 tf_provider_addr=registry.terraform.io\/hashicorp\/aws timestamp=2023-06-29T21:29:21.598+0300\r\n2023-06-29T21:29:21.598+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Served request: tf_proto_version=5.3 @module=sdk.proto tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=57df55ba-e37a-fbb3-70d1-be905a0686b8 tf_resource_type=aws_s3_bucket_versioning @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/tf5server\/server.go:699 tf_rpc=ValidateResourceTypeConfig timestamp=2023-06-29T21:29:21.598+0300\r\n2023-06-29T21:29:21.598+0300 [TRACE] GRPCProvider: PlanResourceChange\r\n2023-06-29T21:29:21.599+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Received request: tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/tf5server\/server.go:770 @module=sdk.proto tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 tf_proto_version=5.3 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_resource_type=aws_s3_bucket_versioning timestamp=2023-06-29T21:29:21.598+0300\r\n2023-06-29T21:29:21.599+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Sending request downstream: @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/internal\/tf5serverlogging\/downstream_request.go:17 tf_proto_version=5.3 tf_rpc=PlanResourceChange tf_resource_type=aws_s3_bucket_versioning @module=sdk.proto tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 timestamp=2023-06-29T21:29:21.599+0300\r\n2023-06-29T21:29:21.599+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: calling downstream server: tf_mux_provider=*schema.GRPCProviderServer tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-mux@v0.10.0\/internal\/logging\/mux.go:16 @module=sdk.mux timestamp=2023-06-29T21:29:21.599+0300\r\n2023-06-29T21:29:21.600+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Calling downstream: tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_mux_provider=*schema.GRPCProviderServer tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 tf_resource_type=aws_s3_bucket_versioning tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema\/schema.go:698 @module=sdk.helper_schema timestamp=2023-06-29T21:29:21.600+0300\r\n2023-06-29T21:29:21.600+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Called downstream: @caller=github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema\/schema.go:700 tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 tf_resource_type=aws_s3_bucket_versioning @module=sdk.helper_schema tf_mux_provider=*schema.GRPCProviderServer tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_rpc=PlanResourceChange timestamp=2023-06-29T21:29:21.600+0300\r\n2023-06-29T21:29:21.600+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Received downstream response: @module=sdk.proto diagnostic_error_count=0 diagnostic_warning_count=0 tf_proto_version=5.3 @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/internal\/tf5serverlogging\/downstream_request.go:37 tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_resource_type=aws_s3_bucket_versioning tf_req_duration_ms=1 tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 tf_rpc=PlanResourceChange timestamp=2023-06-29T21:29:21.600+0300\r\n2023-06-29T21:29:21.600+0300 [TRACE] provider.terraform-provider-aws_v5.5.0_x5: Served request: tf_resource_type=aws_s3_bucket_versioning tf_rpc=PlanResourceChange @caller=github.com\/hashicorp\/terraform-plugin-go@v0.15.0\/tfprotov5\/tf5server\/server.go:796 @module=sdk.proto tf_provider_addr=registry.terraform.io\/hashicorp\/aws tf_proto_version=5.3 tf_req_id=7d790fb5-97f5-684c-0ac2-dec41dac8d83 timestamp=2023-06-29T21:29:21.600+0300\r\n2023-06-29T21:29:21.600+0300 [WARN]  Provider \"registry.terraform.io\/hashicorp\/aws\" produced an invalid plan for aws_s3_bucket_versioning.terraform_state, but we are tolerating it because it is using the legacy plugin SDK.\r\n    The following problems may be the cause of any confusing errors from downstream operations:\r\n      - .expected_bucket_owner: planned value cty.StringVal(\"\") for a non-computed attribute\r\n2023-06-29T21:29:21.600+0300 [TRACE] writeChange: recorded NoOp change for aws_s3_bucket_versioning.terraform_state\r\n2023-06-29T21:29:21.600+0300 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState to workingState for aws_s3_bucket_versioning.terraform_state\r\n2023-06-29T21:29:21.600+0300 [TRACE] NodeAbstractResouceInstance.writeResourceInstanceState: writing state object for aws_s3_bucket_versioning.terraform_state\r\n2023-06-29T21:29:21.600+0300 [TRACE] vertex \"aws_s3_bucket_versioning.terraform_state\": visit complete\r\n2023-06-29T21:29:21.600+0300 [TRACE] vertex \"root\": starting visit (terraform.graphNodeRoot)\r\n2023-06-29T21:29:21.601+0300 [TRACE] vertex \"root\": visit complete\r\n2023-06-29T21:29:21.601+0300 [TRACE] vertex \"aws_s3_bucket_versioning.terraform_state (expand)\": dynamic subgraph completed successfully\r\n2023-06-29T21:29:21.601+0300 [TRACE] vertex \"aws_s3_bucket_versioning.terraform_state (expand)\": visit complete\r\n2023-06-29T21:29:21.601+0300 [TRACE] vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\": starting visit (*terraform.graphNodeCloseProvider)\r\n2023-06-29T21:29:21.601+0300 [TRACE] GRPCProvider: Close\r\n2023-06-29T21:29:21.603+0300 [DEBUG] provider.stdio: received EOF, stopping recv loop: err=\"rpc error: code = Unavailable desc = error reading from server: EOF\"\r\n2023-06-29T21:29:21.630+0300 [DEBUG] provider: plugin process exited: path=.terraform\/providers\/registry.terraform.io\/hashicorp\/aws\/5.5.0\/darwin_amd64\/terraform-provider-aws_v5.5.0_x5 pid=36195\r\n2023-06-29T21:29:21.630+0300 [DEBUG] provider: plugin exited\r\n2023-06-29T21:29:21.630+0300 [TRACE] vertex \"provider[\\\"registry.terraform.io\/hashicorp\/aws\\\"] (close)\": visit complete\r\n2023-06-29T21:29:21.631+0300 [TRACE] dag\/walk: upstream of \"root\" errored, so skipping\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/cloudflare\/cloudflare\"\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/aws\"\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/null\"\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/random\"\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provisioner \"remote-exec\"\r\n2023-06-29T21:29:21.634+0300 [TRACE] LoadSchemas: retrieving schema for provisioner \"file\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/cloudflare\/cloudflare\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/aws\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/null\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provider type \"registry.terraform.io\/hashicorp\/random\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provisioner \"remote-exec\"\r\n2023-06-29T21:29:21.635+0300 [TRACE] LoadSchemas: retrieving schema for provisioner \"file\"\r\n\r\nPlanning failed. Terraform encountered an error while generating this plan.\r\n\r\n\u2577\r\n\u2502 Error: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol.\r\n\u2502\r\n...\r\n```\n\n### Expected Behavior\n\nRegular apply.\n\n### Actual Behavior\n\nAbout 1 time in 3, getting one of the errors below.\n\n### Steps to Reproduce\n\n```\r\n$ terraform apply\r\nReleasing state lock. This may take a few moments...\r\n\u2577\r\n\u2502 Error: Failed to load plugin schemas\r\n\u2502\r\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io\/hashicorp\/aws: failed to retrieve schema from provider\r\n\u2502 \"registry.terraform.io\/hashicorp\/aws\": Request cancelled: The plugin.(*GRPCProvider).GetProviderSchema request was cancelled...\r\n\u2575\r\n$ terraform apply\r\nReleasing state lock. This may take a few moments...\r\n\u2577\r\n\u2502 Error: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol.\r\n\u2502\r\n$ terraform apply\r\nReleasing state lock. This may take a few moments...\r\n\u2577\r\n\u2502 Error: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol.\r\n\u2502\r\n```\n\n### Additional Context\n\nHave also tried downgrading the AWS plugin, no change.\n\n### References\n\n_No response_","comments":["Thanks for reporting this @jareware, and sorry it's acting weirdly.\r\n\r\nThis message typically means that a provider failed to even start up correctly, such as if it just printed a human-oriented error message and exited instead of properly completing the plugin startup handshake.\r\n\r\nThe `hashicorp\/aws` provider seems to have run successfully, completed its work, and exited here. Therefore I expect the problem of actually with one of these other providers that were apparently starting up at the end of the output here, but it's unfortunate that this error message doesn't report which provider it's talking about. I think that's because this error message comes from the underlying library Terraform uses to interact with plugins and so that library doesn't know the identity of the plugin; we should make Terraform intercept that error and annotate it with more information about what failed.\r\n\r\nWithout that though I think probably the best we can do is try to reproduce this with one provider at a time. Can you first try running `terraform providers schema` to see if it reproduces the error? If so we can hopefully use that command as a side-effect-free way to try to narrow this down, so that you won't need to keep applying changes in order to reproduce it.\r\n\r\nThanks!","Thank you for a swift response!\r\n\r\nDid not know such a command even existed... \ud83d\ude05 but indeed it does also trips the error:\r\n\r\n```\r\n$ while terraform providers schema -json > terraform-providers-schema.json; do\r\n> echo OK\r\n> sleep 3\r\n> done\r\nOK\r\nOK\r\n\u2577\r\n\u2502 Error: Failed to load plugin schemas\r\n\u2502\r\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io\/hashicorp\/aws: failed to instantiate provider\r\n\u2502 \"registry.terraform.io\/hashicorp\/aws\" to obtain schema: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol...\r\n\u2575\r\n$\r\n```\r\n\r\nDoes the resulting file help you in debugging? Is it OK to share, or contains secrets?\r\n\r\nAlso, this is a quite new project, and even though I installed the AWS & Cloudflare providers in quite quick succession, I'm _almost certain_ this didn't happen before installing the Cloudflare one. \ud83e\udd14\r\n\r\nSo with that, I'll actually try downgrading that one too, at least.","Well well well!\r\n\r\n```terraform\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source = \"hashicorp\/aws\"\r\n    }\r\n    cloudflare = {\r\n      source  = \"cloudflare\/cloudflare\"\r\n      version = \"4.7.0\" # latest is 4.9.0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI was initially sure this was enough, but:\r\n\r\n```\r\n$ while terraform providers schema -json > schema2.json; do echo OK; sleep 3; done\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\nOK\r\n\u2577\r\n\u2502 Error: Failed to load plugin schemas\r\n\u2502\r\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io\/hashicorp\/aws: failed to instantiate provider\r\n\u2502 \"registry.terraform.io\/hashicorp\/aws\" to obtain schema: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol...\r\n\u2575\r\n$\r\n```\r\n\r\nAt least it's... less flaky?","Ninja edit: nope, not the case after all. \ud83d\ude15\r\n\r\n```\r\n$ while terraform providers schema -json > schema2.json; do echo OK; sleep 3; done\r\nOK\r\n\u2577\r\n\u2502 Error: Failed to load plugin schemas\r\n\u2502\r\n\u2502 Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io\/hashicorp\/aws: failed to instantiate provider\r\n\u2502 \"registry.terraform.io\/hashicorp\/aws\" to obtain schema: Unrecognized remote plugin message:\r\n\u2502\r\n\u2502 This usually means that the plugin is either invalid or simply\r\n\u2502 needs to be recompiled to support the latest protocol...\r\n\u2575\r\n$\r\n```","Thanks for trying that @jareware!\r\n\r\nI'm glad to see that this command _does_ intercept the error from the underlying library and annotate it with which provider it was working with at the time, so we can see this disproves my earlier hypothesis that it might be a different provider that was failing to start. It does seem to definitely be the `hashicorp\/aws` provider.\r\n\r\nThe fact that it's intermittent makes me wonder if this is a resource utilization problem. The AWS provider had grown very large due to covering a very large portion of the AWS API surface and so it could potentially run into memory constraints if either your system has insufficient physical memory to load it or you are running Terraform in an environment with a software-imposed memory limit.\r\n\r\nIf the provider startup were hitting a memory limit then it would probably exit with an error or crash and therefore not complete the plugin protocol handshake, leading to this symptom.\r\n\r\nDo you think it's plausible that the provider could be encountering a memory limit while it tries to start up? It might be only intermittently reproducible because the other providers being used at the same time are also contributing to the peak memory usage.\r\n","Hmm, no idea whether that could be the case... not running the Terraform binary with any kind of virtualization, and the physical machine isn't particularly ancient (Apple M1 Pro, 32 GB RAM).\r\n\r\nThat said, maybe I'll try rebooting and running almost nothing else while doing this.","This bit still seems like too big of a coincidence:\r\n\r\n> even though I installed the AWS & Cloudflare providers in quite quick succession, I'm almost certain this didn't happen before installing the Cloudflare one\r\n\r\n...but I don't think I can just remove it either without changing stuff around pretty drastically.","> maybe I'll try rebooting and running almost nothing else while doing this.\r\n\r\nStill flakes out, though possibly slightly less? Now only like, once every 6 runs or so..? \ud83d\ude05","If it helps: got a new kind of error:\r\n\r\n```\r\n$ terraform apply\r\nReleasing state lock. This may take a few moments...\r\n\u2577\r\n\u2502 Error: Request cancelled\r\n\u2502\r\n\u2502   with provider[\"registry.terraform.io\/hashicorp\/aws\"].secondary,\r\n\u2502   on terraform-providers.tf line 26, in provider \"aws\":\r\n\u2502   26: provider \"aws\" {\r\n\u2502\r\n\u2502 The plugin.(*GRPCProvider).GetProviderSchema request was cancelled.\r\n\u2575\r\n```\r\n\r\n...but a subsequent `apply` without any source changes ran normally.\r\n\r\nThis actually got me thinking whether it might have something to do with the fact that I have 2 instances of the AWS provider, a \"primary\" and a \"secondary\" one. I'll see if I'll be able to temporarily rip one of them out.","Just updated my providers, but unfortunately the issue remains just as bad.\r\n\r\nHere's the upgrade info, if it matters:\r\n\r\n```diff\r\ndiff --git a\/.terraform.lock.hcl b\/.terraform.lock.hcl\r\nindex d09abb1..1d2e7d9 100644\r\n--- a\/.terraform.lock.hcl\r\n+++ b\/.terraform.lock.hcl\r\n@@ -2,46 +2,46 @@\r\n # Manual edits may be lost in future updates.\r\n \r\n provider \"registry.terraform.io\/cloudflare\/cloudflare\" {\r\n-  version = \"4.9.0\"\r\n+  version = \"4.11.0\"\r\n   hashes = [\r\n-    \"h1:Sa4hsmj1+5X\/iuakHLW449JEBuHxyXJ6CkAbFT1g8rY=\",\r\n-    \"zh:0675551cf81db9c137b40e3f10020b1a99906d40be0b5db04be0faf8439fc5e8\",\r\n-    \"zh:0df188403a13b3999a102f813d7bba4e87a796b93b00db39ee94f0e811819d41\",\r\n-    \"zh:19b1480989faa856dbdd26d5a384efd5778b569d5ff111a64841513c5f6f94f6\",\r\n-    \"zh:3707be76a250f1d4da78e3a309e542c261dd7ee37c6796182c7d515dab9c7f51\",\r\n-    \"zh:51635170540e66f15f021451ec5fbb76c2e9c611fa9d73f7beb09687e28ad6ed\",\r\n-    \"zh:55b4cf10f45ddd8f7066d05b65c7a8ad2205013b7f4b2d2c037fbdabf66bdb84\",\r\n-    \"zh:66d266277686c2e5a1deb0be32c42eee3e8ea6cf5ff7d78e661a93658c65cee6\",\r\n-    \"zh:720615ff6003ac5c2cadd25e07e5eaa7c15b1e39207bce050fea65dba6658ae7\",\r\n-    \"zh:85ea3ff9fb01fcc26d83e6dbec188f8f1cd7837cd5300c8da80b24d018c69100\",\r\n+    \"h1:PlO6FpJceI6h72OCstvpXZL6+ARDpPEawD+mjEAs3FA=\",\r\n+    \"zh:09d620903d0f191ab7dee88ce75833307a03c7a9f88dfb2c2a58025283b80ff4\",\r\n+    \"zh:0fb59cccc066c867750d633d6dfea8b99e75f5545ae4e7c090be465c6858eb73\",\r\n+    \"zh:16b35bf2b88a629c05aefc6ebdbcc039447ee23a5b32594d844ca83f92ac8507\",\r\n+    \"zh:5cc3f5df54891bb9efab51cca3266c59a82fd7dcc5667aa3451562325002235a\",\r\n+    \"zh:6f384c9ba3e844b41c3de8455a3b91e3e3b32c1fa34b8b1ece4eae36d347c67e\",\r\n+    \"zh:8000b3567ba7a43837bb8ccf7fdbcd03cc30103ec6abed84a40ee1c5b99f933f\",\r\n+    \"zh:8687603e979a5fe82f2a65bc0cfb2a20acce4d871b01f04ffeabb9aa17c079ca\",\r\n+    \"zh:88ed3e07913ad564ae3ae3280c868054d85e37b16db250b9cbdfca0c58f75dce\",\r\n     \"zh:890df766e9b839623b1f0437355032a3c006226a6c200cd911e15ee1a9014e9f\",\r\n-    \"zh:b331e1d5df54a31c908d40f3a8f9f616a846c6bc836219dbede49c254fc59e02\",\r\n-    \"zh:b714eeacb26dc349292205aa19eff4f983d7555ae87b0107e8c48445d871665c\",\r\n-    \"zh:b9e5934b502f39f869f9910040998a52c647be48ba199df1a9b8720124fc3d36\",\r\n-    \"zh:bb9162845bf4a8c245fc6bc1e712e38e92fb48412218e080308abaf7bc2d75ed\",\r\n-    \"zh:d2bd9590e2b39bc80e80f9dfc566424c2296ff02f81e6e49d4eb8d67b44a0123\",\r\n+    \"zh:a1faa7112d35aee74eb2b90543570ea56209112c0e2c1c06ad503a9c2464676d\",\r\n+    \"zh:a433640c433f1815ca3cf92927a3764669095b8c668a73363ca9017a0b1d0349\",\r\n+    \"zh:a63b6cf55baaa37cd4bf98bce94b7624bb54efe5abf8b86f24384df7996229f0\",\r\n+    \"zh:a6696b0bdadb17d6f2ef7702b922c4006b21b4125530b0a8ac3bcfce1aafe2d8\",\r\n+    \"zh:b2b3e16aa9c9d10409132fa7f181598bb67a1e5684c54535745ce0e3dcbd5d23\",\r\n+    \"zh:d8c65b2e8a18141bb3ee53c7bf37422ff3679a67733702a631696586666ca885\",\r\n   ]\r\n }\r\n \r\n provider \"registry.terraform.io\/hashicorp\/aws\" {\r\n-  version = \"5.6.2\"\r\n+  version = \"5.11.0\"\r\n   hashes = [\r\n-    \"h1:JLGvLK1qBtizbkC0MrdKpdROlAQJ2JM7ptlCRw1b4\/g=\",\r\n-    \"zh:25322d7e1f0054550357d5a03fe29168cc179421e5dcf44b28c25a99d8d6e4e7\",\r\n-    \"zh:394aa5bff70003e76d1d33ef4fe37c4826918577cf339d35e56ae84d01e86765\",\r\n-    \"zh:485b288bf95b5d3014903e386e8ee2d1182e507f746bc988458b9711c7df7171\",\r\n-    \"zh:48cf69750681337d64df7e402116a6753a40b6702c49fc9232ff6621947d85af\",\r\n-    \"zh:6ab11d052d681b5157e261b9dd9167482acffe2018fffd1204575e9bf6a08522\",\r\n-    \"zh:882f22d0e6c16cd5a5f01a0ae817b1e75e928667d21d986b93a4ee74fa62c067\",\r\n+    \"h1:fe40QyKVicffeFFfepqSFanao0O3ybZMiR7sWUmXNYw=\",\r\n+    \"zh:2913af44f9b584f756e5548d5ddc5a251c6d68a7fcd7c41d1418a800a94ef113\",\r\n+    \"zh:31d2bfa84608b74ff5896f41b09e5927d7c37d18875277a51dcd75a1fea3f909\",\r\n+    \"zh:8538ff18e3b4822178e793f06764efdbb84c62227c1051af7d2409ab7be37bfc\",\r\n+    \"zh:8a9295e623327613fc02a6994e73c61b9d0d195bf6fabdb31ee9fd0e6778f62b\",\r\n     \"zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425\",\r\n-    \"zh:ac3403e3ab5c10869b23626467b919e3f010e7cae6e0acf8515e0cefab0dbff0\",\r\n-    \"zh:b959a425c9be83838895e8626037656bf5db81397ad0078595d3b72fd1b816bc\",\r\n-    \"zh:bf390951f21a5fe6b96b206c5496fda4d8b95823bd00d1c03a4a53dd215d882a\",\r\n-    \"zh:c3534972986cd68a421359f07ab86631ffa8731606936276fce18ec8ae9045f4\",\r\n-    \"zh:d4cf29d67ead2c5feb999c2882e5365bd4d04c115e98fb1639b747b682507fea\",\r\n-    \"zh:dea669eea5bca9b57dae2975ec783d577d58a39eec769d1c9bd7fc4d50f241d0\",\r\n-    \"zh:e7a82063d01eb2be3fd192afbad910150fe8054731db20c1b22c714d9391dbe5\",\r\n-    \"zh:fdbbf96948e96dfed614ea4daa4f1706859122a3f978c42c37db8727cb55c94f\",\r\n+    \"zh:a65877248951eadf0d16a3260e85f6b178645da7f1897bc7bda6f12fdbec8e47\",\r\n+    \"zh:a70772851e2c87cc1e10c35389718a544746adc4acbbed129243c0972c367fc6\",\r\n+    \"zh:b10ca631318f8d1d9a2baa318139bc9e545e51efaf677afece173badce75b44c\",\r\n+    \"zh:ca2a5698c33158549fa084ad601610eae94498cba445458391b507da22355402\",\r\n+    \"zh:cdbfc4d64161561bfbcaee5d9b078077ed986131a1eab32ff30e71be09037eec\",\r\n+    \"zh:ce499f93835bf3d28c13ba98a0a220ff541a827fb400fa931601a375b907b56d\",\r\n+    \"zh:da6af610e66e96280a299071a698568b505c2456bb15c906304d6f39578c72e3\",\r\n+    \"zh:e42714e085126c10d8f29664143f97d771b6cc6887d27cdf6c4007ab12af4646\",\r\n+    \"zh:e86dd0c561c73512acba69f55041adfc04d0467f592f52337a7ac600fbc93680\",\r\n+    \"zh:f5da95bbd44809534c6678e9b1ae0b390331a5619f2ae353c6b88e96ae855cc0\",\r\n   ]\r\n }\r\n```","Also just upgraded Terraform to latest (1.5.2 \u2192 1.5.4), no change. \ud83d\ude14 "],"labels":["bug","new"]},{"title":"Allow providers to plan additional imports","body":"### Terraform Version\n\n```shell\nv1.5.1\n```\n\n\n### Use Cases\n\nhttps:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/issues\/8162\r\n\r\nCurrently the Azure API creates certain dependent resources on it's own using the VM creation API call. This is something terraform currently does not understand. By enhancing plannable imports we could put the os disk that gets created during the VM creation into terraform control and stop configuration drift by setting tags\/size\/whatever properly via tf.\n\n### Attempted Solutions\n\nThe workaround MS came up with is by using the azure api directly to modify the os disk, see https:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/issues\/8162#issuecomment-1612611058\n\n### Proposal\n\nChange plannable imports so that you can use them\r\na) not just in the root module, but also within sub modules\r\nb) have it pick up on resources created during the terraform apply, they do not exist prior to running apply yet.\n\n### References\n\nhttps:\/\/github.com\/hashicorp\/terraform-provider-azurerm\/issues\/8162","comments":["Hi @Klaas-,\r\n\r\nThanks for documenting this use case! One aspect of Terraform's operation which we cannot break here is that Terraform will only ever apply the operations which are planned, so this would all need to be accomplished within the plan, however I don't think that alone would preclude this idea. The providers have the same knowledge during the plan about _what_ is going to be applied as they do during apply, and therefore would be able to account for these implicit resource types at that time as well.\r\n\r\nImporting into sub-modules however is a different affair. Modules are treated as read-only by Terraform (except in some exceptions which are maintained for backward compatibility), so imports into submodules must always be targeting existing configuration. The concept of generating configuration within a sub-module's namespace is an idea which will be addressed separately. Currently the solution would be to generate the configuration in the root module, then the user can refactor the configuration via whatever means necessary.","Hello, I just want to bring to your attention that the VMware Cloud Director has a very similar use case.\r\n\r\n\r\nThe vm resource creates the boot disk, but similar to azure, it is a dependent resource that you cannot manage with VM api calls. So the developer did not add support to the vm resource yet.\r\n\r\nOne solution is to import the boot disk in a vcd_vm_internal_disk resource. Then you can manage storage type or size.\r\n\r\nYou can find discussions here:\r\nhttps:\/\/github.com\/vmware\/terraform-provider-vcd\/issues\/519\r\n"],"labels":["enhancement","plannable-import"]},{"title":"A method for providers to influence import config generation","body":"When generating configuration during the import process, Terraform can only guess what portion of the imported state should be used as configuration. There many cases where optional+computed fields are validated differently depending on whether they originate from configuration or the provider, but Terraform has no way to determine if the value should be included in the config.\r\n\r\nThe most obvious example is where legacy SDK marks all `id` attributes as optional+computed even though they can never be included in the configuration, so Terraform must unilaterally remove them from generated config even though a new provider could technically allow that field.\r\n\r\nIn order to solve this we need a new mechanism for the provider to communicate which fields from the state should be used for configuration. This may be related to the possible addition of an import schema, which could relay more structured information to the provider for import instead of a single import id string.","comments":["this relates to #33427. I was imagining a world where dynamic (optional) import blocks could be auto generated based on provider schemas.","When we discussed this before I remember another option we considered was a provider function that takes an object of the form that we'd normally track as \"current state\" (everything populated, no unknowns) and returns the kind of object we'd normally pass to `ValidateResourceConfig` or as the configuration object in `PlanResourceChange`, except that unknown values are not allowed because the input has no unknowns itself.\r\n\r\nThat would then essentially ask the provider \"what is the canonical way to represent this given state in the configuration?\" and the provider can choose to discard any part of the input as something that needn't be included in the configuration, as long as the result is still valid according to the schema and the provider's own validation rules.\r\n\r\nThis is a design with the provider running arbitrary code when requested, rather than a declarative design based on schema, so it'd give provider developers the most flexibility but it also implies that we can only run this operation in contexts where it's acceptable to start up a provider plugin, and not in contexts which rely only on schema snapshots captured earlier.\r\n\r\nWe'll need to weigh these trade-offs as part of designing a solution here."],"labels":["enhancement","plannable-import"]},{"title":"New lifecycle tag \"block changes\" to prevent changes from happening","body":"### Terraform Version\n\n```shell\nAny, eg 1.5\n```\n\n\n### Use Cases\n\nI am looking at using google workforce federation against Azure AD. In this, a GCP resource IAM policy is created which references a `principalset:\/\/` principal type, which encodes the objectID of an AzureAD Group, which is a UUID.  This basically says \"anyone who is a member of this AzureAD group\" will have the permission I assign to this GCP resource. \r\n\r\nUUIDs are a little cumbersome to work with in code and configuration, especially when working with thousands of them. A yaml or json file full of UUIDs is pretty impenetrable. However I understand why google workforce federation works this way, because azure ad groups can be renamed, so the principalset does need to refer to the objectID rather than the displayName to prevent someone in azure getting permissions they shouldnt have just by renaming an azure group. \r\n\r\nSo I was thinking of using a data \"azuread_group\" resource to lookup the objectID based on name, and then config related to who I want to have permission on this or that gcp resource can reference group names instead, which is much more user-friendly for devs to engineer against. \r\n\r\nThis however suffers from the security issue mentioned above, if I start looking up group names using this datasource, and grabbing the objectID and using that in the principalSet:\/\/ principal string in a GCP resource IAM policy, someone could just rename a group. \r\n\r\nI was thinking in this case though, terraform would notice, because the e.g. \"google_folder_iam_member\" field \"member\" would change (an update) which would either result in a \"replace\" operation or an \"update\" operation. \r\n\r\nIt then struck me, that I would like to have this \"fixed\" so that terraform would block these operations (basically error) if this field was updated. Something like:\r\n\r\n```\r\nlifecycle {\r\n    block_changes = [ member ]\r\n}\r\n```\r\n\r\nIt struck me that this doesn't exist right now, but it would be quite useful. \r\n\r\nI think then we could ensure that the permissions were resilient to someone trying to privilege escalate via group naming, whilst keeping the ability for teams to understand their code better by referring to permissions principals by name and not ID. \r\n\n\n### Attempted Solutions\n\nNone in terraform - I am investigating writing tooling that can help developers automatically translate between names and UUIDs in their code, but this isn't really a great approach\n\n### Proposal\n\n```\r\nlifecycle {\r\n    block_changes = [ member ]\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Thanks for sharing this use-case, @gtmtech!\r\n\r\nIn today's Terraform this is an example of something we'd consider to be a kind of \"policy check\", which are usually enforced like this:\r\n\r\n* Run `terraform plan -out=tfplan` to produce a saved plan file.\r\n* Run `terraform show -json tfplan` to get Terraform's description of the plan in JSON format.\r\n* Use your own software to parse that JSON and implement whatever policy rules you have, reporting an error if anything does not meet policy.\r\n* If the policy check succeeded and the plan looks okay otherwise, run `terraform apply tfplan` to apply it.\r\n\r\nAlthough this approach does require some extra steps, it also allows implementing arbitrary rules about what changes are acceptable without every one needing special support in Terraform itself.\r\n\r\nThere is one existing feature in Terraform that works like you've described: `prevent_destroy` makes planning fail if anything marked with it is planning for destruction. However, we consider that to have been a design error because:\r\n- Some users find it too coarse: they want to have some more nuance in the rule such as _ignoring_ a plan to destroy rather than just rejecting it outright.\r\n- Some users find it too fine: they want to just say \"never allow destroying any instance of type `aws_db_instance`\" rather than having to annotate each one separately.\r\n- Putting policy directly in the configuration of the thing the policy applies to makes it too easy to accidentally disable the policy while making other changes. If it lives outside Terraform then it can have a separate explicit change process, possibly controlled by someone other than who the policy is constraining.\r\n- Putting policy in the configuration makes it the responsibility of the module author rather than the workspace operator. This is okay for the situation where those two are the same person, but it's common for someone to want to impose policy on a module they didn't write but are responsible for the effects of nonetheless.\r\n\r\nAll of those considerations led to the current posture of making policy checks something independent of Terraform itself. That approach allows everyone to tailor to exactly the rules they need, allows both hard failure and extra-approval-required conditions (as long as Terraform is running in automations that can support waiting for that extra approval), and makes the policy independent of what it is constraining. On the other hand, it does require some additional effort on the part of the person setting up the automation around Terraform; Terraform Cloud has this built in, but other automation methods may not.\r\n\r\nWith all of that said, I suspect that this feature would end up in the same regret bucket as `prevent_destroy` if it were to be implemented as a built-in, for many of the same reasons. Therefore my instinct is to ask you to implement this as a policy rule like I described above, but I'd be interested to hear if that seems infeasible for reasons I've not considered yet.\r\n\r\nThanks again!\r\n\r\n\r\n","You can also create this type of policy check based on configuration using `check` blocks, and more specifically a [data source within a `check` block](https:\/\/developer.hashicorp.com\/terraform\/tutorials\/configuration-language\/checks#use-a-data-source-within-a-check). Using a data source to read the current state of the resource will allow you compare it against the desired value in the config and assert they are equal.\r\n"],"labels":["enhancement","new"]},{"title":"auto generate optional import blocks for a module","body":"### Terraform Version\n\n```shell\nTerraform 1.5.1\n```\n\n\n### Use Cases\n\n1. Support optional\/dynamic import blocks that can use variables.\r\n2. Auto generate import blocks for all resources in a terraform module\n\n### Attempted Solutions\n\nWriting import blocks for a module on the consumer end and manually tracking down the correct resource names and import values.\n\n### Proposal\n\nconfig-driven imports is great, the native use with traditional cicd via plan and apply is  super powerful!\r\n\r\nTo make it more robust and support module development that takes full advantage of this feature import blocks should be dynamic and ideally auto generated.\r\n\r\nIt would be awesome if imports could be dynamic like other terraform blocks:\r\n\r\n```terraform\r\ndynamic \u201cimport\u201d {\r\n  for_each = local.imports\r\n  contents {\r\n    id    = import.value.id\r\n    key = import.value.key\r\n  }\r\n}\r\n```\r\n\r\nit would also be great if the import block(s) could be auto generated for resources in a module via cli, something like:\r\n`terraform generate imports`\r\n\r\ncould generate import blocks for resources in a module with all the required import values using the correct resource names\/paths. \r\n\r\n\n\n### References\n\n_No response_","comments":["Thanks for this request!","That feature would be super interesting\r\n","Thanks!\r\nI assume this request is only for existing infra. Isn't it?\r\nI hope we'll see that in the next few versions","This is actually for static use in modules. The two use cases happen at different levels of abstraction, but are complementary, and together, provide auto generated optional import statements in terraform modules.\n\nFor example:\n\n1. As a terraform module developer, I want the ability to include optional import statements in my module so that users can provide variables to trigger imports with my module.\n\nThis would mean a module developer could include the import blocks like [`dynamic blocks`](https:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/dynamic-blocks) providing optional imports that abstract resource names (`id`) and import syntax, enabling users to easily import with whatever `key` is needed. For resources that have a calculable `key` before apply, this means the import could be flagged on with a bool and the module developer could use the already provided inputs to derive the key without a user needing to understand any extra implementation details, WYSIWYG.\n\n2. As a terraform module developer, I would like to auto generate scaffolding for dynamic import blocks, so that I can easily provide this to end users without needing to go look at what the specific resource's `key` is.\n\nThe attributes required to achieve an import are defined at the provider level. Import blocks have a very common shape to them. \n\nTo accelerate development of modules and provide a further enhanced developer experience, developers should be able to run a scaffolding command that generates the import statements with the known `id` (since resource names are statically typed + optional key indices) along with placeholders and\/or comments that guide developers on what the `key` is--or derive keys from the module automatically if known before apply.\n\n---\nThis provides a more robust terraform module that can leverage the newest features of terraform to the fullest. And provides an even better user experience to module developers and their users. \n"],"labels":["enhancement","new","plannable-import"]},{"title":"`terraform state rm` does not respect local state backend's `workspace_dir` when creating state backup","body":"### Terraform Version\n\n```shell\nTerraform v1.5.1\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\n```\r\n\r\n```\n\n### Debug Output\n\n-\n\n### Expected Behavior\n\nWhen using local state with custom `workspace_dir`, `terraform state rm` stores the state backup into the correct `workspace_dir`\n\n### Actual Behavior\n\n`terraform state rm` tries and fails to save state backup into the default directory which does not exist.\r\n\r\n```\r\n2023-06-23T23:38:39.169Z [TRACE] statemgr.Filesystem: creating backup snapshot at terraform.tfstate.d\/workspace-a\/terraform.tfstate.1687563517.backup\r\nError saving the state: failed to create local state backup file: open terraform.tfstate.d\/workspace-a\/terraform.tfstate.1687563517.backup: no such file or directory\r\n\r\nThe state was not saved. No items were removed from the persisted\r\nstate. No backup was created since no modification occurred. Please\r\nresolve the issue above and try again.\r\n```\r\n\r\n`terraform.tfstate.d` does not exist because I have set `workspace_dir` to `.terraform-qa.tfstate.d`, so that is where state files reside.\r\n\r\nI can confirm this by looking at `.terraform-qa\/terraform.tfstate` (I am also using a custom `TF_DATA_DIR` pointing to `.terraform-qa`)\r\n\r\n```\r\n{\r\n    \"version\": 3,\r\n    \"serial\": 1,\r\n    \"lineage\": \"336bd162-9070-27cc-079c-6884d4b3246f\",\r\n    \"backend\": {\r\n        \"type\": \"local\",\r\n        \"config\": {\r\n            \"path\": null,\r\n            \"workspace_dir\": \".\/terraform-qa.tfstate.d\"\r\n        },\r\n        \"hash\": 666019178\r\n    },\r\n    \"modules\": [\r\n        {\r\n            \"path\": [\r\n                \"root\"\r\n            ],\r\n            \"outputs\": {},\r\n            \"resources\": {},\r\n            \"depends_on\": []\r\n        }\r\n    ]\r\n}\r\n```\n\n### Steps to Reproduce\n\n`terraform init` w\/local state backend, custom backend config `workspace_dir`\r\nAttempt to `terraform state rm` something from the state.\r\n\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the \ud83d\udc4d reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!"],"labels":["bug","new"]},{"title":"Crash during replication instance destroy","body":"### Terraform Version\r\n\r\n```shell\r\n1.3.3\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\n\r\nterraform {\r\n  required_version = \"~> 1.3.3\"\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp\/aws\"\r\n      version = \"~> 4.62\"\r\n    }\r\n  }\r\n  backend \"s3\" {\r\n    profile        = \"***\"\r\n    region         = \"eu-west-1\"\r\n    bucket         = \"***\"\r\n    key            = \"terraform-states\/****.tfstate\"\r\n    dynamodb_table = \"terraform-locks\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\ncheckable object status report for unexpected checkable object module.landing_zone_dms_tasks.aws_dms_replication_task.dms_task[\"replication-full-load-agoralive-terraform\"]\r\ngoroutine 1070 [running]:\r\nruntime\/debug.Stack()\r\n        \/usr\/local\/go\/src\/runtime\/debug\/stack.go:24 +0x65\r\nruntime\/debug.PrintStack()\r\n        \/usr\/local\/go\/src\/runtime\/debug\/stack.go:16 +0x19\r\ngithub.com\/hashicorp\/terraform\/internal\/logging.PanicHandler()\r\n        \/home\/circleci\/project\/project\/internal\/logging\/panic.go:55 +0x153\r\npanic({0x2bb13e0, 0xc00337f070})\r\n        \/usr\/local\/go\/src\/runtime\/panic.go:884 +0x212\r\ngithub.com\/hashicorp\/terraform\/internal\/checks.(*State).reportCheckResult(0xc006eebe60, {0x35ee538, 0xc0040440f0}, 0xf48ae0?, 0x0, 0x50)\r\n        \/home\/circleci\/project\/project\/internal\/checks\/state_report.go:103 +0x346\r\ngithub.com\/hashicorp\/terraform\/internal\/checks.(*State).ReportCheckResult(0x309fd0e?, {0x35ee538?, 0xc0040440f0?}, 0xc003082080?, 0x1?, 0x54b61?)\r\n        \/home\/circleci\/project\/project\/internal\/checks\/state_report.go:68 +0xd0\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.evalCheckRules(0x35e69d0?, {0xc000e853c0, 0x5, 0xc001ad4230?}, {0x3600638, 0xc0013d80e0}, {0x35ee538, 0xc0040440f0}, {{{{0x0, 0x0}}, ...}, ...}, ...)\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/eval_conditions.go:58 +0x417\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*NodeAbstractResourceInstance).plan(0xc001fd70e0, {0x3600638, 0xc0013d80e0}, 0xc001d736c0, 0x0, 0x0, {0x0, 0x0, 0x0?})\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/node_resource_abstract_instance.go:707 +0x4e5\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*NodeApplyableResourceInstance).managedResourceExecute(0xc0078b90c0, {0x3600638, 0xc0013d80e0})\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/node_resource_apply_instance.go:273 +0x7cb\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*NodeApplyableResourceInstance).Execute(0x0?, {0x3600638?, 0xc0013d80e0?}, 0x40?)\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/node_resource_apply_instance.go:140 +0xba\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*ContextGraphWalker).Execute(0xc000dea690, {0x3600638, 0xc0013d80e0}, {0x206ed3c1550, 0xc0078b90c0})\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/graph_walk_context.go:136 +0xc2\r\ngithub.com\/hashicorp\/terraform\/internal\/terraform.(*Graph).walk.func1({0x2ff2ae0, 0xc0078b90c0})\r\n        \/home\/circleci\/project\/project\/internal\/terraform\/graph.go:74 +0x2f0\r\ngithub.com\/hashicorp\/terraform\/internal\/dag.(*Walker).walkVertex(0xc000c7c8a0, {0x2ff2ae0, 0xc0078b90c0}, 0xc0078b9ac0)\r\n        \/home\/circleci\/project\/project\/internal\/dag\/walk.go:381 +0x2f6\r\ncreated by github.com\/hashicorp\/terraform\/internal\/dag.(*Walker).Update\r\n        \/home\/circleci\/project\/project\/internal\/dag\/walk.go:304 +0xf65\r\n```\r\n\r\n### Expected Behavior\r\n\r\nThe destruction of a DMS instance\r\n\r\n### Actual Behavior\r\n\r\nCrash during the destroy (10mns+)\r\n\r\n### Steps to Reproduce\r\n\r\n1. terraform init --upgrade\r\n2. terraform apply\r\n\r\n\r\n\r\n### Additional Context\r\n\r\nThe steps of creation\r\n1. Create a DMS S3 target endpoints\r\n2. Create a DMS Tasks with transformations column types created with random-integer module\r\n3. Destroy the DMS S3 endpoints and the DMS tasks with it\r\n4. Recreate a new DMS S3 endpoints and assign it to a DMS Task\r\n5. random-integer rule cannot find the older DMS endpoint which provokes a bug \r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for reporting this, @youamara!\r\n\r\nHave you tried this with Terraform v1.5? The concept of custom checks was still quite new in v1.3 and so several bugs in those early releases have been fixed in the meantime, and so this might already be fixed in the latest release.\r\n","There was a bad chain of dependencies which has been resolved by destroying the terraform state of the random integer modules tied to the old DMS Endpoint.\r\nIs it the kind of errors Terraform v1.5 can handle ?"],"labels":["bug","crash","new","v1.3"]},{"title":"Update configuration.mdx","body":"More details on how to use `-backend-config`\r\n\r\nFor me, using -backend-config was not straight-forward. On the Internet you find other people who had the same misconceptions. Therefore, I think the documentation on this feature needs more clarification.\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\n\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\nENHANCEMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33420) <br\/>All committers have signed the CLA.","Thanks for this submission! I've notified the docs team, as it is a larger change they will need to do a review for style and correctness. Thanks again!"],"labels":["waiting-response","documentation","tw-reviewed"]},{"title":"inject the organization name in TF cloud run environment variables","body":"### Terraform Version\n\n```shell\nTF Cloud VCS-driven workflows\n```\n\n\n### Use Cases\n\nI wish to configure a module to obtain provider configuration from remote state with the `tfe_outputs` data source. This requires the TFC organization and TFC workspace names. In most cases, the organization providing the outputs will be the same organization in which the cloud run is occurring. In those majority of cases it would be convenient to obtain the organization name from an env var. This will ease adoption of a root module that I author for the purpose of sharing with others who will execute the module in their respective TFC organizations' workspaces.\n\n### Attempted Solutions\n\nA local or module variable default value or hard-coded value can be used to supply the remote workspace's organization name.\n\n### Proposal\n\nAdd `TFC_ORGANIZATION_NAME` to the list of env vars injected in cloud runs https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/run\/run-environment#environment-variables and update usage hints to mention it's necessary to declare an empty string TF var matching the name of the env var before referencing the var in a module.\r\n\n\n### References\n\n_No response_","comments":["Hi @qrkourier! Thanks for this feature request.\r\n\r\nThe typical way we've designed solutions to things like this elsewhere is to have a relevant provider offer a data source that returns information about the credentials that the provider is using. For example, in `hashicorp\/aws` we have [`aws_caller_identity`](https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/data-sources\/caller_identity) which returns the account ID that the current credentials belong to, and possibly other information depending on what kind of principal the credentials are representing.\r\n\r\nA direct analogy to that for [the `hashicorp\/tfe` provider](https:\/\/registry.terraform.io\/providers\/hashicorp\/tfe) would be a data source that asks the API for information about what the active authentication token represents:\r\n\r\n```hcl\r\ndata \"tfe_token_identity\" \"current\" {}\r\n```\r\n\r\nIf the current token is a single run token (as opposed to e.g. a user account token) then the response could include both the name and ID of the organization of the workspace the current run belongs to. If the current token is a _user_ token then we would not have an organization to return, but we could return information about the user instead. A postcondition associated with the data resource could be used to make the plan fail if it's being run in a non-organization-aware context, so that in situations like yours (where an organization name is required for the configuration to work at all) it will halt with a clear error rather than just doing something strange downstream:\r\n\r\n```hcl\r\ndata \"tfe_token_identity\" \"current\" {\r\n  postcondition {\r\n    condition = self.organization != null\r\n    error_message = \"This configuration must be planned in a Terraform Cloud remote operation.\"\r\n  }\r\n}\r\n\r\ndata \"tfe_outputs\" \"example\" {\r\n  organization = data.tfe_token_identity.current.organization.name\r\n  workspace    = \"example\"\r\n}\r\n```\r\n\r\nLooking at Terraform Cloud's API I don't think there's currently an API endpoint that returns information about the requesting token regardless of its token type. The closest thing I can see is [Get Your Account Details](https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/api-docs\/account#get-your-account-details), but its documentation implies that it's appropriate for user API tokens only, so I assume it wouldn't work if requested from inside a Terraform Cloud run using the run's own token. Implementing this design would therefore require the addition of a new API endpoint, which is conceptually similar to AWS's [`sts:GetCallerIdentity`](https:\/\/docs.aws.amazon.com\/STS\/latest\/APIReference\/API_GetCallerIdentity.html) (what `aws_caller_identity` calls internally).\r\n\r\nWe _could_ potentially exploit the fact that Terraform Cloud and Terraform CLI are aware of one another and do something different here, like the environment variable you proposed, but my inclination is to follow the existing design precedent because that means that all patterns folks are already using for other providers can potentially be adapted to this situation, rather than Terraform Cloud following a needlessly different approach. This model also makes it clear how we'd expose other similar information such as the name\/ID of the current workspace, or the project that the current workspace belongs to.\r\n\r\nDo you think a provider feature like I've described above would help solve your problem?\r\n\r\nThanks again!\r\n","@apparentlymart Thank you for the additional context and thoughtful engagement. I agree with keeping the provider pattern and would use such an endpoint as you suggested. \r\n\r\n---\r\n\r\nHere's some broader context that led me to this suggestion in case it leads to a sanity check or helps to clarify the use case for the suggested enhancement. \r\n\r\nI'm solving for a series of root modules that must temporarily store outputs as inputs to the next root module in the series, with a preference for achieving this in TF Cloud vs. the alternative of depending on yet another cloud provider to store state.\r\n\r\nIt's necessary to chain together root modules when an earlier module in the series of root modules provisions a resource whose details are consumed by a subsequent root module for provider configuration. For example, if module 1 provisions a K8S cluster and has a KUBECONFIG as output, which is consumed by module 2 employing the Helm provider. \r\n\r\nI'm attempting to establish a working example of this pattern using TF Cloud with dual aims of:\r\n\r\n1. drive TF Cloud runs with GitHub integration to effect a GitOps workflow, using GitHub Actions to orchestrate the series of root module TF Cloud runs\r\n2. parameterize the TF plans sufficiently that a forkee could create the necessary TF cloud workspaces to replicate the entire GitOps workflow\r\n"],"labels":["enhancement","cloud"]},{"title":"Map colon syntax is undocumented","body":"### Terraform Version\n\n```shell\nTerraform v1.5.0\r\non darwin_amd64\n```\n\n\n### Affected Pages\n\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/expressions\/types\n\n### What is the docs issue?\n\n`terraform` accepts an alternate syntax assignment separator `:` rather than `=`:\r\n\r\n```terraform\r\nlocals = {\r\n  \"foo\" : [\r\n   \"bar\",\r\n   \"baz\",\r\n  ],\r\n}\r\n```\r\n\r\nThis was previously partially documented in #24771 but at some point this knowledge was lost.\n\n### Proposal\n\n- Document the user of `:` as an allowed syntax\r\n- Consider automatically replacing `:` with `=` in `terraform fmt`?\r\n- Consider deprecating or removing `:` altogether?\n\n### References\n\n- #24198","comments":["Thanks for this request!"],"labels":["documentation","new"]},{"title":"terraform console should allow inspection of outputs","body":"### Terraform Version\r\n\r\n```shell\r\nv1.5.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nInspect output from terraform console\r\n\r\n### Attempted Solutions\r\n\r\n```terraform\r\n#main.tf\r\noutput \"foo\" {\r\n  value = \"hello world\"\r\n}\r\n```\r\n\r\n```bash\r\n$ terraform apply -auto-approve\r\n...\r\n$ terraform console\r\n> output.foo\r\n\u2577\r\n\u2502 Error: Reference to undeclared resource\r\n\u2502\r\n\u2502   on <console-input> line 1:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 A managed resource \"output\" \"foo\" has not been declared in the root module.\r\n\u2575\r\n```\r\n","comments":["Thanks for this request!"],"labels":["enhancement","new"]},{"title":"internal\/backend\/remote-state\/azure: respect NextMarker","body":"When enumerating blob resources in Azure the service may return a result containing all of the results, or it might return a response containing only a subset of the results and a marker intended to be used to fetch the next subset in a subesquent call.\r\n\r\nThe azure backend state does not currently check for or enumerate across returned NextMarker values when using ListBlobs while fetching the list of workspaces. This results in partial results being returned when the API returns results requiring iteration via `marker` and `NextMarker`\r\n\r\nThis fixes the issue by updating the backend state to check for and respect the NextMarker continuation ensuring that the full results set is always processed regardless of whether or not the result set is limited via `maxresults` or is limited by the 5000 item per result set limit.\r\n\r\nFor more information see:\r\nhttps:\/\/learn.microsoft.com\/en-us\/rest\/api\/storageservices\/enumerating-blob-resources#Subheading2\r\nhttps:\/\/learn.microsoft.com\/en-us\/rest\/api\/storageservices\/list-blobs?tabs=azure-ad#uri-parameters\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #31719\r\n\r\n## Target Release\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n### BUG FIXES\r\n\r\n- The AzureRM backend will now return all workspaces when using the azurerm state storage backend even when the blob storage exceeds the API's 5000 item limit for a single result query.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33396) <br\/>All committers have signed the CLA.","I'll be doing ad-hoc testing on this tomorrow at work to ensure it works as expected but I could use some advice or guidance on how to write up some automated tests covering this edge case.\r\n\r\nThis is only my second attempt at anything in go and I'm having a bit of trouble finding my way around the test suite.","Thanks, I'll notify the Azure provider team. ","@crw thanks for your continued help. :)\r\n\r\nFor what it is worth I've deployed this locally and it does seem to have resolved the issue I'm having at work.","We're nearing the 1 year birthday of issue #31719 , is there anything I can do to help get this a little more traction? Every time there's a new Terraform version released I need to re-integrate and re-test a personal build including my fix to ensure we don't run into problems.\r\n\r\nIf the issue is the quality of my code I'm more than happy to incorporate any suggestions or requests from the team. I'd also be happy for my branch to be scrapped in favor of code from an engineer more familiar with Go.\r\n\r\nThe only change that needs to be made is to ensure the state backend paginates through the results of the ListBlobs() fetch API when the API says pagination is necessary (via the NextMarker parameter). The current upstream code doesn't paginate even when it is required.","Hi @coderjoe, I can see this PR is on the triage backlog for the Azure Provider team. Although the backends are maintained by the provider teams, they are not as high a priority as provider work. In my experience, the provider team will irregularly set aside time to review a batch of backend PRs, as their schedule allows. That said, I will raise it again. "],"labels":["enhancement","backend\/azure"]},{"title":"fix function name in comment","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes function name in comment\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES | UPGRADE NOTES | ENHANCEMENTS | BUG FIXES | EXPERIMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\nFix function name in comment\r\n","comments":["Thanks for this submission! To set expectations, we do not normally accept changes to code comments but I will raise this in triage. Thanks again!","> Thanks for this submission! To set expectations, we do not normally accept changes to code comments but I will raise this in triage. Thanks again!\r\n\r\nThanks\uff01"],"labels":["enhancement"]},{"title":"Add more context to Resource type mismatch error","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.6\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI'm trying to move s3 definition to the module which utilize that. And I'm getting the following error:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Resource type mismatch\r\n\u2502 \r\n\u2502 This statement declares a move from aws_s3_bucket.logs to module.logging.s3.bucket, which is a resource of a different type.\r\n\r\n```\r\n\r\n### Proposal\r\n\r\nIf we provide resource types for both modules it would be much easier to understand what's actually wrong. Here is the example error message:\r\n```\r\n\u2577\r\n\u2502 Error: Resource type mismatch\r\n\u2502 \r\n\u2502This statement declares a transition from aws_s3_bucket.logs (Type X) to module.logging.s3.bucket (Type Y), which represent two different types of resources.\r\n\r\n```\r\n\r\n### References\r\n\r\n_No response_","comments":["In my case one of modules had incorrect reference. ","Hi @makarov-roman,\r\n\r\nA resource type should be included because it is part of the resource's address, i.e. the type of an `aws_s3_bucket` is `aws_s3_bucket`. The destination resource type in the error though is only `s3`, which doesn't look correct. Can you show an example of the move block and configuration used?\r\n\r\nThanks!","Hi @jbardin thanks for the quick response.\r\nthe correct code in my case is \r\n```terraform\r\nmoved {\r\n  from = aws_s3_bucket.logs\r\n  to     = module.logging.module.logs-storage-s3[0].aws_s3_bucket.logs\r\n}\r\n```\r\nThis feature request is not about how to correctly write `moved` instruction, but I'm saying that if we had better Error message it would be clear from the first glance what's not correct. E.g. better error message in my case could be\r\n```log\r\n\u2577\r\n\u2502 Error: Resource type mismatch\r\n\u2502 \r\n\u2502This statement declares a transition from aws_s3_bucket.logs (aws_s3_bucket type) to module.logging.s3.bucket (null type), which represent two different types of resources.\r\n```\r\n","Hi @makarov-roman,\r\n\r\nThe resource type of `module.logging.s3.bucket` would be \"s3\", just as the resource type of `aws_s3_bucket.logs` is \"aws_s3_bucket\". There isn't any situation where the resource type isn't already included as part of the resource address, because the resource type is an inherent part of the address of the module.\r\n\r\nIs the feedback you're intending here that it isn't _clear_ that the resource type is part of the address? I can see that this error message does rely on some prior knowledge about Terraform -- which part of the address is the resource type -- and if that is your concern then I think I would propose to keep the message concise by removing the full resource address altogether, since that's clearly available in the configuration already anyway.\r\n\r\n```\r\nError: Resource type mismatch\r\n\r\nThis statement declares a transition from a resource of type \"aws_s3_bucket\" to a\r\nresource of type \"s3\". The source and destination resource type must match.\r\n```\r\n\r\nThis formulation omits the irrelevant information (the module path, resource name, and instance key) and focused only on the information that is relevant to the problem being described.\r\n\r\nWould this updated error message meet your needs better? Thanks!\r\n","Hi @apparentlymart , I would say - yes, partially at least. Does it cover scenario if address is incorrect? My initial idea was that I can use modules outputs definition to access the resource, that's why it looks like that in the first snippet.  e.g. \r\n`logging\/outputs.tf`\r\n```terraform\r\noutput \"s3\" {\r\n  value       = module.logs-storage-s3\r\n}\r\n```\r\n`logging\/modules\/logs-storage-s3\/outputs.tf`\r\n```terraform\r\noutput \"bucket\" {\r\n  value = aws_s3_bucket.logs\r\n}\r\n```\r\nThat was what I expected: `module.logging.s3(resolves to module.logs-storage-s3).bucket(resolves to  aws_s3_bucket.logs)`\r\nThat's not a big deal that it is not working that way, but it's confusing that it compares valid address with invalid address and tells you that it's resource mismatch. Maybe we should check if the address match any definition?\r\nit's like `valid_object.property === null.property` will throw an exception in most languages because null doesn't have a property in the first place.","Thanks for that extra context!\r\n\r\nUnfortunately `module.logging.s3.bucket` _is_ a valid address, but it has a different meaning than you expected: it means a ` resource \"s3\" \"bucket\"` block inside `module.logging`, rather than an output value called `s3` of that module. `moved` is exclusively for rebinding resources and resource instances, so it has no syntax for interacting with output values.\r\n\r\nFor misunderstandings like this we need to be careful in our error messages because we cannot be certain that a particular user had a particular misunderstanding. For situations like that, if they come up sufficiently often to justify the extra logic, we tend to add \"Did you mean ...?\" hints to the error message. In the case you described we could potentially notice that `module.logging` has an `output \"s3\"` block and that it _doesn't_ have a `resource \"s3\" \"bucket\"`, and if so add an extra clause that asks if the author was intending to use that output value. For example:\r\n\r\n```\r\nError: Resource type mismatch\r\n\r\nThis statement declares a transition from a resource of type \"aws_s3_bucket\" to a\r\nresource of type \"s3\". The source and destination resource type must match.\r\n\r\nDid you intend to refer to the output \"s3\" block in module.logging? Moving is\r\nonly for resources, so there is no syntax for moving an output value.\r\n```\r\n\r\nIdeally I'd like to suggest an alternative as part of the second paragraph, but it's not clear to me what we'd suggest in this case because there's no way for us to understand what the author might have meant by declaring that a resource as moved to an output value. So I wrote it to just state that it's not possible, which is a lower-quality error message than I'd typically prefer but seems like the best we could do in this case, to avoid making too many assumptions that could cause confusion if they are incorrect assumptions.\r\n","Agree. This heuristic isn't worth it. Is there a way to check if there is any resource defined under the address? Something similar to that, but for `to`. https:\/\/github.com\/hashicorp\/terraform\/blob\/66e3c20b181407fd93b787a359f97a2ae6aa66ac\/internal\/refactoring\/move_validate.go#L99"],"labels":["enhancement","new"]},{"title":"Enhancement: Provide a workspace parameter to the import block","body":"### Terraform Version\n\n```shell\n1.5.0\n```\n\n\n### Use Cases\n\nGiven a pipeline with multiple environments, I want the import command to be made idempotent across all environments, only operating in the target environment.\n\n### Attempted Solutions\n\nOur workaround currently is to target specific releases to the environments which we want to import, and then make sure the import commands are removed before pushing a new release through the entire environment lifecycle.\n\n### Proposal\n\nIt would be good to have an option of specifying the workspace where the import should take place. For example:\r\n\r\n```HCL\r\nimport {\r\n  to = azurerm_network_security_group.nsg\r\n  id = \"\/subscriptions\/sub1\/resourceGroups\/rg1\/providers\/Microsoft.Network\/networkSecurityGroups\/nsg1\"\r\n  workspace = staging\r\n}\r\n\r\nimport {\r\n  to = azurerm_network_security_group.nsg\r\n  id = \"\/subscriptions\/sub2\/resourceGroups\/rg1\/providers\/Microsoft.Network\/networkSecurityGroups\/nsg1\"\r\n  workspace = production\r\n}\r\n```\n\n### References\n\n_No response_","comments":["Please see also, adjacent issue: https:\/\/github.com\/hashicorp\/terraform\/issues\/33228","Correct me if I'm wrong, but can we use the `-state` flag with the terraform import command. The `-state` flag can be used to specify the path to the state file for the target workspace. Look below for example:\r\n\r\n                    terraform import -state=path\/to\/stagging.tfstate azurerm_network_security_group.nsg \r\n                       \/subscriptions\/sub1\/resourceGroups\/rg1\/providers\/Microsoft.Network\/networkSecurityGroups\/nsg1","@rohan472000 This issue isn't regarding the import command but the new import block that works with a standard Terraform plan or apply run.","For making import block idempotent across multiple environments, can we create a reusable Terraform module like `idempotent_module` or give any name. This module will accept variables for workspace and `import_idempotent_details`, inside the module, a conditional statement will be used to determine if the current environment matches the target workspace, ensuring import commands are executed only in the intended environment. \r\n\r\nThe module will have a  block with a local-exec provisioner, enabling the execution of import commands using the provided `import_idempotent_details`. In the main Terraform configuration, the `idempotent_module` will be instantiated for each environment, with the appropriate workspace and `import_idempotent_details` specified. \r\n\r\n\r\nNot sure that above will work or not, but tell me your thought on it.....","Before going any further I wanted to just confirm you are talking about the same import blocks as detailed here: https:\/\/developer.hashicorp.com\/terraform\/language\/import","This would be great. Since the id strings must be literal, there's not a lot of wiggle room to use the same import block for multiple workspaces (assuming the underlying resource is different in each workspace). We're contemplating generating a unique .tf per workspace in our CI\/CD process that contains the import block it uses, but it's an awkward workaround and, honestly, a lot of work when don't use imports that often."],"labels":["enhancement","plannable-import"]},{"title":"Config-driven import | generate code for modules, too","body":"### Terraform Version\r\n\r\n```shell\r\n1.5.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI love the new import block. Sadly, it seems to be less useful for modules as:\r\n\r\n1. import has to be at root level\r\n2. cannot import into (generate code for) stuff within modules\r\n\r\nMy use case is not only to import but also have the corresponding code generated (which works fine for top-level resources).  \r\nMy current workaround is \r\n* to point the import at top level\r\n* run plan with code generation\r\n* move code into module\r\n* point import to the resource within the module\r\n* apply\r\n\r\nThis usually works but involves a lot of manual steps, and I think the block was intended to remove that manual burden. Maybe I'm missing something here.\r\n\r\n### Attempted Solutions\r\n\r\ncurrent workaround s. a.\r\n\r\n### Proposal\r\n\r\nI'd like tf to generate code for modules as well. Ideally, I'd just point the import to a non-existing resource within a module (local one) and the existing file or folder and tf would generate the code and append it to the file or create a file within the given folder.\r\n\r\nI may fail for non-local modules, of course.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hey @black-snow , thank you for this request!\r\n\r\nWe do intend on supporting import{} natively within modules, but are still working through the design. I'd be happy to chat more about this in detail! Please email me at oismail@hashicorp.com and would love to setup a time to talk :).","@black-snow could you give a little more data on your workaround? We usually create our Terraform based on community modules only, and have to have a pretty big justification for writing and maintaining our own modules. We'd like to use the new import functionality to bring existing resources into community-based modules, but we don't see an easy way to do this right now.","@Vermyndax I don't see how this could currently work. I'd do it manually or write a short pre-processor if it's too much to handle manually.","Is there still no plans to support it within child modules?\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/issues\/33474 <--- i have posted a number of scenarios where it would be helpful for us\r\n\r\nIt's pretty useless as is being limited to the root module, solves a lot of issues with the ability to use it in child modules.\r\n\r\nIf the issue is \"safety\", that's why atlas has a built in plan step so you can review the changes before they are applied.","@TrueCyberAxe @black-snow @Vermyndax \r\n\r\nI'd love to chat through this use case more. Could you email me and we can find time to chat? oismail@hashicorp.com"],"labels":["enhancement","plannable-import"]},{"title":"Code generation recommendation for drift remediation","body":"### Terraform Version\n\n```shell\nTerraform v1.4.4\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nTerraform cloud has a feature of drift remediation, where it shows the attributes of an IaC block which have changed due to drift. Then the user can accept the suggested code changes.\r\nhttps:\/\/www.hashicorp.com\/blog\/terraform-cloud-adds-drift-detection-for-infrastructure-management\r\n\r\nIn native terraform, we have drift detection, but no feature to remediate the drift automatically. So to resolve the drift, we have to edit the IaC code manually.\n\n### Attempted Solutions\n\nNA as this is a new feature request\n\n### Proposal\n\nIs there any plan of bringing the drift remediation feature to native terraform? \r\n\r\nFor example, if i use `terraform plan --remediate-drift` then it make changes to the TF files to bring it in sync with the running infra. Then on `terraform apply`, i can apply those changes to resolve the drift. \n\n### References\n\n_No response_","comments":["Hi @rahulrj! Thanks for this feature request.\r\n\r\nWhat you've described is a Terraform Cloud feature, so you'll need to be a Terraform Cloud user to benefit from it. With that in mind, I'd like to clarify what you are asking: are you a Terraform Cloud user who wants to make use of this feature from the command line instead of the web UI, or are you instead asking for this feature to be available when _not_ using Terraform Cloud?\r\n\r\nI want to be explicit up front that it's unlikely that this feature will be available for anyone not using Terraform Cloud, because as currently implemented this behavior lives inside the Terraform Cloud platform and so there's no remediation functionality in this codebase to make use of outside of Terraform Cloud. Having the CLI workflow notice and react to Terraform Cloud's drift detection signals is a potentially plausible new feature, though I think there would be some non-trivial design work to figure out how to integrate it given than Terraform Cloud drift detection is a background scheduled task asynchronous from Terraform CLI usage.\r\n","> I'd like to clarify what you are asking: are you a Terraform Cloud user who wants to make use of this feature from the command line instead of the web UI, or are you instead asking for this feature to be available when not using Terraform Cloud?\r\n\r\nI am asking for this feature to be available when not using Terraform Cloud\r\n\r\n> hough I think there would be some non-trivial design work to figure out how to integrate it given than Terraform Cloud drift detection is a background scheduled task asynchronous from Terraform CLI usage.\r\n\r\nFor the CLI usage, it doesnt have to be a background scheduled task. For example, we can have an option in the CLI like this, which runs drift remediation during plan\r\n`\r\nterraform plan --remediate-drift\r\n`\r\nThe above command can modify the existing TF file with the new changes. \r\n","Hi @rahulrj,\r\n\r\nThanks for the additional context.\r\n\r\nThis is intentionally a Terraform Cloud-specific feature and we have no current plans to offer it outside of Terraform Cloud.\r\n"],"labels":["enhancement","cloud"]},{"title":"enhancement: use MaxCost to reduce work done by didyoumean.NameSuggestion","body":"https:\/\/pkg.go.dev\/github.com\/agext\/levenshtein@v1.2.3#Params.MaxCost\r\n\r\n> MaxCost overrides the default value of 0 (meaning unlimited) for the maximum cost. The calculation of Distance() stops when the result is guaranteed to exceed this maximum\r\n\r\nThis should improve the performance over any data set, but particularly large ones, which is called out as a concern in the func comment\r\n\r\n```go\r\n\/\/ This function is intended to be used with a relatively-small number of\r\n\/\/ suggestions. It's not optimized for hundreds or thousands of them.\r\nfunc NameSuggestion(given string, suggestions []string) string {\r\n```\r\n\r\n```zsh\r\n# Before\r\ngo test -bench=. -benchtime=60s\r\ngoos: darwin\r\ngoarch: amd64\r\npkg: benchmark\r\ncpu: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz\r\nBenchmarkNameSuggestion10-8     \t1000000000\t         0.0000140 ns\/op\r\nBenchmarkNameSuggestion100-8    \t1000000000\t         0.0001175 ns\/op\r\nBenchmarkNameSuggestion1000-8   \t1000000000\t         0.0008091 ns\/op\r\nPASS\r\nok  \tbenchmark\t0.197s\r\n\r\n#\u00a0After\r\n\r\ngo test -bench=. -benchtime=60s\r\ngoos: darwin\r\ngoarch: amd64\r\npkg: benchmark\r\ncpu: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz\r\nBenchmarkNameSuggestion10-8     \t1000000000\t         0.0000165 ns\/op\r\nBenchmarkNameSuggestion100-8    \t1000000000\t         0.0000216 ns\/op\r\nBenchmarkNameSuggestion1000-8   \t1000000000\t         0.0000093 ns\/op\r\nPASS\r\nok  \tbenchmark\t0.184s\r\n```\r\n\r\nSignificant gain on larger suggestion set :)","comments":["Thanks for this submission!"],"labels":["enhancement"]},{"title":"Move from os.GetEnv to schema.MultiEnvDefaultFun breaks some existing configs","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.3.9\r\non darwin_arm64\r\n\r\nTerraform v1.4.0\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nterraform {\r\n  backend \"gcs\" {\r\n    prefix         = \"tfstate\/state\"\r\n    bucket         = \"bucket\"\r\n    encryption_key = \"\"\r\n  }\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n\r\nna\r\n\r\n### Expected Behavior\r\n\r\nTerraform 1.4.0 working like 1.3.9 did and the `GOOGLE_ENCRYPTION_KEY` being picked up like before\r\n\r\n### Actual Behavior\r\n\r\nThe `GOOGLE_ENCRYPTION_KEY` is ignored and the empty `encryption_key` has preference which results in the error:\r\n```console\r\n# terraform init\r\n\r\nInitializing the backend...\r\nError loading state: Failed to open state file at gs:\/\/guida-aws-production-tf\/tfstate\/guida-eks-dev-1\/default.tfstate: googleapi: got HTTP response code 400 with body: <?xml version='1.0' encoding='UTF-8'?><Error><Code>ResourceIsEncryptedWithCustomerEncryptionKey<\/Code><Message>The resource is encrypted with a customer encryption key.<\/Message><\/Error>\r\n```\r\n\r\nWhile this worked fine in 1.3.9\r\n\r\n### Steps to Reproduce\r\n\r\nSetup the`gcs` backend with `encryption_key = \"\"` in the conf like above\r\n\r\n### 1.3.9\r\n1. export GOOGLE_ENCRYPTION_KEY=\"key\"\r\n2. terraform init\r\n3. works\r\n\r\n### 1.4.0\r\n1. export GOOGLE_ENCRYPTION_KEY=\"key\"\r\n2. terraform init\r\n3. encryption key error\r\n\r\n### Additional Context\r\n\r\nChanges like this will lower the precedence of the env var: https:\/\/github.com\/hashicorp\/terraform\/pull\/31786\/commits\/469824c3244d0cf98d16c4880e12c6eb41d8f91c\r\n\r\nIn our case just removing the key was enough but it was a bit of a search why this broke since there is no mention in the CHANGELOG, just about some new features for the gcs backend.\r\n\r\nI don't know if this is a documentation issue, I just wanted to raise some awareness for changes like this. A mention in the changelog about this would have been nice. It now got sneaked in with a mostly unrelated PR.\r\n\r\n### References\r\n\r\n- #31786","comments":["Thanks for this submission! I'll notify the appropriate team. "],"labels":["bug","backend\/gcs","new"]},{"title":"Registry UI crashes","body":"### Terraform Version\n\n```shell\nn\/a\n```\n\n\n### Terraform Configuration Files\n\nn\/a\n\n### Debug Output\n\nn\/a\n\n### Expected Behavior\n\nhttps:\/\/registry.terraform.io\/modules\/Azure\/aks\/azurerm\/latest page should load\r\n\n\n### Actual Behavior\n\nPage doesn't load, spins forever\r\n\r\nNote that older versions work, for example https:\/\/registry.terraform.io\/modules\/Azure\/aks\/azurerm\/1.0.0\n\n### Steps to Reproduce\n\n1. Open browser\r\n2. Navigate to https:\/\/registry.terraform.io\/modules\/Azure\/aks\/azurerm\/latest\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @MaxHorstmann \ud83d\udc4b Thank you for reporting this issue and apologies that the Registry front end is not working as expected. While the team that manages the public Terraform Registry does not monitor this repository for issues, I have noticed the same behavior locally and forwarded this issue link to them internally."],"labels":["bug","registry","new"]},{"title":"Extend count parameter to include actual count value (length)","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.6\r\non darwin_arm64\r\n+ provider registry.terraform.io\/hashicorp\/aws v5.2.0\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nLet's assume the following example:\r\n\r\n```terraform\r\nresource \"aws_instance\" \"consul-server\" {\r\n  count        = 3\r\n  tags          = { Name = \"Consul-Server-${count.index}\" }\r\n  \/\/ ...\r\n  user_data = templatefile(\"${path.module}\/templates\/server.sh.tpl\", {\r\n    ui_config_enabled = count.index == 0 ? \"true\" : \"false\"\r\n    bootstrap_expect = count.length\r\n  })\r\n}\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nIn the above example using `length(aws_instance.consul-server)` causes a cycle.\r\n\r\nThe only way to mitigate it is to use a variable or a local.\r\n\r\n### Proposal\r\n\r\nIn this way we can safely refer to the current count of resources being created. It reduces the area of possible errors.\r\n\r\n### References\r\n\r\n[Docs for 1.5.x](https:\/\/developer.hashicorp.com\/terraform\/language\/meta-arguments\/count)","comments":["Thanks for this request!"],"labels":["enhancement","new"]},{"title":"TestInit_cancelModules fails for 1.5.0","body":"### Terraform Version\r\n\r\n```shell\r\n1.5.0\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nNo configuration, since the tests fail.\r\n\r\n### Debug Output\r\n\r\n```\r\n--- FAIL: TestInit_cancelModules (0.00s)\r\n    init_test.go:1776: wrong error message\r\n        should contain: Module installation was canceled by an interrupt signal\r\n        got:\r\n        \r\n        Error: Error accessing remote module registry\r\n        \r\n          on main.tf line 1:\r\n           1: module \"foo\" {\r\n        \r\n        Failed to retrieve available versions for module \"foo\" (main.tf:1) from\r\n        registry.does.not.exist: failed to request discovery document: Get\r\n        \"https:\/\/registry.does.not.exist\/.well-known\/terraform.json\": dial tcp:\r\n        lookup registry.does.not.exist: no such host.\r\n        \r\nFAIL\r\nFAIL\tgithub.com\/hashicorp\/terraform\/internal\/command\t42.959s\r\n```\r\n\r\n### Expected Behavior\r\n\r\nAll tests are green.\r\n\r\n### Actual Behavior\r\n\r\nThe test `TestInit_cancelModules` failed due to it giving the wrong error message. It expected `Module installation was canceled by an interrupt signal` but got `Error accessing remote module registry`.\r\n\r\n### Steps to Reproduce\r\n\r\n`go test -mod=readonly .\/...`\r\n\r\n### Additional Context\r\n\r\nThe Arch Linux PKGBUILD now needs to [skip this test](https:\/\/gitlab.archlinux.org\/archlinux\/packaging\/packages\/terraform\/-\/commit\/cf59c0b41f41430d1882bc3c18cf6697e45882ce) to provide 1.5.0.\r\n\r\n### References\r\n\r\n_No response_","comments":["Note: I released the [Arch 1.5.0 package](https:\/\/archlinux.org\/packages\/extra\/x86_64\/terraform\/) anyway, since I don't deem it to be critical.","Thanks @hashworks!\r\n\r\nLooks like that test assumes some delay in the invalid host name resolution, which end up resolving before the intended cancelation can take place. I also don't think this is critical for release, and is only a failure of the individual test's structure.\r\n\r\nThanks!"],"labels":["bug","cli"]},{"title":"feat: check for presence of target resource in ChangedResources","body":"Fixes #33200\r\n\r\n## Target Release\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n`terraform <plan|apply> -target=<resource>`: warn if targeted resource is not present in state.\r\n\r\n----\r\n\r\nI have commented the logic choice I have made as to how I check for presence. If you prefer it to instead be something like\r\n\r\n```go\r\n\t\tresourceMap := make(map[string]struct{}, len(plan.Changes.Resources))\r\n\t\tfor _, resource := range plan.Changes.Resources {\r\n\t\t\tresourceMap[resource.Addr.String()] = struct{}{}\r\n\t\t}\r\n\r\n\t\tfor _, target := range opts.Targets {\r\n\t\t\tif _, ok := resourceMap[target.String()]; !ok {\r\n\t\t\t\tdiags = diags.Append(tfdiags.Sourceless(\r\n\t\t\t\t\ttfdiags.Warning,\r\n\t\t\t\t\t\"Failed to target resource\",\r\n\t\t\t\t\tfmt.Sprintf(\"%s is not a valid resource address. Check for a typo.\", target),\r\n\t\t\t\t))\r\n\t\t\t}\r\n\t\t}\r\n```\r\n\r\nthen I am happy to update it.\r\n\r\nExample run:\r\n\r\n```zsh\r\n\u279c   cat test.tf\r\nterraform {\r\n  backend \"local\" {\r\n    path = \"terraform.tfstate\"\r\n  }\r\n}\r\n\r\n\r\nresource \"random_string\" \"random\" {\r\n  length           = 16\r\n  special          = true\r\n  override_special = \"\/@\u00a3$\"\r\n}\r\n\u279c   terraform plan -replace=random_string.random -target=random_string.rndom\r\n\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration and found no differences,\r\nso no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\u279c  \"$(go env GOPATH)\/bin\/terraform\" plan -replace=random_string.random -target=random_string.rndom\r\n\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration and found no differences,\r\nso no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\u2577\r\n\u2502 Warning: Failed to target resource\r\n\u2502\r\n\u2502 random_string.rndom is not a valid resource address. Check for a typo.\r\n\u2575\r\n\u279c   \"$(go env GOPATH)\/bin\/terraform\" apply -replace=random_string.random -target=random_string.rndom\r\n\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration and found no differences,\r\nso no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\u2577\r\n\u2502 Warning: Failed to target resource\r\n\u2502\r\n\u2502 random_string.rndom is not a valid resource address. Check for a typo.\r\n\u2575\r\n\u2577\r\n\u2502 Warning: Applied changes may be incomplete\r\n\u2502\r\n\u2502 The plan was created with the -target option in effect, so some changes requested in the\r\n\u2502 configuration may have been ignored and the output values may not be fully updated. Run the\r\n\u2502 following command to verify that no other changes are pending:\r\n\u2502     terraform plan\r\n\u2502\r\n\u2502 Note that the -target option is not suitable for routine use, and is provided only for exceptional\r\n\u2502 situations such as recovering from errors or mistakes, or when Terraform specifically suggests to\r\n\u2502 use it as part of an error message.\r\n\u2575\r\n\r\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\r\n\r\n\u279c   \"$(go env GOPATH)\/bin\/terraform\" apply -replace=random_string.random -target=random_string.random\r\nrandom_string.random: Refreshing state... [id=8YireUSutoxZaFyi]\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are\r\nindicated with the following symbols:\r\n-\/+ destroy and then create replacement\r\n\r\nTerraform will perform the following actions:\r\n\r\n  # random_string.random is tainted, so must be replaced\r\n-\/+ resource \"random_string\" \"random\" {\r\n      ~ id               = \"8YireUSutoxZaFyi\" -> (known after apply)\r\n      ~ result           = \"8YireUSutoxZaFyi\" -> (known after apply)\r\n        # (11 unchanged attributes hidden)\r\n    }\r\n\r\nPlan: 1 to add, 0 to change, 1 to destroy.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\r\nDo you want to perform these actions?\r\n  Terraform will perform the actions described above.\r\n  Only 'yes' will be accepted to approve.\r\n\r\n  Enter a value: yes\r\n\r\nrandom_string.random: Destroying... [id=8YireUSutoxZaFyi]\r\nrandom_string.random: Destruction complete after 0s\r\nrandom_string.random: Creating...\r\nrandom_string.random: Creation complete after 0s [id=i1CvW5L8nYqtMQUl]\r\n\u2577\r\n\u2502 Warning: Applied changes may be incomplete\r\n\u2502\r\n\u2502 The plan was created with the -target option in effect, so some changes requested in the\r\n\u2502 configuration may have been ignored and the output values may not be fully updated. Run the\r\n\u2502 following command to verify that no other changes are pending:\r\n\u2502     terraform plan\r\n\u2502\r\n\u2502 Note that the -target option is not suitable for routine use, and is provided only for exceptional\r\n\u2502 situations such as recovering from errors or mistakes, or when Terraform specifically suggests to\r\n\u2502 use it as part of an error message.\r\n\u2575\r\n\r\nApply complete! Resources: 1 added, 0 changed, 1 destroyed.\r\n```","comments":["TODO: add a corresponding test. I think there's also a test failing due to this diag being present now and the diag count not being as previously was.","Thanks for this submission!","No problem @crw :) I've made an enhancement that should make this feature even better by suggesting the resource that you typoed:\r\n\r\n```zsh\r\n\u279c   \"$(go env GOPATH)\/bin\/terraform\" plan -target=random_string.random  -target=wayoff.ra\r\nrandom_string.random: Refreshing state... [id=i1CvW5L8nYqtMQUl]\r\n\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration and found no differences,\r\nso no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\u2577\r\n\u2502 Warning: Failed to target resource\r\n\u2502\r\n\u2502 wayoff.ra is not a valid resource address. Check for a typo.\r\n\u2575\r\n\r\n\u279c   \"$(go env GOPATH)\/bin\/terraform\" plan -target=random_string.random  -target=random_string.rando\r\nrandom_string.random: Refreshing state... [id=i1CvW5L8nYqtMQUl]\r\n\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration and found no differences,\r\nso no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502\r\n\u2502 You are creating a plan with the -target option, which means that the result of this plan may not\r\n\u2502 represent all of the changes requested by the current configuration.\r\n\u2502\r\n\u2502 The -target option is not for routine use, and is provided only for exceptional situations such as\r\n\u2502 recovering from errors or mistakes, or when Terraform specifically suggests to use it as part of\r\n\u2502 an error message.\r\n\u2575\r\n\u2577\r\n\u2502 Warning: Failed to target resource\r\n\u2502\r\n\u2502 random_string.rando is not a valid resource address. Did you mean random_string.random?\r\n```\r\n\r\nI will try to get the test fixed and added ASAP!"],"labels":["enhancement"]},{"title":"\"terraform providers mirror\" skip downloading packages that are already present in the mirror directory ","body":"### Terraform Version\n\n```shell\nv1.4.6\n```\n\n\n### Use Cases\n\nI would like to handle all the providers in a cached path on my or the remote machine which would allow me to:\r\n* Save space on the disk as I would have all the providers cached in a specific path\r\n* Not break multiple terraform commands execution at the same time when a provider is being downloaded\r\n* Keep updated the cached providers once every a while without the need to worry about it when running the CI\n\n### Attempted Solutions\n\nI was not able to achieve this. For now the only solution would be to enter the modules I have in my repository and run `terraform init` for each terraform block found with a different `required_providers` block,\n\n### Proposal\n\nI think that there are two possibilities:\r\n*Add a simple option named `-download-providers-only` which would simply download the providers if not already present in the providers directory without installing them in the .tf files directory\r\n*Create new command plugin-download to simply do the above\n\n### References\n\n_No response_","comments":["Hi @maonat! Thanks for sharing this use-case.\r\n\r\nHave you seen [the `terraform providers mirror` command](https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/providers\/mirror)? It seems like it's related to what you are asking about, though perhaps not exactly the same in the details.\r\n\r\n","Hi @apparentlymart, I've tried using the `terraform providers mirror` and is working ALMOST as I need: It is indeed doing just the download but it does not skip the download if the file already binary already exists in the directory.\r\nIs this expected or should say that it's a possible feature request for this command?","Hi @maonat,\r\n\r\nHaving it skip re-downloading if it can detect that the mirror packages already match the checksums reported by the registry does seem like a very reasonable feature request. Shall we transform this issue into that? :grinning: \r\n\r\n","@apparentlymart that would be great!\r\nHow do I need to proceed? Close this issue and open a new one or proceed and add an `edit:` below the original discussion?\r\nPS: Do you think it would be possible to skip the checksums check?","For now I've just changed the summary to reflect what we discussed and we'll let the discussion above document how we got here. I think that'll be sufficient to give us something to use for prioritization and gathering related use-cases.\r\n\r\nThis command's purpose is to synchronize a mirror with the content it is mirroring and so the checksum part seems important to allow the tool to repair a mirror that has become corrupted somehow, so that there isn't a broken or maliciously modified package present indefinitely.\r\n\r\nHowever, if you can say more about why you'd want to disable checksum verification then of course we could consider that while designing this new behavior.\r\n\r\nThanks!","@apparentlymart I understand the issue on the checksums.\r\nLet's skip this for now \ud83d\udc4d ","Thanks @maonat! Since this is a valid feature request, we will leave it open to gather support and use cases. We appreciate your feedback!","I think I also have an use case as well.\r\n\r\nOn the company that I work for, we use Terraform (1.7.0) with Terragrunt (0.54.17) and we share the same short providers file (most related with AWS and Kubernetes) with all our terraform code. \r\n\r\nCurrently, we have 28 folders that we should interact at the same time with commands like `terragrunt run-all plan` and in every single folder we should init and download those providers, so to avoid spent extra time on `terragrunt run-all init` we created a centralized plugins cache and when it's required, we pull the plugins data from the cache. \r\n\r\nThis update will also improve the time spent downloading the same plugin twice and we also increase the number of folders that we interact at the same time."],"labels":["enhancement","cli","providers","provider_mirrors"]},{"title":"Terraform Cloud|Enterprise static|read only variable set","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\r\non darwin_arm64\n```\n\n\n### Use Cases\n\nAs we scale out Terraform with Cloud\/Enterprise, we need the ability to statically set variables for certain orgs\/projects that are read only and cannot be overridden at any level. This would make this the highest precedence level which is not something currently supported.\r\n\r\nIt also needs to be set up in a way that it requires the org owner to set and the project\/workspace admins can't override it\r\n\r\nDocs explaining current pattern\r\nhttps:\/\/developer.hashicorp.com\/terraform\/enterprise\/workspaces\/variables#precedence\n\n### Attempted Solutions\n\nThere is no solution for this currently as the variables can be overridden by the users\n\n### Proposal\n\nAdd another precedence level variable set that cannot be overridden\n\n### References\n\n_No response_","comments":["Hi @Esity, thanks for this request! As it relates to aspects of the commercial product, I would recommend that you also direct your inquiries to [tf-cloud@hashicorp.support](mailto:tf-cloud@hashicorp.support) or [open a new request](https:\/\/support.hashicorp.com\/hc\/en-us\/requests\/new). \r\n\r\nIn this case, I believe this is relevant to the core open source Terraform product and as such will leave this issue open (in fact, I believe this is a duplicate, but do not know the primary ticket off-hand). \r\n\r\nThanks again for this feedback!\r\n","Hi @Esity! Thanks for sharing this use-case.\r\n\r\nTo help explore the problem space, let's consider a hypothetical edge-case to see what behavior you'd prefer for that situation.\r\n\r\nConsider that you currently have a Terraform workspace whose root module contains the following:\r\n\r\n```hcl\r\nvariable \"example\" {\r\n  type = string\r\n}\r\n\r\noutput \"result\" {\r\n  value = var.example\r\n}\r\n```\r\n\r\nAnd let's also assume that the feature you described already exists and you have a non-overridable rule that this variable is always set to `\"hello\"`. This (presumably) means that it would not be possible to:\r\n- Set this variable in the workspace settings.\r\n- Set this variable in a `.tfvars` file.\r\n\r\nThe above would therefore always generate the result `\"hello\"`, regardless of how the workspace itself is configured or what `.tfvars` files are present. (If this isn't actually what you intended in your description, please let me know!)\r\n\r\n---\r\n\r\nGiven that starting point, imagine now that someone who controls the source code of the root module of this workspace changes it as follows:\r\n\r\n```hcl\r\nvariable \"example\" {\r\n  type = string\r\n}\r\n\r\noutput \"result\" {\r\n  value = \"howdy!\"\r\n}\r\n```\r\n\r\nNotice that this module is now not using `var.example` _at all_, and therefore it's irrelevant what it was set to at the organization\/project level. The result is now `\"howdy!\"`, despite the attempt to set it to `\"hello\"` via a top-down setting.\r\n\r\nDoes that outcome match what you'd expect? Or would you want to impose some sort of additional constraint to make sure that this variable is actually used in a specific way?\r\n\r\nThanks!\r\n"],"labels":["enhancement","new","cloud"]},{"title":"Support providers using localterraform.com","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.6\r\non linux_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWithin TFE one can use the generic hostname `localterraform.com` in order to resolve module sources to the current TFE instance (https:\/\/developer.hashicorp.com\/terraform\/cloud-docs\/registry\/using#generic-hostname-terraform-cloud-and-terraform-enterprise).\r\n\r\nAs of today, this seems to work only for module sources. \r\n\r\nIt would be nice if also provider can use the `localterraform.com` hostname.\r\n\r\nI made some tests and I can actually verify that the providers are successsfully downloaded using `localterraform.com` during init phase:\r\n\r\n```\r\nInitializing provider plugins...\r\n- Finding localterraform.com\/sva-testing\/redfish versions matching \"0.0.4\"...\r\n2023-05-31T11:23:43.337Z [DEBUG] CredentialsForHost found alias localterraform.com for oa89694terrar2.tld.de\r\n2023-05-31T11:23:43.337Z [DEBUG] GET https:\/\/oa89694terrar2.tld.de\/api\/registry\/v1\/providers\/org\/redfish\/versions\r\n2023-05-31T11:23:43.337Z [TRACE] HTTP client GET request to https:\/\/oa89694terrar2.tld.de\/api\/registry\/v1\/providers\/org\/redfish\/versions\r\n```\r\n\r\nhowever the following plan\/apply fails with:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Invalid provider registry host\r\n\u2502 \r\n\u2502 The host \"localterraform.com\" given in in provider source address\r\n\u2502 \"localterraform.com\/org\/redfish\" does not offer a Terraform\r\n\u2502 provider registry.\r\n```\r\n\r\nThus I believe, it might be possible to implement this request since the providers are already successfully fetched.\r\n\r\n### Attempted Solutions\r\n\r\n```hcl\r\nterraform {\r\n  required_version = \">= 1.4.0\"\r\n\r\n  required_providers {\r\n    local = {\r\n      source  = \"localterraform.com\/org\/redfish\"\r\n      version = \"2.4.0\"\r\n    }\r\n  }\r\n\r\n  cloud {}\r\n}\r\n```\r\n\r\n\r\n### Proposal\r\n\r\nSupport `localterraform.com` for providers\r\n\r\n### References\r\n\r\n_No response_","comments":[],"labels":["enhancement","cloud"]},{"title":"Call to function \"merge\" failed: panic","body":"### Terraform Version\n\n```shell\n\u22ca> ~ terraform version                                                                                                                                                                20:02:16\r\nTerraform v1.4.6\r\non darwin_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nmerge(false ? {a={}} : null)\r\n```\r\n\n\n### Debug Output\n\nsee output below\n\n### Expected Behavior\n\n```\r\n{}\r\n```\n\n### Actual Behavior\n\n```\r\n\u2502 Error: Error in function call\r\n\u2502\r\n\u2502   on <console-input> line 1:\r\n\u2502   (source code not available)\r\n\u2502\r\n\u2502 Call to function \"merge\" failed: panic in function implementation: returned value cty.EmptyObjectVal does not conform to expected return type\r\n\u2502 cty.Object(map[string]cty.Type{\"a\":cty.EmptyObject}): missing required attribute \"a\"\r\n\u2502 goroutine 1 [running]:\r\n\u2502 runtime\/debug.Stack()\r\n\u2502 \t\/Users\/runner\/hostedtoolcache\/go\/1.19.6\/x64\/src\/runtime\/debug\/stack.go:24 +0x65\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.errorForPanic(...)\r\n\u2502 \t\/Users\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/error.go:44\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.Function.Call.func1()\r\n\u2502 \t\/Users\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/function.go:294 +0x9f\r\n\u2502 panic({0x2f30ec0, 0xc0007ab560})\r\n\u2502 \t\/Users\/runner\/hostedtoolcache\/go\/1.19.6\/x64\/src\/runtime\/panic.go:884 +0x212\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.Function.Call({0x3983c40?}, {0xc000052c60?, 0x1, 0x1})\r\n\u2502 \t\/Users\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/function.go:310 +0x5e5\r\n\u2502 github.com\/hashicorp\/hcl\/v2\/hclsyntax.(*FunctionCallExpr).Value(0xc000598870, 0xc000012c48)\r\n\u2502 \t\/Users\/runner\/go\/pkg\/mod\/github.com\/hashicorp\/hcl\/v2@v2.16.2\/hclsyntax\/expression.go:456 +0x1c85\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/lang.(*Scope).EvalExpr(0xc00012d5c0, {0x3982938?, 0xc000598870}, {{0x3983b98?, 0x4df6cb8?}})\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/lang\/eval.go:171\r\n\u2502 +0x148\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/repl.(*Session).handleEval(0xc000b379e8, {0xc000056680?, 0x0?})\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/repl\/session.go:55\r\n\u2502 +0x16c\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/repl.(*Session).Handle(0xc0002f0ff0?, {0xc000056680, 0x1c})\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/repl\/session.go:40\r\n\u2502 +0xcc\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/command.(*ConsoleCommand).modeInteractive(0xc000ab8fc0, 0xc00055b9f8?, {0x39871d0, 0xc0002f0ff0})\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/command\/console_interactive.go:52\r\n\u2502 +0x2bc\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/command.(*ConsoleCommand).Run(0xc000ab8fc0, {0xc000132010?, 0xffffffffffffffff?, 0x0?})\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/command\/console.go:163\r\n\u2502 +0xdfd\r\n\u2502 github.com\/mitchellh\/cli.(*CLI).Run(0xc0006c1400)\r\n\u2502 \t\/Users\/runner\/go\/pkg\/mod\/github.com\/mitchellh\/cli@v1.1.5\/cli.go:262 +0x5f8\r\n\u2502 main.realMain()\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/main.go:315\r\n\u2502 +0x1614\r\n\u2502 main.main()\r\n\u2502 \t\/Users\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/af7ecbd19916af909cc37be5db40af0b5b375d92028f49b059a863eebacc3587\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/main.go:58\r\n\u2502 +0x19\r\n\u2502 .\r\n\u2575\r\n```\n\n### Steps to Reproduce\n\n```\r\nterraform console\r\n```\r\n```\r\nmerge(false ? {a={}} : null)\r\n```\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["It seems the `expression.go` in the `hashicorp\/hcl` file is maybe the root cause of this:\r\n\r\nhttps:\/\/github.com\/hashicorp\/hcl\/blob\/7208bce57fadb72db3a328ebc9aa86489cd06fce\/hclsyntax\/expression.go#L641-L665\r\n```go\r\n      switch {\r\n      \/\/ If either case is a dynamic null value (which would result from a\r\n      \/\/ literal null in the config), we know that it can convert to the expected\r\n      \/\/ type of the opposite case, and we don't need to speculatively reduce the\r\n      \/\/ final result type to DynamicPseudoType.\r\n\r\n\r\n      \/\/ If we know that either Type is a DynamicPseudoType, we can be certain\r\n      \/\/ that the other value can convert since it's a pass-through, and we don't\r\n      \/\/ need to unify the types. If the final evaluation results in the dynamic\r\n      \/\/ value being returned, there's no conversion we can do, so we return the\r\n      \/\/ value directly.\r\n      case trueResult.RawEquals(cty.NullVal(cty.DynamicPseudoType)):\r\n              resultType = falseResult.Type()\r\n              convs[0] = convert.GetConversionUnsafe(cty.DynamicPseudoType, resultType)\r\n      case falseResult.RawEquals(cty.NullVal(cty.DynamicPseudoType)):\r\n              resultType = trueResult.Type()\r\n              convs[1] = convert.GetConversionUnsafe(cty.DynamicPseudoType, resultType)\r\n      case trueResult.Type() == cty.DynamicPseudoType, falseResult.Type() == cty.DynamicPseudoType:\r\n              \/\/ the final resultType type is still unknown\r\n              \/\/ we don't need to get the conversion, because both are a noop.\r\n\r\n\r\n      default:\r\n              \/\/ Try to find a type that both results can be converted to.\r\n              resultType, convs = convert.UnifyUnsafe([]cty.Type{trueResult.Type(), falseResult.Type()})\r\n      }\r\n```\r\nIt is going to get into the `second case` statement and the `resultType` will not match `null`, because the true result type has the `a` key in it. `e.g.: merge(false ? {a={}} : null)`\r\n\r\nAlso, it seems to me that if the `resultType` is `falseResult.Type()` then this should not through the error anymore. Not sure if this approach would cause undesired side-effects.  \r\n\r\nI was just looking for the codebase to learn more Golang. @jbardin If you think this could be a issue for outsiders of Core Team, just let me know more context so I can try to help.\r\n\r\n"],"labels":["bug","config","confirmed"]},{"title":"switch to ergochat\/readline","body":"This switches the Terraform console library from [chzyer\/readline](https:\/\/github.com\/chzyer\/readline), which is unmaintained, to a fork [ergochat\/readline](https:\/\/github.com\/ergochat\/readline), maintained by me and @wader. \r\n\r\nThis will fix a number of bugs, including #33195. It may also make possible Solaris support for the console, which is currently compiled out:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/c42e0ee89c793d8dd34f8c56524f8f6862264fce\/internal\/command\/console_interactive_solaris.go#L18-L20\r\n\r\nI can confirm that our fork builds with `GOOS=solaris`, but I don't have a Solaris environment to confirm that the runtime behavior is as expected.\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #33195\r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.5.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### BUG FIXES\r\n* `terraform console`: Fixed crash from Ctrl-C when searching an empty history","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33305) <br\/>All committers have signed the CLA.","Thanks for this submission! To set expectations, we will need to have this reviewed by the security team and we are in the beta\/RC period for 1.5 so it may be some time before this is reviewed. Thanks again!","Just wanted to check in about this again?","Thanks for the bump. I'll bring it up in triage again, to see if we want to pursue this in the near future. ","(Updating to our latest release, [v0.1.0](https:\/\/github.com\/ergochat\/readline\/releases\/tag\/v0.1.0))","Thanks for this. We are going to work on a process for updating dependencies before new releases. I'll keep you posted. "],"labels":["bug","cli"]},{"title":"Allow recovery from failure of terraform_remote_state data source","body":"### Terraform Version\n\n```shell\nTerraform v1.3.9\r\non darwin_amd64\n```\n\n\n### Use Cases\n\n\r\nAllow for some kind of recovery from a situation where a data source fails to return any data. \r\n\r\nCurrent behavior is if the data source fails, the planning stops. Desired behavior is that somehow we can allow the plan to continue, with the code making up for the missing data source.\r\n\r\nWe have several situations where we would use a data source if it exists, but can proceed without it. For example, find the VPC with the tag \"foo\" or else use the default subnet. Or use the latest EBS snapshot, or the default one if there is not a recent custom one. More examples in #16380.\r\n\r\nI am not picky about how it is implemented. It could be a `lifecycle` setting or wrapping references in `try()` or something like a precondition. \n\n### Attempted Solutions\n\nCurrently we have to manually configure whether or not to use a data source, which limits the automation available from data sources. The whole point of a data source is to read configuration from the state, and \"no configuration\" should be allowed to be treated as a valid configuration. \n\n### Proposal\n\n_No response_\n\n### References\n\nThis is nearly a revival of #16380, which was [closed as \"won't do\"](https:\/\/github.com\/hashicorp\/terraform\/issues\/16380#issuecomment-517282340) because it made the plan indeterminate. Now that Terraform has evolved to read data sources during the plan phase, perhaps it can be implemented without violating Terraform's design goals.\r\n","comments":["Hi @Nuru,\r\n\r\nThanks for filing the issue. While it's true that we are able to plan data sources more accurately now that there is no separate \"refresh\" phase, the overall situation around how data sources are meant to be handled in configuration has not changed. \r\n\r\nThe conditions that determine what constitutes a failure are only known to the provider. This means that ignoring a data source failure gives the user no feedback as to whether it was the expected failure or not. Providers are already capable of making data sources which could return 0 values (plural versions of data sources often do this already), and these can still return a failure when the overall configuration is not valid for some other reason. Ignoring failures during plan also leaves the user open to creating invalid configurations which will evaluate differently during or after apply, leading to more confusing errors which neither terraform nor the provider can give any feedback on.\r\n\r\nSee also #30291 and #28933.\r\n\r\n","@jbardin wrote:\r\n\r\n> Thanks for filing the issue. While it's true that we are able to plan data sources more accurately now that there is no separate \"refresh\" phase, the overall situation around how data sources are meant to be handled in configuration has not changed.\r\n> \r\n> The conditions that determine what constitutes a failure are only known to the provider. ... Providers are already capable of making data sources which could return 0 values....\r\n\r\nOK, so I can make this a specific request for the provider about the specific data source I have in mind. Where do I file a feature request for [terraform_remote_state](https:\/\/developer.hashicorp.com\/terraform\/language\/state\/remote-state-data) (more specifically, with the AWS S3 backend)?","The `terraform_remote_state` data source is internal to terraform, so any proposed changes could be made through this repository. \r\n\r\nOutputs in remote state however are not addressed during the reading of the data source, it always returns a single dynamically typed object containing only the outputs which exist in the remote state. To conditionally inspect an object with unknown attributes you can use the [`try` function](https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/try), which will allow you define the default return value for a missing attribute.\r\n\r\nIf you are requesting that the data source return nothing given an invalid backend configuration, that's a bit more nuanced. It does come back to the original premise though that it makes it very difficult to differentiate between an intentionally invalid configuration and any other configuration or external error. The change in the data source would cause legitimate errors to go unnoticed, and allowing a backend itself to intentionally be misconfigured could also lead to confusing situations. I suspect in this case using an alternate means of storage and different data source are more likely to lead to a solution.","@jbardin wrote:\r\n>If you are requesting that the data source return nothing given an invalid backend configuration, that's a bit more nuanced.\r\n\r\nYes, that is what I am requesting. Specifically, I would like an option that, when set, causes `terraform_remote_state` to hide any errors and return the configured `defaults` unless it can successfully retrieve the configured backend. If it had another output that indicated a configuration error (something suitable for use in a postcondition statement) that would be nice, too. \r\n\r\nThis would allow us to gracefully handle initial deployments and other situations where the remote state has yet to be created without having to set a flag (that we would later have to clear) to indicate that the remote state does not yet exist.\r\n"],"labels":["enhancement","new"]},{"title":"Allow {} to be coerced to object of any type","body":"### Terraform Version\n\n```shell\nTerraform v1.3.9\r\non darwin_amd64\n```\n\n\n### Use Cases\n\nI would like to be able to provide an empty object value that can be used in code rather than have to use conditionals in multiple places.\n\n### Attempted Solutions\n\n### What I would like to work:\r\n\r\nThis is, of course, a simplified example. In practice, the objects are more complex, and `local.enabled` is further refined in additional statements.\r\n\r\n```hcl\r\nlocals {\r\n  coin_flip = false \/\/ random_integer.coin.result > 1\r\n\r\n  # Simulate a data source with\r\n  # count = local.coin_flip ? 1 : 0\r\n  datasource = local.coin_flip ? null : {\r\n    enabled = true\r\n    obj = {\r\n      disabled = {\r\n        enabled = false\r\n        list    = []\r\n      }\r\n      enabled = {\r\n        enabled = true\r\n      }\r\n    }\r\n  }\r\n  obj = local.coin_flip ? {} : local.datasource.obj\r\n\r\n  enabled = { for k, v in local.obj : k => v if v.enabled }\r\n}\r\n\r\noutput \"grr\" {\r\n  value = local.enabled\r\n}\r\n```\r\n\r\nBut it fails:\r\n\r\n```\r\n$ terraform apply\r\n\u2577\r\n\u2502 Error: Inconsistent conditional result types\r\n\u2502 \r\n\u2502   on objects.tf line 18, in locals:\r\n\u2502   18:   obj = local.coin_flip ? {} : local.datasource.obj\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.coin_flip is false\r\n\u2502     \u2502 local.datasource.obj is object with 2 attributes\r\n\u2502 \r\n\u2502 The true and false result expressions must have consistent types. The 'false'\r\n\u2502 value includes object attribute \"disabled\", which is absent in the 'true'\r\n\u2502 value.\r\n\u2575\r\n```\r\n\r\n### What I have had to resort to:\r\n\r\n```hcl\r\n  \/\/ Workaround for inconsistent types in ternary\r\n  \/\/ obj = local.coin_flip ? {} : local.datasource.obj\r\n  obj_map = {\r\n    true  = {}\r\n    false = local.coin_flip ? null : local.datasource.obj\r\n  }\r\n\r\n  obj = local.obj_map[local.coin_flip]\r\n```\r\n\r\n\n\n### Proposal\n\nAllow `{}` to be cast to map or any type of object. \n\n### References\n\n\r\n- Comment from @apparentlymart https:\/\/github.com\/hashicorp\/terraform\/issues\/33237#issuecomment-1558146937\r\n- #33294","comments":["Hi @Nuru! Thanks for sharing this use-case.\r\n\r\nUnfortunately I don't think exactly what you've proposed is an appropriate answer to the use-case: `{}` cannot be considered as a subtype of any object type that has attributes, because it doesn't have any attributes. Conversion to an object type with an attribute `foo` cannot meaningfully succeed unless it would be valid to access `.foo` on the resulting value.\r\n\r\nIn the case you've described, I would expect to use `null` as the representation of \"no object\", like this:\r\n\r\n```hcl\r\n  obj = local.coin_flip ? null : local.datasource.obj\r\n```\r\n\r\n`null` is already specified as being convertible to any type, so it is always available as a way to represent the absence of something.\r\n\r\nAlternatively, a list of zero or one elements if in a particular situation it's more convenient to think of this as being a bounded list rather than a single value that's either present or not:\r\n\r\n```hcl\r\n  obj = local.coin_flip ? tolist([]) : tolist([local.datasource.obj])\r\n```\r\n\r\nFinally, if you are intending your `local.datasource.obj` to really be a map of objects rather than an object itself (unclear from the example) then you can help Terraform understand that by writing an explicit conversion to remove the ambiguity:\r\n\r\n```hcl\r\n  obj = local.coin_flip ? tomap({}) : tomap(local.datasource.obj)\r\n```\r\n\r\nHowever, the given example for `local.datasource.obj` cannot convert to a map because the element types don't match. It would be necessary to add `list = null` or `list = tolist([])` to the `enabled` element so that the result type can be `map(object({enabled = bool, list = list(unknown)}))`. (The list element type is unknown here because neither of the objects have an example element to infer the element type from, but that's okay when the list is empty anyway since accessing an element of an empty list can never succeed regardless of type.)\r\n\r\nWould any of these three solutions be suitable for what you are trying to achieve? If not, can you say more about why? Thanks!\r\n\r\n---\r\n\r\nAnother possible way to interpret this request would be to have some way to request an _explicit_ type conversion to an object type with optional attributes. Here's a hypothetical syntax for that which we've prototyped in the past:\r\n\r\n```hcl\r\n# INVALID: This is just an example of a possible future syntax\r\nconvert({}, map(object({\r\n  enabled = optional(bool, false)\r\n  list    = optional(list(string), [])\r\n})))\r\n```\r\n\r\nUnfortunately the only reason we haven't already implemented something like this is legacy constraints: this would be the first time that a type constraint could appear anywhere other than the `type` argument of a `variable` block and Terraform's dependency analyzer doesn't understand how to ignore it in this position and avoid e.g. misinterpreting `bool` as a resource type rather than a keyword. We hope to address that at some point, but are not currently prioritizing it.\r\n\r\nIf we were able to complete this then it would have the same behavior as passing a value through in input variable with the same type constraint.\r\n\r\n(Note for future maintainers reading this: the HCL part of the `convert` function is [still available in HCL's source code](https:\/\/github.com\/hashicorp\/hcl\/blob\/7208bce57fadb72db3a328ebc9aa86489cd06fce\/ext\/typeexpr\/type_type.go#L92-L120). What we haven't yet solved is how to integrate that into Terraform in a way that doesn't upset Terraform's own dependency analyzer, which HCL is not aware of. One candidate solution is to teach the expression analyzer that `string`, `bool`, `number`, and `any` are all valid symbols that it should ignore, but that then raises the question of how to properly handle the error of using those keywords in a non-type-expression context, which remains unsolved at the time I'm writing this.)\r\n\r\n","@apparentlymart Thank you for your detailed response. Unfortunately, none of your suggestions help me.\r\n\r\n\r\nI want some kind of value I can put in the conditional\r\n\r\n```hcl\r\nobj = local.coin_flip ? <SOMETHING> : local.datasource.obj\r\n```\r\n\r\nthat will ensure\r\n\r\n```hcl\r\nenabled = { for k, v in local.obj : k => v if v.enabled }\r\n```\r\n\r\nworks regardless of the coin flip. This is because in the actual code, there are close to 30 lines of additional transformations performed on `enabled`, and I do not want to have to have conditionals on every line. Unfortunately, `null` does not satisfy the need here. \r\n\r\nI understand your comment about `{}` not being a subtype of object, but perhaps you could code it as a special case. I think of it more as an empty object, similar to `[]` being an empty list. I can write\r\n\r\n```hcl\r\nobj = local.coin_flip ? [] : [local.datasource.obj]\r\n```\r\n\r\nand Terraform does not complain about the empty tuple as being a different type. To me it is a parallel construction to write\r\n\r\n```hcl\r\nobj = local.coin_flip ? {} : local.datasource.obj\r\n```\r\n\r\nI would NOT expect\r\n\r\n```hcl\r\nobj = local.coin_flip ? {foo = \"bar\"} : local.datasource.obj\r\n```\r\n\r\nto work, but like I said, I think `{}` should be a special case.\r\n\r\n<details><summary>Side note: When the placeholder is a local (click to reveal)<\/summary>\r\n\r\nBTW, I understand the problem with the following (`placeholder` has a type, even though the value is `null`), but the error message is very confusing:\r\n\r\n```hcl\r\n  placeholder = local.coin_flip ? {} : null\r\n  obj = local.coin_flip ? local.placeholder : local.datasource.obj\r\n```\r\n\r\n```\r\n\u2502 Error: Inconsistent conditional result types\r\n\u2502 \r\n\u2502   on objects.tf line 20, in locals:\r\n\u2502   20:   obj = local.coin_flip ? local.placeholder : local.datasource.obj\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.coin_flip is false\r\n\u2502     \u2502 local.datasource.obj is object with 2 attributes\r\n\u2502     \u2502 local.placeholder is null\r\n\u2502 \r\n\u2502 The true and false result expressions must have consistent types. The 'false'\r\n\u2502 value includes object attribute \"disabled\", which is absent in the 'true'\r\n\u2502 value.\r\n```\r\n\r\n<\/details> \r\n\r\nIn my particular case, I do not know the full type that will be returned by the data source, so I could not cast it into an object type even if that feature were available. It is not a map, so I cannot cast it to a map. \r\n\r\n\r\nAn alternative solution would be to allow `null` in a `for` statement, so that `for k, v in null` works like `for k, v in {}`, but that seems more drastic to me. ","In order for `v.enabled` to work as you showed in your first example, the following would need to be true:\r\n\r\n* `v` has an attribute named `enabled`\r\n* `v.enabled` is either `true` or `false`. (`null` would not work because `if` requires a non-null condition)\r\n\r\nTherefore to make that work would require Terraform to automatically set that attribute to either `false` or `true`, but Terraform doesn't have enough information to know which of those values would be appropriate here.\r\n\r\nTherefore I don't think an automatic approach is appropriate here. You will always need to _at least_ write out an `enabled` attribute that is either `true` or `false`.\r\n\r\nWith today's Terraform to meet your requirement I would define another local value that has the placeholder object to use to represent the absence of an object, and then use that in your expression:\r\n\r\n```hcl\r\nlocals {\r\n  placeholder_something = {\r\n    enabled = false\r\n    list    = tolist([]) # or null, if that seems better\r\n  }\r\n  obj = local.coin_flip ? local.datasource.obj : local.placeholder_something\r\n}\r\n```\r\n\r\n (I called this \"placeholder _something_\" just because the example we're discussing here is so contrived that I don't know what kind of thing `local.datasource.obj` represents to choose a better name.)\r\n\r\nAlternatively, you can make the conditional produce `null` and then handle the fallback to the placeholder at the reference point instead:\r\n\r\n```hcl\r\nenabled = { for k, v in local.obj : k => v if try(v.enabled, false) }\r\n```\r\n\r\nThis is a more concise approach but it's also less precise, and would mask other errors such as `v` not being an object at all. But it seems like you are prioritizing writing less code over being explicit, so you might prefer this option.\r\n","@apparentlymart wrote:\r\n\r\n>In order for `v.enabled` to work as you showed in your first example, the following would need to be true:\r\n>\r\n>    `v` has an attribute named `enabled`\r\n>    `v.enabled` is either true or false. (null would not work because if requires a non-null condition)\r\n\r\nNo, the code works fine if `local.obj` is an empty object or map. There are no keys, so `for k, v in local.obj` does not execute the generator statement. \r\n\r\nI cannot flesh out `placeholder_something` because I do not know all the elements `local.obj` is going to have. I know it has some I am looking for (or else it is fine for that to be an error), but it has extraneous stuff that can vary.\r\n\r\nI am looking for a more precise solution, otherwise I could solve this with `try()` at the expense of masking a lot of errors I would rather see.\r\n","Thanks for the extra context, @Nuru. I thought you were intending `v` to be the empty object, not `local.obj`.\r\n\r\nIt isn't a goal of the Terraform language to help you write code that accepts arbitrary input regardless of type. We generally expect that you, as the module author, will decide exactly what type your module expects as input and return errors if the given input doesn't match that type.\r\n\r\nThat seems in direct conflict with your goal of having \"extraneous stuff that can vary\". I would suggest choosing a different approach where you design your API explicitly, and so that every expression has a well-defined result type regardless of input. Consider the Terraform language as having a static type system, not a dynamic type system.\r\n","@apparentlymart This is not about an input I can declare and control, this is about data returned by a data source, which is beyond my control, much like Terraform allows for JSON input. \r\n\r\nI think Hashicorp opened the door by allowing objects to be treated like maps in `for` statements (for which I am grateful). We have the empty list\/tuple, which the ternary operator accepts as an alternative to a tuple of any type, and which `for` accepts as an empty list. I am just asking for something analogous for objects. Otherwise I have to jump through this indirection via an intermediate object, which both shows Terraform can handle the situation internally and how awkward it is currently."],"labels":["enhancement","config"]},{"title":"Sidebar filter or search not working as expected","body":"### Terraform Version\n\n```shell\nnot relavent\n```\n\n\n### Affected Pages\n\nhttps:\/\/registry.terraform.io\/providers\/aliyun\/alicloud\/latest\/docs\n\n### What is the docs issue?\n\nGo to the sidebar and paste the string: `custom_image`\r\nyou'll see there are no results, which highly confused me to think that there are no such resource\/data available.\r\n\r\nTry: `alicloud_simple_application_server_custom_image` - works\r\nTry: `custom` - works\r\nTry: `custom_` - doesn't work\r\nTry: `image` - works but see that `alicloud_simple_application_server_custom_image` isn't found in the results (?!)\r\n\n\n### Proposal\n\nGive us the ability to filter all parts of the string including underscore etc ...\n\n### References\n\n_No response_","comments":["Thnks for this request, I've notified the Registry team."],"labels":["enhancement","registry"]},{"title":"tf1.5: terraform init -from-module emits a warning for ignoring backend config","body":"### Terraform Version\r\n\r\n```shell\r\n# terraform -v\r\nTerraform v1.5.0-beta2\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nterraform {\r\n  backend \"local\" {\r\n    path = \"root.tfstate\"\r\n  }\r\n}\r\n\r\nresource \"terraform_data\" \"foo\" {}\r\n```\r\n\r\n### Debug Output\r\n\r\nhttps:\/\/gist.github.com\/minamijoyo\/3679077f64886cdb226d1e9bc4609977\r\n\r\n### Expected Behavior\r\n\r\nNo warning\r\n\r\n### Actual Behavior\r\n\r\n```\r\n\u2577\r\n\u2502 Warning: Backend configuration ignored\r\n\u2502\r\n\u2502   on .terraform\/init-from-module\/root\/main.tf line 2, in terraform:\r\n\u2502    2:   backend \"local\" {\r\n\u2502\r\n\u2502 Any selected backend applies to the entire configuration, so Terraform expects provider configurations only in the\r\n\u2502 root module.\r\n\u2502\r\n\u2502 This is a warning rather than an error because it's sometimes convenient to temporarily call a root module as a\r\n\u2502 child module for testing purposes, but this backend configuration block will have no effect.\r\n\u2575\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\n1. create mod1\/main.tf which includes a backend block.\r\n2. mkdir root1 && cd root1\r\n3. terraform init -from-module=..\/mod1\/\r\n\r\n### Additional Context\r\n\r\nI use the `init -from-module` option to set up a testing environment quickly. I first found this issue when using the s3 backend configuration, but it is also easily reproduced with the local backend. I\u2019m unsure if this is a bug or intentional behavior, but I can see this warning starting from the v1.5 beta. I cannot reproduce this issue with the latest stable v1.4.6.\r\n\r\nA module used by the `init -from-module` option appears to be expected to initialize the root module, so I feel it\u2019s natural to have a backend configuration. Please let me know if I'm wrong. Suppose the module should not have a backend configuration; in that case, at least the warning message should not point out an ephemeral temporary file under the `.terraform\/`, which is an implementation detail and not desirable to show to the end user.\r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @minamijoyo! Thanks for reporting this.\r\n\r\nThe `-from-module` argument to `terraform init` is a strange feature because it does something that's not normally possible to do anywhere else in Terraform: using a shared module as if it were a root module.\r\n\r\nTherefore I think it's debatable how it should behave in this case. The current behavior results from this currently being implemented effectively as generating a synthetic root module that calls the specified module, initializing that temporary configuration (with the specified module therefore being treated as shared), and then copying the resulting module directory into the current working directory. This is therefore consistent with the idea that modules retrieved over the network, rather than from the current working directory, are always shared modules.\r\n\r\nOn the other hand, if you think of this option as just a weird Terraform-specific way of retrieving a source package and extracting it into the current directory, in that case it could be reasonable for the module to contain anything at all, and to expect it to behave exactly like what would happen if you fetched that code separately (outside Terraform) and then ran `terraform init` in the resulting directory.\r\n\r\nIn practice I think it's unlikely that we'll significantly redesign this behavior now because it's really only still in Terraform because Terragrunt depends on it, and Terragrunt's intended abstraction is to make shared modules behave like root modules using code generation after retrieving the source code, where the backend configuration should therefore be added by Terragrunt as a followup step rather than it being included in the main module source code. Our main priority with that option is to help Terragrunt offer this \"pretend that a shared module is a root module\" abstraction, and so we don't intend it for any other use.\r\n\r\nPerhaps the best compromise here would be to document better what that option is intended for and to discourage its use outside of that specific situation in Terragrunt.\r\n\r\n\r\n","Hi @apparentlymart, Thanks for your reply.\r\n\r\nYeah, it is indeed a strange feature, as you say. I don't have a strong opinion on how this option should behave since, in my use case, it can be replaced by simply copying and then initializing. If it\u2019s deprecated to use this option except for Terragrunt, I will refrain from using it.\r\n\r\nI will leave this issue open until the documentation is fixed. Thanks!"],"labels":["enhancement","documentation","cli"]},{"title":"Accept recursive calls to templatefile()","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.6.0-dev\r\non windows_amd64\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWe're going to use templatefile() for creating Azure Devops templates. For easier maintenance we want to be able to import templates from templates.\r\n\r\n### Attempted Solutions\r\n\r\nmain.tf\r\n```\r\nresource \"local_file\" \"pipeline-templater\" {\r\n  content  = templatefile(\"pipeline.tftpl\", { vars = local.variables })\r\n  filename = \"pipeline.yaml\"\r\n}\r\n```\r\n\r\npipeline.tftpl\r\n```\r\n${ templatefile(\"template_header.tftpl\",{vars = vars}) }\r\n<pipeline goes here>\r\n```\r\n\r\ntemplate_header.tftpl\r\n```\r\npool: ${vars.agent_pool}\r\n```\r\n\r\nThat gives the following error:\r\n```\r\nError in function call; Call to function\r\n\u2502 \"templatefile\" failed: cannot recursively call templatefile from inside templatefile call..\r\n```\r\n\r\n### Proposal\r\n\r\nIt is well documented behavior:\r\n\r\nhttps:\/\/developer.hashicorp.com\/terraform\/language\/functions\/templatefile\r\n```\r\nThe template may also use any other function available in the Terraform language, except that recursive calls to templatefile are not permitted. \r\n```\r\n\r\nFrom the source code:\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/bc216caa65e0ca5e07dbfbbe5833790562091a5d\/internal\/lang\/funcs\/filesystem.go#LL69C1-L71C38\r\n```\r\n\/\/ As a special exception, a referenced template file may not recursively call\r\n\/\/ the templatefile function, since that would risk the same file being\r\n\/\/ included into itself indefinitely.\r\n```\r\nI don't think terraform should decided whether I can call templatefile() from a template or not. It works if I remove that check from the source code and compile terraform, so I suggest some mechanism should be implemented that informs the user, if the user has accidentally imported the same file info itself.  Some recursion limit maybe ? \r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this request!","@crw Any update on this  ? Please let me know if I should create a PR :) ","@Exchizz, thanks for the great research in the bug description. Usually when there is a well-documented restriction in the Terraform language, it is due to considered trade-offs in the design. I will bring this up in triage but my guess is that a PR that changes this behavior would not be accepted given the intentional design decision to not allow recursion in `templatefile`.","@crw Hm, alright - thanks :)\r\n\r\nWhat if it was an environment variable or an option to terraform that controls whether templatefile() can be called from the template ? \r\nExample:\r\n```\r\nTF_ALLOW_TEMPLATEFILE_FROM_TEMPLATE=true\r\n```\r\n\r\nHaving this option to include files before running the templater would really improve the templating mechanism in terraform.","As someone also invested in this: would it be possible to clarify the reason for the design decision, if this is rejected?  \r\nFrom the comment in the source code it reads to me to as _\"there is a potential for edge cases entering infinite loops, therefore we disable the entire feature\"_, which seems questionable. If that is the reason for the design choice, would a PR that implements recursion limits (or an explicit opt-in, as in @Exchizz's comment) be acceptable?","@crw  Any update on this :) ? \r\n","Yes, I brought this up in triage and the bottom-line feedback is that this issue would need more interest to be considered, particularly as it would mean revisiting a purposeful design decision. The code comment is a bit of a simplification, this was specifically engineered to be restricted to prevent more complicated design patterns (\"programming with templates\", as you might see with Jinja2 or Django). Even with the opt-in, the team would then need to support the more complicated logic of templates-calling-templates. Thus, needing to see more interest in such a design to justify the long-term support implications. We appreciate the feedback and of course will leave this request open to generate more feedback and use cases for this feature. Thanks!","I would love this feature for my use case, though I understand the reasoning to protect the current design.\r\n\r\nIn our case, we have a rather large startup script for one of our Google Cloud Compute instances:\r\n\r\n```\r\n# main.yml\r\nresource \"google_compute_instance\" \"some_compute_instance\" {\r\n  name                      = var.app_name\r\n  # ...\r\n  metadata = {\r\n    startup-script               = templatefile(\"${path.module}\/startup-script.tftpl\", { var: var })\r\n  }\r\n}\r\n```\r\n\r\nIt would be useful to extract pieces of the startup-script into their own templates for ease of maintainability. Then the startup-script would mainly compose those templates together.\r\n\r\n```\r\n# startup-script.tftpl\r\n#! \/bin\/bash\r\n\r\n# Setup authentication\r\n${templatefile(\"authentication.tftpl\", {var: var})}\r\n\r\n# Add configuration\r\n${templatefile(\"configuration.tftpl\", {var: var})}\r\n```\r\n \r\n It's a simple use case and we can use other tooling or techniques to accomplish our goals, but I would still love to have a feature like this in Terraform.","Same situation here.\r\n\r\nI'm using `cloud-init` to setup all my droplets without using any other tool.\r\n\r\nIn my case I have a `boostrap.tftpl` with the basic setup that must run if there's no specific setup for the given droplet.\r\n\r\n\r\n```yaml\r\n  # droplets\r\n  user_data = templatefile(\r\n    fileexists(\"templates\/${each.value.droplet_name}.tftpl\")\r\n    ? \"templates\/${each.value.droplet_name}.tftpl\"\r\n    : \"templates\/base\/boostrap.tftpl\", {\r\n      username   = each.value.droplet_user\r\n      keys_path  = \"${var.config.keys_path}\/${each.value.env_short_name}\"\r\n      name       = each.value.droplet_name\r\n    }\r\n  )\r\n```\r\n\r\nBut I there is a specific template for the droplet (e.g. `droplet-acme.tftpl`) it must be the choose one, but not excluding the boostrap:\r\n```yaml\r\n#cloud-config\r\n\r\n${file(\"templates\/base\/boostrap.tftpl\")}\r\n${file(\"templates\/base\/swarm-worker-fw.tftpl\")}\r\n\r\nusers:\r\n  - name: spaces\r\n    ssh_authorized_keys:\r\n      - ${file(\"${keys_path}\/id_rsa-labs-manager.pub\")}\r\n\r\n  - name: students\r\n    ssh_authorized_keys:\r\n      - ${file(\"${keys_path}\/id_rsa-labs-worker.pub\")}\r\n...\r\n```\r\n\r\nI can try to use the old `template_file` data set to merge all files I need in a big template for each droplet, but for some reason it makes me feel uncomfortable. \r\n\r\nIs there any workaround\/best-practice to implement this kind of setup?\r\n\r\nCheers,","@crw, Any update ?","No new updates since my previous comment: https:\/\/github.com\/hashicorp\/terraform\/issues\/33272#issuecomment-1608051710d "],"labels":["enhancement","new"]},{"title":"Display error \"Failed to persist state to backend\" more prominently","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.4.5\r\non darwin_arm64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\nI currently have about 100 resources to-be-provisioned at once (an EKS cluster). As I can't share the configuration, probably any similar configuration, such as [EKS Blueprints](https:\/\/github.com\/aws-ia\/terraform-aws-eks-blueprints), might also result in the experienced token expiry.\r\n\r\n### Debug Output\r\n```\r\n\u2577\r\n\u2502 Error: Failed to save state\r\n\u2502 \r\n\u2502 Error saving state: failed to upload state: ExpiredToken: The provided token has expired.\r\n\u2502       status code: 400, request id: 1MKEH3235PP8692E, host id: dX9gkgaLJS6+94l7VGC9sQxT8tsHrt5uLJ1rEDkN67Z6rgiqul7NqbPCZEWv8OqxByrMeNkbYPU=\r\n\u2575\r\n\u2577\r\n\u2502 Error: Failed to persist state to backend\r\n\u2502 \r\n\u2502 The error shown above has prevented Terraform from writing the updated state to the configured backend. To allow for recovery, the state has\r\n\u2502 been written to the file \"errored.tfstate\" in the current working directory.\r\n\u2502 \r\n\u2502 Running \"terraform apply\" again at this point will create a forked state, making it harder to recover.\r\n\u2502 \r\n\u2502 To retry writing this state, use the following command:\r\n\u2502     terraform state push errored.tfstate\r\n\u2502 \r\n|\r\n\u2502 Error:  <Slew of other errors such that the above \"Failed to persist state to backend\" one can easily be missed>\r\n| ...\r\n```\r\n\r\n### Expected Behavior\r\n\r\nIdeally, the `ExpiredToken` error would've been avoided altogether by refreshing it in a background. With this not being an option, or when the token expires regardless, the \"Failed to persist state to backend\" error is shown last because it must be (not) acted upon. One should either, as instructed:\r\nA) retry writing the state in `errored.tfstate`, or\r\nB) _not_ retry `terraform apply`\r\n\r\n### Actual Behavior\r\n\r\n\"Failed to persist state to backend\" error is always tucked away as the 2nd error, immediately after the \"Failed to save state\" one.\r\n\r\n### Steps to Reproduce\r\n\r\n1. terraform init\r\n2. terraform apply\r\n\r\n### Additional Context\r\n\r\n_No response_\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for filing this request!"],"labels":["enhancement","cli","UX"]},{"title":"Unable to concat lists containing complex objects into list","body":"### Terraform Version\r\n\r\n```shell\r\n1.4.6\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n```terraform\r\nlocals {\r\n\r\n  list_object_a_enabled = true\r\n  list_object_a = tolist([\r\n    {\r\n      id      = \"a\"\r\n      enabled = true\r\n\r\n      nested = {\r\n        nested_id    = 0\r\n        nested_class = \"test\"\r\n      }\r\n    }\r\n  ])\r\n\r\n  list_object_b = tolist([\r\n    {\r\n      id         = \"b\"\r\n      enabled    = true\r\n      expiration = { days = 31 }\r\n    }\r\n  ])\r\n\r\n  list_of_objects_ternary             = local.list_object_a_enabled == true ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n  list_of_objects_ternary_with_tolist = local.list_object_a_enabled == true ? tolist(concat(local.list_object_a, local.list_object_b)) : local.list_object_b\r\n  list_of_objects_ternary_false       = local.list_object_a_enabled == false ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n}\r\n```\r\n\r\n\r\n### Debug Output\r\n```\r\n\u2502 Error: Inconsistent conditional result types\r\n\u2502\r\n\u2502   on main.tf line 58, in locals:\r\n\u2502   58:   list_of_objects_ternary             = local.list_object_a_enabled == true ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.list_object_a is list of object with 1 element\r\n\u2502     \u2502 local.list_object_a_enabled is true\r\n\u2502     \u2502 local.list_object_b is list of object with 1 element\r\n\u2502\r\n\u2502 The true and false result expressions must have consistent types. The 'true' value is tuple, but the 'false' value is list of object.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Invalid function argument\r\n\u2502\r\n\u2502   on main.tf line 59, in locals:\r\n\u2502   59:   list_of_objects_ternary_with_tolist = local.list_object_a_enabled == true ? tolist(concat(local.list_object_a, local.list_object_b)) : local.list_object_b\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 while calling tolist(v)\r\n\u2502     \u2502 local.list_object_a is list of object with 1 element\r\n\u2502     \u2502 local.list_object_b is list of object with 1 element\r\n\u2502\r\n\u2502 Invalid value for \"v\" parameter: cannot convert tuple to list of any single type.\r\n\u2575\r\n\r\n\u2577\r\n\u2502 Error: Inconsistent conditional result types\r\n\u2502\r\n\u2502   on main.tf line 60, in locals:\r\n\u2502   60:   list_of_objects_ternary_false       = local.list_object_a_enabled == false ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.list_object_a is list of object with 1 element\r\n\u2502     \u2502 local.list_object_a_enabled is true\r\n\u2502     \u2502 local.list_object_b is list of object with 1 element\r\n\u2502\r\n\u2502 The true and false result expressions must have consistent types. The 'true' value is tuple, but the 'false' value is list of object.\r\n```\r\n\r\n### Expected Behavior\r\n\r\nIf I concat lists, I expect a list to be returned, not a tuple. This seems to result from complex objects (like nested maps) being involved in the concatenation. I haven't noticed this if the lists contained string or bool or int. Perhaps this has already been found or the behavior is expected.\r\n\r\n### Actual Behavior\r\n\r\nSee errors\r\n\r\n### Steps to Reproduce\r\n\r\nAttempt to concat two lists that contain complex objects (like a map)\r\n\r\n### Additional Context\r\n\r\nIt'd be useful to concat these types of objects together, particularly in use for modules. I have lists containing complex object variables I'm trying to pass into modules and then perform some modifications on top of those variables if certain conditions are met. Sometimes these variables have a certain nested map in them, sometimes they don't. Sometimes it might just be string fields instead of string fields + map objects within an object in the list.  \r\n\r\n### References\r\n\r\n_No response_","comments":["Hi @mforeman19! Thanks for reporting this.\r\n\r\nLooking at this part of your example:\r\n\r\n```hcl\r\nconcat(local.list_object_a, local.list_object_b) \r\n```\r\n\r\nThese two lists seem to have different element types, so they cannot be concatenated together into a single list with a single element type. Looking at today's implementation of `concat` I see that it does _try_ to return a list, but it can do so only if all of the given lists have compatible element types.\r\n\r\nYour example with `list_of_objects_ternary_with_tolist` seems to confirm that's what's going on, because `tolist` uses the same logic to try to determine a common element type as `concat` does. The difference is just that `concat` falls back on creating a tuple if it cannot create a list, whereas `tolist` has no option but to fail because it would be invalid for it to return a non-list type.\r\n\r\nI expect this would work if you made the two lists have unifyable object types, so that Terraform can find a suitable single object type to use as the type of the resulting list:\r\n\r\n```hcl\r\nlocals {\r\n\r\n  list_object_a_enabled = true\r\n  list_object_a = tolist([\r\n    {\r\n      id      = \"a\"\r\n      enabled = true\r\n      expiration = null\r\n\r\n      nested = {\r\n        nested_id    = 0\r\n        nested_class = \"test\"\r\n      }\r\n    }\r\n  ])\r\n\r\n  list_object_b = tolist([\r\n    {\r\n      id         = \"b\"\r\n      enabled    = true\r\n      expiration = { days = 31 }\r\n      nested = null\r\n    }\r\n  ])\r\n\r\n  list_of_objects_ternary             = local.list_object_a_enabled == true ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n  list_of_objects_ternary_with_tolist = local.list_object_a_enabled == true ? tolist(concat(local.list_object_a, local.list_object_b)) : local.list_object_b\r\n  list_of_objects_ternary_false       = local.list_object_a_enabled == false ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n}\r\n```\r\n\r\nThe extra `null` attributes I added above should allow Terraform to determine that the result type of `concat` would be the following list type, after noticing that `null` is a valid value of any type:\r\n\r\n```hcl\r\nlist(object({\r\n  id = string\r\n  enabled = bool\r\n  expiration = object({\r\n    days = number\r\n  })\r\n  nested = object({\r\n    nested_id    = number\r\n    nested_class = test\r\n  })\r\n}))\r\n```\r\n\r\nCan you give that variant a try and see if you get the behavior you wanted?","Hey @apparentlymart, I gave that a go and got this error back:\r\n```\r\n\u2502 Error: Error in function call\r\n\u2502\r\n\u2502   on main.tf line 128, in locals:\r\n\u2502  128:   list_of_objects_ternary             = local.list_object_a_enabled == true ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 while calling concat(seqs...)\r\n\u2502     \u2502 local.list_object_a is list of object with 1 element\r\n\u2502     \u2502 local.list_object_b is list of object with 1 element\r\n\u2502\r\n\u2502 Call to function \"concat\" failed: panic in function implementation: inconsistent list element types (cty.Object(map[string]cty.Type{\"enabled\":cty.Bool,\r\n\u2502 \"expiration\":cty.DynamicPseudoType, \"id\":cty.String, \"nested\":cty.Object(map[string]cty.Type{\"nested_class\":cty.String, \"nested_id\":cty.Number})}) then\r\n\u2502 cty.Object(map[string]cty.Type{\"enabled\":cty.Bool, \"expiration\":cty.Object(map[string]cty.Type{\"days\":cty.Number}), \"id\":cty.String, \"nested\":cty.DynamicPseudoType}))   \r\n\u2502 goroutine 798 [running]:\r\n\u2502 runtime\/debug.Stack()\r\n\u2502       \/opt\/hostedtoolcache\/go\/1.19.6\/x64\/src\/runtime\/debug\/stack.go:24 +0x65\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.errorForPanic(...)\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/error.go:44\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.Function.Call.func1()\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/function.go:294 +0x9f\r\n\u2502 panic({0x260daa0, 0xc0032750a0})\r\n\u2502       \/opt\/hostedtoolcache\/go\/1.19.6\/x64\/src\/runtime\/panic.go:884 +0x212\r\n\u2502 github.com\/zclconf\/go-cty\/cty.ListVal({0xc001bb92c0, 0x2, 0x2511ae0?})\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/value_init.go:166 +0x42e\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function\/stdlib.glob..func79({0xc001bb90c0, 0x2, 0x2511ae0?}, {{0x3072158?, 0xc003274ae0?}})\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/stdlib\/sequence.go:108 +0x87f\r\n\u2502 github.com\/zclconf\/go-cty\/cty\/function.Function.Call({0x3072158?}, {0xc001bb90c0?, 0x2, 0x2})\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/zclconf\/go-cty@v1.12.1\/cty\/function\/function.go:298 +0x2a4\r\n\u2502 github.com\/hashicorp\/hcl\/v2\/hclsyntax.(*FunctionCallExpr).Value(0xc0001b6690, 0xc0024204b0)\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/hashicorp\/hcl\/v2@v2.16.2\/hclsyntax\/expression.go:456 +0x1c85\r\n\u2502 github.com\/hashicorp\/hcl\/v2\/hclsyntax.(*ConditionalExpr).Value(0xc0001dd340, 0xc0024204b0)\r\n\u2502       \/home\/runner\/go\/pkg\/mod\/github.com\/hashicorp\/hcl\/v2@v2.16.2\/hclsyntax\/expression.go:634 +0x4a\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/lang.(*Scope).EvalExpr(0xc000ff0a20, {0x3070ec0?, 0xc0001dd340}, {{0x3072120?, 0x4518310?}})\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/lang\/eval.go:171\r\n\u2502 +0x148\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/terraform.(*BuiltinEvalContext).EvaluateExpr(0x3070ec0?, {0x3070ec0, 0xc0001dd340}, {{0x3072120?, 0x4518310?}}, {0x0?, 0x0?})    \r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/terraform\/eval_context_builtin.go:283\r\n\u2502 +0xc5\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/terraform.(*NodeLocal).Execute(0x0?, {0x308a1d8, 0xc000f8e0e0}, 0x0?)\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/terraform\/node_local.go:154\r\n\u2502 +0x5aa\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/terraform.(*ContextGraphWalker).Execute(0xc0022fb0e0, {0x308a1d8, 0xc000f8e0e0}, {0x15bd63baac0, 0xc000fe2cf0})\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/terraform\/graph_walk_context.go:136\r\n\u2502 +0xc2\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/terraform.(*Graph).walk.func1({0x28397c0, 0xc000fe2cf0})\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/terraform\/graph.go:75\r\n\u2502 +0x315\r\n\u2502 github.com\/hashicorp\/terraform\/internal\/dag.(*Walker).walkVertex(0xc000ad64e0, {0x28397c0, 0xc000fe2cf0}, 0xc000162040)\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/dag\/walk.go:381\r\n\u2502 +0x2f6\r\n\u2502 created by github.com\/hashicorp\/terraform\/internal\/dag.(*Walker).Update\r\n\u2502       \/home\/runner\/work\/_temp\/actions-go-build\/0.1.7\/b712d316a0484806ec3fa2c744c064f8874ceff1\/verification\/ffa6843d8160ea6b18c9f31a4cfafe4e85dc17655c94b0feed83b13a6dcf747a\/cache\/source\/terraform\/terraform\/6c2c6cfa1b55bd6ff4cf4e26ef86d8d7ab0465ec\/internal\/dag\/walk.go:304\r\n\u2502 +0xf65\r\n\u2502 .\r\n```\r\n\r\nWhere this is the input:\r\n```\r\nlist_object_a_enabled = true\r\n  list_object_a = tolist([\r\n    {\r\n      id      = \"a\"\r\n      enabled = true\r\n      nested = {\r\n        nested_id    = 0\r\n        nested_class = \"test\"\r\n      }\r\n      expiration = null\r\n    }\r\n  ])\r\n\r\n  list_object_b = tolist([\r\n    {\r\n      id         = \"b\"\r\n      enabled    = true\r\n      nested = null\r\n      expiration = { days = 31 }\r\n    }\r\n  ])\r\n```","concat seems to work fine if you have non-complex types within the objects in comparison. Here's an example where one object has attributes that the other does not:\r\n```\r\nlist_object_a_enabled = true\r\n  list_object_a = tolist([\r\n    {\r\n      id      = \"a\"\r\n      enabled = true\r\n      in_a = true\r\n    }\r\n  ])\r\n\r\n  list_object_b = tolist([\r\n    {\r\n      id         = \"b\"\r\n      enabled    = true\r\n      in_b = \"in_b\"\r\n    }\r\n  ])\r\n\r\n  list_of_objects_ternary             = local.list_object_a_enabled == true ? concat(local.list_object_a, local.list_object_b) : local.list_object_b\r\n```\r\n\r\nWhere the output from grabbing type is:\r\n```\r\n> type(local.list_of_objects_ternary)\r\nlist(map(string))\r\n```","Thanks for trying that @mforeman19!\r\n\r\nBased on that \"panic\" it seems like we've found a bug in the implementation of the `concat` function where it's _trying_ to behave as you wanted and as I claimed it should -- it tried to construct a list value to return -- but it's done so incorrectly and so it crashed at the last moment trying to assemble the resulting list.\r\n\r\nSince we're already in a bug report issue anyway I think we should consider this issue to represent fixing that crash bug so the function will correctly return the list it was trying to return. The implementation of this function is actually in a third-party library upstream so we'll need to contribute a fix upstream to that, but we can still use this issue to track upgrading that dependency in Terraform once fixed so that Terraform will benefit from the fix.\r\n\r\nBased on what was reported in the error message, my guess as to the cause is that it seems to be trying to build the resulting list from the values given directly as arguments, rather than using the results of converting the arguments to match the selected result type. I'm basing this on the fact that the two types mentioned in the error message still have `cty.DynamicPseudoType`, which is the placeholder used to represent the fact that `null` can be of any type, but by the time it's constructing its result those should have already been converted to the concrete result element type.\r\n","Sounds good, appreciate the quick response @apparentlymart. If it helps for context, most of the testing I was doing was via running terraform console on a local plan (not stored in like s3). ","I am interested to work on this issue. \r\nCan any one help me from where to start ? ","Hi @jay-laitmatus, please see https:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md#contributing-a-pull-request. To set expectations, this is unlikely to be a good first issue to learn how to contribute to Terraform. ","My relevant terraform code:\r\n\r\n* `variables.tf`:\r\n```hcl\r\nvariable \"home_block_device\" {\r\n  description = \"Settings for the home block device\"\r\n  type        = object({\r\n      device_name           = string,\r\n      delete_on_termination = optional(bool, false),\r\n      encrypted             = optional(bool, false),\r\n      volume_type           = optional(string, \"gp3\"),\r\n      iops                  = optional(number, 3000),\r\n      throughput            = optional(number, 125), # MiB\/s\r\n      volume_size           = number,\r\n      tags                  = optional(map(string))\r\n  })\r\n  default = null\r\n}\r\n\r\nvariable \"secondary_block_devices\" {\r\n  description = \"Settings for secondary block devices\"\r\n  type = list(object({\r\n      device_name           = string,\r\n      delete_on_termination = optional(bool, false),\r\n      encrypted             = optional(bool, false),\r\n      volume_type           = optional(string, \"gp3\"),\r\n      iops                  = optional(number, 3000),\r\n      throughput            = optional(number, 125), # MiB\/s\r\n      volume_size           = number,\r\n      tags                  = optional(map(string))\r\n  }))\r\n  default = []\r\n}\r\n```\r\n\r\n\r\n* `locals.tf`:\r\n```hcl\r\nlocals {\r\n   home_block_device       = var.home_block_device\r\n   secondary_block_devices = var.secondary_block_devices\r\n\r\n   additional_block_devices = local.home_block_device != null ? tolist(concat([local.home_block_device], local.secondary_block_devices)) : local.secondary_block_devices\r\n}\r\n```\r\n\r\nThis seems like another case for this bug, I see the error output:\r\n\r\n```\r\n\u2577\r\n\u2502 Error: Inconsistent conditional result types\r\n\u2502\r\n\u2502   on ec2_instance\/locals.tf line 50, in locals:\r\n\u2502   50:   additional_block_devices = local.home_block_device != null ? tolist(concat([local.home_block_device], local.secondary_block_devices)) : local.secondary_block_devices\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 local.home_block_device is null\r\n\u2502     \u2502 local.secondary_block_devices is empty list of object\r\n\u2502\r\n\u2502 The true and false result expressions must have consistent types. Mismatched list element types: The 'false' value\r\n\u2502 includes object attribute \"delete_on_termination\", which is absent in the 'true' value.\r\n\u2575\r\n```\r\n\r\nI am left scratching my head wondering how the 'false' value can include the object attribute \"delete_on_termination\", when the value itself is shown as being an \"empty list of object\"?!?!\r\n\r\nAdditionally as `local.secondary_block_devices` which is the `false` value also appears as part of the possible `true` value, I would expect Terraform to correctly calculate the result type.\r\n\r\nTested on **Terraform Version:** 1.5.7 and 1.6.1\r\n\r\nIt seems to me that there is a bug in the Type System of Terraform here. If we look at how the types coalesce in another functional language (e.g. Scala) with code which is exactly equivalent to the Terraform code:\r\n\r\n```scala\r\ncase class BlockDevice(device_name: String, delete_on_termination: Boolean, encrypted: Boolean, volume_type: String, iops: Number, throughput: Number, volume_size: Number, tags: Map[String, String])\r\n\r\nval home_block_device : BlockDevice = null\r\nval secondary_block_devices: List[BlockDevice] = List.empty[BlockDevice]\r\n\r\nval result = if (home_block_device != null) List.concat(List(home_block_device), secondary_block_devices) else secondary_block_devices\r\n```\r\n\r\nThis correctly computes the result: `val result: List[BlockDevice] = List()`\r\n\r\nMy guess would be that Terraform is loosing the object type information for the variable `home_block_device` when it is set to a `null` value.\r\n\r\n----\r\n\r\nUltimately I just want a List containing both (a) `var.home_block_device` (if it is present), and (b) any entries from `var.secondary_block_devices`. Is there some other workaround that I can use for this perhaps? \r\n\r\n","An empty list can still have an element type, which is the type that its elements would have if it were not empty. For example, if you declare an input variable of type `list(string)` and then the caller assigns `[]` to it then the result will be an empty list that knows its element type is string.\r\n\r\nNot _all_ empty lists have a known element type -- Terraform also supports \"empty list of unknown element type\" for situations like `tolist([])` -- but where possible Terraform prefers to track the element type of an empty list because it can help catch bugs earlier in the workflow -- that is, during validate or plan instead of only during apply.\r\n\r\nIn the example in the comment directly above this one, I assume that Terraform was able to infer the element type of `local.secondary_block_devices` and so it can see that it's inconsistent. If so, that seems like correct behavior rather than a bug, though of course seeing more context around how those source values are defined might suggest that Terraform made an incorrect conclusion about the empty list's element type, in which case that _would_ be a bug.\r\n","@apparentlymart Thanks for the quick follow-up. As suggested, I have now added the relevant Terraform code into my report above.","Hello @mforeman19 and others,\r\n\r\nhere's the hackishism I would shamefully use in this case:\r\n\r\n```hcl\r\nlocals {\r\n\r\n  list_object_a_enabled = true\r\n  list_object_a = tolist([\r\n    {\r\n      id      = \"a\"\r\n      enabled = true\r\n\r\n      nested = {\r\n        nested_id    = 0\r\n        nested_class = \"test\"\r\n      }\r\n    }\r\n  ])\r\n\r\n  list_object_b = tolist([\r\n    {\r\n      id         = \"b\"\r\n      enabled    = true\r\n      expiration = { days = 31 }\r\n    }\r\n  ])\r\n\r\n  list_of_objects_ternary             = try(local.list_object_a_enabled ? flatten([local.list_object_a, local.list_object_b]) : tolist(false), local.list_object_b)\r\n  list_of_objects_ternary_with_tolist = try(local.list_object_a_enabled ? tolist(flatten([local.list_object_a, local.list_object_b])) : tolist(false), local.list_object_b)\r\n  list_of_objects_ternary_false       = try(!local.list_object_a_enabled ? flatten([local.list_object_a, local.list_object_b]) : tolist(false), local.list_object_b)\r\n}\r\n```\r\n\r\nnamely, flatten overcomes the concat issue, while try and forced error in ternary overcomes the ternary consistent type requirement.\r\n\r\nresult:\r\n```hcl\r\nterraform console\r\n> local.list_of_objects_ternary\r\n[\r\n  {\r\n    \"enabled\" = true\r\n    \"id\" = \"a\"\r\n    \"nested\" = {\r\n      \"nested_class\" = \"test\"\r\n      \"nested_id\" = 0\r\n    }\r\n  },\r\n  {\r\n    \"enabled\" = true\r\n    \"expiration\" = {\r\n      \"days\" = 31\r\n    }\r\n    \"id\" = \"b\"\r\n  },\r\n]\r\n> local.list_of_objects_ternary_with_tolist\r\ntolist([\r\n  {\r\n    \"enabled\" = true\r\n    \"expiration\" = {\r\n      \"days\" = 31\r\n    }\r\n    \"id\" = \"b\"\r\n  },\r\n])\r\n> local.list_of_objects_ternary_false\r\ntolist([\r\n  {\r\n    \"enabled\" = true\r\n    \"expiration\" = {\r\n      \"days\" = 31\r\n    }\r\n    \"id\" = \"b\"\r\n  },\r\n])\r\n>\r\n```","I ended up just encoding the lists as Json and concatting the json lists together.","Hi all... I am too facing a similar issue and I belive you can help.\r\n\r\nBelow is a piece of my code that I am trying out to setup an ALB for my EC2.\r\n```\r\n#Create an EC2 Instance\r\nresource \"aws_instance\" \"terra_prj1_ec2\" {\r\n  ami = \"ami-0014ce3e52359afbd\"\r\n  instance_type = \"t3.micro\"\r\n  availability_zone = \"eu-north-1a\"\r\n  count = 2\r\n  key_name = \"main-key\"\r\n  iam_instance_profile = aws_iam_instance_profile.terra_prj1_instance_profile.name\r\n  user_data = file(\"userdata.sh\")\r\n\r\n  network_interface {\r\n    device_index = 0\r\n    network_interface_id = aws_network_interface.terra_prj1_sec_network_interface.id\r\n  }  \r\n}\r\n\r\nlocals {\r\n  ec2_instace_ids = join(\"\", aws_instance.terra_prj1_ec2.*.id)\r\n}\r\n\r\n#Create a security group for load balancer\r\nresource \"aws_security_group\" \"terra_prj1_alb_sec_grp\" {\r\n  name        = \"allow_terra_web_traffic\"\r\n  description = \"Allow web inbound traffic\"\r\n  vpc_id      = aws_vpc.terra_prj1.id\r\n\r\n  ingress {\r\n    description      = \"HTTP\"\r\n    from_port        = 80\r\n    to_port          = 80\r\n    protocol         = \"tcp\"\r\n    cidr_blocks      = [\"0.0.0.0\/0\"]\r\n    ipv6_cidr_blocks = [\"::\/0\"]\r\n  }\r\n\r\n  egress {\r\n    from_port        = 0\r\n    to_port          = 0\r\n    protocol         = \"-1\"\r\n    cidr_blocks      = [\"0.0.0.0\/0\"]\r\n    ipv6_cidr_blocks = [\"::\/0\"]\r\n  }\r\n}\r\n\r\n#Create a security group rule for EC2\r\nresource \"aws_security_group_rule\" \"terra_prj1_alb_sec_grp_rule\" {\r\n  type = \"ingress\"\r\n  from_port = 8080\r\n  to_port = 8080\r\n  protocol = \"tcp\"\r\n  security_group_id = aws_security_group.terra_prj1_sec_grp.id\r\n  source_security_group_id = aws_security_group.terra_prj1_alb_sec_grp.id\r\n}\r\n\r\n#Create a target group for load balancer\r\nresource \"aws_lb_target_group\" \"terra_prj1_alb_target_grp\" {\r\n  name = \"terra-prj1-alb-target-grp\"\r\n  port = 8080\r\n  protocol = \"HTTP\"\r\n  vpc_id = aws_vpc.terra_prj1.id\r\n  load_balancing_algorithm_type = \"round_robin\"\r\n\r\n  health_check {\r\n    enabled = true\r\n    port = 8081\r\n    interval = 30\r\n    protocol = \"HTTP\"\r\n    path = \"\/health\"\r\n    matcher = 200\r\n    healthy_threshold = 3\r\n    unhealthy_threshold = 3\r\n  }\r\n}\r\n\r\n#Create target group attachment\r\nresource \"aws_lb_target_group_attachment\" \"terra_prj1_alb_target_grp_attachment\" {\r\n  for_each = toset(local.ec2_instace_ids)\r\n\r\n  target_group_arn = aws_lb_target_group.terra_prj1_alb_target_grp.arn\r\n  target_id = each.value\r\n  depends_on = [aws_lb_target_group.terra_prj1_alb_target_grp]\r\n  port = 8080\r\n}\r\n\r\n#Create a load balancer\r\nresource \"aws_lb\" \"terra_prj1_alb\" {\r\n  name = \"terra-prj1-alb\"\r\n  internal = false\r\n  load_balancer_type = \"application\"\r\n  security_groups = [aws_security_group.terra_prj1_alb_sec_grp.id]\r\n  subnets = [aws_subnet.terra_prj1_subnet.id]\r\n}\r\n\r\n#Create a listener for load balancer\r\nresource \"aws_lb_listener\" \"terra_prj1_alb_listener\" {\r\n  load_balancer_arn = aws_lb.terra_prj1_alb.arn\r\n  port = 80\r\n  protocol = \"HTTP\"\r\n\r\n  default_action {\r\n    type = \"forward\"\r\n    target_group_arn = aws_lb_target_group.terra_prj1_alb_target_grp.arn\r\n}\r\n}\r\n```\r\n\r\nI am facing the below error while running the code.\r\n\r\n```\r\n\u2502 Error: Invalid function argument\r\n\u2502 \r\n\u2502   on Main.tf line 298, in resource \"aws_lb_target_group_attachment\" \"terra_prj1_alb_target_grp_attachment\":\r\n\u2502  298:   for_each = toset(local.ec2_instace_ids)\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 while calling toset(v)\r\n\u2502     \u2502 local.ec2_instace_ids is a string\r\n\u2502 \r\n\u2502 Invalid value for \"v\" parameter: cannot convert string to set of any single type.\r\n\u2575\r\n```\r\n\r\ntried multiple sources to fix hence no luck so far. Appreciate if anyone can have a look\r\n"],"labels":["bug","upstream","config"]},{"title":"Update Go Toolchain and code checkout version","body":"#### Github Actions - deprecated warnings\r\nAction URL: https:\/\/github.com\/hashicorp\/terraform\/actions\/runs\/4963923494\r\n\r\nGitHub will be removing support for these commands and plan to fully disable them on 31st May 2023. At this time, any workflow that still utilizes these commands will fail. See https:\/\/github.blog\/changelog\/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands\/.\r\n\r\nGitHub have not finalized a date for deprecating node12 yet but have indicated that this will be summer 2023. So it is advised to switch to node16 asap. See https:\/\/github.blog\/changelog\/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12\/.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33243) <br\/>All committers have signed the CLA.","FYI I have a different version of this bump, in https:\/\/github.com\/hashicorp\/terraform\/pull\/33250","Hi @lukaevet, thanks for this submission! We do not normally accept submissions to changes in the build system, as this is managed between the core team and the release engineering team. That said, @tbehling, shall we close this PR in deference to your existing PR or were you planning to review this PR? Thanks!"],"labels":["waiting-response"]},{"title":"dynamic moved blocks","body":"### Terraform Version\n\n```shell\n1.4.6\n```\n\n\n### Use Cases\n\nMoving large arrays of items is difficult and error prone\n\n### Attempted Solutions\n\n```  \r\ndynamic \"moved\" {\r\n    for_each = var.somevar\r\n    content {\r\n      from = old.resource[each.key]\r\n      to = new.resource[each.key]\r\n    }\r\n  }\r\n ```\n\n### Proposal\n\nI am moving a large amount of resources with computed names into a new submodule and due to lack of having a dynamic way of calculating the moved blocks or just being able to use the top resource name and having terraform figure out that its an array, I have to manually parse the state file and create the moved blocks which is error prone, and doesn't work well when multiple people are working on the same codebase and new array items may be added between me generating the static moved blocks and the actual move of the resources to a submodule.\n\n### References\n\n_No response_","comments":["Hi @archoversight! Thanks for sharing this use-case.\r\n\r\nAlthough you are right that there isn't a generalized mechanism for dynamically transforming instance keys using arbitrary expressions, you mentioned as part of your writeup \"just being able to use the top resource name and having terraform figure out that its an array\" and I'm not sure I'm understanding what you meant by that but note that the following is valid today:\r\n\r\n```hcl\r\nresource \"example\" \"example\" {\r\n  count = 3\r\n}\r\n\r\nmoved {\r\n  from = example.old\r\n  to   = example.example\r\n}\r\n```\r\n\r\nIf neither of the addresses in a `moved` block have an instance key specified then Terraform will interpret it as declaring that the whole resource has moved, with all of the same instance keys. For example, the above would declare that `example.old[0]` moved to `example.example[0]`, and so on for all of the instance keys that are declared for `example.example`.\r\n\r\nIs that what you meant by that part of your writeup, or did I misunderstand?\r\n\r\nThanks again!","@apparentlymart \r\n\r\nI have a `for_each` that is moving from the top-level to inside a module, and when I tried to just use the module directly:\r\n\r\n```\r\nresource \"example\" \"old\" {\r\nfor_each = toset(var.something)\r\n}\r\n```\r\n\r\nTo:\r\n\r\n```\r\nresource \"example\" \"new\" {\r\n   source = \".\/newmodule\"\r\n   somevar = var.something\r\n}\r\n\r\nmoved {\r\n    from = example.old\r\n    to = example.new.resource\r\n}\r\n```\r\n\r\nThis did not work, terraform complained that it has to be a singular object, so I ended up writing:\r\n\r\n```\r\nmoved {\r\n   from = example.old[\"key_name\"]\r\n   to = example.new.resource[\"key_name\"]\r\n}\r\n```\r\n\r\nFor each of the resources. Granted, this was on an older version of terraform, not 1.4.6 (1.3 series), I just didn't get around to writing up the issue. I will try again in the newer versions.\r\n\r\n---\r\n\r\nEdit:\r\n\r\nI will note that we have a lot of keys that are computed on the fly using locals\/function transforms. That may also be causing issues for the moving of the resources wholesale.","I think the fact that we're using contrived \"example\" names is making this a bit harder to follow and so we're talking past each other a little.\r\n\r\nThe example `moved` block you shared has invalid syntax, so if you tried something equivalent to that then I suspect Terraform may just have been confused about what your intent was: `example.new.resource` cannot be valid because resource addresses always come in two parts, like the `example.old` you showed. If this were a path to a resource in a child module instance then it would start with a `module.something.` prefix to describe that module.\r\n\r\nI think the situation you described here is the one explored in [Splitting One Module into Multiple](https:\/\/developer.hashicorp.com\/terraform\/language\/modules\/develop\/refactoring#splitting-one-module-into-multiple), which shows the syntax for declaring that a resource has moved wholesale (with all of its instances unchanged) into a child module. Here's a reduced version of that example:\r\n\r\n```hcl\r\nmodule \"x\" {\r\n  source = \"..\/modules\/x\"\r\n\r\n  # ...\r\n}\r\n\r\nmoved {\r\n  from = aws_instance.a\r\n  to   = module.x.aws_instance.a\r\n}\r\n```\r\n\r\nBecause neither the `from` or `to` address includes an instance key, Terraform understands this as having moved all of the instances of `aws_instance.a` together, so all of the individual instance keys would be preserved.\r\n\r\nWhat this _cannot_ model is any situation where the instance keys changed in some systematic way at the same time as moving into a child module. For example, if you previously had `aws_instance.example[\"foo_a\"]` and `aws_instance.example[\"foo_b\"]` and part of the refactoring is to remove those `foo_` prefixes to produce `module.x.aws_instance.example[\"a\"]` and `module.x.aws_instance.example[\"b\"]` then in today's Terraform there is no alternative but to write out a separate `moved` block for each instance, explicitly describing how each source instance key maps to each destination instance key rather than just writing a general rule about removing the `foo_` prefix.\r\n\r\n","+1 for dynamic moved blocks. \ud83e\udd26 ","I don't want to put words into @archoversight's mouth,  but I think the problem they are trying to solve is moving a `for_each` from a resource in the parent module across to a child module call.  E.g.:\r\n\r\n```terraform\r\n# Parent Module\r\n\r\nresource \"example\" \"old\" {\r\n    for_each = toset(var.something)\r\n\r\n    arg = each.key\r\n}\r\n\r\nmodule \"new_module\" {\r\n    source = \"...\"\r\n\r\n    for_each = toset(var.something)\r\n    something = each.key\r\n}\r\n```\r\n\r\n```terraform\r\n# New Child Module\r\n\r\nresource \"example\" \"new\" {\r\n    arg = var.something\r\n}\r\n```\r\n\r\nand needing to move `example.old[*]` to `module.new_module[*].example.new`\r\n\r\nI don't believe there is currently a way of expressing this with the current `moved` block without explicitly iterating over `toset(var.something)`:\r\n\r\n```terraform\r\nmoved {\r\n    from = example.old[\"something_a\"]\r\n    to = module.new_module[\"something_a\"].example.new\r\n}\r\n\r\nmoved {\r\n    from = example.old[\"something_b\"]\r\n    to = module.new_module[\"something_b\"].example.new\r\n}\r\n\r\nmoved {\r\n    from = example.old[\"something_c\"]\r\n    to = module.new_module[\"something_c\"].example.new\r\n}\r\n\r\n# ...\r\n```\r\n\r\nIf you could use the meta-arguments`count` and `for_each` with `moved` blocks then it would be something like:\r\n```terraform\r\nmoved {\r\n    for_each = toset(var.something)\r\n    from = example.old[each.key]\r\n    to = module.new_module[each.key].example.new\r\n}\r\n```","Thanks for the additional context!\r\n\r\nI think at this point we have a good understanding of what use-cases are missing today, and the remaining problem is that we have no known technical design with which to meet those use-cases.\r\n\r\nInternally a `moved` block is essentially an adjustment to the \"previous run state\", which is what we call the state that was written at the end of the most recent run. Terraform essentially rewrites the addresses of objects in the prior state to pretend that they were originally created at the new addresses, and only _then_ does it begin the normal planning process, with the previous run state already having been updated to reflect the new addresses.\r\n\r\nIn practice that means that move resolution cannot evaluate arbitrary expressions, because expression evaluation happens during the plan phase and relies on the \"planned state\", which is what we call the internal-only state snapshot that Terraform gradually builds during planning -- the diff we show is comparing the planned state against the previous state.\r\n\r\nIn practice I think that means that adding more flexibility here would require one of the following:\r\n1. Use a more constrained mechanism for describing the \"rule\" for a move -- not arbitrary expression evaluation, but something else with more restrictive capabilities -- that Terraform Core can safely evaluate prior to the planning phase, as an extension of the current behavior.\r\n2. Totally rework the approach so that resolving moves happens during the planning phase rather than before it, which will then make the possibility of objects changing address during evaluation a cross-cutting concern, since the whole of Terraform's runtime will need to be made aware that the address of a resource instance can change dynamically at the same time as other work is going on.\r\n\r\nWhen we originally worked on this feature we could not find a sufficient design for option 2 that would not amount to essentially a rewrite of the entire runtime with a different design approach. There might still be a more feasible design out there waiting to be discovered, but my hunch is that path 1 is the more likely to succeed. We don't currently have a candidate design for either path though, so I think both are still on the table for someone to research.\r\n\r\n","yet another barely useful feature by terraform. how do you think am I supposed to move thousands of resources from one resource to another as an author of internal module used by hundreds of configurations?","@gleb-boushev-effem Thanks for the feedback. I acknowledge that it can be frustrating when a particular feature implementation does not contain the features you need to implement a solution. Just a reminder to please follow the [Community Guidelines](https:\/\/www.hashicorp.com\/community-guidelines) when posting. Thanks again!","> yet another barely useful feature by terraform. how do you think am I supposed to move thousands of resources from one resource to another as an author of internal module used by hundreds of configurations?\r\n\r\nI also came here hoping for a better solution but what I'm currently doing is:\r\n\r\n1. make the change which makes X things to create and X things to destroy (the thing you're moving)\r\n2. make a script that is able to identify removed and new \r\n3. make a state-change script where you map each old value to the new and doing terraform state mv OLD_STATE NEW_STATE\r\n\r\nThis isn't fast, ideal or anything but well it works.","A bit late to the party, but perhaps for other people stumbling onto this discussion: given the technical architecture limitations, why not approach this from another angle?\r\n\r\nThe `moved { }` refactoring is by definition temporary, so this can also be fixed a little less elegantly. Perhaps a small script written in a language of choice that generates the terraform moved block permutations into a temporary `refactor-reason.tf` file? After the `apply` this file will be removed from SCM anyways.\r\n\r\nIf this module has a lot of users across other internal repos, one could consider a tool like [mani](https:\/\/manicli.com) to run this script as part of a mani `task` in many local repositories at once to checkout\/commit\/push\/create-mr the same branch name with a proposed fix to all."],"labels":["enhancement","new"]},{"title":"Add `prevent_removal` lifecycle flag as a stronger version of `prevent_destroy`","body":"#17599 has been a longstanding issue around the implementation of the `prevent_destroy` flag. Specifically, the flag does not prevent destroying resources for which the configuration has been removed. This PR addresses this need with a different flag, `prevent_removal`, which implements the desired behaviour. The desired behaviour is described in [this comment](https:\/\/github.com\/hashicorp\/terraform\/issues\/17599#issuecomment-373557799):\r\n\r\n> Currently removing a resource from configuration is treated as intent to remove the resource, since that's an explicit action on your part rather than it happening as a part of some other action, such as resource replacement or a whole-configuration destroy.\r\n> \r\n> However, your idea of retaining this in the state and requiring multiple steps to actually destroy is an interesting one:\r\n> \r\n> 1. Leave resource in config but remove `prevent_destroy`\r\n> 2. Run `terraform apply` to update the state to no longer have that flag set.\r\n> 3. Remove the resource from configuration entirely.\r\n> 4. Run `terraform apply` again to actually destroy it.\r\n\r\n\r\nFixes #17599 \r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\nAny\r\n\r\n## Draft CHANGELOG entry\r\n\r\n\r\n### NEW FEATURES\r\n\r\n-  Introduced `prevent_removal` flag for preventing destroying resources even when the configuration is removed.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33229) <br\/>All committers have signed the CLA.","Thanks for this submission! Given the complexity of this issue, I expect this will need to be prioritized against other 1.6 roadmap items to get into review. I will bring this to triage and report back. Thanks again!\r\n\r\nAlso, if this gets reviewed it will need documentation. Please do not work on that now, as I am not sure if this will get past the review stage. I am just leaving this note as a reminder to myself to make sure this happens if it is needed. ","Hi, is there an update on this feautre?\r\n","I re-reviewed this PR with @apparentlymart. We are working on a project which may inform how we would want to approach this problem. Once that project is complete, we will revisit this PR \/ problem. Thanks!"],"labels":["enhancement"]},{"title":"remote-state\/pg: Cannot lock workspace","body":"### Terraform Version\r\n\r\n```shell\r\nTerrafrom v1.4.5\r\non linux_amd64\r\n```\r\n\r\n\r\n### Terraform Configuration Files\r\n\r\n-\r\n\r\n\r\n### Debug Output\r\n\r\n-\r\n\r\n### Expected Behavior\r\n\r\nWhen using the `pg` backend terraform should allow locking for states in different schemas without blocking.\r\n\r\n### Actual Behavior\r\n\r\nCurrently it appears if two users sharing the same databases but operating on projects in different schemas, terraform is unable to acquire the state lock when someone else is performing a plan\/apply on an unrelated project in a different schema. \r\n\r\nThis can be reproduced by manually running acquiring the lock and running plan\/apply\r\n\r\n### Steps to Reproduce\r\n\r\n1. Acquire advisory lock for pg side for test\r\n```\r\nselect try pg_try_advisory_lock(-1);\r\npg_try_advisory_lock | t\r\n```\r\n2. Run a plan\r\n```\r\nterraform plan\r\n\r\nError:  Error acquiring the state lock\r\nError message: Cannot lock workspace; already locked for workspace creation: default\r\nLock Info:\r\nID:        xxx\r\nPath:      \r\nOperation: OperationTypePlan\r\nWho:       xxx\r\nVersion:   1.4.5\r\nCreated:  xxx\r\nInfo:      \r\nTerraform acquires a state lock to protect the state from being written\r\nby multiple users at the same time. Please resolve the issue above and try\r\nagain. For most commands, you can disable locking with the \"-lock=false\"\r\nflag, but this is not recommended.\r\n```\r\n\r\n3. Unlock from pg side\r\n```raw\r\nselect pg_advisory_unlock(-1);\r\npg_advisory_unlock | t\r\n```\r\n4. Terraform plan works successfully\r\n\r\n\r\n\r\n### Additional Context\r\n\r\nSeems be related to https:\/\/github.com\/hashicorp\/terraform\/blob\/13cb0d03c7d6cca7b22966428c6032ab1ac4c574\/internal\/backend\/remote-state\/pg\/client.go#LL92C110-L92C110 which tries to acquire a lock on both the states table id and (-1).\r\n\r\n### References\r\n\r\n_No response_","comments":["Just FYI, @remilapeyre ","up","Same issue!"],"labels":["bug","backend\/pg","new"]},{"title":"Persistent state and automatic reimport for resources that aren't fully deleted on `destroy`","body":"### Terraform Version\r\n\r\n```shell\r\nTerraform v1.5.0-beta1\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v4.67.0\r\n```\r\n\r\n### Use Cases\r\n\r\nSome resources, like `aws_secretsmanager_secret` or `aws_kms_key`, go into a pending deletion state outside of Terraform when destroyed, rather than being immediately deleted.\r\n\r\nIf a configuration containing a resource like this is intended to be applied and destroyed regularly -- for example, a dev or CI environment -- then it is currently already painful to handle. Because the resource can't be fully deleted by design, it must be manually undeleted and reimported into Terraform before the configuration can be applied again. The new `import` blocks coming in Terraform 1.5.0 seemed like they had the potential to improve this: [if those resources could be enhanced to undelete themselves when imported](https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/31468), then the `import` block would get rid of any manual steps needed in between applies.\r\n\r\n```hcl\r\nresource \"aws_secretsmanager_secret\" \"test\" {\r\n  name = \"test\"\r\n}\r\n\r\nimport {\r\n  # Doesn't actually need to be a full ARN to be imported successfully, in spite of documentation.\r\n  # This resource type gets some random characters added to the end of its ARN by AWS.\r\n  id = \"arn:aws:secretsmanager:us-west-2:xxx:secret:test\"\r\n  to = aws_secretsmanager_secret.test\r\n}\r\n```\r\n\r\nBut now the pain point comes on the first apply. In a configuration like this, the `import` block would be intended to be permanent. But this means that on the first apply, it needs to be commented out, otherwise it fails with `Error: Cannot import non-existent remote object`.\r\n\r\n### Attempted Solutions\r\n\r\nThe way we currently manage resources of this type is with a wrapper script that undeletes and reimports the resource outside of Terraform before running `apply`.\r\n\r\n### Proposal\r\n\r\nA new concept of \"persistence\" is added to resources. A persistent resource continues to have its ID stored in the state after it is destroyed. `Persistent bool` is added as a new attribute to [`schema.Resource`](https:\/\/pkg.go.dev\/github.com\/hashicorp\/terraform-plugin-sdk\/v2@v2.26.1\/helper\/schema#Resource) to effect this change. When a provider adds support for a resource that can't be immediately deleted by Terraform, they should set this to `true` on that resource. \r\n\r\nEvery time state is refreshed, if the resource has been previously destroyed, the resource is read via the stored ID, and is only removed from the state when it is found to have been fully deleted outside of Terraform.\r\n\r\nWhen a destroyed persistent resource is applied, Terraform automatically proposes an import of the previously deleted resource, rather than creation of a new resource. It would be up to providers to ensure that import of the resource includes undeleting\/restoring it.\r\n\r\n`terraform state list` for a resource like this that has been destroyed might show:\r\n\r\n```\r\naws_secretsmanager_secret.test [destroyed]\r\n```\r\n\r\nThe resource could be `terraform state rm`'d to make Terraform fully forget about it and attempt to create a new resource instead of reimporting the previous one. A new `lifecycle` option, `persistence`, of type `bool`, can be used on persistent resources to optionally disable the behavior:\r\n\r\n```\r\nlifecycle {\r\n  persistence = false\r\n}\r\n```\r\n\r\nTrying to use this attribute on a resource that is not persistent would generate an error.\r\n\r\n#### Open Questions\r\n\r\nWhat should `terraform taint` do with a persistent resource? `taint` is intended to destroy and recreate, but this is likely to cause a conflict with the resource that was just destroyed, but isn't actually gone. Maybe it should be an error.\r\n\r\nWhat happens if the configuration for a persistent resource is removed from the configuration? It should likely be retained in the state as if it were destroyed, since it still has the same property of not actually having been deleted. A user might be switching between git branches that do and don't have the resource.\r\n\r\n### References\r\n\r\n- https:\/\/github.com\/hashicorp\/terraform-provider-aws\/issues\/31468\r\n- https:\/\/github.com\/hashicorp\/terraform\/issues\/26364","comments":["Hi @geekofalltrades! Thanks for sharing this use-case.\r\n\r\nThis is an interesting situation and technically an example of providers not implementing the providers protocol correctly: a provider should not report that deletion succeeded if the object hasn't actually been deleted at least enough that another object could be created to replace it.\r\n\r\nBefore we consider complicated new lifecycle behavior like this I think it would help to consider what the options are within the bounds of Terraform's current provider protocol.\r\n\r\nThe most direct design -- but also perhaps annoying -- would be for the provider to return an error whenever it's asked to plan deleting one of these objects that can't actually be deleted, and thus require the object to be \"forgotten\" with `terraform state rm` instead. This seems to solve half of the problem you described, but of course it doesn't deal with the \"recreation\" step you mentioned.\r\n\r\nWe've already been considering a new provider protocol capability that could be a different take on this problem with similar characteristics to what you described, which I'll describe here for comparison's sake:\r\n\r\nThe new idea is to allow providers to respond to `PlanResourceChange` with an extra annotation that says either that the provider is going to treat a create request as an \"adoption\" request (automatically take control of an existing object) or it needs to treat a destroy request as a \"forget\" request (remove the object from the state without destroying it in the remote system).\r\n\r\nAs you can see, \"adopting\" is like an automatic import and \"forgetting\" is like an automatic state rm, both controlled by the provider rather than the user so the provider can more accurately describe situations like taking control over a singleton object that always exists and thus cannot truly be created, or ceding control of such as object after the resource block has been removed.\r\n\r\nThis seems _almost_ equivalent to what you proposed except that the verbs here are slightly different: instead of \"adopt\" we might say \"reactivate\", and instead of \"forget\" we might say \"deactivate\". We would of course need to consider how and whether to keep track of \"deactivated\" objects in the state as you noted... something living on in the state after it's been removed from configuration is unprecedented in Terraform so far, so we would need to consider carefully how to model that so it's clear that the object is still being tracked in some sense by Terraform even though there's no record of it in the most recently-applied configuration.\r\n\r\nAnother variation would be to require some kind of marker to remain in the configuration to commemorate the deactivated object, which could then be replaced by a normal `resource` block later if the object needs to be reactivated. This would be similar to my first idea above of returning an error when something can't be destroyed, except that the resolution of the error in this case would be to add the configuration block to mark the object as deactivated, rather than running `terraform state rm`.\r\n\r\nIf we did make this idea of \"deactivating\" explicit in the configuration then it could also work in situations where both deactivating and destroying are possible, such as suspending an EC2 instance instead of destroying it, although that is already technically possible via a resource-level argument today so perhaps this suggests that \"deactivating\" could be an idea fully implemented inside a provider without Terraform Core being aware of it... from Terraform Core's perspective a deactivated object is still an object that needs to be tracked, so in principle it could just be a normal resource block with a provider-specific setting to say that it should be made inactive.\r\n\r\nI think there's a bunch of details to think about here but the overall use-case makes sense to me, so we'll use this issue to track it and gauge interest. If other folks find this issue and have similar use-cases we should consider, please upvote the original issue comment above with \ud83d\udc4d and leave a comment describing the resource type you are using and how it might benefit from explicit modelling of the object being inactive.\r\n","Adoption and forget decisions by the provider sound perfect, and a lot less complicated than my idea. I continued pulling the thread on it after updating this issue last night and was coming up with all sorts of weird edge cases, like how do you handle changes that force recreation of a persistent resource? Do you continue to store a list of all previous IDs of the resource? What if the user changes it back to the previous state? How does the Terraform core decide when the new state is equivalent to the previous state to propose a reimport?\r\n\r\nWhat if the user tries to import a persistent resource to a different address after it has been destroyed?\r\n\r\nAll questions that the provider seems better equipped to answer.","@apparentlymart I've been waiting for a feature like this so that I don't have to develop and integrate lengthy state removal scripts into our pipeline just to \"forget\" a resource without deleting it. \r\n\r\n> We've already been considering a new provider protocol capability that could be a different take on this problem with similar characteristics to what you described, which I'll describe here for comparison's sake:\r\n> \r\n> The new idea is to allow providers to respond to `PlanResourceChange` with an extra annotation that says either that the provider is going to treat a create request as an \"adoption\" request (automatically take control of an existing object) or it needs to treat a destroy request as a \"forget\" request (remove the object from the state without destroying it in the remote system)."],"labels":["enhancement","providers"]},{"title":"Document the migration path from Terraform Cloud to a standard backend","body":"### Terraform Version\n\n```shell\n1.4.6\n```\n\n\n### Use Cases\n\nAfter the [recent announcement](https:\/\/www.hashicorp.com\/blog\/terraform-cloud-updates-plans-with-an-enhanced-free-tier-and-more-flexibility) which introduces a resource limits for free tiers, I'm planning to migrate my state(s) off of Terraform Cloud.\r\n\r\nHowever, when I tried to migrate to any other backend, I get the following error:\r\n```\r\nError: Migrating state from Terraform Cloud to another backend is not yet implemented.\r\n```\r\n\r\nCould you please share a detailed step-by-step process on how to safely migrate away?\n\n### Attempted Solutions\n\nTraditional migration to an S3 backend, which fails.\n\n### Proposal\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @marco-lancini,\r\n\r\nBecause Terraform Cloud has additional features beyond just state storage, if you are using any of those features then migrating away from Terraform Cloud will require you to find a different way to achieve those capabilities.\r\n\r\nHowever, if your focus is _only_ on state storage -- for example, if you've been using Terraform Cloud exclusively with workspaces in local execution mode and running Terraform CLI just like any user of open source Terraform would -- then something like the following path should work to migrate your latest state snapshots to a new backend of your choice:\r\n\r\n1. In your initialized working directory for your configuration that's currently using Terraform Cloud (with a `cloud` or `backend \"remote\"` block), run `terraform state pull >temp.tfstate` to obtain a local copy of your latest state snapshot.\r\n2. Entirely remove the `.terraform` subdirectory to de-initialize your working directory, and thus \"forget\" the connection with Terraform Cloud. Remove the `cloud` or `backend \"remote\"` block and replace it with the new backend you intend to use. (You should ensure that the new backend configuration doesn't already refer to an existing state or the following steps will not succeed due to a state lineage conflict.)\r\n3. Run `terraform init` again to re-initialize your working directory with the new backend. Since your new backend doesn't yet have any state, you should avoid running `terraform apply` (or similar) until you complete this process, since Terraform will initially propose to create new copies of everything.\r\n4. Run `terraform state push temp.tfstate` to write the previously-saved state snapshot file to your new backend.\r\n\r\nIf you are using multiple workspaces in the same configuration then in steps 1 and 4 you will need to visit each workspace in turn using `terraform workspace select` (and `terraform workspace new` in step 4) to deal with each workspace's state separately, writing each one to a separate filename instead of my `temp.tfstate` placeholder.\r\n\r\nThe following is a _non-exhaustive_ list of settings from your Terraform Cloud workspace that this _won't_ preserve, and so you'll need to manually migrate these if you are relying on them in your workflow:\r\n- Input variable values stored in the workspace settings.\r\n- Settings for remote operations in Terraform Cloud, such as environment variables and SSH keys used for Git cloning.\r\n- Run notification settings, such as external webhooks.\r\n- Historical state snapshots from earlier changes made with Terraform. (`terraform state pull` can only read the very latest snapshot.)\r\n\r\nExactly how you'd migrate those will depend on what you are migrating _to_, since all of the above are Terraform Cloud-specific features that tend to be implemented differently in other systems for running Terraform in automation.\r\n"],"labels":["documentation","cloud"]},{"title":"[plan] `relevant_changes` not behaving as expected","body":"### Terraform Version\n\n```shell\n1.2.2, 1.4.6 tested\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\n#security-group module\r\nmodule \"foo\" {\r\n  source      = \"terraform-aws-modules\/security-group\/aws\"\r\n  version     = \"4.3.0\"\r\n  name        = \"foo\"\r\n  description = \"foo\"\r\n  vpc_id      = \"foo\"\r\n\r\n  computed_ingress_with_source_security_group_id = [\r\n    {\r\n      from_port                = local.instance[\"port\"]\r\n      to_port                  = local.instance[\"port\"]\r\n      protocol                 = \"udp\" # changing from TCP, to force replacement\r\n      description              = \"foo\"\r\n      source_security_group_id = \"foo\"\r\n    }\r\n  ]\r\n  number_of_computed_ingress_with_source_security_group_id = 1\r\n}\r\n```\n\n### Debug Output\n\nto be posted if necessary\n\n### Expected Behavior\n\nthe `relevant_attributes` in the plan output should contain\r\n```terraform\r\n\"relevant_attributes\": [\r\n    {\r\n      \"resource\": \"my-module-address\",\r\n      \"attribute\": [\r\n        \"protocol\"\r\n      ]\r\n    }\r\n```\r\nsince that was changed, and is forcing replacement (the stdout plan output demonstrates this):\r\n```terraform\r\n  # module.foo.aws_security_group_rule.computed_ingress_with_source_security_group_id[0] must be replaced\r\n-\/+ resource \"aws_security_group_rule\" \"computed_ingress_with_source_security_group_id\" {\r\n      - cidr_blocks              = [] -> null\r\n      ~ id                       = \"foo\" -> (known after apply)\r\n      - ipv6_cidr_blocks         = [] -> null\r\n      ~ protocol                 = \"tcp\" -> \"udp\" # forces replacement\r\n      ~ security_group_rule_id   = \"foo\" -> (known after apply)\r\n        # (8 unchanged attributes hidden)\r\n    }\r\n```\n\n### Actual Behavior\n\nthis resource doesn't even appear in `relevant_attributes` at all, despite it being replaced, and the description of it being:\r\n```\r\nrelevant_attributes\" lists the sources of all values contributing to changes in the plan.\r\n```\r\nand replacing the resource w\/ a change in protocol is definitely a change in the plan, I'd say\n\n### Steps to Reproduce\n\n1. create a security group with the config above (may need to substitute `foo` for real values)\r\n2. apply, then change the protocol for a rule\r\n3. the plan output should show the resource getting replaced (the rule)\r\n4. `terraform plan` to a planfile\r\n5. `terraform show` to a JSON\r\n6. inspect the json, and be unhappy because your change isn't in `relevant_attributes`\n\n### Additional Context\n\n_No response_\n\n### References\n\n_No response_","comments":["Hi @tarfeef101!\r\n\r\nIf I recall correctly, the purpose of \"relevant attributes\" is for filtering the \"changes from outside of Terraform\", which is called `resource_drift` in [the JSON plan representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#plan-representation).\r\n\r\nAn attribute can only be \"relevant\" if it was used as part of the expression of an attribute that has changed. It isn't sufficient for an attribute to just have changed _itself_, because those must always be displayed.\r\n\r\nTherefore I think Terraform is behaving as intended here, but it sounds like you are trying to use `relevant_attributes` in a different way than what it was intended to be used for. If you can share some more information about your end goal then I might be able to suggest a different way to achieve it, or if there isn't a way then we could turn this into an enhancement request for some new feature to meet your use-case.\r\n\r\nThanks!\r\n","> Hi @tarfeef101!\r\n> \r\n> If I recall correctly, the purpose of \"relevant attributes\" is for filtering the \"changes from outside of Terraform\", which is called `resource_drift` in [the JSON plan representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#plan-representation).\r\n> \r\n> An attribute can only be \"relevant\" if it was used as part of the expression of an attribute that has changed. It isn't sufficient for an attribute to just have changed _itself_, because those must always be displayed.\r\n> \r\n> Therefore I think Terraform is behaving as intended here, but it sounds like you are trying to use `relevant_attributes` in a different way than what it was intended to be used for. If you can share some more information about your end goal then I might be able to suggest a different way to achieve it, or if there isn't a way then we could turn this into an enhancement request for some new feature to meet your use-case.\r\n> \r\n> Thanks!\r\n\r\nThanks for the reply!\r\n\r\nI'm working on an enhancement for https:\/\/github.com\/bridgecrewio\/checkov to allow their checks to access information about field-level changes in a plan.\r\n\r\nBefore I discovered this field, I was expecting to have to go through the ugly process of diffing before and after states for each resource to discern the changes. The wording for `relevant_attributes` led me to believe it was just \"fields that are causing changes in the plan\". If the intent was as you described and specific to state drift, then I'd expect it to be worded closer to \"fields that are causing changes in the STATE\", as drift exists before you make a plan, you just discover it when you run the plan.\r\n\r\nIf there's a way to get those field changes out of the plan representation without having to diff the entire config that'd be awesome! As for `relevant_changes`, if that's already architected and used as intended, I think maybe an update to that wording in the docs would help prevent confusion like what I encountered.","The specific need that `relevant_attributes` is designed to handle is when the plan looks something like this:\r\n\r\n```\r\nNote: Objects have changed outside of Terraform\r\n\r\n # aws_instance.example as been changed\r\n ~ resource \"aws_instance\" \"example\" {\r\n      ~ tags = {\r\n           ~ Name = \"foo\" -> \"bar\"\r\n         }\r\n    }\r\n\r\n(...)\r\n\r\nTerraform will perform the following actions:\r\n\r\n # aws_instance.example as been changed\r\n ~ resource \"aws_instance\" \"example\" {\r\n      ~ tags = {\r\n           ~ Name = \"bar\" -> \"foo\"\r\n         }\r\n    }\r\n```\r\n\r\nIn the above hypothetical example, the change to the `Name` tag was considered as \"relevant\" because it changed in both the changes outside of Terraform and the proposed changes, so Terraform believes that the change outside of Terraform is a likely cause of the proposed change. The above is a simple form of this scenario; a more complex case would be if the attribute that changed outside of Terraform is _not_ being changed back by this execution plan but some other argument's value was derived from it and so _it_ has a proposed change.\r\n\r\n---\r\n\r\nFrom what you've described it sounds like you are intending to show a diff similar to what `terraform plan` itself would show. If that's true then indeed the way to implement that is what you originally expected: you compare the `before` and `after` properties in each [Change Representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#change-representation) and indicate whenever they don't match.\r\n\r\nYou can find Terraform's own implementation of this, in case that's useful to refer to:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/13cb0d03c7d6cca7b22966428c6032ab1ac4c574\/internal\/command\/jsonformat\/differ\/block.go#L29-L121\r\n\r\nThe `structured.Change` thing in that function is a Go type corresponding to the \"Change Representation\" in the documentation. `terraform plan`'s own diff renderer is implemented in terms of the same JSON output, so there shouldn't be anything it can do here that you can't reproduce in your own renderer. (Terraform Cloud also uses the same source data to produce an HTML-based version of the plan output, but that's not open source so I can't provide a link to that one.)\r\n\r\nYou can also see here that this _is_ making some use of the \"relevant attributes\" data structure, so this may be a good reference for what exactly that data structure is intended to be used for.\r\n","> The specific need that `relevant_attributes` is designed to handle is when the plan looks something like this:\r\n> \r\n> ```\r\n> Note: Objects have changed outside of Terraform\r\n> \r\n>  # aws_instance.example as been changed\r\n>  ~ resource \"aws_instance\" \"example\" {\r\n>       ~ tags = {\r\n>            ~ Name = \"foo\" -> \"bar\"\r\n>          }\r\n>     }\r\n> \r\n> (...)\r\n> \r\n> Terraform will perform the following actions:\r\n> \r\n>  # aws_instance.example as been changed\r\n>  ~ resource \"aws_instance\" \"example\" {\r\n>       ~ tags = {\r\n>            ~ Name = \"bar\" -> \"foo\"\r\n>          }\r\n>     }\r\n> ```\r\n> \r\n> In the above hypothetical example, the change to the `Name` tag was considered as \"relevant\" because it changed in both the changes outside of Terraform and the proposed changes, so Terraform believes that the change outside of Terraform is a likely cause of the proposed change. The above is a simple form of this scenario; a more complex case would be if the attribute that changed outside of Terraform is _not_ being changed back by this execution plan but some other argument's value was derived from it and so _it_ has a proposed change.\r\n> \r\n> From what you've described it sounds like you are intending to show a diff similar to what `terraform plan` itself would show. If that's true then indeed the way to implement that is what you originally expected: you compare the `before` and `after` properties in each [Change Representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#change-representation) and indicate whenever they don't match.\r\n> \r\n> You can find Terraform's own implementation of this, in case that's useful to refer to:\r\n> \r\n> https:\/\/github.com\/hashicorp\/terraform\/blob\/13cb0d03c7d6cca7b22966428c6032ab1ac4c574\/internal\/command\/jsonformat\/differ\/block.go#L29-L121\r\n> \r\n> The `structured.Change` thing in that function is a Go type corresponding to the \"Change Representation\" in the documentation. `terraform plan`'s own diff renderer is implemented in terms of the same JSON output, so there shouldn't be anything it can do here that you can't reproduce in your own renderer. (Terraform Cloud also uses the same source data to produce an HTML-based version of the plan output, but that's not open source so I can't provide a link to that one.)\r\n> \r\n> You can also see here that this _is_ making some use of the \"relevant attributes\" data structure, so this may be a good reference for what exactly that data structure is intended to be used for.\r\n\r\nThanks for the affirmation. In that case, I would recommend altering the wording within the documentation as I described in my previous comment to say `relevant_attributes` applies to fields which resulted in changes in state (drift) vs changes in the plan (could be drift or intentional changes from code)."],"labels":["documentation","new"]},{"title":"Add a warning when \"target\" is not found","body":"### Terraform Version\r\n\r\n```shell\r\nterraform version\r\nTerraform v1.4.5\r\non linux_amd64\r\n+ provider registry.terraform.io\/hashicorp\/aws v4.66.1\r\n+ provider registry.terraform.io\/hashicorp\/helm v2.9.0\r\n+ provider registry.terraform.io\/hashicorp\/kubernetes v2.20.0\r\n+ provider registry.terraform.io\/terraform-providers\/opsgenie v0.6.11\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nWhen running terraform with the \"-target\" option, if we do a typo in the target, we get:\r\n\r\n```\r\nNo changes. Your infrastructure matches the configuration.\r\n\r\nTerraform has compared your real infrastructure against your configuration\r\nand found no differences, so no changes are needed.\r\n\u2577\r\n\u2502 Warning: Resource targeting is in effect\r\n\u2502 \r\n\u2502 You are creating a plan with the -target option, which means that the\r\n\u2502 result of this plan may not represent all of the changes requested by the\r\n\u2502 current configuration.\r\n\u2502 \r\n\u2502 The -target option is not for routine use, and is provided only for\r\n\u2502 exceptional situations such as recovering from errors or mistakes, or when\r\n\u2502 Terraform specifically suggests to use it as part of an error message.\r\n```\r\n\r\n### Attempted Solutions\r\n\r\nnot doing typos in target :)\r\n\r\n### Proposal\r\n\r\nIdea is to add a warning (or error ?) message when there are no resources matching the target option.\r\nThis would avoid confusion when trying to find out why no changes are expected when we expect some (and more importantly, avoid thinking the infrastructure is all up to date when actually here are pending changes).\r\n\r\nie: this is all to prevent human error ;)\r\n\r\n### References\r\n\r\n_No response_","comments":["Thanks for this request!"],"labels":["enhancement","new"]},{"title":"[COMPLIANCE] Add Copyright and License Headers","body":"Hi there \ud83d\udc4b\n\nThis PR was auto-generated as part of an internal review of public repositories that are not in compliance with HashiCorp's licensing standards.\n\n**Frequently Asked Questions**\n\n<details><summary>Why am I getting this PR?<\/summary>\nThis pull request was created because one or more source code files were found missing copyright and\/or license headers.\n\nMore info is available in the [RFC](https:\/\/go.hashi.co\/rfc-engsrv-059)\n<\/details>\n\n<details><summary>How do you determine which files should get copyright headers?<\/summary>\nAttempts are made to skip scanning autogenerated files (e.g., `go.mod`) and prose. If you find file types you feel should be excluded from future scans, please reach out to:\n\n[#proj-software-copyright](https:\/\/go.hashi.co\/proj-software-copyright)\n\n<\/details>\n\n<details><summary>I have a file or folder which should be exempted, how do I do that?<\/summary>\nYou may exempt certain files or folders from being scanned by adding a `.copywrite.hcl` config in the root of your repo. You can use the [`copywrite init`](https:\/\/go.hashi.co\/copywrite) command to interactively create a config for this project.\n\nAn example schema can be found below. Add a [doublestar**](https:\/\/github.com\/bmatcuk\/doublestar)-capable pattern to the `header_ignore` list to skip it in future scans.\n\n```hcl\n# (OPTIONAL) Overrides the copywrite config schema version\n# Default: 1\nschema_version = 1\n\nproject {\n  # (OPTIONAL) SPDX-compatible license identifier\n  # Leave blank if you don't wish to license the project\n  # Default: \"MPL-2.0\"\n  # license = \"\"\n\n  # (OPTIONAL) Represents the year that the project initially began\n  # Default: <the year the repo was first created>\n  # copyright_year = 0\n\n  # (OPTIONAL) A list of globs that should not have copyright or license headers .\n  # Supports doublestar glob patterns for more flexibility in defining which\n  # files or folders should be ignored\n  # Default: []\n  header_ignore = [\n    # \"vendor\/**\",\n    # \"**autogen**\",\n  ]\n}\n\nMore information about configuration options is available in [the documentation](https:\/\/github.com\/hashicorp\/copywrite#config-structure).\n\n```\n\n<\/details>\n\n<details><summary>I added a config. How do I trigger this PR to be rebased?<\/summary>\nHashiCorp employees can use the [Copywrite SlackBot](https:\/\/hashicorp.slack.com\/archives\/D052WARFFS8) to trigger a rebase. You can DM the slackbot with \"headers terraform\" to trigger a PR rebasing.\n\n<\/details>\n\n<details><summary>Why don't the headers include a copyright date?<\/summary>\nCopyright headers are not required to include a year. In the interest of pragmatism, HashiCorp has decided to exclude the year from headers and instead list it in the LICENSE file at the root of the repository instead.\n\n<\/details>\n\nAdditional FAQs are available at https:\/\/go.hashi.co\/header-faq\n\nPlease approve and merge this PR in a timely manner to keep this source code compliant with our OSS license agreement. If you have any questions or feedback, reach out to [#proj-software-copyright](https:\/\/go.hashi.co\/proj-software-copyright).\n\nThank you!\n\n---\n\n<!-- DEBUG:\ncopywrite-version: v0.16.4\ntooling-version: 8a10fcd60e84047cea1a9ba66c91676f2d207bbf\nbatch-id: 5787329323\nrun-id: 5787346798\n-->\n\nPowered by [copywrite](https:\/\/github.com\/hashicorp\/copywrite), made with :heart: by @HashiCorp","comments":[],"labels":["automated","legal"]},{"title":"Panic in Terraform console command","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\r\non linux_amd64\n```\n\n\n### Terraform Configuration Files\n\nNo config files required\n\n### Debug Output\n\nhttps:\/\/gist.github.com\/pcarles\/da14c26485822361018252868c66e62f\n\n### Expected Behavior\n\nTerraform should exit gracefully\n\n### Actual Behavior\n\nTerraform panic with a segfault\n\n### Steps to Reproduce\n\n1. `terraform console` anywhere\r\n2. Hit Ctrl + R to go in backward search\r\n3. Hit Ctrl + C to exit Terraform console\n\n### Additional Context\n\nYou don't need any configuration file to reproduce this error, and doing it with configuration files present do not change the behavior, it seems to be consistent.\r\n\r\nI may open a PR to to fix it when I have a bit of time\n\n### References\n\n_No response_","comments":["Thanks for this report! ","I took a look into this and I think it might actually be a bug from the ReadLine library. I'm seeing they have a very similar problem on exiting the console in this issue [here](https:\/\/github.com\/chzyer\/readline\/issues\/163).\r\n\r\nEdit: After a closer look I think this is just an issue with ReadLine, which is getting called in (and crashing) from the console. ","Seems like perfect timing.\r\n\r\n@crw, could you please assess if chzyer\/readline can be swapped out for a maintained fork - ergochat\/readline?\r\n\r\nhttps:\/\/github.com\/chzyer\/readline\/pull\/226#issuecomment-1571510856","Hi, I'm one of the ergochat\/readline maintainers and I'm very interested in seeing if our fork is suitable for use in terraform! From my initial investigation:\r\n\r\n1. Our latest release, [v0.0.3](https:\/\/pkg.go.dev\/github.com\/ergochat\/readline@v0.0.3), can be used in terraform as a drop-in replacement. (Please don't be scared off by the unstable version number; the excessive size of the upstream API had become a tarpit. We are working on shrinking the public API to something that is more maintainable.)\r\n2. The panic from upstream's [issue 163](https:\/\/github.com\/chzyer\/readline\/issues\/163) is still present in our fork, but it should be an easy fix that I can work on immediately.","I opened a PR for this: https:\/\/github.com\/ergochat\/readline\/pull\/33","I released the previously mentioned fix, released it as v0.0.5, and wrote a draft PR for Terraform that switches to ergochat\/readline@v0.0.5: #33305","Thanks for this information and the pull request! The question of changing \/ updating dependencies is being run past the security team. We appreciate the background and info on these libraries!"],"labels":["bug","upstream","new"]},{"title":"Skip flag if flag value is empty","body":"<!--\r\n\r\nDescribe in detail the changes you are proposing, and the rationale.\r\n\r\nSee the contributing guide:\r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/main\/.github\/CONTRIBUTING.md\r\n\r\n-->\r\n\r\n<!--\r\n\r\nLink all GitHub issues fixed by this PR, and add references to prior\r\nrelated PRs.\r\n\r\n-->\r\n\r\nFixes #33176 \r\n\r\n## Target Release\r\n\r\n<!--\r\n\r\nIn normal circumstances we only target changes at the upcoming minor\r\nrelease, or as a patch to the current minor version. If you need to\r\nport a security fix to an older release, highlight this here by listing\r\nall targeted releases.\r\n\r\nIf targeting the next patch release, also add the relevant x.y-backport\r\nlabel to enable the backport bot.\r\n\r\n-->\r\n\r\n1.4.x\r\n\r\n## Draft CHANGELOG entry\r\n\r\n<!--\r\n\r\nChoose a category, delete the others:\r\n\r\n-->\r\n\r\n### NEW FEATURES | UPGRADE NOTES | ENHANCEMENTS | BUG FIXES | EXPERIMENTS\r\n\r\n<!--\r\n\r\nWrite a short description of the user-facing change. Examples:\r\n\r\n- `terraform show -json`: Fixed crash with sensitive set values.\r\n- When rendering a diff, Terraform now quotes the name of any object attribute whose string representation is not a valid identifier.\r\n- The local token configuration in the cloud and remote backend now has higher priority than a token specified in a credentials block in the CLI configuration.\r\n\r\n--> \r\n\r\n-  Skips flags with empty value instead of failing the command.\r\n","comments":["[![CLA assistant check](https:\/\/cla.hashicorp.com\/pull\/badge\/signed)](https:\/\/cla.hashicorp.com\/hashicorp\/terraform?pullRequest=33177) <br\/>All committers have signed the CLA.","Thanks for this submission. Per the conversation in https:\/\/github.com\/hashicorp\/terraform\/issues\/33176, this behavior may not be desirable for the current design philosophy of Terraform. However, I'll leave this PR open until we have reached a resolution on the issue. Thanks again!"],"labels":["enhancement"]},{"title":"Empty flag values make the command fail","body":"### Terraform Version\n\n```shell\nTerraform v1.4.6\r\non darwin_amd64\n```\n\n\n### Terraform Configuration Files\n\n```terraform\r\nAny configuration would do.\r\n```\r\n\n\n### Debug Output\n\n```\r\n\u2502 Error: Failed to read variables file\r\n\u2502\r\n\u2502 Given variables file  does not exist.\r\n```\n\n### Expected Behavior\n\nWhen running the command `terraform plan -var-file=` I expect the command to act like it was run as I'm running `terraform plan`.\n\n### Actual Behavior\n\nThe command `terraform plan -var-file=` fails with error:\r\n```\r\nGiven variables file  does not exist.\r\n```\r\n\n\n### Steps to Reproduce\n\nRunning terraform apply or init with empty flag value on any configuration.\n\n### Additional Context\n\nThe reason that I need this is in building reusable github workflows I receive the path to the variable files in the inputs and sometimes user doesn't have a variable file, and we don't want the code to fail in this case.\n\n### References\n\n_No response_","comments":["Hi @bshramin,\r\n\r\nThanks for filing the issue. Passing a flag which is missing a required argument is not valid, so would usually indicate a mistake which we want to communicate to the user. Normally you would conditionally add the flag and argument together, is there a reason you can't do that in the workflow you are building?\r\n\r\nThanks!","I understand that, but imagine the case that someone is reading the value of these flags from the environment variables, and in case an environment variable is not set he wants to treat it as if the flag is not passed at all if the value is empty.\r\nWith the current implementation, the user must write an if statement for each flag value to check if the environment variable is set.","Hi @bshramin,\r\n\r\nBecause that sort of situation arises with lots of programs that are run from the command line, shells typically provide their own builtins to deal with it.\r\n\r\nFor example, if you are using Bash then I believe the following could concisely achieve the effect of only including a `-var-file=...` option if there's a non-empty filename to include after it:\r\n\r\n```bash\r\nterraform plan \"${TFVARS_FILE:+-var-file=$TFVARS_FILE}\"\r\n```\r\n\r\nThis exact syntax is Bash-specific, but other shells have different syntax with a similar effect. Can you use something like this to solve your problem?\r\n\r\nAs @jbardin said, we return an error here to give better feedback to folks who are using Terraform incorrectly, so I expect we would not implement exactly what you asked for here, but we could potentially add a specialized error for this situation which notices that the argument is an empty string and mentions that explicitly in the error message, instead of returning what seems like a broken sentence with a missing word.\r\n","In my case, I'm trying to build a reusable GitHub workflow and I'm taking the variable file path as an input to the workflow. Sometimes the users of the workflow don't have a variables file and will not pass any.\r\n\r\nI still think that it makes sense for `terraform plan -var-file=` and `terraform plan` to act the same way. I also understand your viewpoint of trying to prevent the user from making any mistakes. In that case, I agree that we can make the error message clearer."],"labels":["bug","new"]},{"title":"Custom error code for Terraform CLI if the check assertion fails","body":"### Terraform Version\n\n```shell\nTerraform v1.5.0-alpha20230504\n```\n\n\n### Use Cases\n\nWith the current implementation of the `check` block, assertion results do not affect the exit code of the **apply** command: if the apply is successful, a user will get `exit code 0` despite any failing assertions.\r\n\r\nIt would be useful to have a detailed exit code for such a case to rely on it in CI when `check` blocks are used for smoke\/functional tests of the created infrastructure.\r\n\r\nExample use case:\r\n```hcl\r\nresource \"local_file\" \"example\" {\r\n  filename = \"${path.module}\/example.txt\"\r\n  content  = \"foobars\"\r\n}\r\n\r\nresource \"aws_s3_bucket\" \"example\" {\r\n  bucket_prefix = \"tf-checks-example\"\r\n  force_destroy = true\r\n}\r\n\r\nresource \"aws_s3_object\" \"localfile\" {\r\n  bucket       = aws_s3_bucket.example.id\r\n  key          = \"example.txt\"\r\n  source       = local_file.example.filename\r\n  content_type = \"text\/*\"\r\n}\r\n\r\ncheck \"verify_s3object_content\" {\r\n  data \"aws_s3_object\" \"this\" {\r\n    bucket = aws_s3_bucket.example.id\r\n    key    = aws_s3_object.localfile.key\r\n  }\r\n  assert {\r\n    condition     = data.aws_s3_object.this.body == \"bananasapples\"\r\n    error_message = \"S3 object content must be bananasapples\"\r\n  }\r\n}\r\n```\r\n\r\nTerraform apply:\r\n```shell\r\n\u279c  check-block  # terraform apply -auto-approve                                                                              \r\nlocal_file.example: Refreshing state... [id=*****]\r\naws_s3_bucket.example: Refreshing state... [id=*****]\r\naws_s3_object.localfile: Refreshing state... [id=example.txt]\r\ndata.aws_s3_object.this: Reading...\r\ndata.aws_s3_object.this: Read complete after 0s [id=*****\/example.txt]\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\r\n <= read (data resources)\r\n\r\nTerraform will perform the following actions:\r\n\r\n  # data.aws_s3_object.this will be read during apply\r\n  # (config will be reloaded to verify a check block)\r\n <= data \"aws_s3_object\" \"this\" {\r\n      + body                   = \"foobars\"\r\n      + bucket                 = \"*****\"\r\n      + bucket_key_enabled     = false\r\n      + content_length         = 11\r\n      + content_type           = \"text\/*\"\r\n      + etag                   = \"7fac576e6ee21e27ce056bb55cbc1db0\"\r\n      + id                     = \"*****\/example.txt\"\r\n      + key                    = \"example.txt\"\r\n      + last_modified          = \"Wed, 10 May 2023 09:37:21 UTC\"\r\n      + metadata               = {}\r\n      + server_side_encryption = \"AES256\"\r\n      + storage_class          = \"STANDARD\"\r\n      + tags                   = {}\r\n    }\r\n\r\nPlan: 0 to add, 0 to change, 0 to destroy.\r\ndata.aws_s3_object.this: Reading...\r\ndata.aws_s3_object.this: Read complete after 1s [id=*****\/example.txt]\r\n\u2577\r\n\u2502 Warning: Check block assertion failed\r\n\u2502 \r\n\u2502   on main.tf line 33, in check \"verify_s3object_content\":\r\n\u2502   33:     condition     = data.aws_s3_object.this.body == \"bananasapples\"\r\n\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\u2502     \u2502 data.aws_s3_object.this.body is \"foobars\"\r\n\u2502 \r\n\u2502 S3 object content must be bananasapples\r\n\u2575\r\n```\r\n\r\nExit code:\r\n\r\n```shell\r\n\u279c  check-block  # echo $?                                                                                                    \r\n0\r\n```\r\n\n\n### Attempted Solutions\n\nnone\n\n### Proposal\n\n`-detailed-exitcode` option for apply command and status `2` for the case of failed assertions, similar to the one that `plan` command has ([-detailed-exitcode](https:\/\/developer.hashicorp.com\/terraform\/cli\/commands\/plan#detailed-exitcode))\n\n### References\n\n_No response_","comments":["Hi @vasylenko! Thanks for sharing this use-case.\r\n\r\nPart of the design requirements for `check` blocks is that they are not treated as errors, and instead just as warnings after the apply is complete. This is what differentiates `check` blocks from `precondition` and `postcondition` blocks elsewhere in the language.\r\n\r\nFor your use-case, would it be sufficient for you to define a postcondition on `resource \"aws_s3_object\" \"localfile\"`, so that it'll be considered failed if the content doesn't match what you specified?\r\n\r\n```hcl\r\nresource \"aws_s3_object\" \"localfile\" {\r\n  bucket       = aws_s3_bucket.example.id\r\n  key          = \"example.txt\"\r\n  source       = local_file.example.filename\r\n  content_type = \"text\/*\"\r\n\r\n  lifecycle {\r\n    postcondition {\r\n      condition = self.content == \"bananasapples\"\r\n      error_message = \"S3 object content must be bananasapples.\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAlternatively, you could write it as a precondition checking that the `local_file.example` object meets the expectation, which would then allow Terraform to detect it before even creating the S3 object, so you would never have an invalid object in your bucket:\r\n\r\n```hcl\r\nresource \"aws_s3_object\" \"localfile\" {\r\n  bucket       = aws_s3_bucket.example.id\r\n  key          = \"example.txt\"\r\n  source       = local_file.example.filename\r\n  content_type = \"text\/*\"\r\n\r\n  lifecycle {\r\n    precondition {\r\n      condition = local_file.example.content == \"bananasapples\"\r\n      error_message = \"File content must be bananasapples.\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIn both of these cases `terraform apply` should fail with a nonzero exit code if the condition isn't met. In the latter case, Terraform may be able to catch the problem during the plan phase, as long as the content of the local file content is known during planning as is the case in these contrived examples.\r\n\r\n---\r\n\r\nAnother different approach would be to analyze the checks in the resulting state after `terraform apply` has finished. To do that, you could run `terraform show -json` to obtain [the documented JSON state representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#state-representation), and then use [the checks representation](https:\/\/developer.hashicorp.com\/terraform\/internals\/json-format#checks-representation) to examine the check results using some custom software written by you.\r\n\r\nA simple implementation would be to check just the top-level checks property `\"status\"`, which summarizes the statuses of all checkable objects into a single string value.\r\n\r\nThat particular part of the JSON output format is not yet committed as stable, but feedback about folks successfully using it will help establish the confidence to include it in Terraform's compatibility promises in a future version.\r\n\r\nWe typically use these JSON formats as the integration point for new features because JSON is a far more flexible and extensible format for sharing data than process exit codes. `-detailed-exitcode` is covered by Terraform's compatibility promises and so will be preserved for the foreseeable future, but it's also possible to achieve the effect of that option by analyzing the output from `terraform show -json PLANFILE`, so this JSON-based approach is a superset of what we can possibly represent by process exit codes.\r\n\r\n","Hi @apparentlymart, and thank you for such a detailed response! \r\n\r\nMy use case was an example one, not a real-life one. I just wanted to illustrate how different resources were created together to compose some logical unit\/functionality that should have been tested as a whole afterward (so postcondition would not fit because it is bound to a specific resource). Looks like, not the best example \ud83d\ude05\r\n\r\nBut anyway, the point was in the way to automate the post-apply checks validation. \r\n\r\nI appreciate your explanation of the JSON approach \ud83d\udc4d I did not know that `checks` are stored in the state file. Actually, I thought (for some reason) that they were not stored intentionally. \r\nAnd it seems that such an approach fits well into CI automation. \r\n\r\nOne note, though, is that verifying the checks from the plan file looks like less reliable than reading the post-apply state file because some check assertions might be altered by the apply. I am referring to the last part of your response, where you mentioned `terraform show -json PLANFILE`. \r\n","Indeed, if you check the plan JSON then some of the checks may have an unknown status, if the condition depends on something Terraform cannot predict until the apply step. There isn't any way to avoid that; Terraform can only report what it knows. Anything that Terraform knows enough to generate an error or warning about during the plan phase will also be available in the plan JSON though, since these both use the same source of data.\r\n","> Part of the design requirements for check blocks is that they are not treated as errors, and instead just as warnings after the apply is complete. This is what differentiates check blocks from precondition and postcondition blocks elsewhere in the language.\r\n\r\nAnother thing that differentiates them though is that they're 'global' - not attached to a particular resource. You can make assertions in a `check` block for which there's no obvious `resource` to put it in as a `lifecycle` `precondition`.\r\n\r\nFor example:\r\n```terraform\r\ncheck \"not_default_workspace\" {\r\n  assert {\r\n    condition     = terraform.workspace != \"default\"\r\n    error_message = \"Selected workspace must be [...] Do not use the default workspace.\"\r\n  }\r\n}\r\n```\r\n\r\nA workaround I suppose could be:\r\n```terraform\r\nresource \"null_resource\" \"not_default_workspace\" {\r\n  lifecycle {\r\n      precondition {\r\n       condition     = terraform.workspace != \"default\"\r\n       error_message = \"Selected workspace must be [...] Do not use default workspace.\"\r\n     }\r\n   }\r\n}\r\n```\r\n\r\nbut it seems a bit unfortunate and (as if it would be) unidiomatic - seeing that I could easily think 'oh, it should just be a `check`', overlooking the difference in exit code behaviour.\r\n\r\nAnother way of saying this I suppose is that there's no way of doing validation on quasi input variables like `terraform.workspace`.","Indeed, the requirement that a precondition or postcondition be able to prevent evaluation of stuff downstream means that they must be attached to _something_ so that there's a clear idea of what is \"downstream\".\r\n\r\n`check` blocks cannot actually block anything from happening and so they don't have that requirement. But it also means that they aren't useful for a check like what you showed here -- that the workspace is valid -- because Terraform would still complete all of the other operations implied by the configuration before actually checking that rule.\r\n\r\nAttaching a precondition to a `null_resource` or `terraform_data` and then making everything else depend on that resource is the best we can do for that use-case right now, but I agree that it's not really a valid solution. That's just not a use-case the current features were designed to address.\r\n\r\nMy first idea for that particular use-case would be something like whole-module preconditions that must hold or else Terraform will not evaluate anything inside that module at all. Some sense of pre and postconditions for whole modules is something we considered but cut from the initial scope because we wanted to do more research before committing to a design, and this use-case will be a good input into that research, so thanks!\r\n\r\nThis particular example pokes at one of the specific things we wanted to research more after the initial round of pre\/postconditions work: for resources there's both logic written by the provider developer that applies to any use of a resource type, and logic in the calling resource block (the condition blocks) that only applies to a particular resource block. By analogy then, it seems like an equivalent mechanism for modules would require both a way to specify preconditions inside the module that apply to all uses of it, and preconditions inside the _call_ (the `module` block in the parent module) that apply only to one particular call to the module. The details of that are a little more tricky than they seem though, because provider developers get to return errors from the perspective of the calling resource block rather than from the internals of the provider, and we ought to provide a similar facility for module authors so that their modules can return useful error messages that refer to the caller's mistake directly, rather than to some internal detail inside the module (as tends to happen with both the \"condition on a null resource\" workaround and with slightly misusing check blocks for configuration enforcement rather than the \"is the remote system working as expected after apply?\" checks that feature is designed for.)\r\n\r\nThe example of enforcing a particular workspace seems like it would be most at home as a precondition inside the root module. It being in the root module rather than a child module does simplify things a little because there is no calling module block to try to attribute the error to, and so maybe there's room for a partial design that initially only deals with that simpler case, as long as there seems to be a plausible path from there to the more general solution later.\r\n","Thank you for the reply, that's all very thorough and perfect-sense-making, as usual.\n\nI only wanted to add that your 'and then [make] everything else depend on it' addition to my proposed workaround made me realise that was missing - for something other than the workspace, which is a bit of a special case - but I think for a broader class adding the thing which is conditioned on to `triggers` should be sufficient.\n\nRegarding your last paragraph, actually something I na\u00efvely tried (I thought the docs slightly vague in a way that made it seem plausible) was a `precondition` block outside of any resource or `lifecycle` block. I think that's what you were alluding to anyway, but that would seem to me a neat continuation of syntax - it could benefit root modules as well as children outside of what's validatable (\ud83e\udd28) via variables. Including as syntactic sugar for cases where some `local` variable is determined entirely by module variables, but the derivation is complex and a module-level condition would be significantly simpler than variable-level validation. In fact I believe it would be _required_ if the condition were dependent on multiple variables? (Sum to less than X, combine to form a valid subnet, etc.)","Yeah, to show something a bit more concrete about what I was describing as \"attributing an error to the call\", one of the earlier design sketches included something like this:\r\n\r\n```hcl\r\nprecondition {\r\n  condition     = var.b == var.a\r\n  error_message = \"b must equal a.\"\r\n\r\n  # Possible new addition: names a particular object to \"blame\" if the\r\n  # condition doesn't hold, so we have a single unambiguous thing\r\n  # to highlight in the error message.\r\n  cause = var.b\r\n}\r\n```\r\n\r\nI think the original sketch I'm thinking of actually had this `cause` thing illustrated inside a resource-scoped `precondition` rather than a toplevel, and I'd imagined that it could also be valid to say something like `cause = aws_instance.foo` if the problem should be attributed to another resource inside the same module, but that ended up being of marginal benefit since you can write a similar thing with a `postcondition` inside `aws_instance.foo` anyway.\r\n\r\nUsing an input variable as a cause does allow something that cannot be expressed today though, particularly if placed in a top-level `precondition` block as you imagined, since it would be a bit odd to have a \"postcondition\" for an input variable when input variables don't actually have any side-effects for it to be a postcondition of.\r\n","Just to add to this conversation, I have a use case with AWS Aurora where if you have auto-minor-updates enabled, AWS will automatically update your db instance and cause drift between terraform and AWS. This can be disastrous as AWS will destroy and recreate your database if you try to apply a lower minor version. \r\n\r\nWe have added the `check` block to check for a delta in the version where our input variable is a lower version number than what is in AWS. It would be nice if this would cause an error rather than a warning specifically as warnings are very easy to over look within a CI pipeline.","Hi @andrewhertog,\r\n\r\nWith the checks mechanism as implemented today it's not designed to support use-cases that compare the planned new state to the prior state; I assume you've achieved that by doing something a little risky with fetching the same object as a `data` block as you're also managing with a `resource` block.\r\n\r\nThe current intended way to deal with policy around what _changes_ are acceptable is to save the plan to disk, export it as JSON, and implement the checks in an external program that analyzes the plan JSON. I would suggest using that approach instead with today's Terraform because it is the mechanism that was designed to solve this problem, rather than lightly abusing a feature that was intended to solve a different problem -- verifying that the system is in a consistent state _after apply is complete_.\r\n\r\nWith all of that said, I don't mean to say that it's not valid to want a Terraform language feature for constraining what proposed changes are valid or invalid, to avoid the need for an external program. I just mean that I think that would want to be a new feature separate from checks, since the use-cases that motivated checks were all about post-apply checking rather than plan checking. I think the testing language coming in Terraform v1.6 will provide some inspiration about what it might look like to describe rules for validating planned changes, since testing that a particular change to input variables doesn't cause knock-on disruptive changes is one of the intended use-cases. Perhaps we will eventually borrow some of the syntax and vocabulary from the testing language to use in the main language too, so that assertions about plan validity will look the same across both main configuration and test cases.\r\n\r\n","> Part of the design requirements for `check` blocks is that they are not treated as errors, and instead just as warnings after the apply is complete. This is what differentiates `check` blocks from `precondition` and `postcondition` blocks elsewhere in the language.\r\n\r\nHi @apparentlymart \r\n\r\nI stumbled across this thread today.\r\n\r\nI agree with the design choice here, however I'd love there to be a native method to pull check status without resorting to `state pull` and `jq`.\r\n\r\nI think there is a parallel with `plan` where the changes detected is not an error, however we can represent it as one by choice. Therefore I think that a `-detailed-exitcode` for `apply` could work.\r\n\r\nAlternatively, perhaps an entire subcommand could be dedicated to retrieving check status?\r\n\r\nProposal, inspired by `output`:\r\n\r\n```\r\n$ terraform check -help\r\nUsage: terraform [global options] check [options] [NAME]\r\n\r\n  Displays check information from a Terraform state file and prints the status and error message.\r\n  Leave the name empty to show all checks.\r\n\r\nOptions:\r\n    -failed       List failed checks only\r\n    -json         Machine readable output\r\n```\r\n","Thanks for that suggestion, @matt-FFFFFF!\r\n\r\nFunnily enough one of the earlier prototypes that the current checks functionality evolved from did have a CLI command for retrieving the checks, although for that prototype I only implemented a human-oriented form of it: https:\/\/github.com\/hashicorp\/terraform\/pull\/31268\r\n\r\nIt does seem reasonable to me to have a specialized command to report on the current check status in the latest state snapshot. I've not personally been working on the details of checks for a while now, so I can't say anything about feasibility or priority at the moment, but it does seem like an idea worth exploring.\r\n\r\n\r\n\r\n"],"labels":["enhancement","new"]},{"title":"Function to convert boolean to 1 or 0","body":"### Terraform Version\n\n```shell\nTerraform v1.4.3\r\non linux_amd64\n```\n\n\n### Use Cases\n\nThe main use case I have for such a function is to use as the value to the `count` meta argument, to simplify conditionally creating resources.\n\n### Attempted Solutions\n\nThe standard way to do this would be to use a ternary operator with something like:\r\n\r\n```terraform\r\nresource \"null_resource\" \"example\" {\r\n  count = var.should_create ? 1 : 0\r\n}\r\n```\r\n\r\nAnd this works, but it is kind of awkward, and IMO not very readable. \n\n### Proposal\n\nAdd a new function that takes a boolean, and returns 1 if the boolean is true, or 0 if it is false. It would  effectively be something like:\r\n\r\n```\r\nf(x) = x ? 1 : 0\r\n```\r\n\r\nThis could potentially just be extending the `tonumber` function to accept a boolean value or it could be new function. If the  latter, I'm not sure what the best name for it would be. Perhaps \"condcount\", or \"boolocount\", or possibly something like \"enabled\" to indicate the specific use case it is useful for. \n\n### References\n\nThis has a similar goal to #21953, but is a smaller change, and doesn't require any fundamental changes to the language or design of terraform, so could be landed in a more reasonable amount of time. ","comments":["I would be happy to help with the implementation if adding this function is something the maintainers are willing to accept, and we decide on a name for it.","Thanks for this request, I'll bring it to triage to discuss your proposal. ","I'd normally do the the following in a local\r\n```\r\nlocals {\r\n  my_var_true = var.should_create ? 1 : 0\r\n}\r\n\r\nresource \"thing\" \"this\" {\r\n  count = local.my_var_true\r\n```\r\n","Another function that has a somewhat related usecase is a function that is the inverse of `one`. meanting that it takes a scalar possibly null value , and if it is null creates an empty list, if it is non-null creates a singleton list containing just that item, so it woudl be equivalen to:\r\n\r\n```\r\nsingleton(x) = x != null ? [x] : []\r\n```\r\n\r\nthis would be useful for dynamic blocks, since you could do something like:\r\n\r\n```terraform\r\n\r\ndynamic \"block\" {\r\n  for_each = singleton(var.my_var)\r\n\r\n  content {\r\n     some_attr = block.x\r\n  }\r\n}\r\n```","@tmccombs I brought up the original use case in triage and the response was that we do not usually introduce new ways to solve a problem (as a built-in function, for example) if there is already a (relative concise) way to accomplish the same thing. In this case the ternary operator was deemed to be sufficiently uncomplicated for the use case. The subjective part here is \"what is more or less readable code,\" but that is the reasoning of the maintainers in this case.\r\n\r\nThe second looks like it would follow similar logic, but is unique enough that I can re-raise the issue. ","IMO, something like:\r\n\r\n```\r\ncount = if(var.enabled)\r\n```\r\n\r\nor even\r\n```\r\ncount = tonumber(var.enabled)\r\n```\r\n\r\nis a lot more readable than \r\n\r\n```\r\ncount = var.enabled ? 1 : 0 \r\n```\r\n\r\nand even more so, if instead of just a variable reference we have something like `var.something != null` since you now have a lot of symbols , and either need to worry about order of operations, or add parenthesis, which adds even more symbols.\r\n\r\nFor my `singleton` function, it also has the benefit, that you no longer have to repeat the expression multiple times. That is more of an issue if the expression is more complicated than just a variable lookup. Sometimes you can use a local to factor that out, but not always, for example, if the dynamic block is part of a resource that itself uses for_each or count, and you need to reference the iteration variable. ","Thank you for your continued interest in this issue. \r\n\r\n[Terraform version 1.8](https:\/\/github.com\/hashicorp\/terraform\/blob\/v1.8.0-beta1\/CHANGELOG.md) launches with support of provider-defined functions. It is now possible to implement your own functions! We would love to see this implemented as a provider-defined function. \r\n\r\nPlease see the [provider-defined functions documentation](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework\/functions) to learn how to implement functions in your providers. If you are new to provider development, learn how to [create a new provider](https:\/\/developer.hashicorp.com\/terraform\/plugin\/framework) with the Terraform Plugin Framework. If you have any questions, please visit the [Terraform Plugin Development category in our official forum](https:\/\/discuss.hashicorp.com\/c\/terraform-providers\/tf-plugin-sdk\/43). \r\n\r\nWe hope this feature unblocks future function development and provides more flexibility for the Terraform community. Thank you for your continued support of Terraform!","https:\/\/developer.hashicorp.com\/terraform\/language\/functions\/tonumber should simply accept booleans as arguments.\r\nYou already can uglily to do: tonumber(tostring(true))\r\n\r\nI have another use case for the request also."],"labels":["enhancement","functions","new"]},{"title":"Add an option to validate S3 PutObject with ContentMD5","body":"### Terraform Version\r\n\r\n```shell\r\n1.3.7\r\n```\r\n\r\n\r\n### Use Cases\r\n\r\nI'm using a custom S3 based solution which works with the S3 backends perfectly 99% of the time\r\nHowever, i've encountered cases where S3 reported that a state has been updated successfully when in fact the state uploaded somehow did not include all of the information of the created resources\r\n\r\n### Attempted Solutions\r\n\r\nIn order to work around this issue i'm currently using a local state and manually upload it using S3's [uploader Upload](https:\/\/stackoverflow.com\/questions\/38442512\/difference-between-upload-and-putobject-for-uploading-a-file-to-s3) functionality which has built in validation support\r\n\r\n### Proposal\r\n\r\nPutObject has support for validating the file uploaded using ContentMD5\r\ntherefore, defining a flag, i.e. `content_md5_validation = true` and then accordingly having a configuration similar to \r\n\r\nhttps:\/\/github.com\/hashicorp\/terraform\/blob\/ddd87994bfbd6765328739c2e185f700b68245c4\/internal\/backend\/remote-state\/s3\/client.go#L157\r\n\r\n```\r\ni := &s3.PutObjectInput{\r\n\t\tBody:        aws.ReadSeekCloser(data),\r\n\t\tContentMD5:  aws.String(fmt.Sprintf(\"%x\", md5.Sum(data))),\r\n}\r\n```\r\n\r\nwould most likely solve my issue\r\n\r\n### References\r\n\r\n_No response_","comments":[],"labels":["enhancement","backend\/s3"]}]
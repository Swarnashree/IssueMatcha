[{"title":"Fix missing docs in time series source reader","body":"If we reach the [max_page_size](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106705\/files#diff-8bb21ea96e1c7ab1c4e97c1ad295414b084f9703c335962e740ddff28ff00c72L271) limit, and the [tsid](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106705\/files#diff-8bb21ea96e1c7ab1c4e97c1ad295414b084f9703c335962e740ddff28ff00c72L274) changes, we will miss that document because we will call [nextDoc](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106705\/files#diff-8bb21ea96e1c7ab1c4e97c1ad295414b084f9703c335962e740ddff28ff00c72L247)() again when resuming reading. I found this issue while working on the [rate](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106703) function. I think this operator should respect the max_page_size limit and not emit pages exceeding that threshold.","comments":[],"labels":["v8.14.0"]},{"title":"Move more XContent parsing to test codebase","body":"Follow up to #105801 moving more parsers that are test-only over to the test codebase.\r\n","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">non-issue",":Core\/Infra\/Core","Team:Core\/Infra","v8.14.0"]},{"title":"Add rate aggregation function","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"fix comment in IndicesAdminClient","body":"fix comment around prepareOpen method","comments":["<!-- CLA-CHECK:106702 -->\n&#10060; Author of the following commits did not sign a [Contributor Agreement](https:\/\/www.elastic.co\/contributor-agreement):\n   c58aa2cff2128c9fe893eabec381dc8e483d5cd6\n\n   Please, read and sign the above mentioned agreement if you want to contribute to this project","\n\n\n    I have signed the above mentioned agreement \uff0cSee the attached file for details .  But cla-checker check failed again. \n\n\n\n\n\n\n\n\n\n\n\n\n\nAt 2024-03-23 22:25:52, \"cla-checker-service[bot]\" ***@***.***> wrote:\n\n\u274c Author of the following commits did not sign a Contributor Agreement:\nc58aa2c\n\nPlease, read and sign the above mentioned agreement if you want to contribute to this project\n\n\u2014\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>"],"labels":["needs:triage","external-contributor","v8.14.0"]},{"title":"Log skipped elections due to shutdown marker","body":"Relates ES-6576\r\n","comments":[],"labels":[">non-issue",":Distributed\/Cluster Coordination","Team:Distributed","v8.14.0"]},{"title":"[ML] Adding elementType to service settings and persisting byte instead of int8","body":"This PR aims to make it easier to retrieving the `element_type` from the inference endpoint. Currently cohere is the only service that supports an `element_type` of something other than `float`. Cohere is also the only service that persists this information. The others (huggingface, openai) simply return `float` in the `elementType` method.\r\n\r\n## Open questions\r\n\r\n- Should we switch the field from `embedding_type` to `element_type` for Cohere?\r\n- Should we add a new `element_type` field to huggingface and openai, we can make it optional in a PUT request and default it to `float`?","comments":[],"labels":[">non-issue",":ml","Team:ML","v8.14.0"]},{"title":"FIX : ProactiveStorageDeciderService#scale throws IndexNotFoundException","body":"ProactiveStorageDeciderService#scale attempts to forecast disk usage by simulating rollovers of data streams\uff0cwhich it does by copying the index metadata of the current write index, but current write index closed throws IndexNotFoundException.\r\nuse open write index .\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/105844","comments":[],"labels":["needs:triage","external-contributor","v8.14.0"]},{"title":"Updating FullClusterRestartIT.testWatcher to account for watcher running","body":"FullClusterRestartIT.testWatcher calls `_watcher\/watch\/new_watch` twice and asserts that the version of the watch is then 2. However, when the watch is executed every 500ms, the watch document is updated with status information. If that happens in between calls to `_watcher\/watch\/new_watch`, then the version will be 3 rather than 2. This PR changes the assertion to make sure that the version is 2 or greater.\r\nThis is part of #48381, but not the whole thing.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">test",":Data Management\/Watcher","Team:Data Management","v8.14.0"]},{"title":"WIP - Port docker distribution tests away from test fixture plugin","body":null,"comments":["Pinging @elastic\/es-delivery (Team:Delivery)"],"labels":[">non-issue",":Delivery\/Build","Team:Delivery","v8.14.0"]},{"title":"Unified highlighter with bool query containing nested query throws illegal argument exception","body":"### Elasticsearch Version\n\n8.10+\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\n_bundled_\n\n### Problem Description\n\nWhen running a bool query containing nested query with unified highlighting we sometimes get illegal argument exception.\r\nQuery:\r\n```\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": {\r\n        \"match\": {\r\n          \"subject\": {\r\n            \"query\": \"bike\"\r\n          }\r\n        }\r\n      },\r\n      \"must_not\": {\r\n        \"nested\": {\r\n          \"query\": {\r\n            \"match\": {\r\n              \"attachments.name\": {\r\n                \"query\": \"invite\"\r\n              }\r\n            }\r\n          },\r\n          \"path\": \"attachments\"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"highlight\": {\r\n    \"require_field_match\": true,\r\n    \"fields\": {\r\n      \"subject\": {\r\n        \"type\": \"unified\"\r\n      }\r\n    }\r\n  }\r\n}\r\n``` \r\nError:\r\n```\r\n\"type\": \"illegal_argument_exception\",\r\n\"reason\": \"Reader FilterLeafReader(FilterLeafReader(FilterLeafReader(FieldUsageTrackingLeafReader(reader=FilterLeafReader(_gp(8.11.1):C3054\/2:[diagnostics={java.runtime.version=11.0.10+9-LTS, mergeFactor=2, os.arch=x86_64, source=merge, java.vendor=Amazon.com Inc., os.version=10.16, mergeMaxNumSegments=-1, java.version=11.0.10, java.vm.version=11.0.10+9-LTS, lucene.version=8.11.1, timestamp=1697821346583, os=Mac OS X}]:[attributes={Lucene87StoredFieldsFormat.mode=BEST_SPEED}]:fieldInfosGen=1:dvGen=1 :softDel=2 :id=9mvdr58lxesb0l6jrs63zxpmt))))) does not support caching\"\r\n``` \r\n\r\nWe have done some investigating and testing:\r\n\r\n- We think the issue we are experiencing is related to this change #96068 introduced in 8.10. We have tested with 8.9. and indeed there was no error, we also tested with 8.11.3 and 8.12.2 where we got the error.\r\n- When we change to \"plain\" highlighter or we set \"require_field_match\" to false (i.e. we turn off Weight#matches mode) there is no error.\r\n- There was similar problem reported here #101667 and resolved here #101713 by disabling Weight#matches mode in that specific case. Two more issues related to this are #101141 and #101621 .\n\n### Steps to Reproduce\n\nWe have tried creating minimal set of steps for reproducing this but we failed. Query posted above is indeed minimal one that produced an error. In the process we learned that query itself is not enough to reproduce it, indexed data set is also relevant. Both fields hit by the query \"subject\" and \"attachments.name\" are mapped as following:\r\n```\r\n\"type\": \"text\",\r\n\"term_vector\": \"with_positions_offsets\"\r\n``` \r\nAlso, if \"must_not\" part of the query contains a query which hits no documents we don't get the error, only if some documents are hit by that query error appears.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">bug",":Search\/Search","Team:Search"]},{"title":"[ESQL] Migrate BooleanFunctionEqualsElimination optimization","body":"Relates to https:\/\/github.com\/elastic\/elasticsearch\/pull\/105217\r\n\r\nThis copies the BooleanFunctionEqualsElimination logical optimization into ESQL, following the pattern established in https:\/\/github.com\/elastic\/elasticsearch\/pull\/106499. I've copied the optimization rule into the ESQL version of OptimizerRules, and the tests into OpitmizerRulesTests, and changed the imports &c to point to the appropriate ESQL classes instead of their QL counterparts.\r\n\r\nI only saw two tests for this one.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">non-issue","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"Fix range queries for float\/half_float fields when bounds are out of type's range","body":"https:\/\/github.com\/elastic\/elasticsearch\/issues\/105079\r\n\r\nIn the case of float\/half_float types when the provided bounds for `range` are out of the type's range, we will no longer fail with a `supports only finite values, but got [Infinity]` error, but construct the range query such that the overflowing bound is the min\/max values of that type's range.\r\n\r\nFor example:\r\n\r\n```\r\nPOST \/testidx\/_search\r\n{\r\n  \"query\": {\r\n    \"range\": {\r\n      \"half_float_field\": {\r\n        \"lt\": 1e+300\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwill be equivalent to:\r\n```\r\n{\r\n  \"query\": {\r\n    \"range\": {\r\n      \"half_float_field\": {\r\n        \"lt\": 65504\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nsince `65504` is the largest positive value a `half_float` field can take.\r\n","comments":[],"labels":["v8.14.0"]},{"title":"ESQL: Fix count pushdown for unmapped fields","body":"Fix https:\/\/github.com\/elastic\/elasticsearch\/issues\/105400\r\n\r\nUnmapped fields are multi-valued per default. We wrongly treat them as single-valued, though, when pushing `COUNT` down to Lucene. This leads to `COUNT(field)` returning the number docs with non-null `field`, rather than the total number of `field` values.","comments":[],"labels":[">bug","auto-backport",":Analytics\/ES|QL","v8.13.1","v8.14.0"]},{"title":"ES|QL: Improve support for TEXT fields in functions","body":"Related to https:\/\/github.com\/elastic\/elasticsearch\/issues\/105379\r\n\r\nESQL functions that support KEYWORD fields, typically also support TEXT fields.\r\nA few functions, indentified by https:\/\/github.com\/elastic\/elasticsearch\/issues\/105379, did not respect this convention.\r\n\r\nThis PR adds support for TEXT fields (where KEYWORD fields were allowed) for:\r\n- split()\r\n- cidr_match()\r\n- date_format()\r\n- date_parse()\r\n- date_extract()\r\n\r\nIt also adds proper unit tests for `cidr_match()` and `date_format()`","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106688.docs-preview.app.elstc.co\/diff)","Hi @luigidellaquila, I've created a changelog YAML for you.","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"Test vector search with query_vector_builder on one shard","body":"To avoid possible fluctuations in number of neighbors (with dsl), one shard should be used in the yaml test.\r\n\r\ncloses #106650 ","comments":[],"labels":[">test","needs:triage","v8.14.0"]},{"title":"SharedBlobCacheService.maybeFetchRegion should use computeCacheFileRegionSize","body":"This method computes the exact ranges to fetch using the length of the blob, but that does not work for `SharedBlobCacheService` implementations that use a specific `computeCacheFileRegionSize` which is not based on blob length.\r\n\r\nNote: I'll hold on merging this change until callers of `maybeFetchRegion` use the appropriate `copyToCacheFileAligned` method that fully consumes input streams.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","Hi @tlrx, I've created a changelog YAML for you."],"labels":[">bug",":Distributed\/Snapshot\/Restore","Team:Distributed","v8.14.0"]},{"title":"Add release notes for v8.13.0 release","body":"This PR was auto-generated. Please review, edit, and move out of Draft.","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106684.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-docs (Team:Docs)"],"labels":[">docs","Team:Docs","v8.13.0","v8.13.1"]},{"title":"org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.testFetchTooManyMvLongs Lucen Snapshot Failure","body":"### CI Link\n\nN\/A\n\n### Repro line\n\n .\/gradlew ':test:external-modules:test-esql-heap-attack:javaRestTest' --tests \"org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.testFetchTooManyMvLongs\" -Dtests.seed=F1F12175C991F463 -Dtests.locale=mt-MT -Dtests.timezone=Asia\/Jerusalem -Druntime.java=22\n\n### Does it reproduce?\n\nYes\n\n### Applicable branches\n\nlucene_snapshot\n\n### Failure history\n\n_No response_\n\n### Failure excerpt\n\nThis only fails on the Lucene Snapshot branch. It didn't fail with the same seed on main. Also, I tried pulling in the latest main to ensure we didn't miss a commit, and it still failed. \r\n\r\nSomething funky with the new lucene snapshot and this test maybe?\r\n\r\n```\r\norg.elasticsearch.xpack.esql.heap_attack.HeapAttackIT > testFetchTooManyMvLongs FAILED\r\n    java.lang.AssertionError:\r\n    Expected: <0>\r\n         but: was <1>\r\n        at __randomizedtesting.SeedInfo.seed([F1F12175C991F463:BA3FFB02B463516E]:0)\r\n        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\r\n        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\r\n        at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2147)\r\n        at org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.assertWriteResponse(HeapAttackIT.java:558)\r\n        at org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.initIndex(HeapAttackIT.java:546)\r\n        at org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.initMvLongsIndex(HeapAttackIT.java:517)\r\n        at org.elasticsearch.xpack.esql.heap_attack.HeapAttackIT.testFetchTooManyMvLongs(HeapAttackIT.java:406)\r\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Pinging @elastic\/es-search (Team:Search)","@dnhatn may be interested in this one.","Ah yeah, i should have adjusted the test label to `low-risk`. \r\n\r\nLooking at `CHANGES.txt` for Lucene 9.11, nothing immediately stands out to me as a big enough optimization to keep this from tripping. "],"labels":[":Search\/Search",">test-failure","Team:Search","Team:Analytics",":Analytics\/ES|QL","low-risk"]},{"title":"Retrieve routing hash from id when missing","body":"In `applyTranslogOperation`, the routing field in the `DocumentParserContext`'s `SourceToParse` is not set, while the id is. Post parsing in `TimeSeriesRoutingHashFieldMapper` should retrieve the routing hash from the id prefix in this case.\r\n\r\nFixes #106550","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">non-issue",":StorageEngine\/TSDB","Team:StorageEngine","v8.14.0"]},{"title":"[Profiling, 9.0.0] Remove backwards compatibility for missing `cloud.provider`, `cloud.region`, `host.type`","body":"https:\/\/github.com\/elastic\/elasticsearch\/blob\/0eca8dda92d0b680d9549b2f387677f5f09adf01\/x-pack\/plugin\/profiling\/src\/main\/java\/org\/elasticsearch\/xpack\/profiling\/InstanceType.java#L76","comments":["Pinging @elastic\/obs-knowledge-team (Team:obs-knowledge)"],"labels":[":UniversalProfiling\/Application","Team:obs-knowledge"]},{"title":"ESQL QL Grand Refactor","body":"### Description\r\n\r\nWhen we built ESQL we built it on top of QL's core, which itself was refactored fairly quickly out of SQL. It's what we had time for and it had a lot of goodies. But it brought a lot of baggage. SQL and EQL live on top of _search and ESQL doesn't, for example.\r\n\r\n* [ ] Find all of the tests in SQL, EQL, and QL that we rely on from ESQL and port them to ESQL\r\n* [ ] Do not depend on QL's `Expression`\r\n* [ ] Replace `EvalMapper` with calls to `EvaluatorMapper#toEvaluator`\r\n* [ ] Move everything from `EvaluatorMapper` to ESQL's `Expression` superclass\r\n* [ ] Unify data types management (ie. avoid mix of`DataTypes` and `EsqlDataTypes`)\r\n* [ ] Unify type resolution management (ie. avoid mix of `TypeResolutions` and `EsqlTypeResolutions`)\r\n* [ ] Double-check exception management (ie. make sure that all the exceptions thrown are `EsqlClientException`, and not only `QlClientException`)\r\n* [ ] Double-check the usage of QL optimization rules (see [QL OptimizerRules](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/ql\/src\/main\/java\/org\/elasticsearch\/xpack\/ql\/optimizer\/OptimizerRules.java) vs [ESQL OptimizerRules](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/optimizer\/OptimizerRules.java))\r\n\r\nmore","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL"]},{"title":"Fix concurrency bug in AbstractStringScriptFieldAutomatonQuery","body":"Back when we introduced queries against runtime fields, Elasticsearch did not support\r\ninter-segment concurrency yet. At the time, it was fine to assume that segments will be\r\nsearched sequentially. AbstractStringScriptFieldAutomatonQuery used to have a BytesRefBuilder\r\ninstance shared across the segments, which gets re-initialized when each segment starts its work.\r\nThis is no longer possible with inter-segment concurrency.\r\n\r\nCloses https:\/\/github.com\/elastic\/elasticsearch\/issues\/105911","comments":["Pinging @elastic\/es-search (Team:Search)","Hi @javanna, I've created a changelog YAML for you."],"labels":[">bug",":Search\/Search","Team:Search","v8.13.1","v8.14.0"]},{"title":"Expose effective watermark thresholds via APIs","body":"The effective low\/high\/flood watermarks that apply to the cluster depends on both the [thresholds and the max_headrooms](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/modules-cluster.html#disk-based-shard-allocation) settings. To make it easier to see what the effective value is, we could calculate\/expose the threshold under `_nodes\/stats` and\/or `_cat\/allocation` APIs.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","Hi @pxsalehi , shall we need to add an extra disk watermark thresholds into `_cat\/allocation` APIs? Since each node thresholds would be the same, is that necessary to add these thresholds in the follow default colums?\r\n\r\n`_cat\/allocations` :\r\n`shards disk.indices disk.used disk.avail disk.total disk.percent host         ip           node\r\n    16        5.7tb     5.9tb    958.9gb      6.9tb           86 127.0.0.1 127.0.0.1 node-1\r\n`\r\n"],"labels":[">enhancement",":Distributed\/Allocation","Team:Distributed"]},{"title":"Fix test failure 106647","body":"The test failure occurs when we have a `threshold`. Even when only gathering a single document for a query, because of this line: \r\n\r\n```\r\n    public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {\r\n        if (hitsThresholdChecker.totalHitsThreshold == Integer.MAX_VALUE) {\r\n            return super.getLeafCollector(context);\r\n        }\r\n        earlyTerminateIfNeeded();\r\n```\r\n\r\nWe will check for earlyTermination even if the leaf has no matching docs. This means if we have collected a single doc from another leaf, we will throw an early termination here.\r\n\r\nThis then causes the test to fail as we matched a single doc. I figured updating the test was easier. \r\n\r\nThe code here for earlyTerminating eagerly on leaf collectors has been around since before we split the early terminating and partial hit collectors.\r\n\r\ncloses: https:\/\/github.com\/elastic\/elasticsearch\/issues\/106647","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">test",":Search\/Search","Team:Search","auto-backport-and-merge","v8.13.1","v8.14.0","v8.12.3"]},{"title":"Remove unused `SnapshotsInRepo#remaining`","body":"We only discard snapshots using the `?size` parameter when constructing\r\nthe final response, so we can count the `remaining` snapshots in a local\r\nvariable rather than tracking an unnecessary `0` on every\r\n`SnapshotsInRepo`. Indeed by inlining `sortSnapshots` into the routine\r\nthat constructs the final response we can avoid even having to build the\r\na final `SnapshotsInRepo` here.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">non-issue",":Distributed\/Snapshot\/Restore","Team:Distributed","v8.14.0"]},{"title":"ESQL: Fix fully pruned aggregates","body":"Fix https:\/\/github.com\/elastic\/elasticsearch\/issues\/106427\r\n\r\nShould be (rebased and) merged after https:\/\/github.com\/elastic\/elasticsearch\/pull\/105454 as it re-uses [its `emptyRow` mechanism](https:\/\/github.com\/elastic\/elasticsearch\/pull\/105454\/files#diff-45f172b501f6b73039f2a486c29da73ed84d03468540659d86dfa8e10222ade9R285-R289).","comments":["Hi @alex-spies, I've created a changelog YAML for you."],"labels":[">bug","auto-backport",":Analytics\/ES|QL","v8.13.1","v8.14.0"]},{"title":"Metrics: Measure synthetic source per-request document reconstruction latency","body":"### Description\r\n\r\nIn the following months we will see usage of synthetic source increasing as a result of integrations adopting new index modes like `tsdb` and `logs`. When using synthetic source we reconstruct the original document using doc values and stored fields instead of just fetching the `_source` field. This implies larger latencies resulting from having to iterate all fields of a document to reconstruct the source instead of just fetching a single field. We would like to keep track of the time spent to reconstruct documents using synthetic source, measuring the per-request latency to reconstruct all the document sources involved in a specific request.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"Metrics: Count synthetic source mapping failures","body":"### Description\r\n\r\nAs part of our effort when it comes to improve our Serverless monitoring we would like to be able to count errors happening when creating a mapping including the synthetic source setting. We would like to count errors resulting from a new template creation or from a new index creation as long as they include synthetic source. As more integrations are switching or are going to switch to synthetic source, as a result of adopting TSDB and\/or logs index mode, we would be in a position where we can detect possible issues in our Serverless deployments.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"Random sampler aggregation results depend on segment count","body":"### Elasticsearch Version\r\n\r\n`main`\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nOS-independent issue\r\n\r\n### Problem Description\r\n\r\nWe'd like the random sampler aggregation to produce consistent results regardless of the number of segments in an index. However, the returned document count per bucket depends on the number of segments. \r\n\r\n### Steps to Reproduce\r\n\r\nThe following test case reproduces the issue. We explicitly flush after each document to provoke the issue. If we do a force merge at the end, the random sampler aggregation behaves as expected. Note that the test also eliminates all randomness by using the recently introduced shard seed.\r\n\r\n```java\r\n@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 1)\r\npublic class RandomSamplerAggregationIT extends ESIntegTestCase {\r\n\r\n    public static final String TEST_INDEX = \"test-001\";\r\n\r\n    @Override\r\n    protected Collection<Class<? extends Plugin>> nodePlugins() {\r\n        return List.of(getTestTransportPlugin());\r\n    }\r\n\r\n    @Override\r\n    protected boolean addMockHttpTransport() {\r\n        return false; \/\/ enable http\r\n    }\r\n\r\n    @Before\r\n    public void setupData() throws Exception {\r\n        prepareCreate(TEST_INDEX, indexSettings(1, 0)).setMapping(\r\n            \"@timestamp\",\r\n            \"type=date\",\r\n            \"transaction.name\",\r\n            \"type=keyword\",\r\n            \"transaction.stacktrace\",\r\n            \"type=keyword\"\r\n        ).get();\r\n\r\n        ensureGreen(TEST_INDEX);\r\n\r\n        index(\r\n            TEST_INDEX,\r\n            \"1\",\r\n            \"{\\\"@timestamp\\\": \\\"1698624000\\\", \\\"transaction.name\\\":  \\\"encodeSha1\\\", \\\"transaction.stacktrace\\\": [\\\"aaaaaa\\\"]}\"\r\n        );\r\n        flush(TEST_INDEX);\r\n        index(\r\n            TEST_INDEX,\r\n            \"2\",\r\n            \"{\\\"@timestamp\\\": \\\"1698624001\\\", \\\"transaction.name\\\":  \\\"encodeSha1\\\", \\\"transaction.stacktrace\\\": [\\\"aaaaaa\\\"]}\"\r\n        );\r\n        flush(TEST_INDEX);\r\n        index(\r\n            TEST_INDEX,\r\n            \"3\",\r\n            \"{\\\"@timestamp\\\": \\\"1698624002\\\", \\\"transaction.name\\\":  \\\"encodeSha1\\\", \\\"transaction.stacktrace\\\": [\\\"aaaaaa\\\"]}\"\r\n        );\r\n        flush(TEST_INDEX);\r\n        index(\r\n            TEST_INDEX,\r\n            \"4\",\r\n            \"{\\\"@timestamp\\\": \\\"1698624003\\\", \\\"transaction.name\\\":  \\\"encodeSha1\\\", \\\"transaction.stacktrace\\\": [\\\"aaaaaa\\\"]}\"\r\n        );\r\n        flush(TEST_INDEX);\r\n        refresh();\r\n        \/\/ uncomment this - it produces the expected result\r\n        \/\/ forceMerge();\r\n    }\r\n\r\n    @TestLogging(\r\n        value = \"org.elasticsearch.search.aggregations.bucket.sampler.random.RandomSamplingQuery:TRACE,org.elasticsearch.search:TRACE\",\r\n        reason = \"debugging\"\r\n    )\r\n    public void testRandomSampler() {\r\n        int seed = 947097719;\r\n        TermQueryBuilder query = QueryBuilders.termQuery(\"transaction.name\", \"encodeSha1\");\r\n        RandomSamplerAggregationBuilder randomSampler = new RandomSamplerAggregationBuilder(\"sample\").setSeed(seed)\r\n            .setProbability(0.2d)\r\n            .subAggregation(new TermsAggregationBuilder(\"group_by\").field(\"transaction.stacktrace\"));\r\n        randomSampler.setShardSeed(42);\r\n\r\n        assertResponse(\r\n            prepareSearch(TEST_INDEX).setTrackTotalHits(false)\r\n                .setSize(0)\r\n                \/\/ take advantage of request cache and keep a consistent order for the same request\r\n                .setRequestCache(true)\r\n                .setPreference(String.valueOf(seed))\r\n                .setQuery(query)\r\n                .addAggregation(new MinAggregationBuilder(\"min_time\").field(\"@timestamp\"))\r\n                .addAggregation(new MaxAggregationBuilder(\"max_time\").field(\"@timestamp\"))\r\n                .addAggregation(randomSampler),\r\n            response -> {\r\n                SingleBucketAggregation sample = response.getAggregations().get(\"sample\");\r\n                Terms stacktraces = sample.getAggregations().get(\"group_by\");\r\n                \/\/ as the sampling rate is 0.2, we see 5 times more samples (random sampler agg automatically adjusts sample count)\r\n                assertEquals(5L, stacktraces.getBucketByKey(\"aaaaaa\").getDocCount());\r\n            }\r\n        );\r\n    }\r\n}\r\n```\r\n\r\nIf we add a couple of trace statements:\r\n\r\n```patch\r\ndiff --git a\/server\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/bucket\/sampler\/random\/RandomSamplingQuery.java b\/server\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/bucket\/sampler\/random\/RandomSamplingQuery.java\r\nindex 1878f15824b..a2cd6480e4c 100644\r\n--- a\/server\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/bucket\/sampler\/random\/RandomSamplingQuery.java\r\n+++ b\/server\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/bucket\/sampler\/random\/RandomSamplingQuery.java\r\n@@ -8,6 +8,8 @@\r\n \r\n package org.elasticsearch.search.aggregations.bucket.sampler.random;\r\n \r\n+import org.apache.logging.log4j.LogManager;\r\n+import org.apache.logging.log4j.Logger;\r\n import org.apache.lucene.index.LeafReaderContext;\r\n import org.apache.lucene.search.ConstantScoreScorer;\r\n import org.apache.lucene.search.DocIdSetIterator;\r\n@@ -29,6 +31,7 @@ import java.util.function.IntSupplier;\r\n  * A query that randomly matches documents with a user-provided probability within a geometric distribution\r\n  *\/\r\n public final class RandomSamplingQuery extends Query {\r\n+    private static final Logger logger = LogManager.getLogger(RandomSamplingQuery.class);\r\n \r\n     private final double p;\r\n     private final int seed;\r\n@@ -41,6 +44,7 @@ public final class RandomSamplingQuery extends Query {\r\n      *                  can be generated\r\n      *\/\r\n     public RandomSamplingQuery(double p, int seed, int hash) {\r\n+        logger.trace(\"Create RandomSamplingQuery with p=[{}], seed=[{}], hash=[{}]\", p, hash, seed);\r\n         if (p <= 0.0 || p >= 1.0) {\r\n             throw new IllegalArgumentException(\"RandomSampling probability must be between 0.0 and 1.0, was [\" + p + \"]\");\r\n         }\r\n@@ -56,6 +60,7 @@ public final class RandomSamplingQuery extends Query {\r\n \r\n     @Override\r\n     public Weight createWeight(IndexSearcher searcher, ScoreMode scoreMode, float boost) throws IOException {\r\n+        logger.trace(\"createWeight(searcher=[\" + searcher + \"], scoreMode=[\" + scoreMode + \"], boost=[\" + boost + \"])\");\r\n         return new Weight(this) {\r\n             @Override\r\n             public boolean isCacheable(LeafReaderContext ctx) {\r\n@@ -75,7 +80,9 @@ public final class RandomSamplingQuery extends Query {\r\n \r\n             @Override\r\n             public Scorer scorer(LeafReaderContext context) {\r\n-                final SplittableRandom random = new SplittableRandom(BitMixer.mix(hash ^ seed));\r\n+                int randomSeed = BitMixer.mix(hash ^ seed);\r\n+                logger.trace(\"random seed=[\" + randomSeed + \"]\");\r\n+                final SplittableRandom random = new SplittableRandom(randomSeed);\r\n                 int maxDoc = context.reader().maxDoc();\r\n                 return new ConstantScoreScorer(\r\n                     this,\r\n@@ -119,8 +126,10 @@ public final class RandomSamplingQuery extends Query {\r\n \r\n         @Override\r\n         public int advance(int target) {\r\n+            logger.trace(\"advance to [{}]\", target);\r\n             while (doc < target && doc < maxDoc) {\r\n                 doc += distribution.next();\r\n+                logger.trace(\"doc=[{}]\", doc);\r\n             }\r\n             doc = doc < maxDoc ? doc : NO_MORE_DOCS;\r\n             return doc;\r\n```\r\n\r\nthe test output indicates that the doc iterator is called for each segment (it is created four times):\r\n\r\n```\r\n  1> [2024-03-22T07:24:35,469][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] random seed=[-1436384276]\r\n  1> [2024-03-22T07:24:35,469][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] random seed=[-1436384276]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [0]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [0]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[0]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[0]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [1]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [1]\r\n  1> [2024-03-22T07:24:35,492][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[15]\r\n  1> [2024-03-22T07:24:35,491][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[15]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] random seed=[-1436384276]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] random seed=[-1436384276]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [0]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [0]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[0]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[0]\r\n  1> [2024-03-22T07:24:35,493][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [1]\r\n  1> [2024-03-22T07:24:35,494][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] advance to [1]\r\n  1> [2024-03-22T07:24:35,494][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[15]\r\n  1> [2024-03-22T07:24:35,494][TRACE][o.e.s.a.b.s.r.RandomSamplingQuery] [node_t3] doc=[15]\r\n````\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Pinging @elastic\/ml-core (Team:ML)","I wonder if segment concurrency is part of the issue here.  Moving the issue to ML, they have developed this aggregation.","> I wonder if segment concurrency is part of the issue here\r\n\r\nI might not have done it correctly but during the investigation I did try to override `supportsParallelCollection` in `RandomSamplerAggregationBuilder` (returning `false`) and the test still failed."],"labels":[">bug",":ml","Team:ML","v8.14.0"]},{"title":"ESQL: Refactor Row LogicalPlan","body":"[Row](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/plan\/logical\/Row.java) is the `LogicalPlan` corresponding to the source command of the same name, which creates a single row.\r\n\r\n[LocalRelation](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/plan\/logical\/local\/LocalRelation.java) is similar, but it can create more than one row.\r\n\r\nWe should refactor the two, so we do not maintain duplicated functionality.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">refactoring","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[CI] XPackRestIT test {p0=ml\/inference_crud\/Test force delete given model with alias referenced by pipeline} failing","body":"Many other tests in the same build scan (https:\/\/gradle-enterprise.elastic.co\/s\/uprnbtvfhrrvk) fail with the same message: \"Unable to reset machine learning feature as there are ingest pipelines still referencing trained machine learning models\"\n\nPossibly related to: https:\/\/github.com\/elastic\/elasticsearch\/issues\/71072\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/uprnbtvfhrrvk\/tests\/:x-pack:plugin:yamlRestTest\/org.elasticsearch.xpack.test.rest.XPackRestIT\/test%20%7Bp0=ml%2Finference_crud%2FTest%20force%20delete%20given%20model%20with%20alias%20referenced%20by%20pipeline%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:yamlRestTest' --tests \"org.elasticsearch.xpack.test.rest.XPackRestIT.test {p0=ml\/inference_crud\/Test force delete given model with alias referenced by pipeline}\" -Dtests.seed=5314F766DCACC2B2 -Dtests.locale=uk -Dtests.timezone=Asia\/Thimphu -Druntime.java=22\n```\n\n**Applicable branches:**\n8.13\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.test.rest.XPackRestIT#test {p0=ml\/inference_crud\/Test force delete given model with alias referenced by pipeline}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Bp0%3Dml\/inference_crud\/Test%20force%20delete%20given%20model%20with%20alias%20referenced%20by%20pipeline%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.test.rest.XPackRestIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Failure at [ml\/inference_crud:680]: expected [2xx] status code but api [ml.delete_trained_model] returned [503 Service Unavailable] [{\"error\":{\"root_cause\":[{\"type\":\"no_shard_available_action_exception\",\"reason\":\"[yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\",\"stack_trace\":\"org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"Partial shards failure\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\".ml-stats-000001\",\"node\":\"yCCTleljQ7eVm9bxerfJ4Q\",\"reason\":{\"type\":\"no_shard_available_action_exception\",\"reason\":\"[yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\",\"stack_trace\":\"org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\"}}],\"stack_trace\":\"Failed to execute phase [query], Partial shards failure; shardFailures {[yCCTleljQ7eVm9bxerfJ4Q][.ml-stats-000001][0]: org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n}\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:712)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:418)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:744)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.successfulShardExecution(AbstractSearchAsyncAction.java:616)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardResultConsumed(AbstractSearchAsyncAction.java:600)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.lambda$onShardResult$12(AbstractSearchAsyncAction.java:583)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.QueryPhaseResultConsumer$PendingMerges.consume(QueryPhaseResultConsumer.java:383)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.QueryPhaseResultConsumer.consumeResult(QueryPhaseResultConsumer.java:116)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardResult(AbstractSearchAsyncAction.java:583)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchQueryThenFetchAsyncAction.onShardResult(SearchQueryThenFetchAsyncAction.java:120)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction$1.innerOnResponse(AbstractSearchAsyncAction.java:324)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchActionListener.onResponse(SearchActionListener.java:33)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchActionListener.onResponse(SearchActionListener.java:18)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchExecutionStatsCollector.onResponse(SearchExecutionStatsCollector.java:62)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchExecutionStatsCollector.onResponse(SearchExecutionStatsCollector.java:26)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:48)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleResponse(SearchTransportService.java:628)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$UnregisterChildTransportResponseHandler.handleResponse(TransportService.java:1742)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1465)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1562)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1536)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:35)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:32)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:19)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListener.respondAndRelease(ActionListener.java:289)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$3.accept(ActionRunnable.java:73)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$3.accept(ActionRunnable.java:70)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$4.doRun(ActionRunnable.java:95)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\\n\\tat java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:1570)\\nCaused by: org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.NoShardAvailableActionException.forOnShardFailureWrapper(NoShardAvailableActionException.java:28)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:532)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:479)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:335)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:31)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:53)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:634)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$UnregisterChildTransportResponseHandler.handleException(TransportService.java:1751)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1475)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1609)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1584)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:44)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:44)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:146)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\\n\\t... 5 more\\n\"},\"status\":503}]\n\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:561)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n  Caused by: java.lang.AssertionError: expected [2xx] status code but api [ml.delete_trained_model] returned [503 Service Unavailable] [{\"error\":{\"root_cause\":[{\"type\":\"no_shard_available_action_exception\",\"reason\":\"[yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\",\"stack_trace\":\"org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"Partial shards failure\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\".ml-stats-000001\",\"node\":\"yCCTleljQ7eVm9bxerfJ4Q\",\"reason\":{\"type\":\"no_shard_available_action_exception\",\"reason\":\"[yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\",\"stack_trace\":\"org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\"}}],\"stack_trace\":\"Failed to execute phase [query], Partial shards failure; shardFailures {[yCCTleljQ7eVm9bxerfJ4Q][.ml-stats-000001][0]: org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n}\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:712)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:418)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:744)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.successfulShardExecution(AbstractSearchAsyncAction.java:616)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardResultConsumed(AbstractSearchAsyncAction.java:600)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.lambda$onShardResult$12(AbstractSearchAsyncAction.java:583)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.QueryPhaseResultConsumer$PendingMerges.consume(QueryPhaseResultConsumer.java:383)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.QueryPhaseResultConsumer.consumeResult(QueryPhaseResultConsumer.java:116)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardResult(AbstractSearchAsyncAction.java:583)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchQueryThenFetchAsyncAction.onShardResult(SearchQueryThenFetchAsyncAction.java:120)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction$1.innerOnResponse(AbstractSearchAsyncAction.java:324)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchActionListener.onResponse(SearchActionListener.java:33)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchActionListener.onResponse(SearchActionListener.java:18)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchExecutionStatsCollector.onResponse(SearchExecutionStatsCollector.java:62)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchExecutionStatsCollector.onResponse(SearchExecutionStatsCollector.java:26)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:48)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleResponse(SearchTransportService.java:628)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$UnregisterChildTransportResponseHandler.handleResponse(TransportService.java:1742)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1465)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1562)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1536)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:35)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:32)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:19)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListener.respondAndRelease(ActionListener.java:289)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$3.accept(ActionRunnable.java:73)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$3.accept(ActionRunnable.java:70)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable$4.doRun(ActionRunnable.java:95)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\\n\\tat java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:1570)\\nCaused by: org.elasticsearch.action.NoShardAvailableActionException: [yamlRestTest-0][127.0.0.1:41851][indices:data\/read\/search[phase\/query]]\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.NoShardAvailableActionException.forOnShardFailureWrapper(NoShardAvailableActionException.java:28)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:532)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:479)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:335)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:31)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:53)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:634)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$UnregisterChildTransportResponseHandler.handleException(TransportService.java:1751)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1475)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1609)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1584)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:44)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:44)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:146)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:28)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\\n\\t... 5 more\\n\"},\"status\":503}]\n\n    at org.junit.Assert.fail(Assert.java:89)\n    at org.elasticsearch.test.rest.yaml.section.DoSection.checkResponseException(DoSection.java:529)\n    at org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:385)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:541)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n    at java.lang.reflect.Method.invoke(Method.java:580)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n    at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","Another set of failures: https:\/\/gradle-enterprise.elastic.co\/s\/6ksrvsmntrdr6"],"labels":["blocker",">test-failure",":ml","Team:ML"]},{"title":"[CI] CacheTests testDependentKeyDeadlock failing","body":"It happened in Windows, and it is a timeout, but the timeout on a concurrency primitive looks suspiciously like a deadlock.\n\nTried on Mac, does not repro. Wasn't able to try on Windows\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/63bmf7um5lcjc\/tests\/:server:test\/org.elasticsearch.common.cache.CacheTests\/testDependentKeyDeadlock\n\n\n**Reproduction line:**\n```\ngradlew ':server:test' --tests \"org.elasticsearch.common.cache.CacheTests.testDependentKeyDeadlock\" -Dtests.seed=FF978136D53D8B7D -Dtests.locale=de -Dtests.timezone=Europe\/Andorra -Druntime.java=22\n```\n\n**Applicable branches:**\n8.13\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.common.cache.CacheTests#testDependentKeyDeadlock`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testDependentKeyDeadlock'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.common.cache.CacheTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: safeAwait: CountDownLatch did not reach zero within the timeout\n\n  at __randomizedtesting.SeedInfo.seed([FF978136D53D8B7D:DEF551B633C72A39]:0)\n  at org.junit.Assert.fail(Assert.java:89)\n  at org.junit.Assert.assertTrue(Assert.java:42)\n  at org.elasticsearch.test.ESTestCase.safeAwait(ESTestCase.java:2078)\n  at org.elasticsearch.common.cache.CacheTests.testDependentKeyDeadlock(CacheTests.java:738)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":["blocker",":Core\/Infra\/Core",">test-failure","Team:Core\/Infra"]},{"title":"[CI] XPackRestIT test {p0=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder} failing","body":"Relates #106068\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/6xqob32ehojzc\/tests\/:x-pack:plugin:yamlRestTest\/org.elasticsearch.xpack.test.rest.XPackRestIT\/test%20%7Bp0=ml%2Fsearch_knn_query_vector_builder%2FTest%20vector%20search%20with%20query_vector_builder%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:yamlRestTest' --tests \"org.elasticsearch.xpack.test.rest.XPackRestIT.test {p0=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder}\" -Dtests.seed=B64BF5B42C7250F7 -Dtests.locale=es-SV -Dtests.timezone=Europe\/Stockholm -Druntime.java=22\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.test.rest.XPackRestIT#test {p0=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Bp0%3Dml\/search_knn_query_vector_builder\/Test%20vector%20search%20with%20query_vector_builder%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.test.rest.XPackRestIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Failure at [ml\/search_knn_query_vector_builder:123]: \nExpected: <3>\n     but: was <5>\n\n  at __randomizedtesting.SeedInfo.seed([B64BF5B42C7250F7:3E1FCA6E828E3D0F]:0)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:593)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n  Caused by: java.lang.AssertionError: \n  Expected: <3>\n       but: was <5>\n\n    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n    at org.junit.Assert.assertThat(Assert.java:964)\n    at org.junit.Assert.assertThat(Assert.java:930)\n    at org.elasticsearch.test.rest.yaml.section.MatchAssertion.doAssert(MatchAssertion.java:99)\n    at org.elasticsearch.test.rest.yaml.section.Assertion.execute(Assertion.java:65)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:573)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n    at java.lang.reflect.Method.invoke(Method.java:580)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n    at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":["blocker",":Search\/Search",">test-failure","Team:Search"]},{"title":"Mute \"Test vector search with query_vector_builder\" (AwaitsFix #106535)","body":"Ref: https:\/\/github.com\/elastic\/elasticsearch\/issues\/106535","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[":ml",">test-mute","Team:ML","auto-merge","v8.14.0"]},{"title":"[CI] PartialHitCountCollectorTests testCollectedHitCount failing","body":"Reproduces with `.\/gradlew ':server:test' --tests \"org.elasticsearch.search.query.PartialHitCountCollectorTests.testCollectedHitCount\" -Dtests.seed=4DAD7A6F655975F4 -Dtests.locale=es-CU -Dtests.timezone=America\/Rio_Branco -Druntime.java=21\t`\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/gktrvnkxedbgk\/tests\/:server:test\/org.elasticsearch.search.query.PartialHitCountCollectorTests\/testCollectedHitCount\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:test' --tests \"org.elasticsearch.search.query.PartialHitCountCollectorTests.testCollectedHitCount\" -Dtests.seed=4DAD7A6F655975F4 -Dtests.locale=es-CU -Dtests.timezone=America\/Rio_Branco -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nYes\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.search.query.PartialHitCountCollectorTests#testCollectedHitCount`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testCollectedHitCount'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.search.query.PartialHitCountCollectorTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: (No message provided)\n\n  at __randomizedtesting.SeedInfo.seed([4DAD7A6F655975F4:86F44693FDEA5D2C]:0)\n  at org.junit.Assert.fail(Assert.java:86)\n  at org.junit.Assert.assertTrue(Assert.java:41)\n  at org.junit.Assert.assertFalse(Assert.java:64)\n  at org.junit.Assert.assertFalse(Assert.java:74)\n  at org.elasticsearch.search.query.PartialHitCountCollectorTests.testCollectedHitCount(PartialHitCountCollectorTests.java:128)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)","Yeah, this is repeatable, gonna mute.","OK, I changed the test to\r\n```\r\nint threshold = 1;\r\n```\r\n\r\nAnd this reliably fails every time. I am guessing our check isn't actually `>=` or we have a weird fence-post error."],"labels":[":Search\/Search",">test-failure","Team:Search","low-risk"]},{"title":"[CI] EsqlActionBreakerIT testFromProjectStatsGroupByAlias failing","body":"Many other tests (links to build scans below) started to fail, also with the same error ` Leftover exchanges ExchangeService{sinks=...} on node ...`\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/q33bdwzpazpbi\/tests\/:x-pack:plugin:esql:internalClusterTest\/org.elasticsearch.xpack.esql.action.EsqlActionBreakerIT\/testFromProjectStatsGroupByAlias\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:esql:internalClusterTest' --tests \"org.elasticsearch.xpack.esql.action.EsqlActionBreakerIT.testFromProjectStatsGroupByAlias\" -Dtests.seed=ABF4B4B26F74217C -Dtests.locale=is-IS -Dtests.timezone=Asia\/Pontianak -Druntime.java=17 -Dtests.fips.enabled=true\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.esql.action.EsqlActionBreakerIT#testFromProjectStatsGroupByAlias`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testFromProjectStatsGroupByAlias'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.esql.action.EsqlActionBreakerIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Leftover exchanges ExchangeService{sinks=[EkxkigfDReOF8_8zpB5EzQ:276]} on node node_s3\n\n  at __randomizedtesting.SeedInfo.seed([ABF4B4B26F74217C:5B9E7FC2E52AE96C]:0)\n  at org.junit.Assert.fail(Assert.java:89)\n  at org.junit.Assert.assertTrue(Assert.java:42)\n  at org.elasticsearch.xpack.esql.action.AbstractEsqlIntegTestCase.lambda$ensureExchangesAreReleased$0(AbstractEsqlIntegTestCase.java:48)\n  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1274)\n  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1247)\n  at org.elasticsearch.xpack.esql.action.AbstractEsqlIntegTestCase.ensureExchangesAreReleased(AbstractEsqlIntegTestCase.java:48)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:568)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:833)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Other scans with a similar error over the past 12 hours:\r\n\r\n- https:\/\/gradle-enterprise.elastic.co\/s\/q33bdwzpazpbi\r\n- https:\/\/gradle-enterprise.elastic.co\/s\/c6r5hea5j44xc\r\n- https:\/\/gradle-enterprise.elastic.co\/s\/d46debo7vyghm\r\n- https:\/\/gradle-enterprise.elastic.co\/s\/eikor5eojvlqq\r\n- https:\/\/gradle-enterprise.elastic.co\/s\/7ppjdpbnprlhc\r\n"],"labels":[">test-failure","Team:Analytics",":Analytics\/ES|QL","low-risk"]},{"title":"Health report warning if fewer than 3 master-eligible nodes","body":"We sometimes encounter on-prem users running with just one or two master-eligible nodes, and many more data-only nodes, despite the [docs](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/high-availability-cluster-design.html) indicating that a resilient cluster needs three master-eligible nodes. Typically we encounter such users when they've lost the master and their cluster can no longer form, which is a little late to be learning this lesson. Could we add a health indicator to warn users if they should make more of their nodes master-eligible?\r\n\r\nMore precisely, if there's only one node in the cluster then that's fine, no need to warn, a one-node cluster is clearly not resilient to failure. If there's \u22653 nodes in the cluster and \u22653 of them are master-eligible then that's also fine, but if there's \u22653 nodes in the cluster and <3 of them are master-eligible then that deserves a warning.\r\n\r\nThe two-node case is a little trickier: a two-node cluster is not totally unreasonable, but I can see how folks might assume such a cluster has enough redundancy to survive the loss of either node even though this isn't the case. In this case we [recommend](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/high-availability-cluster-small-clusters.html#high-availability-cluster-design-two-nodes) making exactly one of the nodes master-eligible, so I think we should warn if a two-node cluster has two master-eligible nodes. That way folks can avoid the warning if they really want a two-node cluster, but only by doing something that fairly obviously makes the cluster not resilient to master failure.\r\n\r\n---\r\n\r\nETA: we should probably have a generous anti-flapping time threshold on this check too, no need to complain if we're restarting\/upgrading a master node and it takes a few minutes to come back.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Hi @DaveCTurner , shall we going to add warning messages in `_cluster\/health` API?"],"labels":[">enhancement","Team:Data Management",":Data Management\/Health"]},{"title":"Allow users to get status of own async search tasks ","body":"This consists of 3 changes:\r\n1. Refactoring the code so that all the security logic in the async search code is moved to `AsyncSearchSecurity`\r\n2. Changing `TransportGetAsyncStatusAction` to check for ownership if the user does not have explicit access to the `GetAsyncStatusAction` (if they have such access it means that they can get the status of all async searches)\r\n3. In RBACEngine, if a user does not have permission to `GetAsyncStatusAction` but does have permission to submit async searches, then let them run the action, relying on point 2 above.","comments":["## Manual testing results, comparing code on `main` to this branch.\r\n\r\n### Summary of testing results\r\n\r\nI found two changes in this branch compared to `main`:\r\n\r\n* First, the core goal of this ticket was achieved. A user with only `index:read` permissions (and not `cluster:monitor`) can now access the `_async_search\/status` endpoint for their own searches.\r\n\r\n* Second, the error a user with only `index:read` permissions (and not `cluster:monitor`) gets when trying to see the `status` of another users search has changed. In main it is `403 Forbidden`. In this branch it is `404 NotFound`. This could be considered a regression\/breaking change?\r\n\r\n### Details\r\n\r\nI created an async-search with the user `mp_user`, having only `read` permissions, but not `cluster:monitor`. The I queried the `_async_search\/status` endpoint with 3 different users:\r\n\r\n```\r\n|---------+-----------------------------|\r\n| role    | privileges                  |\r\n|---------+-----------------------------|\r\n| mp_role | cluster:none, index:read    |\r\n| yy_role | cluster:monitor, index:read |\r\n|---------+-----------------------------|\r\n```\r\n\r\n```\r\n|---------+---------|\r\n| user    | role    |\r\n|---------+---------|\r\n| mp_user | mp_role |\r\n| xx_user | mp_role |\r\n|---------+---------|\r\n| yy_user | yy_role |\r\n|---------+---------|\r\n```\r\n\r\nThe overall test was run twice on each branch to ensure I capture the data correctly. Test outcome:\r\n\r\n```\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| action                                    | search status | user    | behavior in main | Tim's PR     | comments      |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| create async-search                       | N\/A           | mp_user | works            | works        |               |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | running       | mp_user | 200 OK           | 200 OK       |               |\r\n| GET async-search\/status\/:id               | running       | mp_user | 403 Forbidden    | 200 OK       | Fixed!        |\r\n| GET async-search\/status\/:id&keep_alive=1m | running       | mp_user | 403 Forbidden    | 200 OK       | Fixed!        |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | completed     | mp_user | 200 OK           | 200 OK       |               |\r\n| GET async-search\/status\/:id               | completed     | mp_user | 403 Forbidden    | 200 OK       | Fixed!        |\r\n| GET async-search\/status\/:id&keep_alive=1m | completed     | mp_user | 403 Forbidden    | 200 OK       | Fixed!        |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | running       | xx_user | 404 NotFound     | 404 NotFound |               |\r\n| GET async-search\/status\/:id               | running       | xx_user | 403 Forbidden    | 404 NotFound | Regression?   |\r\n| GET async-search\/status\/:id&keep_alive=1m | running       | xx_user | 403 Forbidden    | 404 NotFound | Regression?   |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | completed     | xx_user | 404 NotFound     | 404 NotFound |               |\r\n| GET async-search\/status\/:id               | completed     | xx_user | 403 Forbidden    | 404 NotFound | Regression?   |\r\n| GET async-search\/status\/:id&keep_alive=1m | completed     | xx_user | 403 Forbidden    | 404 NotFound | Regression?   |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | running       | yy_user | 404 NotFound     | 404 NotFound |               |\r\n| GET async-search\/status\/:id               | running       | yy_user | 200 OK           | 200 OK       |               |\r\n| GET async-search\/status\/:id&keep_alive=1m | running       | yy_user | 404 NotFound     | 404 NotFound | Expected?     |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n| GET async-search\/:id                      | completed     | yy_user | 404 NotFound     | 404 NotFound |               |\r\n| GET async-search\/status\/:id               | completed     | yy_user | 200 OK           | 200 OK       |               |\r\n| GET async-search\/status\/:id&keep_alive=1m | completed     | yy_user | 200 OK           | 200 OK       | Existing bug? |\r\n|-------------------------------------------+---------------+---------+------------------+--------------+---------------|\r\n```"],"labels":["v8.14.0"]},{"title":"ESQL: Add OPTIONS clause to FROM command","body":"This adds an `OPTIONS` clause to `FROM` command, allowing to specify search or index resolution options.\r\nFor now, the following options are supported:\r\n* `preference`\r\n* `allow_no_indices`\r\n* `ignore_unavailable`.\r\n\r\nThe `OPTIONS` clause take a comma-separated list of quoted AVPs. Example:\r\n```\r\nFROM index OPTIONS \"preference\"=\"_shards:1,2\",\"allow_no_indices\"=\"true\"\r\n```","comments":["Hi @bpintea, I've created a changelog YAML for you.","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui","v8.14.0"]},{"title":"List supported formats (parsers) for the attachment processor","body":"### Description\n\nCustomers request us to update [our doc](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/attachment.html) listing the exhaustive list of supported formats \/ parsers","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",">docs",":Data Management\/Ingest Node","Team:Data Management","Team:Docs"]},{"title":"Accumulate results directly in TransportGetSnapshotsAction","body":"Rather than constructing a temporary object to hold the per-repository\r\nresults and counts, we can just add this data directly to the final\r\naccumulators. This saves some more intermediate collections.\r\n\r\nMoreover we were calling `sortSnapshots` twice; the first invocation is\r\nonly really there to filter out using `afterPredicate` (the collection\r\nis sorted again later, and we don't apply any limit or offset), so the\r\nsecond invocation doesn't need to apply `afterPredicate` again. With\r\nthis commit we apply `afterPredicate` while building the results,\r\nshrinking some intermediate collections, and then sort\/offset\/limit only\r\nonce.","comments":[],"labels":[">non-issue",":Distributed\/Snapshot\/Restore","v8.14.0"]},{"title":"SLM Snapshots should warn or fail when no indices match and the snapshot is empty","body":"### Description\n\nA customer said they had an issue recently where a syntax error in the wildcard index pattern in SLM meant that even though the snapshot was considered successful, there were no indices being snapshotted (empty snapshots).\r\n\r\nPerhaps we should generate a warning or error message that can be alerted on when the snapshot configuration captures no data on execution. Or we should fail the Snapshot so that the SLM Health Report can report on it. \r\n\r\nFor the benefit of users for whom the correct SLM policy really is \"back up all my foo_* indices, if there are any, and sometimes there aren't, and that's fine\" perhaps we can couple the new error behavior with an \"allow_no_indices\": true option at the SLM policy level. ","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/ILM+SLM","Team:Data Management"]},{"title":"[ES|QL] Refactor data type conversions between String\/UnsignedLong and other data types","body":"This is a subtask of #104596.\r\n\r\nTwo items are covered in the PR:\r\n\r\nRefactor data type conversions between String and other data types.\r\nRefactor data type conversions between UnsignedLong and other data types.\r\n\r\n\r\n","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"WIP: Create new async-search-status endpoint accessible with read index permissions","body":"This looks promising based on manual testing.\r\n\r\nAgainst a secured cluster with a user having permissions set as:\r\n\r\n```\r\n{\r\n  \"mp_role\": {\r\n    \"cluster\": [],\r\n    \"indices\": [\r\n      {\r\n        \"names\": [\r\n          \"blogs\"\r\n        ],\r\n        \"privileges\": [\r\n          \"read\"\r\n        ],\r\n        \"allow_restricted_indices\": false\r\n      },\r\n```\r\n\r\nI can create an async-search and the query it via the new endpoint:\r\n\r\n```\r\n\u2514\u2500 $ \u25b6 curlhj -X GET \"https:\/\/localhost:9200\/_async_search_status\/$SID?error_trace=false\" --cacert primary-http_ca.crt -u mp_user:$MYPASS  | jq .\r\nHTTP\/1.1 200 OK\r\nX-elastic-product: Elasticsearch\r\ncontent-type: application\/json\r\ncontent-length: 253\r\n\r\n{\r\n  \"id\": \"FjNVWFJVUktGVENpR05vdmxFc3ZXWVEbeXpsQTRkaTJUZGVudFF5SUFDS0YzUToxNzg3\",\r\n  \"is_running\": true,\r\n  \"is_partial\": true,\r\n  \"start_time_in_millis\": 1711046614126,\r\n  \"expiration_time_in_millis\": 1711478614126,\r\n  \"_shards\": {\r\n    \"total\": 13,\r\n    \"successful\": 9,\r\n    \"skipped\": 0,\r\n    \"failed\": 0\r\n  }\r\n}\r\n```","comments":[],"labels":["v8.14.0"]},{"title":"Revert to more robust assertion in SimpleThreadPoolIT.testThreadPoolMetrics","body":"This test will always have a race between pool stats collection and measurement collection if things are happening in the background.\r\n\r\nRevert to using  `greaterThanOrEqualTo` to allow for additional threads, e.g. for cluster coordination.\r\n\r\nIf this isn't enough, certain thread pools should be excluded here.\r\nOr, alternatively, sum active + completed threads and assert using `greaterThanOrEqualTo`.\r\n\r\n(closes #104652)\r\n\r\n```\r\n[2024-03-21T15:23:57,138][INFO ][o.e.t.SimpleThreadPoolIT ] [testThreadPoolMetrics] Stats of `cluster_coordination`: {cluster_coordination.threads.active.current=0, cluster_coordination.threads.completed.total=33, cluster_coordination.threads.count.current=1, cluster_coordination.threads.largest.current=1, cluster_coordination.threads.queue.size=0}\r\n[2024-03-21T15:23:57,138][INFO ][o.e.t.SimpleThreadPoolIT ] [testThreadPoolMetrics] Measurements of `cluster_coordination`: {cluster_coordination.threads.active.current=[0], cluster_coordination.threads.completed.total=[34], cluster_coordination.threads.count.current=[1], cluster_coordination.threads.largest.current=[1], cluster_coordination.threads.queue.size=[0]}\r\n```","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">test",":Core\/Infra\/Core","Team:Core\/Infra","v8.14.0"]},{"title":"Support test cluster JVM arguments conditional on Java version","body":"Currently when we configure test clusters we pass supplied extra JVM arguments to `ES_JAVA_OPTS`. However, when using a [JVM options file](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/advanced-configuration.html#jvm-options-syntax) we can use Java version-specific syntax (ex: `17-18:-Xmx2g`).\r\n\r\nSee https:\/\/github.com\/elastic\/elasticsearch\/pull\/106586\/files#r1534257448","comments":["Pinging @elastic\/es-delivery (Team:Delivery)"],"labels":[":Delivery\/Build","Team:Delivery"]},{"title":"[8.13] Add gradle plugin for publishing docker based test fixtures (#106229)","body":"Backports the following commits to 8.13:\n - Add gradle plugin for publishing docker based test fixtures (#106229)","comments":[],"labels":[">non-issue",":Delivery\/Build","backport","Team:Delivery","auto-merge","v8.13.1"]},{"title":"[CI] HdfsSearchableSnapshotsIT testClearCache failing","body":"Looks like it only failed in the past 3 weeks, and only on the 7.17 branch: \nhttps:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:'2024-02-29T22:00:00.000Z',to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,id:control_group_cda80263-fbb3-4579-9ef5-3d714484248d,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!(testClearCache),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!(org.elasticsearch.xpack.searchablesnapshots.hdfs.HdfsSearchableSnapshotsIT),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)),showApplySelections:!f))\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/oakn2ahreuzo4\/tests\/:x-pack:plugin:searchable-snapshots:qa:hdfs:javaRestTestSecure\/org.elasticsearch.xpack.searchablesnapshots.hdfs.HdfsSearchableSnapshotsIT\/testClearCache\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:searchable-snapshots:qa:hdfs:javaRestTestSecure' --tests \"org.elasticsearch.xpack.searchablesnapshots.hdfs.HdfsSearchableSnapshotsIT.testClearCache\" -Dtests.seed=8871AFF6F9E3187 -Dtests.locale=ja-JP-u-ca-japanese-x-lvariant-JP -Dtests.timezone=America\/New_York -Druntime.java=18\n```\n\n**Applicable branches:**\n7.17\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.searchablesnapshots.hdfs.HdfsSearchableSnapshotsIT#testClearCache`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testClearCache'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.searchablesnapshots.hdfs.HdfsSearchableSnapshotsIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: <52832L>\n     but: was <52848L>\n\n  at __randomizedtesting.SeedInfo.seed([8871AFF6F9E3187:C6582033ED0D7EC7]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.junit.Assert.assertThat(Assert.java:964)\n  at org.junit.Assert.assertThat(Assert.java:930)\n  at org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsRestTestCase.lambda$testClearCache$19(AbstractSearchableSnapshotsRestTestCase.java:365)\n  at org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsRestTestCase.runSearchableSnapshotsTest(AbstractSearchableSnapshotsRestTestCase.java:229)\n  at org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsRestTestCase.runSearchableSnapshotsTest(AbstractSearchableSnapshotsRestTestCase.java:91)\n  at org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsRestTestCase.runSearchableSnapshotsTest(AbstractSearchableSnapshotsRestTestCase.java:87)\n  at org.elasticsearch.xpack.searchablesnapshots.AbstractSearchableSnapshotsRestTestCase.testClearCache(AbstractSearchableSnapshotsRestTestCase.java:352)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n  at java.lang.reflect.Method.invoke(Method.java:577)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)\n  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)\n  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:833)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":["blocker",":Distributed\/Snapshot\/Restore",">test-failure","Team:Distributed"]},{"title":"Fix possible NPE when calling `populateFieldLookup`","body":"The `PreloadedFieldLookupProvider#storedFields` are set only after constructing `SearchHit` and getting the list of loaded fields. It is possible that field data loading is requested before and in that case there would be no stored fields available. Moreover if `setNextReader` is not called no backup loader is available and we can still get a NPE. This last case should not happen anyway as we call `setNextReader` when iterating search hits before we try to fetch fields.\r\n\r\nIf the `ValueFetcher` is used before `storedFields` is set we get a NPE.","comments":["Hi @salvatore-campagna, I've created a changelog YAML for you.","Pinging @elastic\/es-search (Team:Search)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">bug",":Search\/Search","Team:Search","Team:StorageEngine",":StorageEngine\/Logs","v8.14.0"]},{"title":"Support swapping the user context to the secondary auth user for named actions ","body":"work in progress\r\n","comments":[],"labels":["WIP","v8.14.0"]},{"title":"[8.13] Avoid using small inactive exchange timeout in breaker tests (#106394)","body":"Backports the following commits to 8.13:\n - Avoid using small inactive exchange timeout in breaker tests (#106394)","comments":[],"labels":[">test","backport","Team:Analytics","auto-merge",":Analytics\/ES|QL","v8.13.1"]},{"title":"Add an index option recording the highest index version that has read an index","body":"This option will be needed to decide index shard allocations in an updated https:\/\/github.com\/elastic\/elasticsearch\/pull\/102708","comments":["This seems suspiciously simple - are there any other places this option needs to be updated?","Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">non-issue",":Distributed\/Allocation","Team:Distributed","v8.14.0"]},{"title":"[Transform] Raise loglevel of events related to transform lifecycle from DEBUG to INFO","body":"This PR raises loglevel of events related to transform lifecycle from `DEBUG` to `INFO` in order to ease debugging.\r\n","comments":["Hi @przemekwitek, I've created a changelog YAML for you.","Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml\/Transform","Team:ML","auto-backport-and-merge","v8.13.1","v8.14.0"]},{"title":"[ML] Create uniform Service Settings interface for Text Embedding inference services","body":"### Description\n\nAll text embeddings models in the Inference API should report these 2 fields for mapping purposes:\r\n- dimensions\r\n- similarity\r\n\r\nAt the moment there is inconsistency between the different services\r\n\r\n- OpenAI and Cohere report dimensions & similarity\r\n- Builtin Elasticsearch E5 reports neither\r\n- HuggingFace has dimensions but no similarity\r\n\r\nThe interface should be enforced in code rather than relying on convention","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML"]},{"title":"[ILM] Do not force read-only for shrink during warm phase","body":"### Description\n\nDuring the warm phase, the ILM shrink operation makes the source index read-only (expected) but I discovered that it also set the target index as read-only after the shrink is done (unexpected).\r\n\r\nI found this which explains why it was done : #33485\r\nThe decision was to force the read-only if there is a shrink or forcemerge during the warm phase and it was done in #33907.\r\nI actually agree with the [original position of dakrone](https:\/\/github.com\/elastic\/elasticsearch\/issues\/33485#issuecomment-419260920).\r\n> In my opinion, marking an index read-only should be optionally opt-in and user configurable in the `warm` or `cold` phases, instead of hardcoded to occur at the start of the `warm` phase.\r\n\r\n\r\n\r\n**I would like to re-open this discussion as I think it can makes sense for most cases but would make more sense to only rely on the explicit read-only action to do that instead.**\r\n\r\nIn my case, I have write heavy indices with lot of shards to spread the write load accross the nodes. When they go warm after a month, they can still be rarely updated. To avoid keeping lots of now useless shards, I wanted to shrink during the warm phase and came accross this limitation which prevent my indices to be written after that. My only solutions right now are to not shrink (might cause balance issues in the long run having lots of shards for nothing) or write a custom script to monitor the ilm and revert the read-only once it's done.\r\n\r\nI think it should at least be documented that the target index will remain read-only (the ilm shrink documentation just says that for the source index : \"Sets a source index to read-only and shrinks it into a new index with fewer primary shards.\").\r\nBut I would personally find more logical to only make an index read-only if the \"read-only\" action is explicitly requested in the ilm configuration. I don't really understand why this block is forced for shrink (for forcemerge I could understand as it's not recommanded to write to a forcemerged index, but even there I would still prefer to control this with the explicit \"read-only\" action).","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/ILM+SLM","Team:Data Management"]},{"title":"[POC] Add security index field migration poc","body":"## Background\r\nWhile working on the Query Users API, a discussion around making user metadata searchable by indexing the metadata field came up. We decided against it because for that to work the security index would have to be reindexed (to make existing data searchable) and there is no mechanism in place to do that today.\r\n\r\nThis discussion has come up again, but this time with regards to the new Query Roles API that I'm adding. It's using the same field from the security index and has a more immediate need from the enterprise search team. \r\n\r\nFor API keys this was solved by indexing the `metadata` in the `metadata_flattened` field. \r\n\r\nThis PR is a proof of concept for migrating all data in the security index `metadata` field to `metadata_flattened` and by doing that, allowing metadata to be searched. This proposal migrates all `metadata`, as a result both ` query users` and `query roles` would be able to query metadata. \r\n\r\nWhen doing the next major upgrade this approach would allow us to drop the `metadata` field in favour of the `metadata_flattened` field.\r\n\r\n## Implementation\r\nThis implementation uses a [PersistentTasksExecutor](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/server\/src\/main\/java\/org\/elasticsearch\/persistent\/PersistentTasksExecutor.java) which is responsible for executing restartable tasks that can survive disappearance of a coordinating and executor nodes. It also gives us guarantees that only a single instance of a task with the same id will run concurrently.\r\n\r\nThe status of the migration job (completed or not) is kept in cluster state in a custom field and updated from the [SecurityIndexManager](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/security\/src\/main\/java\/org\/elasticsearch\/xpack\/security\/support\/SecurityIndexManager.java) using a [ClusterStateTaskExecutor](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/server\/src\/main\/java\/org\/elasticsearch\/cluster\/ClusterStateTaskExecutor.java).  \r\n\r\nThe migration is triggered by a security index state change and therefore the cluster state will be checked for if the migration has happened or not on every security index state change. \r\n\r\n## Notes\r\n- There needs to be a good way to track that this migration is actually successful and a strategy to handle it when it fails. \r\n- The migration could involve a lot of data, especially since there a customers with a lot of API keys, we probably want to exclude API keys when doing this since they're already using `metadata_flattened`. \r\n- The security index mapping `_meta` can't be used to store the status since we can't update the mapping. \r\n\r\n> To update the mapping after the index field migration, an update to the mapping held in cluster state would be required. I tested this by submitting a `TransportPutMappingAction` and it has a [check](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/mapping\/put\/TransportPutMappingAction.java#L194-L201) that makes sure we don't update the mapping for a system index. That's the full extent of my investigation and maybe my conclusion was premature. Looking closer at this it looks like we could in theory just use `MetadataMappingService` directly, so if we go ahead with this approach we should probably just do that instead. \r\n\r\n- A strategy for dealing with queries while this is being updated is needed. \r\n- How should we handle mixed versions clusters? \r\n\r\n## Excluded in Proof of Concept\r\n- Picking a good threadpool to run migration on\r\n- Code to make sure the persistent task runs on a node with the security index\r\n- Unit\/Integration Testing\r\n- Very good production ready code \u2122\ufe0f ","comments":[],"labels":["v8.14.0"]},{"title":"[Profiling, 9.0.0] Remove backwards compatibility code for `host.arch`","body":"https:\/\/github.com\/elastic\/elasticsearch\/blob\/8a7697bdc962e4265ad9248ad80adb26086517cc\/x-pack\/plugin\/profiling\/src\/main\/java\/org\/elasticsearch\/xpack\/profiling\/HostMetadata.java#L38\r\n\r\nUnused field `profiling.host.machine`:\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/8a7697bdc962e4265ad9248ad80adb26086517cc\/x-pack\/plugin\/core\/template-resources\/src\/main\/resources\/profiling\/component-template\/profiling-hosts.json#L151","comments":["Pinging @elastic\/obs-knowledge-team (Team:obs-knowledge)"],"labels":[":UniversalProfiling\/Application","Team:obs-knowledge"]},{"title":"[Profiling, 9.0.0] Remove backwards compatibility code in CO2 calculator","body":"https:\/\/github.com\/elastic\/elasticsearch\/blob\/8a7697bdc962e4265ad9248ad80adb26086517cc\/x-pack\/plugin\/profiling\/src\/main\/java\/org\/elasticsearch\/xpack\/profiling\/CO2Calculator.java#L57","comments":["Pinging @elastic\/obs-knowledge-team (Team:obs-knowledge)"],"labels":[":UniversalProfiling\/Application","Team:obs-knowledge"]},{"title":"[Profiling, 9.0.0] Remove unused fields from the stacktraces and flamegraph API","body":"Several fields in the API responses are unused meanwhile but need to be kept in order to allow upgrading 8.x clusters.\r\n\r\n**UNUSED fields in stacktraces response**\r\n`total_frames` indicates the number of frames including the inline frames. UNUSED in Kibana.\r\n`stack_trace_events` is a map of stack trace IDs to the number of events of the stack trace. UNUSED in Kibana since 8.12 (replaced by stack_traces.*.count).\r\n\r\n**UNUSED fields in flamegraph response**\r\n`TotalCPU` is UNUSED in Kibana.\r\n`TotalSamples` is the same as `SelfCPU`.\r\nAnd both are the same as `CountInclusive[0]` (0 is the root frame index), which is a sum of all `CountExclusive[1..]`.\r\n","comments":["Pinging @elastic\/obs-knowledge-team (Team:obs-knowledge)"],"labels":[":UniversalProfiling\/Application","Team:obs-knowledge"]},{"title":"Universal Profiling cleanups for 9.0.0","body":"This is a meta-issue to collect all cleanups to be done for the next major version.\n\n\n```[tasklist]\n### Tasks\n- [ ] https:\/\/github.com\/elastic\/elasticsearch\/issues\/106593\n- [ ] https:\/\/github.com\/elastic\/elasticsearch\/issues\/106596\n- [ ] https:\/\/github.com\/elastic\/elasticsearch\/issues\/106597\n- [ ] https:\/\/github.com\/elastic\/elasticsearch\/issues\/106681\n```\n","comments":["Pinging @elastic\/obs-knowledge-team (Team:obs-knowledge)","I'd suggest adding the `@UpdateForV9` annotation to relevant points in the code to keep track of these things today. When we decide to start working on v9, we'll go through all the places where this annotation is used to make sure we don't miss anything."],"labels":["Meta",":UniversalProfiling\/Application","Team:obs-knowledge"]},{"title":"Make dense vector field type updatable","body":"Make `index_options` updatable in `DenseVectorFieldMapper`.\r\n\r\nUsers should be able to switch between existing dense vector field types (`hnsw`, `int8_hnsw`) in case they're willing to (e.g. for scaling or accuracy purposes).\r\nFor example they might change dense vector field types' mappings from\r\n```\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"text_embedding\": {\r\n        \"type\": \"dense_vector\",\r\n        \"dims\": 384,\r\n        \"index\": true,\r\n        \"similarity\": \"cosine\",\r\n        \"index_options\": {\r\n          \"type\": \"hnsw\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nto\r\n```\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"text_embedding\": {\r\n        \"type\": \"dense_vector\",\r\n        \"dims\": 384,\r\n        \"index\": true,\r\n        \"similarity\": \"cosine\",\r\n        \"index_options\": {\r\n          \"type\": \"int8_hnsw\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n","comments":[],"labels":["v8.14.0"]},{"title":"SLM acts as min_count =0 when Minimum number of snapshots is not specified","body":"### Elasticsearch Version\r\n\r\n8.12.2\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nNA\r\n\r\n### Problem Description\r\n\r\n## SLM acts as min_count =0 when Minimum number of snapshots is not specified\r\nWhen user doesn't specify the min_count in a SLM policy, SLM acts as min_count =0, which means the minimum number of snapshots to retain is zero. It will cause all snapshots to get deleted. \r\n- The default value for optional setting - `min_count` is not documented in the SLM policy [document](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/slm-api-put-policy.html).\r\n- The code doesn't accept the min_count below 1. \r\n[Code](https:\/\/github.com\/elastic\/elasticsearch\/blob\/a1305373f2d8391387ba2138153f3612c7ce7f18\/x-pack\/plugin\/core\/src\/main\/java\/org\/elasticsearch\/xpack\/core\/slm\/SnapshotRetentionConfiguration.java#L95)\r\n```\r\n        if (this.minimumSnapshotCount != null && this.minimumSnapshotCount < 1) {\r\n            throw new IllegalArgumentException(\"minimum snapshot count must be at least 1, but was: \" + this.minimumSnapshotCount);\r\n        }\r\n        if (this.maximumSnapshotCount != null && this.maximumSnapshotCount < 1) {\r\n            throw new IllegalArgumentException(\"maximum snapshot count must be at least 1, but was: \" + this.maximumSnapshotCount);\r\n        }\r\n        if ((maximumSnapshotCount != null && minimumSnapshotCount != null) && this.minimumSnapshotCount > this.maximumSnapshotCount) {\r\n            throw new IllegalArgumentException(\r\n                \"minimum snapshot count \"\r\n                    + this.minimumSnapshotCount\r\n                    + \" cannot be larger than maximum snapshot count \"\r\n                    + this.maximumSnapshotCount\r\n            );\r\n        }\r\n```\r\n`min_count` default value being 0 is an unexpected behavior as the logic in the API validation doesn't allow user to set it below 1. This default value will cause data loss as all snapshots will be deleted. \r\n\r\n## The questions in this issue\r\n- Whether to change the default value to 1 as it would make more sense, given `min_count=0` is against what we allow in Elasticsearch API.\r\n- Could we document the default values for the optional properties in the `retention` (c.f https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/slm-api-put-policy.html#slm-api-put-request-body)?\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1. create a test v8.12.2 environment on ESS \r\n2. generate some snapshots by manually running the default [cloud-snapshot-policy] SLM policy \r\n3. modify the default policy by removing the `min_count` e.g. API query\r\n```\r\nPUT \/_slm\/policy\/cloud-snapshot-policy?pretty\r\n{\r\n  \"retention\": { \r\n    \"expire_after\": \"5m\", \r\n    \"max_count\": 1 \r\n  }\r\n}\r\n```\r\n\\# the `expire_after` is set to 5m to speed up the reproducing. \r\n\r\n4. Wait till the SLM snapshot retention executed, verify that all snapshots got deleted.\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">bug",">docs",":Data Management\/ILM+SLM","Team:Data Management","Team:Docs"]},{"title":"Test modifying and removing data in ProfileIT","body":"User Profiles can be used to store application data against a user (e.g. user preferences). This commit extends the integration tests for profile date storage to include explict tests for partial updates and clearing existing data\r\n","comments":["Pinging @elastic\/es-security (Team:Security)","I recently answered a question from someone about how to remove data from a profile, and realised we didn't have an integration test for it, so I wrote one."],"labels":[">test","Team:Security","v8.14.0",":Security\/Profile"]},{"title":"ESQL: Allow grouping key inside stats expressions","body":"Similar to aggs, allow grouping keys to used inside STATS expressions by\r\n introducing a synthetic eval, e.g.:\r\n\r\nSTATS a = x + count(*) BY x becomes\r\nSTATS c = count(*) BY x | EVAL a = x + c | KEEP a, x\r\n","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Hi @costin, I've created a changelog YAML for you.","> added some clarifying comments.\r\n\r\nI cannot see them as part of this PR - only updates to tests. Did you push that?"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"[ML] Inference API Caching","body":"### Description\n\nInvestigate caching to improve performance especially when generating text embeddings.","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML","Feature:GenAI"]},{"title":"Reduce InternalComposite in a streaming fashion","body":"We are currently using a merge sort algorithm to merge buckets on InternalComposite. The issue with this algorithm is that it requires to have all buckets to be merged on heap which is inefficient. This PR proposes to only keep the competitive aggregations in memory.\r\n\r\nIn order to do that be use a priority queue with a hashmap to keep traclk of the competitive buckets. We still delayed the merging of the child aggregations by introducing a DelayedMultiBucketAggregatorsReducer.\r\n\r\nI did compare the algorithms by merging 500 shards with 65k different values in 2gib heap nodes. The results are encouraging, the current algorithm will go out of memory for composite sizes >= 30k:\r\n\r\n```\r\nsize 1000 : 61.26956375\r\nsize 10000 : 39.356867625\r\nsize 20000 : 40.178292459000005\r\nsize 30000 : * node crash *\r\n```\r\n\r\nWith the new algorithm, the process is twice as fast and does not go out of memory:\r\n\r\n```\r\nsize 1000 : 31.356643000000002\r\nsize 10000 : 20.6347565\r\nsize 20000 : 17.899176792000002\r\nsize 30000 : 18.150707125\r\nsize 40000 : 18.304503375\r\nsize 50000 : 18.923937417\r\nsize 65536 : 20.001164250000002\r\n```\r\n\r\nrelates https:\/\/github.com\/elastic\/elasticsearch\/pull\/105207","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">non-issue",":Analytics\/Geo",":Analytics\/Aggregations","Team:Analytics","v8.14.0"]},{"title":"Add background filters of significant terms aggregations to can match query.","body":"This fixes the issue that the background query of a significant terms aggregation is ignored in the can match phase, leading to incorrect background results.","comments":["Hi @jan-elastic, I've created a changelog YAML for you.","Pinging @elastic\/es-search (Team:Search)","Hi @jan-elastic, I've created a changelog YAML for you."],"labels":[">bug",":Search\/Search","Team:Search","auto-backport-and-merge","v8.13.1","v8.14.0"]},{"title":"Allow users with read role to use the async_search\/status endpoint","body":"# Summary of issue\r\n\r\nCurrently, the `async_search` action and `async_search\/status` actions have radically different action names:\r\n\r\nGetAsyncStatusAction has `cluster:monitor\/async_search\/status`\r\nGetAsyncSearchAction has `indices:data\/read\/async_search\/get`\r\n\r\nThe status endpoint requires the monitor, manage or all privileges, which is not something all Kibana users will have, which results in this error on a secured cluster:\r\n\r\n```\r\naction [cluster:monitor\/async_search\/status] is unauthorized for user [my_kibana_user] with effective roles [my_kibana_role], this action is granted by the cluster privileges [monitor,manage,all]\r\n```\r\n\r\nThe async_search\/status endpoint was added in this PR: https:\/\/github.com\/elastic\/elasticsearch\/pull\/62947\r\n\r\nThe original request for this endpoint came from Kibana (https:\/\/github.com\/elastic\/elasticsearch\/issues\/57537) and their scope was to have background jobs running to check on status, using the kibana_admin user, so that is likely why a `cluster:monitor` action name was chosen rather than `indices:data`.\r\n\r\nHowever, there are two arguments for changing this:\r\n\r\n1. If you have the privilege to start an async search and retrieve its results, you should also have the permissions to check its status.\r\n\r\n2. Cross-cluster async search has recently made a change to do [incremental merges of search results](https:\/\/github.com\/elastic\/elasticsearch\/pull\/105781) whenever a user requests them via the `GET _async_search\/:id` endpoint. This will have CPU cost, and since Kibana regularly polls this endpoint to check for status, we (Elasticsearch) have asked Kibana to move to polling status via `_async_search\/status`. The current permissions settings on the status endpoint now blocks that move.\r\n\r\n## Options for changing\r\n\r\n1) I tried the simplest change I could think of, which is to add `cluster:monitor\/async_search\/status` to the IndexPrivilege.READ_AUTOMATON, but that does not work. It still fails with the same security error mentioned above. See the details section below.\r\n\r\nI also tried adding that to IndexPrivilege.ALL and it also still fails with the same error message. So my guess is that the cluster:monitor prefix is causing some issue.\r\n\r\n2) Change the GetAsyncStatusAction to have the same action name as GetAsyncSearchAction, namely: `indices:data\/read\/async_search\/get`\r\n\r\nIs this allowed? Is there a way to make this backwards compatible? We'd need roles with only `manage` or `monitor` and not `read` roles to still have access to async-search-status so as not to break existing functionality.\r\n\r\n3) Other options?\r\n\r\n\r\n### Attempt 1: add `cluster:monitor\/async_search\/status` to the IndexPrivilege.READ_AUTOMATON\r\n\r\nWith a user having the following privileges:\r\n\r\n```\r\n{\r\n  \"mp_role\": {\r\n    \"cluster\": [],\r\n    \"indices\": [\r\n      {\r\n        \"names\": [\r\n          \"blogs\"\r\n        ],\r\n        \"privileges\": [\r\n          \"read\"\r\n        ],\r\n        \"allow_restricted_indices\": false\r\n      },\r\n```\r\n\r\nI can start an async-search no problem (not shown), but when I query for status I get an error, even with the code changes in this first commit where I added `cluster:monitor\/async_search\/status` to the IndexPrivilege.READ_AUTOMATON.\r\n\r\n<details><summary>(toggle for error info and stack trace)<\/summary>\r\n\r\n```\r\n{\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"action [cluster:monitor\/async_search\/status] is unauthorized for user [mp_user] with effective roles [mp_role], this action is granted by the cluster privileges [monitor,manage,all]\",\"stack_trace\":\"org.elasticsearch.ElasticsearchSecurityException: action [cluster:monitor\/async_search\/status] is unauthorized for user [mp_user] with effective roles [mp_role], this action is granted by the cluster privileges [monitor,manage,all]\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.support.Exceptions.authorizationError(Exceptions.java:36)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.denialException(AuthorizationService.java:993)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.actionDenied(AuthorizationService.java:970)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.handleFailure(AuthorizationService.java:1049)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:1035)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:996)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:32)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:473)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.authorizeClusterAction(RBACEngine.java:192)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:463)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:439)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$3(AuthorizationService.java:326)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListener$2.onResponse(ActionListener.java:171)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:32)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$0(RBACEngine.java:153)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$getRoles$4(CompositeRolesStore.java:193)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$getRole$5(CompositeRolesStore.java:211)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.lambda$buildRole$0(RoleReferenceIntersection.java:49)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.GroupedActionListener.onResponse(GroupedActionListener.java:56)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.buildRoleFromRoleReference(CompositeRolesStore.java:291)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.lambda$buildRole$1(RoleReferenceIntersection.java:53)\r\n\tat java.base\/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.buildRole(RoleReferenceIntersection.java:53)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRole(CompositeRolesStore.java:209)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:186)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:149)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:342)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$4(SecurityActionFilter.java:161)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$MappedActionListener.onResponse(ActionListenerImplementations.java:95)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticatorChain.authenticate(AuthenticatorChain.java:93)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:264)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:157)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:114)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:93)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.doExecute(NodeClient.java:83)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\r\n\tat org.elasticsearch.xpack.search.RestGetAsyncStatusAction.lambda$prepareRequest$0(RestGetAsyncStatusAction.java:40)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:106)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.doHandleRequest(SecurityRestFilter.java:89)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$intercept$0(SecurityRestFilter.java:81)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListener$2.onResponse(ActionListener.java:171)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$3(SecondaryAuthenticator.java:99)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:109)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:90)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:75)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:465)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:561)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:438)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:126)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:116)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat io.netty.codec@4.1.107.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat io.netty.codec@4.1.107.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.forwardData(Netty4HttpHeaderValidator.java:194)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.forwardFullRequest(Netty4HttpHeaderValidator.java:137)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.lambda$requestStart$1(Netty4HttpHeaderValidator.java:120)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat java.base\/java.lang.Thread.run(Thread.java:1570)\r\n\"}],\"type\":\"security_exception\",\"reason\":\"action [cluster:monitor\/async_search\/status] is unauthorized for user [mp_user] with effective roles [mp_role], this action is granted by the cluster privileges [monitor,manage,all]\",\"stack_trace\":\"org.elasticsearch.ElasticsearchSecurityException: action [cluster:monitor\/async_search\/status] is unauthorized for user [mp_user] with effective roles [mp_role], this action is granted by the cluster privileges [monitor,manage,all]\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.support.Exceptions.authorizationError(Exceptions.java:36)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.denialException(AuthorizationService.java:993)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.actionDenied(AuthorizationService.java:970)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.handleFailure(AuthorizationService.java:1049)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:1035)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:996)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:32)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:473)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.authorizeClusterAction(RBACEngine.java:192)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:463)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:439)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$3(AuthorizationService.java:326)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListener$2.onResponse(ActionListener.java:171)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:32)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$0(RBACEngine.java:153)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$getRoles$4(CompositeRolesStore.java:193)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$getRole$5(CompositeRolesStore.java:211)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.lambda$buildRole$0(RoleReferenceIntersection.java:49)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.GroupedActionListener.onResponse(GroupedActionListener.java:56)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.buildRoleFromRoleReference(CompositeRolesStore.java:291)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.lambda$buildRole$1(RoleReferenceIntersection.java:53)\r\n\tat java.base\/java.lang.Iterable.forEach(Iterable.java:75)\r\n\tat org.elasticsearch.xcore@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.core.security.authz.store.RoleReferenceIntersection.buildRole(RoleReferenceIntersection.java:53)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRole(CompositeRolesStore.java:209)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:186)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:149)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:342)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$4(SecurityActionFilter.java:161)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$MappedActionListener.onResponse(ActionListenerImplementations.java:95)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticatorChain.authenticate(AuthenticatorChain.java:93)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:264)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:157)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:114)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:93)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.doExecute(NodeClient.java:83)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\r\n\tat org.elasticsearch.xpack.search.RestGetAsyncStatusAction.lambda$prepareRequest$0(RestGetAsyncStatusAction.java:40)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:106)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.doHandleRequest(SecurityRestFilter.java:89)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$intercept$0(SecurityRestFilter.java:81)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListener$2.onResponse(ActionListener.java:171)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$3(SecondaryAuthenticator.java:99)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:109)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:90)\r\n\tat org.elasticsearch.security@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:75)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:465)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:561)\r\n\tat org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:438)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:126)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:116)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat io.netty.codec@4.1.107.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat io.netty.codec@4.1.107.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.forwardData(Netty4HttpHeaderValidator.java:194)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.forwardFullRequest(Netty4HttpHeaderValidator.java:137)\r\n\tat org.elasticsearch.transport.netty4@8.14.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpHeaderValidator.lambda$requestStart$1(Netty4HttpHeaderValidator.java:120)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\r\n\tat io.netty.transport@4.1.107.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\r\n\tat io.netty.common@4.1.107.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat java.base\/java.lang.Thread.run(Thread.java:1570)\r\n\"},\"status\":403}\r\n```\r\n\r\n<\/details>\r\n\r\n\r\nWhat is the best way forward on this issue?","comments":[],"labels":[":Search\/Search",":Security\/Authorization","Team:Search","Team:Security","v8.14.0"]},{"title":"[CI] JvmErgonomicsTests testConcGCThreadsNotSetBasedOnProcessors failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/sqlmucw2rymtk\/tests\/:distribution:tools:server-cli:test\/org.elasticsearch.server.cli.JvmErgonomicsTests\/testConcGCThreadsNotSetBasedOnProcessors\n\n\n**Reproduction line:**\n```\n.\/gradlew ':distribution:tools:server-cli:test' --tests \"org.elasticsearch.server.cli.JvmErgonomicsTests.testConcGCThreadsNotSetBasedOnProcessors\" -Dtests.seed=41B53DAE03CBF902 -Dtests.locale=zh-Hant-TW -Dtests.timezone=Africa\/Khartoum -Druntime.java=22\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.server.cli.JvmErgonomicsTests#testConcGCThreadsNotSetBasedOnProcessors`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testConcGCThreadsNotSetBasedOnProcessors'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.server.cli.JvmErgonomicsTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.IllegalArgumentException: Failed to parse value [86] for setting [node.processors] must be <= 32\n\n  at __randomizedtesting.SeedInfo.seed([41B53DAE03CBF902:98B8286F669C566]:0)\n  at org.elasticsearch.common.util.concurrent.EsExecutors.lambda$static$0(EsExecutors.java:64)\n  at org.elasticsearch.common.settings.Setting.get(Setting.java:536)\n  at org.elasticsearch.common.settings.Setting.get(Setting.java:530)\n  at org.elasticsearch.common.util.concurrent.EsExecutors.allocatedProcessors(EsExecutors.java:79)\n  at org.elasticsearch.server.cli.JvmErgonomics.choose(JvmErgonomics.java:74)\n  at org.elasticsearch.server.cli.JvmErgonomicsTests.testConcGCThreadsNotSetBasedOnProcessors(JvmErgonomicsTests.java:196)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Likely relates #106526, these tests were only just added. I've muted the whole suite since I didn't see anything obviously different about this one test."],"labels":[":Core\/Infra\/Core",">test-failure","Team:Core\/Infra","low-risk"]},{"title":"[CI] SmokeTestMultiNodeClientYamlTestSuiteIT test {yaml=tsdb\/25_id_generation\/generates a consistent id} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vf33lxy2fa6xe\/tests\/:qa:smoke-test-multinode:yamlRestTest\/org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT\/test%20%7Byaml=tsdb%2F25_id_generation%2Fgenerates%20a%20consistent%20id%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':qa:smoke-test-multinode:yamlRestTest' --tests \"org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT.test {yaml=tsdb\/25_id_generation\/generates a consistent id}\" -Dtests.seed=601FD37E79DE53D1 -Dtests.locale=zh-HK -Dtests.timezone=Africa\/Kigali -Druntime.java=22\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT#test {yaml=tsdb\/25_id_generation\/generates a consistent id}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Byaml%3Dtsdb\/25_id_generation\/generates%20a%20consistent%20id%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Failure at [tsdb\/25_id_generation:100]: \nExpected: \"cn4excfoxSs_KdA5AAABeRnRFAY\"\n     but: was \"AAAAAMfoxSs_KdA5AAABeRnRFAY\"\n\n  at __randomizedtesting.SeedInfo.seed([601FD37E79DE53D1:E84BECA4D7223E29]:0)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:593)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n  Caused by: java.lang.AssertionError: \n  Expected: \"cn4excfoxSs_KdA5AAABeRnRFAY\"\n       but: was \"AAAAAMfoxSs_KdA5AAABeRnRFAY\"\n\n    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n    at org.junit.Assert.assertThat(Assert.java:964)\n    at org.junit.Assert.assertThat(Assert.java:930)\n    at org.elasticsearch.test.rest.yaml.section.MatchAssertion.doAssert(MatchAssertion.java:99)\n    at org.elasticsearch.test.rest.yaml.section.Assertion.execute(Assertion.java:65)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:573)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n    at java.lang.reflect.Method.invoke(Method.java:580)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n    at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)","Looking at the failure history, this seems to started to fail after #106080 was merged. It seems to fail once a day (except for 16-03, zero failures).","This [failed](https:\/\/gradle-enterprise.elastic.co\/s\/wr46il2r6z54u\/tests\/task\/:qa:smoke-test-multinode:yamlRestTest\/details\/org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT\/test%20%7Byaml=tsdb%2F25_id_generation%2Fgenerates%20a%20consistent%20id%7D?top-execution=1) after #106558 was merged. The get call that was added before the search request retrieves the right document with the id. The search response shows a different id. This suggests that something goes wrong with synthesizing the id.","Re-opening this issue. The failure occurred after the fix was merged: \r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/t2iq2kp27klqy","Maybe the somehow the `_ts_routing_hash` doesn't contain the right value hash. I will update the test to return the value in `_ts_routing_hash` as part of the search response. In order to verify that the right value is stored."],"labels":["blocker",">test-failure",":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"Syntax for querying both local and remote indices","body":"Currently, if we want to query local and remote clusters, we have to repeat index patterns. E.g., for querying `logs-*` on both the local cluster and the remote cluster, we have to construct the following index pattern:\r\n\r\n`logs-*,*:logs-*`\r\n\r\nThis quickly becomes cumbersome if the number of index patterns grows. Ideally, we would have some kind of syntax that signals that we should query both the local and the remote cluster, e.g., as a quick example:\r\n\r\n`*?:*`","comments":["Pinging @elastic\/es-search (Team:Search)","Pinging @elastic\/es-security (Team:Security)"],"labels":[":Search\/Search",":Security\/Security","team-discuss","Team:Search","Team:Security"]},{"title":"[CI] MlWithSecurityIT test {yaml=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder} failing","body":"Also fails MlWithSecurityInsufficientRoleIT in the same way, but on centos-7 instead\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/4c5bz7tr5xizc\/tests\/:x-pack:plugin:ml:qa:ml-with-security:yamlRestTest\/org.elasticsearch.smoketest.MlWithSecurityIT\/test%20%7Byaml=ml%2Fsearch_knn_query_vector_builder%2FTest%20vector%20search%20with%20query_vector_builder%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ml:qa:ml-with-security:yamlRestTest' --tests \"org.elasticsearch.smoketest.MlWithSecurityIT.test {yaml=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder}\" -Dtests.seed=55BA8E65316CA79B -Dtests.locale=cs -Dtests.timezone=Asia\/Novokuznetsk -Druntime.java=22\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.smoketest.MlWithSecurityIT#test {yaml=ml\/search_knn_query_vector_builder\/Test vector search with query_vector_builder}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Byaml%3Dml\/search_knn_query_vector_builder\/Test%20vector%20search%20with%20query_vector_builder%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.smoketest.MlWithSecurityIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Failure at [ml\/search_knn_query_vector_builder:123]: \nExpected: <3>\n     but: was <6>\n\n  at __randomizedtesting.SeedInfo.seed([55BA8E65316CA79B:DDEEB1BF9F90CA63]:0)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:593)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n  Caused by: java.lang.AssertionError: \n  Expected: <3>\n       but: was <6>\n\n    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n    at org.junit.Assert.assertThat(Assert.java:964)\n    at org.junit.Assert.assertThat(Assert.java:930)\n    at org.elasticsearch.test.rest.yaml.section.MatchAssertion.doAssert(MatchAssertion.java:99)\n    at org.elasticsearch.test.rest.yaml.section.Assertion.execute(Assertion.java:65)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:573)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:536)\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n    at java.lang.reflect.Method.invoke(Method.java:580)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","Another failure: https:\/\/gradle-enterprise.elastic.co\/s\/oug352jdahsp4\/tests\/task\/:x-pack:plugin:yamlRestTest\/details\/org.elasticsearch.xpack.test.rest.XPackRestIT\/test%20%7Bp0=ml%2Fsearch_knn_query_vector_builder%2FTest%20vector%20search%20with%20query_vector_builder%7D?top-execution=1\r\n\r\nI will mute it"],"labels":["blocker",">test-failure",":ml","Team:ML"]},{"title":"Get and Query API Key with profile uid","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"Updating the transport CA name","body":"Updated the transport CA in Security Auto-Configuration.\r\n\r\nCloses #106455 ","comments":["@elasticmachine test this please \ud83d\ude4f","Pinging @elastic\/es-security (Team:Security)","@tvernum the changelog validation was failing I corrected it (updated area to Security). Correct me if I am wrong. \r\nPlease can you rerun the builds.  ","@elasticmachine test this please \ud83d\ude4f","This looks good, but we need to add tests.\r\n\r\nIf it were me, I would add the tests in `AutoConfigureNodeTests`. I would add a method `testSubjectandIssuerForGeneratedCertificates()` that looks like the first block (`\/\/ test no publish settings`) from `testGeneratedHTTPCertificateSANs`\r\n\r\nTo do that we'll need to add a new method derived from `runAutoConfigAndReturnHTTPCertificate` like `Tuple<X509Certificate, X509Certificate> runAutoConfigAndReturnCertificates()` that can return both HTTP and transport certificates.\r\n\r\nThe existing `runAutoConfigAndReturnHTTPCertificate` method can delegate to the new `runAutoConfigAndReturnCertificates` method, and just return `v1()` (the http cert).\r\n\r\nThen you want to check that the subject and issuer identities of each certificate match what we expect.\r\n\r\nAre you able to do that, or do you need one of us to pick it up?\r\n\r\n\r\n\r\n\r\n","@tvernum Thanks for explanation. I have added a test case. Please review and let me know if it is in the right direction like you described."],"labels":[">bug","Team:Security","external-contributor",":Security\/AutoConfiguration","v8.14.0"]},{"title":"ESQL: perform a reduction on the data node","body":"So far, the reduction step for limit, sort, topN, stats was performed on the coordinator node.\r\nThis changes this by introducing a reduction at data node level, before the data is sent to the coordinator node who performs the final reduction step. For `stats` this will need a follow up PR.","comments":["@elasticmachine run elasticsearch-ci\/part-1","@elasticsearchmachine run elasticsearch-ci\/part-1","@elasticmachine run elasticsearch-ci\/part-1","@elasticmachine update branch","@elasticmachine run elasticsearch-ci\/part-1","Pinging @elastic\/es-analytical-engine (Team:Analytics)","Hi @astefan, I've created a changelog YAML for you."],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"Add granular error list to alias action response","body":"This commit changes the behavior when calling `POST _aliases` to remove an alias that does not exist. If the remove action has `must_exist==false` (or is not present), and a list of actions succeeds partially, the response will now contain a list of results for each input action.\r\n\r\nFor example on the request:\r\n```\r\nPOST _aliases\r\n{\r\n  \"actions\": [\r\n    {\r\n      \"remove\": {\r\n        \"index\": \"new_index\",\r\n        \"alias\": \"non_existing_alias\",\r\n        \"must_exist\": false\r\n      }\r\n    },\r\n    {\r\n      \"add\": {\r\n        \"index\": \"new_index\",\r\n        \"alias\": \"other_alias\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\nif the first action fails and the second action succeeds, the response will be:\r\n```\r\n{\r\n   \"acknowledged\": true,\r\n   \"errors\": true,\r\n   \"action_results\": [\r\n      {\r\n        \"action\u201d: \"remove\",\r\n        \"success:\" false,\r\n        \"error\": \"alias_not_found_exception\"\r\n      },\r\n      {\r\n        \"action\u201d: \"add\",\r\n        \"success:\" true,\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\n\r\nAdditionally, this commit changes the error message returned when removing an non-existent alias with `must_exist==true`. It now throws a `alias_not_found_exception` so the behavior is the same as when `must_exist==false` and all actions fail.\r\n\r\nThis closes #94478 , though is different than the behavior requested in that ticket. \r\n","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106514.docs-preview.app.elstc.co\/diff)","Hi @parkertimmins, I've created a changelog YAML for you.","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">feature",":Data Management\/Indices APIs","Team:Data Management","v8.14.0"]},{"title":"ESQL: Support ST_CONTAINS and ST_WITHIN","body":"This was a relatively small update on top of the main work on ST_INTERSECTS at https:\/\/github.com\/elastic\/elasticsearch\/pull\/104907. Most of the code is either test code, or generated code, since the foundations in `SpatialRelatesFunction` and related classes covered most of the needs.\r\n\r\nWe do both `ST_CONTAINS` and `ST_WITHIN` at the same time, due to the commutative relationship between these two. In particular we want to move literals to the right field, to both minimise the number of evaluators and simplify Lucene push-down. This is only possible if we can also swap `ST_CONTAINS` with `ST_WITHIN`, since:\r\n\r\n```\r\n  ST_CONTAINS(a, b) = ST_WITHIN(b, a)\r\n```\r\n\r\nTasks:\r\n\r\n* [x] Implement SpatialContains, and add it to PlanNamedTypes and EsqlFunctionRegistry\r\n* [x] Implement SpatialContainsTests and make all tests pass:\r\n  * [x] Fix multi-components (See [LatLongShape.makeContainsGeometryQuery](https:\/\/github.com\/apache\/lucene\/blob\/c78533c53cba8072cecabf62269ec8f0bfb35bed\/lucene\/core\/src\/java\/org\/apache\/lucene\/document\/LatLonShape.java#L355) and [ShapeDocValuesQuery.getContainsWeight](https:\/\/github.com\/elastic\/elasticsearch\/blob\/cf5fbfc857b36438baf357a31a7c2bc960111797\/server\/src\/main\/java\/org\/elasticsearch\/lucene\/spatial\/ShapeDocValuesQuery.java#L165)\r\n  * [x] Fix rectangles crossing dateline (see Lucene's Rectangle2D)\r\n* [x] Implement `ST_WITHIN` to avoid needing too many evaluators (support inverting between contains and within)\r\n  * [x] Implement SpatialWithin, and add it to PlanNamedTypes and EsqlFunctionRegistry\r\n  * [x] Implement LogicalPlanOptimizer rule to invert between contains and within when left argument is constant and right is not\r\n  * [x] Implement SpatialWithinTests and make all tests pass:\r\n* [x] Implement integration tests (using existing airports and airport_city_boundaries datasets)\r\n  * [x] Basic tests loading data from airports and city_boundaries and performing predicates\r\n  * [x] Tests together with ST_CENTROID for doc-values support\r\n  * [x] tests with EVAL with and without centroid\r\n* [x] Develop Lucene-pushdown in `LocalPhysicalPlanOptimizer` and `PhysicalPlanOptimizerTests`\r\n  * [x] Validate interaction with point doc-values (`ST_CONTAINS` and `ST_WITHIN` before `ST_CENTROID`)\r\n* [x] Write documentation\r\n* [ ] Either implement `ST_DISJOINT` or create an issue to do that\r\n","comments":["Hi @craigtaverner, I've created a changelog YAML for you.","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement",":Analytics\/Geo","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"[ML] Inference API dev console autocomplete","body":"### Description\n\nSome inference APIs don't autocomplete in the kibana dev console. This issue is to determine which APIs do not autocomplete and create a fix for them.\r\n\r\nExample: `GET \/_inference\/_all`","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">bug",":ml","Team:ML","Feature:GenAI"]},{"title":"[ML] Retrieving all inference endpoints","body":"### Description\n\nCurrently we can retrieve all inference endpoints via `GET \/_inference\/_all` to match the `_ml` APIs we should also support:\r\n\r\n`GET \/_inference`\r\n`GET \/_inference\/*`","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML","Feature:GenAI"]},{"title":"[ML] Allow users to specify similarity field","body":"This PR preserves the user provided `similarity` field value if specified. For OpenAI and Cohere if the user did not specify a value, then it defaults to `DOT_PRODUCT`.\r\n\r\nFor e5 if the user does not specify it defaults to `COSINE`. The e5 configuration does not allow the user to specify the dimensions. Right now they are hardcoded to 384.\r\n\r\nThis also changes how huggingface works. It allows the user to specify a value, if they don't we won't default it.\r\n\r\n\r\n## E5 Examples\r\n\r\n<details><summary>Defaults used<\/summary>\r\n\r\n```\r\nPUT _inference\/text_embedding\/test\r\n{\r\n  \"service\": \"elasticsearch\",\r\n  \"service_settings\": {\r\n    \"model_id\": \".multilingual-e5-small\",\r\n    \"num_threads\": 1,\r\n    \"num_allocations\": 1\r\n  }\r\n}\r\n\r\nResponse:\r\n\r\n{\r\n  \"model_id\": \"test\",\r\n  \"task_type\": \"text_embedding\",\r\n  \"service\": \"elasticsearch\",\r\n  \"service_settings\": {\r\n    \"num_allocations\": 1,\r\n    \"num_threads\": 1,\r\n    \"model_id\": \".multilingual-e5-small\",\r\n    \"similarity\": \"cosine\",\r\n    \"dimensions\": 384\r\n  },\r\n  \"task_settings\": {}\r\n}\r\n```\r\n\r\n<\/details>\r\n\r\n<details><summary>Specifying l2_norm<\/summary>\r\n\r\n```\r\nPUT _inference\/text_embedding\/test\r\n{\r\n  \"service\": \"elasticsearch\",\r\n  \"service_settings\": {\r\n    \"model_id\": \".multilingual-e5-small\",\r\n    \"num_threads\": 1,\r\n    \"num_allocations\": 1,\r\n    \"similarity\": \"l2_norm\"\r\n  }\r\n}\r\n\r\n{\r\n  \"model_id\": \"test\",\r\n  \"task_type\": \"text_embedding\",\r\n  \"service\": \"elasticsearch\",\r\n  \"service_settings\": {\r\n    \"num_allocations\": 1,\r\n    \"num_threads\": 1,\r\n    \"model_id\": \".multilingual-e5-small\",\r\n    \"similarity\": \"l2_norm\",\r\n    \"dimensions\": 384\r\n  },\r\n  \"task_settings\": {}\r\n}\r\n```\r\n\r\n<\/details>\r\n\r\n","comments":["@elasticmachine merge upstream","Pinging @elastic\/ml-core (Team:ML)"],"labels":[">non-issue",":ml","Team:ML","v8.14.0"]},{"title":"Enable LogsDB by default for logs-*","body":"### Description\n\nWe will introduce the [new index.mode \"logs\" as a tech preview](https:\/\/github.com\/elastic\/elasticsearch\/issues\/106462) in Elasticsearch 8.14. As a result, while in tech preview, the choice of adopting the new index mode is up to integrations.\r\n\r\nAfter we GA the new index mode in Elasticsearch 8.15 we need, anyway, to apply the new index mode \"logs\" by default to all \"logs-*\" index templates.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"Support `ignore_dynamic_beyond_limit` for all fields using synthetic source","body":"### Description\n\nWhen index requests exceed the `ignore_dynamic_beyond_limit` setting new fields are not dynamically mapped and new fields will be ignored. As a result, when reconstructing the original object, we are not able to reconstruct the field from doc values as no doc value fields are written for fields that are ignored. Here we need to apply a similar approach to what we do when supporting `ignore_malformed` and `ingore_above`, storing the field value \"as-is\" in a hidden stored field.\r\n\r\nNotice that this is a bit kore complicated then the other two cases because in this case the field is not mapped.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"Create a custom parser for parsing ISO8601 datetime variants","body":null,"comments":[],"labels":[":Core\/Infra\/Core",">refactoring","v8.14.0"]},{"title":"Support `ignore_above` for all string fields using synthetic source","body":"### Description\n\nString fields need to support `ignore_above` when synthetic source is enabled. The idea is to store the field value \"as-is\" in its json representation and later use the the value reading it from a hidden stored field. Note that because of [supporting synthetic source for all field types](https:\/\/github.com\/elastic\/elasticsearch\/issues\/106460) it is likely that a hidden field is already available for some of them. We can reuse that, if a hidden stored field already exists.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"S3 repository- support for AWS_CONTAINER_CREDENTIALS_FULL_URI","body":"### Description\n\nCurrently, S3 repository can be either authenticated with AWS credentials or IAM role token created by OIDC (available only in EKS) ([source](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.11\/repository-s3.html#iam-kubernetes-service-accounts))\r\n\r\nWe would like to authenticate S3 repository using the credentials that can be rotated without restarting the service, while not using EKS.\r\n\r\nLately, credentials endpoint defined by env `AWS_CONTAINER_CREDENTIALS_FULL_URI` is becoming more prevalent- it's vendor agnostic, allows rotation, it's supported by most of libraries.\r\n\r\nAre there any plans regarding the new methods of s3 repository auth?","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">enhancement",":Distributed\/Snapshot\/Restore","Team:Distributed"]},{"title":"Support `ignore_malformed` for all non-string fields using synthetic source","body":"### Description\r\n\r\nNon-string fields need to support `ignore_malformed` when synthetic source is enabled. The idea is to store the field value \"as-is\" in its json representation and later use the the value reading it from a hidden stored field. Note that because of [supporting synthetic source for all field types](https:\/\/github.com\/elastic\/elasticsearch\/issues\/106460) it is likely that a hidden field is already available for some of them. We can reuse that, if a hidden stored field already exists.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"ESQL: COLUMNSTATS","body":"### Description\n\nKibana is doing some string manipulation to try and make some stats for columns - https:\/\/github.com\/elastic\/kibana\/pull\/178433#issuecomment-2007220171 - and that's OK, but string manipulation on user queries is possibly a problem with language versioning and, well, easy to make mistakes in.\r\n\r\nAnyway! I think we should build something to build the column statistics for them. I have no idea what it'd look like - a command, I guess. Or an endpoint. Or a body parameter. But I don't know what they'd want returned and how we'd do it.\r\n\r\nBut one critical thing is that it has to be fast - we'd want to perform whatever tricks we can to make it so. They are sneaking a `LIMIT` right after the initial `FROM`. We could probably do better than that - if the `WHERE` clause can be pushed down we could let the limit be after that. Maybe we could even do a `SORT @timestamp DESC | LIMIT 1000` - that's generally pretty fast.\r\n\r\nAnd we could use field statistics from the index itself in some cases too!","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL"]},{"title":"ESQL: MV_MEDIAN should always return double and not round","body":"`MV_MEDIAN` returns the same type as its input, while `MV_AVG` always returns `double`. This also makes `MV_MEDIAN` inconsistent with `MEDIAN`, which always returns `double` (similarly to `AVG`).\r\n\r\nAdditionally, `MV_MEDIAN` rounds down to the nearest integer, which `MEDIAN` does not.\r\n\r\nWe should make `MV_MEDIAN` consistent by having it always return `double` and not round down, so that `MV_MEDIAN([1,2]) == 1.5`.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","This also partially blocks https:\/\/github.com\/elastic\/elasticsearch\/issues\/100634 as we cannot just replace `MEDIAN(const)` by `MV_MEDIAN(const)`.","My reasoning for `MV_MEDIAN` returning the input type as that it always generates something that can fit in the original type. Except for the silly rounding.  I'm not sure that that reasoning is valid, but it's much more reasonable than trying to make the `MEDIAN` agg return the same type as the input."],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL"]},{"title":"Transport mount searchable snapshot action set parentTaskId","body":"In the masterOperation method of TransportMountSearchableSnapshotAction, set the parentTaskId for MountSearchableSnapshotRequest.\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/105830","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/Snapshot\/Restore","Team:Distributed","external-contributor","v8.14.0"]},{"title":"[Inference API] OpenAI Completions API","body":"This PR adds support for a new task type named `COMPLETION` with OpenAI as the first implemented provider for completions.\r\nThe integrated API is [OpenAI's chat completions API](https:\/\/platform.openai.com\/docs\/guides\/text-generation\/chat-completions-api). Currently only one prompt with one response is supported as we should think about what multiple inputs imply (a conversation\/context? multiple single prompts independent from each other?) before supporting them IMO.\r\n\r\nSample requests:\r\n\r\n**Put OpenAI completion model**:\r\n\r\n```\r\nPUT {{ES_HOST}}\/_inference\/completion\/openai_chat_completions\r\n\r\n{\r\n    \"service\": \"openai\",\r\n    \"service_settings\": {\r\n        \"api_key\": \"{{OPEN_AI_API_KEY}}\",\r\n        \"model_id\": \"gpt-3.5-turbo\"\r\n    }\r\n}\r\n```\r\n\r\n**Perform Summarization**:\r\n\r\n```\r\nPOST {{ES_HOST}}\/_inference\/completion\/openai_chat_completions\r\n\r\n{\r\n  \"input\": \"Summarize the following text: text...\"\r\n}\r\n```\r\n\r\n**Perform Q&A**:\r\n```\r\nPOST {{ES_HOST}}\/_inference\/completion\/openai_chat_completions\r\n\r\n{\r\n  \"input\": \"Answer the following question: question\"\r\n}\r\n```\r\n\r\n**400 error for multiple inputs**:\r\n```\r\nPOST {{ES_HOST}}\/_inference\/completion\/openai_chat_completions\r\n\r\n{\r\n    \"input\": [\r\n        \"What is Elastic?\",\r\n        \"What is MongoDB?\"\r\n    ]\r\n}\r\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","> How should we handle it for completions? One option, is we leave it as it is, the input text won't be truncated and the user will get a similar error back indicating they need to reduce their message.\r\n\r\nI would say that's the way to go as it feels a little bit arbitrary to drop for example content, when you summarize something. So I would prefer to return an error and tell the user to reduce the input. I'll change that according to your suggestion.","I know that @joshdevins advocates for the simplest possible API but I wonder if we should implement the [Messages API](https:\/\/huggingface.co\/blog\/tgi-messages-api) instead? It seems like a good standard that is applicable broadly and would ease the migration from\/to this new API.","Just to clarify, I only want to avoid predefined\/canned prompts. I also think that a chat\/messages format API might be better as that's the direction most APIs are headed. It can be used for non-chat purposes as well so it doesn't close the door on other functionality. We discussed this briefly in [the doc](https:\/\/docs.google.com\/document\/d\/1bttChR1ePHSF_7-imq_tLFLlixKOxr666Rmq0Muv__A\/edit#heading=h.sfxzgbi0id38) but didn't come to a conclusion AFAIK. _Some_ structure in the input is very useful, not only for multi-turn messages but for system prompts as well. I'd assume that any \"static\" components are in the model config though (truncation, preamble system prompt, etc.).\r\n\r\nCan't we offer two options? Cohere does something similar with other parameters. You can only use one of `prompt` or `messages`:\r\n\r\n```\r\nprompt: \"\",\r\nmessages: [\r\n  {role: system, text: foo},\r\n]\r\n```\r\n\r\nEventually offering a structured `documents` parameter might also make sense, as Cohere does. Not now, but when we get to ES|QL `RETRIEVE | RAG`.","> How should we handle it for completions?\r\n\r\n@jonathan-buttner @timgrein regarding truncation, the APIs usually have options already for how to handle it. Shouldn't this be part of configuring the model?","> Can't we offer two options? Cohere does something similar with other parameters. You can only use one of prompt or messages\r\n\r\nTo me it would make sense to offer both options. I can add `messages` in a separate PR as a follow-up as this one is already pretty big. WDYT?","If we use mutually exclusive parameters, we can always add them later. I'd continue as planned with a simple `prompt` and we add `messages` later if @jimczi and others agree to that approach. Something we can discuss in the sync tomorrow too as the ML team should have a review as the maintainers of the API.","> @jonathan-buttner @timgrein regarding truncation, the APIs usually have options already for how to handle it. Shouldn't this be part of configuring the model?\r\n\r\nThere's an option [`max_tokens`](https:\/\/platform.openai.com\/docs\/api-reference\/chat\/create#chat-create-max_tokens), which limits the number of tokens in the output. The `input + output` is limited by the overall context window length, at least that's how I understood it. We can detect, whether a response was truncated by looking at the `stop_reason`. So AFAIU we would need to know the number of input tokens (by counting tokens) and output tokens (setting `max_tokens`) to know, if we need to truncate.\r\n\r\nSo if `context_window_length = 100; max_tokens = 50; input_tokens = 70` we could truncate the input by `20` on our side, which would imply that we count the tokens correctly. The tokenizers from this [library (jtokkit)](https:\/\/github.com\/knuddelsgmbh\/jtokkit) seem to be used by OpenAI (at least according to this [article](https:\/\/cookbook.openai.com\/examples\/how_to_count_tokens_with_tiktoken), which they also reference in the official docs). So we could count input tokens accurately, so that would be an option for truncating accurately (if we want to). This could be a problem, if the tokenization algorithm changes and may not be publicly accessible? I don't know how likely that is, but it should probably be part of the discussion.","We should not add tokenization into the Inference API. That's model specific and we don't want to maintain compatibility across all possible models.","I'm not sure I understand the problem exactly. Why do we need to capture if the input was truncated? That's the responsibility of the user and the model provider. We should only be a proxy.","[This concerns me a lot](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/inference\/src\/main\/java\/org\/elasticsearch\/xpack\/inference\/external\/openai\/OpenAiResponseHandler.java#L65-L82). We really ought not to try and decode error messages, but rather just rely on status codes and relaying those back to the user. The way we inspect things now is very brittle and puts us in a position of needing to maintain compatibility across any changes the provider makes. Why do we need to do this error message inspection?","> > How should we handle it for completions?\r\n> \r\n> @jonathan-buttner @timgrein regarding truncation, the APIs usually have options already for how to handle it. Shouldn't this be part of configuring the model?\r\n\r\nThe truncating was mainly built as a stop gap until we have chunking when were building out the text embedding integrations. When we were adding OpenAI text embedding support, we were trying to handle the scenario where we are sending too many tokens. OpenAI will return an error indicating you need to shorten the length of the text. In that scenario we could return the error to the user and they'd need to do the chunking. What we thought might be helpful is to truncate the text and retry the request to see if we can get it small enough to retrieve the text embeddings. We also had plans to return a field in our response to indicate that it was truncated but we haven't implemented that yet \ud83d\ude05 \r\n\r\n> the APIs usually have options already for how to handle it\r\n\r\nCohere does provide options for truncating the text using the `truncate` field. For cohere we don't truncate, we just pass the text through and return errors. My understanding is that for text embeddings for OpenAI and huggingface (for HF it probably depends on the model and I haven't done as much testing), they just return an error if you send too many tokens. And the only indication that there were too many tokens was in the error message string returned in the response.\r\n\r\n\r\n\r\n> [This concerns me a lot](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/inference\/src\/main\/java\/org\/elasticsearch\/xpack\/inference\/external\/openai\/OpenAiResponseHandler.java#L65-L82). We really ought not to try and decode error messages, but rather just rely on status codes and relaying those back to the user. The way we inspect things now is very brittle and puts us in a position of needing to maintain compatibility across any changes the provider makes. Why do we need to do this error message inspection?\r\n\r\nYeah fair point. I think the error message decoding is only for truncation which I think we're planning on removing once we have chunking support. Most of those if-blocks are just looking at the status codes and returning a generic error message + error message returned by the 3rd party service.","> [This concerns me a lot](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/inference\/src\/main\/java\/org\/elasticsearch\/xpack\/inference\/external\/openai\/OpenAiResponseHandler.java#L65-L82). We really ought not to try and decode error messages, but rather just rely on status codes and relaying those back to the user.\r\n\r\nIMO it's worth to add this point to the sync agenda tomorrow as it's seems like a more broader topic (at least for OpenAI as a provider), which can probably be discussed separately from this PR. WDYT?","> I'm not sure I understand the problem exactly. Why do we need to capture if the input was truncated? That's the responsibility of the user and the model provider. We should only be a proxy.\r\n\r\nOur response looks like this:\r\n\r\n```\r\n{\r\n    \"completion\": [\r\n       \"result\": \"inferred content...\"\r\n    ]\r\n}\r\n```\r\n\r\nSo currently there's no indication, if something was truncated or not. We could also simply return more information (token usage, stop reason etc.) from OpenAI's chat completion API without any adaption on our side, which would fit the \"simply a proxy\" model. WDYT?","> IMO it's worth to add this point to the sync agenda tomorrow as it's seems like a more broader topic (at least for OpenAI as a provider), which can probably be discussed separately from this PR. WDYT?\r\n\r\nYeah, I agree let's talk about it during the sync \ud83d\udc4d "],"labels":[">non-issue",":ml","Team:ML","v8.14.0"]},{"title":"[CI] FullClusterRestartIT testBehavioralAnalyticsDataRetention {cluster=OLD} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/hqve3p5ucvesq\/tests\/:x-pack:plugin:ent-search:qa:full-cluster-restart:v8.8.2%23bwcTest\/org.elasticsearch.xpack.application.FullClusterRestartIT\/testBehavioralAnalyticsDataRetention%20%7Bcluster=OLD%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ent-search:qa:full-cluster-restart:v8.8.2#bwcTest' -Dtests.class=\"org.elasticsearch.xpack.application.FullClusterRestartIT\" -Dtests.method=\"testBehavioralAnalyticsDataRetention {cluster=OLD}\" -Dtests.seed=855A489FC19ADFA9 -Dtests.bwc=true -Dtests.locale=mk -Dtests.timezone=America\/Winnipeg -Druntime.java=21\n```\n\n**Applicable branches:**\nmain, 8.13\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.application.FullClusterRestartIT#testBehavioralAnalyticsDataRetention {cluster=OLD}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testBehavioralAnalyticsDataRetention%20%7Bcluster%3DOLD%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.application.FullClusterRestartIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.client.ResponseException: method [GET], host [http:\/\/[::1]:43971], URI [_ilm\/policy\/behavioral_analytics-events-default_policy], status line [HTTP\/1.1 404 Not Found]\n{\"error\":{\"root_cause\":[{\"type\":\"resource_not_found_exception\",\"reason\":\"Lifecycle policy not found: behavioral_analytics-events-default_policy\"}],\"type\":\"resource_not_found_exception\",\"reason\":\"Lifecycle policy not found: behavioral_analytics-events-default_policy\"},\"status\":404}\n\n  at __randomizedtesting.SeedInfo.seed([855A489FC19ADFA9:2A200F143838A22D]:0)\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:351)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\n  at org.elasticsearch.xpack.application.FullClusterRestartIT.assertUsingLegacyDataRetentionPolicy(FullClusterRestartIT.java:106)\n  at org.elasticsearch.xpack.application.FullClusterRestartIT.lambda$testBehavioralAnalyticsDataRetention$1(FullClusterRestartIT.java:78)\n  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1262)\n  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1247)\n  at org.elasticsearch.xpack.application.FullClusterRestartIT.testBehavioralAnalyticsDataRetention(FullClusterRestartIT.java:78)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ent-search-eng (Team:Enterprise Search)"],"labels":["blocker",">test-failure",":EnterpriseSearch\/Application","Team:Enterprise Search"]},{"title":"CLI launcher \/ terminal improvements","body":"This removes `getWriter()` and `getErrorWriter()` from `Terminal` in order to prepare for using log4j in server-cli.\r\n\r\nAdditionally, this contains a few minor launcher and terminal improvements ported from #104055:\r\n- make sure IO failures on the pump thread are handled once completed (`close`).\r\n- catch throwables in `Command#main` to print them (formatted) via the terminal.\r\n\r\n","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">non-issue","Team:Core\/Infra",":Core\/Infra\/CLI","v8.14.0"]},{"title":"Inefficiency with PerFieldDocValuesFormat and ES87TSDBDocValuesProducer","body":"`SegmentReader` instances in tsdb indices with a sufficient number of fields do take some space (1000+ fields, could use 800kb of jvm heap). This is small, but many segment readers may exist for these tsdb indices and could use a non-trivial amount of jvm heap (e.g. 200mb). Looking closer it looks like often the `docValuesProducer` uses up to 50% of the memory being used by a `SegmentReader` (much more than `fieldInfos` field). There are two large maps in these cases in `org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsReader` and `org.elasticsearch.index.codec.tsdb.ES87TSDBDocValuesProducer` (for `sortedNumerics` field). \r\n\r\nIt seems wasteful that the same fields are being mentioned in both of these maps. Maybe we can use a single map instead to keep track of the right doc values producer for a field?","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">bug",":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"[Transform] Transform health should show timestamp of problems","body":"### Elasticsearch Version\n\nSeen in 8.12.0\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nn\/a\n\n### Problem Description\n\nWhen a transform fails, it goes to a `FAILED` state. The persistent task remains and the most recent error messages are available via _health. This helps with troubleshooting..\r\n\r\nHowever, in the UI, we do not show any indication of when the failure happened. There are no messages which indicate when the failure happened. There are no timestamps on the health page. \r\n\r\nFailures can often be due to cluster instability which cause search failures. This makes investigations more challenging as it could have been 1 hour, or 30 days ago.\r\n\r\nIs the transform able to indicate a date for its failure... ?\r\nIs this already in the API response somewhere, and it's just missing from the UI? \r\n\r\n<img width=\"1030\" alt=\"image\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/4185750\/2c50953e-d446-4fb8-b31b-fea77a892b6f\">\r\n\n\n### Steps to Reproduce\n\nI _think_ you can make a transform fail as follows, but tbc:\r\n- Create a basic transform, configure to run continuously\r\n- Delete\/close the source index\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/ml-core (Team:ML)","> Is the transform able to indicate a date for its failure... ?\r\n\r\n`TransformHealthIssue` class has a field `first_occurrence` which should contain a timestamp.\r\nUnfortunately, for some reason this field is not set in all cases. More precisely it is currently set in 3 out of 7 cases in the backend code.\r\nSo definitely this is something that should be fixed.\r\n\r\n> Is this already in the API response somewhere, and it's just missing from the UI?\r\n\r\nFrom what I can see, the column was in the UI since `8.7` so most likely this is a backend issue, **not** UI issue.\r\nSee image below:\r\n![Untitled](https:\/\/github.com\/elastic\/elasticsearch\/assets\/19312454\/7d224614-2778-49be-8403-3b738f761f94)\r\n"],"labels":[">enhancement",":ml\/Transform","Team:ML","Supportability"]},{"title":"Drop points for _seq_no","body":"### Description\r\n\r\nWe would like to drop the points data structure for the _seq_no field. We observed this field requires considerable disk space and we would like to reclaim storage once points are not used anymore. The points data structure is used to efficiently implement range queries on such field. Anyway, the seq_no field is used mainly to support replication and, after checkpointing happens, fast range queries on the _seq_no field are not necessary anymore.\r\n\r\nWe need to evaluate, anyway, when is the right time to drop points.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs"]},{"title":"Introduce a new `logs` index mode","body":"### Description\n\nWe would like to introduce a new index mode named `logs`. The idea is to use it as a gate for a set of features which will be used by default for the logging use case. The index mode will apply the following settings by default:\r\n\r\n* enable synthetic source\r\n* apply index sorting by (hostname, timestamp)\r\n* enable doc _id k-ordered compression\r\n* enable ZSTD compression\r\n* enable TSDB codecs for numbers, dates and keywords\r\n* dropping points for _seq_no","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs","v8.14.0"]},{"title":"Support synthetic source for all field types","body":"### Description\r\n\r\nSome field types are still missing support for synthetic source. We would like to implement the missing functionality adopting a strategy that allows us to use a stored field behind the scenes. The idea is to use the stored field later on to support reconstructing the original field value instead of reconstructing it from doc values. The missing types include:\r\n\r\n- [ ] binary\r\n- [ ] object\r\n- [ ] nested\r\n- [ ] long_range\r\n- [ ] double_range\r\n- [ ] date_range\r\n- [ ] ip_range\r\n- [ ] murmur3\r\n- [ ] annotated-text\r\n- [ ] completion\r\n- [ ] search_as_you_type\r\n- [ ] token_count\r\n- [ ] sparse_vector\r\n- [ ] rank_feature\r\n- [ ] rank_features\r\n- [ ] geo_shape\r\n- [ ] point\r\n- [ ] shape\r\n- [ ] percolator\r\n\r\nWe would like to enable this feature by default if synthetic source is enable. This will enable usage of these field types for both logs and TSDB index mode.\r\n\r\nOne support is implemented please check the box next to the corresponding type and link the PR next to it.","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[">enhancement","Team:StorageEngine",":StorageEngine\/Logs","v8.14.0"]},{"title":"Security Auto-Configuration: Transport CA is named \"HTTP CA\"","body":"In security auto configuration we generate separate CAs for transport & HTTP\r\nBut they have the same name: `CN=Elasticsearch security auto-configuration HTTP CA`\r\n- https:\/\/github.com\/elastic\/elasticsearch\/blob\/b7eafce32cb1e7dfdcb259aab89b886ff93a1c66\/x-pack\/plugin\/security\/cli\/src\/main\/java\/org\/elasticsearch\/xpack\/security\/cli\/AutoConfigureNode.java#L117\r\n\r\nThe transport CA should be named `CN=Elasticsearch security auto-configuration transport CA` (or something like that)","comments":["Pinging @elastic\/es-security (Team:Security)","Hi @tvernum, I've opened a pull request for this issue. Could you please review it when you have some time? Thanks!\r\n\r\nps I am new to making contributions here."],"labels":[">bug","good first issue","Team:Security",":Security\/AutoConfiguration"]},{"title":"Fix rolling over aliases to data streams","body":"Currently trying to rollover data streams via aliases fails. This commit fixes that by applying the rollover on the data stream directly.\r\n\r\nFixes #106137","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Hi @jbaiera, that sounds good, I'll update the PR shortly..!"],"labels":[">bug",":Data Management\/Data streams","Team:Data Management","external-contributor","v8.14.0"]},{"title":"[Supportability] Separate `_cluster\/reroute` API from `manage` cluster privilege into an individual cluster role, i.e. `manage_cluster_reroute`","body":"### Description\n\n### Description\r\n\r\nFrom a supportability perspective, we would like to have a feature, which is to separate `_cluster\/reroute` API from `manage` cluster privilege into an individual cluster role, i.e. `manage_cluster_reroute`.\r\n\r\nReason behind the scene is, `_cluster\/reroute` API is frequently used by Elasticsearch administrators or maintainers, as the shard retry will fail after reaching a certain limit. When that happens, calling this API is required. \r\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/cluster-reroute.html\r\n\r\nBut using this API doesn't need to touch cluster informations like templates, license, \r\n- https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.12\/update-license.html\r\n- https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.12\/indices-put-template.html\r\n\r\nIt would be great if we separate this out, and make a dedicated role for it.\r\n\r\n---\r\n\r\ncc @TomonoriSoejima ","comments":["Pinging @elastic\/es-security (Team:Security)","Pinging @elastic\/es-distributed (Team:Distributed)","I think the ability for less-privileged users to retry makes sense but the cluster reroute API encompasses way more privileges than just this one ability (e.g. you can use it to move or deallocate shards, and get a copy of the whole cluster state in response). We probably deserve a dedicated API just for this one use case so we can authorize it differently."],"labels":[">enhancement",":Distributed\/Allocation",":Security\/Authorization","Team:Distributed","Team:Security"]},{"title":"ESQL: Add configuration-like property to FROM command","body":"### Description\n\nAdd an 'escape-hatch' for configuring sources (`FROM` command) to allow parameters such as shard preference or indices options to be specified without having to manually extend the command.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Blocks https:\/\/github.com\/elastic\/elasticsearch\/pull\/106370"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"ESQL requests should wait for remote sinks","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"Validating mappings when simulate ingest is called","body":"This adds functionality to the simulate ingest API (#101409). If the index exists when the ingest simulate API is called that the transformed documents would be indexed into if this were not a simulation, then it now reports any mapping validations that would occur. For example, if we have an index named `test-index` with strict mappings and a single property named `foo`, then:\r\n```\r\nPOST _ingest\/_simulate?pretty&index=test-index\r\n{\r\n  \"docs\": [\r\n    {\r\n      \"_source\": {\r\n        \"foob\": \"bar\"\r\n      }\r\n    },\r\n    {\r\n      \"_source\": {\r\n        \"foo\": \"baz\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\nwould result in something like:\r\n```\r\n{\r\n    \"docs\" : [\r\n      {\r\n        \"doc\" : {\r\n          \"_id\" : \"id\",\r\n          \"_index\" : \"test-index\",\r\n          \"_version\" : -3,\r\n          \"_source\" : {\r\n            \"foob\" : \"bar\"\r\n          },\r\n          \"executed_pipelines\" : [\r\n            \"test-pipeline\"\r\n          ],\r\n          \"error\" : {\r\n            \"type\" : \"strict_dynamic_mapping_exception\",\r\n            \"reason\" : \"[1:9] mapping set to strict, dynamic introduction of [foob] within [_doc] is not allowed\"\r\n          }\r\n        }\r\n      },\r\n      {\r\n        \"doc\" : {\r\n          \"_id\" : \"id\",\r\n          \"_index\" : \"test-index\",\r\n          \"_version\" : -3,\r\n          \"_source\" : {\r\n            \"foo\" : \"baz\"\r\n          },\r\n          \"executed_pipelines\" : [\r\n            \"test-pipeline\"\r\n          ]\r\n        }\r\n      }\r\n    ]\r\n}\r\n```","comments":["Hi @masseyke, I've created a changelog YAML for you."],"labels":[">enhancement",":Data Management\/Ingest Node","v8.14.0"]},{"title":"[WIP] Text similarity rank retriever","body":"TBD","comments":[],"labels":["v8.14.0"]},{"title":"Block readiness on file settings being applied","body":"The readiness service is supposed to wait on the cluster being formed as well as file settings being applied. Yet file settings application is only checked on the master node. Since master election itself does not guarantee file settings have been applied, non-master nodes may mark themselves as ready even though they should not yet accept requests.\r\n\r\nThis commit reworks how the readiness service looks ofr file settings being applied. Previously it only worked on the master node where it got a callback directly from the file settings service. With this change we now only look at cluster state. Note that this means the readiness service and file based settings are tightly coupled. In practice this was always the case as they are both meant to be used in cloud environments, but the coupling is a bit tighter now since the readiness service will never report ready unless file based settings exist.","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Looks like a few related tests are failing here. Hopefully the fixes are not too bad.","Hi @rjernst, I've created a changelog YAML for you.","> Looks like a few related tests are failing here. Hopefully the fixes are not too bad.\r\n\r\nI've made a few adjustments. Primarily I changed the file settings reserved state to write an empty (-1) version when the file settings do not exist on the master node. That allows the readiness service to always look for the reserved state metadata, and check if there are any errors in the initial application."],"labels":[">refactoring","Team:Core\/Infra",":Core\/Infra\/Node Lifecycle","v8.14.0"]},{"title":"ESQL: keep after stats can lead to non-aggregating query","body":"Consider the query\r\n```\r\n              FROM test\r\n              | STATS by emp_no\r\n              | EVAL x = 1\r\n              | KEEP x\r\n```\r\nThis should return a row (with the value `x = 1`) for _each unique value of `emp_no`_.\r\n\r\nInstead, it returns one row for each document in `test`.\r\n\r\nInspecting the optimized logical plan shows why (to repro, add as test to `LogicalPlanOptimizerTests.java`):\r\n```\r\nEsqlProject[[x{r}#4]]\r\n\\_Eval[[1[INTEGER] AS x]]\r\n  \\_Limit[1000[INTEGER]]\r\n    \\_EsRelation[test][_meta_field{f}#12, emp_no{f}#6, first_name{f}#7, ge..]\r\n```\r\nThe `EsRelation` at the root is wrong, instead there has to be a logical plan that produces the number of unique `emp_no` values. (`Aggregation` can do this.)","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[CI] AsyncEsqlQueryActionIT classMethod failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vtm2mlzovzqwe\/tests\/:x-pack:plugin:esql:internalClusterTest\/org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT\r\n\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vtm2mlzovzqwe\/tests\/:x-pack:plugin:esql:internalClusterTest\/org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vtm2mlzovzqwe\/tests\/:x-pack:plugin:esql:internalClusterTest\/org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT\/testFinishingBeforeTimeoutKeep\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vtm2mlzovzqwe\/tests\/task\/:x-pack:plugin:esql:internalClusterTest\/details\/org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT\/testBasicAsyncExecution\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/vtm2mlzovzqwe\/tests\/task\/:x-pack:plugin:esql:internalClusterTest\/details\/org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT?top-execution=1\r\n\r\n**Reproduction line:**\r\n```\r\nnull\r\n```\r\n\r\n**Applicable branches:**\r\nmain\r\n\r\n**Reproduces locally?:**\r\nDidn't try\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT#classMethod`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('classMethod'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.esql.action.AsyncEsqlQueryActionIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.IllegalStateException: Some shards are still open after the threadpool terminated. Something is leaking index readers or store references.\r\n\r\n  at __randomizedtesting.SeedInfo.seed([D273A558C01BFDA9]:0)\r\n  at org.elasticsearch.node.Node.awaitClose(Node.java:703)\r\n  at org.elasticsearch.test.InternalTestCluster$NodeAndClient.close(InternalTestCluster.java:1061)\r\n  at org.elasticsearch.core.IOUtils.close(IOUtils.java:71)\r\n  at org.elasticsearch.core.IOUtils.close(IOUtils.java:119)\r\n  at org.elasticsearch.test.InternalTestCluster.close(InternalTestCluster.java:910)\r\n  at org.elasticsearch.test.CloseableTestClusterWrapper.close(CloseableTestClusterWrapper.java:26)\r\n  at org.elasticsearch.core.IOUtils.close(IOUtils.java:71)\r\n  at org.elasticsearch.core.IOUtils.close(IOUtils.java:119)\r\n  at org.elasticsearch.test.ESIntegTestCase.clearClusters(ESIntegTestCase.java:547)\r\n  at org.elasticsearch.test.ESIntegTestCase.afterClass(ESIntegTestCase.java:2336)\r\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\r\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:568)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:909)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:833)\r\n\r\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":["blocker",">test-failure","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[CI] SingleNodeDiscoveryIT testCannotJoinNodeWithSingleNodeDiscovery failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/a5dvvhunctzui\/tests\/:server:internalClusterTest\/org.elasticsearch.discovery.single.SingleNodeDiscoveryIT\/testCannotJoinNodeWithSingleNodeDiscovery\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.discovery.single.SingleNodeDiscoveryIT.testCannotJoinNodeWithSingleNodeDiscovery\" -Dtests.seed=EDFBA5FF9920092C -Dtests.locale=ga-IE -Dtests.timezone=Pacific\/Saipan -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.discovery.single.SingleNodeDiscoveryIT#testCannotJoinNodeWithSingleNodeDiscovery`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testCannotJoinNodeWithSingleNodeDiscovery'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.discovery.single.SingleNodeDiscoveryIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: (an empty collection or iterable containing [a string starting with \"JNDI lookup class is not available because this JRE does not support JNDI. JNDI string lookups will not be available, continuing configuration.\", a string starting with \"JMX runtime input lookup class is not available because this JRE does not support JMX. JMX lookups will not be available, continuing configuration. \"] or iterable containing [a string starting with \"JMX runtime input lookup class is not available because this JRE does not support JMX. JMX lookups will not be available, continuing configuration. \"])\n     but: was <[Attempted to append to non-started appender mock]>\n\n  at __randomizedtesting.SeedInfo.seed([EDFBA5FF9920092C:120F4759CD49A567]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2147)\n  at org.elasticsearch.test.ESTestCase.checkStaticState(ESTestCase.java:718)\n  at org.elasticsearch.test.ESTestCase.after(ESTestCase.java:524)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":["blocker",":Distributed\/Network",">test-failure","Team:Distributed"]},{"title":"Draft: Combine @timestamp, event.ingested and event.created into single timestamp range in cluster state","body":"IndexShard changed - created new allTimestampFieldsMissingOrNotIndexed method\u2026; seeing if all tests pass\r\n\r\nExploratory coding. No new tests have been added on testing event.ingested in cluster state. Before I do that I'd like feedback on whether this is the right approach structurally.\r\n","comments":[],"labels":["v8.14.0"]},{"title":"Add failure store generation field to DataStream","body":"As preparation for implementing rollovers for failure stores, I've added the `failureStoreGeneration` field on the `DataStream` class. We'll need this to keep track of the current \"generation\" for when we're generating a new index for the failure store.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">non-issue",":Data Management\/Data streams","Team:Data Management","v8.14.0"]},{"title":"Add es|ql rate aggregate function","body":"Develop a rate function. This is not just like adding a new grouping function, but it requires changes to the infra structure around it, because it requires time series id and timestamp information additionally be available to compute the rate of a counter field.\r\n\r\nRelates to #105397","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"Update `Aggregator` and `GroupingAggregator` interfaces to accept sorted pages\/blocks.","body":"Time series function like rate additionally need access to timestamp field. However the current `Aggregator` and `GroupingAggregator` interfaces only allow access to the blocks of the field an aggregate should be computed for. \r\n\r\nThis issue is about changing the interface so that when needed time series function can access value for additional  fields (like `@timestamp`).\r\n\r\nThis task is only required if the rate function code is going to be auto generated.\r\n\r\nRelates to #105397","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"[Transform] Consolidate permissions checks","body":"When we defer permissions checks, unattended Transforms will start and fail immediately with errors related to the internal transform index. The transform will progress beyond the 0th checkpoint, but the search repeatedly fails for missing permissions.\r\n\r\nRather than searching and failing, we will reuse the initial permissions check error, which includes the correct permission to set to get the Transform working. The check will happen before the initial search, so it will not progress beyond the 0th checkpoint.\r\n\r\nFix #105794","comments":["Pinging @elastic\/ml-core (Team:ML)","Hi @prwhelan, I've created a changelog YAML for you."],"labels":[">bug",":ml\/Transform","Team:ML","v8.14.0"]},{"title":"Implement time series grouping.","body":"Add or modify the existing hashing aggregator to group by time series emited by the time series grouping operator.\r\nThis implementation would group by the tsid hash and assume time series span across backing indices. So time serie grouping would be performed when aggregation mode is final.\r\n\r\nSubsequent grouping operators can perform additional grouping \/ aggregation. The grouping operator is enabled when the time series pragma is enabled.\r\n\r\nLocal time serie grouping would be implemented in followups. Often time series don't span across backing indices and therefor an optimal grouping using ordinals can be implemented.\r\n\r\nRelates to #105397","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"[DON'T MERGE] EsqlDataExtractor proof of concept v2","body":"Proof of concept for the EsqlDataExtractor. This works end-to-end for datafeeds with and without aggregations.\r\nThis is the next iteration of #105337. This version calls the ES|QL cleanly.\r\n\r\nNote that just one EsqlDataExtractor can handle both aggregated and non-aggregated data, contrary to the multiple implementations for the Query DSL data extractors.\r\n\r\nOne caveat: at the moment you have to manually set the chunking config \/ scroll size, to work around the ES|QL shortcoming of only returning a max number of results and providing no scroll functionality. (Alternatively, we should tweak our chunking in case the max number of results is hit.)\r\n\r\nExample usage of ES|QL with and without aggregations and Query DSL for comparison (all four data extractors give the same anomaly detection results):\r\n\r\n```\r\nPUT _ml\/anomaly_detectors\/query-dsl-job\r\n{\r\n  \"analysis_config\": {\r\n    \"bucket_span\": \"1h\",\r\n    \"detectors\": [\r\n      {\r\n        \"detector_description\": \"Total sales\",\r\n        \"function\": \"sum\",\r\n        \"field_name\": \"taxful_total_price\"\r\n      }\r\n    ]\r\n  },\r\n  \"data_description\": {\r\n    \"time_field\": \"order_date\"\r\n  },\r\n  \"results_index_name\": \"query-dsl-job-output\",\r\n  \"datafeed_config\": {\r\n    \"indices\": [\r\n      \"kibana_sample_data_ecommerce\"\r\n    ]\r\n  }\r\n}\r\n\r\nPUT _ml\/anomaly_detectors\/esql-job\r\n{\r\n  \"analysis_config\": {\r\n    \"bucket_span\": \"1h\",\r\n    \"detectors\": [\r\n      {\r\n        \"detector_description\": \"Total sales\",\r\n        \"function\": \"sum\",\r\n        \"field_name\": \"taxful_total_price\"\r\n      }\r\n    ]\r\n  },\r\n  \"data_description\": {\r\n    \"time_field\": \"order_date\"\r\n  },\r\n  \"results_index_name\": \"esql-job-output\",\r\n  \"datafeed_config\": {\r\n    \"esql_query\": \"\"\"\r\n      FROM kibana_sample_data_ecommerce | KEEP order_date, taxful_total_price\r\n    \"\"\",\r\n    \"chunking_config\": {\r\n      \"mode\": \"manual\",\r\n      \"time_span\": \"1d\"\r\n    }\r\n  }\r\n}\r\n\r\nPUT \/_ml\/anomaly_detectors\/query-dsl-aggs-job\r\n{\r\n  \"analysis_config\": {\r\n    \"bucket_span\": \"1h\",\r\n    \"summary_count_field_name\": \"doc_count\",\r\n    \"detectors\": [\r\n      {\r\n        \"detector_description\": \"Total sales\",\r\n        \"function\": \"sum\",\r\n        \"field_name\": \"taxful_total_price\"\r\n      }\r\n    ]\r\n  },\r\n  \"data_description\": {\r\n    \"time_field\": \"order_date\"\r\n  },\r\n  \"results_index_name\": \"query-dsl-aggs-job-output\",\r\n  \"datafeed_config\": {\r\n    \"indices\": [\r\n      \"kibana_sample_data_ecommerce\"\r\n    ],\r\n    \"aggs\": {\r\n      \"buckets\": {\r\n        \"date_histogram\": {\r\n          \"field\": \"order_date\",\r\n          \"fixed_interval\": \"10m\",\r\n          \"time_zone\": \"UTC\"\r\n        },\r\n        \"aggregations\": {\r\n          \"order_date\": {  \r\n            \"max\": {\"field\": \"order_date\"}\r\n          },\r\n          \"taxful_total_price\": {  \r\n            \"sum\": {\r\n              \"field\": \"taxful_total_price\"\r\n            }\r\n          }\r\n        }\r\n      }   \r\n    }\r\n  }\r\n}\r\n\r\nPUT _ml\/anomaly_detectors\/esql-aggs-job\r\n{\r\n  \"analysis_config\": {\r\n    \"bucket_span\": \"1h\",\r\n    \"summary_count_field_name\": \"doc_count\",\r\n    \"detectors\": [\r\n      {\r\n        \"detector_description\": \"Total sales\",\r\n        \"function\": \"sum\",\r\n        \"field_name\": \"taxful_total_price\"\r\n      }\r\n    ]\r\n  },\r\n  \"data_description\": {\r\n    \"time_field\": \"order_date\"\r\n  },\r\n  \"results_index_name\": \"esql-job-aggs-output\",\r\n  \"datafeed_config\": {\r\n    \"esql_query\": \"\"\"\r\n      FROM kibana_sample_data_ecommerce\r\n      | EVAL bucket = DATE_TRUNC(10 minute, order_date)\r\n      | STATS order_date = MAX(order_date), taxful_total_price = SUM(taxful_total_price), doc_count = COUNT(*) BY bucket\r\n    \"\"\",\r\n    \"chunking_config\": {\r\n      \"mode\": \"manual\",\r\n      \"time_span\": \"1d\"\r\n    }\r\n  }\r\n}\r\n```","comments":["Hi @jan-elastic, I've created a changelog YAML for you."],"labels":[">feature",":ml","Team:ML","v8.14.0"]},{"title":"[CI] DownsampleActionSingleNodeTests testCannotDownsampleWhileOtherDownsampleInProgress failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/hjehcw3ak4ets\/tests\/:x-pack:plugin:downsample:test\/org.elasticsearch.xpack.downsample.DownsampleActionSingleNodeTests\/testCannotDownsampleWhileOtherDownsampleInProgress\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:downsample:test' --tests \"org.elasticsearch.xpack.downsample.DownsampleActionSingleNodeTests.testCannotDownsampleWhileOtherDownsampleInProgress\" -Dtests.seed=8E5AEEE1E9924404 -Dtests.locale=zh-Hans-CN -Dtests.timezone=AST -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.downsample.DownsampleActionSingleNodeTests#testCannotDownsampleWhileOtherDownsampleInProgress`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testCannotDownsampleWhileOtherDownsampleInProgress'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.downsample.DownsampleActionSingleNodeTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.ElasticsearchException: downsample task [downsample-downsample-ynbrxltcjhvoej-0-13d] failed\n\n  at __randomizedtesting.SeedInfo.seed([8E5AEEE1E9924404:AE3519837F3E6A11]:0)\n  at org.elasticsearch.xpack.downsample.TransportDownsampleAction$2.onResponse(TransportDownsampleAction.java:425)\n  at org.elasticsearch.xpack.downsample.TransportDownsampleAction$2.onResponse(TransportDownsampleAction.java:417)\n  at org.elasticsearch.persistent.PersistentTasksService$1.onNewClusterState(PersistentTasksService.java:213)\n  at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onNewClusterState(ClusterStateObserver.java:379)\n  at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.clusterChanged(ClusterStateObserver.java:230)\n  at org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateListener(ClusterApplierService.java:560)\n  at org.elasticsearch.cluster.service.ClusterApplierService.callClusterStateListeners(ClusterApplierService.java:547)\n  at org.elasticsearch.cluster.service.ClusterApplierService.applyChanges(ClusterApplierService.java:505)\n  at org.elasticsearch.cluster.service.ClusterApplierService.runTask(ClusterApplierService.java:429)\n  at org.elasticsearch.cluster.service.ClusterApplierService$UpdateTask.run(ClusterApplierService.java:154)\n  at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:917)\n  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:217)\n  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:183)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)","The cause of the downsampling failure: \r\n\r\n```\r\n[2024-03-17T20:39:16,407][WARN ][o.e.p.AllocatedPersistentTask] [node_s_0] task [downsample-downsample-ynbrxltcjhvoej-0-13d] failed with an exception\t\r\norg.elasticsearch.xpack.downsample.DownsampleShardIndexerException: Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d] on shard [ynbrxltcjhvoej][0] failed indexing [452]\t\r\n\tat org.elasticsearch.xpack.downsample.DownsampleShardIndexer.execute(DownsampleShardIndexer.java:202) ~[main\/:?]\t\r\n\tat org.elasticsearch.xpack.downsample.DownsampleShardPersistentTaskExecutor$1.doRun(DownsampleShardPersistentTaskExecutor.java:220) ~[main\/:?]\t\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984) ~[elasticsearch-8.14.0-SNAPSHOT.jar:8.14.0-SNAPSHOT]\t\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-8.14.0-SNAPSHOT.jar:8.14.0-SNAPSHOT]\t\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\t\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n```\r\n\r\nI think logging should here be improved, so that we can get insight how the indexing failed.","The reason that indexing failed is because the target index started to block writes:\r\n\r\n```\r\n[2024-03-17T20:39:16,345][ERROR][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [[ynbrxltcjhvoej][0]] failed to populate downsample index. Failures: [{null=org.elasticsearch.cluster.block.ClusterBlockException: index [downsample-ynbrxltcjhvoej] blocked by: [FORBIDDEN\/8\/index write (api)];}]\r\n```","Looking better at the test logs, it looks like the first downsample attempt completes successfully, but just after the persistent task completes, the duplicate downsample operations starts downsampling. Failing later, because the initial downsample operation made target index read only.  I think just before the downsample shard operation starts, a check should be added that checks whether the `index.downsample.status` has been set to `success`. There is a similar check at pre check in `TransportDownsampleAction`, but in this case that isn't enough.\r\n\r\n```\r\n[2024-03-17T20:39:16,127][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d on shard [ynbrxltcjhvoej][0] started\t\r\n[2024-03-17T20:39:16,132][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [ynbrxltcjhvoej][0] processed [1034] docs, created [452] downsample buckets\t\r\n[2024-03-17T20:39:16,215][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [[ynbrxltcjhvoej][0]] successfully sent [1034], received source doc [452], indexed downsampled doc [452], failed [0], took [0s]\t\r\n[2024-03-17T20:39:16,216][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d on shard [ynbrxltcjhvoej][0] completed\t\r\n[2024-03-17T20:39:16,236][INFO ][o.e.x.d.TransportDownsampleAction] [node_s_0] Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d completed for shard [ynbrxltcjhvoej][0]\t\r\n[2024-03-17T20:39:16,236][INFO ][o.e.x.d.TransportDownsampleAction] [node_s_0] All downsampling tasks completed [1]\t\r\n[2024-03-17T20:39:16,329][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d on shard [ynbrxltcjhvoej][0] started\t\r\n[2024-03-17T20:39:16,334][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [ynbrxltcjhvoej][0] processed [1034] docs, created [452] downsample buckets\t\r\n[2024-03-17T20:39:16,345][ERROR][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [[ynbrxltcjhvoej][0]] failed to populate downsample index. Failures: [{null=org.elasticsearch.cluster.block.ClusterBlockException: index [downsample-ynbrxltcjhvoej] blocked by: [FORBIDDEN\/8\/index write (api)];}]\t\r\n[2024-03-17T20:39:16,346][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Shard [[ynbrxltcjhvoej][0]] successfully sent [1034], received source doc [452], indexed downsampled doc [452], failed [452], took [0s]\t\r\n[2024-03-17T20:39:16,350][INFO ][o.e.x.d.DownsampleShardIndexer] [node_s_0] Downsampling task [downsample-downsample-ynbrxltcjhvoej-0-13d] on shard [ynbrxltcjhvoej][0] failed indexing [452]\t\r\n````"],"labels":[">test-failure",":StorageEngine\/TSDB","low-risk","Team:StorageEngine"]},{"title":"[CI] SearchStatesIT testCanMatch failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/ae4m7l4z6olcs\/tests\/:qa:ccs-rolling-upgrade-remote-cluster:v8.9.2%23oldClusterTest\/org.elasticsearch.upgrades.SearchStatesIT\/testCanMatch\n\n\n**Reproduction line:**\n```\n.\/gradlew ':qa:ccs-rolling-upgrade-remote-cluster:v8.9.2#oldClusterTest' -Dtests.class=\"org.elasticsearch.upgrades.SearchStatesIT\" -Dtests.method=\"testCanMatch\" -Dtests.seed=263FCDC8D1FBB3FC -Dtests.bwc=true -Dtests.locale=es-CU -Dtests.timezone=Africa\/Khartoum -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.upgrades.SearchStatesIT#testCanMatch`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testCanMatch'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.upgrades.SearchStatesIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.client.ResponseException: method [POST], host [http:\/\/127.0.0.1:42577], URI [\/_features\/_reset], status line [HTTP\/1.1 500 Internal Server Error]\n{\"error\":{\"root_cause\":[{\"type\":\"timeout_exception\",\"reason\":\"Unknown error occurred while updating cluster state\"}],\"type\":\"timeout_exception\",\"reason\":\"Unknown error occurred while updating cluster state\"},\"status\":500}\n\n  at __randomizedtesting.SeedInfo.seed([263FCDC8D1FBB3FC:301BF88680481FD5]:0)\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:351)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\n  at org.elasticsearch.test.rest.ESRestTestCase.wipeCluster(ESRestTestCase.java:847)\n  at org.elasticsearch.test.rest.ESRestTestCase.cleanUpCluster(ESRestTestCase.java:543)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","This doesn't seem to be related to ILM (or Data Management for that matter). Not sure who to re-assign this to though. The test class looks like it's owned by one of the search teams, but the exception happens in the `ESRestRestCase#cleanUpCluster` method.","Pinging @elastic\/es-search (Team:Search)","The last failure of this test #103358 was attributed to Data Management. I've reassigned to Search","Ah hmm, I see that @jbaiera merged https:\/\/github.com\/elastic\/elasticsearch\/pull\/103484 to fix https:\/\/github.com\/elastic\/elasticsearch\/issues\/103358. Maybe he can take a look to see why this is still\/again failing?","As mentioned already this _might_ be related to #106461 but I'm not so sure. The fact that the test is doing a rolling upgrade with a version before 8.11 could support that hypothesis. I downloaded the build log zip from CI. There seems to be some kind of hiccup while forming the remote cluster around the time of the failure in the log for `remote-0`, but I can't find for sure that it's because of the template install in #106461. In fact, it looks like the cluster is all the same version still.","Another one, on 8.10.1: https:\/\/gradle-enterprise.elastic.co\/s\/4d4zmomk5dujq\/tests\/:qa:ccs-rolling-upgrade-remote-cluster:v8.10.1%23oneThirdUpgraded\/org.elasticsearch.upgrades.SearchStatesIT\/testCanMatch","The recent failures have occured together with [SearchStatesIT \u00bb testBWCSearchStates](https:\/\/gradle-enterprise.elastic.co\/s\/f2yk3xxvlz7lc\/tests\/:qa:ccs-rolling-upgrade-remote-cluster:v8.9.1%23oneThirdUpgraded\/org.elasticsearch.upgrades.SearchStatesIT\/testBWCSearchStates)\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/96436#issuecomment-2013141820\r\n\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/af2qeczdb7kqc\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/6s2sgf6hvcu2e\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/oleqsgspwjz7q"],"labels":["blocker",":Search\/Search",">test-failure","Team:Search"]},{"title":"feat: delete request write to translog with routing key","body":"As issue #97915 mentioned, since translog does not store the routing key, the ccr can't work correctly. So I raise this pull request to try to fix the problem. \r\nThe main idea is that when we get a delete request, we try to save the routing key as index request do.\r\nSince the snapshot recovery runs in localhost, I don't think it needs routing key, as that default value \"null\" is set.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/CCR","Team:Distributed","external-contributor","v8.14.0"]},{"title":"Ordinal-based BytesRef block for TSID","body":"For large BytesRef tsid blocks, containing one or a few tsid, it may be more efficient to implement a dictionary and an array of ordinals instead of storing the BytesRef multiple times. Also, when designing such blocks, we might consider using the artificial 'global' ordinals (generated by the TimeSeriesSortedSourceOperator) so that consumers can take advantage of them.\r\n\r\nRelates #105397","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":["Team:Analytics",":StorageEngine\/TSDB",":Analytics\/Compute Engine","Team:StorageEngine"]},{"title":"Include document size information in ingest stats","body":"Ingest pipelines can change the size of the ingested documents, sometimes substantially. It'd be awfully useful if the `ingest` component of nodes stats included information about the total number of bytes received by each ingest pipeline and the total size of the resulting documents.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/Ingest Node","Team:Data Management","Supportability"]},{"title":"[ML] Add Cohere rerank to _inference service","body":"# DRAFT \r\nTODO:\r\n\r\n- finish parsing the response from cohere\r\n- add more integration tests\r\n\r\n# Add support for [cohere rerank API](https:\/\/docs.cohere.com\/reference\/rerank-1).\r\n\r\n\r\n\r\n```\r\ncurl -X PUT \"localhost:9200\/_inference\/rerank\/test_cohere_rerank\" \\\r\n-H 'Content-Type: application\/json' -u elastic-admin:elastic-password \\\r\n-d'  \r\n{\r\n  \"service\": \"cohere\",\r\n  \"service_settings\": {\r\n    \"model_id\": \"rerank-english-v2.0\",\r\n    \"api_key\": \"<Cohere-API-KEY>\"\r\n  }\r\n}\r\n'\r\n```\r\n\r\n```\r\ncurl -X POST \"localhost:9200\/_inference\/rerank\/test_cohere_rerank\" \\\r\n-H 'Content-Type: application\/json' -u elastic-admin:elastic-password \\\r\n-d'  \r\n{\r\n  \"input\": [\"some\", \"documents\"],\r\n  \"query\": \"The ES search query\"\r\n}\r\n'\r\n```","comments":["Hi @maxhniebergall, I've created a changelog YAML for you.","It would be good if the input API accepted document IDs. This might make it easier to map back to results. WDYT @demjened ?\r\n\r\n```\r\n{\r\n  \"input\": [\r\n    {\"id\": \"AAA\", \"text\": \"some\"},\r\n    {\"id\": \"BBB\", \"text\": \"documents\"},\r\n  ],\r\n  \"query\": \"The ES search query\"\r\n}\r\n```\r\n\r\nResponses can then be returned ranked but with ID references and scores:\r\n\r\n```\r\n{\r\n  \"results\": [\r\n    {\"id\": \"BBB\", \"score\": 0.899},\r\n    {\"id\": \"AAA\", \"score\": 0.723}\r\n  ]\r\n}\r\n```\r\n","Hey @joshdevins\r\n> It would be good if the input API accepted document IDs. This might make it easier to map back to results.\r\n\r\nIt might, but as long as the API strictly fulfills the contract of \"array of N texts go in -> N scores come out in the exact same order\", we don't need a map. Furthermore requiring IDs in the input assumes that reranking will only be invoked in context of documents, making certain abstractions (e.g. simulate reranking with just text input) harder.\r\n\r\nOn the other hand the [Cohere rerank API](https:\/\/docs.cohere.com\/docs\/reranking?ref=txt.cohere.com) returns a rich object of documents and indices.\r\n\r\nMaybe we can use the existing inference API task types as a reference and align the rerank solution to look similar.","I'm thinking of the case where we have a pairwise or listwise reranker. LLMs are slow but very good at this, for example. In this case, you feed a list of documents and get back a reranked list of documents, but _no scores_. It's not on the immediate roadmap but once we have GPU for inference, it could be something we add in the future. So any API we design now ought to at least consider it.","That's a good point. At least we can make the API _support_ adding IDs in the future, something like:\r\n\r\nRequest:\r\n```\r\n{\r\n  \"input\": [\r\n      {\"text\": \"some\"},\r\n      {\"text\": \"documents\"}\r\n  ],\r\n  \"query\": \"The ES search query\"\r\n}\r\n```\r\n\r\nResponse:\r\n```\r\n{\r\n  \"results\": [\r\n    {\"score\": 0.899},\r\n    {\"score\": 0.723}\r\n  ]\r\n}\r\n```\r\n\r\nThen later if the need arises, we can add an `id` field (or any other metadata field) to both the input and response objects.\r\nWDYT? cc @maxhniebergall "],"labels":[">feature",":ml","v8.14.0"]},{"title":"Add transport version for search load autoscaling","body":"Adds a new transport version for search load autoscaling","comments":["Pinging @elastic\/es-search (Team:Search)","Hi @JVerwolf, I've created a changelog YAML for you."],"labels":[">enhancement",":Search\/Search","Team:Search","v8.14.0"]},{"title":"Update field-usage-stats.asciidoc to clarify counter behaviour","body":"Added additional information to clarify that counter are kept in memory and reset on node restart or shard rellocation.\r\n\r\n- Have you signed the [contributor license agreement](https:\/\/www.elastic.co\/contributor-agreement)?\r\n- Have you followed the [contributor guidelines](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/CONTRIBUTING.md)?\r\n- If submitting code, have you built your formula locally prior to submission with `gradle check`?\r\n- If submitting code, is your pull request against main? Unless there is a good reason otherwise, we prefer pull requests against main and will backport as needed.\r\n- If submitting code, have you checked that your submission is for an [OS and architecture that we support](https:\/\/www.elastic.co\/support\/matrix#show_os)?\r\n- If you are submitting this code for a class then read our [policy](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.\r\n","comments":["<!-- CLA-CHECK:106364 -->\n&#10060; Author of the following commits did not sign a [Contributor Agreement](https:\/\/www.elastic.co\/contributor-agreement):\n   ad7a0a2ff1ad6fd67ac4f9afba4234ff1337254e\n\n   Please, read and sign the above mentioned agreement if you want to contribute to this project","Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106364.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-search (Team:Search)","Pinging @elastic\/es-docs (Team:Docs)","Hi, I've signed the cla already, not sure why is not updating the PR"],"labels":[">docs",":Search\/Search","Team:Docs","Team:Search","external-contributor","v8.14.0"]},{"title":"[Transform] `latest` transform skipping some source documents","body":"### Elasticsearch Version\r\n\r\n8.13\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nMacOS\r\n\r\n### Problem Description\r\n\r\nLatest transform was reported to skip some source documents.\r\n\r\nI identified 2 potential issues:\r\n1. When there are multiple source documents with the same `@timestamp` value, the `latest` transform only picks one of them.\r\n2. `sync.time.delay` field does not seem to influence the filter `range` queries issued by the `latest` transform.\r\n\r\nAd 1.:\r\nThis is how we build the range query in the code:\r\n```\r\n        \/\/ We are only interested in documents that were created in the timeline of the current checkpoint.\r\n        \/\/ Older documents cannot influence the transform results as we require the sort field values to change monotonically over time.\r\n        return QueryBuilders.rangeQuery(synchronizationField)\r\n            .gte(lastCheckpoint.getTimeUpperBound())\r\n            .lt(nextCheckpoint.getTimeUpperBound())\r\n            .format(\"epoch_millis\");\r\n```\r\n\r\nSo I think it can be that because of this `lt` the documents that have the same timestamp as the document that was already involved in the checkpoint will not get processed.\r\nThis should be taken care of by the `time.sync.delay` but apparently it doesn't work in this case (Ad 2.)\r\n\r\n### Steps to Reproduce\r\n\r\nThis has been reproduced by the Kibana team (https:\/\/github.com\/elastic\/security-team\/issues\/8893).\r\nNow I'm working on reproducing it locally.\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">bug",":ml\/Transform","Team:ML"]},{"title":"[CI] ConnectorSyncJobIndexServiceTests testTransformConnectorFilteringToSyncJobRepresentation_WithFilteringEqualNull failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/lhkeavcyvzmde\/tests\/:x-pack:plugin:ent-search:test\/org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests\/testTransformConnectorFilteringToSyncJobRepresentation_WithFilteringEqualNull\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ent-search:test' --tests \"org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.testTransformConnectorFilteringToSyncJobRepresentation_WithFilteringEqualNull\" -Dtests.seed=F2916A12BDD02D3C -Dtests.locale=ga -Dtests.timezone=Europe\/Busingen -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nYes\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests#testTransformConnectorFilteringToSyncJobRepresentation_WithFilteringEqualNull`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testTransformConnectorFilteringToSyncJobRepresentation_WithFilteringEqualNull'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.util.concurrent.ExecutionException: org.elasticsearch.index.mapper.DocumentParsingException: [1:3191] failed to parse field [last_access_control_sync_scheduled_at] of type [date] in document with id 'aHjhN44Be9FYZew6hpKI'. Preview of field's value: '-227369528-07-23T18:41:56.060Z'\n\n  at __randomizedtesting.SeedInfo.seed([F2916A12BDD02D3C:435FBEEB54268371]:0)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.getValue(PlainActionFuture.java:336)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.get(PlainActionFuture.java:310)\n  at org.elasticsearch.action.support.PlainActionFuture.get(PlainActionFuture.java:69)\n  at org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.createConnector(ConnectorSyncJobIndexServiceTests.java:90)\n  at org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.setup(ConnectorSyncJobIndexServiceTests.java:75)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n  Caused by: org.elasticsearch.index.mapper.DocumentParsingException: [1:3191] failed to parse field [last_access_control_sync_scheduled_at] of type [date] in document with id 'aHjhN44Be9FYZew6hpKI'. Preview of field's value: '-227369528-07-23T18:41:56.060Z'\n\n    at org.elasticsearch.index.mapper.FieldMapper.rethrowAsDocumentParsingException(FieldMapper.java:232)\n    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:185)\n    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n    at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n    at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n    at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    at java.lang.Thread.run(Thread.java:1583)\n\n    Caused by: java.lang.IllegalArgumentException: failed to parse date field [-227369528-07-23T18:41:56.060Z] with format [strict_date_optional_time||epoch_millis]\n\n      at org.elasticsearch.common.time.JavaDateFormatter.parse(JavaDateFormatter.java:211)\n      at org.elasticsearch.index.mapper.DateFieldMapper$DateFieldType.parse(DateFieldMapper.java:490)\n      at org.elasticsearch.index.mapper.DateFieldMapper.parseCreateField(DateFieldMapper.java:910)\n      at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:183)\n      at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n      at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n      at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n      at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n      at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n      at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n      at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n      at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n      at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n      at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n      at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n      at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n      at java.lang.Thread.run(Thread.java:1583)\n\n      Caused by: java.time.format.DateTimeParseException: Failed to parse with all enclosed parsers\n\n        at org.elasticsearch.common.time.JavaDateFormatter.doParse(JavaDateFormatter.java:238)\n        at org.elasticsearch.common.time.JavaDateFormatter.parse(JavaDateFormatter.java:209)\n        at org.elasticsearch.index.mapper.DateFieldMapper$DateFieldType.parse(DateFieldMapper.java:490)\n        at org.elasticsearch.index.mapper.DateFieldMapper.parseCreateField(DateFieldMapper.java:910)\n        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:183)\n        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n        at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n        at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n        at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n        at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n        at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n        at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n        at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n        at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n        at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ent-search-eng (Team:Enterprise Search)"],"labels":[">test","blocker",">test-failure",":EnterpriseSearch\/Application","Team:Enterprise Search"]},{"title":"Don't use `CLUSTER_COORDINATION` to deserialize incoming cluster states","body":"Today we deserialize a cluster state received from the master on the `CLUSTER_COORDINATION` thread:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/df8202206a8e9c33c737ce3a0bfa3d8cb5f40298\/server\/src\/main\/java\/org\/elasticsearch\/cluster\/coordination\/PublicationTransportHandler.java#L106-L113\r\n\r\nI suspect there's no good reason to do this work here, we're not using `Coordinator#mutex` until we call `acceptState`, and for humongous cluster states this work might block other cluster coordination activity for multiple minutes. In particular, if we just joined a cluster then we need to update the term in the `FollowersChecker` which happens on the slow path through `handleFollowerCheck`, and that has to happen within 30s to avoid the node being dropped from the cluster again.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Cluster Coordination","Team:Distributed"]},{"title":"Clean up builtin privileges translator","body":"Removing an unused parameter `restrictResponse` from the translator interface. This will break CI until we merge the corresponding PR.","comments":["@elasticmachine update branch","Pinging @elastic\/es-security (Team:Security)"],"labels":[">non-issue",":Security\/Security","Team:Security","v8.14.0"]},{"title":"Text fields are stored by default in TSDB indices","body":"Synthetic source requires text fields to be stored or have keyword\r\nsub-field that supports synthetic source. If there are no keyword fields\r\n users currently have to explicitly set 'store' to 'true' or get a\r\nvalidation exception. This is not the best experience. It is quite\r\nlikely that setting `store` to `true` is  the correct thing to do but\r\nusers still get an error and need to investigate it. With this change if\r\n `store` setting is not specified in such context it  will be set to\r\n `true` by default. Setting it explicitly to `false` results in the\r\n exception.\r\n\r\nCloses #97039\r\n","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106338.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)","Hi @lkts, I've created a changelog YAML for you.","Pinging @elastic\/es-search (Team:Search)","I have pushed change to use option 2 from the discussion above. \r\n\r\nOne issue i discovered is that it is not possible to implement validation using `Parameter#addValidator` API. The problem is that we need to provide full name of the field in the error message e.g. `k8s.pod.agent.name` (as we currently do). It is not possible to get this information in constructor of `TextFieldMapper.Builder`, this information is only exposed during `build()` via `MapperBuilderContext#buildFullName`. We can only report field name as `name` in this case which is obviously confusing. Additionally, performing validation in constructor changes the error returned to user from `illegal_argument_exception` to `illegal_argument_exception` nested inside `mapper_parsing_exception`. I don't think this is a problem by itself but it would mean that errors are different for TSDB indices and non TSDB indices and that seems undesirable.  \r\n\r\nAs a result i removed validation logic and i am relying on existing code for that.","CI uncovered another interesting problem. `store` parameter was not serialized here https:\/\/github.com\/elastic\/elasticsearch\/blob\/2b67444a465ca50a6854c36025fd7f23aac5a117\/server\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/TextFieldMapper.java#L1423 because `includeDefaults` seems to be always `false`. During downsampling new index is created which is _not a TSDB index_ and since `store` value was not serialized it defaults to `false`. We successfully transfer synthetic source setting from original index and as a result we now have synthetic source and text field that is not stored.\r\n\r\nI have fixed serialization but i honestly don't know if it is a proper solution. I don't really like the state of \"defaults can be interpreted differently in different contexts\"."],"labels":[">enhancement",":Search\/Mapping","Team:Search",":StorageEngine\/TSDB","Team:StorageEngine","v8.14.0"]},{"title":"Reindex API reports no errors when it fails against a cluster with skip_unavailable=true","body":"### Elasticsearch Version\n\n8.14 SNAPSHOT\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMac\n\n### Problem Description\n\nIn the PR to change the default value of skip_unavailable from `false` to `true` (https:\/\/github.com\/elastic\/elasticsearch\/pull\/105792), I found that the `reindex` API is affected by the `skip_unavailable` setting when doing a remote reindexing operation. \r\n\r\nWhen  `skip_unavailable`=`false` and a cross-cluster reindex targets an index that does not exist, then `IndexNotFoundException` is returned with an HTTP status error code. (expected behavior)\r\n\r\nHowever, when `skip_unavailable`=`true` it does not return an error status code, and worse it does not report any error in the response object:\r\n\r\n```json\r\n{\r\n    \"took\": 36,\r\n    \"timed_out\": false,\r\n    \"total\": 0,\r\n    \"updated\": 0,\r\n    \"created\": 0,\r\n    \"deleted\": 0,\r\n    \"batches\": 0,\r\n    \"version_conflicts\": 0,\r\n    \"noops\": 0,\r\n    \"retries\": {\r\n        \"bulk\": 0,\r\n        \"search\": 0\r\n    },\r\n    \"throttled_millis\": 0,\r\n    \"requests_per_second\": -1,\r\n    \"throttled_until_millis\": 0,\r\n    \"failures\": []\r\n}\r\n```\r\n\r\nMy guess as to what is happening is that `reindex` is doing a cross-cluster search action and when skip_unavailable=true, search will not return an error code (such as 404). Instead it returns a 200 OK and you have to parse the `_cluster\/details` metadata section of the response to see that the remote cluster is marked as \"skipped\" and parse the failures section to determine why.\r\n\r\nShould logic to handle skip_unavailable=true be added to reindex to parse errors and report them in the `failures` section of the response, or throw an error (such as `IndexNotFoundException` for this case) as it does when skip_unavailable=false? Should the behavior of reindex depend on the `skip_unavailable` cluster setting?\r\n\n\n### Steps to Reproduce\n\nRun CrossClusterReindexIT with skip_unavailable=true. Change this code to do so: https:\/\/github.com\/elastic\/elasticsearch\/pull\/105792\/files#diff-4c824afa4dd48c2429ae2c9711019301367429a8159a1d40cda8a22f6ef8032dR44\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Reindex","Team:Distributed"]},{"title":"Fetch meta fields in FetchFieldsPhase using ValueFetcher","body":"Here we extract the logic to populate metadata fields such as `_ignored`, `_routing`, `_size` and the deprecated `_type` into `FetchFieldsPhase so that we can use the `ValueFetcher` interface to retrieve field values. This allows us to fetch values no matter if the Mapper uses stored or doc values.\r\n\r\nThis is a refactoring required for #101373 ","comments":["Pinging @elastic\/es-search (Team:Search)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":[":Search\/Search",">refactoring","Team:Search","Team:StorageEngine",":StorageEngine\/Logs","v8.14.0"]},{"title":"[Transform] Implement robustness test that constantly creates\/starts\/stops\/deletes a continuous transform","body":"DRAFT","comments":[],"labels":["WIP","v8.14.0"]},{"title":"Reset root logger to info after any test","body":"Some tests might be playing with a root level and might not correctly reset it back to the original level.\r\nthis commit adds an afterclass rule to always reset this logger\r\n","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">non-issue",">test",":Core\/Infra\/Logging","Team:Core\/Infra","v8.14.0"]},{"title":"update lease sync actions not circuit breakers ","body":"RetentionLeaseSyncAction and RetentionLeaseBackgroundSyncAction  need for circuit-breaker and indexing pressure checks.\r\nwe should set forceExecutionOnPrimary=true when calling the superclass constructor.\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/105926","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/CCR","Team:Distributed","external-contributor","v8.14.0"]},{"title":"Log skipped elections due to shutdown marker","body":"Relates ES-6576","comments":["I have this so far. Test pending. Let me know if you'd rather do this differently."],"labels":[">non-issue",":Distributed\/Cluster Coordination","v8.14.0"]},{"title":"Remove multi-value restriction from time series dimension fields.","body":"### Elasticsearch Version\n\n8.7.0 and above\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\n-\n\n### Problem Description\n\nAs identified in #106203 [documentation](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/keyword.html) states `Field values cannot be an array or multi-value.`. We should consider removing these restrictions.\n\n### Steps to Reproduce\n\n-\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)","Related to #99044 too.","@lkts are you comfortable opening a docs PR? Sounds like a small change? :)","@leemthompo This also involves a code change, so this one falls on the storage engine team."],"labels":[">enhancement",":StorageEngine\/TSDB",">tech debt","Team:StorageEngine"]},{"title":"Allow to specify a different barrier version for historical features in tests","body":"Allow to be \"more lenient\" in tests: when we add feature specs with historical features, we check if we are trying to add  historical features with versions after the version on which features have been added (so a feature should have been used). \r\nHowever, it happened that we needed to retrofit some tests with features introduced in 8.12 and 8.13. \r\nTo avoid 'bumping' the latest version for everything, this small change allows to to that just for tests.\r\n\r\nRelated to https:\/\/github.com\/elastic\/elasticsearch\/pull\/106293","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Do we need to specify a separate version for tests? It'll be easier just to bump the minimum version globally... (this is only in existance for the lifetime of 8.x)","I thought it would be nicer to limit the scope and be more strict in production code, but no hard opinion here. I'm fine wither way :) \r\nIf we think bumping the minimum version is OK, we can ditch this PR and just do that.","As its a simple sanity check, I think its simple enough to just bump the limit, but I'm happy to take +1\/-1s on this","> I thought it would be nicer to limit the scope and be more strict in production code\r\n\r\n@ldematte Not sure I understand the intention right, are you trying to prevent devs from defining historical features in prod on recent versions because these should have been defined as \"real\" feature?","> are you trying to prevent devs from defining historical features in prod on recent versions because these should have been defined as \"real\" feature?\r\n\r\nExactly. Then again, bumping it to 8.13 should be OK (we forbid new 8.14 historical features, so who adds a feature will know they have to do it with \"real\" ones).","> As its a simple sanity check, I think its simple enough to just bump the limit, but I'm happy to take +1\/-1s on this\r\n\r\nMe too. @mosche, can you cast your vote? We need a tie breaker :)","> so who adds a feature will know they have to do it with \"real\" ones).\r\n\r\nUnfortunately that's not the case at the moment, as you can see in #106323 people are still doing version checks for 8.14.\r\nIMHO if we want to keep this check, we also have to forbid skipping \/ requiring version ranges until or starting from 8.14. Otherwise people will simply continue doing that rather than adding a feature. There's already ~ 30 skip sections using `- 8.13.99` :\/\r\n\r\nIMHO we have to raise the barrier to even 8.14 for now and at the same time make sure we prevent the above from happening for 8.15 as well. Or how likely is it that we can get respective teams to add features for those cases?\r\n\r\n","> we also have to forbid skipping \/ requiring version ranges until or starting from 8.14.\r\n\r\nYes, now that we have features and we announced them (a couple of days ago), we can go and change the VersionRange parsing code to reject anything past 8.14. (8.13 would be better, but maybe it's too late).\r\n\r\n> IMHO we have to raise the barrier to even 8.14 for now\r\n\r\nDo we? Can we have 8.13 and change all the 8.14 checks to features (real features), or is it already too late?","I think it depends if there are any actual code changes with the version range check, or if it just checking for new functionality. The more correct way of doing it is, I think, to move it to a historical feature, but I think either will do (if 8.14 isn't released yet)","> if there are any actual code changes with the version range check, or if it just checking for new functionality\r\n\r\nYes, that's the key: we were just talking about this with @mosche. If there is a code change and a version check in production code, this should definitely be turned into a feature. If it is just for the tests, to check the new functionality, we both agree it would not be good to \"pollute\" cluster state with that. It's a possibility, but do we really want it?\r\n\r\nA second option is to introduce them as test-only historical features. But this would mean we would have to support Version-based, \"historical\" features forever in test code. And these tests would be tricky to use in serverless (at the very least, we have to figure out how to treat these features in serverless).\r\n\r\nA third option is something completely new, that will allow us to introduce non-historical test-only features. @mosche has put an item on the team sync agenda to discuss this.\r\n\r\nRelated: https:\/\/github.com\/elastic\/elasticsearch\/pull\/106332 (\"real\" features introduced for check in tests -- i.e. following the first option here)."],"labels":[">test",":Core\/Infra\/Core","Team:Core\/Infra","v8.14.0"]},{"title":"[ES|QL] Add mv_find","body":"Add mv_find, this is not ready for review or merge yet.","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106289.docs-preview.app.elstc.co\/diff)","A subtask of #105322."],"labels":[">enhancement",":Analytics\/ES|QL","v8.14.0"]},{"title":"Profile id for get and query api with decorator","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"AffixUpdateConsumer triggers invalid updates with default value if using fallback prefix","body":"`ScopedSettingsTests.testAffixUpdateConsumerWithAlias` demonstrates how the update consumer is triggered with an unexpected update (using the default value) on an `AffixSetting` with `prefix` and `fallbackPrefix`.","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Just to check my understading here: if we have a setting with \"prefix.key\" and \"fallback.key\" and default value \"default\", when I update the setting using \"prefix.key\" and \"value\", the update consumer is invoked with \"value\", but if I update the setting using \"prefix.key\" and \"value2\", the update consumer is invoked with \"default\" instead of \"value2\"? Or with both \"default\" and \"value2\"?","> but if I update the setting using \"prefix.key\" and \"value2\", the update consumer is invoked with \"default\" instead of \"value2\"? Or with both \"default\" and \"value2\"?\r\n\r\nIn this case when using the fallback key with both, there's an additional undesired invocation with the default value."],"labels":[">bug",":Core\/Infra\/Settings","Team:Core\/Infra","low-risk"]},{"title":"[8.13] Fix a downsample persistent task assignment bug (#106247)","body":"Backports the following commits to 8.13:\n - Fix a downsample persistent task assignment bug (#106247)","comments":["@elasticmachine run elasticsearch-ci\/bwc-snapshots"],"labels":[">bug","backport","auto-merge",":StorageEngine\/Downsampling","Team:StorageEngine","v8.13.1"]},{"title":"Rename `text_expansion` & remove `weighted_tokens`","body":"### Description\n\n`text_expansion` is GA'd and requires support and backwards compatibility. But, we should rename it to `sparse_vector` as that is what it is. \r\n\r\nAdditionally, we should remove `weighted_tokens`, it is an unnecessary separate query. Instead `sparse_vector` should either:\r\n\r\n - Allow a model & model text be set\r\n - Allow a sparse vector (aka, the weighted tokens, but we shouldn't call them tokens, they are vector dimensions).\r\n","comments":["Pinging @elastic\/ml-core (Team:ML)","Pinging @elastic\/es-search (Team:Search)","I assume we want to create a _new_ `sparse_vector` query that's essentially a copy of `text_expansion` and then deprecate `text_expansion` in favor of `sparse_vector`? ","@kderusso correct."],"labels":[">enhancement",":ml","Team:Search","Team:ML",":Search\/Vectors"]},{"title":"Profile id for query api keys 2","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"Add `_name` support for top level `knn` clauses","body":"### Description\n\nKnn as a query already, by definition as a query, supports `_name` and thus can show its contribution to scoring and matching. \r\n\r\nHowever, the top-level kNN object does not support `_name`. We should add support for it. \r\n\r\nThe changes required belong here: `org.elasticsearch.search.vectors.KnnSearchBuilder` and when the shard level queries are created, their `_name` needs to be provided and serialized.\r\n\r\nAdditionally, I expect us to have to add `_name` to the KnnScoreDocQueryBuilders as well.","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">enhancement","Team:Search",":Search\/Vectors"]},{"title":"Fix for from parameter when using sub_searches and rank","body":"It seems that pagination does not work as expected for `rrf` searches, so in this PR we update the `RRFRankCoordinatorContext` to simply skip the first `from` results from the combined top ranked docs prior to generating the final list to return. This has the following implications: \r\n\r\n* We must make sure that `window_size >= (from + size)` to make sure that we have enough docs to rank and do the necessary pagination. (maybe we should also enforce an upper limit on `window_size` ? \ud83e\udd14 )\r\n* All pagination will be handled by the ranker on the coordinator node prior to exiting the `reducedQueryPhase` . Once we're done with `RRFRankCoordinatorContext#rank` we reset `from` to be 0 and proceed as usual. \r\n\r\nCloses https:\/\/github.com\/elastic\/elasticsearch\/issues\/99011","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106253.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-search (Team:Search)","Hi @pmpailis, I've created a changelog YAML for you.","> When we fetch data, we fetch from + size, it seems to me we should be fetching from + max(size + window_size). This is what we do with other things that use window_size (e.g. rescore).\r\n\r\nFWICT when reaching out to shards, we currently fetch `from + window_size` docs from each one, as we override `size` in `RankSearchContext`, and use `from` from the global `SearchContext`. IIUC and given the current restriction of `window_size >= size`, I think that we should be in line with your suggestion.","@pmpailis This looks great so far! Thanks for fixing this."],"labels":[">bug",":Search\/Ranking","Team:Search","v8.14.0"]},{"title":"Draft: Add event.ingested min\/max to cluster state for searchable snapshots","body":"Add event.ingested min\/max to cluster state for searchable snapshots.\r\n\r\nExploratory coding. As of 13-Mar-2024, this is basically a refactoring to add in support for tracking `event.ingested` min\/max in cluster state like we do for `@timestamp`. No new tests have been added on testing `event.ingested` in cluster state. Before I do that I'd like feedback on whether this is the right approach structurally.","comments":[],"labels":["v8.14.0"]},{"title":"Ability to bypass traffic filters for watcher http input","body":"### Description\n\nWatcher currently allow to bypass traffic filters in Elasticsearch Services for webhook action only by settings [xpack.notification.webhook.additional_token_enabled](https:\/\/www.elastic.co\/guide\/en\/cloud\/current\/ec-add-user-settings.html#ec-es-elasticsearch-settings)\r\n\r\nThere is no such possibility for watcher http input so no way to query a remote elasticsearch cluster with http input when using traffic filters\r\n\r\nThe workaround solution is to [set up cross-cluster search](https:\/\/www.elastic.co\/guide\/en\/cloud\/current\/ec-enable-ccs.html) which is [compatible with traffic filtering](https:\/\/www.elastic.co\/guide\/en\/cloud\/current\/ec-configure-as-remote-clusters.html#ec-ccs-ccr-traffic-filtering)","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/Watcher","Team:Data Management"]},{"title":"[Transform] Auto retry Transform start","body":"Currently, Transforms can fail to start due to failure to load the Config from its internal index.  This usually happens when a Transform is created and immediately started by a system.  The error looks like:\r\n\r\n```\r\nFailed to load transform configuration for transform [id]\r\n```\r\n\r\nNow, we will automatically retry the startup logic until the Config is ready.\r\n\r\nSome notes:\r\n- We cannot determine if a transform is unattended or not, so at this stage we will assume all transforms are unattended.\r\n- The persistent task running the transform will move into the `STARTED` state.  Users can stop the persistent task and retry logic using the Transform's Stop API.\r\n- Transforms can still only be started once.  A subsequent call to start the Transform, even while it is retrying, will still fail.\r\n- While retrying, the Transform will report `Yellow` health in the API and `degraded` in Kibana.  The health message will include that the transform is automatically retrying and what error it had encountered.\r\n- The Transform's audit log will include an info statement that a retry was scheduled and include what error it had encountered.\r\n- The first retry will happen before 1 minute has passed, and then at a random time up until two minutes after that for ~292 years. The random retry interval should help stagger the scheduled actions to keep the system healthy.\r\n\r\n----------------------------\r\n\r\nTesting:\r\n\r\nCalling the stats API\r\n```\r\n\"health\": {\r\n  \"status\": \"yellow\",\r\n  \"issues\": [\r\n    {\r\n      \"type\": \"transform_starting\",\r\n      \"issue\": \"Transform task is automatically retrying its startup process\",\r\n      \"details\": \"Retrying Transform action due to error: java.lang.IllegalStateException: Failed to load transform configuration for transform [last-log-from-clientip].\",\r\n      \"count\": 1,\r\n      \"first_occurrence\": 1710250715674\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nKibana:\r\n![retryunattended](https:\/\/github.com\/elastic\/elasticsearch\/assets\/1223797\/14eb3007-bffa-4ec9-a4b9-5145e0d2862b)\r\n","comments":["Hi @prwhelan, I've created a changelog YAML for you.","Pinging @elastic\/ml-core (Team:ML)"],"labels":[">bug",":ml\/Transform","Team:ML","v8.14.0"]},{"title":"Port krb5kdc to test container and rework hdfs handling","body":"- [x] port krb5kdc to testcontainer \r\n- [x] port hdfs to plain junit test rule base\r\n- [x] support hdfs2 and hdfs3 as testfixtures\r\n- [x] update tests in :x-pack:plugin:snapshot-repo-test-kit:qa:hdfs \r\n- [x] update javaRestTest in :plugins:repository-hdfs\r\n- [x] update yamlRestTest in :plugins:repository-hdfs\r\n- [x] support multi hdfs versions in tests in :plugins:repository-hdfs\r\n- [x] port hdfs tests in :x-pack:plugins:searchable-snapshots:qa:hdfs\r\n- [x] port :qa:kerberos-tests \r\n- [x] publish kerberos testcontainer to our internal docker registry ","comments":["Pinging @elastic\/es-delivery (Team:Delivery)","@mark-vieira I think I've addressed all your review comments."],"labels":[">non-issue",":Delivery\/Build","Team:Delivery","v7.17.19","v8.13.1","v8.14.0"]},{"title":"[CI] XPackRestIT test {p0=ml\/frequent_item_sets_agg\/Test frequent item sets unsupported types} failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/jfp6o2gzvelrq\/tests\/:x-pack:plugin:yamlRestTest\/org.elasticsearch.xpack.test.rest.XPackRestIT\/test%20%7Bp0=ml%2Ffrequent_item_sets_agg%2FTest%20frequent%20item%20sets%20unsupported%20types%7D\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':x-pack:plugin:yamlRestTest' --tests \"org.elasticsearch.xpack.test.rest.XPackRestIT.test {p0=ml\/frequent_item_sets_agg\/Test frequent item sets unsupported types}\" -Dtests.seed=53970100C94E212F -Dtests.locale=zh-HK -Dtests.timezone=America\/North_Dakota\/New_Salem -Druntime.java=21\r\n```\r\n\r\n**Applicable branches:**\r\n8.13\r\n\r\n**Reproduces locally?:**\r\nNo\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.test.rest.XPackRestIT#test {p0=ml\/frequent_item_sets_agg\/Test frequent item sets unsupported types}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Bp0%3Dml\/frequent_item_sets_agg\/Test%20frequent%20item%20sets%20unsupported%20types%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.test.rest.XPackRestIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.AssertionError: Failure at [ml\/frequent_item_sets_agg:435]: expected [4xx|5xx] status code but api [search] returned [200 OK] [{\"took\":3,\"timed_out\":false,\"_shards\":{\"total\":2,\"successful\":1,\"skipped\":0,\"failed\":1,\"failures\":[{\"shard\":0,\"index\":\"store\",\"node\":\"aAKM5ZwrS4Kh2L0mV16u6g\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Field [geo_point] of type [geo_point] is not supported for aggregation [frequent_item_sets]\"}}]},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"fi\":{\"buckets\":[]}}}]\r\n\r\n  at __randomizedtesting.SeedInfo.seed([53970100C94E212F:DBC33EDA67B24CD7]:0)\r\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:561)\r\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\r\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n  at java.lang.reflect.Method.invoke(Method.java:580)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:1583)\r\n\r\n  Caused by: java.lang.AssertionError: expected [4xx|5xx] status code but api [search] returned [200 OK] [{\"took\":3,\"timed_out\":false,\"_shards\":{\"total\":2,\"successful\":1,\"skipped\":0,\"failed\":1,\"failures\":[{\"shard\":0,\"index\":\"store\",\"node\":\"aAKM5ZwrS4Kh2L0mV16u6g\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"Field [geo_point] of type [geo_point] is not supported for aggregation [frequent_item_sets]\"}}]},\"hits\":{\"total\":{\"value\":0,\"relation\":\"eq\"},\"max_score\":null,\"hits\":[]},\"aggregations\":{\"fi\":{\"buckets\":[]}}}]\r\n\r\n    at org.junit.Assert.fail(Assert.java:89)\r\n    at org.elasticsearch.test.rest.yaml.section.DoSection.failIfHasCatch(DoSection.java:401)\r\n    at org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:361)\r\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:541)\r\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\r\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n    at java.lang.reflect.Method.invoke(Method.java:580)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n    at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\r\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n    at java.lang.Thread.run(Thread.java:1583)\r\n\r\n```","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":["blocker",">test-failure",":ml","Team:ML"]},{"title":"test pr","body":"Just to run CI","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106214.docs-preview.app.elstc.co\/diff)"],"labels":["v8.14.0"]},{"title":"Add documentation to thread pool and executor code","body":"Some odds and ends that I think would be helpful after going through the code.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">non-issue",":Distributed\/Network","Team:Distributed","v8.14.0"]},{"title":"[ML] ELSER crashes in local serverless setup","body":"### Description\r\n\r\nWhen interacting with ELSER in serverless locally it crashes when attempting to perform inference.\r\n\r\nSteps to reproduce\r\n- Ensure docker is setup and running\r\n\r\n1. Checkout kibana and bootstrap it\r\n2. Start elasticsearch serverless locally: `yarn es serverless --projectType=security --ssl`\r\n3. Start kibana locally `yarn start --serverless=security --ssl`\r\n4. Download elser\r\n5. Deploy elser via the inference API\r\n\r\n```\r\nPUT _inference\/sparse_embedding\/elser\r\n{\r\n  \"service\": \"elser\",\r\n  \"service_settings\": {\r\n    \"model_id\": \".elser_model_2\",\r\n    \"num_allocations\": 1,\r\n    \"num_threads\": 1\r\n  },\r\n  \"task_settings\": {}\r\n}\r\n```\r\n\r\n7. Add an ingest processor\r\n\r\n```\r\nPUT _ingest\/pipeline\/elser\r\n{\r\n  \"processors\": [\r\n    {\r\n      \"inference\": {\r\n        \"model_id\": \"elser\",\r\n        \"input_output\": [\r\n            {\r\n                \"input_field\": \"content\",\r\n                \"output_field\": \"text_embedding\"\r\n            }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"set\": {\r\n        \"field\": \"timestamp\",\r\n        \"value\": \"{{_ingest.timestamp}}\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n8. Attempt to perform inference\r\n\r\n```\r\nPOST _ingest\/pipeline\/elser\/_simulate\r\n{\r\n  \"docs\": [\r\n    {\r\n      \"_source\": {\r\n             \"content\": \"hello\" \r\n      }\r\n    }]\r\n}\r\n```\r\n\r\n9. Retrieve the stats from the trained models api to observe that the process has crashed\r\n\r\n```\r\n            \"routing_state\": {\r\n              \"routing_state\": \"failed\",\r\n              \"reason\": \"\"\"inference process crashed due to reason [[my-elser-model] pytorch_inference\/659 process stopped unexpectedly: Fatal error: 'si_signo 11, si_code: 1, si_errno: 0, address: 0xffff83b20140, library: \/lib\/aarch64-linux-gnu\/libc.so.6, base: 0xffff83a13000, normalized address: 0x10d140', version: 8.14.0-SNAPSHOT (build 38a5b0ec077958)\r\n]\"\"\"\r\n            },\r\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","I just confirmed that these steps _don't_ cause a crash in the ESS CFT region running 8.14.0-SNAPSHOT. This is interesting, because the code should be very similar.\r\n\r\nServerless is running on c6i instances in AWS. CFT is running on n2 instances in GCP. So the problem might be down to serverless or might be down to the exact type of hardware.","Logs show the crash happened on ARM:\r\n\r\n```\r\n\"inference process crashed due to reason [[.elser_model_2] pytorch_inference\/644 process stopped unexpectedly: Fatal error: 'si_signo 11, si_code: 1, si_errno: 0, address: 0xffff7a188140, library: \/lib\/aarch64-linux-gnu\/libc.so.6, base: 0xffff7a07b000, normalized address: 0x10d140', version: 8.14.0-SNAPSHOT (build 38a5b0ec077958)\\n]\"\r\n```\r\n\r\nML nodes on serverless are supposed to be on Intel hardware. I just tried reproducing this in a serverless project and the steps worked fine. However, as expected, my ML node was on Intel.\r\n\r\nSo it may be that the bug here is really \"ELSER crashes on ARM\".\r\n\r\nAnd then the next question would be how did we end up with an ML node on ARM in serverless?","Just reading through the report more closely, this wasn't even using real serverless. It was using simulated serverless running locally on a Mac. That explains why it was on ARM.\r\n\r\nBut also, running locally on a Mac, it's running Docker images in a Linux VM. We don't know how much memory that Linux VM had. It may be that it was trying to do too much in too little memory and because of the vagaries of Docker on a Mac that ended up as a SEGV rather than an out-of-memory error.\r\n\r\nGiven the circumstances I don't think this bug is anywhere near as serious as the title makes it sound.","I tried these steps on an `m6g.2xlarge` AWS instance, and they ran successfully without the process crashing.\r\n\r\n(Originally, I tried on an `m6g.large` instance with 8GB RAM, and there `pytorch_inference` was killed by the OOM killer. But that was running Elasticsearch as a single node cluster, so 50% memory to the JVM heap, and Kibana also running on the same machine. So that problem really was due to lack of memory. On the 32GB `m6g.2xlarge` inference worked fine.)\r\n\r\nTherefore, this problem really does seem to be confined to running in a Docker container in a Linux VM on ARM macOS. It's not great that this crash happens, and it's still a bug that running in Docker on a Mac doesn't work, but at least it's not going to affect customers in production."],"labels":[">bug",":ml","Team:ML"]},{"title":"[8.12] First version of the LTR guide. (#105956)","body":"Backports the following commits to 8.12:\n - First version of the LTR guide. (#105956)","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_106194.docs-preview.app.elstc.co\/diff)"],"labels":[">docs",":Search\/Ranking","backport","Team:Docs","Team:Search","v8.12.3"]},{"title":"ESQL: Exceptions for warnings when processing expressions","body":"### Description\n\nIf we hit an exception while processing an expression and it's registered as a warning exception we'll just insert null. If we've already began a position we'll stop that position and create a new one. That's going to break the `Page` we try to build because it won't have the same number of positions.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL"]},{"title":"Cannot add parameter \"typed_keys\" to Search Application Search aggregation","body":"### Elasticsearch Version\n\n8.12.2\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nelasticsearch cloud\n\n### Problem Description\n\nWhen defining a SearchApplication template containing an aggregation, there is currently no way to add the `typed_keys` parameter defined for [Aggregations](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-aggregations.html#return-agg-type), or at least I didn't find a way to add it, nor did I manage to find documentation about it.\r\n\r\nSupport for `typed_keys`, in particular `typed_keys=true` is crucial for client libraries for strongly typed languages (e.g. Java, .Net, Go) as they allow identifying the concrete aggregation result class that should be instantiated.\n\n### Steps to Reproduce\n\nIn Kibana:\r\n\r\n```\r\nPUT test-typed-keys-index\r\n```\r\n\r\nExample of the parameter used in a simple Aggregation\r\n```\r\nGET \/test-typed-keys-index\/_search?typed_keys\r\n{\r\n  \"aggs\": {\r\n    \"my-agg-name\": {\r\n      \"histogram\": {\r\n        \"field\": \"date\",\r\n        \"interval\": 1000\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nWith the result being\r\n```\r\n{\r\n...\r\n  \"aggregations\": {\r\n    \"histogram#my-agg-name\": {\r\n      \"buckets\": []\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nCreating a search application template using the same aggregation:\r\n```\r\nPUT _application\/search_application\/my_search_application\r\n{\r\n  \"indices\": [\"test-typed-keys-index\"],\r\n  \"template\": {\r\n    \"script\": {\r\n      \"source\": \"\"\"\r\n        {\r\n          \"aggs\": {\r\n            \"my-agg-name\": {\r\n              \"histogram\": {\r\n                \"field\": \"date\",\r\n                \"interval\": 1000\r\n              }\r\n            }\r\n          }\r\n        }\r\n      \"\"\",\r\n      \"params\": {\r\n        \"text_fields\": [\r\n          {\"name\": \"date\", \"boost\": 10}\r\n        ],\r\n        \"explain\": false,\r\n        \"from\": 0,\r\n        \"size\": 10\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nSearch Application Search: \r\n\r\n```\r\nPOST _application\/search_application\/my-app\/_search?typed_keys\r\n{\r\n  \"params\": {\r\n    \"query_string\": \"*\"\r\n  }\r\n}\r\n```\r\n\r\nWhich fails with \r\n\r\n```\r\n\"type\": \"illegal_argument_exception\",\r\n\"reason\": \"request [\/_application\/search_application\/my-app\/_search] contains unrecognized parameter: [typed_keys]\"\r\n```\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/search-relevance (Team:Search - Relevance)","Pinging @elastic\/ent-search-eng (Team:SearchOrg)"],"labels":[">bug",":SearchOrg\/Relevance"]},{"title":"[ML] Inference API chunking large documents","body":"### Description\r\n\r\nLarge documents need to be chunked otherwise tokens exceeding the model's limit won't be used.\r\n\r\nMVP\r\n- Use a sliding window approach\r\n- Chunk into 200 words\r\n- Try splitting on whitespace or newline if possible\r\n- Maybe fallback to only doing it based on character length (don't have to search for whitespace\/newlines, and also avoids problems with languages that don't really use whitespace)\r\n\r\n","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML","Feature:GenAI"]},{"title":"[ML] Inference API splitting large bulk requests","body":"### Description\n\nThe inference API supports client side batching by leveraging the `input` array field. External services implement different limits for batched requests. Cohere limits the text to [96 items](https:\/\/docs.cohere.com\/reference\/embed). We need to implement functionality to split large requests into smaller ones and reassemble before returning the response to the client.","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML","Feature:GenAI"]},{"title":"Semantic text query","body":"Add the `semantic_query` to the Query DSL, which is used to query `semantic_text` fields:\r\n\r\n```\r\nGET <index>\/_search\r\n{\r\n    \"query\": {\r\n        \"semantic_query\": {\r\n            \"inference_field\": {\r\n                \"query\": \"my query text\"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nNote that this PR limits scope to models that produce sparse embeddings (i.e. ELSER) and queries that target a single index. Dense embedding support will be added in a follow-up PR prior to 8.14 release. Multi-index support will be added when time allows, it's considered a \"nice to have\" for 8.14.","comments":["Pinging @elastic\/es-search (Team:Search)","@elasticmachine run elasticsearch-ci\/part-1","@elasticmachine run elasticsearch-ci\/part-4","@elasticmachine run elasticsearch-ci\/part-1","@elasticmachine run elasticsearch-ci\/part-4","@elasticmachine run elasticsearch-ci\/part-1","@elasticmachine run elasticsearch-ci\/part-2","@elasticmachine run elasticsearch-ci\/part-4"],"labels":[">non-issue",":Search\/Search","Team:Search",":Search\/Vectors","v8.14.0"]},{"title":"Profile id for get api keys take 2","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"[Meta] Data stream lifecycle global retention ","body":"### Description\r\n\r\nWith this issue we define the global retention for data stream lifecycle and we track the progress. \r\n\r\nWe would like to introduce a global retention configuration that would enable the project owners to introduce default and max retention for all data streams managed by data stream lifecycle.\r\n\r\nThe work will be completed in the following steps:\r\n\r\n- [x] Introduce the cluster state metadata that will store the global retention, this is effectively not adding any functionality since there is no way to add global retention. (https:\/\/github.com\/elastic\/elasticsearch\/pull\/106170).\r\n- [ ] Use the global retention in the data stream lifecycle, this is effectively not adding any functionality since there is no way to add global retention (Part 1: https:\/\/github.com\/elastic\/elasticsearch\/pull\/106268 & Part 2: https:\/\/github.com\/elastic\/elasticsearch\/pull\/106221).\r\n- [ ] Introduce `PUT`, `GET`, `DELETE` REST APIs to enable the user with the correct privileges to manage the global retention. With this PR the feature becomes available and it will include docs and tests (https:\/\/github.com\/elastic\/elasticsearch\/pull\/105682).\r\n- [ ] Add telemetry about the global retention \r\n\r\nWe propose the above sequence of steps to ensure that the functionality will not be exposed to the users until its complete. ","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/Data streams","Team:Data Management"]},{"title":"categorize_text takes a long time on small data sets with low cardinality","body":"I am running a `categorize_text` aggregation on a relatively small data set, and the response time is about 18s for 185k docs, and three aggregations, so each aggregation processes about 30k docs\/s. I briefly spoke to @droberts195 about this and we are not sure if this is expected. Each aggregation (after removing\/adding) seems to take up a third of the response time. \r\n\r\nIn this case, the aggregations run over `process.executable.text`, `user.name.text` and `host.os.name.text`, which are `match_only_text` fields. Some observations:\r\n\r\n- These are low cardinality fields - 1 or 2 values per field\r\n- The aggregation returns no results\r\n- Running the aggregation over the keyword siblings _does_ return results, and take up the same amount of time.\r\n\r\nMy questions:\r\n\r\n- is 30k docs per second per shard expected?\r\n- can we speed it up (significantly)?\r\n- is not returning any results expected?\r\n- is performance affected by the cardinality of the data set, or only the document count (documents with values)? what other factors come into play?\r\n- how do we determine whether results are statistically significant?\r\n\r\nContext here is that I'm trying to figure out if we can make the log rate analysis API realtime (e.g. instead of a double-digit seconds response). The goal here is to get statistically significant results within <=2.5s from the API, and I'm trying to figure out what options we have to get there, e.g. use a sampler agg and re-poll with a greater probability if the results are not significant, etc.\r\n","comments":["Pinging @elastic\/ml-core (Team:ML)","@edsavage please could you investigate this one and see if there's some silly duplicated processing happening or processing that's supposed to be one-off singleton initialisation but isn't.","cc @arisonl ","I've attempted to reproduce this locally (on my mac) in order to profile the aggregation (using `async-profiler` integrated with IntelliJ IDEA), and while I'm not seeing anywhere as near a long response time as 3s, the profiler does show up some areas that might be useful to investigate further.\r\n\r\nThe dataset I used was 472898 documents from one of the `filebeat-nginx` indices contained in the `filebeat` snapshot. I ran a query containing multiple `categorize_text` aggregations, on low cardinality fields: \r\n\r\n```\r\nGET \/filebeat-nginx-elasticco-2017.02.01\/_search?filter_path=aggregations\r\n{\r\n  \"aggs\": {\r\n    \"categories1\": {\r\n      \"categorize_text\": {\r\n        \"field\": \"nginx.access.url\"\r\n      }\r\n    },\r\n    \"categories2\": {\r\n      \"categorize_text\": {\r\n        \"field\": \"nginx.access.user_agent.os_name\"\r\n      }\r\n    },\r\n    \"categories3\": {\r\n      \"categorize_text\": {\r\n        \"field\": \"nginx.access.user_agent.name\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nViewing the `CPU Samples` profiler output:\r\n\r\n![image](https:\/\/github.com\/elastic\/elasticsearch\/assets\/32410745\/c38235b3-814e-41f0-b8c0-521a0b2b9b49)\r\n\r\nshows that the `incrementToken` stack is taking up ~ 66% of the time of its parent `computeCategory`\r\n\r\nsimilarly for the `Memory Allocations`:\r\n\r\n![image](https:\/\/github.com\/elastic\/elasticsearch\/assets\/32410745\/c761c8b3-2f8d-4247-93aa-bf4255b61b9d)\r\n\r\nWhile these observations haven't provided conclusive evidence of a smoking gun, they perhaps do indicate areas of the code worthy of closer inspection.\r\n","@edsavage do you have benchmarks, so I can have an understanding of how fast it is _supposed_ to be? "],"labels":[":ml","Team:ML"]},{"title":"[DOCS] Splits API reference page to reference and examples","body":"## Overview\r\n\r\n**DO NOT MERGE THIS PULL REQUEST! THIS IS ONLY FOR TESTING!**\r\n\r\nRelated issue: https:\/\/github.com\/elastic\/search-docs-team\/issues\/62\r\n\r\n### Preview\r\n\r\n**Mixed API reference page:**\r\n* [Cluster allocation explain - now published version](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/cluster-allocation-explain.html)\r\n\r\n**Split content:**\r\n* [Cluster allocation explain - API ref](https:\/\/elasticsearch_bk_106164.docs-preview.app.elstc.co\/guide\/en\/elasticsearch\/reference\/master\/cluster-allocation-explain.html)\r\n* [Cluster allocation explain examples - explanatory content](https:\/\/elasticsearch_bk_106164.docs-preview.app.elstc.co\/guide\/en\/elasticsearch\/reference\/master\/cluster-allocation-explain-examples.html)","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_106164.docs-preview.app.elstc.co\/diff)"],"labels":[">docs","Team:Docs","v8.14.0"]},{"title":"[CI] DocsClientYamlTestSuiteIT test {yaml=reference\/sql\/endpoints\/rest\/line_574} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/u3ln5vofvnbtw\/tests\/:docs:yamlRestTest\/org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\/test%20%7Byaml=reference%2Fsql%2Fendpoints%2Frest%2Fline_574%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':docs:yamlRestTest' --tests \"org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT.test {yaml=reference\/sql\/endpoints\/rest\/line_574}\" -Dtests.seed=461A50DBB2EAB7FF -Dtests.locale=ar-SD -Dtests.timezone=SystemV\/EST5 -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nYes\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT#test {yaml=reference\/sql\/endpoints\/rest\/line_574}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Byaml%3Dreference\/sql\/endpoints\/rest\/line_574%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.client.WarningFailureException: method [DELETE], host [http:\/\/127.0.0.1:39763], URI [*,-.ds-ilm-history-*,-.ds-.slm-history-*?expand_wildcards=open%2Cclosed%2Chidden], status line [HTTP\/1.1 200 OK]\nWarnings: [this request accesses system indices: [.async-search], but in a future major version, direct access to system indices will be prevented by default]\n{\"acknowledged\":true}\n\n  at __randomizedtesting.SeedInfo.seed([461A50DBB2EAB7FF:CE4E6F011C16DA07]:0)\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:347)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\n  at org.elasticsearch.test.rest.ESRestTestCase.wipeAllIndices(ESRestTestCase.java:1113)\n  at org.elasticsearch.test.rest.ESRestTestCase.wipeCluster(ESRestTestCase.java:853)\n  at org.elasticsearch.test.rest.ESRestTestCase.cleanUpCluster(ESRestTestCase.java:539)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","I've assigned this one to myself, it's caused by https:\/\/github.com\/elastic\/elasticsearch\/pull\/106140.","https:\/\/gradle-enterprise.elastic.co\/s\/yocwlc3kri3jg\/tests\/task\/:docs:yamlRestTest\/details\/org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\/test%20%7Byaml=reference%2Fsql%2Fendpoints%2Frest%2Fline_574%7D","I had unmuted these tests on https:\/\/github.com\/elastic\/elasticsearch\/pull\/106140, they were originally muted via https:\/\/github.com\/elastic\/elasticsearch\/pull\/75237. I have re-muted them via https:\/\/github.com\/elastic\/elasticsearch\/pull\/106196.\r\n\r\nSince https:\/\/github.com\/elastic\/elasticsearch\/pull\/94191 has been merged, the async search feature is being reset when we clean up the cluster between rest tests. However, I think the `.async-search` index is being re-created as a race condition because of the async nature of writing to that index (or something \ud83e\udd37).\r\n\r\nSo the flow in the cleanup code ends up being:\r\n1. The test runs, it will create an async-search\r\n2. The async-search feature is reset, the `.async-search` index will be deleted if it exists\r\n3. The async-search is actually created and the `.async-search` index is created\r\n4. We get to `wipeAllIndices(...)` and it complains that `.async-search` is a system index\r\n\r\n\/cc @williamrandolph since we talked about exactly this sort of possibility","The failures reproduce really easily, btw.\r\n\r\n```\r\njoegallo@simulacron:~\/Code\/elastic\/elasticsearch $ git reset --hard upstream\/main\r\nHEAD is now at 0dd6ce2df6c Delete DownsampleClusterDisruptionIT (#106225)\r\njoegallo@simulacron:~\/Code\/elastic\/elasticsearch $ git log --oneline -1\r\n0dd6ce2df6c (HEAD -> main, upstream\/main, upstream\/HEAD) Delete DownsampleClusterDisruptionIT (#106225)\r\njoegallo@simulacron:~\/Code\/elastic\/elasticsearch $ git revert --no-edit 8be3fb229d8c29d7a3931ffcb5e03e0bb0476c0f\r\n[main 27c07af19e4] Revert \"Mute failing snippet tests (#106196)\"\r\n Date: Tue Mar 12 09:52:24 2024 -0400\r\n 1 file changed, 6 deletions(-)\r\n```\r\n\r\nAnd then it's just a matter of:\r\n\r\n```\r\n.\/gradlew ':docs:yamlRestTest' --tests \"org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT.test {yaml=reference\/sql\/endpoints\/rest\/*}\"\r\n```\r\n\r\nThat doesn't fail 100% of the time, but it's pretty reliable (it failed 2 out of the 4 times I tried this on my laptop just now).","I've unassigned myself from this issue because this seems to be a legitimate issue and none of it's actually my area.","I suppose something like the following diff would square this away, but that seems contrary to the point of https:\/\/github.com\/elastic\/elasticsearch\/pull\/94191:\r\n\r\n```diff\r\ndiff --git a\/test\/framework\/src\/main\/java\/org\/elasticsearch\/test\/rest\/ESRestTestCase.java b\/test\/framework\/src\/main\/java\/org\/elasticsearch\/test\/rest\/ESRestTestCase.java\r\nindex 307daddd17c..24f6424e71e 100644\r\n--- a\/test\/framework\/src\/main\/java\/org\/elasticsearch\/test\/rest\/ESRestTestCase.java\r\n+++ b\/test\/framework\/src\/main\/java\/org\/elasticsearch\/test\/rest\/ESRestTestCase.java\r\n@@ -1109,7 +1109,7 @@ public abstract class ESRestTestCase extends ESTestCase {\r\n         try {\r\n             \/\/ remove all indices except some history indices which can pop up after deleting all data streams but shouldn't interfere\r\n             final List<String> indexPatterns = new ArrayList<>(\r\n-                List.of(\"*\", \"-.ds-ilm-history-*\", \"-.ds-.slm-history-*\", \"-.ds-.watcher-history-*\")\r\n+                List.of(\"*\", \"-.async-search\", \"-.ds-ilm-history-*\", \"-.ds-.slm-history-*\", \"-.ds-.watcher-history-*\")\r\n             );\r\n             if (preserveSecurityIndices) {\r\n                 indexPatterns.add(\"-.security-*\");\r\n```","Pinging @elastic\/es-search (Team:Search)","Given the similarity to #104013, I'm labeling this one as `low-risk` instead of `blocker` (that is, labeled the same as that issue already is).","I think this one is similar to this:\r\n```\r\nREPRODUCE WITH: .\/gradlew ':docs:yamlRestTest' --tests \"org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT.test {yaml=reference\/esql\/esql-rest\/line_297}\" -Dtests.seed=53ABCE485C630DC4 -Dtests.locale=hu -Dtests.timezone=Asia\/Irkutsk -Druntime.java=21\r\n```","Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Since this has to do with system indices and those being cleaned up, I am gonna tag core infra as well.\r\n\r\nI don't know what the focus would be for search, since `.async_search` is a system index, its management, etc. should be pretty transparent to us. \r\n\r\n@williamrandolph would we have to add some logic on waiting for tasks or something in the async search cleanup?","Also failing on 8.13 https:\/\/gradle-enterprise.elastic.co\/s\/35culdxu6ttss\/tests\/:docs:yamlRestTest\/org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\/test%20%7Byaml=reference%2Fesql%2Fesql-rest%2Fline_297%7D","It would be ideal if I could implement some cleanup logic by overriding `SystemIndexPlugin#cleanUpFeature` in `AsyncResultsIndexPlugin`. I'll take a look at doing that."],"labels":[":Core\/Infra\/Core",":Search\/Search",">test-failure","Team:Core\/Infra","Team:Search","low-risk"]},{"title":"ESQL: Add MATCHING_ROW and VALUE_AT","body":"This adds two functions: `MATCHING_ROW` and `VALUE_AT`. `MATCHING_ROW` takes pairs of values and the second one must always be a constant and matches the variable value to offset in the constant value. It looks like:\r\n```\r\n  FROM inventory\r\n| EVAL r=MATCHING_ROW(size, [\"XS\", \"S, \"M\", \"L\", \"XL\"])\r\n```\r\n\r\nThat'd generate these hypothetical results\r\n```\r\n     Cool-Shirt |  20.00 | XL | 4\r\nExpensive-Shirt | 120.00 | XL | 4\r\n     Cool-Shirt |  20.00 |  S | 1\r\n```\r\n\r\n`VALUE_AT` takes an index and an array of values and returns the value at that offset. So:\r\n```\r\n  FROM employees\r\n| EVAL languages_word = VALUE_AT(languages, [\"zero\", \"one\", \"two\", \"three\", \"four\"])\r\n| SORT emp_no\r\n| LIMIT 4\r\n| KEEP first_name, languages, languages_word\r\n```\r\n\r\nWould make:\r\n```\r\nGeorgi             |                 2 | two\r\nBezalel            |                 5 | null\r\nParto              |                 4 | four\r\nChirstian          |                 5 | null\r\n```\r\n\r\nYou can combine them together:\r\n```\r\n  FROM inventory\r\n| EVAL r=MATCHING_ROW(size, [\"XS\", \"S, \"M\", \"L\", \"XL\"])\r\n| EVAL avg_price=VALUE_AT(r, [null, 20.00, null, null, 70.00])\r\n| DROP r\r\n| WHERE price > avg_price\r\n```\r\n\r\nWhich would yield:\r\n```\r\nExpensive-Shirt | 120.00 | XL | 70.00\r\n```\r\n\r\nIf *that* looks familiar then you've been paying close attention! It's another join strategy, specifically one that makes sense when the data you are joining against is small. Which is precisely what should happen for the `INLINESTATS` command that we implemented in the grammar a long time ago but never implemented in the engine.\r\n","comments":["> 5\\. For this to work for `INLINESTATS` the `MATCHING_ROW` function needs to match _all_ values of all columns - so `MATCHING_ROW([1, 2], [1, 2, 3])` will returning `[0, 1]` - but this will get multiplicative when combining more than one field. How do we make sure not to make huge `Block`s?\r\n\r\nI think this isn't true. I think, at least for now, we're better off doing our standard stuff and only supporting single-valued fields for `MATCHING_ROW` and the `INLINESTATS` implementation can add an `MV_EXPAND` operation. Those are free for single-valued fields and they protect against the combinatorial explosion."],"labels":["v8.14.0"]},{"title":"Optimize for single data node or remote cluster in ESQL","body":"If a query request targets a single data node or a single remote cluster, the final reduction can happen on the target data node or remote cluster instead of the coordinator. This should reduce the data transfer and computation on the coordinator.\r\n\r\nRelates #99498","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":["Team:Analytics",":Analytics\/ES|QL"]},{"title":"Reduce BWC testing to include only latest patch releases","body":"This commit alters our backward compatibility testing logic to restrict testing to the latest patch release for any given minor version. Since our policy is not to do any kind of breaking changes in patches, most of this testing is superfluous. This change reduces the number of versions we need to test by over 70%.\r\n","comments":["I think this makes sense and removing the intermediate patches safes us time and money while only giving very little value. \r\nDo we have (anecdotal) historical data these tests between patches ever uncovered an actual BWC issue?\r\n \r\nIf we want to be 100% sure we do not break compatibility between patches we _could_ probably test compatibility between patch releases when releasing the actual patch. Meaning when releasing 8.10.4 we test it is 100% compatible with 8.10.3. ","Tests that rarely, if ever, provide any kind of useful signal should be run less often or never (depending on the cost\/risk of missing something they might have caught).\r\n\r\nI don't have the knowledge or data to say if they can be removed entirely, but it sounds like they could _at least_ be moved to being tested weekly, if not removed entirely.\r\n\r\nMoving to 1x per week would still cut the infrastructure cost of testing these versions by about 95%. But, again, they should just be removed if they aren't useful"],"labels":[">non-issue",":Delivery\/Build","discuss","v7.17.19","v8.13.1","v8.14.0","v8.12.3"]},{"title":"Consistency between _search and _msearch","body":"Closes https:\/\/github.com\/orgs\/elastic\/projects\/785\/views\/30?pane=issue&itemId=27287872\r\n\r\nAdds consistency on options parsing for _msearch and _search.\r\n\r\n`RestSearchAction#parseSearchRequest` is used from _msearch to parse options. This implies getting a new abstraction for holding request parameters for both a RestRequest and options included as headers in _msearch.\r\n\r\nRequestParams class holds request parameters for requests, independently of their origin. RestRequest now delegates on this class to hold params, and it can be reused from _msearch.\r\n\r\nI'd like to get a general overview on the changes. \r\n\r\nMy biggest concern is about test coverage for the existing features. There are more tests that we should add for ensuring the behaviour has not changed.\r\n","comments":[],"labels":[":Search\/Search","Team:Search","v8.14.0"]},{"title":"Rolling over a data stream alias causes a null pointer exception","body":"When a executing a rollover operation on an alias, if the alias is a data stream alias the operation will fail with a null pointer exception.\r\n\r\nAffected versions: Tested on 8.12.0\r\n\r\nReproduction:\r\n\r\n```\r\nPUT \/_index_template\/jimmy-ds-test\r\n{\r\n  \"index_patterns\": [\"jimmy-ds-*\"],\r\n  \"data_stream\": {},\r\n  \"template\": {\r\n    \"aliases\": {\r\n      \"ds-alias\": {\r\n        \"is_write_index\": true\r\n      }\r\n    }, \r\n    \"settings\": {\r\n      \"number_of_shards\": 1\r\n    },\r\n    \"mappings\": {\r\n      \"properties\": {\r\n        \"idNumber\": {\r\n          \"type\": \"long\"\r\n        },\r\n        \"@timestamp\": {\r\n          \"type\": \"date\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT \/jimmy-ds-test-1\/_create\/1\r\n{\r\n  \"@timestamp\": \"2024-03-08\",\r\n  \"idNumber\": 4\r\n}\r\n\r\nPOST \/ds-alias\/_rollover?error_trace\r\n```\r\n\r\nResults in the following exception message:\r\n```\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"null_pointer_exception\",\r\n        \"reason\": \"\"\"Cannot invoke \"org.elasticsearch.cluster.metadata.AliasMetadata.writeIndex()\" because \"aliasMetadata\" is null\"\"\",\r\n        \"stack_trace\": \"\"\"org.elasticsearch.ElasticsearchException$1: Cannot invoke \"org.elasticsearch.cluster.metadata.AliasMetadata.writeIndex()\" because \"aliasMetadata\" is null\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:673)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.ElasticsearchException.generateFailureXContent(ElasticsearchException.java:601)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.rest.RestResponse.build(RestResponse.java:180)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.rest.RestResponse.<init>(RestResponse.java:140)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.rest.RestResponse.<init>(RestResponse.java:111)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.rest.action.RestActionListener.onFailure(RestActionListener.java:55)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.rest.action.RestCancellableNodeClient$1.onFailure(RestCancellableNodeClient.java:96)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.tasks.TaskManager$1.onFailure(TaskManager.java:214)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:27)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:39)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:27)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:27)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations$RunBeforeActionListener.onFailure(ActionListenerImplementations.java:308)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.lambda$doStart$2(TransportMasterNodeAction.java:233)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations$DelegatingResponseActionListener.acceptException(ActionListenerImplementations.java:186)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListenerImplementations$DelegatingResponseActionListener.onFailure(ActionListenerImplementations.java:191)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction$RolloverTask.onFailure(TransportRolloverAction.java:251)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$ExecutionResult.notifyFailure(MasterService.java:975)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$ExecutionResult.onClusterStateUnchanged(MasterService.java:938)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.executeAndPublishBatch(MasterService.java:245)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$BatchingTaskQueue$Processor.lambda$run$2(MasterService.java:1626)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListener.run(ActionListener.java:386)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$BatchingTaskQueue$Processor.run(MasterService.java:1623)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$5.lambda$doRun$0(MasterService.java:1237)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListener.run(ActionListener.java:386)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$5.doRun(MasterService.java:1216)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\r\n\tat java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base\/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.elasticsearch.cluster.metadata.AliasMetadata.writeIndex()\" because \"aliasMetadata\" is null\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.MetadataRolloverService.rolloverAlias(MetadataRolloverService.java:202)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.MetadataRolloverService.rolloverClusterState(MetadataRolloverService.java:117)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction$RolloverExecutor.executeTask(TransportRolloverAction.java:332)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction$RolloverExecutor.execute(TransportRolloverAction.java:268)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.innerExecuteTasks(MasterService.java:1039)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:1004)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.executeAndPublishBatch(MasterService.java:232)\r\n\t... 11 more\r\n\"\"\"\r\n      }\r\n    ],\r\n    \"type\": \"null_pointer_exception\",\r\n    \"reason\": \"\"\"Cannot invoke \"org.elasticsearch.cluster.metadata.AliasMetadata.writeIndex()\" because \"aliasMetadata\" is null\"\"\",\r\n    \"stack_trace\": \"\"\"java.lang.NullPointerException: Cannot invoke \"org.elasticsearch.cluster.metadata.AliasMetadata.writeIndex()\" because \"aliasMetadata\" is null\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.MetadataRolloverService.rolloverAlias(MetadataRolloverService.java:202)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.MetadataRolloverService.rolloverClusterState(MetadataRolloverService.java:117)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction$RolloverExecutor.executeTask(TransportRolloverAction.java:332)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction$RolloverExecutor.execute(TransportRolloverAction.java:268)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.innerExecuteTasks(MasterService.java:1039)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:1004)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService.executeAndPublishBatch(MasterService.java:232)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$BatchingTaskQueue$Processor.lambda$run$2(MasterService.java:1626)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListener.run(ActionListener.java:386)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$BatchingTaskQueue$Processor.run(MasterService.java:1623)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$5.lambda$doRun$0(MasterService.java:1237)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.action.ActionListener.run(ActionListener.java:386)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.cluster.service.MasterService$5.doRun(MasterService.java:1216)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\r\n\tat org.elasticsearch.server@8.12.0\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\r\n\tat java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base\/java.lang.Thread.run(Thread.java:1583)\r\n\"\"\"\r\n  },\r\n  \"status\": 500\r\n}\r\n```","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Hi @jbaiera , I am trying to fix this NPE issue.","@howardhuanghua are you still looking into this? I was working with code close to this area and I think I know what's going on.\r\n\r\nFirst of all, rolling over the data stream works properly, and the path taken for rolling over the alias vs the data stream is pretty similar until [this point](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6d040d9bea355b1b8f97e2bec41747f35f1ad695\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/rollover\/MetadataRolloverService.java#L117)\r\n\r\nThe null pointer exception happens within the call to `rolloverAlias(...)` around [here](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6d040d9bea355b1b8f97e2bec41747f35f1ad695\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/rollover\/MetadataRolloverService.java#L201) with the issue specifically being that, while we have the correct `writeIndex` (the backing index of the ds), its `aliases` map is empty (so when `writeIndex.getAliases().get(alias.getName())` is called in the following line, the `get()` portion returns null)\r\n\r\nSo I looked into what happens when the index is being created in the first place to see why its aliases map is empty, and saw that IndexMetadata for indices belonging to data streams [intentionally](https:\/\/github.com\/elastic\/elasticsearch\/blob\/dce8a6b77f61ede59d23af10b45e702048c9d0f7\/server\/src\/main\/java\/org\/elasticsearch\/cluster\/metadata\/MetadataCreateIndexService.java#L672) don't have links to the parent data stream's aliases\r\n(I can also confirm that the DataStreamMetadata inside Metadata's `customs` has a reference to the alias there)\r\n\r\nAdditionally, back in the `rolloverAlias(...)` method `aliasMetadata` is only used to determine if the alias has a [write index](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6d040d9bea355b1b8f97e2bec41747f35f1ad695\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/rollover\/MetadataRolloverService.java#L203) and to determine if the alias [is hidden](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6d040d9bea355b1b8f97e2bec41747f35f1ad695\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/rollover\/MetadataRolloverService.java#L224), both of which I think can be determined from the `IndexAbstraction.Alias alias` that is being passed into this method. \r\nSo then I tested out what would happen if you used `alias` instead of `aliasMetadata`, but then it actually fails further down the line [here](https:\/\/github.com\/elastic\/elasticsearch\/blob\/b6f876f32a270e7240117bdc6cd46d01c563f549\/server\/src\/main\/java\/org\/elasticsearch\/cluster\/metadata\/MetadataIndexAliasesService.java#L245-L249)\r\n\r\nSo since it seems that `rolloverAlias(...)` was designed specifically for aliases to indices, I'm thinking the fix might be to change the [rollover target](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6d040d9bea355b1b8f97e2bec41747f35f1ad695\/server\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/rollover\/MetadataRolloverService.java#L106) (where we split between over aliases vs data streams) to the data stream instead of the alias so we go into the `rolloverDataStream(...)` path instead.","@limotova thanks for the detail analysis. Since you've already got the root cause, would you please help to fix it directly? I haven't started yet."],"labels":[">bug",":Data Management\/Data streams","Team:Data Management"]},{"title":"POC: Cap shard failure lists to a fixed small size (March 2024)","body":"This is a POC exploratory coding attempt to address https:\/\/github.com\/elastic\/elasticsearch\/issues\/103708 and https:\/\/github.com\/elastic\/elasticsearch\/issues\/99220\r\n\r\nAfter some earlier exploratory code, I decided not to change the AtomicAtomic of ShardSearchFailures in `AbstractSearchAsyncAction`. Changing it really messes up the lock-free thread safety model of that class. In addition, other classes keep AtomicArray's of all shard results, so this is not the only offender.\r\n\r\nInstead, I focused on reducing the number of failures reported in the SearchResponse. The SearchResponse does not track failed shard count independent of the ShardSearchFailure array, so that new field had to be added.\r\n\r\nMost tests are passing, but need to do further work on those. Also CCS MRT=false is not yet truncating the number of failures in the _cluster\/details\/failures section so I need to track down where that occurs.","comments":[],"labels":["v8.14.0"]},{"title":"[WIP] Add an optimised vector distance function for aarch64.","body":"This commit adds an optimised vector distance function implementation for aarch64.\r\n\r\nBased on Lucene 9.10, I had to copy-and-modify just three of the Lucene99 codec classes. So we end up with:\r\n1. ES814HnswScalarQuantizedVectorsFormat\r\n2. ES814ScalarQuantizedVectorsFormat\r\n3. ES814ScalarQuantizedVectorsWriter\r\n\r\nThe first two are quite trivial, the third is mostly a copy with some small changes in mergeOneFieldToIndex - could be better, but seems not too bad.\r\n\r\nNote: much of the infrastructural and build changes in this PR will be superseded by work going on elsewhere. For now, they exist in this PR just to get things working.","comments":[],"labels":["WIP","v8.14.0"]},{"title":"Add time series grouping operator that performs grouping when aggregation mode is final","body":"WIP\r\n\r\nExample request:\r\n\r\n```\r\nPOST \/_query?format=txt\r\n{\r\n    \"query\": \"FROM cpu_tsbs | STATS max_usage_system = max(usage_system)\",\r\n    \"pragma\": {\r\n        \"time_series\": true,\r\n        \"time_series_period\": \"1h\"\r\n    }\r\n}\r\n```","comments":["Thanks @martijnvg. I wonder if we should consider introducing a new BlockHash implementation that takes advantage of pages sorted by `_tsid`, then `@timestamp`, instead of introducing the TimeSeriesAggregationOperatorFactory.\r\n\r\nI also think we need to introduce the rate aggregation function, which is independent of this change. The partial result of the rate function should consist of 5 blocks: earliest timestamp, earliest value, latest timestamp, latest value, and compensation.",">  I wonder if we should consider introducing a new BlockHash implementation that takes advantage of pages sorted by _tsid, then @timestamp, instead of introducing the TimeSeriesAggregationOperatorFactory.\r\n\r\nI think a new `BlockHash` implementation makes sense. However I still think we need the `TimeSeriesAggregationOperatorFactory ` here.  The grouping should only take place if the aggregation mode is final, because only then a rate (or other time series statistic) can be computed, once we have all the values for a time serie. Do general intermediate grouping can cause an incorrect rate to be computed. I think the `TimeSeriesAggregationOperatorFactory` can return a `HashAggregationOperator` instance with a 'time series' `BlockHash` implementation.  Does this make sense?\r\n\r\n> I also think we need to introduce the rate aggregation function, which is independent of this change. The partial result of the rate function should consist of 5 blocks: earliest timestamp, earliest value, latest timestamp, latest value, and compensation.\r\n\r\n\ud83d\udc4d "],"labels":[":StorageEngine\/TSDB","v8.14.0"]},{"title":"Update hotspotting.asciidoc","body":"Fix invalid jq command","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_106121.docs-preview.app.elstc.co\/diff)","@fdartayre please enable the option \"Allow edits and access to secrets by maintainers\" on your PR. For more information, [see the documentation](https:\/\/docs.github.com\/en\/pull-requests\/collaborating-with-pull-requests\/working-with-forks\/allowing-changes-to-a-pull-request-branch-created-from-a-fork).","Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">docs",":Distributed\/Distributed","Team:Distributed","Team:Docs","external-contributor","v8.12.3"]},{"title":"TransportVersionUtils#randomVersionBetween does not work with version extensions","body":"When a version constant is provided by a version extension, `TransportVersionUtils#randomVersionBetween` will fail with a message similar to the following\r\n```\r\njava.lang.IllegalArgumentException: maxVersion [8599010] does not exist.\r\n```\r\nSuch a version constant can be automatically returned by calling `TransportVersion.current()` if a version extension happens to define the highest version.","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">test",":Core\/Infra\/Core","Team:Core\/Infra"]},{"title":"[CI] TransformIT testStopWaitForCheckpoint failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/zc5izwxayffgm\/tests\/:x-pack:plugin:transform:qa:multi-node-tests:javaRestTest\/org.elasticsearch.xpack.transform.integration.TransformIT\/testStopWaitForCheckpoint\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:transform:qa:multi-node-tests:javaRestTest' --tests \"org.elasticsearch.xpack.transform.integration.TransformIT.testStopWaitForCheckpoint\" -Dtests.seed=EFA8DB76DD3CD4F1 -Dtests.locale=ro-RO -Dtests.timezone=Pacific\/Fakaofo -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.transform.integration.TransformIT#testStopWaitForCheckpoint`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testStopWaitForCheckpoint'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.transform.integration.TransformIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: \"stopped\"\n     but: was \"stopping\"\n\n  at __randomizedtesting.SeedInfo.seed([EFA8DB76DD3CD4F1:927DF71D11A83EC7]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2130)\n  at org.elasticsearch.xpack.transform.integration.TransformIT.testStopWaitForCheckpoint(TransformIT.java:428)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ent-search-eng (Team:SearchOrg)"],"labels":["blocker",">test-failure",":SearchOrg\/Extract&Transform","Team:Search - Extract & Transform","Team:SearchOrg"]},{"title":"OIDC reaml's client_secret should be hot reloadable","body":"### Description\r\n\r\nToday, the OpenID Connect (OIDC) realm's client secret requires a full cluster restart to pick up the changes made to the value that is persisted in the keystore. \r\n\r\nThis makes for a very awkward client secret key rotation workflow and is near impossible to perform a rotation without any downtime. Ideally the OIDC realm's client secret would be hot re-loadable. The JWT realm recently implemented something [similar](https:\/\/github.com\/elastic\/elasticsearch\/pull\/99278) for it's client secret, but that functionality is only available with the JWT realm. \r\n\r\nAlso, the documentation for this value should be updated to be explicit that a cluster restart is needed to update this value (at least until the fix is implemented) https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/oidc-guide.html\r\n","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">enhancement",":Security\/Authentication","Team:Security"]},{"title":"Enrich Cache should be based on memory usage instead of a flat document count","body":"When enriching documents via an ingest processor, we cache recently retrieved documents in a global enrich cache. This cache is set at 1000 entries by default. The cache does not take memory consumption of documents into account when storing values and evicting them. This causes heap issues on smaller node sizes when large documents are returned by enrich indices. \r\n\r\nAs far as I can tell, the practical size limit of a document based on various limitations is in the ballpark of 50-100mb. An enrich index filled with extremely large documents can quickly fill the heap of an Elasticsearch node since the cache retains 1000 entries by default. The only fix in this scenarios is to either cease using Enrich in this fashion or drop the cache size severely.\r\n\r\nMarking as a bug since without any guards in place this can lead to system instability.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Discussed with Data Management team: Since documents are deserialized from search responses before being stored in the cache, we could use the document source size to inform the cache of how large the document is. \r\n\r\nThe upside is that this is easy to measure when reading the search result in enrich before storing it in the cache. The cache entries should then be weighted based on the document size. We could approximate a good starting value to be something like `1000 * the typical size of an enrich document in SMILE`, then select the minimum between that value and a flat percentage of the node's maximum heap size.\r\n\r\nThe downsides are that this would not an exact read of the memory footprint. Enrich documents are serialized in SMILE format and thus are more compact than JSON, but likely less compact than their on-heap footprint. The cache weight would need to be an abstract value selected based on what we consider regular operating tolerances to be. Tuning this cache value then becomes more challenging for end users. They may not have much knowledge of their average document size in SMILE format.\r\n\r\nIn the future if we ever want to support different content types from enrich indices, then the cache logic may be skewed depending on how compact the other content types are. Furthermore, cache settings are node scope and non-dynamic. Tuning these values requires a node restart to take effect. Since performance based on cache size will become more dependent on enrich document size in each specific case, making the cache tunable without requiring a restart may be a helpful enhancement as well."],"labels":[">bug",":Data Management\/Ingest Node","Team:Data Management"]},{"title":"Fix merging component templates with a mix of dotted and nested object mapper definitions","body":"Fixes https:\/\/github.com\/elastic\/elasticsearch\/issues\/105482\r\n\r\n## The background\r\n\r\nThis regression was introduced in https:\/\/github.com\/elastic\/elasticsearch\/pull\/97317. That PR added a new way how component templates get merged. When creating mappings for an index that's about to be created, we previously merged component templates one-by-one. The issue with this sequential merging approach is that some settings (like `subobjects: false`) in one component template may affect how other component templates need to be parsed. This makes it dependent on ordering. Consider that there are component templates c1 and c2 that are merged sequentially. In c1, `subobjects` is set to false. This affects how c2 needs to be parsed. However, if c2 sets `subobjects: false`, before the PR, it couldn't affect how c1 is parsed, because it has already been parsed before considering c2.\r\n\r\nThe PR introduced a way to bulk-merge multiple component templates at once (see the [changes in MetadataCreateIndexService](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106077#diff-c02a1326ed501a89ce49b7e7a27df4bb9a2256785d00d7278bccb3609334b939)). In the first step, the mapping sources are merged (merging of a `Map<String, Object>`). This ensures that the `subobjects` parameter, for example, is determined for the combination of the mappings, before the mapping source is actually parsed into a `RootObjectMapper`.\r\n\r\nSo far, all is well and good.\r\n\r\nWhen there are two component templates that have a mix of dotted and object notation, their sources get merged like this:\r\n\r\n```json\r\n{\r\n  \"properties\": {\r\n    \"parent.child\": {\r\n      \"type\": \"keyword\"\r\n    },\r\n    \"parent\" : {\r\n      \"properties\" : {\r\n        \"child\" : {\r\n          \"type\" : \"integer\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nStill, so far, so good.\r\n\r\nWhen parsing the mappings, the `RootObjectMapper` will then have two `ObjectMapper.Builder` instances for the `parent` object. This is also not issue so far, because there's code in `ObjectMapper.Builder#buildMappers` that deals with that exact scenario and merges the two objects:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/a97552b6f883fccbbb73eb7e4d130d88b9e8465e\/server\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/ObjectMapper.java#L160-L167\r\n\r\nThis is where the trouble starts: when looking at the implementation of `ObjectMapper#merge`, it the merge reason is hard-coded to `MAPPING_UPDATE`.\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/a97552b6f883fccbbb73eb7e4d130d88b9e8465e\/server\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/ObjectMapper.java#L458-L460\r\n\r\nBut wait, we're still in a context where the merge reason should be `INDEX_TEMPLATE` - we're merging multiple component templates.\r\n\r\nWhy does the merge reason matter?\r\n\r\nWithin the merge process, the merge reason determines how to deal with field mappers for the same field:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/a97552b6f883fccbbb73eb7e4d130d88b9e8465e\/server\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/ObjectMapper.java#L592-L598\r\n\r\nBecause the wrong merge reason is selected, the two different definitions of the `child` field (`{\"child\" : {\"type\" : \"keyword\"}}` and `{\"child\" : {\"type\" : \"integer\"}}` ) are merged instead of replaced. However, it's not allowed to change the type of a field when merging two different field mappers. This is why this saga ends with an exception being thrown here:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/179739effdad01e0f9e35b09799cec07603e05bf\/server\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/FieldMapper.java#L404-L408\r\n\r\n## The fix\r\n\r\nThe PR https:\/\/github.com\/elastic\/elasticsearch\/pull\/97317 made crucial improvements on how component templates get merged. We don't want to roll these back. Instead, the focus of the bug fix is to preserve the right merge reason, depending on the context.\r\n\r\nTo do so, I've added the merge reason to `MapperBuilderContext`. This seemed like the most natural choice to hand down the merge reason and avoids having to add another `MergeReason` parameter to even more methods. This merge reason is the one to use when merging mappers while building the mapper (when calling `ObjectMapper#merge` in `ObjectMapper.Builder#buildMappers`).\r\n\r\nWhen creating a `MapperBuilderContext`, a `MergeReason` can be supplied. I kept the `public static MapperBuilderContext root(boolean isSourceSynthetic, boolean isDataStream)` method which is using a `MAPPING_UPDATE` merge reason by default. This is mainly so that I don't clutter this PR with a gazillion updated callers in test code. There are also a couple of callers in production code that I didn't feel the need to update by explicitly providing the `MAPPING_UPDATE` merge reason. YMMV, but previously the entire code base was implicitly assuming a `MAPPING_UPDATE` merge reason for `ObjectMappepr#merge` and this is now adding the ability to specify a different one. But in most places, `MAPPING_UPDATE` is the appropriate merge reason.\r\n","comments":["Pinging @elastic\/es-search (Team:Search)","Hi @felixbarny, I've created a changelog YAML for you."],"labels":[">bug",":Search\/Mapping","Team:Search","external-contributor","auto-backport-and-merge","v8.13.0","v8.14.0"]},{"title":"Authentication API Logging","body":"### Description\n\nCurrently, when logging authentication via the API, in case of an error, the following message is displayed:\r\n\r\n`{\"@timestamp\":\"2024-03-07T14:00:20.001Z\", \"log.level\": \"WARN\", \"message\":\"Authentication using apikey failed - unable to find apikey with id HBjhbh\", \"ecs.version\": \"1.2.0\",\"service.name\":\"ES_ECS\",\"event.dataset\":\"elasticsearch.server\",\"process.thread.name\":\"elasticsearch[test-elk-coordinator][transport_worker][T#11]\",\"log.logger\":\"org.elasticsearch.xpack.security.authc.ApiKeyAuthenticator\",\"elasticsearch.cluster.uuid\":\"sdafsdfafdsf\",\"elasticsearch.node.id\":\"asdgfsadf\",\"elasticsearch.node.name\":\"test-elk-coordinator\",\"elasticsearch.cluster.name\":\"test-elk-cluster\"}`\r\n\r\nUnfortunately, based on this event, it is impossible to determine which client is experiencing authentication issues.\r\nI kindly request considering the possibility of adding client IP address information to the log output.","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">enhancement",":Security\/Authentication","Team:Security"]},{"title":"Undeleted indices data due to shrink operations","body":"### Elasticsearch Version\r\n\r\n7.17.6\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nUbuntu 20.04.6 LTS\r\n\r\n### Problem Description\r\n\r\nIn at least one case (see privately linked issue), it has been observed that:\r\n\r\n- ILM force merges and shrinks an index, when passing it from the hot to the warm phase\r\n- The original source index's data is not deleted from the hard disk\r\n- This can happen for many shrunk indices, and amount to a big discrepancy in a node's reports `disk.indices` and `disk.used` metrics.\r\n\r\nThe aim of this bug ticket is to further investigate the bug, potentially reproduce it, and finally correct it.\r\n\r\nThis ticket may relate to https:\/\/github.com\/elastic\/elasticsearch\/issues\/104735 as a sub-case.\r\n\r\n### Steps to Reproduce\r\n\r\nSee privately linked issue\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Store","Team:Distributed"]},{"title":"Introduce setting aliases to migrate settings more easily.","body":"Fallback settings are not well-suited when migrating a setting from one key to another. The problem is that update consumers are not properly triggered if both the old (fallback) and new setting are present causing changes to be silently ignored if the old version is used.\r\n\r\nAlias settings behave similar to fallback settings with the difference that updates to the deprecated alias are copied to the new setting in addition. This ensures that update consumers are always triggered properly to ensure a more intuitive \/ less confusing behavior.\r\n\r\nUsage of a setting alias will always trigger a deprecation warning.\r\n\r\n(relates to ES-7815)","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Can fallbacks be implemented on top of aliases, so we don't have two separate concepts that do kinda the same thing but in different ways?","> Can fallbacks be implemented on top of aliases, so we don't have two separate concepts that do kinda the same thing but in different ways?\r\n\r\nI've been wondering about this myself, fair question. Fallbacks and Aliases are certainly fairly similar, but there's differences:\r\n\r\n- the fallback setting approach (using a separate setting instance) unfortunately doesn't work for AffixSettings as it is based on default value functions, which are not applicable in that case. That's the main reason I decided to follow @stu-elastic's approach for affix settings in #104345 and generalize that. \r\n\r\n- alias keys are rewritten in [`AbstractScopedSettings.updateDynamicSettings`](https:\/\/github.com\/elastic\/elasticsearch\/pull\/106061\/files#diff-7d6621dfd4e57d51a347f3a3b5ccf34d902f163d507ecc013f8197b3f5041dcbR838-R846) to ensure update consumers are triggered.\r\n\r\n- `Setting.exits` doesn't consider a fallback, but it should consider an alias.\r\n\r\n- `Setting.get` using primary and secondary settings (used by `ClusterSettings.get(Setting<?>)`) evaluates fallbacks in a different order.\r\n\r\nA few thoughts without having an ultimate answer:\r\n\r\n- The code required to support fallbacks is minimal, actually it's mostly just a default value function. The trouble arises from the high number of permutations of the various setting factories having both fallbacks and aliases.\r\n\r\n- Fallback settings are often shared as default by various other more specialised settings. The current implementation captures that dependency in a more explicit way. With the aliasKey any such dependency would be fairly lose just based on repeated usage of the same alias key.","I share Simon's concerns about supporting both aliases and fallbacks. While it seems like more work to have a consistent, single concept, I fear adding yet another concept to settings is going to complicate them too much.\r\n\r\nSome of the things you mention seem like bugs in the same way that fallbacks are not considered today for updates. For example:\r\n\r\n> Setting.exists doesn't consider a fallback, but it should consider an alias.\r\n\r\nThat seems like another bug. Fallbacks are _supposed_ to be what you have framed as aliases. While they are clunky today (eg so many method variants to take fallbacks), we should find a way to make them work consistently (and maybe that means changing the concept from fallbacks to aliases, but not having both).",">> Setting.exists doesn't consider a fallback, but it should consider an alias.\r\n\r\n> That seems like another bug. Fallbacks are supposed to be what you have framed as aliases.\r\n\r\nInterestingly that's one of the important differences and actually required so that fallbacks work as intended. Reasons is\r\n\r\n```java\r\npublic final T get(Settings primary, Settings secondary) {\r\n    if (exists(primary)) {\r\n        return get(primary);\r\n    }\r\n    if (exists(secondary)) {\r\n        return get(secondary);\r\n    }\r\n    if (fallbackSetting == null) {\r\n        return get(primary);\r\n    }\r\n    if (fallbackSetting.exists(primary)) {\r\n        return fallbackSetting.get(primary);\r\n    }\r\n    return fallbackSetting.get(secondary);\r\n}\r\n```\r\n\r\nwhich is called from `ClusterSettings.get(Setting)` using the last settings applied as `primary` and the initial node settings as `secondary`.\r\n\r\nAnother feature fallbacks support is a chain of fallbacks. I think I've seen at least one place using multiple levels of fallbacks.  \r\n\r\nWith the above in mind I'll give this another look to see if I can actually do the opposite, implement the alias behavior on top of fallbacks. I expect that `AffixSettings` would remain different in that case (and continue using the \"alias\" behavior Stu implemented) but it might be possible to remove any such logic if rewriting setting keys both during dynamic updates (as currently done in this PR) and additional rewriting the initial node settings as well.\r\n\r\n> they are clunky today (eg so many method variants to take fallbacks)\r\n\r\nThe variants are definitely a hassle regardless if alias or fallback. I think it would greatly improve things if moving away from the current factory methods capturing everything in all the possible combinations to a more fluent configuration style starting off  a few factories with a basic, common usage pattern. Anyways, that's a much simpler and separate thing to look at.\r\n\r\n","For simple & list settings this can be re-implemented on top of fallbacks without much effort.\r\nThe alias behavior would be triggered based on a new property `Alias` on the fallback.\r\n\r\nWhen looking into affix settings I ran into #106284. Also, I wouldn't be surprised to find more bugs there.\r\nIMHO, before making any changes on affix settings, we'd have to increase test coverage to better understand the status quo.\r\n\r\nMy proposal would be to:\r\n- Implement aliases on top of fallbacks for simple and list settings. AffixSettings don't support the same fallback mechanism, so this doesn't apply there.\r\n- Independently, improve test coverage for affix settings (particularly in combination with a fallback prefix) and fix related issues. Then re-evaluate next steps from there.","Any thoughts on above @thecoop and @rjernst ?","Do more complex settings still need to be kept separate between fallbacks and aliases? What's the plan to combine those at some point in the future, or does that need the more exhaustive tests first?","> Do more complex settings still need to be kept separate between fallbacks and aliases?\r\n\r\n- The fallback implementation of `AffixSetting` in `main` currently corresponds more closely to aliases than fallbacks (both, in terms of behavior and implementation). So yes & no, it's different, but only this single implementation exists (and will exist).\r\n- `GroupSetting` doesn't support fallback (nor alias).\r\n\r\n> What's the plan to combine those at some point in the future, or does that need the more exhaustive tests first?\r\n\r\nMost of all, the implementation is buggy and needs more through testing to identify the gaps first.\r\nBut as mentioned above, there's just a single implementation for `AffixSetting`. \r\n","That looks sensible to me - implementing one on top of the other, and increasing test coverage, should get us to a good solution"],"labels":[">non-issue",":Core\/Infra\/Settings",":Core\/Infra\/Core","Team:Core\/Infra","v8.14.0"]},{"title":"Profile id for get api keys","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"[CI] TasksIT testGetTaskWaitForCompletionWithoutStoringResult failing","body":"Seems like a weird race condition. Either `.tasks` index doesn't exist, or we checked its status too quickly after it was created or something.\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/p6l3ddtlwelwk\/tests\/:server:internalClusterTest\/org.elasticsearch.action.admin.cluster.node.tasks.TasksIT\/testGetTaskWaitForCompletionWithoutStoringResult\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.testGetTaskWaitForCompletionWithoutStoringResult\" -Dtests.seed=D5FAF539943B576A -Dtests.locale=pl-PL -Dtests.timezone=America\/Belem -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.action.admin.cluster.node.tasks.TasksIT#testGetTaskWaitForCompletionWithoutStoringResult`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testGetTaskWaitForCompletionWithoutStoringResult'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.action.admin.cluster.node.tasks.TasksIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.util.concurrent.ExecutionException: org.elasticsearch.transport.RemoteTransportException: [node_s2][127.0.0.1:18702][cluster:monitor\/task\/get]\n\n  at __randomizedtesting.SeedInfo.seed([D5FAF539943B576A:B3ABA602864640A6]:0)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.getValue(PlainActionFuture.java:287)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.get(PlainActionFuture.java:274)\n  at org.elasticsearch.action.support.PlainActionFuture.get(PlainActionFuture.java:93)\n  at org.elasticsearch.client.internal.support.AbstractClient$RefCountedFuture.get(AbstractClient.java:1534)\n  at org.elasticsearch.client.internal.support.AbstractClient$RefCountedFuture.get(AbstractClient.java:1514)\n  at org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.waitForCompletionTestCase(TasksIT.java:617)\n  at org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.testGetTaskWaitForCompletionWithoutStoringResult(TasksIT.java:565)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n  Caused by: org.elasticsearch.transport.RemoteTransportException: [node_s2][127.0.0.1:18702][cluster:monitor\/task\/get]\n\n\n    Caused by: org.elasticsearch.ResourceNotFoundException: task [FzH5E4EYT1a-chDD8aVTxA:363] isn't running and hasn't stored its results\n\n      at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.lambda$getFinishedTaskFromIndex$6(TransportGetTaskAction.java:214)\n      at org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\n      at org.elasticsearch.action.ActionListener$2.onFailure(ActionListener.java:179)\n      at org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\n      at org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\n      at org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:31)\n      at org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:39)\n      at org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\n      at org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\n      at org.elasticsearch.action.ActionListener$3.onFailure(ActionListener.java:324)\n      at org.elasticsearch.tasks.TaskManager$1.onFailure(TaskManager.java:214)\n      at org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\n      at org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\n      at org.elasticsearch.action.DelegatingActionListener.onFailure(DelegatingActionListener.java:31)\n      at org.elasticsearch.action.ActionListenerImplementations$RunBeforeActionListener.onFailure(ActionListenerImplementations.java:317)\n      at org.elasticsearch.action.ActionListenerImplementations.safeAcceptException(ActionListenerImplementations.java:62)\n      at org.elasticsearch.action.ActionListenerImplementations.safeOnFailure(ActionListenerImplementations.java:73)\n      at org.elasticsearch.action.ActionListener$3.onFailure(ActionListener.java:324)\n      at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:103)\n      at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\n      at org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\n      at org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\n      at org.elasticsearch.client.internal.node.NodeClient.doExecute(NodeClient.java:83)\n      at org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\n      at org.elasticsearch.client.internal.FilterClient.doExecute(FilterClient.java:54)\n      at org.elasticsearch.client.internal.OriginSettingClient.doExecute(OriginSettingClient.java:43)\n      at org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\n      at org.elasticsearch.client.internal.support.AbstractClient.get(AbstractClient.java:456)\n      at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.getFinishedTaskFromIndex(TransportGetTaskAction.java:210)\n      at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.getRunningTaskFromNode(TransportGetTaskAction.java:138)\n      at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.doExecute(TransportGetTaskAction.java:90)\n      at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.doExecute(TransportGetTaskAction.java:60)\n      at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\n      at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\n      at org.elasticsearch.action.support.HandledTransportAction.lambda$new$0(HandledTransportAction.java:50)\n      at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n      at org.elasticsearch.transport.InboundHandler.doHandleRequest(InboundHandler.java:288)\n      at org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:273)\n      at org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:115)\n      at org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:96)\n      at org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:821)\n      at org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:124)\n      at org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:96)\n      at org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:61)\n      at org.elasticsearch.transport.netty4.Netty4MessageInboundHandler.channelRead(Netty4MessageInboundHandler.java:48)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n      at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n      at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n      at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n      at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n      at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n      at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n      at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n      at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n      at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\n      at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\n      at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n      at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n      at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n      at java.lang.Thread.run(Thread.java:1583)\n\n      Caused by: org.elasticsearch.index.IndexNotFoundException: no such index [.tasks]\n\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.notFoundException(IndexNameExpressionResolver.java:553)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$ExplicitResourceNameFilter.ensureAliasOrIndexExists(IndexNameExpressionResolver.java:1712)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$ExplicitResourceNameFilter.filterUnavailable(IndexNameExpressionResolver.java:1692)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.resolveExpressions(IndexNameExpressionResolver.java:252)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:340)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:299)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:285)\n        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:632)\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.<init>(TransportSingleShardAction.java:161)\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:106)\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:53)\n        at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\n        at org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\n        at org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\n        at org.elasticsearch.client.internal.node.NodeClient.doExecute(NodeClient.java:83)\n        at org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\n        at org.elasticsearch.client.internal.FilterClient.doExecute(FilterClient.java:54)\n        at org.elasticsearch.client.internal.OriginSettingClient.doExecute(OriginSettingClient.java:43)\n        at org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\n        at org.elasticsearch.client.internal.support.AbstractClient.get(AbstractClient.java:456)\n        at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.getFinishedTaskFromIndex(TransportGetTaskAction.java:210)\n        at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.getRunningTaskFromNode(TransportGetTaskAction.java:138)\n        at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.doExecute(TransportGetTaskAction.java:90)\n        at org.elasticsearch.action.admin.cluster.node.tasks.get.TransportGetTaskAction.doExecute(TransportGetTaskAction.java:60)\n        at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\n        at org.elasticsearch.action.support.HandledTransportAction.lambda$new$0(HandledTransportAction.java:50)\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n        at org.elasticsearch.transport.InboundHandler.doHandleRequest(InboundHandler.java:288)\n        at org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:273)\n        at org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:115)\n        at org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:96)\n        at org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:821)\n        at org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:124)\n        at org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:96)\n        at org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:61)\n        at org.elasticsearch.transport.netty4.Netty4MessageInboundHandler.channelRead(Netty4MessageInboundHandler.java:48)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n        at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","I suspect this is related to the fix done to https:\/\/github.com\/elastic\/elasticsearch\/issues\/97923\r\nThere are no recent production changes around tasks so I am going to change the priority of this one.\r\nFeel free to update if you do not agree","@idegtiarenko that fix is 2mo old? This failed in the last week?"],"labels":[">test-failure",":Distributed\/Task Management","Team:Distributed","medium-risk"]},{"title":"[Transform] Clarify best practices around using timestamps in continuous transform","body":"### Description\n\nOne user expressed confusion wrt which timestamp field should be used as `date_histogram.field` and `sync.time.field`.\r\n\r\nThe problem is that:\r\n- on one hand we recommend the `sync.time.field` to be ingestion timestamp set by the ingest pipeline\r\nbut\r\n- on the other hand, we recommend setting `date_histogram` timestamp field and `sync.time.field` to the same value in order to take advantage of the `align_checkpoints` optimization.\r\n\r\nCurrently, there is no way of having both at the same time which leads to confusion.\r\nWe should either fix it in code or at least set up expectations with users and give them guidance on how to best set these fields.","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml\/Transform","Team:ML"]},{"title":"ST_CENTROID returns POINT(NaN NaN) on empty records","body":"### Elasticsearch Version\r\n\r\n8.14\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nall\r\n\r\n### Problem Description\r\n\r\nIf we aggregate over zero records, we get POINT (NaN NaN). I think the default behaviour in ES|QL is to return null instead of NaN, and in this case a null centroid, not null coordinates.\r\n\r\nFor example:\r\n\r\n```\r\nFROM airports\r\n| WHERE ST_INTERSECTS(location, city_location)\r\n| STATS centroid=ST_CENTROID(location), count=COUNT()\r\n```\r\nReturns:\r\n```\r\ncentroid:geo_point  |  count:long\r\nPOINT (NaN NaN)     |  0\r\n```\r\nBut should return:\r\n```\r\ncentroid:geo_point  |  count:long\r\nnull                |  0\r\n```\r\nAnd a warning.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nCreate an index with two geo_point fields, but do not add any documents.\r\n\r\nQuery with:\r\n```\r\nFROM index | STATS centroid=ST_CENTROID(location, other_location), count=COUNT(*)\r\n```\r\n\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug",":Analytics\/Geo","Team:Analytics","v8.14.0"]},{"title":"Deprecation warning for built-in viewer role due to privileges mismatch with backing indices ","body":"### Elasticsearch Version\r\n\r\n8.12\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nN\/A \r\n\r\n### Problem Description\r\n\r\nThis deprecation warning is logged when \"alerts-as-data\" is used: \r\n\r\n```\r\nRole [viewer] contains index privileges covering the [.alerts-observability.metrics.alerts-default] alias but which do not cover some of the indices that it points to [.internal.alerts-observability.metrics.alerts-default-000001].\r\n```\r\n\r\nThis happens because the [role descriptor](https:\/\/github.com\/elastic\/elasticsearch\/blob\/e9ff896738cba9e0ded5ab1bd6b098075a5b6e67\/x-pack\/plugin\/core\/src\/main\/java\/org\/elasticsearch\/xpack\/core\/security\/authz\/store\/ReservedRolesStore.java#L651-L654) for `viewer` with regard to alerts is `.alerts*` and `.preview.alerts*`. \r\n\r\nKibana creates an `.alerts-ilm-policy` that looks like this: \r\n```javascript\r\n{\r\n    \"version\" : 32,\r\n    \"modified_date\" : \"2024-02-20T16:47:11.219Z\",\r\n    \"policy\" : {\r\n      \"phases\" : {\r\n        \"hot\" : {\r\n          \"min_age\" : \"0ms\",\r\n          \"actions\" : {\r\n            \"rollover\" : {\r\n              \"max_age\" : \"30d\",\r\n              \"max_primary_shard_size\" : \"50gb\"\r\n            }\r\n          }\r\n        }\r\n      },\r\n      \"_meta\" : {\r\n        \"managed\" : true\r\n      }\r\n    },\r\n    \"in_use_by\" : {\r\n      \"indices\" : [\r\n\t\u2026\r\n        \".internal.alerts-observability.metrics.alerts-default-000001\",\r\n        \u2026\r\n      ],\r\n      \"data_streams\" : [ ],\r\n      \"composable_templates\" : [\r\n        \u2026\r\n      ]\r\n    }\r\n  }\r\n```\r\n\r\nAnd an alias:\r\n```javascript\r\n...\r\n  .internal.alerts-observability.metrics.alerts-default-000001\" : {\r\n    \"aliases\" : {\r\n      \".alerts-observability.metrics.alerts-default\" : {\r\n        \"is_write_index\" : false\r\n      }\r\n    }\r\n  }\r\n...\r\n```\r\n\r\nThis results in the warning message above. \r\n\r\nThe same warning is also present in cloud.\r\n\r\nAfter discussing with the Security team, the decision is that the `viewer` role should have the same privileges on the backing indices as the alias to mute this warning. \r\n\r\n### Steps to Reproduce\r\n\r\nEnable \"alerts-as-data\" and wait for a rollover. \r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">bug",":Security\/Security","Team:Security"]},{"title":"[ML] Inference API scale testing","body":"### Description\n\nPerform scale testing with the inference API to get a better idea of throughput and latency.\r\n\r\nTry generating the text embeddings for 10 million documents.\r\n\r\nPotential dataset: https:\/\/huggingface.co\/datasets\/wikipedia","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">test",":ml","Team:ML","Feature:GenAI"]},{"title":"[CI] Multiple test failures JDK22 on 8.12","body":"Multiple test failures due to \"Unsupported class file major version 66\".\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/hkm5o5zei6wts\/tests\/:libs:elasticsearch-plugin-scanner:test\/org.elasticsearch.plugin.scanner.ClassReadersTests\/testMultipleJarsInADir\n\n\n**Reproduction line:**\n```\n.\/gradlew ':libs:elasticsearch-plugin-scanner:test' --tests \"org.elasticsearch.plugin.scanner.ClassReadersTests.testMultipleJarsInADir\" -Dtests.seed=5E261D9284F491C2 -Dtests.locale=hu-HU -Dtests.timezone=America\/Argentina\/Catamarca -Druntime.java=22\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.plugin.scanner.ClassReadersTests#testMultipleJarsInADir`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testMultipleJarsInADir'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.plugin.scanner.ClassReadersTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.IllegalArgumentException: Unsupported class file major version 66\n\n  at __randomizedtesting.SeedInfo.seed([5E261D9284F491C2:8A296B192E6D6168]:0)\n  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:199)\n  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:180)\n  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:166)\n  at org.elasticsearch.plugin.scanner.ClassReaders.lambda$classesInPath$5(ClassReaders.java:97)\n  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:212)\n  at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:194)\n  at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:194)\n  at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:194)\n  at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:212)\n  at java.util.Iterator.forEachRemaining(Iterator.java:133)\n  at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1939)\n  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:556)\n  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:546)\n  at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)\n  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:265)\n  at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:702)\n  at org.elasticsearch.plugin.scanner.ClassReaders.classesInPath(ClassReaders.java:102)\n  at org.elasticsearch.plugin.scanner.ClassReaders.classesInJar(ClassReaders.java:83)\n  at org.elasticsearch.plugin.scanner.ClassReaders.lambda$ofPaths$1(ClassReaders.java:73)\n  at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:288)\n  at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:194)\n  at java.util.Iterator.forEachRemaining(Iterator.java:133)\n  at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1939)\n  at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:556)\n  at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:546)\n  at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:622)\n  at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:291)\n  at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:631)\n  at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:637)\n  at java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:642)\n  at org.elasticsearch.plugin.scanner.ClassReaders.ofPaths(ClassReaders.java:77)\n  at org.elasticsearch.plugin.scanner.ClassReaders.ofDirWithJars(ClassReaders.java:48)\n  at org.elasticsearch.plugin.scanner.ClassReadersTests.testMultipleJarsInADir(ClassReadersTests.java:125)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1570)\n\n```","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","This does not reproduce for me. I'm passing back to delivery since it is clearly build related.","Pinging @elastic\/es-delivery (Team:Delivery)","This was already fixed for 8.13 earlier in https:\/\/github.com\/elastic\/elasticsearch\/pull\/104085. I've opened https:\/\/github.com\/elastic\/elasticsearch\/pull\/106033 to backport that change."],"labels":["blocker",":Delivery\/Build",">test-failure","Team:Delivery"]},{"title":"[Search] Allow to exclude fields from search requests","body":"### Description\n\nWhen using `_source` for Elasticsearch search requests, it is possible to exclude fields like this\r\n\r\n```\r\n \"_source\": {\r\n    \"includes\": [ \"obj1.*\", \"obj2.*\" ],\r\n    \"excludes\": [ \"*.description\" ]\r\n  }\r\n```\r\n\r\nWe recommend to use `fields` instead of `_source`:\r\n\r\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-fields.html#field-retrieval-methods\r\n\r\n```\r\n \"fields\": [\r\n    \"user.id\",\r\n    \"http.response.*\",         \r\n    {\r\n      \"field\": \"@timestamp\",\r\n      \"format\": \"epoch_millis\" \r\n    }\r\n  ],\r\n  ```\r\n\r\nWhat is missing here is a way to exclude fields, which could be done in a similar way like when using `_source`, for consistency.\r\n\r\n```\r\n \"fields\":{\r\n  \"includes\": [ \"obj1.*\", \"obj2.*\" ],\r\n   \"excludes\": [ \"*.description\" ]\r\n },\r\n  ```\r\n  \r\nIn Kibana we switched to use `fields` a while ago. We needed a way to exclude fields, for feature parity with the `_source` implementation. Kibana users can define fields to exclude. The way we did this was to exclude a field, we request all other fields. \r\n\r\nThis has the following disadvantages:\r\n* When there are e.g. 300 fields, and one is excluded, we send 299, when there are 3000 fields, 2999, .... so we bloat the search payload, which would be much slimmer when having a way to exclude single or more fields\r\n* We are using `SearchSource` for the search request when KQL is involved, this consumes DataViews, which use field_caps. We aim to reduce the requests for field_caps by removing redundancy, just requesting what's needed. For the exclusion of fields we need to get all fields upfront. Having a way to exclude those we don't need, would make a field_caps request in this case redundant.\r\n\r\n\r\n\r\n","comments":["Pinging @elastic\/es-search (Team:Search)","Hi @kertal , we discussed this with the team. It does make sense to allow for exclusions overall, although we don't necessarily think that consistency with `_source` is a good reason. We kept the surface of the `fields` API small to start with, and we are open to increasing it where necessary given real-life scenarios. You have provided a good reason, and we'd like to look into this, whether there would be performance implications. This is currently not high priority for the team.\r\n\r\n"],"labels":[">enhancement",":Search\/Search","Team:Search"]},{"title":"Fix HeapBufferedAsyncResponseConsumer limit heap memory usage for chunked responses","body":"The HeapBufferedAsyncResponseConsumer is used to limit heap memory usage for responses by utilizing the Content-Length header. However, this approach becomes ineffective for responses with Transfer-Encoding: chunked as the Content-Length header does not apply in this case, making it impossible the total size of the response. To address this, I can modify the onContentReceived method of the HeapBufferedAsyncResponseConsumer to cumulatively check the bufferLimitBytes during multiple chunked reads.\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/105684","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Pinging @elastic\/clients-team (Team:Clients)"],"labels":[":Clients\/Java Low Level REST Client","Team:Clients","external-contributor","v8.14.0"]},{"title":"[ML] Add throttling support to inference API","body":"### Description\r\n\r\nAdd the ability to throttle the inference API requests to external services. Currently, external services respond with many 429 depending on the user's license. We don't have any visibility into how many retries occur while generating the text embeddings.\r\n\r\n- [ ] Capture some metrics and output them somewhere\r\n- [ ] Determine how often we encounter retries\r\n- [ ] Implement a throttling mechanism for requests","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">enhancement",":ml","Team:ML","Feature:GenAI"]},{"title":"Describe the use cases for each thread pool in ThreadPool#Names","body":"There's no written explanation about what thread pool would be appropriate for a new TransportAction. It would be nice to have an explanation of the different thread pools, so developers can determine what best fits their new use case.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/Network","Team:Distributed"]},{"title":"Extend default expiration for transport certs generated by elasticsearch-certutil","body":"There aren't strong reasons for the expiration limit of TLS certificates that Elasticsearch nodes use to communicate between themselves. Therefore we should default, in the elasticsearch-certutil cmd line utility, to not using any default expiration limit, or to have a more distant default one (it's currently 3 years), for any compatibility reasons.\r\n \r\nIn general, beyond tradition, the expiration limit can is useful to get around limitations:\r\n * unbounded CRLs\r\n * evolving security standards\r\n * changing ownership of domains\r\n\r\nNone of these apply in this context, and closely expiring certificates are not alerted for out-of-the-box, and require a rotation procedure that's not trivial.","comments":["Pinging @elastic\/es-security (Team:Security)","Do I have it right that it is currently not possible to define a lifetime beyond 3 years when creating a CA certificate using elasticsearch-certutil ?","@foss4ever This issue is about the **default** expiration time limit of **ES transport-level** certs.\r\nTo generate a new CA cert and key with a, for eg, 10_000 days expiration limit run `elasticsearch-certutil ca --days 10000`."],"labels":[">enhancement",":Security\/TLS","Team:Security"]},{"title":"[CI] ReactiveStorageDeciderDecisionTests testStoragePreventsAllocation failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/nquynbhzzsdcw\/tests\/:x-pack:plugin:autoscaling:test\/org.elasticsearch.xpack.autoscaling.storage.ReactiveStorageDeciderDecisionTests\/testStoragePreventsAllocation\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':x-pack:plugin:autoscaling:test' --tests \"org.elasticsearch.xpack.autoscaling.storage.ReactiveStorageDeciderDecisionTests.testStoragePreventsAllocation\" -Dtests.seed=2BCE202E19A4B5D1 -Dtests.locale=sr-ME -Dtests.timezone=Etc\/GMT+2 -Druntime.java=17\r\n```\r\n\r\n**Applicable branches:**\r\nmain\r\n\r\n**Reproduces locally?:**\r\nYes\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.autoscaling.storage.ReactiveStorageDeciderDecisionTests#testStoragePreventsAllocation`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testStoragePreventsAllocation'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.autoscaling.storage.ReactiveStorageDeciderDecisionTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.AssertionError: \r\nExpected: sameInstance(<cluster uuid: _na_ [committed: false]\r\nversion: 0\r\nstate uuid: -4ZrEf4JTj2mQndGjOiwaQ\r\nfrom_diff: false\r\nmeta data version: 0\r\n   coordination_metadata:\r\n      term: 0\r\n      last_committed_config: VotingConfiguration{}\r\n      last_accepted_config: VotingConfiguration{}\r\n      voting tombstones: []\r\n   [test-3]: v[9], mv[1], sv[1], av[1]\r\n      0: p_term [1], isa_ids [ec5BqwZBT6mqOyI2FUMHRg, mMLUEpWpQKyPnfHXZxXQ8g]\r\n      1: p_term [1], isa_ids [uvo_rzB0Rf6Kym-B3b-Mug, m-8s1dPtQUOABnEuWhboPQ]\r\n      2: p_term [1], isa_ids [BBdn-HuwRTGEWQ6Xps-PjA, WUFjxQ7FRUmVyO_grRCvlw]\r\nmetadata customs:\r\n   index-graveyard: IndexGraveyard[[]]\r\nnodes (node-left generation: 0):\r\n   {hot_2}{hot_2}{-C5v6i-kQaKXIdyHp7lvLQ}{8Oj_LkMHRHSeI94ly3SrgA}{0.0.0.0}{0.0.0.0:17}{h}{8.13.0}{7000099-8502001}\r\n   {hot_1}{hot_1}{6DDHiw7mQni9TnpdunIsvA}{ln2XChMBReuj9Xq2UTlUrA}{0.0.0.0}{0.0.0.0:16}{h}{8.13.0}{7000099-8502001}\r\n   {hot_0}{hot_0}{MNeDvWzdQWeGkAUl6b1vUw}{oyc-_bOQTcKLAVGP3oPGVg}{0.0.0.0}{0.0.0.0:15}{h}{8.13.0}{7000099-8502001}\r\n   {warm_2}{warm_2}{-OMQMMfkSZePnORRIGipqw}{8SS8SE2CRfSVlzVMLGjkvQ}{0.0.0.0}{0.0.0.0:20}{w}{8.13.0}{7000099-8502001}\r\n   {warm_1}{warm_1}{uE53YF6zRgqx9sdjPpAM9g}{j9D8Dv54TX-BlgTKfY0XmQ}{0.0.0.0}{0.0.0.0:19}{w}{8.13.0}{7000099-8502001}\r\n   {warm_0}{warm_0}{Nx8BScO6QMCVfILKmGBPxQ}{1O0LsISXSh2_lNMq6DhnIw}{0.0.0.0}{0.0.0.0:18}{w}{8.13.0}{7000099-8502001}\r\ncluster features:\r\n   hot_2: []\r\n   hot_1: []\r\n   hot_0: []\r\n   warm_2: []\r\n   warm_1: []\r\n   warm_0: []\r\nrouting_table (version 0):\r\n-- index [[test-3]]\r\n----shard_id [test-3][0]\r\n--------[test-3][0], node[hot_0], [R], s[STARTED], a[id=mMLUEpWpQKyPnfHXZxXQ8g], failed_attempts[0]\r\n--------[test-3][0], node[hot_2], [P], s[STARTED], a[id=ec5BqwZBT6mqOyI2FUMHRg], failed_attempts[0]\r\n----shard_id [test-3][1]\r\n--------[test-3][1], node[hot_1], relocating [hot_2], [R], s[RELOCATING], a[id=m-8s1dPtQUOABnEuWhboPQ, rId=ugIDHeyeTRi5WgN4oHhNkg], failed_attempts[0], expected_shard_size[0]\r\n--------[test-3][1], node[hot_0], [P], s[STARTED], a[id=uvo_rzB0Rf6Kym-B3b-Mug], failed_attempts[0]\r\n----shard_id [test-3][2]\r\n--------[test-3][2], node[hot_0], relocating [hot_1], [R], s[RELOCATING], a[id=BBdn-HuwRTGEWQ6Xps-PjA, rId=9bdchFBJScW7elppTWRcGw], failed_attempts[0], expected_shard_size[1]\r\n--------[test-3][2], node[hot_2], [P], s[STARTED], a[id=WUFjxQ7FRUmVyO_grRCvlw], failed_attempts[0]\r\n\r\nrouting_nodes:\r\n-----node_id[hot_1][V]\r\n--------[test-3][1], node[hot_1], relocating [hot_2], [R], s[RELOCATING], a[id=m-8s1dPtQUOABnEuWhboPQ, rId=ugIDHeyeTRi5WgN4oHhNkg], failed_attempts[0], expected_shard_size[0]\r\n--------[test-3][2], node[hot_1], relocating [hot_0], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=9bdchFBJScW7elppTWRcGw, rId=BBdn-HuwRTGEWQ6Xps-PjA], failed_attempts[0], expected_shard_size[1]\r\n-----node_id[hot_0][V]\r\n--------[test-3][0], node[hot_0], [R], s[STARTED], a[id=mMLUEpWpQKyPnfHXZxXQ8g], failed_attempts[0]\r\n--------[test-3][1], node[hot_0], [P], s[STARTED], a[id=uvo_rzB0Rf6Kym-B3b-Mug], failed_attempts[0]\r\n--------[test-3][2], node[hot_0], relocating [hot_1], [R], s[RELOCATING], a[id=BBdn-HuwRTGEWQ6Xps-PjA, rId=9bdchFBJScW7elppTWRcGw], failed_attempts[0], expected_shard_size[1]\r\n-----node_id[warm_2][V]\r\n-----node_id[warm_1][V]\r\n-----node_id[warm_0][V]\r\n-----node_id[hot_2][V]\r\n--------[test-3][0], node[hot_2], [P], s[STARTED], a[id=ec5BqwZBT6mqOyI2FUMHRg], failed_attempts[0]\r\n--------[test-3][1], node[hot_2], relocating [hot_1], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=ugIDHeyeTRi5WgN4oHhNkg, rId=m-8s1dPtQUOABnEuWhboPQ], failed_attempts[0], expected_shard_size[0]\r\n--------[test-3][2], node[hot_2], [P], s[STARTED], a[id=WUFjxQ7FRUmVyO_grRCvlw], failed_attempts[0]\r\n---- unassigned\r\n>)\r\n     but: was <cluster uuid: _na_ [committed: false]\r\nversion: 0\r\nstate uuid: -4ZrEf4JTj2mQndGjOiwaQ\r\nfrom_diff: false\r\nmeta data version: 0\r\n   coordination_metadata:\r\n      term: 0\r\n      last_committed_config: VotingConfiguration{}\r\n      last_accepted_config: VotingConfiguration{}\r\n      voting tombstones: []\r\n   [test-3]: v[10], mv[1], sv[1], av[1]\r\n      0: p_term [1], isa_ids [ec5BqwZBT6mqOyI2FUMHRg, mMLUEpWpQKyPnfHXZxXQ8g]\r\n      1: p_term [1], isa_ids [ugIDHeyeTRi5WgN4oHhNkg, uvo_rzB0Rf6Kym-B3b-Mug]\r\n      2: p_term [1], isa_ids [WUFjxQ7FRUmVyO_grRCvlw, 9bdchFBJScW7elppTWRcGw]\r\nmetadata customs:\r\n   index-graveyard: IndexGraveyard[[]]\r\nnodes (node-left generation: 0):\r\n   {hot_2}{hot_2}{-C5v6i-kQaKXIdyHp7lvLQ}{8Oj_LkMHRHSeI94ly3SrgA}{0.0.0.0}{0.0.0.0:17}{h}{8.13.0}{7000099-8502001}\r\n   {hot_1}{hot_1}{6DDHiw7mQni9TnpdunIsvA}{ln2XChMBReuj9Xq2UTlUrA}{0.0.0.0}{0.0.0.0:16}{h}{8.13.0}{7000099-8502001}\r\n   {hot_0}{hot_0}{MNeDvWzdQWeGkAUl6b1vUw}{oyc-_bOQTcKLAVGP3oPGVg}{0.0.0.0}{0.0.0.0:15}{h}{8.13.0}{7000099-8502001}\r\n   {warm_2}{warm_2}{-OMQMMfkSZePnORRIGipqw}{8SS8SE2CRfSVlzVMLGjkvQ}{0.0.0.0}{0.0.0.0:20}{w}{8.13.0}{7000099-8502001}\r\n   {warm_1}{warm_1}{uE53YF6zRgqx9sdjPpAM9g}{j9D8Dv54TX-BlgTKfY0XmQ}{0.0.0.0}{0.0.0.0:19}{w}{8.13.0}{7000099-8502001}\r\n   {warm_0}{warm_0}{Nx8BScO6QMCVfILKmGBPxQ}{1O0LsISXSh2_lNMq6DhnIw}{0.0.0.0}{0.0.0.0:18}{w}{8.13.0}{7000099-8502001}\r\ncluster features:\r\n   hot_2: []\r\n   hot_1: []\r\n   hot_0: []\r\n   warm_2: []\r\n   warm_1: []\r\n   warm_0: []\r\nrouting_table (version 0):\r\n-- index [[test-3]]\r\n----shard_id [test-3][0]\r\n--------[test-3][0], node[hot_0], [R], s[STARTED], a[id=mMLUEpWpQKyPnfHXZxXQ8g], failed_attempts[0]\r\n--------[test-3][0], node[hot_2], [P], s[STARTED], a[id=ec5BqwZBT6mqOyI2FUMHRg], failed_attempts[0]\r\n----shard_id [test-3][1]\r\n--------[test-3][1], node[hot_0], [P], s[STARTED], a[id=uvo_rzB0Rf6Kym-B3b-Mug], failed_attempts[0]\r\n--------[test-3][1], node[hot_2], relocating [hot_1], [R], s[RELOCATING], a[id=ugIDHeyeTRi5WgN4oHhNkg, rId=Xfhb5zH0RdqN2tpmxTVuVg], failed_attempts[0], expected_shard_size[1]\r\n----shard_id [test-3][2]\r\n--------[test-3][2], node[hot_1], [R], s[STARTED], a[id=9bdchFBJScW7elppTWRcGw], failed_attempts[0]\r\n--------[test-3][2], node[hot_2], [P], s[STARTED], a[id=WUFjxQ7FRUmVyO_grRCvlw], failed_attempts[0]\r\n\r\nrouting_nodes:\r\n-----node_id[hot_1][V]\r\n--------[test-3][1], node[hot_1], relocating [hot_2], [R], recovery_source[peer recovery], s[INITIALIZING], a[id=Xfhb5zH0RdqN2tpmxTVuVg, rId=ugIDHeyeTRi5WgN4oHhNkg], failed_attempts[0], expected_shard_size[1]\r\n--------[test-3][2], node[hot_1], [R], s[STARTED], a[id=9bdchFBJScW7elppTWRcGw], failed_attempts[0]\r\n-----node_id[hot_0][V]\r\n--------[test-3][0], node[hot_0], [R], s[STARTED], a[id=mMLUEpWpQKyPnfHXZxXQ8g], failed_attempts[0]\r\n--------[test-3][1], node[hot_0], [P], s[STARTED], a[id=uvo_rzB0Rf6Kym-B3b-Mug], failed_attempts[0]\r\n-----node_id[warm_2][V]\r\n-----node_id[warm_1][V]\r\n-----node_id[warm_0][V]\r\n-----node_id[hot_2][V]\r\n--------[test-3][0], node[hot_2], [P], s[STARTED], a[id=ec5BqwZBT6mqOyI2FUMHRg], failed_attempts[0]\r\n--------[test-3][1], node[hot_2], relocating [hot_1], [R], s[RELOCATING], a[id=ugIDHeyeTRi5WgN4oHhNkg, rId=Xfhb5zH0RdqN2tpmxTVuVg], failed_attempts[0], expected_shard_size[1]\r\n--------[test-3][2], node[hot_2], [P], s[STARTED], a[id=WUFjxQ7FRUmVyO_grRCvlw], failed_attempts[0]\r\n---- unassigned\r\n>\r\n\r\n  at __randomizedtesting.SeedInfo.seed([2BCE202E19A4B5D1:1386B774A1EB3848]:0)\r\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\r\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\r\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2119)\r\n  at org.elasticsearch.xpack.autoscaling.storage.ReactiveStorageDeciderDecisionTests.testStoragePreventsAllocation(ReactiveStorageDeciderDecisionTests.java:235)\r\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\r\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:568)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:833)\r\n\r\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">test-failure",":Distributed\/Autoscaling","Team:Distributed","medium-risk"]},{"title":"[CI] InferenceIngestIT testPipelineIngestWithModelAliases failing","body":"The test failed in the single-processor-node-tests pipeline\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/g2ys6irgpclr4\/tests\/:x-pack:plugin:ml:qa:native-multi-node-tests:javaRestTest\/org.elasticsearch.xpack.ml.integration.InferenceIngestIT\/testPipelineIngestWithModelAliases\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ml:qa:native-multi-node-tests:javaRestTest' --tests \"org.elasticsearch.xpack.ml.integration.InferenceIngestIT.testPipelineIngestWithModelAliases\" -Dtests.seed=F41853BAD2E190FF -Dtests.configure_test_clusters_with_one_processor=true -Dtests.locale=et -Dtests.timezone=America\/Asuncion -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.ml.integration.InferenceIngestIT#testPipelineIngestWithModelAliases`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testPipelineIngestWithModelAliases'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.ml.integration.InferenceIngestIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.client.ResponseException: method [GET], host [http:\/\/[::1]:44003], URI [\/_ml\/trained_models\/_stats?size=10000], status line [HTTP\/1.1 500 Internal Server Error]\n{\"error\":{\"root_cause\":[],\"type\":\"exception\",\"reason\":\"Searching for stats for models [lang_ident_model_1,test_regression_2,test_regression_1] failed\",\"caused_by\":{\"type\":\"search_phase_execution_exception\",\"reason\":\"\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[],\"caused_by\":{\"type\":\"search_phase_execution_exception\",\"reason\":\"Search rejected due to missing shards [[.ml-stats-000001][0]]. Consider using `allow_partial_search_results` setting to bypass this error.\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[]}}},\"status\":500}\n\n  at __randomizedtesting.SeedInfo.seed([F41853BAD2E190FF:609C4D14DE135688]:0)\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:351)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\n  at org.elasticsearch.xpack.core.ml.integration.MlRestTestStateCleaner.deleteAllTrainedModelIngestPipelines(MlRestTestStateCleaner.java:42)\n  at org.elasticsearch.xpack.core.ml.integration.MlRestTestStateCleaner.resetFeatures(MlRestTestStateCleaner.java:33)\n  at org.elasticsearch.xpack.ml.integration.InferenceIngestIT.cleanUpData(InferenceIngestIT.java:78)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","This is failing during post-test cleanup.  The call that's failing is: https:\/\/github.com\/elastic\/elasticsearch\/blob\/7be2df922c5d3c0c1eca5da170d1352f7f6d7326\/x-pack\/plugin\/core\/src\/test\/java\/org\/elasticsearch\/xpack\/core\/ml\/integration\/MlRestTestStateCleaner.java#L40\r\n\r\nProbably what we should do is have our stats endpoints wait at least a couple of seconds for yellow status on the stats indices they want to search. If those indices are still unavailable after the short wait then these endpoints can fail like they do now. But waiting a small amount of time might help avoid these occasional failures.\r\n\r\nMarking as `low-risk` as the code involved here hasn't changed for years."],"labels":[">test-failure",":ml","Team:ML","low-risk"]},{"title":"Watcher executes on a node after the .watches shard has been moved to a different node","body":"### Elasticsearch Version\n\n8.8.2 (but likely others as well)\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nn\/a\n\n### Problem Description\n\nThis happens only rarely, but I have seen evidence of it twice in the same cluster a few days apart. As part of normal shard reallocation, a `.watches` shard gets moved off of a node (I'll call it `nodeA`) and onto another node (`nodeB`). This ought to mean that nodeA stops running watches. And we see in nodeA's logs:\r\n```\r\npaused watch execution, reason [no local watcher shards found], cancelled [0] queued tasks\r\n```\r\nBut if we search `.watcher-history-*`, we see that now both nodeA and nodeB are executing the same watch, at nearly the same time on the same schedule. So instead of getting executed once every 10 minutes (for example), the watch gets executed twice every 10 minutes.\r\nAside from the message above, I haven't seen anything relevant in the logs. Restarting the nodes solves the problem.\n\n### Steps to Reproduce\n\nUnknown\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Also, my initial thought was just that since watcher history records are written asynchronously, there could be a little lag. But it goes on for days until the cluster is restarted, with the same timestamps as the watcher history records from nodeB."],"labels":[">bug",":Data Management\/Watcher","Team:Data Management"]},{"title":"Retention lease sync actions should not trip circuit breakers","body":"Today `RetentionLeaseSyncAction` and `RetentionLeaseBackgroundSyncAction` are both subject to circuit-breaker and indexing pressure checks, and in particular can fail the primary if such a request were to fail. However these actions don't really need any resources so it'd seem better to me if we didn't fail the primary in this situation.\r\n\r\nSpecifically, I think we should set `forceExecutionOnPrimary` when calling the superclass constructor (which overrides the indexing pressure checks too, see `org.elasticsearch.action.support.replication.TransportWriteAction#force`).","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/CRUD","Team:Distributed"]},{"title":"Data race condition in automaton queries","body":"### Elasticsearch Version\r\n\r\n8.12.1\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\narchlinux 6.7.8\r\n\r\n### Problem Description\r\n\r\nFirst described in the post https:\/\/discuss.elastic.co\/t\/data-race-condition-in-automaton-queries\/353937?u=yfful there seems to be a data race condition when evaluating automaton queries like `regexp`. In a local unit test involving a runtime field and a regexp query, I have experienced search inconsistencies with the result count. \r\n\r\n### Steps to Reproduce\r\n\r\nThe query shown below uses the script `parity` which returns `even` or `odd` depending on the value of another numeric field. It returns sometimes an unexpected number of hits. \r\n\r\n```json\r\n{\r\n  \"query\": {\r\n    \"regexp\": {\r\n      \"outer_parity\": {\r\n        \"value\": \"e.e.\"\r\n      }\r\n    }\r\n  },\r\n  \"runtime_mappings\": {\r\n    \"outer_parity\": {\r\n      \"type\": \"keyword\",\r\n      \"script\": {\r\n        \"lang\": \"painless\",\r\n        \"source\": \"parity\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI believe that following this [change](https:\/\/github.com\/elastic\/elasticsearch\/pull\/101230) that enabled parallelization by default, the queries extending `AbstractStringScriptFieldAutomatonQuery` have a data race condition. If I understand that change correctly, then the code below can be called concurrently:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/f0e43173eac4549ed78375b991ebd61c0cc13d2e\/server\/src\/main\/java\/org\/elasticsearch\/search\/runtime\/AbstractScriptFieldQuery.java#L71-L77\r\n\r\nTherefore, the `BytesRefBuilder` scratch below is shared by all threads that execute a search on different segments, which would lead to the race condition I am seeing. With the query shared above, the `scratch` variable would contain `even` for example, although the `values` list passed in argument contains only `odd`. The race condition would explain this inconsistency.\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/f0e43173eac4549ed78375b991ebd61c0cc13d2e\/server\/src\/main\/java\/org\/elasticsearch\/search\/runtime\/AbstractStringScriptFieldAutomatonQuery.java#L20\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">bug",":Search\/Search","Team:Search"]},{"title":"[CI] ResolveClusterActionResponseTests testEqualsAndHashcode failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/o4dwpco7p25fw\/tests\/:server:test\/org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponseTests\/testEqualsAndHashcode\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':server:test' --tests \"org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponseTests.testEqualsAndHashcode\" -Dtests.seed=E7520D27252DAE83 -Dtests.locale=el -Dtests.timezone=Indian\/Chagos -Druntime.java=21\r\n```\r\n\r\n**Applicable branches:**\r\n8.13\r\n\r\n**Reproduces locally?:**\r\nDidn't try\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponseTests#testEqualsAndHashcode`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testEqualsAndHashcode'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponseTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.AssertionError: ResolveClusterActionResponse mutation should not be equal to original\r\nExpected: not <org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponse@47fbffe4>\r\n     but: was <org.elasticsearch.action.admin.indices.resolve.ResolveClusterActionResponse@47fbffe4>\r\n\r\n  at __randomizedtesting.SeedInfo.seed([E7520D27252DAE83:965D75EAEACAE7AC]:0)\r\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\r\n  at org.junit.Assert.assertThat(Assert.java:964)\r\n  at org.elasticsearch.test.EqualsHashCodeTestUtils.checkEqualsAndHashCode(EqualsHashCodeTestUtils.java:95)\r\n  at org.elasticsearch.test.AbstractWireTestCase.testEqualsAndHashcode(AbstractWireTestCase.java:60)\r\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n  at java.lang.reflect.Method.invoke(Method.java:580)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:1583)\r\n\r\n```","comments":["https:\/\/buildkite.com\/elastic\/elasticsearch-periodic\/builds\/1937#018dfd92-7e08-4565-afcb-89d3f6d0776a","This is still failing in 8.13: https:\/\/gradle-enterprise.elastic.co\/s\/yth53wvgsyn6g\r\nMaybe it needs to be backported?"],"labels":[">test-failure","needs:triage","low-risk"]},{"title":"[ILM] a TSDS with a policy without rollover does never delete the data stream","body":"### Elasticsearch Version\n\n8.10 onwards\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nDarwin\n\n### Problem Description\n\nA time series data stream (TSDS) only managed by a data stream without rollover will be unable to make progress in actions that require the backing indices to have moved past [their time bounds](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/tsds.html#time-bound-indices).\r\n\r\nThe simplest example if having an ILM policy with just the `delete` phase and notice, as the TSDS is never rolled over, as the write index of the TSDS reaches the delete phase but gets \"stuck\" in the `check-ts-end-time-passed` step as the end time of the write index gets \"locked in\" on rollover, so ILM can never move past this step to actually delete the index (and data stream itself, as it only has one index) \r\n\r\nAs the data stream is past its retention, one solution here is to check if we're in `check-ts-end-time-passed` in the `delete` action, and if that's the case just skip the step to allow for the data stream to be deleted. \r\n\r\nThe workaround here is to use a rollover action in the hot phase of the ILM policy. \n\n### Steps to Reproduce\n\nSet up a TSDS as described here https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/set-up-tsds.html\r\nbut instead of the ILM policy advertised in the documentation use one that only has a delete phase e.g.\r\n```\r\n\r\n\r\nPUT _ilm\/policy\/my-weather-sensor-lifecycle-policy\r\n{\r\n  \"policy\": {\r\n    \"phases\": {\r\n      \"delete\": {\r\n        \"min_age\": \"2m\",\r\n        \"actions\": {\r\n          \"delete\": {}\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">bug",":Data Management\/ILM+SLM","Team:Data Management"]},{"title":"Remove duplicate ForUtil","body":"We copied this into our codebase twice. The version with customizable block size is unnecessary, we always use the same block size anyway. => no need to duplicate 1k lines.\r\n","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">non-issue",":Analytics\/Aggregations","Team:Analytics","v8.14.0"]},{"title":"NOOP update  _bulk operation, the noop_update_total is updated","body":"When a no-op update occurs during a _bulk operation, the noop_update_total is updated.\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/105742\r\n","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/CRUD","Team:Distributed","external-contributor","v8.14.0"]},{"title":"[CI] JdbcResultSetIT testMultiValueFieldWithMultiValueLeniencyDisabled failing","body":"Intermitted failures since 2024-02-27.\nLikely related to #105867 and #105866.\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/xeluctyewddbs\/tests\/:x-pack:plugin:sql:qa:jdbc:single-node:v7.9.3%23bwcTest\/org.elasticsearch.xpack.sql.qa.jdbc.single_node.JdbcResultSetIT\/testMultiValueFieldWithMultiValueLeniencyDisabled\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:sql:qa:jdbc:single-node:v7.9.3#bwcTest' -Dtests.class=\"org.elasticsearch.xpack.sql.qa.jdbc.single_node.JdbcResultSetIT\" -Dtests.method=\"testMultiValueFieldWithMultiValueLeniencyDisabled\" -Dtests.seed=8CE132721CF11CC2 -Dtests.bwc=true -Dtests.locale=ru-RU -Dtests.timezone=America\/Panama -Druntime.java=21\n```\n\n**Applicable branches:**\n7.17\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.sql.qa.jdbc.single_node.JdbcResultSetIT#testMultiValueFieldWithMultiValueLeniencyDisabled`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testMultiValueFieldWithMultiValueLeniencyDisabled'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.sql.qa.jdbc.single_node.JdbcResultSetIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: (No message provided)\n\n  at __randomizedtesting.SeedInfo.seed([8CE132721CF11CC2:25D832F63FEEE99]:0)\n  at org.junit.Assert.fail(Assert.java:87)\n  at org.junit.Assert.assertTrue(Assert.java:42)\n  at org.junit.Assert.assertTrue(Assert.java:53)\n  at org.elasticsearch.xpack.sql.qa.jdbc.ResultSetTestCase.testMultiValueFieldWithMultiValueLeniencyDisabled(ResultSetTestCase.java:138)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)\n  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)\n  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)\n  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)\n  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","Pinging @elastic\/es-analytical-engine (Team:Analytics)","Sorry for the ping `es-distributed` team, too many tabs open"],"labels":["blocker",">test-failure",":Analytics\/SQL","Team:Analytics"]},{"title":"[CI] CrossClusterAsyncSearchIT testClusterDetailsAfterCCSWithFailuresOnAllShards failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/b3o77yrofkc6i\/tests\/:x-pack:plugin:async-search:internalClusterTest\/org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT\/testClusterDetailsAfterCCSWithFailuresOnAllShards\n\n\n**Reproduction line:**\n```\ngradlew ':x-pack:plugin:async-search:internalClusterTest' --tests \"org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT.testClusterDetailsAfterCCSWithFailuresOnAllShards\" -Dtests.seed=631462E691F58763 -Dtests.locale=ca-ES -Dtests.timezone=Australia\/NSW -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT#testClusterDetailsAfterCCSWithFailuresOnAllShards`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testClusterDetailsAfterCCSWithFailuresOnAllShards'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: should have 'index corrupted' in reason\n\n  at __randomizedtesting.SeedInfo.seed([631462E691F58763:A50FF6118371EE18]:0)\n  at org.junit.Assert.fail(Assert.java:88)\n  at org.junit.Assert.assertTrue(Assert.java:41)\n  at org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT.assertAllShardsFailed(CrossClusterAsyncSearchIT.java:1595)\n  at org.elasticsearch.xpack.search.CrossClusterAsyncSearchIT.testClusterDetailsAfterCCSWithFailuresOnAllShards(CrossClusterAsyncSearchIT.java:449)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[":Search\/Search",">test-failure","Team:Search","low-risk"]},{"title":"[CI] AzureRepositoriesMeteringIT multiple tests failing failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testStatsAreArchivedAfterRepositoryDeletion\r\n\r\n**Test failures**\r\n[elasticsearch \/ periodic \/ 8.13 \/ third-party \/ azure](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc)\r\nTest failures:\r\n[AzureRepositoriesMeteringIT \u00bb testClearRepositoriesStats](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testClearRepositoriesStats)\r\n[AzureRepositoriesMeteringIT \u00bb testStatsAreTracked](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testStatsAreTracked)\r\n[AzureRepositoriesMeteringIT \u00bb testRegisterMultipleRepositoriesAndGetStats](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testRegisterMultipleRepositoriesAndGetStats)\r\n[AzureRepositoriesMeteringIT \u00bb testDeleteThenAddRepositoryWithTheSameName](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testDeleteThenAddRepositoryWithTheSameName)\r\n[AzureRepositoriesMeteringIT \u00bb testStatsAreArchivedAfterRepositoryDeletion](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testStatsAreArchivedAfterRepositoryDeletion)\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest' --tests \"org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT.testStatsAreArchivedAfterRepositoryDeletion\" -Dtests.seed=35579160B1CCDAFB -Dtests.locale=lt -Dtests.timezone=Etc\/GMT+9 -Druntime.java=21\r\n```\r\n\r\n**Applicable branches:**\r\n8.13\r\n\r\n**Reproduces locally?:**\r\nNo\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT#testStatsAreArchivedAfterRepositoryDeletion`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testStatsAreArchivedAfterRepositoryDeletion'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\norg.elasticsearch.client.ResponseException: method [POST], host [http:\/\/[::1]:35993], URI [_snapshot\/yvwzjtrfot\/botvekesxg\/_restore?wait_for_completion=true], status line [HTTP\/1.1 500 Internal Server Error]\r\n{\"error\":{\"root_cause\":[{\"type\":\"snapshot_restore_exception\",\"reason\":\"[yvwzjtrfot:botvekesxg\/9xSEHhi-RyehlqqA0XEQ9A] cannot restore index [bqkleqgcdm] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\"}],\"type\":\"snapshot_restore_exception\",\"reason\":\"[yvwzjtrfot:botvekesxg\/9xSEHhi-RyehlqqA0XEQ9A] cannot restore index [bqkleqgcdm] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\"},\"status\":500}\r\n\r\n  at __randomizedtesting.SeedInfo.seed([35579160B1CCDAFB:417ABC912FB90BDB]:0)\r\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:351)\r\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\r\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\r\n  at org.elasticsearch.test.rest.ESRestTestCase.restoreSnapshot(ESRestTestCase.java:1990)\r\n  at org.elasticsearch.xpack.repositories.metering.AbstractRepositoriesMeteringAPIRestTestCase.snapshotAndRestoreIndex(AbstractRepositoriesMeteringAPIRestTestCase.java:252)\r\n  at org.elasticsearch.xpack.repositories.metering.AbstractRepositoriesMeteringAPIRestTestCase.snapshotAndRestoreIndex(AbstractRepositoriesMeteringAPIRestTestCase.java:214)\r\n  at org.elasticsearch.xpack.repositories.metering.AbstractRepositoriesMeteringAPIRestTestCase.testStatsAreArchivedAfterRepositoryDeletion(AbstractRepositoriesMeteringAPIRestTestCase.java:144)\r\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n  at java.lang.reflect.Method.invoke(Method.java:580)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\r\n  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:1583)\r\n\r\n```","comments":["similar:\r\n[AzureRepositoriesMeteringIT \u00bb testDeleteThenAddRepositoryWithTheSameName](https:\/\/gradle-enterprise.elastic.co\/s\/bjvkojojf4ekc\/tests\/:x-pack:plugin:repositories-metering-api:qa:azure:javaRestTest\/org.elasticsearch.xpack.repositories.metering.azure.AzureRepositoriesMeteringIT\/testDeleteThenAddRepositoryWithTheSameName)","Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[":Distributed\/Snapshot\/Restore",">test-failure","Team:Distributed","medium-risk"]},{"title":"[CI] ConnectorSyncJobIndexServiceTests testGetConnectorSyncJob failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/peqyooa4g26v4\/tests\/:x-pack:plugin:ent-search:test\/org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests\/testGetConnectorSyncJob\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ent-search:test' --tests \"org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.testGetConnectorSyncJob\" -Dtests.seed=B88251B605F1B331 -Dtests.locale=is-IS -Dtests.timezone=Australia\/Canberra -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests#testGetConnectorSyncJob`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testGetConnectorSyncJob'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.util.concurrent.ExecutionException: org.elasticsearch.index.mapper.DocumentParsingException: [1:3169] failed to parse field [last_incremental_sync_scheduled_at] of type [date] in document with id 'jp1P-o0Bj5nJ04GRDtd7'. Preview of field's value: '-142315230-10-21T08:40:32.454Z'\n\n  at __randomizedtesting.SeedInfo.seed([B88251B605F1B331:E50FF544876A9488]:0)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.getValue(PlainActionFuture.java:336)\n  at org.elasticsearch.action.support.PlainActionFuture$Sync.get(PlainActionFuture.java:310)\n  at org.elasticsearch.action.support.PlainActionFuture.get(PlainActionFuture.java:69)\n  at org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.createConnector(ConnectorSyncJobIndexServiceTests.java:90)\n  at org.elasticsearch.xpack.application.connector.syncjob.ConnectorSyncJobIndexServiceTests.setup(ConnectorSyncJobIndexServiceTests.java:75)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n  Caused by: org.elasticsearch.index.mapper.DocumentParsingException: [1:3169] failed to parse field [last_incremental_sync_scheduled_at] of type [date] in document with id 'jp1P-o0Bj5nJ04GRDtd7'. Preview of field's value: '-142315230-10-21T08:40:32.454Z'\n\n    at org.elasticsearch.index.mapper.FieldMapper.rethrowAsDocumentParsingException(FieldMapper.java:232)\n    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:185)\n    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n    at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n    at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n    at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n    at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    at java.lang.Thread.run(Thread.java:1583)\n\n    Caused by: java.lang.IllegalArgumentException: failed to parse date field [-142315230-10-21T08:40:32.454Z] with format [strict_date_optional_time||epoch_millis]\n\n      at org.elasticsearch.common.time.JavaDateFormatter.parse(JavaDateFormatter.java:211)\n      at org.elasticsearch.index.mapper.DateFieldMapper$DateFieldType.parse(DateFieldMapper.java:490)\n      at org.elasticsearch.index.mapper.DateFieldMapper.parseCreateField(DateFieldMapper.java:910)\n      at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:183)\n      at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n      at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n      at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n      at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n      at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n      at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n      at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n      at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n      at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n      at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n      at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n      at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n      at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n      at java.lang.Thread.run(Thread.java:1583)\n\n      Caused by: java.time.format.DateTimeParseException: Failed to parse with all enclosed parsers\n\n        at org.elasticsearch.common.time.JavaDateFormatter.doParse(JavaDateFormatter.java:238)\n        at org.elasticsearch.common.time.JavaDateFormatter.parse(JavaDateFormatter.java:209)\n        at org.elasticsearch.index.mapper.DateFieldMapper$DateFieldType.parse(DateFieldMapper.java:490)\n        at org.elasticsearch.index.mapper.DateFieldMapper.parseCreateField(DateFieldMapper.java:910)\n        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:183)\n        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:411)\n        at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:642)\n        at org.elasticsearch.index.mapper.DocumentParser.innerParseObject(DocumentParser.java:342)\n        at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrNested(DocumentParser.java:293)\n        at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:144)\n        at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:92)\n        at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1012)\n        at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:960)\n        at org.elasticsearch.index.shard.IndexShard.applyIndexOperationOnPrimary(IndexShard.java:904)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest(TransportShardBulkAction.java:360)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction$2.doRun(TransportShardBulkAction.java:224)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.performOnPrimary(TransportShardBulkAction.java:291)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:142)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.dispatchedShardOperationOnPrimary(TransportShardBulkAction.java:74)\n        at org.elasticsearch.action.support.replication.TransportWriteAction$1.doRun(TransportWriteAction.java:216)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:33)\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:983)\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n        at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ent-search-eng (Team:Enterprise Search)"],"labels":["blocker",">test-failure",":EnterpriseSearch\/Application","Team:Enterprise Search"]},{"title":"[CI] ExchangeServiceTests testFailToRespondPage failing","body":"Very rare test failures on main \/ 8.13, but can be reproduced if running repeatedly.\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/wbfwdmkyleiz6\/tests\/:x-pack:plugin:esql:compute:test\/org.elasticsearch.compute.operator.exchange.ExchangeServiceTests\/testFailToRespondPage\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:esql:compute:test' --tests \"org.elasticsearch.compute.operator.exchange.ExchangeServiceTests.testFailToRespondPage\" -Dtests.seed=977CE4E8E1277638 -Dtests.locale=es-BO -Dtests.timezone=America\/Marigot -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nYes\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.compute.operator.exchange.ExchangeServiceTests#testFailToRespondPage`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testFailToRespondPage'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.compute.operator.exchange.ExchangeServiceTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: <0L>\n     but: was <96L>\n\n  at __randomizedtesting.SeedInfo.seed([977CE4E8E1277638:5C33FACD3910DBCF]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2119)\n  at org.elasticsearch.compute.operator.exchange.ExchangeServiceTests.allMemoryReleased(ExchangeServiceTests.java:509)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">test-failure","Team:Analytics",":Analytics\/ES|QL","low-risk"]},{"title":"Gradle throws exception snapshotting task inputs","body":"We've started seeing [the following error](https:\/\/gradle-enterprise.elastic.co\/scans\/failures?failures.failureClassification=all_failures&failures.failureMessage=Execution%20failed%20for%20task%20%27*%27.%0A%3E%20Expected%20a%20previously%20known%20directory%20snapshot%20at%20%2F*&search.relativeStartTime=P28D&search.timeZoneId=Europe%2FPrague) on the `7.17` branch following the Gradle 8.6 upgrade:\r\n\r\n```\r\n* What went wrong:\r\nExecution failed for task ':test:fixtures:krb5kdc-fixture:postProcessFixture'.\r\n> Expected a previously known directory snapshot at \/dev\/shm\/bk\/bk-agent-prod-gcp-1709222143308594389\/elastic\/elasticsearch-intake\/test\/fixtures\/krb5kdc-fixture\/testfixtures_shared\/shared\/hdfs\/keytabs but got MissingFileSnapshot\/keytabs\r\n```\r\n\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/hddvjcb5a2pje\/failure","comments":["@breskeby Let's reach out to the Gradle folks. This sounds like some kind of regression and maybe we can get it fixed for 8.7.","Pinging @elastic\/es-delivery (Team:Delivery)","There's another one of these on #105862 \r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/asv5telo5km74"],"labels":[":Delivery\/Build","Team:Delivery","low-risk"]},{"title":"Update Gradle wrapper to 8.7-rc-4","body":"Update Gradle wrapper to 8.7-rc-3","comments":["just a draft till 8.7 is released"],"labels":[":Delivery\/Build","Team:Delivery","v8.14.0"]},{"title":"Example of Time Zone in 8.2 documentation seems not right","body":"### Description\r\n\r\nIn Elasticsearch 8.2 [documentation](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-aggregations-bucket-datehistogram-aggregation.html#datehistogram-aggregation-time-zone), specifically in the Date Histogram Aggregation section, it seems there might be an error in the time zone conversion\u3002\r\n\r\n![1](https:\/\/github.com\/elastic\/elasticsearch\/assets\/6985548\/8019d9b7-7df9-408e-909b-726ace6c311a)\r\n\r\n\r\nsince `America\/New_York` is in UTC-5, Does this converting is not right? If I'm mistaken, could you please correct me?\r\n\r\nBTW, this kind issuse seems not belong here. sorry for that.","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">docs",":Analytics\/Aggregations","Team:Docs","Team:Analytics"]},{"title":"`ProactiveStorageDeciderService#scale` throws `IndexNotFoundException` when simulating the rollover of a closed index","body":"`ProactiveStorageDeciderService#scale` attempts to forecast disk usage by simulating rollovers of data streams, which it does by copying the index metadata of the current write index...\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/067eb34aeec38b3349b9dff78980170fbcd04743\/x-pack\/plugin\/autoscaling\/src\/main\/java\/org\/elasticsearch\/xpack\/autoscaling\/storage\/ReactiveStorageDeciderService.java#L826-L829\r\n\r\n... and then adding its shards to the routing table ...\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/067eb34aeec38b3349b9dff78980170fbcd04743\/x-pack\/plugin\/autoscaling\/src\/main\/java\/org\/elasticsearch\/xpack\/autoscaling\/storage\/ReactiveStorageDeciderService.java#L702-L704\r\n\r\n... but `RoutingTable.Builder#addAsNew` doesn't create shards for closed indices ...\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/997fbb372cdec82eb803d32de5147dbccec21e64\/server\/src\/main\/java\/org\/elasticsearch\/cluster\/routing\/RoutingTable.java#L501-L510\r\n\r\n... and that makes us throw an INFE here:\r\n\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/067eb34aeec38b3349b9dff78980170fbcd04743\/x-pack\/plugin\/autoscaling\/src\/main\/java\/org\/elasticsearch\/xpack\/autoscaling\/storage\/ReactiveStorageDeciderService.java#L713\r\n\r\n---\r\n\r\n### Workaround\r\n\r\nOpen any closed write indices.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Autoscaling","Team:Distributed"]},{"title":"Add Range*Handler java docs","body":"As title suggests, add basic javadocs for RangeAvailableHandler and RangeMissingHandler","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">non-issue",":Distributed\/Distributed","Team:Distributed","v8.14.0"]},{"title":"[CI] RandomSamplerIT testRandomSamplerConsistentSeed failing","body":"Did not reproduce locally, so I did not mute it. Labled it Search\/Vectors, because this test was recently added by Ben.\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/jdg6o7b6k7cg6\/tests\/:server:internalClusterTest\/org.elasticsearch.search.aggregations.bucket.RandomSamplerIT\/testRandomSamplerConsistentSeed\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.search.aggregations.bucket.RandomSamplerIT.testRandomSamplerConsistentSeed\" -Dtests.seed=FE58ABAC7FAB9A68 -Dtests.locale=ja-JP -Dtests.timezone=America\/Buenos_Aires -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.search.aggregations.bucket.RandomSamplerIT#testRandomSamplerConsistentSeed`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testRandomSamplerConsistentSeed'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.search.aggregations.bucket.RandomSamplerIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: <3642.808705180804>\n     but: was <3656.327664597294>\n\n  at __randomizedtesting.SeedInfo.seed([FE58ABAC7FAB9A68:DF0801823F88134D]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2120)\n  at org.elasticsearch.search.aggregations.bucket.RandomSamplerIT.lambda$testRandomSamplerConsistentSeed$1(RandomSamplerIT.java:122)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertResponse(ElasticsearchAssertions.java:375)\n  at org.elasticsearch.search.aggregations.bucket.RandomSamplerIT.testRandomSamplerConsistentSeed(RandomSamplerIT.java:111)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)","Another failure:\r\n\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/m3npn7ov53p7k\/tests\/:server:internalClusterTest\/org.elasticsearch.search.aggregations.bucket.RandomSamplerIT\/testRandomSamplerConsistentSeed","Pinging @elastic\/es-analytical-engine (Team:Analytics)","This isn't really a blocker, its a testing consistency failure. Downgrading severity."],"labels":[":Analytics\/Aggregations",">test-failure","Team:Analytics","low-risk"]},{"title":"Allow custom realm to handle basic auth with additional headers (ReservedRealm parses all basic auth)","body":"### Description\n\nHi \r\nI am bringing up a hinderance with implementing custom realms using a basic auth header. \r\n\r\n**Problem**\r\n\r\nBasically, the [ReservedRealm ](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/security\/src\/main\/java\/org\/elasticsearch\/xpack\/security\/authc\/esnative\/ReservedRealm.java) will parse any Authorization: Basic XXX Header and create a UsernamePasswordToken because it is a CachingUsernamePasswordRealm. \r\n\r\nThis is a problem for custom realms that also use basic auth but need to read an additional header for context. Because the reserved realm is always first and will always create the token the custom realm has never access to ThreadContext (i.e. the HTTP headers).\r\n\r\nIn my concrete case, I would need to read a second header e.g. \"X-Custom-Profile\" to determine which set of roles the user gets based on the selection of his profile. However, my token method is never called because the ReservedRealm creates the token based on the basic auth. So I can never read that header value and create a custom Token format. \r\n\r\nThis problem was already mentioned 6 years ago in the forums by one of the moderators (https:\/\/discuss.elastic.co\/t\/custom-realm-building-usernamepasswordtoken\/82699\/3). \r\n\r\n**Proposed Solution**\r\nMy preferred solution would be that ReservedRealm does not return a token unless the principal is one of the built-in users it is supposed to handle.  This could be implemented by overwriting the token method it inherits and then calling the super class method and in the end just checking the principal whether it should return the token or return null.\r\n\r\nWhy I prefer this: it does not need any change in how Realms work. The problem does not exist with other realms because the custom realm can just be set to come first in order and it will create the token. (This is not possible for ReservedRealm as this one is hard coded to come first and cannot be disabled.)\r\n\r\n**Alternative Solution**\r\nAs mentioned by the mod in his answer one could simply make sure the Realm has access to all headers or the ThreadContext in the authenticate method. Big disadvantage is that this is probably a breaking change for all realms while the unsolvable problem is only in the ReservedRealm. \r\n\r\n**Possible Workaround Without Changes**\r\nThe only workaround is using a non-standard header name or authentication scheme. However, at my company this is not simple, because there are concerns about middleware either logging a non-standard  header or balking at a non-standard authorization scheme. So it's not clear that this is actually feasible in all environments.\r\n\r\n\r\n\r\n","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">enhancement",":Security\/Authentication","Team:Security"]},{"title":"Prioritise movement of non-write-index shards where possible","body":"Moving the shard of a write index is typically more expensive and disruptive than moving shards of other indices. Can we make any changes to the desired balance computation so that it prefers to keep shards of write indices where they are, instead using movements of other shards to achieve a balanced cluster.\r\n\r\nRelates #68513","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">enhancement",":Distributed\/Allocation","team-discuss","Team:Distributed"]},{"title":"Publish desired balance after assigning new shards","body":"Today we always run the desired balance computation until it converges or is superseded by a new cluster state. If we create some new shards, they start out unassigned, and will wait for the desired balance computation to complete before they can be assigned. This could be quite some time to wait. Should we instead notice that we assigned some unassigned shards during the computation and, if so, publish the intervening state for reconcililation to start while the computation continues?","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">enhancement",":Distributed\/Allocation","team-discuss","Team:Distributed"]},{"title":"`cluster:admin\/snapshot\/mount` does not link to its child `cluster:admin\/snapshot\/restore` task","body":"Mounting a searchable snapshot triggers a snapshot restore, but the snapshot restore task does not record the mount task as its parent. We should avoid orphan tasks like this by specifying the parent task ID in the restore request created within `TransportMountSearchableSnapshotAction`.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Snapshot\/Restore","Team:Distributed","Supportability"]},{"title":"Log shard movements","body":"Related to: #60747","comments":["Hi @idegtiarenko, I've created a changelog YAML for you.","Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">enhancement",":Distributed\/Allocation","Team:Distributed","v8.14.0"]},{"title":"ESQL: ProjectAwayColumns rule not handling synthetic attribute correctly","body":"The logical optimizer rules `PushdownEval`, `PushdownRegexExtract` and `PushdownEnrich` create temporary attributes when an `EVAL`, `GROK`\/`DISSECT` or `ENRICH` is pushed past a `SORT` with conflicting names; e.g. `SORT x | EVAL x = y` essentially becomes `EVAL $$x = x, x = y | SORT $$x | DROP $$x`.\r\n\r\nThe temporary attribute `$$x` should be [synthetic](https:\/\/github.com\/elastic\/elasticsearch\/blob\/b4b32aa53a53975d1540dfc37c985729d622c6b6\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/optimizer\/LogicalPlanOptimizer.java#L946-L947), but currently that results in inconsistent physical plans as the rule [`ProjectAwayColumns` will wrongly eliminate this attribute](https:\/\/github.com\/elastic\/elasticsearch\/blob\/e568f7038daa8791f6f5db92e514cf9d8408418b\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/optimizer\/PhysicalPlanOptimizer.java#L108-L111) from the execution plan.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[DOCS] Add warning about manually mounting a snapshot that was captured by ILM","body":"### Description\n\nRelated to https:\/\/github.com\/elastic\/elasticsearch\/issues\/105647, we should add some type of warning to the [mount snapshot API](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/searchable-snapshots-api-mount-snapshot.html) docs to discourage manually mounting a snapshot that was captured by an ILM policy.","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-data-management (Team:Data Management)","Pinging @elastic\/es-distributed (Team:Distributed)","@n0othing would you be comfortable opening a PR against the [master version](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/master\/searchable-snapshots-api-mount-snapshot.html?edit) of this doc to get the ball rolling, if you have the needed context? :)\r\n\r\nYou can use the edit button to take you directly to GitHub source file."],"labels":[">enhancement",">docs",":Distributed\/Snapshot\/Restore","Team:Distributed","Team:Docs"]},{"title":"ESQL: ENRICH attribute handling inconsistent with EVAL, GROK, ...","body":"For `ENRICH` with a `WITH` statement, the attributes it creates are handled differently from `EVAL` and `GROK`\/`DISSECT`.\r\n\r\nE.g. a query with`... | ENRICH languages_policy ON a WITH name = language_name` has an `Enrich` logical\/physical plan, and the Expression for the `language_name` attribute is an Alias called `name`.\r\n\r\nWeirdly, we need to handle the unaliased `language_name` ReferenceAttribute [directly in `PhysicalPlanOptimizer.ProjectAwayColumns`](https:\/\/github.com\/elastic\/elasticsearch\/blob\/e568f7038daa8791f6f5db92e514cf9d8408418b\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/optimizer\/PhysicalPlanOptimizer.java#L122-L123). A [similar hack is currently required](https:\/\/github.com\/elastic\/elasticsearch\/pull\/105650\/files\/b33f66787c2a4045f848848ce9bc9a02c52a118e#r1499946901) to make the DependencyCheck work on LogicalPlans.\r\n\r\nThis should be refactored so that the attribute handling is analogous to `EVAL` and `GROK`\/`DISSECT`.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":["Team:Analytics",">tech debt",":Analytics\/ES|QL"]},{"title":"Add pipeline support for bulk partial update","body":"To close #105804 \r\n\r\n1. support default\/final pipeline for partial update in bulk;\r\n2. support `pipeline` parameter in doc body for partial update in bulk;","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[":Data Management\/Indices APIs","Team:Data Management","external-contributor","v8.14.0"]},{"title":"Pipeline is not supported for bulk partial update","body":"### Description\n\nI'm using the pipeline to trigger field auto update in the document, and found that **not all the update oprations** can trigger default\/final pipeline successfully:\r\n- single update:\r\n  + upsert: CAN;\r\n  + partial update: CAN;\r\n- bulk update:\r\n  + upsert: CAN;\r\n  + partial update: **CAN NOT**;\r\n\r\nTo put it clearer, here is an example:\r\n\r\nthe pipeline:\r\n```json\r\nPUT _ingest\/pipeline\/trigger_update_time\r\n{\r\n  \"description\": \"sets update_time when updating the doc\",\r\n  \"processors\": [\r\n    {\r\n      \"script\": {\r\n        \"source\": \"\"\"\r\n          if (ctx != null) {\r\n            ctx.update_time = new Date().getTime();\r\n          }\r\n        \"\"\",\r\n        \"lang\": \"painless\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nthe index:\r\n```json\r\nPUT pipeline_test\/_mappings\r\n{\r\n  \"properties\": {\r\n    \"age\": {\r\n      \"type\": \"integer\"\r\n    },\r\n    \"update_time\": {\r\n      \"type\": \"date\",\r\n      \"format\": \"epoch_millis\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nthe docs:\r\n```json\r\nPUT pipeline_test\/_doc\/1\r\n{\r\n    \"age\": 18,\r\n    \"update_time\": \"1708500148000\"\r\n}\r\n\r\nPUT pipeline_test\/_doc\/2\r\n{\r\n    \"age\": 33,\r\n    \"update_time\": \"1708500271000\"\r\n}\r\n```\r\n\r\nmake the pipeline default(or final):\r\n```json\r\nPUT pipeline_test\/_settings\r\n{\r\n  \"index.default_pipeline\": \"trigger_update_time\"\r\n}\r\n```\r\n\r\nThe bulk update operations will not trigger the default\/final pipeline:\r\n```json\r\nPOST _bulk?list_executed_pipelines=true\r\n{ \"update\" : {\"_id\" : \"1\", \"_index\" : \"pipeline_test\", \"retry_on_conflict\" : 3 } }\r\n{ \"doc\" : {\"age\" : \"5\"} }\r\n{ \"update\" : {\"_id\" : \"2\", \"_index\" : \"pipeline_test\", \"retry_on_conflict\" : 3 } }\r\n{ \"doc\" : {\"age\" : \"5\" } }\r\n```\r\nexcept that the doc is marked as `\"doc_as_upsert\" : true` like this:\r\n```json\r\nPOST _bulk?list_executed_pipelines=true\r\n{ \"update\" : {\"_id\" : \"1\", \"_index\" : \"pipeline_test\", \"retry_on_conflict\" : 3 } }\r\n{ \"doc\" : {\"age\" : \"5\"}, \"doc_as_upsert\" : true }\r\n{ \"update\" : {\"_id\" : \"2\", \"_index\" : \"pipeline_test\", \"retry_on_conflict\" : 3 } }\r\n{ \"doc\" : {\"age\" : \"5\" }, \"doc_as_upsert\" : true }\r\n```\r\n\r\nSince both the partial update(single) and the upsert(single\/bulk) operations support default\/final pipeline, I think there is no reason that partial update(bulk) doesn't. And in the [bulk api](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/docs-bulk.html#docs-bulk-api-query-params), the `pipeline` parameter states that:\r\n> If a **final pipeline** is configured it **will always run**, regardless of the value of this parameter.\r\n\r\nThe doc and the final pipeline behavior(for bulk partial update) do not match either.\r\n\r\nAfter digging deeper in the code, I found that both the upsert operation in bulk and the single update operation treat the requests as index operations, for which default\/final pipeline is supported. So we can make the partial update in bulk an index request too, to support defaut\/final pipeline.\r\n","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/Indices APIs","Team:Data Management"]},{"title":"Log undesired shard allocation at info-level","body":"Only log a warning about the number of undesired shard allocations if the\r\nnumber increases. If the number is high, but decreasing, switch to info-level\r\nlogging.\r\n\r\n------------------------------------\r\n\r\nThis needs to wait for Ievgen's troubleshooting guide addition, so it can be referenced in the log message. But other than fixing the URL reference, it's done.","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","The idea is to warn only if the cluster becomes more imbalanced, not less -- whereas now we warn for everything. We could also up the LogThreshold from 10% to 20%, to avoid logging entirely. ",">We could also up the LogThreshold from 10% to 20%, to avoid logging entirely.\r\n\r\nI am not sure we should rely on a plain %. This logs too often in a clusters with small amount of shards and too late in a big clusters with lots of shards. I wonder if we should rely only on `undesiredAllocations > concurrent_rebalances * node count`?",">> We could also up the LogThreshold from 10% to 20%, to avoid logging entirely.\r\n\r\n> I am not sure we should rely on a plain %. This is logs too often in a clusters with small amount of shards and too late in a big clusters with lots of shards. I wonder if we should rely only on undesiredAllocations > concurrent_rebalances * node count?\r\n\r\nCould you elaborate on where `concurrent_rebalances` comes from and what `concurrent_rebalances * node count` implies?",">Could you elaborate on where concurrent_rebalances comes from and what concurrent_rebalances * node count implies?\r\n\r\nThis assumes we eventually implement https:\/\/github.com\/elastic\/elasticsearch\/issues\/98087 and become better at relocating shards. Namely rather than have a fixed amount of relocations in cluster defined by `cluster.routing.allocation.cluster_concurrent_rebalance` we would scale this number with the amount of nodes, assuming each node does `cluster.routing.allocation.node_concurrent_recoveries` recoveries at once. This way we would be able to relocate up to `node_count * node_concurrent_recoveries` simultaneously. ","I updated the code a little, https:\/\/github.com\/elastic\/elasticsearch\/pull\/105796\/commits\/a86899a868cbabc859f1b5d523acb94c5afff3f4 , and responded on a couple threads. ","I talked with Ievgen over zoom about this. He observed that there could be cases where the number of undesired shards is alarmingly high and decreasing very slowly, where intervention would be necessary, but then my logging changes cease logging at WARN levels.\r\n\r\nFor myself, I'm not enthusiastic about the idea of purely INFO logging because a large number of undesired shards could be the source of, or explanation for, cluster performance issues, and that seems like it should be flagged for the user with a WARN message.\r\n\r\nThese ideas are leading me to the conclusion of keeping WARN always and adding the troubleshooting link, to make it actionable to diagnose and leave it up to the user to ignore or fine-tune on their end."],"labels":[">non-issue",":Distributed\/Allocation","Team:Distributed","v8.14.0"]},{"title":"[CI] TransformInsufficientPermissionsIT testTransformPermissionsDeferUnattendedDest failing","body":"My guess its related to: https:\/\/github.com\/elastic\/elasticsearch\/pull\/105759\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/mygnkhwtgbyv4\/tests\/:x-pack:plugin:transform:qa:multi-node-tests:javaRestTest\/org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT\/testTransformPermissionsDeferUnattendedDest\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:transform:qa:multi-node-tests:javaRestTest' --tests \"org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT.testTransformPermissionsDeferUnattendedDest\" -Dtests.seed=5581E79B57FE7891 -Dtests.locale=zh-SG -Dtests.timezone=US\/Alaska -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT#testTransformPermissionsDeferUnattendedDest`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testTransformPermissionsDeferUnattendedDest'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Stats were: {checkpointing={last={checkpoint=1, timestamp_millis=1708711038777, time_upper_bound_millis=1708711037777}}, node={transport_address=127.0.0.1:34363, name=javaRestTest-1, attributes={}, id=0bKVAalARLaq0gfB-9rY7A, ephemeral_id=JrmGy6E9SyWN9clmO2B8Gg}, stats={pages_processed=1, index_time_in_ms=0, documents_deleted=0, search_failures=0, index_failures=0, search_total=1, processing_total=1, delete_time_in_ms=0, documents_indexed=0, trigger_count=1, documents_processed=0, search_time_in_ms=9, index_total=0, exponential_avg_checkpoint_duration_ms=47.0, exponential_avg_documents_processed=0.0, processing_time_in_ms=0, exponential_avg_documents_indexed=0.0}, health={issues=[{issue=Privileges check failed, count=1, details=Cannot create transform [transform-permissions-defer-unattended-dest-exists] because user john_junior lacks the required permissions [transform-permissions-defer-unattended-dest-exists-dest:[index, read], transform-permissions-defer-unattended-dest-exists-index:[read, view_index_metadata]], type=privileges_check_failed}], status=red}, id=transform-permissions-defer-unattended-dest-exists, state=started}\nExpected: <0L>\n     but: was <1L>\n\n  at __randomizedtesting.SeedInfo.seed([5581E79B57FE7891:FAAB422B5C6B195D]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2125)\n  at org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT.assertRed(TransformInsufficientPermissionsIT.java:595)\n  at org.elasticsearch.xpack.transform.integration.TransformInsufficientPermissionsIT.testTransformPermissionsDeferUnattendedDest(TransformInsufficientPermissionsIT.java:470)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","Another build that failed: https:\/\/gradle-enterprise.elastic.co\/s\/lxv7ezg7xm4fq\r\n\r\nThis was on a commit well after: https:\/\/github.com\/elastic\/elasticsearch\/pull\/105759","I pushed the mute commit for this to 8.13 as well."],"labels":["blocker",">test-failure",":ml\/Transform","Team:ML"]},{"title":"Change skip_unavailable default value to true","body":"In order to improve the experience of cross-cluster search, we propose changing\r\nthe default value of the remote cluster `skip_unavailable` setting from `false` to `true`.\r\n\r\nThis setting causes any cross-cluster search to entirely fail when any remote cluster\r\nwith `skip_unavailable=false` is either unavailable (connection to it fails) or if the\r\nsearch on it fails on all shards.\r\n\r\nSetting `skip_unavailable=true` allows partial results from other clusters to be\r\nreturned. In that case, the search response cluster metadata will show a `skipped`\r\nstatus, so the user can see that no data came in from that cluster. Kibana also\r\nnow leverages this metadata in the cross-cluster search responses to allow users\r\nto see how many clusters returned data and drill down into which clusters did not\r\n(including failure messages).\r\n\r\nCurrently, the user\/admin has to specifically set the value to `true` in the configs, like so:\r\n\r\n```\r\ncluster:\r\n    remote:\r\n        remote1:\r\n            seeds: 10.10.10.10:9300\r\n            skip_unavailable: true\r\n```\r\n\r\neven though that is probably what search admins want in the vast majority of cases.\r\n\r\nSetting `skip_unavailable=false` should be a conscious (and probably rare) choice\r\nby an Elasticsearch admin that a particular cluster's results are so essential to a\r\nsearch (or visualization in dashboard or Discover panel) that no results at all should\r\nbe shown if it cannot return any results.","comments":["Hi @quux00, I've created a changelog YAML for you. *Note* that since this PR is labelled `>breaking`, you need to update the changelog YAML to fill out the extended information sections.","In the process of doing this work (mostly fixing tests), I found some potential issues, both of which are pre-existing but now perhaps become more acute, so they may need to be addressed.\r\n\r\n**(1)** For `_search` (and `_async_search`), when `skip_unavailable`=`false` and a cluster has an issue (such as a Security\/permissions exception or IndexNotFound), that error is explicitly returned, along with an error status code such as 404 or 403. As expected, when `skip_unavailable`=`true` and the same issue occurs, an HTTP status of 200 is returned, but the error reported in the SearchResponse is `NoSuchRemoteClusterException`, not the  `IndexNotFoundException`. This feels like a bug in the cluster resolution logic (es-distributed side?).\r\n\r\n**(2)** The `reindex` API is also affected by the `skip_unavailable` when doing a remote reindexing operation. When  `skip_unavailable`=`false` and a reindex targets an index that does not exist, then `IndexNotFoundException` is returned with an HTTP status error code.\r\n\r\nHowever, when `skip_unavailable`=`true` it does not return an error status code, and worse it does not report any error in the response object:\r\n\r\n```json\r\n{\r\n    \"took\": 36,\r\n    \"timed_out\": false,\r\n    \"total\": 0,\r\n    \"updated\": 0,\r\n    \"created\": 0,\r\n    \"deleted\": 0,\r\n    \"batches\": 0,\r\n    \"version_conflicts\": 0,\r\n    \"noops\": 0,\r\n    \"retries\": {\r\n        \"bulk\": 0,\r\n        \"search\": 0\r\n    },\r\n    \"throttled_millis\": 0,\r\n    \"requests_per_second\": -1,\r\n    \"throttled_until_millis\": 0,\r\n    \"failures\": []\r\n}\r\n```\r\n\r\nShould the reindex API report an issue in the `failures` section of this response in this case? (It likely needs to parse the search response against the remote cluster.) Or should it return an error status code? Should the reindex API have a dependency on the `skip_unavailable` setting?","Pinging @elastic\/es-search (Team:Search)"],"labels":[">breaking",":Search\/Search","Team:Search","v8.14.0"]},{"title":"Prevent long value saturation within scaled_float mappings","body":"It is easy to have long value saturation and overflow with `scaled_float` and allowing users to  saturate longs seems dangerous and confusing.\r\n\r\nI would generally expect any saturation to be an input error by the user as it will then break subsequent queries against the field (see example range query in the issue).\r\n\r\nThis commit allows saturation to occur if the index was created in a previous version or if `ignore_malformed: true`. \r\n\r\ncloses: https:\/\/github.com\/elastic\/elasticsearch\/issues\/105361","comments":["Pinging @elastic\/es-search (Team:Search)","@javanna I do not know. But using range queries (one of the main reasons to have scaled_float) over saturated values just doesn't work. \r\n\r\nWe can update the docs separately to ensure that folks know this.","> Opting folks into rejecting documents just by upgrading seems like something dangerous at this point.\r\n\r\nI understand these concerns. \r\n\r\nWe can just document, \"Hey, don't do this\". \r\n\r\nI definitely don't want to add a new parameter, and `ignore_malformed` seemed to fit this situation.\r\n\r\nWhat do y'all think of adding a deprecation here? Seems like we should warn users and they shouldn't be doing this and if they want to keep doing it, they gotta allow `ignore_malformed`. @nik9000 @javanna "],"labels":[":Search\/Mapping","Team:Search","v8.14.0"]},{"title":"[ci] Retry installing gradle\/wrapper up to 5 times","body":"Gradle wrapper does not have a built-in mechanism to retry downloads\/installs, and downloads timeout fairly frequently.\r\n\r\nCalling `.\/gradlew --version` will cause `gradlew` to install everything without doing any other real work. Let's use that in a retry loop.\r\n\r\nAdding this retry mechanism to these two scripts should cover almost all of CI.","comments":["Pinging @elastic\/es-delivery (Team:Delivery)","@elasticmachine merge upstream ","> LGTM, although, one question is how often are our CI builds actually downloading the wrapper? It should already be cached on CI agents. Does this only happen occasionally, like when we bump the wrapper version or do we consistently see that builds are having to download the wrapper?\r\n\r\n@mark-vieira I somehow didn't realize, but the original complaint was actually for the lucene builds (I've added this there). I believe we should only download it between bumps and next image build. Still seems worth adding here since it's already done?","> @mark-vieira I somehow didn't realize, but the original complaint was actually for the lucene builds (I've added this there). I believe we should only download it between bumps and next image build. Still seems worth adding here since it's already done?\r\n\r\nSure."],"labels":[">non-issue","Team:Delivery",":Delivery\/Tooling","v7.17.19","v8.13.1","v8.14.0","v8.12.3"]},{"title":"Update analyze.asciidoc","body":"GET request doesn't work on _analyze endpoint, so I'm suggesting to replace all GET requests with POST on this page.","comments":["<!-- CLA-CHECK:105782 -->\n&#10060; Author of the following commits did not sign a [Contributor Agreement](https:\/\/www.elastic.co\/contributor-agreement):\n   340670297e2601aa3b614f8a8c2f2a0393e091d7\n\n   Please, read and sign the above mentioned agreement if you want to contribute to this project","Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_105782.docs-preview.app.elstc.co\/diff)","@profuel GET works for me fine so I don't think this PR is justified. Can you share an example request and Elasticsearch version where GET didn't work with `_analyze` endpoint?","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[":Data Management\/Indices APIs","Team:Data Management","external-contributor","v8.12.3"]},{"title":"Support for multiple files in synonym_graph token filter","body":"### Description\n\nCurrently, the synonym_graph token filter in Elasticsearch supports loading synonyms from a single file using the synonyms_path parameter. However, there are scenarios where it would be beneficial to support loading synonyms from multiple files.\r\n\r\nThis feature request proposes extending the synonym_graph token filter to support specifying multiple synonym files.\r\n\r\nIn our specific use case :\r\nConsider an e-commerce platform where different departments manage their own synonym sets. By supporting multiple synonym files, each department can maintain its own file, making it easier to manage and update synonym data independently.\r\n","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">enhancement",":Search\/Analysis","Team:Search"]},{"title":"Temporarily enable tsdb codecs for rally elastic\/logging indices","body":"This branch is expected to be used to run some performance tests enabling\r\nsome TSDB codec for logging indices too.","comments":[],"labels":["v8.14.0"]},{"title":"ILM policies retrieval API is taking a long time in a large cluster","body":"We've had reports of users running into issues while trying to load their ILM policies through Kibana, as the `GET \/_ilm\/policy` request is taking a long time when there are a lot of indices that have an ILM policy configured.\r\n\r\nThe reason that this call is taking so long, is that it's computing the `in_use_by` response field. However, currently, Kibana shows the `Linked Indices` column which is based on that response field. Therefore, we'll need to come up with a solution together with the Kibana team.\r\n\r\nPossible solutions discussed thus far are:\r\n1. Making the `in_use_by` field opt-out which would allow Kibana to still use that endpoint to show the list of ILM policies, but in a much quicker way. The downside is that we'll have to come up with another way of showing the number of indices that are using a specific ILM policy. (it might be worth checking if people actually use this information currently)\r\n2. Adding a request parameter along the lines of `max_resources` which would limit the number of linked indices that are returned. This would require some more investigation to assess whether this would actually improve performance on the ES side or not. Additionally, this would require Kibana to make it clear to users that there are, in fact, more indices.","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">enhancement",":Data Management\/ILM+SLM","Team:Data Management"]},{"title":"Fix `noop_update_total` is not being updated when using the `_bulk`","body":"Closes #105742","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","@elasticmachine test this please","@elasticmachine update branch","buildkite test this","@arteam I fixed the bwc test cases based on the build title, but I don\u2019t have access to buildkite, is there any way to know which test cases failed?"],"labels":[">bug",":Distributed\/CRUD","Team:Distributed","external-contributor","v8.14.0"]},{"title":"`noop_update_total` is not being updated when using the `_bulk`","body":"### Elasticsearch Version\n\n8.12.1\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nofficial image\n\n### Problem Description\n\n`noop_update_total` is not being updated when using the `_bulk`, but it is being updated when using the `_update`.\n\n### Steps to Reproduce\n\nBulk\r\n```\r\nDELETE noop_update_demo\r\n\/\/ index\r\nPOST noop_update_demo\/_bulk\r\n{\"create\":{\"_id\":\"1\"}}\r\n{\"foo\":\"bar\"}\r\n{\"create\":{\"_id\":\"2\"}}\r\n{\"foo\":\"bar\"}\r\n\r\n\/\/ noop update\r\nPOST noop_update_demo\/_bulk\r\n{\"update\":{\"_id\":\"1\"}}\r\n{\"doc\":{\"foo\": \"bar\"}}\r\n{\"update\":{\"_id\":\"2\"}}\r\n{\"doc\":{\"foo\": \"bar\"}}\r\n\r\n\/\/ index_total = 2, noop_update_total = 0 (should be 2)\r\nGET _stats?filter_path=indices.noop_update_demo.total.indexing\r\n```\r\nReturn:\r\n```json\r\n{\r\n  \"indices\": {\r\n    \"noop_update_demo\": {\r\n      \"total\": {\r\n        \"indexing\": {\r\n          \"index_total\": 2,\r\n          \"index_time_in_millis\": 1,\r\n          \"index_current\": 0,\r\n          \"index_failed\": 0,\r\n          \"delete_total\": 0,\r\n          \"delete_time_in_millis\": 0,\r\n          \"delete_current\": 0,\r\n          \"noop_update_total\": 0,  \/\/Should be 2\r\n          \"is_throttled\": false,\r\n          \"throttle_time_in_millis\": 0,\r\n          \"write_load\": 0.0002581126849195685\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nUpdate\r\n```\r\nDELETE noop_update_demo\r\n\/\/ index\r\nPOST noop_update_demo\/_bulk\r\n{\"create\":{\"_id\":\"1\"}}\r\n{\"foo\":\"bar\"}\r\n{\"create\":{\"_id\":\"2\"}}\r\n{\"foo\":\"bar\"}\r\n\r\n\/\/ noop update\r\nPOST noop_update_demo\/_update\/1\r\n{\"doc\":{\"foo\": \"bar\"}}\r\nPOST noop_update_demo\/_update\/2\r\n{\"doc\":{\"foo\": \"bar\"}}\r\n\r\n\/\/ index_total = 2, noop_update_total = 2\r\nGET _stats?filter_path=indices.noop_update_demo.total.indexing\r\n```\r\n\r\nReturn:\r\n```json\r\n{\r\n  \"indices\": {\r\n    \"noop_update_demo\": {\r\n      \"total\": {\r\n        \"indexing\": {\r\n          \"index_total\": 2,\r\n          \"index_time_in_millis\": 1,\r\n          \"index_current\": 0,\r\n          \"index_failed\": 0,\r\n          \"delete_total\": 0,\r\n          \"delete_time_in_millis\": 0,\r\n          \"delete_current\": 0,\r\n          \"noop_update_total\": 2,  \/\/\r\n          \"is_throttled\": false,\r\n          \"throttle_time_in_millis\": 0,\r\n          \"write_load\": 0.0002900231347040275\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/CRUD","Team:Distributed"]},{"title":"Allow combining index sorting and nested mappings.","body":"### Description\n\nUntil now, we disallowed configuring `nested` fields in mappings if an index sort was configured as the index sort could break the way that we organize doc IDs.\r\n\r\nLucene recently fixed this by enabling recording root documents via `IndexWriterConfig#setParentField`. When a parent field is configured, index sorting will make sure to preserve the block structure. We should now remove the Elasticsearch limitation by removing the check and configuring a parent field when index sorting is enabled. This would then allow `nested` mappings to be added.","comments":["Pinging @elastic\/es-search (Team:Search)","@jpountz does Elastic have any workaround to bypass this behavior? \r\n\r\nI need to access nested information but I'm paginating a whole index and using the sort and `search_after` to do so. \r\n\r\n```python\r\ndef scrolling(...):\r\n    result = elastic_client.search(\r\n        index=self.index,\r\n        size=self.batch_size,\r\n        search_after=self._search_after,\r\n        sort=self._sort\r\n    ),\r\n\r\n    hits = result.get(\"hits\", {}).get(\"hits\", [])\r\n    if not hits:\r\n        return\r\n   \r\n    self._search_after = hits[-1][\"sort\"]\r\n```"],"labels":[">enhancement",":Search\/Search","Team:Search"]},{"title":"Faster Netty Integration","body":"WIP just a basis for discussion","comments":[],"labels":["WIP","v8.14.0"]},{"title":"Optimize ApplicationPrivilege#resolve for many action names","body":"### Description\n\nIn practice, we're seeing cases where application privileges are assembling >2000 action names (not patterns).\r\nThis ultimately results in high heap memory requirements to compute the internal automaton of the role's application privilege.\r\n\r\nWe should investigate if we really need an automaton, or a predicate is enough. If we need an automaton we should look into more efficient and\/or eager computation and caching, in order to avoid the one-time build of an automaton from >2000 non-pattern strings.","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">enhancement",":Security\/Authorization","Team:Security"]},{"title":"Add option for Append Processor to skip\/allow empty values","body":"If one is using templates to generate values, this option can be helpful to avoid adding values that resolve into empty strings\r\n\r\nFixes #104813","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_105718.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[":Data Management\/Ingest Node","Team:Data Management","external-contributor","v8.14.0"]},{"title":"Adding failsafe for peer recovery of local shards from primary to obey low disk watermark","body":"### Description\n\n## Scenario\r\n- Consider a warm node with 500GiB disk space using default watermark (425GiB is low watermark at 85%)\r\n- Most shards are ~50GiB on that node (read-only indices) with single segment file\r\n- When recovery is enabled after a node restart, replica shards run peer recovery\u2026\r\n- Most shards recover immediately and require no extra disk space but in some cases recovery may involve to override the local replica shard data with copy of the primary shard data\u2026\r\n- For such scenario, recovery will copy the segment file from the primary shards, the old segment file(s) get deleted at the end of the peer recovery in this case so a shard of 50GiB with this type of recovery will reach a disk usage of 100GiB just before completion\r\n- There is no failsafe currently so the peer recovery (up to 2 concurrently by default) can start so long as node is below low watermark at beginning of recovery operation - potentially causing watermarks being reached or disk to become full\r\n\r\n## Enhancement around documentation\r\n\r\n- At the moment we do [state](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.12\/modules-cluster.html#disk-based-shard-allocation) it is normal for nodes to temporarily exceed the high watermark from time to time and we mention never to increase `cluster.routing.allocation.node_concurrent_recoveries` (in scenario mentioned here, this can make it more likely to hit full disk scenario)\r\n- We do not seem to specifically document that if peer recovery override with primary shard data during the recovery of shard, the old data is kept when primary shard data is replacing replica shard data resulting in increased disk usage that disk allocator watermarks won't be aware of in advance, should this scenario be documented ?\r\n\r\n## Enhancements around peer recovery process\r\n\r\nPotential improvements to discuss (just 2 potential way to change behaviour - may be other ways too) : \r\n- When doing peer recovery and we decide to override (part of) local disk shard with the primary shard, should we start by deleting the local replica data so disk usage of the shard does not potentially double in size.\r\n- Should we have some failsafe to not start any peer recovery without evaluating all current recovery operation disk usage cost that will take the node above low disk watermark ? If we delay those recoveries we know would take the node above low watermark, the issue might be the node just below low watermark may never have disk space so should the delayed allocation still take effect despite the node having rejoined the cluster so shard gets deleted after `\"index.unassigned.node_left.delayed_timeout\": \"1m\"` which would free up the 50GiB of that shard so that will effectively run a peer recovery from scratch","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","This could piggyback on work in https:\/\/github.com\/elastic\/elasticsearch\/issues\/98087 but documentation updates are of course welcome in the meantime."],"labels":[">enhancement",":Distributed\/Recovery","Team:Distributed"]},{"title":"Add build version to snapshot info","body":null,"comments":[],"labels":["v8.14.0"]},{"title":"[CI] RestEsqlIT testWarningHeadersOnFailedConversions {p0=ASYNC} failing","body":"**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/45cfvousxtrwa\/tests\/:x-pack:plugin:esql:qa:server:single-node:javaRestTest\/org.elasticsearch.xpack.esql.qa.single_node.RestEsqlIT\/testWarningHeadersOnFailedConversions%20%7Bp0=ASYNC%7D\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':x-pack:plugin:esql:qa:server:single-node:javaRestTest' --tests \"org.elasticsearch.xpack.esql.qa.single_node.RestEsqlIT.testWarningHeadersOnFailedConversions {p0=ASYNC}\" -Dtests.seed=649050D8ED5C8A1 -Dtests.locale=ar-YE -Dtests.timezone=Asia\/Thimphu -Druntime.java=21\r\n```\r\n\r\n**Applicable branches:**\r\n8.13\r\n\r\n**Reproduces locally?:**\r\nDidn't try\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.esql.qa.single_node.RestEsqlIT#testWarningHeadersOnFailedConversions {p0=ASYNC}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testWarningHeadersOnFailedConversions%20%7Bp0%3DASYNC%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.esql.qa.single_node.RestEsqlIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.AssertionError: \r\nExpected: a string containing \"java.lang.NumberFormatException: For input string: \\\\\\\"keyword1\\\\\\\"\"\r\n     but: was \"Line 1:36: java.lang.NumberFormatException: For input string: \\\"keyword55\\\"\"\r\n\r\n  at __randomizedtesting.SeedInfo.seed([649050D8ED5C8A1:1A2AFDFA8C166279]:0)\r\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\r\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\r\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2119)\r\n  at org.elasticsearch.xpack.esql.qa.rest.RestEsqlTestCase.testWarningHeadersOnFailedConversions(RestEsqlTestCase.java:423)\r\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n  at java.lang.reflect.Method.invoke(Method.java:580)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:1583)\r\n\r\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","I cannot reproduce this. Not the latest `main` but also not the code around the time when the CI failure happened."],"labels":[">test-failure","Team:Analytics",":Analytics\/ES|QL","medium-risk"]},{"title":"[CI] QueryExtractorBuilderTests testConcurrentToXContent failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/5b5q46heso36i\/tests\/:x-pack:plugin:core:test\/org.elasticsearch.xpack.core.ml.inference.trainedmodel.ltr.QueryExtractorBuilderTests\/testConcurrentToXContent\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:core:test' --tests \"org.elasticsearch.xpack.core.ml.inference.trainedmodel.ltr.QueryExtractorBuilderTests.testConcurrentToXContent\" -Dtests.seed=C35D8D0B2E4079B6 -Dtests.locale=ja-JP-u-ca-japanese-x-lvariant-JP -Dtests.timezone=Africa\/Mogadishu -Druntime.java=17\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.core.ml.inference.trainedmodel.ltr.QueryExtractorBuilderTests#testConcurrentToXContent`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testConcurrentToXContent'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.core.ml.inference.trainedmodel.ltr.QueryExtractorBuilderTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.util.concurrent.ExecutionException: java.util.NoSuchElementException\n\n  at __randomizedtesting.SeedInfo.seed([C35D8D0B2E4079B6:AB24A1E2635A81D5]:0)\n  at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n  at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n  at org.elasticsearch.test.AbstractWireTestCase.concurrentTest(AbstractWireTestCase.java:117)\n  at org.elasticsearch.test.AbstractSerializationTestCase.testConcurrentToXContent(AbstractSerializationTestCase.java:79)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:568)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:833)\n\n  Caused by: java.util.NoSuchElementException: (No message provided)\n\n    at java.util.ArrayList$Itr.next(ArrayList.java:970)\n    at org.elasticsearch.xcontent.XContentBuilder.value(XContentBuilder.java:1133)\n    at org.elasticsearch.xcontent.XContentBuilder.unknownValue(XContentBuilder.java:946)\n    at org.elasticsearch.xcontent.XContentBuilder.map(XContentBuilder.java:1106)\n    at org.elasticsearch.xcontent.XContentBuilder.unknownValue(XContentBuilder.java:944)\n    at org.elasticsearch.xcontent.XContentBuilder.map(XContentBuilder.java:1106)\n    at org.elasticsearch.xcontent.XContentBuilder.map(XContentBuilder.java:1052)\n    at org.elasticsearch.xcontent.XContentBuilder.field(XContentBuilder.java:1048)\n    at org.elasticsearch.xpack.core.ml.inference.trainedmodel.ltr.QueryExtractorBuilder.toXContent(QueryExtractorBuilder.java:90)\n    at org.elasticsearch.common.xcontent.XContentHelper.toXContent(XContentHelper.java:633)\n    at org.elasticsearch.test.AbstractSerializationTestCase.lambda$testConcurrentToXContent$2(AbstractSerializationTestCase.java:82)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:264)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n    at java.lang.Thread.run(Thread.java:833)\n\n```","comments":["Pinging @elastic\/ml-core (Team:ML)","This doesn't reproduce locally. It must be a race condition.\r\n\r\nThe line that's causing the error is: https:\/\/github.com\/elastic\/elasticsearch\/blob\/be98a4697e2719dbcf9a542cac377ed37b9cdb6d\/x-pack\/plugin\/core\/src\/main\/java\/org\/elasticsearch\/xpack\/core\/ml\/inference\/trainedmodel\/ltr\/QueryExtractorBuilder.java#L90\r\n\r\nSo it suggests that `query.getQuery()` is not safe to call simultaneously from multiple threads. But that's a bit crazy as it's just returning a reference to a map and in this test it's a hierarchy of nested immutable maps created in the test code.\r\n\r\nOne interesting thing is that this test failed in GraalVM, so maybe it's some quirk of Graal. If this test fails again that's the first thing to look at to see if there's a pattern.\r\n\r\ncc @afoucret as it looks like you're currently maintaining these tests."],"labels":[">test-failure",":ml","Team:ML","low-risk"]},{"title":"ESQL: Load csv data as ClassRule in single\/multi-node","body":"Currently, the EsqlSpecTestCase tries to upload the CSVs before each and [every test run](https:\/\/github.com\/elastic\/elasticsearch\/blob\/0da52203723cb75755320acd83fc0aeee8e3834d\/x-pack\/plugin\/esql\/qa\/server\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/qa\/rest\/EsqlSpecTestCase.java#L99-L103). If an index already exists, a test case will treat the upload as successful and just run.\r\n\r\nThis is bad if the upload fails, as most tests will fail and we get spammed with cryptic, unrelated error messages.\r\n\r\nAvoid running an setup\/upload method before each and every test case, instead perform a single setup method as a ClassRule.\r\n\r\nCovers only the `single-node` and `multi-node` test suites, so far.","comments":[],"labels":["v8.14.0"]},{"title":"Fix noisy logging in tests from StartupSelfGeneratedLicenseTask","body":"This logs endlessly and at error level during some node-restarting tests, making debugging leaks and other test failures just unnecessarily painful. Lets just log debug on node shutdown here.\r\n","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">non-issue",":Security\/License","Team:Security","v8.14.0"]},{"title":"[TSDB] Strange rate result when applying time shift","body":"### Elasticsearch Version\n\n8.13.0, 8.14.0\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMacOS\n\n### Problem Description\n\nThere's a TSDB dataset in Kibana with a `counter` field named `bytes_counter` which is monotonically increasing roughly 100k bytes per hour.\r\nNote that there's **no reset** in the entire dataset for the `bytes_counter` field.\r\n\r\nHere's a look of the documents daily distribution:\r\n<img width=\"1262\" alt=\"Screenshot 2024-02-21 at 10 43 59\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/924948\/211e9486-9c37-4ec2-a343-2a08caa0b104\">\r\n\r\nNow configuring 2 counter rate metrics in Lens gives the following chart:\r\n\r\n<img width=\"1224\" alt=\"Screenshot 2024-02-21 at 10 45 09\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/924948\/b718e0f4-4bcc-4533-96b1-09cb6d30c361\">\r\n\r\nAs highlighted in the tooltip at 11:30 in the chart the value is pretty high, which leads to some ideas:\r\n* a reset happened - impossible given the dataset\r\n* few events happened in a very short time that skewed the final rate value (given the TSDB rate formula)\r\n\r\nthe problem happens when the first metric (non time shifted) is removed:\r\n\r\n<img width=\"1227\" alt=\"Screenshot 2024-02-21 at 10 54 59\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/924948\/9dfa20bd-d6d9-4927-aba9-1f99b998d1de\">\r\n\r\nNotice how the spike is gone.\r\n\r\nLooking at the raw documents in Discover at 23:30 (11:30 - 12h) documents this is what is there:\r\n\r\n<img width=\"1268\" alt=\"Screenshot 2024-02-21 at 10 54 06\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/924948\/82644b57-6633-496e-9946-b7f033d6fba4\">\r\n\r\nIt might justify a spike at 23:30 looking at raw documents, but why the spike went away?\r\n\r\nHere's another reproduction of the same issue, this time starting with 3 counter rate, where 2 have a time shift:\r\n\r\n![tsdb_rate_issue_3](https:\/\/github.com\/elastic\/elasticsearch\/assets\/924948\/85f3c9fd-e22d-4c8c-b7a9-dc362dc560d2)\r\n\r\nDeleting the 1 day metric configuration, the spike moves from the 12h to the metric without the shift.\r\nAlso every time I generate this same chart the spike seems to move from one day to another, always around the 23:30 - 00:00 bucket.\r\n\r\nHere's the request for the 2 metrics (no shifted metric + 12h time shift): \r\n```json\r\n{\r\n  \"aggs\": {\r\n    \"time_offset_split\": {\r\n      \"filters\": {\r\n        \"filters\": {\r\n          \"0\": {\r\n            \"range\": {\r\n              \"@timestamp\": {\r\n                \"format\": \"strict_date_optional_time\",\r\n                \"gte\": \"2024-02-20T09:00:00.000Z\",\r\n                \"lte\": \"2024-02-21T09:57:56.827Z\"\r\n              }\r\n            }\r\n          },\r\n          \"43200000\": {\r\n            \"range\": {\r\n              \"@timestamp\": {\r\n                \"format\": \"strict_date_optional_time\",\r\n                \"gte\": \"2024-02-19T21:00:00.000Z\",\r\n                \"lte\": \"2024-02-20T21:57:56.827Z\"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      \"aggs\": {\r\n        \"0\": {\r\n          \"date_histogram\": {\r\n            \"field\": \"@timestamp\",\r\n            \"fixed_interval\": \"30m\",\r\n            \"time_zone\": \"Europe\/Rome\"\r\n          },\r\n          \"aggs\": {\r\n            \"1\": {\r\n              \"sum_bucket\": {\r\n                \"buckets_path\": \"1-bucket>1-metric\"\r\n              }\r\n            },\r\n            \"2\": {\r\n              \"sum_bucket\": {\r\n                \"buckets_path\": \"2-bucket>2-metric\"\r\n              }\r\n            },\r\n            \"1-bucket\": {\r\n              \"time_series\": {},\r\n              \"aggs\": {\r\n                \"1-metric\": {\r\n                  \"rate\": {\r\n                    \"field\": \"bytes_counter\",\r\n                    \"unit\": \"minute\"\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            \"2-bucket\": {\r\n              \"time_series\": {},\r\n              \"aggs\": {\r\n                \"2-metric\": {\r\n                  \"rate\": {\r\n                    \"field\": \"bytes_counter\",\r\n                    \"unit\": \"minute\"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"size\": 0,\r\n  \"fields\": [\r\n    {\r\n      \"field\": \"@timestamp\",\r\n      \"format\": \"date_time\"\r\n    },\r\n    {\r\n      \"field\": \"timestamp\",\r\n      \"format\": \"date_time\"\r\n    },\r\n    {\r\n      \"field\": \"utc_time\",\r\n      \"format\": \"date_time\"\r\n    }\r\n  ],\r\n  \"script_fields\": {},\r\n  \"stored_fields\": [\r\n    \"*\"\r\n  ],\r\n  \"runtime_mappings\": {\r\n    \"hour_of_day\": {\r\n      \"type\": \"long\",\r\n      \"script\": {\r\n        \"source\": \"emit(doc['timestamp'].value.getHour());\"\r\n      }\r\n    }\r\n  },\r\n  \"_source\": {\r\n    \"excludes\": []\r\n  },\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"must\": [],\r\n      \"filter\": [\r\n        {\r\n          \"bool\": {\r\n            \"should\": [\r\n              {\r\n                \"bool\": {\r\n                  \"filter\": [\r\n                    {\r\n                      \"range\": {\r\n                        \"@timestamp\": {\r\n                          \"format\": \"strict_date_optional_time\",\r\n                          \"gte\": \"2024-02-20T09:00:00.000Z\",\r\n                          \"lte\": \"2024-02-21T09:57:56.827Z\"\r\n                        }\r\n                      }\r\n                    }\r\n                  ]\r\n                }\r\n              },\r\n              {\r\n                \"bool\": {\r\n                  \"filter\": [\r\n                    {\r\n                      \"range\": {\r\n                        \"@timestamp\": {\r\n                          \"format\": \"strict_date_optional_time\",\r\n                          \"gte\": \"2024-02-19T21:00:00.000Z\",\r\n                          \"lte\": \"2024-02-20T21:57:56.827Z\"\r\n                        }\r\n                      }\r\n                    }\r\n                  ]\r\n                }\r\n              }\r\n            ],\r\n            \"minimum_should_match\": 1\r\n          }\r\n        }\r\n      ],\r\n      \"should\": [],\r\n      \"must_not\": []\r\n    }\r\n  }\r\n}\r\n```\n\n### Steps to Reproduce\n\nStart Kibana and install the TSDB Sample data logs dataset.\r\nGo to Lens and create a chart as follow:\r\n* add `@timestamp` as Horizontal axis\r\n* configure a Vertical Axis as `Counter rate` of `bytes_counter`\r\n  * click on `Advanced` and configure `per minute` normalization\r\n* configure another Vertical axis metric as `Counter rate` of `bytes_counter`\r\n  * click on `Advanced` and configure `per minute` normalization\r\n  * also add a time shift of `12 h`\r\n\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)","I see a similar spike:\r\n\r\n<img width=\"1108\" alt=\"Screenshot 2024-02-21 at 16 15 29\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/131142368\/680eece6-558f-4395-8c23-72d8d2fb557d\">\r\n\r\nTIme offset in this case just shifts the spike, it doesn't seem to be required.\r\n\r\nEDIT: these don't use the TSDB dataset, they actually have a spike in the data.","Try to declare a third one with 1d shift, then remove it.\r\nAre those spike still there?","I played with 3 metrics, this is what I got:\r\n\r\n<img width=\"1111\" alt=\"Screenshot 2024-02-21 at 20 10 57\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/131142368\/48a8931e-789f-43c5-bdd7-33f980710082\">\r\n\r\nAm I missing something? Tried to remove them, didn't see any jump I think.. I'm using Kibana 8.12 tho, wonder if there's an issue on Kibana side.\r\n","Uploading files to reproduce, for posterity: \r\n[data4.json](https:\/\/github.com\/elastic\/elasticsearch\/files\/14363555\/data4.json)\r\n[mappings.json](https:\/\/github.com\/elastic\/elasticsearch\/files\/14363565\/mappings.json)\r\n","I was able to reproduce this, using the attached query. There's a 20k spike in rate per minute. Through data inspection, the delta in the counter values is ~60k over ~40 minutes, so the rate should be less than 2k. I do see the following entry though:\r\n\r\n```\r\n{\r\n                \"1\": {\r\n                  \"value\": 19582.360889325635\r\n                },\r\n                \"2\": {\r\n                  \"value\": 19582.360889325635\r\n                },\r\n                \"key_as_string\": \"2024-02-20T15:00:00.000+01:00\",\r\n                \"key\": 1708437600000,\r\n                \"doc_count\": 14,\r\n                \"2-bucket\": {\r\n                  \"buckets\": {\r\n                    \"{event.dataset=sample_web_logs}\": {\r\n                      \"key\": {\r\n                        \"event.dataset\": \"sample_web_logs\"\r\n                      },\r\n                      \"doc_count\": 14,\r\n                      \"2-metric\": {\r\n                        \"value\": 19582.360889325635\r\n                      }\r\n                    }\r\n                  }\r\n                },\r\n                \"1-bucket\": {\r\n                  \"buckets\": {\r\n                    \"{event.dataset=sample_web_logs}\": {\r\n                      \"key\": {\r\n                        \"event.dataset\": \"sample_web_logs\"\r\n                      },\r\n                      \"doc_count\": 14,\r\n                      \"1-metric\": {\r\n                        \"value\": 19582.360889325635\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              },\r\n```\r\n\r\nThis goes away when the ranges are shrunk, so it may have to do with the overlapping ranges for the filters - we use a single query to produce 2 rates over the same metric, using shifted but overlapping time ranges.\r\n\r\n\r\n[query.txt](https:\/\/github.com\/elastic\/elasticsearch\/files\/14374308\/query.txt)\r\n","This value gets generated from a pair of successive values:\r\n\r\n```\r\n\"@timestamp\": \"2024-02-20T14:01:09.591Z\"  \"bytes_counter\": 12984480\r\n\"@timestamp\": \"2024-02-20T14:00:53.309Z\", \"bytes_counter\": 12979166\r\n```\r\n\r\nStill wrong, in so many ways.. ","My current theory for the root cause is that TimeSeries aggregation only knows how to group by tsid, i.e. over all dimensions.  If a subset of dimensions is used in a query, groups are not tracked as expected iiuc.","As suggested by @salvatore-campagna, this is another case where `TimeSeriesAggregation` doesn't handle pipelined aggregations well. In this particular case, the sequence is `FilterAggregation` -> `DateHistogramAggregation` -> `TimeSeriesAggregation` -> `TimeSeriesRateAggregator`. The problem is that the output of `FilterAggregation` is used as input for the `TimeSeriesRateAggregation`, breaking how bucketing happens there: there's no \"reduce\" call to combine multiple 1-m rate values per 30-m bucket, but only a single rate calculation is seemingly used per bucket (e.g. for a single minute, see above). This is probably due to the `InternalTimeSeries` reducer not calling `accept` on its buckets to propagate bucket processing to `TimeSeriesRateAggregator`.\r\n\r\nA workaround here is to remove the filter agg preceding the date histogram and solely rely on the query spec to provide the time range. This likely leads to separate queries for each metric."],"labels":[">bug",":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"`HeapBufferedAsyncResponseConsumer` does not limit heap memory usage for chunked responses","body":"`org.elasticsearch.client.HeapBufferedAsyncResponseConsumer` imposes a limit (default 100MiB) on the size of responses it'll accept. However this limit only applies to responses with a `Content-length` header, and therefore does not restrict the heap memory usage when receiving a response with `Transfer-encoding: chunked`. These days pretty much all large responses are chunked so in practice this limit doesn't really do anything any more. Should we fix it to apply to all responses so as to protect clients from OOMEs? At the very least we should probably mention this anomaly in our docs somewhere.\r\n\r\n---\r\n\r\n### Workaround\r\n\r\nUse `org.elasticsearch.client.RequestOptions.Builder#setHttpAsyncResponseConsumerFactory` to provide a response consumer which does handle chunked responses correctly.","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">bug",":Core\/Infra\/REST API","Team:Core\/Infra"]},{"title":"[DSL] Introduce data stream global retention - Part 3","body":"In this PR we introduce the API that will expose the global retention configuration and will allow users to take advantage of it.\r\n\r\nThese APIs are protected by the dedicated introduced privileges:\r\n\r\n-  `manage_data_stream_global_retention` or higher, which allows all operations on the global retention configuration\r\n-  `monitor_data_stream_retention` or higher, which allows the retrieval of the global retention configuration\r\n\r\nThis PR is not delivering the complete feature since the global retention is not used yet.\r\n\r\nRelevant documentation:\r\n- [Retention Tutorial](https:\/\/elasticsearch_bk_105682.docs-preview.app.elstc.co\/guide\/en\/elasticsearch\/reference\/master\/tutorial-manage-data-stream-retention.html)\r\n- [Data stream lifecycle page](https:\/\/elasticsearch_bk_105682.docs-preview.app.elstc.co\/guide\/en\/elasticsearch\/reference\/master\/data-stream-lifecycle.html)\r\n- [Global retention APIs](https:\/\/elasticsearch_bk_105682.docs-preview.app.elstc.co\/guide\/en\/elasticsearch\/reference\/master\/data-stream-apis.html) ","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_bk_105682.docs-preview.app.elstc.co\/diff)","Pinging @elastic\/es-data-management (Team:Data Management)","Investigating the bwc failures","The bwc failures, happened because of the missing feature `health.extended_repository_indicator`. It was fixed as soon as I updated the branch with main. I expect the tests will pass now.","The PR look really good to me. The only additional thing we've been discussing offline is whether we need to add a feature flag to prevent the transport action from being called on a node that doesn't have it in a mixed-mode cluster.","After careful consideration this PR cannot be merged as is because it introduces public APIs without the supporting functionality. This means that we cannot use the transport version or a node feature to communicate to the user in a simple way when this feature will be supported. \r\n\r\nFor this reason, we proposed an alternative chunking of the work in https:\/\/github.com\/elastic\/elasticsearch\/issues\/106169.\r\n\r\nOne of the reviewed outcomes of this PR is merged by https:\/\/github.com\/elastic\/elasticsearch\/pull\/106170, since there are no public APIs there is not impact for out users.","We will evaluate if this can be used when the other parts in https:\/\/github.com\/elastic\/elasticsearch\/issues\/106169 are done.","@elasticmachine update branch","Hi @gmarouli, I've created a changelog YAML for you."],"labels":[">feature",":Data Management\/Data streams","Team:Data Management","v8.14.0"]},{"title":"[CI] JdbcPreparedStatementIT testDatetimeWithNanos failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/tzbqbd22w432i\/tests\/:x-pack:plugin:sql:qa:jdbc:multi-node:javaRestTest\/org.elasticsearch.xpack.sql.qa.jdbc.multi_node.JdbcPreparedStatementIT\/testDatetimeWithNanos\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:sql:qa:jdbc:multi-node:javaRestTest' --tests \"org.elasticsearch.xpack.sql.qa.jdbc.multi_node.JdbcPreparedStatementIT.testDatetimeWithNanos\" -Dtests.seed=4A843999DDB3A0A3 -Dtests.locale=en -Dtests.timezone=Europe\/Madrid -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nYes\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.sql.qa.jdbc.multi_node.JdbcPreparedStatementIT#testDatetimeWithNanos`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testDatetimeWithNanos'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.sql.qa.jdbc.multi_node.JdbcPreparedStatementIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: (No message provided)\n\n  at __randomizedtesting.SeedInfo.seed([4A843999DDB3A0A3:E2A79223F408E51F]:0)\n  at org.junit.Assert.fail(Assert.java:87)\n  at org.junit.Assert.assertTrue(Assert.java:42)\n  at org.junit.Assert.assertFalse(Assert.java:65)\n  at org.junit.Assert.assertFalse(Assert.java:75)\n  at org.elasticsearch.xpack.sql.qa.jdbc.PreparedStatementTestCase.testDatetimeWithNanos(PreparedStatementTestCase.java:177)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":["blocker",">test-failure",":Analytics\/SQL","Team:Analytics"]},{"title":"Create a single instance of `PersistentTasksService` and pass it to `PersistentTaskPlugin` ","body":"### Description\n\nToday an instance of `PersistentTasksService` is created in `server` (`NodeConstruction`) and passed down to usages in `server` (e.g. `HealthNodeTaskExecutor`). \r\nModules and plugins (e.g. `IngestGeoIpPlugin`, plus others in serverless) create their own instance. \r\n\r\nThis should be not a problem currently - it's just a wrapper over three other services and doesn't have any meaningful state - but it may become a problem if we make changes to it (e.g. state is introduced) and we don't realize we have multiple instances.\r\n\r\nWe should have just one `PersistentTasksService` and pass it around, either via `Plugin#createComponents(PluginServices services)` or via `PersistentTaskPlugin#getPersistentTasksExecutor`.","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[":Core\/Infra\/Core",">refactoring","Team:Core\/Infra"]},{"title":"ESQL: Helpful error when you use a grouping function in EVAL","body":"### Description\n\nWe should make sure the error message you get when you use a grouping function like `MIN` or `MAX` with `EVAL` tells you to move it to `STATS`.\r\n","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"Bump version ids on a new release","body":"Add an option to bump transport and index version ids on a new release, controlled by the build automation\r\n\r\nRelates #105295","comments":[],"labels":[":Core\/Infra\/Core",">refactoring","v8.14.0"]},{"title":"Artificial term vectors are generated only for stored fields","body":"### Elasticsearch Version\r\n\r\n8.14 and above\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nAll\r\n\r\n### Problem Description\r\n\r\nAs a result of making the [`_ignored` field not stored ](https:\/\/github.com\/elastic\/elasticsearch\/pull\/101373)and having just doc values we are missing generation of term vectors for it. `TermVectorsService#addGeneratedTermVectors` indeed, generates term vectors only for stored fields. Since `_ignored` is no longer a stored field term vectors are not generated anymore. We need to fix that and we might have two ways to do that:\r\n1. add term vectors generation logic for non-stored fields, generating term vectors for fields having just doc values  (more in general using `ValueFetcher` whatever the field type is, which would use stored fields or doc values depending on what is configured in the mapper as a `ValueFetcher`)\r\n2. we might add generation of term vectors at indexing time and later on relying on fetching term vectors for the `_ignored` field rather then generating them \"on-the-fly\"\r\n\r\n### Steps to Reproduce\r\n\r\nJust run the test after [PR](https:\/\/github.com\/elastic\/elasticsearch\/pull\/101373) is merged.\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["> we might add generation of term vectors at indexing time and later on relying on fetching term vectors for the _ignored field rather then generating them \"on-the-fly\"\r\n\r\nI don't think this is an option, as it would dramatically increase size on disk for the field.\r\n\r\n\r\n> add term vectors generation logic for non-stored fields generating term vectors for fields having just doc values (more in general using ValueFetcher whatever the field type is, which would use stored fields or doc values depending on what is configured in the mapper as a ValueFetcher\r\n\r\nAgreed. I want to point out that this issue is not only for `_ignored`, and I even wonder what the usecase is for retrieving term vectors for the `_ignored` field. Fields that are excluded from `_source` have the same issue, as well as fields that are indexed but not stored in the index nor in `_source.` I think that `TermVectorsService` is quite outdated, it does not support runtime fields and does not go through value fetchers to retrieve field values, and I wonder if it is worthwhile to expand its functionalities. I am not sure how widely used this API is, especially the on-the-fly generation of term vectors.","Pinging @elastic\/es-search (Team:Search)","I agree that option 1 is the way to go...e definitely option 2 would increase disk size."],"labels":[">bug",":Search\/Search","Team:Search"]},{"title":"[ILM] Searchable snapshot action tries to snapshot partially mounted index in cold","body":"### Elasticsearch Version\n\n7.x, 8.x\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nDarwin\n\n### Problem Description\n\nWhen ILM mounts and index it first takes a snapshot of the index including its ILM state (e.g. the snapshot for fully mounting and index in the cold phase will contain the ILM execution state of the index being in the `cold\/searchable_snapshot\/create-snapshot` step with the next step being `cold\/searchable_snapshot\/cleanup-snapshot`)\r\nWhen this index transitions to frozen to be partially mounted ILM will reuse the snapshot it took in the `cold` phase and just re-mount the index as partially mounted. \r\n\r\nNow, say you'd like to manually mount the index in a deployment reusing the snapshot ILM took in cold. A call like :\r\n```\r\nPOST \/_snapshot\/found-snapshots\/2024.02.14-test-000001-test-policy-g52geraxtnuhna780zgnha\/_mount?wait_for_completion=true&storage=shared_cache\r\n{\r\n  \"index\": \"test-000001\", \r\n  \"renamed_index\": \"partial-restored-test-000001\"\r\n}\r\n```\r\n\r\nwill create the partially mounted index `partial-restored-test-000001` but with the ILM policy that was configured for index `test-000001` and the _ILM state_ the index had when the snapshot was created (i.e. the index was in `cold\/searchable_snapshot\/create-snapshot` ) \r\nSo ILM will see `partial-restored-test-000001` in `cold\/searchable_snapshot\/create-snapshot`  and power through to try and create a snapshot for this index, fail as `2024.02.14-test-000001-test-policy-g52geraxtnuhna780zgnha` exists already, so it will delete this snapshot and attempt **to create a new snapshot of the index**. The tricky bit here is that the index is now a partially mounted index, as opposed to a regular index (e.g. `test-000001`) as the `cold` phase usually handles. \r\n\r\nThis results in ILM creating a snapshot of a frozen index that's mistakenly in the cold phase. \r\n\r\nThis is a tricky one for ILM to handle as one might say \"well, ILM should just skip cold if it sees the index is partially mounted\" however, what if there is no frozen tier in the cluster? In this case, perhaps one would expect ILM to fully mount the regular index from the snapshot it detects ? (do what the `cold` phase is meant to do i.e. fully mount the index) \r\n\r\nWe should discuss the options we have here (ILM not doing anything in this case but signaling an error is also an option) but it'd be great to avoid this data-loss scenario where ILM snapshots a partially mounted index, losing the snapshot containing the regular index.\r\n\r\nIn the meantime, if an index is partially mounted, ignore the `index.lifecycle.name` index setting when restoring (so the index is not picked up by ILM) \r\ne.g. \r\n```\r\nPOST \/_snapshot\/found-snapshots\/2024.02.14-test-000001-test-policy-g52geraxtnuhna780zgnha\/_mount?wait_for_completion=true&storage=shared_cache\r\n{\r\n  \"index\": \"test-000001\", \r\n  \"renamed_index\": \"partial-restored-test-000001\",\r\n  \"ignore_index_settings\": [ \"index.lifecycle.name\" ]\r\n}\r\n```\r\nand **before** attaching a new ILM policy to the index, remove its ILM execution state using the [ILM remove API](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/ilm-remove-policy.html): \r\n```\r\nPOST partial-restored-test-000001\/_ilm\/remove\r\n``` \n\n### Steps to Reproduce\n\nCreate an ILM policy with cold and frozen phases. \r\nWait for ILM to mount the index in frozen. \r\nTake a note of the snapshot name for the frozen index listed in the ILM explain API : \r\n```\r\nGET partial-restored-test-000001\/_ilm\/explain?human\r\n```\r\nDelete the index so we can re-mount it manually.\r\n```\r\nDELETE partial-restored-test-000001\r\n```\r\n\r\nManually mount the index:\r\n```\r\nPOST \/_snapshot\/found-snapshots\/2024.02.14-test-000001-test-policy-g52geraxtnuhna780zgnha\/_mount?wait_for_completion=true&storage=shared_cache\r\n{\r\n  \"index\": \"test-000001\", \r\n  \"renamed_index\": \"partial-restored-test-000001\"\r\n}\r\n```\r\n\r\nRestart the master node so ILM re-executes the current async step (cold\/searchable_snapshot\/create-snapshot)\r\n\r\nNote how the freshly _partially_ mounted index `partial-restored-test-000001` is now int he `cold` phase, ILM deletes its backing snapshot and attempts to recreate it.Except it will now take a snapshot for he partially mounted index.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-data-management (Team:Data Management)","Related https:\/\/github.com\/elastic\/elasticsearch\/issues\/91970","We discussed this today and decided that ILM should check when in the `create-snapshot` or `cleanup-snapshot` steps if the managed index is already mounted (fully or partially) in which case ILM should just fall through (no-op) the rest of the `searchable_snapshot` action. \r\n"],"labels":[">bug",":Data Management\/ILM+SLM","Team:Data Management","medium-risk"]},{"title":"Reintroduce non-semantic version in NodeInfo (revert #102636)","body":"Re-introduce the change in https:\/\/github.com\/elastic\/elasticsearch\/pull\/100746 and reverted in https:\/\/github.com\/elastic\/elasticsearch\/pull\/102636\r\n\r\nThe changed caused issue in Kibana with serverless. Kibana has now updated its codebase to handle non-semver (https:\/\/github.com\/elastic\/kibana\/pull\/172093), so this can go in safely now.\r\n","comments":[],"labels":[":Core\/Infra\/Core","test-update-serverless","v8.14.0"]},{"title":"ESQL should support the \"flattened\" field type","body":"### Elasticsearch Version\n\n8.12.1\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nrockylinux:a\n\n### Problem Description\n\nAs mentioned in https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.12\/esql-limitations.html#_unsupported_types the `flattened` field type is not supported by ESQL. I don't find another issue tracking that support. Is it planned and any idea when, like in 8.x or not until 9?\n\n### Steps to Reproduce\n\nTry to use ESQL to aggregate on a `flattened` field.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","I don't have an estimate. One of the fun parts about flattened is that we don't know what the actual sub-fields are at planning time.","Hey @nik9000, I noticed that I can't even fetch a flattened field using ESQL.\r\nIt shows up as empry.\r\nIs there any reason not to support this at least?\r\n![image](https:\/\/github.com\/elastic\/elasticsearch\/assets\/3016806\/3e36e5a0-55c3-48f9-94a0-362067d2b0f6)\r\n","> Is there any reason not to support this at least?\r\n\r\nThat fetch is sort of step 0 in supporting `flattened`. I can only think of two reasons we're not working on it now though - we're busy with other stuff, and, maybe worse, `flattened` fields are very hard to resolve into columns which is something that ESQL needs to do up front. That's not really possible for flattened fields so we'd need to come up with an interesting way to *make* them visible. More than any other field type they need a \"design\".\r\n\r\nThe tricky thing is that `flattened` fields fundamentally don't know the names of their sub-fields. That's the point of them - we don't want any per-sub-field overhead. ESQL *could* load any particular sub-field of a flattened field similarly to a `keyword` field if you knew the name up front. I imagine we could push queries to it and all of that too.\r\n\r\nBut flattened fields need some way to load all of the sub-fields. Maybe some kind of object response format or something. But I'm not sure.","I'd also be curious on how Elastic is prioritizing this feature (understanding that it is a complex topic). `flattened` fields always seem to be somewhat of the odd one out (right in front of nested fields). One of my biggest frustrations with Elastic recently has been with the disconnect around `flattened` fields adoption and product support.\r\n\r\nOn one hand you have the Elastic integration teams widely adopting `flattened` fields, [~106 integrations using them](https:\/\/github.com\/search?q=repo%3Aelastic%2Fintegrations+%22%7C+flattened+%7C%22&type=code&p=1), but on the other hand you have other core Elastic products (EQL https:\/\/github.com\/elastic\/elasticsearch\/issues\/101985, ES|QL, Kibana https:\/\/github.com\/elastic\/kibana\/issues\/25820) that seem to stay far away from supporting them. This makes a significant portion of valuable data in Elasticsearch hard to use.\r\n\r\nI was hoping that ES|QL would cover this, https:\/\/github.com\/elastic\/kibana\/issues\/25820#issuecomment-1803659184, but as pointed out in that issue, and by this issue's existence, this gap in support has largely remained open and frustrating to deal with."],"labels":[">feature","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[test] Don't fail on BrokenBarrierException","body":"If the barrier has been reset that means the test has already completed and we don't care about node connection attempts and shouldn't throw an assertion error and fail the test\r\n\r\nResolve #105556\r\n\r\n","comments":[],"labels":[">test",":Distributed\/Distributed","v8.14.0"]},{"title":"Decouple `allowRemoteIndices` from the `Replaceable` interface","body":"### Description\n\nSome request types allow referencing indices from remote clusters, per the [allowsRemoteIndices](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6a8c1d0480d742765bf07fb027a0dbb2e3937bd3\/server\/src\/main\/java\/org\/elasticsearch\/action\/IndicesRequest.java#L58) method, while (most) other types permit the original requested index expression to be overriden during processing (e.g. by the Security authz code).\r\n\r\nBut, there is no inherent relationship between the 2 traits, so they should be completely decoupled (as of this writing, only \"replaceable\" types may be allowed to reference indices from remote clusters).","comments":["Pinging @elastic\/es-security (Team:Security)"],"labels":[">refactoring",":Security\/Authorization","Team:Security"]},{"title":"Unable to update the number of replica shards for the Watcher internal index","body":"### Elasticsearch Version\r\n\r\nVersion 8.8.0 and above\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nN.A\r\n\r\n### Problem Description\r\n\r\n### Background\r\n\r\nSince Elasticsearch version 8.8, users are able to update the number of replica shards for the `.watches` index using the [Update Watcher index settings API](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.8\/watcher-api-update-settings.html). This was the relevant PR: https:\/\/github.com\/elastic\/elasticsearch\/pull\/95342\r\n\r\n### Problem\r\n\r\nHowever, some users could encounter the below error when running the API. For example:\r\n\r\n```\r\nPUT \/_watcher\/settings\r\n{\r\n  \"index.auto_expand_replicas\": \"0-4\"\r\n}\r\n```\r\n\r\n- API output:\r\n\r\n```\r\n#! this request accesses system indices: [.watches-reindexed-for-8], but in a future major version, direct access to system indices will be prevented by default\r\n{\r\n    \"error\": {\r\n        \"root_cause\": [\r\n            {\r\n                \"type\": \"resource_not_found_exception\",\r\n                \"reason\": \"no Watches found on which to modify settings\"\r\n            }\r\n        ],\r\n        \"type\": \"resource_not_found_exception\",\r\n        \"reason\": \"no Watches found on which to modify settings\"\r\n    },\r\n    \"status\": 404\r\n}\r\n```\r\n\r\n### Root cause\r\n\r\nThe above errors happens because the `.watches` refers to an alias and not an index. The [Update Watcher index settings API](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.8\/watcher-api-update-settings.html) expects an index name (c.f [code](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/watcher\/src\/main\/java\/org\/elasticsearch\/xpack\/watcher\/transport\/actions\/TransportUpdateWatcherSettingsAction.java#L87)).\r\n\r\n### Workaround\r\n\r\nThe following steps can be followed to work around the problem. **Important**: we strongly recommend to take an Elasticsearch snapshot of the `watcher` feature state beforehand (c.f [Snapshot and restore](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/snapshot-restore.html))\r\n\r\n1) Stop Watcher: `POST _watcher\/_stop`\r\n\r\n2) Create a role\/user that can access the Watcher indices:\r\n\r\n```\r\nPOST _security\/role\/watches_superuser\r\n{\r\n  \"indices\": [\r\n    {\r\n      \"names\": [\r\n        \".watches*\"\r\n      ],\r\n      \"privileges\": [\r\n        \"all\"\r\n      ],\r\n      \"allow_restricted_indices\": true\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n```\r\nPOST _security\/user\/temp_user\r\n{\r\n  \"password\": \"temp_password\",\r\n  \"roles\": [\r\n    \"superuser\",\r\n    \"watches_superuser\"\r\n  ]\r\n}\r\n```\r\n\r\n3) Login to Kibana using the above `temp_user` with the associated password\r\n\r\n4) In Kibana Dev Tools, run the below APIs:\r\n\r\na) Delete the `.watches` alias. In our example above, the `.watches` alias is associated to index `.watches-reindexed-for-8`. If the `.watches` alias is associated to a different index, replace the index name accordingly. For example:\r\n\r\n`DELETE .watches-reindexed-for-8\/_alias\/.watches`\r\n\r\nb) Reindex the content of the corresponding index to the `.watches` index. For example:\r\n\r\n```\r\nPOST _reindex\r\n{\r\n  \"source\": {\r\n    \"index\": \".watches-reindexed-for-8\"\r\n  },\r\n  \"dest\": {\r\n    \"index\": \".watches\"\r\n  }\r\n}\r\n```\r\n\r\nc) Delete the previous index: `DELETE .watches-reindexed-for-8`\r\n\r\nd) Start Watcher: `POST _watcher\/_start`\r\n\r\ne) Delete `temp_user` user: `DELETE _security\/user\/temp_user`\r\n\r\nf) Delete `watches_superuser` role: `DELETE _security\/role\/watches_superuser`\r\n\r\ng) Log off from Kibana and log in again using another user and run the [Update Watcher index settings API](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/8.8\/watcher-api-update-settings.html) to update the number of replica shards.\r\n\r\nh) Verify that the API works\r\n\r\n### Steps to Reproduce\r\n\r\nIn ESS:\r\n\r\n1) Create a cluster version 6.8.23 and create a sample watch.\r\n\r\n2) Upgrade to version 7.17.x and run the Kibana Upgrade Assistant\r\n\r\n3) Upgrade to version 8.x \r\n\r\n4) Run the API:\r\n\r\n```\r\nPUT \/_watcher\/settings\r\n{\r\n  \"index.auto_expand_replicas\": \"0-4\"\r\n}\r\n```\r\n\r\nand notice the same error as reported above.\r\n\r\n5) Check the list of aliases and notice that the `.watches` is an alias associated to index  `.watches-reindexed-for-8`\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">bug",":Data Management\/Watcher","Team:Data Management"]},{"title":"Docs: Replace synced flush with flush for upgrades in 7.17 docs","body":"As flush is doing the same and synced flush endpoint is removed in 8.0 (and has been deprecated since 7.6 which was released in february 2020), it makes sense to use flush, so you will not run into issues after upgrading.","comments":["<!-- CLA-CHECK:105590 -->\n&#x1F49A; CLA has been signed","Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">docs",":Distributed\/Store","Team:Distributed","Team:Docs","external-contributor","v7.17.19"]},{"title":"Tsdb bwc id test","body":"Attempt to simulate id inconsistency spotted in #105452 ","comments":[],"labels":[">test",":StorageEngine\/TSDB","test-full-bwc","v8.14.0"]},{"title":"`ClusterFormationFailureHandler` shouldn't log anything once the node is stopping","body":"We expect a node to leave the cluster as part of its shutdown sequence, but if the shutdown takes a long time then today the `ClusterFormationFailureHandler` continues to log a warning every 10s. We shouldn't be logging those warnings, it's normal not to be in a cluster any more at this point in the lifecycle of a node.\r\n\r\nRelates https:\/\/discuss.elastic.co\/t\/master-not-discovered-or-elected-yet-when-stopping-elasticsearch-service\/353377","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">bug",":Distributed\/Cluster Coordination","Team:Distributed"]},{"title":"Add assertion that system index version hashes match at join time","body":"We need to start verifying that system index version hashes match so that we can trigger system index mappings updates appropriately.","comments":["Hi @williamrandolph, I've created a changelog YAML for you.","Pinging @elastic\/es-core-infra (Team:Core\/Infra)"],"labels":[">enhancement",":Core\/Infra\/Core","Team:Core\/Infra","v8.14.0"]},{"title":"[CI] NodeConnectionsServiceTests testOnlyBlocksOnConnectionsToNewNodes failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/m4vy4ftjv7cya\/tests\/:server:test\/org.elasticsearch.cluster.NodeConnectionsServiceTests\/testOnlyBlocksOnConnectionsToNewNodes\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:test' --tests \"org.elasticsearch.cluster.NodeConnectionsServiceTests.testOnlyBlocksOnConnectionsToNewNodes\" -Dtests.seed=2FFE3EE707EDD8D1 -Dtests.locale=nb -Dtests.timezone=Pacific\/Tahiti -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.cluster.NodeConnectionsServiceTests#testOnlyBlocksOnConnectionsToNewNodes`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testOnlyBlocksOnConnectionsToNewNodes'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.cluster.NodeConnectionsServiceTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.Exception: Test abandoned because suite timeout was reached.\n\n  at __randomizedtesting.SeedInfo.seed([2FFE3EE707EDD8D1]:0)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","> **Reproduces locally?:**\r\n> No\r\n\r\nFWIW this reproduced for me locally after ~350k iterations (~12h). I think that's a record."],"labels":[":Distributed\/Network",">test-failure","Team:Distributed","low-risk"]},{"title":"Add task to update min CCS version","body":"Add a task to update the min CCS transport version on release. Alternative to #105141","comments":["Pinging @elastic\/es-core-infra (Team:Core\/Infra)","Pinging @elastic\/es-delivery (Team:Delivery)","@elasticmachine update branch"],"labels":[":Delivery\/Build",":Core\/Infra\/Transport API",">refactoring","Team:Core\/Infra","Team:Delivery","v8.14.0"]},{"title":"[CI] MixedClusterEsqlSpecIT test {keep.AverageOfEvalValue} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/6gmqe4ivszcpq\/tests\/:x-pack:plugin:esql:qa:server:mixed-cluster:v8.14.0%23javaRestTest\/org.elasticsearch.xpack.esql.qa.mixed.MixedClusterEsqlSpecIT\/test%20%7Bkeep.AverageOfEvalValue%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:esql:qa:server:mixed-cluster:v8.14.0#javaRestTest' -Dtests.class=\"org.elasticsearch.xpack.esql.qa.mixed.MixedClusterEsqlSpecIT\" -Dtests.method=\"test {keep.AverageOfEvalValue}\" -Dtests.seed=3835B841284A3E99 -Dtests.bwc=true -Dtests.locale=zh -Dtests.timezone=America\/Argentina\/Buenos_Aires -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.esql.qa.mixed.MixedClusterEsqlSpecIT#test {keep.AverageOfEvalValue}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Bkeep.AverageOfEvalValue%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.esql.qa.mixed.MixedClusterEsqlSpecIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.junit.ComparisonFailure: Row[0] Column[0] expected:<27517.27973714994[7]> but was:<27517.27973714994[4]>\n\n  at __randomizedtesting.SeedInfo.seed([3835B841284A3E99:B061879B86B65361]:0)\n  at org.junit.Assert.assertEquals(Assert.java:117)\n  at org.elasticsearch.xpack.esql.CsvAssert.assertData(CsvAssert.java:223)\n  at org.elasticsearch.xpack.esql.qa.rest.EsqlSpecTestCase.assertResults(EsqlSpecTestCase.java:174)\n  at org.elasticsearch.xpack.esql.qa.rest.EsqlSpecTestCase.doTest(EsqlSpecTestCase.java:154)\n  at org.elasticsearch.xpack.esql.qa.rest.EsqlSpecTestCase.test(EsqlSpecTestCase.java:128)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","It seems a precision problem specific of Avg function, most likely the same problem is also causing this https:\/\/github.com\/elastic\/elasticsearch\/issues\/103866"],"labels":[">test-failure","Team:Analytics",":Analytics\/ES|QL","low-risk"]},{"title":"Test field caps non empty fields with engine closed","body":"\r\n\r\n_Originally posted by @javanna in https:\/\/github.com\/elastic\/elasticsearch\/pull\/105374#discussion_r1488492948_\r\n            ","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">test",":Search\/Search","Team:Search"]},{"title":"Elasticsearch DSL clause like `terms` on multiple fields","body":"### Description\n\nIn the Security Solution we have [indicator match rules](https:\/\/www.elastic.co\/guide\/en\/security\/current\/rules-ui-create.html#create-indicator-rule) that essentially `join` documents from an \"indicator index\" with the source data indices based on values in one or more fields. Indicators, or threats, are known suspicious values or tuples of values such as suspicious IP addresses, domains, etc. An indicator index might have thousands or tens of thousands of these indicators.\r\n\r\nTo join this indicator index with the source data, we typically load a few thousand documents from the indicator index and build a query to look for source documents that match **any** indicator from the list. If we are joining based on a single keyword field, the necessary logic can be encapsulated in a single `terms` clause:\r\n```\r\n{\r\n  \"terms\": {\r\n    \"source.ip\": [ \"127.0.0.1\", \"127.0.0.2\",... <thousands more>] \/\/ Each value in the array comes from a separate indicator document\r\n  }\r\n}\r\n```\r\nAs soon as we want to join on more than one field, though, each indicator document has to be put in a separate clause so we can match `\"(field A AND field B from document 1) OR (field A AND field B from document 2) OR ...\"`, e.g. if we want to join on `host.id` and `source.ip`,\r\n```\r\n{\r\n  should: [ \/\/ Each object in the should clause represents one indicator document. Every clause has the same structure and fields, just different values\r\n    {\r\n      bool: {\r\n        filter: [\r\n          {\r\n            term: { \/\/ Each term clause is a field from an indicator document \r\n              'host.id': 1,\r\n            },\r\n          },\r\n          {\r\n            term: {\r\n              'source.ip': '127.0.0.1',\r\n            },\r\n          },\r\n        ],\r\n      },\r\n    },\r\n    {\r\n      bool: {\r\n        filter: [\r\n          {\r\n            term: {\r\n              'host.id': 2,\r\n            },\r\n          },\r\n          {\r\n            term: {\r\n              'source.ip': '127.0.0.2',\r\n            },\r\n          },\r\n        ],\r\n      },\r\n    },\r\n  ],\r\n}\r\n```\r\nThe effect here is that we can query for tens of thousands of indicators at once with only one clause if we're joining on a single field, but as soon as a second field is introduced the number of clauses explodes and `max_clause_count` limits us.\r\n\r\nWhat would be very useful is something like `terms` clauses but for multiple fields at once, so that even in the multi-field case we can use a single query for many indicators.\r\n```\r\n{\r\n  \"multi_field_terms\": {\r\n    \"fields\": [\"host.id\", \"user.ip],\r\n    \"terms\": [[1, 127.0.0.1], [2, 127.0.0.2], ...] \/\/ We want docs where (host.id==1 AND user.ip==127.0.0.1) OR (host.id==2 AND user.ip==127.0.0.2) etc\r\n  }\r\n}\r\n```\r\nWe could accomplish something similar with runtime fields today to merge the values in the different fields into a single value then use `terms` on that runtime field, but runtime fields come with their own performance hit and the query builder implementation would be more complex.","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">enhancement",":Search\/Search","Team:Search"]},{"title":"[ML] Double deployment of trained model causes assertion error","body":"### Elasticsearch Version\r\n\r\n8.14.0-SNAPSHOT\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\nJBR-17.0.9+8-1166.2-nomod\r\n\r\n### OS Version\r\n\r\n23.3.0 Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T6000 arm64\r\n\r\n### Problem Description\r\n\r\nWhen locally building elasticsearch (in debug mode), an assertion error occurs when attempting to perform inference.\r\n\r\n```\r\n[2024-02-14T14:14:06,790][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [runTask-0] fatal error in thread [elasticsearch[runTask-0][ml_native_inference_comms][T#3]], exiting java.lang.AssertionError\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.deployment.NlpInferenceInput.extractInput(NlpInferenceInput.java:55)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.deployment.InferencePyTorchAction.doRun(InferencePyTorchAction.java:104)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.pytorch.PriorityProcessWorkerExecutorService$OrderedRunnable.run(PriorityProcessWorkerExecutorService.java:58)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.job.process.AbstractProcessWorkerExecutorService.start(AbstractProcessWorkerExecutorService.java:122)\r\n        at java.base\/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\r\n        at java.base\/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:917)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n        at java.base\/java.lang.Thread.run(Thread.java:1583)\r\n\r\n```\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1. create deployment (I don't think this necessarily needs to be with the inference service, but thats what I tried)\r\n\r\n```\r\n(base) mh@Maxs-MacBook-Pro elasticsearch % curl -X PUT \"localhost:9200\/_inference\/text_embedding\/a-deployment-id2?pretty\" \\\r\n-H 'Content-Type: application\/json' -u elastic-admin:elastic-password \\\r\n-d'\r\n  {\r\n    \"service\": \"text_embedding\",\r\n    \"service_settings\": {\r\n      \"num_allocations\": 1,\r\n      \"num_threads\": 1,\r\n      \"model_id\": \".multilingual-e5-small\"\r\n    }\r\n  }\r\n'\r\n{\r\n  \"model_id\" : \"a-deployment-id2\",\r\n  \"task_type\" : \"text_embedding\",\r\n  \"service\" : \"text_embedding\",\r\n  \"service_settings\" : {\r\n    \"num_allocations\" : 1,\r\n    \"num_threads\" : 1,\r\n    \"model_id\" : \".multilingual-e5-small\"\r\n  },\r\n  \"task_settings\" : { }\r\n}\r\n```\r\n\r\n2. Put the same model:\r\n\r\n```\r\nPUT \/_ml\/trained_models\/.multilingual-e5-small?pretty\r\n{\r\n  \"input\": {\r\n\t\"field_names\": [\"text_field\"]\r\n }\r\n}\r\n```\r\n3. Run inference and the process crashes due to an assertion error.\r\n```\r\n[2024-02-14T14:14:06,790][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [runTask-0] fatal error in thread [elasticsearch[runTask-0][ml_native_inference_comms][T#3]], exiting java.lang.AssertionError\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.deployment.NlpInferenceInput.extractInput(NlpInferenceInput.java:55)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.deployment.InferencePyTorchAction.doRun(InferencePyTorchAction.java:104)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.inference.pytorch.PriorityProcessWorkerExecutorService$OrderedRunnable.run(PriorityProcessWorkerExecutorService.java:58)\r\n        at org.elasticsearch.ml@8.14.0-SNAPSHOT\/org.elasticsearch.xpack.ml.job.process.AbstractProcessWorkerExecutorService.start(AbstractProcessWorkerExecutorService.java:122)\r\n        at java.base\/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\r\n        at java.base\/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\n        at org.elasticsearch.server@8.14.0-SNAPSHOT\/org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:917)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n        at java.base\/java.lang.Thread.run(Thread.java:1583)\r\n```\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/ml-core (Team:ML)","So the tripping assertion is this: https:\/\/github.com\/elastic\/elasticsearch\/blob\/e443a7b6baed6ad10d3ba1c978c2228d9280b94e\/x-pack\/plugin\/ml\/src\/main\/java\/org\/elasticsearch\/xpack\/ml\/inference\/deployment\/NlpInferenceInput.java#L55","I also disabled the assertion and it does cause the process to crash, so I'm actively investigating this one","On main I tried just putting the inference service and then doing the reindex, and I am still getting this error. I am going to try to do a clean build to make sure this isn't just picking up some bad artifact from one of my development branches. I would be surprised if this error is actually occurring on main as it appears to be preventing any inference with the internal inference services.","Still on main, I did a gradlew clean and a gradlew build. The build failed for some seemingly unrelated reason about a search outtage \/\/ all shards failed. A lot of builds are failing this morning it seems, so probably someone broke something. \r\n\r\nI tried adding a new integration test to test for a double deployment issue, but the integration tests passed. I tried running the server and doing a manual test, and it failed again."],"labels":[">bug",":ml","Team:ML","v8.13.0","v8.14.0"]},{"title":"ESQL: document CASE with columns, not just values","body":"In the [docs for CASE](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/master\/esql-functions-operators.html#esql-case), we should say that we can use expressions, not just values; additionally, we should add an example for this. Something like\r\n```\r\nFROM sample_data\r\n| EVAL error = CASE(message LIKE \"*error*\", message, \"not an error message\")\r\n```\r\nor, more usefully\r\n```\r\nFROM sample_data\r\n| EVAL error_messages = CASE(message LIKE \"*error*\", message, null)\r\n| STATS distinct_error_messages = COUNT_DISTINCT(error_message)\r\n```","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">docs","Team:Docs","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[CI] TermsDocCountErrorIT testStringValueFieldSingleShard failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/nmvhirxqjhgio\/tests\/:server:internalClusterTest\/org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT\/testStringValueFieldSingleShard\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT.testStringValueFieldSingleShard\" -Dtests.seed=BC1F3C6CF02B639F -Dtests.locale=ar-YE -Dtests.timezone=SystemV\/PST8 -Druntime.java=17 -Dtests.fips.enabled=true\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT#testStringValueFieldSingleShard`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testStringValueFieldSingleShard'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: <0L>\n     but: was <2L>\n\n  at __randomizedtesting.SeedInfo.seed([BC1F3C6CF02B639F:ADBD5BE50A343DAC]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)\n  at org.elasticsearch.test.ESTestCase.assertThat(ESTestCase.java:2119)\n  at org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT.assertNoDocCountError(TermsDocCountErrorIT.java:222)\n  at org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT.lambda$testStringValueFieldSingleShard$2(TermsDocCountErrorIT.java:310)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.lambda$assertNoFailuresAndResponse$9(ElasticsearchAssertions.java:354)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertResponse(ElasticsearchAssertions.java:375)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailuresAndResponse(ElasticsearchAssertions.java:352)\n  at org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT.lambda$testStringValueFieldSingleShard$3(TermsDocCountErrorIT.java:301)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.lambda$assertNoFailuresAndResponse$9(ElasticsearchAssertions.java:354)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertResponse(ElasticsearchAssertions.java:375)\n  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailuresAndResponse(ElasticsearchAssertions.java:352)\n  at org.elasticsearch.search.aggregations.bucket.TermsDocCountErrorIT.testStringValueFieldSingleShard(TermsDocCountErrorIT.java:292)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)\n  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:568)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:833)\n\n```","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","I had a look and this test seems to be failing very rarely. I was not able to reproduce locally but setting low risk as it seems very rare"],"labels":[":Analytics\/Aggregations",">test-failure","Team:Analytics","low-risk"]},{"title":"Move test only code out of TransportShardBulkAction into test codebase","body":"The overload is only used in tests, move it to the replication test case as a utility. Also, we always use the thread-pool for the current absolute time in production and can always use it in tests -> remove one argument here to make this code a little less confusing.\r\n","comments":["Pinging @elastic\/es-distributed (Team:Distributed)"],"labels":[">non-issue",":Distributed\/CRUD","Team:Distributed","v8.14.0"]},{"title":"[CI] DownsampleActionIT testDownsampleTwiceSameInterval failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/kn7wwwezqdmta\/tests\/:x-pack:plugin:ilm:qa:multi-node:javaRestTest\/org.elasticsearch.xpack.ilm.actions.DownsampleActionIT\/testDownsampleTwiceSameInterval\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ilm:qa:multi-node:javaRestTest' --tests \"org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testDownsampleTwiceSameInterval\" -Dtests.seed=11F47EEC69862F8C -Dtests.locale=en-AU -Dtests.timezone=Asia\/Bishkek -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.ilm.actions.DownsampleActionIT#testDownsampleTwiceSameInterval`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testDownsampleTwiceSameInterval'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.ilm.actions.DownsampleActionIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: \nExpected: is <true>\n     but: was <false>\n\n  at __randomizedtesting.SeedInfo.seed([11F47EEC69862F8C:CC57A5BD1735D5B6]:0)\n  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n  at org.junit.Assert.assertThat(Assert.java:956)\n  at org.junit.Assert.assertThat(Assert.java:923)\n  at org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.lambda$testDownsampleTwiceSameInterval$20(DownsampleActionIT.java:553)\n  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1278)\n  at org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testDownsampleTwiceSameInterval(DownsampleActionIT.java:552)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)","OK, this failed again, I am going to mute, it seems like a valid failure caused by some internal race condition.\r\n\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/ysgqivco4ajcs\r\n\r\nSince it isn't reliably reproducible, I feel for the individual who has to debug it, seems like a weird race condition thing :).\r\n\r\nIt will likely require being unmuted + using trace logging to fully debug."],"labels":["blocker",">test-failure",":StorageEngine\/Downsampling","Team:StorageEngine"]},{"title":"[CI] ConcurrentSnapshotsIT testDeleteIndexWithOutOfOrderFinalization failing","body":"Might be transient, but opening the ticket just in case, so it's first taken a deeper look.\n\n```\n  1> [2024-02-14T02:38:36,051][INFO ][o.e.c.r.a.AllocationService] [node_t0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[index-2][0]]]).\n  1> [2024-02-14T02:38:36,065][INFO ][o.e.s.ConcurrentSnapshotsIT] [testDeleteIndexWithOutOfOrderFinalization] --> creating or updating repository [test-repo] [fs]\n  1> [2024-02-14T02:38:36,077][INFO ][o.e.r.RepositoriesService] [node_t0] put repository [test-repo]\n  1> [2024-02-14T02:38:36,108][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-0\/qPioiixeSi-2KOFwisJKVw] started\n  1> [2024-02-14T02:38:36,114][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-1\/BiR5Q9HLT0Sn0SPAP-ZxLA] started\n  1> [2024-02-14T02:38:36,126][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-2\/XlkfD5MAQ62f9V-WL4Ml1Q] started\n  1> [2024-02-14T02:38:36,174][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-1\/BiR5Q9HLT0Sn0SPAP-ZxLA] completed with state [SUCCESS]\n  1> [2024-02-14T02:38:36,199][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-2\/XlkfD5MAQ62f9V-WL4Ml1Q] completed with state [SUCCESS]\n  1> [2024-02-14T02:38:36,222][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:snapshot-with-index-0\/qPioiixeSi-2KOFwisJKVw] completed with state [SUCCESS]\n  2> \u4e8c\u6708 14, 2024 2:39:06 \u4e0a\u5348 com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\n  2> \u8b66\u544a: Uncaught exception in thread: Thread[Thread-64,5,TGRP-ConcurrentSnapshotsIT]\n  2> java.lang.AssertionError: java.util.concurrent.TimeoutException: Timeout waiting for task.\n  2> \tat __randomizedtesting.SeedInfo.seed([E41F8D023ED55CA0]:0)\n  2> \tat org.elasticsearch.snapshots.ConcurrentSnapshotsIT.lambda$testDeleteIndexWithOutOfOrderFinalization$24(ConcurrentSnapshotsIT.java:2116)\n  2> \tat java.base\/java.lang.Thread.run(Thread.java:829)\n  2> Caused by: java.util.concurrent.TimeoutException: Timeout waiting for task.\n  2> \tat org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:228)\n  2> \tat org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:53)\n  2> \tat org.elasticsearch.test.ClusterServiceUtils.awaitClusterState(ClusterServiceUtils.java:234)\n  2> \tat org.elasticsearch.snapshots.AbstractSnapshotIntegTestCase.awaitClusterState(AbstractSnapshotIntegTestCase.java:571)\n  2> \tat org.elasticsearch.snapshots.AbstractSnapshotIntegTestCase.awaitClusterState(AbstractSnapshotIntegTestCase.java:563)\n  2> \tat org.elasticsearch.snapshots.ConcurrentSnapshotsIT.lambda$testDeleteIndexWithOutOfOrderFinalization$24(ConcurrentSnapshotsIT.java:2093)\n  2> \t... 1 more\n\n  1> [2024-02-14T02:39:06,240][INFO ][o.e.s.ConcurrentSnapshotsIT] [testDeleteIndexWithOutOfOrderFinalization] --> creating full snapshot [final-full-snapshot] in [test-repo]\n  1> [2024-02-14T02:39:06,249][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:final-full-snapshot\/U7OBQ7rBQeWDoDC_tOVq0Q] started\n  1> [2024-02-14T02:39:06,303][INFO ][o.e.s.ConcurrentSnapshotsIT] [testDeleteIndexWithOutOfOrderFinalization] --> deleting snapshot [final-full-snapshot] from repo [test-repo]\n  1> [2024-02-14T02:39:06,303][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshot [test-repo:final-full-snapshot\/U7OBQ7rBQeWDoDC_tOVq0Q] completed with state [SUCCESS]\n  1> [2024-02-14T02:39:06,304][INFO ][o.e.s.SnapshotsService   ] [testDeleteIndexWithOutOfOrderFinalization] deleting snapshots [final-full-snapshot] from repository [test-repo]\n  1> [2024-02-14T02:39:06,344][INFO ][o.e.s.SnapshotsService   ] [node_t0] snapshots [final-full-snapshot\/U7OBQ7rBQeWDoDC_tOVq0Q] deleted\n  1> [2024-02-14T02:39:06,350][INFO ][o.e.s.SnapshotsService   ] [node_t0] deleting snapshots [old-version-snapshot-*] from repository [test-repo]\n  1> [2024-02-14T02:39:06,352][INFO ][o.e.a.a.c.r.c.TransportCleanupRepositoryAction] [testDeleteIndexWithOutOfOrderFinalization] Running cleanup operations on repository [test-repo][4]\n  1> [2024-02-14T02:39:06,361][INFO ][o.e.a.a.c.r.c.TransportCleanupRepositoryAction] [node_t0] Done with repository cleanup on [test-repo][4] with result [{\"deleted_bytes\":0,\"deleted_blobs\":0}]\n  1> [2024-02-14T02:39:06,395][INFO ][o.e.s.ConcurrentSnapshotsIT] [testDeleteIndexWithOutOfOrderFinalization] [ConcurrentSnapshotsIT#testDeleteIndexWithOutOfOrderFinalization]: cleaning up after test\n  1> [2024-02-14T02:39:06,411][INFO ][o.e.c.m.MetadataDeleteIndexService] [node_t0] [index-to-delete\/A8vj5S5DTlyt-LjgCxW9Ag] deleting index\n  1> [2024-02-14T02:39:06,411][INFO ][o.e.c.m.MetadataDeleteIndexService] [node_t0] [index-2\/ZdixiPpISiyCa4brRsR4DA] deleting index\n  1> [2024-02-14T02:39:06,411][INFO ][o.e.c.m.MetadataDeleteIndexService] [node_t0] [index-0\/9KYqsU3KRX6ipZqsABwW9Q] deleting index\n```\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/scqs3t3grvcnm\/tests\/:server:internalClusterTest\/org.elasticsearch.snapshots.ConcurrentSnapshotsIT\/testDeleteIndexWithOutOfOrderFinalization\n\n\n**Reproduction line:**\n```\n.\/gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.snapshots.ConcurrentSnapshotsIT.testDeleteIndexWithOutOfOrderFinalization\" -Dtests.seed=E41F8D023ED55CA0 -Dtests.locale=zh-Hant-TW -Dtests.timezone=Antarctica\/McMurdo -Druntime.java=11\n```\n\n**Applicable branches:**\n7.17\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.snapshots.ConcurrentSnapshotsIT#testDeleteIndexWithOutOfOrderFinalization`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testDeleteIndexWithOutOfOrderFinalization'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.snapshots.ConcurrentSnapshotsIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: java.util.concurrent.TimeoutException: Timeout waiting for task.\n\n  at __randomizedtesting.SeedInfo.seed([E41F8D023ED55CA0]:0)\n  at org.elasticsearch.snapshots.ConcurrentSnapshotsIT.lambda$testDeleteIndexWithOutOfOrderFinalization$24(ConcurrentSnapshotsIT.java:2116)\n  at java.lang.Thread.run(Thread.java:829)\n\n  Caused by: java.util.concurrent.TimeoutException: Timeout waiting for task.\n\n    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:228)\n    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:53)\n    at org.elasticsearch.test.ClusterServiceUtils.awaitClusterState(ClusterServiceUtils.java:234)\n    at org.elasticsearch.snapshots.AbstractSnapshotIntegTestCase.awaitClusterState(AbstractSnapshotIntegTestCase.java:571)\n    at org.elasticsearch.snapshots.AbstractSnapshotIntegTestCase.awaitClusterState(AbstractSnapshotIntegTestCase.java:563)\n    at org.elasticsearch.snapshots.ConcurrentSnapshotsIT.lambda$testDeleteIndexWithOutOfOrderFinalization$24(ConcurrentSnapshotsIT.java:2093)\n    at java.lang.Thread.run(Thread.java:829)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","The test comes from #103817 and it was backport to 7.17 where it fails regularly. I'm gonna mute it on the 7.17 branch"],"labels":[":Distributed\/Snapshot\/Restore",">test-failure","Team:Distributed","low-risk"]},{"title":"Index template regression when composed of component template with different mapping notations","body":"### Description\n\nOn 8.11.0+, if we have a component templates that defines a mapping with a different notation than the index template that uses it (dotted notation vs object with property subobject) we get a mapper parsing exception.\r\nA concrete example of this (component template defined as object with property subobject):\r\n```\r\nPUT _component_template\/component-template-test\r\n{\r\n  \"template\": {\r\n    \"mappings\": {\r\n      \"properties\": {\r\n        \"object\": {\r\n          \"properties\": {\r\n            \"subobject\": {\r\n              \"type\": \"keyword\"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nWhile the index template that uses the component template has in the mapping definition the dotted notation:\r\n```\r\nPOST _index_template\/_simulate\r\n{\r\n  \"index_patterns\": [\r\n    \"smops-test-*\"\r\n  ],\r\n  \"composed_of\": [\r\n    \"component-template-test\"\r\n  ],\r\n  \"template\": {\r\n    \"mappings\": {\r\n      \"properties\": {\r\n        \"object.subobject\": {\r\n          \"type\": \"version\"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\nthrows this error:\r\n```\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"composable template [simulate_template_nzv683ensognv2oxiqvyca] template after composition with component templates [component-template-test] is invalid\"\r\n      }\r\n    ],\r\n    \"type\": \"illegal_argument_exception\",\r\n    \"reason\": \"composable template [simulate_template_nzv683ensognv2oxiqvyca] template after composition with component templates [component-template-test] is invalid\",\r\n    \"caused_by\": {\r\n      \"type\": \"illegal_argument_exception\",\r\n      \"reason\": \"invalid composite mappings for [simulate_template_nzv683ensognv2oxiqvyca]\",\r\n      \"caused_by\": {\r\n        \"type\": \"mapper_parsing_exception\",\r\n        \"reason\": \"Failed to parse mapping: mapper [object.subobject] cannot be changed from type [keyword] to [version]\",\r\n        \"caused_by\": {\r\n          \"type\": \"illegal_argument_exception\",\r\n          \"reason\": \"mapper [object.subobject] cannot be changed from type [keyword] to [version]\"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"status\": 400\r\n}\r\n```\r\nThe same happens if the dotted notation is used in the component template while the object with property subobject is used in the index template.\r\nThis was not the case in versions < 8.11.0. The regression was possibly introduced by https:\/\/github.com\/elastic\/elasticsearch\/pull\/97317","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">regression",":Search\/Mapping","Team:Search"]},{"title":"[CI] SmokeTestMultiNodeClientYamlTestSuiteIT test {yaml=suggest\/30_context\/Indexing and Querying without contexts is forbidden} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/4klghj6zpvyaw\/tests\/:qa:smoke-test-multinode:yamlRestTest\/org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT\/test%20%7Byaml=suggest%2F30_context%2FIndexing%20and%20Querying%20without%20contexts%20is%20forbidden%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':qa:smoke-test-multinode:yamlRestTest' --tests \"org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT.test {yaml=suggest\/30_context\/Indexing and Querying without contexts is forbidden}\" -Dtests.seed=F464A069D16BB6D1 -Dtests.locale=es-MX -Dtests.timezone=Indian\/Kerguelen -Druntime.java=18\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT#test {yaml=suggest\/30_context\/Indexing and Querying without contexts is forbidden}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('test%20%7Byaml%3Dsuggest\/30_context\/Indexing%20and%20Querying%20without%20contexts%20is%20forbidden%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.smoketest.SmokeTestMultiNodeClientYamlTestSuiteIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Failure at [suggest\/30_context:360]: the error message was expected to match the provided regex but didn't\nExpected: Missing mandatory contexts in context query\n     but: was \"{root_cause=[], type=search_phase_execution_exception, reason=, phase=query, grouped=true, failed_shards=[], caused_by={type=search_phase_execution_exception, reason=Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error., phase=query, grouped=true, failed_shards=[], stack_trace=Failed to execute phase [query], Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error.\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchPhase.doCheckNoMissingShards(SearchPhase.java:61)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:233)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:454)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:223)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1166)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:914)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$8(TransportSearchAction.java:342)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:109)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:455)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:309)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:113)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:124)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:107)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\\n\\tat org.elasticsearch.security@8.13.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:69)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:458)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:554)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:431)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:127)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:117)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:833)\\n}, stack_trace=Failed to execute phase [query], \\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:712)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:459)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:223)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1166)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:914)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$8(TransportSearchAction.java:342)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:109)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:455)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:309)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:113)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:124)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:107)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\\n\\tat org.elasticsearch.security@8.13.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:69)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:458)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:554)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:431)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:127)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:117)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:833)\\nCaused by: Failed to execute phase [query], Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error.\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchPhase.doCheckNoMissingShards(SearchPhase.java:61)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:233)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:454)\\n\\t... 66 more\\n}\"\n\n  at __randomizedtesting.SeedInfo.seed([F464A069D16BB6D1:7C309FB37F97DB29]:0)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:561)\n  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n  at java.lang.reflect.Method.invoke(Method.java:577)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:833)\n\n  Caused by: java.lang.AssertionError: the error message was expected to match the provided regex but didn't\n  Expected: Missing mandatory contexts in context query\n       but: was \"{root_cause=[], type=search_phase_execution_exception, reason=, phase=query, grouped=true, failed_shards=[], caused_by={type=search_phase_execution_exception, reason=Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error., phase=query, grouped=true, failed_shards=[], stack_trace=Failed to execute phase [query], Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error.\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchPhase.doCheckNoMissingShards(SearchPhase.java:61)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:233)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:454)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:223)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1166)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:914)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$8(TransportSearchAction.java:342)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:109)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:455)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:309)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:113)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:124)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:107)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\\n\\tat org.elasticsearch.security@8.13.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:69)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:458)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:554)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:431)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:127)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:117)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:833)\\n}, stack_trace=Failed to execute phase [query], \\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:712)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:459)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.start(AbstractSearchAsyncAction.java:223)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeSearch(TransportSearchAction.java:1166)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeLocalSearch(TransportSearchAction.java:914)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.lambda$executeRequest$8(TransportSearchAction.java:342)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.ActionListenerImplementations$ResponseWrappingActionListener.onResponse(ActionListenerImplementations.java:245)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:109)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.query.Rewriteable.rewriteAndFetch(Rewriteable.java:77)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.executeRequest(TransportSearchAction.java:455)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:309)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:113)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:96)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:68)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.tasks.TaskManager.registerAndExecute(TaskManager.java:196)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.node.NodeClient.executeLocally(NodeClient.java:105)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.RestCancellableNodeClient.doExecute(RestCancellableNodeClient.java:81)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.client.internal.support.AbstractClient.execute(AbstractClient.java:356)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.action.search.RestSearchAction.lambda$prepareRequest$2(RestSearchAction.java:124)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:107)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:452)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController$1.onResponse(RestController.java:446)\\n\\tat org.elasticsearch.security@8.13.0-SNAPSHOT\/org.elasticsearch.xpack.security.rest.SecurityRestFilter.intercept(SecurityRestFilter.java:69)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:446)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:606)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:329)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:458)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:554)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:431)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.handlePipelinedRequest(Netty4HttpPipeliningHandler.java:127)\\n\\tat org.elasticsearch.transport.netty4@8.13.0-SNAPSHOT\/org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:117)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.codec@4.1.94.Final\/io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:689)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:652)\\n\\tat io.netty.transport@4.1.94.Final\/io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\\n\\tat io.netty.common@4.1.94.Final\/io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat java.base\/java.lang.Thread.run(Thread.java:833)\\nCaused by: Failed to execute phase [query], Search rejected due to missing shards [[test][0]]. Consider using `allow_partial_search_results` setting to bypass this error.\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.SearchPhase.doCheckNoMissingShards(SearchPhase.java:61)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.run(AbstractSearchAsyncAction.java:233)\\n\\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.action.search.AbstractSearchAsyncAction.executePhase(AbstractSearchAsyncAction.java:454)\\n\\t... 66 more\\n}\"\n\n    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)\n    at org.junit.Assert.assertThat(Assert.java:964)\n    at org.elasticsearch.test.rest.yaml.section.DoSection.checkResponseException(DoSection.java:543)\n    at org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:385)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:541)\n    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:504)\n    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n    at java.lang.reflect.Method.invoke(Method.java:577)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n    at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:833)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)","Pinging @elastic\/es-distributed (Team:Distributed)","The cause of the failure:\r\n\r\n```\r\n[2024-02-13T10:31:31,680][WARN ][o.e.i.e.Engine           ] [test-cluster-0] [test][0] failed engine [refresh failed source[peer-recovery]] org.apache.lucene.index.CorruptIndexException: Problem reading index from store(ByteSizeCachingDirectory(HybridDirectory@\/dev\/shm\/bk\/bk-agent-prod-gcp-1707800573979169837\/elastic\/elasticsearch-periodic\/qa\/smoke-test-multinode\/build\/testrun\/yamlRestTest\/temp\/test-cluster11011275744689617495\/test-cluster-0\/data\/indices\/HVq8s0MwSsadd0FggN9q0A\/0\/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@54a98358)) (resource=store(ByteSizeCachingDirectory(HybridDirectory@\/dev\/shm\/bk\/bk-agent-prod-gcp-1707800573979169837\/elastic\/elasticsearch-periodic\/qa\/smoke-test-multinode\/build\/testrun\/yamlRestTest\/temp\/test-cluster11011275744689617495\/test-cluster-0\/data\/indices\/HVq8s0MwSsadd0FggN9q0A\/0\/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@54a98358)))\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:165)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:96)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:178)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.ReadersAndUpdates.getLatestReader(ReadersAndUpdates.java:243)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.SoftDeletesRetentionMergePolicy.keepFullyDeletedSegment(SoftDeletesRetentionMergePolicy.java:82)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.FilterMergePolicy.keepFullyDeletedSegment(FilterMergePolicy.java:118)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.FilterMergePolicy.keepFullyDeletedSegment(FilterMergePolicy.java:118)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.ReadersAndUpdates.keepFullyDeletedSegment(ReadersAndUpdates.java:822)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter.isFullyDeleted(IndexWriter.java:6078)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter.publishFlushedSegment(IndexWriter.java:2895)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter.lambda$publishFlushedSegments$26(IndexWriter.java:5919)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:102)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:133)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriter.purgeFlushTickets(DocumentsWriter.java:188)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter.publishFlushedSegments(IndexWriter.java:5895)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter$1.afterSegmentsFlushed(IndexWriter.java:437)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:531)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriter.maybeFlush(DocumentsWriter.java:446)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:636)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:576)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:381)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:355)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:345)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.FilterDirectoryReader.doOpenIfChanged(FilterDirectoryReader.java:112)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)\r\n\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.engine.ElasticsearchReaderManager.refreshIfNeeded(ElasticsearchReaderManager.java:48)\r\n\tat org.elasticsearch.server@8.13.0-SNAPSHOT\/org.elasticsearch.index.engine.ElasticsearchReaderManager.refreshIfNeeded(ElasticsearchReaderManager.java:27)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:167)\r\n<snip>\r\nCaused by: java.io.FileNotFoundException: No sub-file with id .kdi found in compound file \"_1.cfs\" (fileName=_1.kdi files: [_0.pos, .nvm, .fnm, _0.tip, _Lucene90_0.dvd, _0.doc, _0.tim, _Lucene90_0.dvm, _ES87BloomFilter_0.bfm, .fdm, .nvd, _ES87BloomFilter_0.bfi, _0.tmd, .fdx, .fdt])\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.codecs.lucene90.Lucene90CompoundReader.openInput(Lucene90CompoundReader.java:170)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.codecs.lucene90.Lucene90PointsReader.<init>(Lucene90PointsReader.java:63)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.codecs.lucene90.Lucene90PointsFormat.fieldsReader(Lucene90PointsFormat.java:74)\r\n\tat org.apache.lucene.core@9.9.2\/org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:152)\r\n\t... 70 more\r\n```\r\n\r\nAssigning distributed team.","I think it's the same error as #105122 and https:\/\/github.com\/elastic\/elasticsearch\/issues\/99075"],"labels":[":Search\/Search",">test-failure","Team:Search","medium-risk"]},{"title":"Enhance `RankBuilder`s to be able to access additional data after the query phase","body":"Currently, `RankBuilder` is limited to only gather data during the query phase. This limitation is fine for RRF, but is not sufficient to support future ranking enhancements. Collecting data during the query phase means that it must be collected for all documents which is very inefficient for re-ranking on only top documents. We should add a way for additional data to be collected during the fetch phase (or possibly even a new phase in between query and fetch) so re-ranking can take place with data gathered efficiently on only top documents after the fetch phase prior to a search response.","comments":["Pinging @elastic\/es-search (Team:Search)"],"labels":[">enhancement",":Search\/Ranking","Team:Search"]},{"title":"ESQL: Sum, Min, Max and Avg of constants","body":"Addresses part of https:\/\/github.com\/elastic\/elasticsearch\/issues\/100634.\r\n\r\nAllow expressions like `... | STATS sum([1, -9]), sum(null), min(21.0*3), avg([1,2,3])`.\r\n\r\nThis is achieved by substituting `sum(const)` by `mv_sum(const)*count(*)` and `min(const)` by `mv_min(const)` (and similarly for `max` and `avg`).","comments":["Hi @alex-spies, I've created a changelog YAML for you.","Pinging @elastic\/es-analytical-engine (Team:Analytics)","Implemented `avg(const)` and plan to add min\/max as well.\r\n\r\nI'd like to do `percentile` in a separate PR, though, as this PR's approach would require adding a `mv_percentile` function first: `percentile([1,2,3], 75)` would be a surrogate for the corresponding `mv_`-function then. I can implement `median`, though."],"labels":[">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"[Transform] Add skip_dest_index_creation setting","body":"DRAFT","comments":[],"labels":["WIP","v8.14.0"]},{"title":"[CI] IndexingIT testIndexing {upgradedNodes=2} failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/ened3yzjpisnq\/tests\/:qa:rolling-upgrade:v8.10.3%23bwcTest\/org.elasticsearch.upgrades.IndexingIT\/testIndexing%20%7BupgradedNodes=2%7D\n\n\n**Reproduction line:**\n```\n.\/gradlew ':qa:rolling-upgrade:v8.10.3#bwcTest' -Dtests.class=\"org.elasticsearch.upgrades.IndexingIT\" -Dtests.method=\"testIndexing {upgradedNodes=2}\" -Dtests.seed=53F4AE70D163BC85 -Dtests.bwc=true -Dtests.locale=ar-AE -Dtests.timezone=Africa\/Mogadishu -Druntime.java=21\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.upgrades.IndexingIT#testIndexing {upgradedNodes=2}`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testIndexing%20%7BupgradedNodes%3D2%7D'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.upgrades.IndexingIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.elasticsearch.client.ResponseException: method [GET], host [http:\/\/[::1]:34545], URI [\/_cluster\/health?wait_for_nodes=3&wait_for_status=yellow], status line [HTTP\/1.1 408 Request Timeout]\n{\"cluster_name\":\"test-cluster\",\"status\":\"red\",\"timed_out\":true,\"number_of_nodes\":2,\"number_of_data_nodes\":2,\"active_primary_shards\":5,\"active_shards\":10,\"relocating_shards\":0,\"initializing_shards\":0,\"unassigned_shards\":2,\"delayed_unassigned_shards\":0,\"number_of_pending_tasks\":0,\"number_of_in_flight_fetch\":0,\"task_max_waiting_in_queue_millis\":0,\"active_shards_percent_as_number\":83.33333333333334}\n\n  at __randomizedtesting.SeedInfo.seed([53F4AE70D163BC85:B72EE179F716FF0E]:0)\n  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:351)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:317)\n  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:292)\n  at org.elasticsearch.upgrades.IndexingIT.testIndexing(IndexingIT.java:63)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.elasticsearch.test.cluster.local.DefaultLocalElasticsearchCluster$1.evaluate(DefaultLocalElasticsearchCluster.java:47)\n  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-distributed (Team:Distributed)","Hm actually this whole suite seems pretty damn flaky right now:\r\n\r\n<img width=\"1163\" alt=\"image\" src=\"https:\/\/github.com\/elastic\/elasticsearch\/assets\/5058284\/4e1e42fe-6939-40b4-973e-bd986cef81f4\">\r\n\r\nSee e.g. latest failure: https:\/\/gradle-enterprise.elastic.co\/s\/wdzyszxt4ckmw\r\n","Looks to be a TSDB issue:\r\n\r\n```\r\n[2024-02-13T11:42:27,715][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [v8.12.2-0] fatal error in thread [elasticsearch[v8.12.2-0][generic][T#5]], exiting\r\njava.lang.AssertionError: unexpected failure while replicating translog entry\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.lambda$indexTranslogOperations$4(RecoveryTarget.java:463) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.action.ActionListener.completeWith(ActionListener.java:270) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.indexTranslogOperations(RecoveryTarget.java:432) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$TranslogOperationsRequestHandler.performTranslogOps(PeerRecoveryTargetService.java:611) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$TranslogOperationsRequestHandler.handleRequest(PeerRecoveryTargetService.java:565) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$TranslogOperationsRequestHandler.handleRequest(PeerRecoveryTargetService.java:557) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRequestHandler.messageReceived(PeerRecoveryTargetService.java:644) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.PeerRecoveryTargetService$RecoveryRequestHandler.messageReceived(PeerRecoveryTargetService.java:631) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.transport.InboundHandler.doHandleRequest(InboundHandler.java:288) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.transport.InboundHandler$1.doRun(InboundHandler.java:301) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:984) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n    at java.lang.Thread.run(Thread.java:1583) ~[?:?]\r\nCaused by: org.elasticsearch.index.mapper.DocumentParsingException: [1:124] failed to parse: _id must be unset or set to [AAAAAKuNxDxOOw8DAAABhW0z9gA] but was [4tlSJauNxDxOOw8DAAABhW0z9gA] because [locations] i\r\n    at org.elasticsearch.index.mapper.DocumentParser.wrapInDocumentParsingException(DocumentParser.java:246) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:153) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:96) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1031) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:970) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyTranslogOperation(IndexShard.java:1944) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyTranslogOperation(IndexShard.java:1931) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.lambda$indexTranslogOperations$4(RecoveryTarget.java:457) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    ... 15 more\r\nCaused by: java.lang.IllegalArgumentException: _id must be unset or set to [AAAAAKuNxDxOOw8DAAABhW0z9gA] but was [4tlSJauNxDxOOw8DAAABhW0z9gA] because [locations] is in time_series mode\r\n    at org.elasticsearch.index.mapper.TsidExtractingIdFieldMapper.createField(TsidExtractingIdFieldMapper.java:120) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.TimeSeriesIdFieldMapper.postParse(TimeSeriesIdFieldMapper.java:140) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentParser.internalParseDocument(DocumentParser.java:150) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:96) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:96) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:1031) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyIndexOperation(IndexShard.java:970) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyTranslogOperation(IndexShard.java:1944) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.index.shard.IndexShard.applyTranslogOperation(IndexShard.java:1931) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.lambda$indexTranslogOperations$4(RecoveryTarget.java:457) ~[elasticsearch-8.13.0-SNAPSHOT.jar:?]\r\n    ... 15 more\r\n```","Pinging @elastic\/es-storage-engine (Team:StorageEngine)","> Caused by: java.lang.IllegalArgumentException: _id must be unset or set to [AAAAAKuNxDxOOw8DAAABhW0z9gA] but was [4tlSJauNxDxOOw8DAAABhW0z9gA] because [locations] is in time_series mode\r\n\r\nI think `locations` should be an index, but I don't see this index being created in one of the tests in this qa module. ","A theory that I tried verify is that one of the recent changes that were made somehow creates a slightly different id for tsdb documents. By indexing a document in a pre 8.13 version and checking the id is same when updating to a 8.13\/8.14, but I've been unsuccessful so far.","I think the failure with the `locations` index occurred in the mixed cluster QA module: https:\/\/github.com\/elastic\/elasticsearch\/blob\/ac574acca98d34838cd28ffee547bfdd90e00885\/rest-api-spec\/src\/yamlRestTest\/resources\/rest-api-spec\/test\/tsdb\/130_position_fields.yml#L9\r\n\r\nThe failure occurred in the rolling upgrade module was `Caused by: org.elasticsearch.index.mapper.DocumentParsingException: [1:62] failed to parse: _id must be unset or set to [AAAAAECsHS75xpHXAAABdrs-cAA] but was [itaMgECsHS75xpHXAAABdrs-cAA] because [tsdb] is in time_series mode`. An _id of `AAAAAECsHS75xpHXAAABdrs-cAA` can be generated from IndexingIT without dimensions and with a timestamp of 1609459200000. I am still working on connecting these failure events.","> See e.g. latest failure: https:\/\/gradle-enterprise.elastic.co\/s\/wdzyszxt4ckmw\r\n\r\nThis build scan from an unmerged PR (https:\/\/github.com\/elastic\/elasticsearch\/pull\/105073), where we know it has some issues. I've relabelled this issue.","I thin Nhat is correct...I can spot the pattern \"AAAAA\" at the beginning of the id which means that a buffer including only 0-value bytes has been Base64 encoded. So probably the issue is caused by having no dimensions.","A lot more failures, on 8.10.4: https:\/\/gradle-enterprise.elastic.co\/s\/4w5djc46sngzk"],"labels":[">test-failure",":StorageEngine\/TSDB","medium-risk","Team:StorageEngine"]},{"title":"Reduce heap usage in BestBucketsDeferringCollector","body":"BestBucketsDeferringCollector delays the creation of sub-aggregations by collecting on the heap the values of documents and buckets received in LeafBucketCollector#collect(int doc, long bucket). Those values are replayed in a later stage if necessary.\r\n\r\nWe are currently using Lucene PackedLongValues to compress those integers on heap. In https:\/\/github.com\/elastic\/elasticsearch\/pull\/103624 we added PFOR-delta for compression of postings  in Elasticsearch which is probably more space efficient that the algorithm in  PackedLongValues which is essentially a FOR (without the P) so likely more wasteful.\r\n\r\nThis PR introduces PForLongValues which follows the PackedLongValues API so it can easily replace the current usage of PackedLongValues in BestBucketsDeferringCollector.","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Hi @iverase, I've created a changelog YAML for you."],"labels":[">enhancement",":Analytics\/Aggregations","Team:Analytics","v8.14.0"]},{"title":"[CI] DownsampleActionIT testRollupIndex failing","body":"The test failed 9 times in the last month and there seems to be a history of failures in the past, at least these ones fail with the same error message:\r\n\r\n* https:\/\/github.com\/elastic\/elasticsearch\/issues\/95447\r\n* https:\/\/github.com\/elastic\/elasticsearch\/issues\/95156\r\n\r\n**Build scan:**\r\nhttps:\/\/gradle-enterprise.elastic.co\/s\/k4rkaj5pl7yvo\/tests\/:x-pack:plugin:ilm:qa:multi-node:javaRestTest\/org.elasticsearch.xpack.ilm.actions.DownsampleActionIT\/testRollupIndex\r\n\r\n\r\n**Reproduction line:**\r\n```\r\n.\/gradlew ':x-pack:plugin:ilm:qa:multi-node:javaRestTest' --tests \"org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testRollupIndex\" -Dtests.seed=60DC99456040484D -Dtests.locale=it -Dtests.timezone=America\/Montserrat -Druntime.java=21\r\n```\r\n\r\n**Applicable branches:**\r\nmain\r\n\r\n**Reproduces locally?:**\r\nNo\r\n\r\n**Failure history:**\r\n[Failure dashboard for `org.elasticsearch.xpack.ilm.actions.DownsampleActionIT#testRollupIndex`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testRollupIndex'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.ilm.actions.DownsampleActionIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\r\n\r\n\r\n**Failure excerpt:**\r\n```\r\njava.lang.AssertionError: Cannot retrieve rollup index name\r\n\r\n  at __randomizedtesting.SeedInfo.seed([60DC99456040484D:86F8E813090C4181]:0)\r\n  at org.junit.Assert.fail(Assert.java:89)\r\n  at org.junit.Assert.assertTrue(Assert.java:42)\r\n  at org.junit.Assert.assertNotNull(Assert.java:713)\r\n  at org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testRollupIndex(DownsampleActionIT.java:203)\r\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n  at java.lang.reflect.Method.invoke(Method.java:580)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\r\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\r\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\r\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\r\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\r\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\r\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\r\n  at java.lang.Thread.run(Thread.java:1583)\r\n\r\n```","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":["blocker",">test-failure",":StorageEngine\/Rollup","Team:StorageEngine"]},{"title":"ESQL: Re-enable dependency checks","body":"### Description\n\nRe-enable the optimizers dependency checkers - see:\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/pull\/105371#issuecomment-1940377580","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","When re-enabling, we should make sure that a failed dependency check results in a 500 response code sent back to the user; therefore, we can't throw a `VerificationException` or any other client exception - we could throw an `IllegalStateException` instead. See discussion [here](https:\/\/github.com\/elastic\/elasticsearch\/pull\/105371#discussion_r1505453452)."],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"ESQL: Renaming the grouping field before a stats still relies on the original field","body":"### Description\n\nA potential bug in CombineProjections:\r\n`from employees | rename gender AS foo | stats bar = count(*) by foo | drop foo | sort bar` - the stats still refers to the original field (gender) for grouping which is problematic as the field no longer exists in the child.\r\nRelated to #105433","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug",">enhancement","Team:Analytics",":Analytics\/ES|QL","v8.14.0"]},{"title":"ESQL: Field extractor ignoring grouping","body":"### Description\n\nThe field extractor used in the local planner specifically [drops the grouping key](https:\/\/github.com\/elastic\/elasticsearch\/blob\/main\/x-pack\/plugin\/esql\/src\/main\/java\/org\/elasticsearch\/xpack\/esql\/optimizer\/LocalPhysicalPlanOptimizer.java#L170) (because the ordinal implementation doesn't need to load it).\r\nHowever this invalidates the plan since the grouping key is \"magically\" consumed by the `Aggregate` and potentially made available downstream.\r\n","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement",">non-issue","Team:Analytics",":Analytics\/ES|QL"]},{"title":"[CI] DownsampleActionIT testTsdbDataStreams failing","body":"**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/332bq7k5x65zg\/tests\/:x-pack:plugin:ilm:qa:multi-node:javaRestTest\/org.elasticsearch.xpack.ilm.actions.DownsampleActionIT\/testTsdbDataStreams\n\n\n**Reproduction line:**\n```\n.\/gradlew ':x-pack:plugin:ilm:qa:multi-node:javaRestTest' --tests \"org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testTsdbDataStreams\" -Dtests.seed=517B4BACD1EDFE0E -Dtests.locale=lv -Dtests.timezone=Africa\/Kigali -Druntime.java=21\n```\n\n**Applicable branches:**\n8.12\n\n**Reproduces locally?:**\nNo\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.ilm.actions.DownsampleActionIT#testTsdbDataStreams`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('testTsdbDataStreams'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.ilm.actions.DownsampleActionIT'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\njava.lang.AssertionError: Cannot retrieve rollup index [null]\n\n  at __randomizedtesting.SeedInfo.seed([517B4BACD1EDFE0E:F14856C62F4101DE]:0)\n  at org.junit.Assert.fail(Assert.java:88)\n  at org.junit.Assert.assertTrue(Assert.java:41)\n  at org.junit.Assert.assertNotNull(Assert.java:712)\n  at org.elasticsearch.xpack.ilm.actions.DownsampleActionIT.testTsdbDataStreams(DownsampleActionIT.java:342)\n  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n  at java.lang.reflect.Method.invoke(Method.java:580)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:48)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)\n  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":["blocker",">test-failure",":StorageEngine\/TSDB","Team:StorageEngine"]},{"title":"Fix missing commas in JdbcDatabaseMetaData.getTimeDateFunctions()","body":"Several date-time functions were merged because of the missing commas: DAYOFYEAREXTRACT, MONTHNAMEQUARTER","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Please add a set of tests that validate the bug fix.","Is there anything I still need to do about this PR?"],"labels":[":Analytics\/SQL","Team:Analytics","external-contributor","v8.14.0"]},{"title":"Wrong error message returned on delete trained model when model already deleted","body":"### Elasticsearch Version\n\n8.13\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\n23.3.0 Darwin Kernel Version 23.3.0 arm64 (M1 mac)\n\n### Problem Description\n\nCalling `DELETE _ml\/trained_models\/` when that model ID is referenced by an ingest processor \/ pipeline returns the error \r\n\r\n```\r\n  \"error\" : {\r\n    \"root_cause\" : [\r\n      {\r\n        \"type\" : \"status_exception\",\r\n        \"reason\" : \"Cannot delete model [.elser_model_2] as it is still referenced by ingest processors; use force to delete the model\"\r\n      }\r\n    ],\r\n```\r\n\r\neven when the model has already been force deleted. \n\n### Steps to Reproduce\n\n`PUT ELSER`\r\n(there should be a builtin pipeline referencing ELSER)\r\n`DELETE ELSER`\r\n(you should receive the error message)\r\n\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/ml-core (Team:ML)"],"labels":[">bug",":ml","Team:ML","low-risk"]},{"title":"Update tsdb binary doc value format to use compression from LUCENE-9211.","body":"The binary doc values implementation from LUCENE-9211 (apache\/lucene-solr#1234) is used here, which stores the values in LZ4 compressed blocks, in order to reduce storage usage (in the Lucene default doc values coded binary doc values are stored without any compression).\r\n\r\nFollow up from #105301","comments":[],"labels":[":StorageEngine\/TSDB","v8.14.0"]},{"title":"ES|QL: Adding tests for =~ operator","body":"\r\nWIP: DO NOT MERGE!\r\n\r\nfor now this PR also activates `field =~ field` capabilities (rather than `field =~ \"literal\"` as it is now).\r\nThis is not our intention for now though, so we'll have to re-enable the validation for literals only and make the tests work.\r\n\r\n","comments":[],"labels":["v8.14.0"]},{"title":"ESQL: Move expression classes into common package","body":"There are some expressions both in `org.elasticsearch.xpack.esql.evaluator` and `org.elasticsearch.xpack.esql.expression`.\r\n\r\n- Move them all into `org.elasticsearch.xpack.esql.expression`.\r\n- Make the `...Evaluator.Factory` nested classes public.\r\n- Delete some obsolete generated files and classes.\r\n\r\nNearly all changed lines are mechanical.\r\n\r\nMoved classes:\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.Equals;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.GreaterThan;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.GreaterThanOrEqual;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.InsensitiveEquals;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.LessThan;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.comparison.LessThanOrEqual;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.regex.RLike;\r\n- org.elasticsearch.xpack.esql.evaluator.predicate.operator.regex.WildcardLike;","comments":["Documentation preview:\n  - \u2728 [Changed pages](https:\/\/elasticsearch_105407.docs-preview.app.elstc.co\/diff)"],"labels":["v8.14.0"]},{"title":"[CI] ActiveDirectorySessionFactoryTests classMethod failing","body":"Seems docker related\n\n**Build scan:**\nhttps:\/\/gradle-enterprise.elastic.co\/s\/gkulbnzrdipfg\/tests\/:x-pack:qa:third-party:active-directory:test\/org.elasticsearch.xpack.security.authc.ldap.ActiveDirectorySessionFactoryTests\n\n\n**Reproduction line:**\n```\nnull\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nDidn't try\n\n**Failure history:**\n[Failure dashboard for `org.elasticsearch.xpack.security.authc.ldap.ActiveDirectorySessionFactoryTests#classMethod`](https:\/\/es-delivery-stats.elastic.dev\/app\/dashboards#\/view\/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupInput:(chainingSystem:HIERARCHICAL,controlStyle:twoLine,ignoreParentSettings:(ignoreFilters:!f,ignoreQuery:!f,ignoreTimerange:!f,ignoreValidations:!t),panels:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:task.keyword,grow:!t,id:'0c0c9cb8-ccd2-45c6-9b13-96bac4abc542',searchTechnique:wildcard,selectedOptions:!(),singleSelect:!t,title:'Gradle%20Task',width:medium),grow:!t,order:0,type:optionsListControl,width:small),'144933da-5c1b-4257-a969-7f43455a7901':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:name.keyword,grow:!t,id:'144933da-5c1b-4257-a969-7f43455a7901',searchTechnique:wildcard,selectedOptions:!('classMethod'),title:Test,width:medium),grow:!t,order:2,type:optionsListControl,width:medium),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(explicitInput:(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,enhancements:(),fieldName:className.keyword,grow:!t,id:'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850',searchTechnique:wildcard,selectedOptions:!('org.elasticsearch.xpack.security.authc.ldap.ActiveDirectorySessionFactoryTests'),title:Suite,width:medium),grow:!t,order:1,type:optionsListControl,width:medium)))))\n\n\n**Failure excerpt:**\n```\norg.testcontainers.containers.ContainerFetchException: Can't get Docker image: RemoteDockerImage(imageName=<resolving>, imagePullPolicy=DefaultPullPolicy(), imageNameSubstitutor=org.testcontainers.utility.ImageNameSubstitutor$LogWrappedImageNameSubstitutor@27332164)\n\n  at __randomizedtesting.SeedInfo.seed([7FA184586C7298D8]:0)\n  at org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1367)\n  at org.testcontainers.containers.GenericContainer.logger(GenericContainer.java:674)\n  at org.elasticsearch.test.fixtures.testcontainers.DockerEnvironmentAwareTestContainer.start(DockerEnvironmentAwareTestContainer.java:67)\n  at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1085)\n  at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n  at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n  at java.lang.Thread.run(Thread.java:1583)\n\n  Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: failed to get digest sha256:cc95a733c5d8e97e125081874caaaf90cc1e1bb5d68b2af165a989cc7673b7aa: open \/var\/lib\/docker\/image\/overlay2\/imagedb\/content\/sha256\/cc95a733c5d8e97e125081874caaaf90cc1e1bb5d68b2af165a989cc7673b7aa: no such file or directory\n\n    at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)\n    at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)\n    at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:159)\n    at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)\n    at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:20)\n    at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:41)\n    at org.testcontainers.shaded.com.google.common.util.concurrent.Futures$1.get(Futures.java:536)\n    at org.testcontainers.images.RemoteDockerImage.getImageName(RemoteDockerImage.java:130)\n    at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:67)\n    at org.testcontainers.images.RemoteDockerImage.resolve(RemoteDockerImage.java:28)\n    at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:20)\n    at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:41)\n    at org.testcontainers.containers.GenericContainer.getDockerImageName(GenericContainer.java:1365)\n    at org.testcontainers.containers.GenericContainer.logger(GenericContainer.java:674)\n    at org.elasticsearch.test.fixtures.testcontainers.DockerEnvironmentAwareTestContainer.start(DockerEnvironmentAwareTestContainer.java:67)\n    at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1085)\n    at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)\n    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)\n    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)\n    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)\n    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)\n    at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)\n    at java.lang.Thread.run(Thread.java:1583)\n\n```","comments":["Pinging @elastic\/es-search (Team:Search)","Not sure why this was tagged as `:search\/search`\r\n\r\nThe failure was due to a docker image failing to download and if it wasn't due to that, it the test is located within the security plugin.\r\n\r\nTagging delivery & security.","Pinging @elastic\/es-security (Team:Security)","Pinging @elastic\/es-delivery (Team:Delivery)","So far only a single occurrence. We'll keep an eye on this.\r\n","eerily similar failure here: https:\/\/gradle-enterprise.elastic.co\/s\/zlqmdntkflpbs\r\n\r\nactive-directory test failure due to docker failure."],"labels":["blocker",":Delivery\/Build",">test-failure",":Security\/Security","Team:Security","Team:Delivery"]},{"title":"ESQL: Undocumented behavior of COUNT with multi-values","body":"`COUNT` treats multi-values differently than other aggregations - e.g. `AVG(x)` will treat a row with `x = [1,2,3]` the same as if it was 3 rows with `x=1, x=2` and `x=3`, resp.\r\n\r\nIn contrast, `COUNT(x)` only does this _without a `BY`-clause_; with a `BY`-clause, it seems to treat the values inside multi-values separately, after all.\r\n\r\nThis should be double-checked and documented. Currently, [`COUNT`'s docs](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/master\/esql-functions-operators.html#esql-agg-count) just say that we count input values, which is imprecise.\r\n\r\nExamples:\r\n```\r\nPOST \/testidx\/_doc\r\n{\"x\": [1,2,3], \"y\": 0}\r\n{\"x\": null, \"y\": 0}\r\n\r\nfrom testidx | stats count(x)\r\n   count(x)    \r\n---------------\r\n1\r\n\r\nfrom testidx | stats count(x) by y\r\n   count(x)    |       y       \r\n---------------+---------------\r\n3              |0\r\n```","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-analytical-engine (Team:Analytics)","Per discussion with the team, `COUNT(x)` should actually count all single values, so `[0,1,2,3]` counts as `4`. So the docs are okay and the expected behavior is:\r\n```\r\nPOST \/testidx\/_doc\r\n{\"x\": [1,2,3], \"y\": 0}\r\n{\"x\": null, \"y\": 0}\r\n\r\n\/\/ 3 values for the field x\r\nfrom testidx | stats count(x)\r\n   count(x)    \r\n---------------\r\n3\r\n\r\nfrom testidx | stats count(x) by y\r\n   count(x)    |       y       \r\n---------------+---------------\r\n3              |0\r\n```","I found that the behavior is also buggy when using `count` over a constant multi-value:\r\n```\r\n... | stats c = count([1,2])\r\n\r\nc\r\n----\r\n1              <-- this should be 2, i.e. count([1,2]) should actually be the same as mv_count([1,2])\r\n```\r\n\r\n**UPDATE**: let's tackle this particular case as part of https:\/\/github.com\/elastic\/elasticsearch\/issues\/100634, as the work for that issue should cover this easily."],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL"]},{"title":"Add time series support to compute engine","body":"This is the meta issue that tracks the work to be done to the compute engine in order to power time series support. This for now at least doesn't include the language changes to ES|QL. The compute engine components should only be active via enabling specific query pragmas, until the time series compute engine components are more stable and the es|ql language is ready to adopt it.\r\n\r\n## General overview\r\n\r\n![image](https:\/\/github.com\/elastic\/elasticsearch\/assets\/580421\/1ec6830a-a154-4a39-a167-766285d7b2b0)\r\n\r\n(an overview of how time series aggregation can work in the compute engine (assuming all time series don't cross backing index boundary))\r\n\r\nThe idea is that a new source operator will emit all matching document in time series order (`_tsid` ascending, `@timestamp` descending). Documents are sorted in that order at the segment level, but not at the shard level. A page will additionally also include tsid and timestamp blocks. Documents of the same time serie should be contained by the block. A new time series grouping operator will make use of the sorted nature of the pages that the source operator emits and groups by tsid or tsid and timestamp interval. The output of this operator can be used by other operates such as the `HashAggregationOperator`.\r\n\r\nSometimes not all samples or a time series are in the same shard. This can happen when a query targets multiple backing indices of a tsdb data stream. In this case we need for the affected time series post pone grouping in the new time series grouping operator. The new time series grouping operator needs to group these time series on the coordinating node (when the aggregation mode is final in `AggregateExec`). Initially we will build a time series grouping  operator that assumes that time series are always scattered across multiple backing indices and thus performs the grouping when the aggregate mode is final. In follow ups, we can then improve the new time series grouping operator to detect when time series don't cross backing index boundaries. In that case the grouping can perform locally, when aggregation mode is partial.\r\n\r\nInitially we will only allow filtering on dimension fields. More specifically the filters that get pushed down to the time series source operator. If filters on labels or metrics get pushed down to the source operator we run at risk of breaking the ordered samples of a time serie apart. \r\n\r\n## Tasks\r\n- [x] Add a time series source operator, that gets activated when a time serie pragma is enabled. (#105398)\r\n- [ ] #106411\r\n- [ ] Update the grouping operator to perform grouping can happen locally when ever the query allows for it. For example a `@timestamp` filter in `WHERE` clause doesn't cross the boundary of a backing index. Or timestamp interval group is contained within a backing index.\r\n- [ ] #106414\r\n- [ ] Enhancing the time  serie grouping operator to also group by time series and time interval. A typical use case would group by time serie and time interval. This is when the `BUCKET` syntax is used.\r\n- [ ] Integrate the time series operators in the proposed `TSTATS` syntax.\r\n- [ ] Develop an alternative parallelization strategy for tsdb indices in the time serie source operator.\r\n- [ ] #106415\r\n- [ ] Add support of sparse index to easily navigate a time series documents (#95701). This is required for determining the last value of a metric and skipping to the next last value of the next time serie. And other functionally like interpolation and geo fencing.  Additionally a query may be too selective, and mask documents which are valid metric of a time serie. A sparse index would allow us to access the metrics even if that would be the case.\r\n- [ ] Downsampling support. Big part of this is support for _doc_count field and aggregated double metric field type.\r\n- [ ] Downsampling support. Support the `_doc_count` field and and aggregate double metric field type.\r\n- [ ] Query planning should understand the `index.time_series.start_time` and `index.time_series.end_time` index settings, so that backing indices that will never match with the ES|QL query will be excluded from the query execution. (This is based on where filter on `@timestamp` field.)\r\n\r\n## Optional:\r\n- [ ] Ordinal-based BytesRef block for TSID #106387\r\n- [ ] ...","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Pinging @elastic\/es-storage-engine (Team:StorageEngine)"],"labels":["Team:Analytics",":StorageEngine\/TSDB",":Analytics\/Compute Engine","Team:StorageEngine"]},{"title":"[ES|QL] Throwing on STATS BY constant statement","body":"### Elasticsearch Version\n\n8.13.0\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMacOS\n\n### Problem Description\n\nThe following error:\r\n\r\n```\r\nverification_exception\\n\\tRoot causes:\\n\\t\\tverification_exception: Found 1 problem\\nline 1:42: expected an aggregate function but found [1]\r\n```\r\n\r\nis thrown by the following query:\r\n\r\n```\r\nfrom a_index | stats avg(numberField) by 1\r\n```\r\n\r\nI think the problem is double:\r\n* I would expect to error in the first place as user\r\n* The error message is wrong: expected an aggregate function at `by` level?\n\n### Steps to Reproduce\n\nSee above.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Another variant of the same inconsistency: `from employees | stats b = count(salary), x = max(languages) by b` throws\r\n\r\n```\r\n                \"type\": \"verification_exception\",\r\n                \"reason\": \"Found 1 problem\\nline 1:65: invalid stats declaration; [b] is not an aggregate function\",\r\n                \"stack_trace\": \"org.elasticsearch.xpack.esql.VerificationException: Found 1 problem\\nline 1:65: invalid stats declaration; [b] is not an aggregate function\r\n```"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"[ES|QL] Function throwing on number field as arg expected constant value","body":"### Elasticsearch Version\n\n8.13.0\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMacOS\n\n### Problem Description\n\nThe following error or a similar variant is thrown:\r\n\r\n```\r\nverification_exception\\n\\tRoot causes:\\n\\t\\tverification_exception: Found 1 problem\\nline 1:27: second argument of [<function>(numberField, ...)] must be a constant, received [numberField]\r\n```\r\n\r\nWith he following queries:\r\n\r\n```\r\nfrom a_index | eval var = auto_bucket(dateField, numberField, stringField, stringField)\r\nfrom a_index | stats var = percentile(numberField, numberField)\r\n```\r\n\r\nAs a user, I would expect numeric arguments to support numeric fields or constant interchangeably.\r\nOtherwise I would expect it documented somewhere. \n\n### Steps to Reproduce\n\nSee above.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","Thanks @dej611 !\r\n\r\n\r\nI think this is a docs issue, not a bug.\r\n\r\nFor `percentile`, we recently added to our docs that the second argument needs to be a constant. The docs for `auto_bucket` are indeed missing this info.","Pinging @elastic\/es-docs (Team:Docs)","It would be nice to express the same bit of information also at `SHOW FUNCTIONS` level."],"labels":[">docs","Team:Docs","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"[ES|QL] some functions cannot operate on text field type, only on keyword types","body":"### Elasticsearch Version\r\n\r\n8.13.0\r\n\r\n### Installed Plugins\r\n\r\n_No response_\r\n\r\n### Java Version\r\n\r\n_bundled_\r\n\r\n### OS Version\r\n\r\nMacOS\r\n\r\n### Problem Description\r\n\r\nEdit note: it started with only `split` function, but I've updated it with a full list of testing functions throwing the same error.\r\n\r\nThe following error:\r\n\r\n```\r\nverification_exception\\n\\tRoot causes:\\n\\t\\tverification_exception: Found 1 problem\\nline 1:29\r\n  <function>(stringField, ...)] cannot operate on first argument field of data type [text]: No keyword\/multi-field defined exact matches for [stringField]; define one or use MATCH\/QUERY instead\r\n```\r\n\r\nWith the following query:\r\n```\r\nfrom a_index | where length(split(stringField, stringField)) > 0\r\nfrom a_index | eval var = date_format(stringField, dateField)\r\nfrom a_index | eval date_format(stringField, dateField)\r\nfrom a_index | eval var = date_parse(stringField, stringField)\r\nfrom a_index | eval date_parse(stringField, stringField)\r\nfrom a_index | eval var = split(stringField, stringField)\r\nfrom a_index | eval stringField in (\\\"a\\\", \\\"b\\\", \\\"c\\\")\r\nfrom a_index | eval stringField not in (\\\"a\\\", \\\"b\\\", \\\"c\\\")\r\nfrom a_index | eval stringField not in (\\\"a\\\", \\\"b\\\", \\\"c\\\", stringField)\r\n```\r\n\r\nThe `stringField` mapping is set to `text`.\r\nFor consistency with other functions operating on text, I would expect no difference on operating on either a `keyword` or ` text` field type here. Otherwise it should be documented.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\nSee above.\r\n\r\n### Logs (if relevant)\r\n\r\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","IN \/ NOT IN operator fixed by https:\/\/github.com\/elastic\/elasticsearch\/pull\/106654"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"[ES|QL] concat throws an error with a single string argument","body":"### Elasticsearch Version\n\n8.13.0\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMacOS\n\n### Problem Description\n\nThe following error is thrown:\r\n\r\n```\r\nparsing_exception\\n\\tCaused by:\\n\\t\\tql_illegal_argument_exception: expects at least two arguments\\n\\tRoot causes:\\n\\t\\tql_illegal_argument_exception: expects at least two arguments\r\n```\r\n\r\nFor the following queries:\r\n\r\n```\r\n\"row var = concat(\\\"a\\\")\",\r\n \"row concat(\\\"a\\\")\",\r\n \"row var = concat(to_string(\\\"a\\\"))\",\r\n \"from a_index | where length(concat(stringField)) > 0\",\r\n \"from a_index | eval var = concat(stringField)\",\r\n \"from a_index | eval concat(stringField)\",\r\n \"from a_index | eval var = concat(to_string(stringField))\"\r\n```\r\n\r\nWhile I can see some reasoning behind that, such behaviour is inconsistent with other variadic functions like `greatest`, `least`, `coalesce`.\r\nIt would be nice to have a consistent behaviour here.\n\n### Steps to Reproduce\n\nSee above.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)","@dej611 what is the inconsistency and what is your expectation?\r\n`coalesce`, `least`, `greatest` do make sense to return first non-null, minimum of values and maximum of values respectively, and it make sense to have one argument only, but I don't see a reason for concatenating a single string value. Concatenation implies an operation on at least two arguments.\r\n\r\nComparing `coalesce`, `least` and `greatest` with `concat` is not comparing apples to apples. `concat` should be compared more with `replace` for example.","I see your point. To me it's more an ergonomic problem rather than functional.\r\nI've just quickly checked in JS and it works with an empty list\/no list at all.\r\nI guess this is debatable, so I can address it at Kibana validation level. \ud83d\udc4d "],"labels":["Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"[ES|QL] auto_bucket() throws error on literal number as third argument","body":"### Elasticsearch Version\n\n8.13.0 @ 7c039b172836b90a77099ca5ba79115dca979b0e\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nMacOS\n\n### Problem Description\n\nThe following error:\r\n\r\n```\r\n\"verification_exception\\n\\tRoot causes:\\n\\t\\tverification_exception: Found 1 problem\\nline 1:11: third argument of [auto_bucket(now(), 5, 5, 5)] must be [datetime or string], found value [5] type [integer]\"\r\n```\r\n\r\nis thrown with the following queries:\r\n\r\n```\r\nrow var = auto_bucket(now(), 5, 5, 5)\r\nrow auto_bucket(now(), 5, 5, 5)\r\nfrom index | auto_bucket(now(), 5, 5, 5)\r\n```\r\n\r\nIn the `auto_bucket` documentation is stated that both the third and fourth args can be either a number of a date string.\r\nI guess this is mostly a documentation issue as the string version is required due to the date type passed as first argument, but need confirmation here.\r\nIn such case I would expect:\r\n* to make the documentation more clear on this\r\n* to have a better error message that clarifies the context.\n\n### Steps to Reproduce\n\nSee above.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">bug","Team:Analytics",":Analytics\/ES|QL","ES|QL-ui"]},{"title":"Redirect shard-level bulk failures to a failure store if applicable","body":"When indexing data from something like a fleet integration, a mapping change that is incompatible with the incoming data often results in the mass rejection of documents with little to no feedback to the end user. While a data shipper may be receiving error responses, there is little that the shipper can do about them, and thus the data is often dropped. The issue is usually only discovered by an end user after the realization that little to no data has been ingested since the mapping changes, and often still requires more effort still to diagnose the actual mapping problem.\r\n\r\nThis PR expands upon previous work in the failure store by inspecting failed shard-level bulk operations and possibly redirecting them to a failure store. Documents are eligible to have their failures stored if they are writing to a data stream with a failure store enabled. Responses from the TransportShardBulkAction are saved in the working bulk response list, but any failures that are eligible for redirection will be transformed into failure documents and retried. The second attempt for storing faliures will overwrite the original failure on the working response.","comments":["Hi @jbaiera, I've created a changelog YAML for you.","Pinging @elastic\/es-distributed (Team:Distributed)","Pinging @elastic\/es-data-management (Team:Data Management)"],"labels":[">non-issue",":Distributed\/CRUD",":Data Management\/Data streams","Team:Data Management","Team:Distributed","v8.14.0"]},{"title":"scale_float does not honor limits for long values","body":"### Elasticsearch Version\n\n7.17 \/ 8.x\n\n### Installed Plugins\n\n_No response_\n\n### Java Version\n\n_bundled_\n\n### OS Version\n\nN\/A\n\n### Problem Description\n\nWhen using `scaled_float`, there is no range check on the value and scaling factor to be outside the `long` value range.\r\n\r\nAccording to [documentation](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/number.html) for `scaled_float`:\r\n\r\n> scaled_float - A floating point number that is backed by a\u00a0long, scaled by a fixed\u00a0double\u00a0scaling factor\r\n\r\nHowever, that assumption is not checked on document ingestion. The following mapping:\r\n\r\n\"properties\": {\r\n  \"a_scaled_float\": {\r\n    \"scaling_factor\": 1000000,\r\n    \"type\": \"scaled_float\"\r\n  }\r\nThere's a scaled_float mapping configuration with value 1000000.\r\nSo, why did the number 1707339753294418 become 9223372036854.775? Shouldn't it be 1707339753294418000000?\r\n\r\n{\r\n  \"_index\": \"test-index\",\r\n  \"_id\": \"Z5t_iY0BLRoRMvLbP6e3\",\r\n  \"_score\": 1,\r\n  \"_source\": {\r\n    \"a_scaled_float\": 1707339753294418\r\n  },\r\n  \"fields\": {\r\n    \"a_scaled_float\": [\r\n      9223372036854.775\r\n    ]\r\n  }\r\n}\r\n\r\nThis is due to the rounding up done [here](https:\/\/github.com\/elastic\/elasticsearch\/blob\/17bc83f481ddcfc032f6743a7a64c64000d55740\/modules\/mapper-extras\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/extras\/ScaledFloatFieldMapper.java#L388), which produces a `long` value which is outside range, and thus returns `MAX_LONG`.\r\n\r\nAs this provides an incorrect result, and the value is outside of the `long` range, the correct behaviour should be to error on the value as other numeric types do.\n\n### Steps to Reproduce\n\nSet a mapping with properties:\r\n\r\n```json\r\nPUT test-index\r\n{\r\n      \"mappings\": {\r\n        \"properties\": {\r\n            \"scaled_float\": {\r\n              \"type\": \"scaled_float\",\r\n              \"scaling_factor\": 1000000\r\n            }\r\n        }\r\n      }\r\n}\r\n```\r\n\r\nIngest a document:\r\n\r\n```json\r\nPUT test-index\/_doc\/1\r\n{\r\n  \"scaled_float\": 1707339753294418\r\n}\r\n```\r\n\r\nThis should return an error, as the number is outside the `long` range.\n\n### Logs (if relevant)\n\n_No response_","comments":["Pinging @elastic\/es-search (Team:Search)","We need to update docs [here](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/number.html#_which_type_should_i_use) to provide proper guidance. \r\n\r\nAlso, the new behaviour should be corrected depending on the `IndexVersion` the index was created on. In case it's greater or equal than the fix version, it should return an error for a value that is not indexable using a `long` - unless `ignore_malformed` is specified as `true`.","To move the point that saturation should probably be disabled by default.\r\n\r\n```\r\nPUT scaled_float\r\n{\r\n\t\"mappings\": {\r\n\t\t\"properties\": {\r\n\t\t\t\"value\": {\r\n\t\t\t    \"type\": \"scaled_float\",\r\n\t\t\t    \"scaling_factor\": 1000000\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n\r\nPOST scaled_float\/_doc\r\n{\r\n\t\"value\": 100000000000000.0\r\n}\r\n\r\nGET scaled_float\/_search\r\n{\r\n\t\"fields\": [ \"value\" ],\r\n\t\"query\": {\r\n\t\t\"range\": {\r\n\t\t\t\"value\": {\r\n\t\t\t\t\"lt\": 10000000000000.0,\r\n\t\t\t\t\"gt\": 100000000000000.0\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nReturns hits, even though its impossible to have a number that is less than `10000000000000.0` and greater than `100000000000000.0`."],"labels":[">bug",":Search\/Mapping","Team:Search"]},{"title":"ESQL: mention `mv_expand` under \"ES|QL multivalued fields\" page","body":"### Description\n\nWe should mention `mv_expand` command under \"ES|QL multivalued fields\" page. This is currently not the best, but only way to (eventually) filter on a MV field.","comments":["Pinging @elastic\/es-docs (Team:Docs)","Pinging @elastic\/es-analytical-engine (Team:Analytics)"],"labels":[">enhancement",">docs","Team:Docs","Team:Analytics",":Analytics\/ES|QL"]}]
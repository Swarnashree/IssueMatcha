[{"title":"build(deps): bump react-router-dom from 5.3.4 to 6.22.3 in \/web\/ui","body":"Bumps [react-router-dom](https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom) from 5.3.4 to 6.22.3.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/remix-run\/react-router\/releases\">react-router-dom's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>v6.4.4<\/h2>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Throw an error if an <code>action<\/code>\/<code>loader<\/code> function returns <code>undefined<\/code> as revalidations need to know whether the loader has previously been executed. <code>undefined<\/code> also causes issues during SSR stringification for hydration. You should always ensure your <code>loader<\/code>\/<code>action<\/code> returns a value, and you may return <code>null<\/code> if you don't wish to return anything. (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9511\">#9511<\/a>)<\/li>\n<li>Properly handle redirects to external domains (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9590\">#9590<\/a>, <a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9654\">#9654<\/a>)<\/li>\n<li>Preserve the HTTP method on 307\/308 redirects (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9597\">#9597<\/a>)<\/li>\n<li>Support <code>basename<\/code> in static data routers (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9591\">#9591<\/a>)<\/li>\n<li>Enhanced <code>ErrorResponse<\/code> bodies to contain more descriptive text in internal 403\/404\/405 scenarios<\/li>\n<li>Fix issues with encoded characters in <code>NavLink<\/code> and descendant <code>&lt;Routes&gt;<\/code> (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9589\">#9589<\/a>, <a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9647\">#9647<\/a>)<\/li>\n<li>Properly serialize\/deserialize <code>ErrorResponse<\/code> instances when using built-in hydration (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9593\">#9593<\/a>)<\/li>\n<li>Support <code>basename<\/code> in static data routers (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/9591\">#9591<\/a>)<\/li>\n<li>Updated dependencies:\n<ul>\n<li><code>@remix-run\/router@1.0.4<\/code><\/li>\n<li><code>react-router@6.4.4<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><strong>Full Changelog<\/strong>: <a href=\"https:\/\/github.com\/remix-run\/react-router\/compare\/react-router-dom@6.4.3...react-router-dom@6.4.4\">https:\/\/github.com\/remix-run\/react-router\/compare\/react-router-dom@6.4.3...react-router-dom@6.4.4<\/a><\/p>\n<h2>react-router-dom@6.4.0-pre.15<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>\n<p>fix: remove internal router singleton (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/9227\">#9227<\/a>)<\/p>\n<p>This change removes the internal module-level <code>routerSingleton<\/code> we create and maintain inside our data routers since it was causing a number of headaches for non-simple use cases:<\/p>\n<ul>\n<li>Unit tests are a pain because you need to find a way to reset the singleton in-between tests\n<ul>\n<li>Use use a <code>_resetModuleScope<\/code> singleton for our tests<\/li>\n<li>...but this isn't exposed to users who may want to do their own tests around our router<\/li>\n<\/ul>\n<\/li>\n<li>The JSX children <code>&lt;Route&gt;<\/code> objects cause non-intuitive behavior based on idiomatic react expectations\n<ul>\n<li>Conditional runtime <code>&lt;Route&gt;<\/code>'s won't get picked up<\/li>\n<li>Adding new <code>&lt;Route&gt;<\/code>'s during local dev won't get picked up during HMR<\/li>\n<li>Using external state in your elements doesn't work as one might expect (see <a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/9225\">#9225<\/a>)<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Instead, we are going to lift the singleton out into user-land, so that they create the router singleton and manage it outside the react tree - which is what react 18 is encouraging with <code>useSyncExternalStore<\/code> anyways! This also means that since users create the router - there's no longer any difference in the rendering aspect for memory\/browser\/hash routers (which only impacts router\/history creation) - so we can get rid of those and trim to a simple <code>RouterProvider<\/code><\/p>\n<pre lang=\"jsx\"><code>\/\/ Before\nfunction App() {\n  &lt;DataBrowserRouter&gt;\n    &lt;Route path=&quot;\/&quot; element={&lt;Layout \/&gt;}&gt;\n      &lt;Route index element={&lt;Home \/&gt;}&gt;\n    &lt;\/Route&gt;\n  &lt;DataBrowserRouter&gt;\n}\n<p>\/\/ After\nlet router = createBrowserRouter([{\npath: &quot;\/&quot;,\nelement: &lt;Layout \/&gt;,\nchildren: [{\nindex: true,\n<\/code><\/pre><\/p>\n<\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/remix-run\/react-router\/blob\/main\/packages\/react-router-dom\/CHANGELOG.md\">react-router-dom's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h2>6.22.3<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Updated dependencies:\n<ul>\n<li><code>@remix-run\/router@1.15.3<\/code><\/li>\n<li><code>react-router@6.22.3<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>6.22.2<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Updated dependencies:\n<ul>\n<li><code>@remix-run\/router@1.15.2<\/code><\/li>\n<li><code>react-router@6.22.2<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>6.22.1<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Updated dependencies:\n<ul>\n<li><code>react-router@6.22.1<\/code><\/li>\n<li><code>@remix-run\/router@1.15.1<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>6.22.0<\/h2>\n<h3>Minor Changes<\/h3>\n<ul>\n<li>Include a <code>window__reactRouterVersion<\/code> tag for CWV Report detection (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/11222\">#11222<\/a>)<\/li>\n<\/ul>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Updated dependencies:\n<ul>\n<li><code>@remix-run\/router@1.15.0<\/code><\/li>\n<li><code>react-router@6.22.0<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>6.21.3<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Fix <code>NavLink<\/code> <code>isPending<\/code> when a <code>basename<\/code> is used (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/11195\">#11195<\/a>)<\/li>\n<li>Remove leftover <code>unstable_<\/code> prefix from <code>Blocker<\/code>\/<code>BlockerFunction<\/code> types (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/11187\">#11187<\/a>)<\/li>\n<li>Updated dependencies:\n<ul>\n<li><code>react-router@6.21.3<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<h2>6.21.2<\/h2>\n<h3>Patch Changes<\/h3>\n<ul>\n<li>Leverage <code>useId<\/code> for internal fetcher keys when available (<a href=\"https:\/\/redirect.github.com\/remix-run\/react-router\/pull\/11166\">#11166<\/a>)<\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/67009e1727e7c1d06896c1a1fd78dc6161d86c94\"><code>67009e1<\/code><\/a> chore: Update version for release (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11329\">#11329<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/0c37f37d75d830869eb3566c2fc2f487b4a1ebe2\"><code>0c37f37<\/code><\/a> chore: Update version for release (pre) (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11327\">#11327<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/78c843bc792fef942a0896fdba7275907bb219fd\"><code>78c843b<\/code><\/a> chore: Update version for release (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11314\">#11314<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/e1421befe5dd2cfb3ce9b9856b3b3255b1e6723f\"><code>e1421be<\/code><\/a> chore: Update version for release (pre) (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11309\">#11309<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/95acd38639dadde8552cd8d1c6594445045740d7\"><code>95acd38<\/code><\/a> Merge branch 'release-next' into dev<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/6b22f919c5575603263d1182953c3e192c8f902c\"><code>6b22f91<\/code><\/a> Preserve hydrated errors during partial hydration (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11305\">#11305<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/5f0cf324351281b786bb80dde1b1b98297495a2f\"><code>5f0cf32<\/code><\/a> chore: Update version for release (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11283\">#11283<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/25d16dbed15005d9e07d5c4a78975a38cfff835a\"><code>25d16db<\/code><\/a> chore: Update version for release (pre) (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11280\">#11280<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/241f2d4b7a0d091b0c634304c6f5f121a8833bca\"><code>241f2d4<\/code><\/a> Fix issues with pre-encoded param names not being properly decoded (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11199\">#11199<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/remix-run\/react-router\/commit\/3a667193c1da5eaf72970b3c953c0724a0b10bf6\"><code>3a66719<\/code><\/a> chore: Update version for release (<a href=\"https:\/\/github.com\/remix-run\/react-router\/tree\/HEAD\/packages\/react-router-dom\/issues\/11242\">#11242<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/remix-run\/react-router\/commits\/react-router-dom@6.22.3\/packages\/react-router-dom\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=react-router-dom&package-manager=npm_and_yarn&previous-version=5.3.4&new-version=6.22.3)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","javascript"]},{"title":"build(deps-dev): bump typescript from 4.9.5 to 5.4.3 in \/web\/ui","body":"Bumps [typescript](https:\/\/github.com\/Microsoft\/TypeScript) from 4.9.5 to 5.4.3.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/releases\">typescript's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>TypeScript 5.4.3<\/h2>\n<p>For release notes, check out the <a href=\"https:\/\/devblogs.microsoft.com\/typescript\/announcing-typescript-5-4\/\">release announcement<\/a>.<\/p>\n<p>For the complete list of fixed issues, check out the<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.0%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.0 (Beta)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.1%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.1 (RC)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.2%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.2 (Stable)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.3%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.3 (Stable)<\/a>.<\/li>\n<\/ul>\n<p>Downloads are available on:<\/p>\n<ul>\n<li><a href=\"https:\/\/www.nuget.org\/packages\/Microsoft.TypeScript.MSBuild\">NuGet package<\/a><\/li>\n<\/ul>\n<h2>TypeScript 5.4<\/h2>\n<p>For release notes, check out the <a href=\"https:\/\/devblogs.microsoft.com\/typescript\/announcing-typescript-5-4\/\">release announcement<\/a>.<\/p>\n<p>For the complete list of fixed issues, check out the<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.0%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.0 (Beta)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.1%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.1 (RC)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.2%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.2 (Stable)<\/a>.<\/li>\n<\/ul>\n<p>Downloads are available on:<\/p>\n<ul>\n<li><a href=\"https:\/\/www.nuget.org\/packages\/Microsoft.TypeScript.MSBuild\">NuGet package<\/a><\/li>\n<\/ul>\n<h2>TypeScript 5.4 RC<\/h2>\n<p>For release notes, check out the <a href=\"https:\/\/devblogs.microsoft.com\/typescript\/announcing-typescript-5-4-rc\/\">release announcement<\/a>.<\/p>\n<p>For the complete list of fixed issues, check out the<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.0%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.0 (Beta)<\/a>.<\/li>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.1%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.1 (RC)<\/a>.<\/li>\n<\/ul>\n<p>Downloads are available on:<\/p>\n<ul>\n<li><a href=\"https:\/\/www.nuget.org\/packages\/Microsoft.TypeScript.MSBuild\">NuGet package<\/a><\/li>\n<\/ul>\n<h2>TypeScript 5.4 Beta<\/h2>\n<p>For release notes, check out the <a href=\"https:\/\/devblogs.microsoft.com\/typescript\/announcing-typescript-5-4-beta\/\">release announcement<\/a>.<\/p>\n<p>For the complete list of fixed issues, check out the<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/issues?utf8=%E2%9C%93&amp;q=milestone%3A%22TypeScript+5.4.0%22+is%3Aclosed+\">fixed issues query for Typescript 5.4.0 (Beta)<\/a>.<\/li>\n<\/ul>\n<p>Downloads are available on:<\/p>\n<ul>\n<li><a href=\"https:\/\/www.npmjs.com\/package\/typescript\">npm<\/a><\/li>\n<li><a href=\"https:\/\/www.nuget.org\/packages\/Microsoft.TypeScript.MSBuild\">NuGet package<\/a><\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/6ea273cdcca99db809074d2b2d38d0e5b59ee81b\"><code>6ea273c<\/code><\/a> Update LKG<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/cd06f92c43eede05731cc0592bcb4ee7fe4d38cc\"><code>cd06f92<\/code><\/a> \ud83e\udd16 Pick PR <a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57853\">#57853<\/a> (Revert PR 56161) into release-5.4 (<a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57854\">#57854<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/ca8e720a36618eb1323f8dbc36a45a8fd5b35d78\"><code>ca8e720<\/code><\/a> Update LKG<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/010b1885c089189425a0f7d34bd02305401f8928\"><code>010b188<\/code><\/a> release-5.4: Revert PR 56087 (<a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57850\">#57850<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/fc7006c125914a2d7146027744500ef8a308c5f6\"><code>fc7006c<\/code><\/a> Update LKG<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/b45a41823ecaff6c1554a4c69161db10b2eba6ab\"><code>b45a418<\/code><\/a> \ud83e\udd16 Pick PR <a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57801\">#57801<\/a> (Distribute mapped types over array\/...) into release-5.4 (#...<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/609560f36b84bf031fa14cc4b0d1b2aeb8cfc6f9\"><code>609560f<\/code><\/a> Bump version to 5.4.3 and LKG<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/f42605fcef04f329e9bb5cb94677ca0101dcdd5c\"><code>f42605f<\/code><\/a> \ud83e\udd16 Pick PR <a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57746\">#57746<\/a> (Revert &quot;Defer processing of nested ...) into release-5.4 (#...<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/485c7c5d08f985528a83d9879658912181ef7f11\"><code>485c7c5<\/code><\/a> Revert &quot;Allow (non-assert) type predicates to narrow by discriminant&quot;\u2026 (<a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57795\">#57795<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/microsoft\/TypeScript\/commit\/7f11456f4afd816776c08208389e10d8c5a62dd8\"><code>7f11456<\/code><\/a> \ud83e\udd16 Pick PR <a href=\"https:\/\/redirect.github.com\/Microsoft\/TypeScript\/issues\/57751\">#57751<\/a> (Exclude generic string-like types f...) into release-5.4 (#...<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/Microsoft\/TypeScript\/compare\/v4.9.5...v5.4.3\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=typescript&package-manager=npm_and_yarn&previous-version=4.9.5&new-version=5.4.3)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","javascript"]},{"title":"build(deps): bump github.com\/aws\/aws-sdk-go from 1.50.32 to 1.51.4","body":"Bumps [github.com\/aws\/aws-sdk-go](https:\/\/github.com\/aws\/aws-sdk-go) from 1.50.32 to 1.51.4.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/releases\">github.com\/aws\/aws-sdk-go's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h1>Release v1.51.4 (2024-03-20)<\/h1>\n<h3>Service Client Updates<\/h3>\n<ul>\n<li><code>service\/accessanalyzer<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/codebuild<\/code>: Updates service documentation\n<ul>\n<li>This release adds support for new webhook events (RELEASED and PRERELEASED) and filter types (TAG_NAME and RELEASE_NAME).<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/connect<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/dynamodb<\/code>: Updates service API, documentation, waiters, paginators, and examples\n<ul>\n<li>This release introduces 3 new APIs ('GetResourcePolicy', 'PutResourcePolicy' and 'DeleteResourcePolicy') and modifies the existing 'CreateTable' API for the resource-based policy support. It also modifies several APIs to accept a 'TableArn' for the 'TableName' parameter.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/managedblockchain-query<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/savingsplans<\/code>: Updates service API and documentation<\/li>\n<\/ul>\n<h1>Release v1.51.3 (2024-03-19)<\/h1>\n<h3>Service Client Updates<\/h3>\n<ul>\n<li><code>service\/cloudformation<\/code>: Updates service documentation\n<ul>\n<li>Documentation update, March 2024. Corrects some formatting.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/ec2<\/code>: Updates service API, documentation, and paginators\n<ul>\n<li>This release adds the new DescribeMacHosts API operation for getting information about EC2 Mac Dedicated Hosts. Users can now see the latest macOS versions that their underlying Apple Mac can support without needing to be updated.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/finspace<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/logs<\/code>: Updates service API and documentation\n<ul>\n<li>Update LogSamples field in Anomaly model to be a list of LogEvent<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/managedblockchain-query<\/code>: Updates service API, documentation, and paginators<\/li>\n<\/ul>\n<h1>Release v1.51.2 (2024-03-18)<\/h1>\n<h3>Service Client Updates<\/h3>\n<ul>\n<li><code>service\/cloudformation<\/code>: Updates service API and documentation\n<ul>\n<li>This release supports for a new API ListStackSetAutoDeploymentTargets, which provider auto-deployment configuration as a describable resource. Customers can now view the specific combinations of regions and OUs that are being auto-deployed.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/kms<\/code>: Updates service API and documentation\n<ul>\n<li>Adds the ability to use the default policy name by omitting the policyName parameter in calls to PutKeyPolicy and GetKeyPolicy<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/mediatailor<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/rds<\/code>: Updates service API, documentation, waiters, paginators, and examples\n<ul>\n<li>This release launches the ModifyIntegration API and support for data filtering for zero-ETL Integrations.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/s3<\/code>: Updates service API and examples\n<ul>\n<li>Fix two issues with response root node names.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/timestream-query<\/code>: Updates service documentation<\/li>\n<\/ul>\n<h1>Release v1.51.1 (2024-03-15)<\/h1>\n<h3>Service Client Updates<\/h3>\n<ul>\n<li><code>service\/backup<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/codebuild<\/code>: Updates service API and documentation\n<ul>\n<li>AWS CodeBuild now supports overflow behavior on Reserved Capacity.<\/li>\n<\/ul>\n<\/li>\n<li><code>service\/connect<\/code>: Updates service API and documentation<\/li>\n<li><code>service\/ec2<\/code>: Updates service API and documentation<\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/fb4c556a07d40bcc5dcc654fa3d58a56822feb08\"><code>fb4c556<\/code><\/a> Release v1.51.4 (2024-03-20) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5203\">#5203<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/9d5e5d99364bce7b10479eb43f5b9176237524d4\"><code>9d5e5d9<\/code><\/a> Release v1.51.3 (2024-03-19) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5202\">#5202<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/8c9d1521bf404a160352bba945fedd47e99817bd\"><code>8c9d152<\/code><\/a> Release v1.51.2 (2024-03-18) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5200\">#5200<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/fdce8a5fc70227970e0adffd97f3e8edee308925\"><code>fdce8a5<\/code><\/a> Release v1.51.1 (2024-03-15) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5199\">#5199<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/53e4759915361d72a33be783d5e878a63d85f807\"><code>53e4759<\/code><\/a> Release v1.51.0 (2024-03-14) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5198\">#5198<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/113a726d383cc26c0d4e8c6ffbc1ed187656c2b1\"><code>113a726<\/code><\/a> Deprecate iotroborunner service (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5197\">#5197<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/a0561ce128368d6c348127a98eb24f852ae7a92e\"><code>a0561ce<\/code><\/a> Release v1.50.38 (2024-03-13) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5196\">#5196<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/9b3b7152dbf877a2534e0a96ef48e9d97ce301e9\"><code>9b3b715<\/code><\/a> Release v1.50.37 (2024-03-12) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5195\">#5195<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/540615210bad312028c2abc168fbb9f36eb883ef\"><code>5406152<\/code><\/a> Release v1.50.36 (2024-03-11) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5193\">#5193<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/commit\/b965d6132d94bb7314bd114e3691a3f7005c191d\"><code>b965d61<\/code><\/a> Release v1.50.35 (2024-03-08) (<a href=\"https:\/\/redirect.github.com\/aws\/aws-sdk-go\/issues\/5192\">#5192<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/aws\/aws-sdk-go\/compare\/v1.50.32...v1.51.4\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=github.com\/aws\/aws-sdk-go&package-manager=go_modules&previous-version=1.50.32&new-version=1.51.4)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","go"]},{"title":"build(deps): bump github.com\/Azure\/azure-sdk-for-go\/sdk\/resourcemanager\/compute\/armcompute\/v5 from 5.5.0 to 5.6.0","body":"Bumps [github.com\/Azure\/azure-sdk-for-go\/sdk\/resourcemanager\/compute\/armcompute\/v5](https:\/\/github.com\/Azure\/azure-sdk-for-go) from 5.5.0 to 5.6.0.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/releases\">github.com\/Azure\/azure-sdk-for-go\/sdk\/resourcemanager\/compute\/armcompute\/v5's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>sdk\/resourcemanager\/compute\/armcompute\/v5.6.0<\/h2>\n<h2>5.6.0 (2024-03-22)<\/h2>\n<h3>Features Added<\/h3>\n<ul>\n<li>New field <code>VirtualMachineID<\/code> in struct <code>GalleryArtifactVersionFullSource<\/code><\/li>\n<\/ul>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/01e95ff415ca3f8c5a4330297f218e95d0192be4\"><code>01e95ff<\/code><\/a> [Release] sdk\/resourcemanager\/compute\/armcompute\/5.6.0 (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22506\">#22506<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/79be3862535d0490e95ded1081684d4969e92ef1\"><code>79be386<\/code><\/a> [cleanup] Remove old cognitiveservices\/azopenai module (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22583\">#22583<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/74feca15e5e88f99210bc11fece59521f5601d8b\"><code>74feca1<\/code><\/a> Go mgmt auto release -&gt; <code>1es-templates<\/code> (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22616\">#22616<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/b56179f1bc81ba5f3693996b6f49b763e0f5d449\"><code>b56179f<\/code><\/a> Bump the proxy version to the last one resolving mac timeout issues (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22604\">#22604<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/b59b15cc44407f8ecd616c0b4cf68d2a7dbe714b\"><code>b59b15c<\/code><\/a> [Cosmos] add preferred regions logging (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22598\">#22598<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/3a7a4e77893bb655b5335eea08f11429d36bee59\"><code>3a7a4e7<\/code><\/a> Update tooling (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22618\">#22618<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/2647d4f75389b68651214ba8aea636c8a946a073\"><code>2647d4f<\/code><\/a> [Release] sdk\/resourcemanager\/servicelinker\/armservicelinker\/2.0.0-beta.1 gen...<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/26c0dbb92417dd47756561681490f158a1617de7\"><code>26c0dbb<\/code><\/a> remove all references to docker (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22614\">#22614<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/d80f30f8d3895b8e282dc1f45dcf7acd56309f09\"><code>d80f30f<\/code><\/a> Add AllowInsecureAuth to ClientOptions (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22573\">#22573<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/commit\/046681965cdaf055219c4a8de8df615ea72d036e\"><code>0466819<\/code><\/a> Fix go identity matrix\/pool selection (<a href=\"https:\/\/redirect.github.com\/Azure\/azure-sdk-for-go\/issues\/22610\">#22610<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/Azure\/azure-sdk-for-go\/compare\/sdk\/resourcemanager\/compute\/armcompute\/v5.5.0...sdk\/resourcemanager\/compute\/armcompute\/v5.6.0\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=github.com\/Azure\/azure-sdk-for-go\/sdk\/resourcemanager\/compute\/armcompute\/v5&package-manager=go_modules&previous-version=5.5.0&new-version=5.6.0)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","go"]},{"title":"build(deps): bump github.com\/prometheus\/prometheus from 0.50.1 to 0.51.0 in \/documentation\/examples\/remote_storage","body":"Bumps [github.com\/prometheus\/prometheus](https:\/\/github.com\/prometheus\/prometheus) from 0.50.1 to 0.51.0.\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/c05c15512acb675e3f6cd662a6727854e93fc024\"><code>c05c155<\/code><\/a> Cut v2.51.0 (<a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13787\">#13787<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/8d53e7ba90e0a6707d88a9573ee21e88a7866f1b\"><code>8d53e7b<\/code><\/a> Cut v2.51.0-rc.0 (<a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13729\">#13729<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/e8bf2ce4e170913ec8c582f91074e681a5a62906\"><code>e8bf2ce<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13735\">#13735<\/a> from bboreham\/fix-notifier-relabel<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/54f50e14980850796cf53eb61ba4164db9ea622e\"><code>54f50e1<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13737\">#13737<\/a> from bboreham\/fix-scrape-tolerance<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/6c41ec984f9bb19bba38e9429de7dff620cdf109\"><code>6c41ec9<\/code><\/a> [BUGFIX] Scraping: Tolerance should be max 1% of interval<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/8c4e4b72a8b54ef66d71de76461c719957bf51cc\"><code>8c4e4b7<\/code><\/a> Notifier: pass parameters to goroutine explicitly<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/57c799132b8d5b3bc1c0d5489653a789ccb617dc\"><code>57c7991<\/code><\/a> Notifier: don't reuse payload after relabeling<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/9acae57937f3034e31312f4e68493b91e0c9274e\"><code>9acae57<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13681\">#13681<\/a> from krajorama\/native-latency-histograms<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/3d16d398815d03a756dd8361efe6d1580fba7fcc\"><code>3d16d39<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13716\">#13716<\/a> from prometheus\/update-go-deps<\/li>\n<li><a href=\"https:\/\/github.com\/prometheus\/prometheus\/commit\/b0c0961f9d3048292798f3196aa072142a4c3790\"><code>b0c0961<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/prometheus\/prometheus\/issues\/13725\">#13725<\/a> from prometheus\/beorn7\/promql2<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/prometheus\/prometheus\/compare\/v0.50.1...v0.51.0\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=github.com\/prometheus\/prometheus&package-manager=go_modules&previous-version=0.50.1&new-version=0.51.0)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","go"]},{"title":"build(deps): bump actions\/setup-node from 4.0.1 to 4.0.2","body":"Bumps [actions\/setup-node](https:\/\/github.com\/actions\/setup-node) from 4.0.1 to 4.0.2.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/actions\/setup-node\/releases\">actions\/setup-node's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>v4.0.2<\/h2>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Add support for <code>volta.extends<\/code> by <a href=\"https:\/\/github.com\/ThisIsManta\"><code>@\u200bThisIsManta<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/setup-node\/pull\/921\">actions\/setup-node#921<\/a><\/li>\n<li>Add support for arm64 Windows by <a href=\"https:\/\/github.com\/dmitry-shibanov\"><code>@\u200bdmitry-shibanov<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/setup-node\/pull\/927\">actions\/setup-node#927<\/a><\/li>\n<\/ul>\n<h2>New Contributors<\/h2>\n<ul>\n<li><a href=\"https:\/\/github.com\/ThisIsManta\"><code>@\u200bThisIsManta<\/code><\/a> made their first contribution in <a href=\"https:\/\/redirect.github.com\/actions\/setup-node\/pull\/921\">actions\/setup-node#921<\/a><\/li>\n<\/ul>\n<p><strong>Full Changelog<\/strong>: <a href=\"https:\/\/github.com\/actions\/setup-node\/compare\/v4.0.1...v4.0.2\">https:\/\/github.com\/actions\/setup-node\/compare\/v4.0.1...v4.0.2<\/a><\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/actions\/setup-node\/commit\/60edb5dd545a775178f52524783378180af0d1f8\"><code>60edb5d<\/code><\/a> Add support for arm64 Windows (<a href=\"https:\/\/redirect.github.com\/actions\/setup-node\/issues\/927\">#927<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/setup-node\/commit\/d86ebcd40b3cb50b156bfa44dd277faf38282d12\"><code>d86ebcd<\/code><\/a> Add support for <code>volta.extends<\/code> (<a href=\"https:\/\/redirect.github.com\/actions\/setup-node\/issues\/921\">#921<\/a>)<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/actions\/setup-node\/compare\/b39b52d1213e96004bfcb1c61a8a6fa8ab84f3e8...60edb5dd545a775178f52524783378180af0d1f8\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=actions\/setup-node&package-manager=github_actions&previous-version=4.0.1&new-version=4.0.2)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","github_actions"]},{"title":"build(deps): bump actions\/cache from 4.0.1 to 4.0.2","body":"Bumps [actions\/cache](https:\/\/github.com\/actions\/cache) from 4.0.1 to 4.0.2.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/actions\/cache\/releases\">actions\/cache's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>v4.0.2<\/h2>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Fix <code>fail-on-cache-miss<\/code> not working by <a href=\"https:\/\/github.com\/cdce8p\"><code>@\u200bcdce8p<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/cache\/pull\/1327\">actions\/cache#1327<\/a><\/li>\n<\/ul>\n<p><strong>Full Changelog<\/strong>: <a href=\"https:\/\/github.com\/actions\/cache\/compare\/v4.0.1...v4.0.2\">https:\/\/github.com\/actions\/cache\/compare\/v4.0.1...v4.0.2<\/a><\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/actions\/cache\/blob\/main\/RELEASES.md\">actions\/cache's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h1>Releases<\/h1>\n<h3>4.0.2<\/h3>\n<ul>\n<li>Fixed restore <code>fail-on-cache-miss<\/code> not working.<\/li>\n<\/ul>\n<h3>4.0.1<\/h3>\n<ul>\n<li>Updated <code>isGhes<\/code> check<\/li>\n<\/ul>\n<h3>4.0.0<\/h3>\n<ul>\n<li>Updated minimum runner version support from node 12 -&gt; node 20<\/li>\n<\/ul>\n<h3>3.3.3<\/h3>\n<ul>\n<li>Updates <code>@\u200bactions\/cache<\/code> to v3.2.3 to fix accidental mutated path arguments to <code>getCacheVersion<\/code> <a href=\"https:\/\/redirect.github.com\/actions\/toolkit\/pull\/1378\">actions\/toolkit#1378<\/a><\/li>\n<li>Additional audit fixes of npm package(s)<\/li>\n<\/ul>\n<h3>3.3.2<\/h3>\n<ul>\n<li>Fixes bug with Azure SDK causing blob downloads to get stuck.<\/li>\n<\/ul>\n<h3>3.3.1<\/h3>\n<ul>\n<li>Reduced segment size to 128MB and segment timeout to 10 minutes to fail fast in case the cache download is stuck.<\/li>\n<\/ul>\n<h3>3.3.0<\/h3>\n<ul>\n<li>Added option to lookup cache without downloading it.<\/li>\n<\/ul>\n<h3>3.2.6<\/h3>\n<ul>\n<li>Fix zstd not being used after zstd version upgrade to 1.5.4 on hosted runners.<\/li>\n<\/ul>\n<h3>3.2.5<\/h3>\n<ul>\n<li>Added fix to prevent from setting MYSYS environment variable globally.<\/li>\n<\/ul>\n<h3>3.2.4<\/h3>\n<ul>\n<li>Added option to fail job on cache miss.<\/li>\n<\/ul>\n<h3>3.2.3<\/h3>\n<ul>\n<li>Support cross os caching on Windows as an opt-in feature.<\/li>\n<li>Fix issue with symlink restoration on Windows for cross-os caches.<\/li>\n<\/ul>\n<h3>3.2.2<\/h3>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/actions\/cache\/commit\/0c45773b623bea8c8e75f6c82b208c3cf94ea4f9\"><code>0c45773<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/actions\/cache\/issues\/1327\">#1327<\/a> from cdce8p\/fix-fail-on-cache-miss<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/cache\/commit\/8a55f839aa4b4578e47bdc8a52828637cbb9a454\"><code>8a55f83<\/code><\/a> Add test case for process exit<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/cache\/commit\/3884cace147bdf9307fcc52a277f421af7b30798\"><code>3884cac<\/code><\/a> Bump version<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/cache\/commit\/e29dad3e36390db18fc19fb666cb1302f4929002\"><code>e29dad3<\/code><\/a> Fix fail-on-cache-miss not working<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/actions\/cache\/compare\/ab5e6d0c87105b4c9c2047343972218f562e4319...0c45773b623bea8c8e75f6c82b208c3cf94ea4f9\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=actions\/cache&package-manager=github_actions&previous-version=4.0.1&new-version=4.0.2)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","github_actions"]},{"title":"build(deps): bump actions\/checkout from 4.1.1 to 4.1.2","body":"Bumps [actions\/checkout](https:\/\/github.com\/actions\/checkout) from 4.1.1 to 4.1.2.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/actions\/checkout\/releases\">actions\/checkout's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>v4.1.2<\/h2>\n<p>We are investigating the following issue with this release and have rolled-back the <code>v4<\/code> tag to point to <code>v4.1.1<\/code><\/p>\n<ul>\n<li><code>sparse-checkout<\/code> is not available on git versions prior to 2.27.0 (see <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1651\">actions\/checkout#1651<\/a>)<\/li>\n<\/ul>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Fix: Disable sparse checkout whenever <code>sparse-checkout<\/code> option is not present <a href=\"https:\/\/github.com\/dscho\"><code>@\u200bdscho<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1598\">actions\/checkout#1598<\/a><\/li>\n<li>Bump tough-cookie from 4.0.0 to 4.1.3 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1406\">actions\/checkout#1406<\/a><\/li>\n<li>Bump <code>@\u200bbabel\/traverse<\/code> from 7.20.5 to 7.24.0 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1642\">actions\/checkout#1642<\/a><\/li>\n<\/ul>\n<h2>New Contributors<\/h2>\n<ul>\n<li><a href=\"https:\/\/github.com\/jww3\"><code>@\u200bjww3<\/code><\/a> made their first contribution in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1616\">actions\/checkout#1616<\/a><\/li>\n<\/ul>\n<p><strong>Full Changelog<\/strong>: <a href=\"https:\/\/github.com\/actions\/checkout\/compare\/v4.1.1...v4.1.2\">https:\/\/github.com\/actions\/checkout\/compare\/v4.1.1...v4.1.2<\/a><\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/actions\/checkout\/blob\/main\/CHANGELOG.md\">actions\/checkout's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h1>Changelog<\/h1>\n<h2>v4.1.2<\/h2>\n<ul>\n<li>Fix: Disable sparse checkout whenever <code>sparse-checkout<\/code> option is not present <a href=\"https:\/\/github.com\/dscho\"><code>@\u200bdscho<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1598\">actions\/checkout#1598<\/a><\/li>\n<\/ul>\n<h2>v4.1.1<\/h2>\n<ul>\n<li>Correct link to GitHub Docs by <a href=\"https:\/\/github.com\/peterbe\"><code>@\u200bpeterbe<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1511\">actions\/checkout#1511<\/a><\/li>\n<li>Link to release page from what's new section by <a href=\"https:\/\/github.com\/cory-miller\"><code>@\u200bcory-miller<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1514\">actions\/checkout#1514<\/a><\/li>\n<\/ul>\n<h2>v4.1.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1396\">Add support for partial checkout filters<\/a><\/li>\n<\/ul>\n<h2>v4.0.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1067\">Support fetching without the --progress option<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1436\">Update to node20<\/a><\/li>\n<\/ul>\n<h2>v3.6.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1377\">Fix: Mark test scripts with Bash'isms to be run via Bash<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/579\">Add option to fetch tags even if fetch-depth &gt; 0<\/a><\/li>\n<\/ul>\n<h2>v3.5.3<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1196\">Fix: Checkout fail in self-hosted runners when faulty submodule are checked-in<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1287\">Fix typos found by codespell<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1369\">Add support for sparse checkouts<\/a><\/li>\n<\/ul>\n<h2>v3.5.2<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1289\">Fix api endpoint for GHES<\/a><\/li>\n<\/ul>\n<h2>v3.5.1<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1246\">Fix slow checkout on Windows<\/a><\/li>\n<\/ul>\n<h2>v3.5.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1237\">Add new public key for known_hosts<\/a><\/li>\n<\/ul>\n<h2>v3.4.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1209\">Upgrade codeql actions to v2<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1210\">Upgrade dependencies<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1225\">Upgrade <code>@\u200bactions\/io<\/code><\/a><\/li>\n<\/ul>\n<h2>v3.3.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1045\">Implement branch list using callbacks from exec function<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1050\">Add in explicit reference to private checkout options<\/a><\/li>\n<li>[Fix comment typos (that got added in <a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/770\">#770<\/a>)](<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1057\">actions\/checkout#1057<\/a>)<\/li>\n<\/ul>\n<h2>v3.2.0<\/h2>\n<ul>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/942\">Add GitHub Action to perform release<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/967\">Fix status badge<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1002\">Replace datadog\/squid with ubuntu\/squid Docker image<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/964\">Wrap pipeline commands for submoduleForeach in quotes<\/a><\/li>\n<li><a href=\"https:\/\/redirect.github.com\/actions\/checkout\/pull\/1029\">Update <code>@\u200bactions\/io<\/code> to 1.1.2<\/a><\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/9bb56186c3b09b4f86b1c65136769dd318469633\"><code>9bb5618<\/code><\/a> Prep for release of  v4.1.2  (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1649\">#1649<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/8eb1f6a495037164bea451156472f35fdd6bafc0\"><code>8eb1f6a<\/code><\/a> Bump <code>@\u200bbabel\/traverse<\/code> from 7.20.5 to 7.24.0 (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1642\">#1642<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/556e4c3cb0b8b54b734286d5439adadcb0a8cb92\"><code>556e4c3<\/code><\/a> Bump tough-cookie from 4.0.0 to 4.1.3 (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1406\">#1406<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/b32f140b0c872d58512e0a66172253c302617b90\"><code>b32f140<\/code><\/a> Warn on attempts to publish <code>test-ubuntu-git<\/code> from non-main branch. (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1623\">#1623<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/2650dbd060003e3b5ae211e4358852f336b682a7\"><code>2650dbd<\/code><\/a> Give <code>test-ubuntu-git<\/code> its own <code>README<\/code> (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1620\">#1620<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/aadec899646c8e0f34c52d9219c2faac36626b55\"><code>aadec89<\/code><\/a> Explicitly disable sparse checkout unless asked for (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1598\">#1598<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/df0bcddf6d6823307c716b56a7ef9c3b25078874\"><code>df0bcdd<\/code><\/a> Refine workflow for generating <code>test-ubuntu-git<\/code> (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1617\">#1617<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/actions\/checkout\/commit\/473055ba18d6d2da209cd46110aadb9275e3194e\"><code>473055b<\/code><\/a> Create <code>test-ubuntu-git<\/code> Docker Container for Proxy Tests (<a href=\"https:\/\/redirect.github.com\/actions\/checkout\/issues\/1616\">#1616<\/a>)<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/actions\/checkout\/compare\/b4ffde65f46336ab88eb53be808477a3936bae11...9bb56186c3b09b4f86b1c65136769dd318469633\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=actions\/checkout&package-manager=github_actions&previous-version=4.1.1&new-version=4.1.2)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","github_actions"]},{"title":"tsdb.BeyondTimeRetention: Fix comment and test at retention duration","body":"I noticed that the comment in `tsdb.BeyondTimeRetention` regarding which blocks to delete is out of date since #9633. I'm proposing to make it reflect that blocks with max time difference, versus the first block, greater than _or equal to_ the retention duration are deleted.\r\n\r\nI also propose to make the test `TestTimeRetention` contain also a test case for when a block is exactly at the retention duration (which is what #9633 changes). Currently that particular case is untested. On suggestion from @machine424, remove `TestDB_BeyondTimeRetention` since it overlaps with `TestTimeRetention`.\r\n\r\ncc @darshanime ","comments":["Nice catch @aknuds1, The test doesn't check the `=` case indeed. For the `>` case, we already have [TestTimeRetention](https:\/\/github.com\/prometheus\/prometheus\/blob\/11fc7b1d835e04a7f426841fd0cc8ab178d12717\/tsdb\/db_test.go#L1493), maybe we can go further and add `TestDB_BeyondTimeRetention` as a case in there (merge the two tests)?","I didn't see `TestTimeRetention` @machine424, but sounds like a good idea.","I'll have a look at `TestTimeRetention` tomorrow @machine424, and consider merging the tests.","@machine424 I merged `TestDB_BeyondTimeRetention` into `TestTimeRetention`, just making sure that the case of blocks exactly on the retention boundary is covered. PTAL. Thanks for the suggestion! Agree it's better to avoid two overlapping tests."],"labels":["component\/tsdb"]},{"title":"enable querying of old data with new UTF-8 names","body":"See https:\/\/github.com\/prometheus\/proposals\/blob\/main\/proposals\/2023-11-13-utf8-migration.md#mixed-format-scenarios\r\n\r\nThis bug is for making changes to the read path of Prometheus so that it returns the correct answers for questions asked about UTF-8 metric names and labels. Specifically, it might be asked to find those names in data that was written with an older version of prometheus and is escaped.\r\n\r\n@dimitarvdimitrov had several useful questions and comments, I will paste them here.","comments":["@dimitarvdimitrov says:\r\nI managed to go through the doc and am leaving my notes here. I think my biggest questions are around how promQL will handle the mixed representations:\r\n\r\n* `-promql.utf8_broad_lookup.until=<date-time>` the doc wasn\u2019t explicit, but it will make more sense if this is based on the samples\u2019 time range that a block covers, not on the block\u2019s ULID. This allows for recompaction of old blocks without breaking the compatibility\r\n* another thing I didn\u2019t see mentioned is changes in the promQL engine. While some series will be queried in their old, others as new and both will be retrieved from the block, how do you want to treat those series in query execution - is `{\"label.name\"=\"123\", __name__=\"up\"} `joined with `{\"label_name\"=\"123\", __name__=\"up\"} `in promQL joins or will those two be considered two distinct series by the engine? In other words when I query for {\"label.name\"=\"123\"} do I get back two series with one sample each or one series with two samples?\r\n  * I think the former makes more sense from a user\u2019s perspective\r\n  * if the latter, then doesn\u2019t this trigger the \u201cmultiple vectors with the same label set\u201d\r\n  * as an extention to this - what happens if the query aggregates by a UTF8 label name? `sum by (\"label.name\") ({\"label.name\"=\"123\"} ` and `sum by (label_name) ({\"label.name\"=\"123\"}`\r\n* I don\u2019t fully understand the direction of the support. Building on the example above:\r\n  * `{label_name=\"123\"}` would include both series or only the legacy-style series and return one series with one sample?\r\n  * is there a difference if I query {\"label_name\"=\"123\"}?","me:\r\n\r\n* if the query is for \"label.name\", series with \"label_name\" should be considered that same, not distinct series. (This is why we have the caution not to create new utf8 metric or label names that, under substitution, would collide with existing distinct labels -- there's no way to differentiate that case)\r\n* I would call the aggregation query malformed to contain both utf8 and nonutf8 representations. The intent is that the user would only write the utf8 version. If the user did write the query as you did, the `sum by (label_name)` would (should? might? undefined behavior?) not pick up `label.name`  so the results would be incorrect.\r\n* queries for label_name would not correctly return  data for `label.name` . I cannot think of a reasonable way to up-convert such queries. (since underscores are completely ambiguous)","@dimitarvdimitrov  says:\r\n\r\nI see. So the goal is to make \u201cnew\u201d queries work with old data. Old queries would generally not work with new data. That makes sense to me.\r\n\r\nas for places to modify - i\u2019ll leave some pointers in the prometheus codebase and can later dig into mimir when you get to it. I suspect you\u2019re already familiar with some of them\r\n\r\n\r\n1. doing the query aggregation validation you mentioned while parsing the query - [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/e01e7d36e284857cd6b961cc1627bedbd62be890\/promql\/parser\/parse.go#L630) (i\u2019m not 100% sure about this, perhaps asking someone like @ Jeanette Tan or @ beorn about the right place to modify the promql parser at\r\n2. finding the series which match the vector selector - [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/querier.go#L249); i suspect here we do the multiple variations of the label names\r\n3. choosing the right strings for the label name - [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/querier.go#L560); but generally i\u2019d check all the invocations of Series since there might be other places that could use this relabelling. Perhaps adding another IndexReader implementation which transparently does the translation would make sense\r\n  a. the tricky bit here is that the SeriesSet implementation must return series in lexicographical order. Today the series IDs returned from PostingsForMatchers (step 2. above) are already sorted like that. In prometheus blocks the series ID uint64 sorting implies lexicographical sorting of labels - [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/index\/index.go#L1770-L1774)\r\n  b. when you have the same series with different string literals they will not sort lexicographically\r\n  c. so perhaps changing the Reader.SortedPostings to put postings with different (same [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/index\/index.go#L1770-L1774) link as above)\r\n  d. same probably applies to the other IndexReader implementaitons - for example the [Head](https:\/\/github.com\/dimitarvdimitrov\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/head_read.go#L124) and [OOOHead](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/tsdb\/ooo_head_read.go#L438-L441)\r\n4. The SeriesSet should also merge the series with effectively same label sets and also merge their chunks so that the two block series appear as one and their chunks appear interleaved. This is some [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/f477e0539a4f196759cc9aa5c79698c758ca04b9\/storage\/merge.go#L290) which merges the same series. Perhaps extending that or adding a new similar implementation (utf8EquivalentMergedSeriesSet?) might do the trick\r\n\r\n\r\nafter all of those the SeriesSet returns one entry per duplicate series, so the promQL engine and everything else that follows should \u201cjust work\u201d :tm:\r\n\r\nThese are pointers only and there may be other places that need changing along those lines (e.g. multiple SeriesSets or multiple IndexReaders).","one more from @dimitarvdimitrov \r\n\r\ni realized that people may be aggregating by \u201cold\u201d labels without noticing it. You can add query annotations during query execution when the aggregated by label name exists in a \u201cnew\u201d form somewhere in the series labels. Grafana would show these as warnings. Here\u2019s one place where we add the warning when there are float and native histogram samples for the same series (which shouldn\u2019t normally happen) - [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/9187bcbdd52c28da9696bf9e2ac2f6adb66082b4\/promql\/engine.go#L2931)"],"labels":["component\/tsdb"]},{"title":"[ENHANCEMENT] PromQL: Re-structure aggregations for clarity and performance","body":"I found it very hard to follow the twists and turns of the `aggregation()` function.\r\n\r\nThis PR splits it into three implementations divided by how they output:\r\n* `aggregation` for sum, avg, count, stdvar, stddev and quantile, produces one output series for each group specified in the expression, with just the labels from `by(...)`.\r\n* `aggregationK`, for topk and bottomk, in contrast produces output that has the same labels as the input, but just `k` of them per group.\r\n* `aggregationCountValues` outputs as many series per group as there are values in the input.\r\n\r\nThe main `aggregation` function no longer looks up each input and output series in a map on every timestamp; instead the lookup is done once per input series and a slice mapping input to output indexes is used subsequently.\r\n\r\nOne test had to change: because we no longer generate an intermediate vector of input samples the peak-sample count has gone down.\r\n\r\n`aggregation()` was 325 lines before and 190 lines now.\r\n\r\nThere are a lot of commits in this PR, and some of them say \"WIP\"; I have some hopes of cleaning them up, but could also squash the lot.\r\n\r\nBenchmarks, first steps=100 then steps=1000:\r\n```\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\n                                                                              \u2502 before100.txt \u2502            after100.txt             \u2502\r\n                                                                              \u2502    sec\/op     \u2502    sec\/op     vs base               \u2502\r\nRangeQuery\/expr=a_hundred,steps=100-8                                            481.3\u00b5 \u00b1  2%   496.0\u00b5 \u00b1  8%   +3.05% (p=0.026 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=100-8                                  1.042m \u00b1  9%   1.026m \u00b1  5%        ~ (p=0.699 n=6)\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-8                561.2m \u00b1  3%   573.0m \u00b1 13%        ~ (p=0.240 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=100-8                               99.79m \u00b1  2%   98.44m \u00b1  4%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=100-8                                  77.21m \u00b1  3%   76.13m \u00b1  1%   -1.39% (p=0.041 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-8                      45.20m \u00b1  2%   44.95m \u00b1  1%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=100-8                                           491.7\u00b5 \u00b1 16%   494.3\u00b5 \u00b1  6%        ~ (p=0.310 n=6)\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-8                                3.064m \u00b1  3%   3.055m \u00b1  4%        ~ (p=0.937 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-8               1.620m \u00b1  4%   1.602m \u00b1  1%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-8                1.944m \u00b1 11%   1.934m \u00b1  0%        ~ (p=0.485 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-8            1.624m \u00b1  6%   1.602m \u00b1  1%        ~ (p=0.180 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-8                549.4\u00b5 \u00b1  5%   545.8\u00b5 \u00b1  1%        ~ (p=0.589 n=6)\r\nRangeQuery\/expr=abs(a_hundred),steps=100-8                                       949.5\u00b5 \u00b1  3%   955.9\u00b5 \u00b1  0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-8    515.4\u00b5 \u00b1  1%   515.6\u00b5 \u00b1  0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-8           508.2\u00b5 \u00b1  2%   498.3\u00b5 \u00b1  1%        ~ (p=0.065 n=6)\r\nRangeQuery\/expr=sum(a_hundred),steps=100-8                                       609.5\u00b5 \u00b1  8%   513.7\u00b5 \u00b1  0%  -15.71% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-8                           7.147m \u00b1  3%   5.529m \u00b1  4%  -22.64% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-8                          9.090m \u00b1  4%   5.628m \u00b1  1%  -38.09% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-8                                9.519m \u00b1  3%   5.603m \u00b1  1%  -41.13% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-8                               7.252m \u00b1  6%   5.521m \u00b1  1%  -23.87% (p=0.002 n=6)\r\nRangeQuery\/expr=count_values('value',_h_hundred),steps=100-8                     135.7m \u00b1  1%   134.3m \u00b1  1%   -1.00% (p=0.009 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=100-8                                   636.0\u00b5 \u00b1  1%   536.1\u00b5 \u00b1  1%  -15.71% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=100-8                                   845.1\u00b5 \u00b1  0%   737.0\u00b5 \u00b1  1%  -12.79% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-8            4.033m \u00b1  0%   4.038m \u00b1  2%        ~ (p=0.937 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-8                 1.152m \u00b1  0%   1.048m \u00b1  9%   -9.08% (p=0.002 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-8         21.47m \u00b1  1%   21.22m \u00b1  1%   -1.19% (p=0.004 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-8                  958.0\u00b5 \u00b1  1%   947.0\u00b5 \u00b1  1%   -1.15% (p=0.026 n=6)\r\nRangeQuery\/expr=timestamp(a_hundred),steps=100-8                                 965.9\u00b5 \u00b1  1%   969.5\u00b5 \u00b1 10%        ~ (p=0.310 n=6)\r\ngeomean                                                                          3.513m         3.249m         -7.51%\r\n\r\n                                                                              \u2502 before100.txt \u2502            after100.txt             \u2502\r\n                                                                              \u2502     B\/op      \u2502     B\/op      vs base               \u2502\r\nRangeQuery\/expr=a_hundred,steps=100-8                                            75.05Ki \u00b1 0%   75.06Ki \u00b1 0%        ~ (p=0.675 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=100-8                                  100.6Ki \u00b1 0%   100.6Ki \u00b1 0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-8                3.489Mi \u00b1 0%   3.487Mi \u00b1 0%        ~ (p=0.242 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=100-8                               3.044Mi \u00b1 2%   3.004Mi \u00b1 0%   -1.30% (p=0.041 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=100-8                                  3.041Mi \u00b1 2%   3.024Mi \u00b1 1%        ~ (p=0.589 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-8                      3.180Mi \u00b1 1%   3.181Mi \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=100-8                                           78.64Ki \u00b1 0%   78.64Ki \u00b1 0%        ~ (p=0.859 n=6)\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-8                                288.3Ki \u00b1 0%   284.8Ki \u00b1 0%   -1.21% (p=0.002 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-8               680.3Ki \u00b1 0%   678.0Ki \u00b1 0%   -0.34% (p=0.002 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-8                1.195Mi \u00b1 0%   1.192Mi \u00b1 0%   -0.21% (p=0.002 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-8            680.5Ki \u00b1 0%   677.8Ki \u00b1 0%   -0.40% (p=0.002 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-8                127.7Ki \u00b1 0%   125.9Ki \u00b1 0%   -1.38% (p=0.002 n=6)\r\nRangeQuery\/expr=abs(a_hundred),steps=100-8                                       123.2Ki \u00b1 0%   123.2Ki \u00b1 0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-8    92.46Ki \u00b1 0%   92.45Ki \u00b1 0%   -0.01% (p=0.039 n=6)\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-8           83.82Ki \u00b1 0%   83.83Ki \u00b1 0%        ~ (p=0.368 n=6)\r\nRangeQuery\/expr=sum(a_hundred),steps=100-8                                      136.75Ki \u00b1 0%   83.41Ki \u00b1 0%  -39.01% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-8                          1400.7Ki \u00b1 0%   868.7Ki \u00b1 0%  -37.98% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-8                         3862.0Ki \u00b1 0%   901.8Ki \u00b1 0%  -76.65% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-8                               3862.2Ki \u00b1 0%   901.9Ki \u00b1 0%  -76.65% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-8                              1400.6Ki \u00b1 0%   868.8Ki \u00b1 0%  -37.97% (p=0.002 n=6)\r\nRangeQuery\/expr=count_values('value',_h_hundred),steps=100-8                     97.76Mi \u00b1 0%   85.49Mi \u00b1 0%  -12.55% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=100-8                                   142.5Ki \u00b1 0%   121.5Ki \u00b1 0%  -14.71% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=100-8                                   181.4Ki \u00b1 0%   141.5Ki \u00b1 0%  -22.00% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-8            333.9Ki \u00b1 0%   330.5Ki \u00b1 0%   -1.02% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-8                 163.0Ki \u00b1 0%   109.2Ki \u00b1 0%  -32.97% (p=0.002 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-8         1.453Mi \u00b1 0%   1.453Mi \u00b1 0%        ~ (p=0.589 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-8                  170.0Ki \u00b1 0%   168.2Ki \u00b1 0%   -1.05% (p=0.002 n=6)\r\nRangeQuery\/expr=timestamp(a_hundred),steps=100-8                                 583.9Ki \u00b1 0%   583.9Ki \u00b1 0%        ~ (p=0.589 n=6)\r\ngeomean                                                                          564.4Ki        465.8Ki       -17.47%\r\n\r\n                                                                              \u2502 before100.txt \u2502             after100.txt             \u2502\r\n                                                                              \u2502   allocs\/op   \u2502  allocs\/op   vs base                 \u2502\r\nRangeQuery\/expr=a_hundred,steps=100-8                                             1.199k \u00b1 0%   1.199k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=100-8                                   1.644k \u00b1 0%   1.644k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-8                 37.09k \u00b1 0%   37.08k \u00b1 0%        ~ (p=0.242 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=100-8                                36.99k \u00b1 0%   36.99k \u00b1 0%        ~ (p=0.061 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=100-8                                   37.39k \u00b1 0%   37.39k \u00b1 0%        ~ (p=0.240 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-8                       37.08k \u00b1 0%   37.08k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=100-8                                            1.217k \u00b1 0%   1.217k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-8                                 2.996k \u00b1 0%   2.997k \u00b1 0%        ~ (p=0.182 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-8                2.989k \u00b1 0%   2.989k \u00b1 0%        ~ (p=0.545 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-8                 3.149k \u00b1 0%   3.149k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-8             2.989k \u00b1 0%   2.989k \u00b1 0%        ~ (p=0.545 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-8                 1.568k \u00b1 0%   1.568k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=abs(a_hundred),steps=100-8                                        1.334k \u00b1 0%   1.334k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-8     1.740k \u00b1 0%   1.740k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-8            1.505k \u00b1 0%   1.505k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=sum(a_hundred),steps=100-8                                        1.640k \u00b1 0%   1.225k \u00b1 0%  -25.30% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-8                            15.15k \u00b1 0%   12.12k \u00b1 0%  -20.02% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-8                           34.17k \u00b1 0%   12.22k \u00b1 0%  -64.23% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-8                                 34.17k \u00b1 0%   12.22k \u00b1 0%  -64.23% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-8                                15.15k \u00b1 0%   12.12k \u00b1 0%  -20.02% (p=0.002 n=6)\r\nRangeQuery\/expr=count_values('value',_h_hundred),steps=100-8                      574.3k \u00b1 0%   573.0k \u00b1 0%   -0.22% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=100-8                                    1.763k \u00b1 0%   1.546k \u00b1 0%  -12.31% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=100-8                                    2.369k \u00b1 0%   1.748k \u00b1 0%  -26.21% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-8             3.881k \u00b1 0%   3.881k \u00b1 0%        ~ (p=0.455 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-8                  2.188k \u00b1 0%   1.673k \u00b1 0%  -23.54% (p=0.002 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-8          17.80k \u00b1 0%   17.80k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-8                   1.601k \u00b1 0%   1.601k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=timestamp(a_hundred),steps=100-8                                  1.935k \u00b1 0%   1.935k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\ngeomean                                                                           5.631k        4.969k       -11.75%\r\n\u00b9 all samples are equal\r\n\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\n                                                                               \u2502 before1000.txt \u2502            after1000.txt            \u2502\r\n                                                                               \u2502     sec\/op     \u2502    sec\/op     vs base               \u2502\r\nRangeQuery\/expr=a_hundred,steps=1000-8                                              2.673m \u00b1 1%   2.672m \u00b1  1%        ~ (p=0.699 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-8                                    7.397m \u00b1 0%   7.266m \u00b1  6%        ~ (p=0.065 n=6)\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-8                   5.239 \u00b1 4%    5.308 \u00b1  0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-8                                 762.8m \u00b1 1%   760.0m \u00b1  0%   -0.36% (p=0.041 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-8                                    541.1m \u00b1 1%   548.0m \u00b1  4%   +1.27% (p=0.026 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-8                        239.6m \u00b1 2%   241.6m \u00b1  1%        ~ (p=0.132 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=1000-8                                             2.723m \u00b1 0%   2.724m \u00b1  0%        ~ (p=0.589 n=6)\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-8                                  25.55m \u00b1 1%   25.67m \u00b1  1%        ~ (p=0.093 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-8                 12.25m \u00b1 7%   12.30m \u00b1  0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-8                  15.47m \u00b1 1%   15.48m \u00b1  0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-8              12.32m \u00b1 1%   12.33m \u00b1  0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-8                  3.296m \u00b1 0%   3.276m \u00b1  8%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=abs(a_hundred),steps=1000-8                                         7.080m \u00b1 5%   7.152m \u00b1  0%        ~ (p=0.065 n=6)\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-8      2.704m \u00b1 0%   2.702m \u00b1  0%        ~ (p=0.485 n=6)\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-8             2.684m \u00b1 0%   2.686m \u00b1  0%        ~ (p=0.180 n=6)\r\nRangeQuery\/expr=sum(a_hundred),steps=1000-8                                         3.982m \u00b1 4%   3.041m \u00b1  4%  -23.62% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-8                             52.36m \u00b1 2%   35.08m \u00b1  0%  -32.99% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-8                            74.45m \u00b1 3%   36.03m \u00b1  1%  -51.60% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-8                                  75.09m \u00b1 1%   35.83m \u00b1  9%  -52.28% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-8                                 52.73m \u00b1 1%   34.79m \u00b1  1%  -34.01% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=1000-8                                     4.145m \u00b1 0%   3.188m \u00b1  1%  -23.08% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=1000-8                                     6.181m \u00b1 1%   5.147m \u00b1  0%  -16.72% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-8              34.72m \u00b1 2%   34.24m \u00b1  1%   -1.36% (p=0.004 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-8                   8.715m \u00b1 0%   8.808m \u00b1 13%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-8           182.2m \u00b1 1%   181.7m \u00b1  4%        ~ (p=0.818 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-8                    7.233m \u00b1 1%   7.110m \u00b1  0%   -1.70% (p=0.002 n=6)\r\nRangeQuery\/expr=timestamp(a_hundred),steps=1000-8                                   7.125m \u00b1 2%   7.178m \u00b1  0%        ~ (p=0.394 n=6)\r\ngeomean                                                                             21.72m        19.46m        -10.40%\r\n\r\n                                                                               \u2502 before1000.txt \u2502            after1000.txt            \u2502\r\n                                                                               \u2502      B\/op      \u2502     B\/op      vs base               \u2502\r\nRangeQuery\/expr=a_hundred,steps=1000-8                                             353.0Ki \u00b1 0%   353.0Ki \u00b1 0%        ~ (p=0.699 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-8                                   345.8Ki \u00b1 0%   345.8Ki \u00b1 0%        ~ (p=0.394 n=6)\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-8                 3.709Mi \u00b1 1%   3.709Mi \u00b1 1%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-8                                3.354Mi \u00b1 3%   3.467Mi \u00b1 7%        ~ (p=0.082 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-8                                   3.375Mi \u00b1 3%   3.374Mi \u00b1 3%        ~ (p=0.853 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-8                       4.796Mi \u00b1 0%   4.845Mi \u00b1 1%        ~ (p=0.288 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=1000-8                                            356.5Ki \u00b1 0%   356.5Ki \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-8                                 887.2Ki \u00b1 0%   883.5Ki \u00b1 0%   -0.42% (p=0.002 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-8                5.396Mi \u00b1 0%   5.395Mi \u00b1 0%   -0.03% (p=0.026 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-8                 10.62Mi \u00b1 0%   10.61Mi \u00b1 0%   -0.05% (p=0.009 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-8             5.397Mi \u00b1 0%   5.394Mi \u00b1 0%   -0.05% (p=0.041 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-8                 447.8Ki \u00b1 0%   446.0Ki \u00b1 0%   -0.40% (p=0.002 n=6)\r\nRangeQuery\/expr=abs(a_hundred),steps=1000-8                                        422.4Ki \u00b1 0%   422.4Ki \u00b1 0%        ~ (p=0.894 n=6)\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-8     370.4Ki \u00b1 0%   370.4Ki \u00b1 0%        ~ (p=0.240 n=6)\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-8            361.8Ki \u00b1 0%   361.8Ki \u00b1 0%        ~ (p=0.589 n=6)\r\nRangeQuery\/expr=sum(a_hundred),steps=1000-8                                        562.5Ki \u00b1 0%   361.3Ki \u00b1 0%  -35.77% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-8                            6.214Mi \u00b1 0%   3.819Mi \u00b1 0%  -38.54% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-8                          29.886Mi \u00b1 0%   3.850Mi \u00b1 0%  -87.12% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-8                                29.886Mi \u00b1 0%   3.851Mi \u00b1 0%  -87.12% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-8                                6.213Mi \u00b1 0%   3.819Mi \u00b1 0%  -38.53% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=1000-8                                    610.3Ki \u00b1 0%   554.4Ki \u00b1 0%   -9.16% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=1000-8                                    994.1Ki \u00b1 0%   750.4Ki \u00b1 0%  -24.52% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-8             867.7Ki \u00b1 0%   864.8Ki \u00b1 0%   -0.33% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-8                  559.5Ki \u00b1 0%   354.4Ki \u00b1 0%  -36.66% (p=0.002 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-8          4.465Mi \u00b1 0%   4.465Mi \u00b1 1%        ~ (p=0.974 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-8                   494.8Ki \u00b1 0%   493.0Ki \u00b1 0%   -0.36% (p=0.002 n=6)\r\nRangeQuery\/expr=timestamp(a_hundred),steps=1000-8                                  4.394Mi \u00b1 0%   4.394Mi \u00b1 0%        ~ (p=0.132 n=6)\r\ngeomean                                                                            1.724Mi        1.364Mi       -20.88%\r\n\r\n                                                                               \u2502 before1000.txt \u2502            after1000.txt             \u2502\r\n                                                                               \u2502   allocs\/op    \u2502  allocs\/op   vs base                 \u2502\r\nRangeQuery\/expr=a_hundred,steps=1000-8                                              5.206k \u00b1 0%   5.206k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-8                                    5.164k \u00b1 0%   5.164k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-8                  40.51k \u00b1 0%   40.51k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-8                                 40.42k \u00b1 0%   40.42k \u00b1 0%        ~ (p=0.082 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-8                                    40.82k \u00b1 0%   40.82k \u00b1 0%        ~ (p=0.946 n=6)\r\nRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-8                        40.50k \u00b1 0%   40.50k \u00b1 0%        ~ (p=0.076 n=6)\r\nRangeQuery\/expr=-a_hundred,steps=1000-8                                             5.224k \u00b1 0%   5.224k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-8                                  12.81k \u00b1 0%   12.81k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-8                 16.92k \u00b1 0%   16.92k \u00b1 0%        ~ (p=0.260 n=6)\r\nRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-8                  18.50k \u00b1 0%   18.50k \u00b1 0%        ~ (p=0.794 n=6)\r\nRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-8              16.92k \u00b1 0%   16.92k \u00b1 0%        ~ (p=0.857 n=6)\r\nRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-8                  7.375k \u00b1 0%   7.375k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=abs(a_hundred),steps=1000-8                                         6.241k \u00b1 0%   6.241k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-8      5.748k \u00b1 0%   5.748k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-8             5.512k \u00b1 0%   5.512k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nRangeQuery\/expr=sum(a_hundred),steps=1000-8                                         9.248k \u00b1 0%   5.232k \u00b1 0%  -43.43% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-8                             86.16k \u00b1 0%   56.07k \u00b1 0%  -34.92% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-8                           274.62k \u00b1 0%   56.18k \u00b1 0%  -79.54% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-8                                 274.62k \u00b1 0%   56.18k \u00b1 0%  -79.54% (p=0.002 n=6)\r\nRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-8                                 86.17k \u00b1 0%   56.07k \u00b1 0%  -34.93% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(1,_a_hundred),steps=1000-8                                    10.271k \u00b1 0%   8.253k \u00b1 0%  -19.65% (p=0.002 n=6)\r\nRangeQuery\/expr=topk(5,_a_hundred),steps=1000-8                                     16.28k \u00b1 0%   10.26k \u00b1 0%  -36.99% (p=0.002 n=6)\r\nRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-8              12.72k \u00b1 0%   12.72k \u00b1 0%        ~ (p=0.182 n=6)\r\nRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-8                  10.208k \u00b1 0%   5.192k \u00b1 0%  -49.13% (p=0.002 n=6)\r\nRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-8           63.56k \u00b1 0%   63.56k \u00b1 0%        ~ (p=0.680 n=6)\r\nRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-8                    7.453k \u00b1 0%   7.453k \u00b1 0%        ~ (p=1.000 n=6)\r\nRangeQuery\/expr=timestamp(a_hundred),steps=1000-8                                   8.648k \u00b1 0%   8.648k \u00b1 0%        ~ (p=0.273 n=6)\r\ngeomean                                                                             18.54k        14.87k       -19.80%\r\n\u00b9 all samples are equal\r\n```","comments":["\/prombench main\r\n","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13744`**](http:\/\/prombench.prometheus.io\/13744\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13744\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13744\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13744)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213744%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","\/prombench cancel\r\n","Benchmark cancel is in progress.\n","Note: I should rebase this before it is merged, to check that tests changed by #13667 still pass.","Similar idea: #8602. \r\n(Which I don\u2019t think I would have understood at the time. I have to try changing the code myself before I understand it)"],"labels":["prombench"]},{"title":"build(deps-dev): bump prettier from 2.8.8 to 3.2.5 in \/web\/ui","body":"Bumps [prettier](https:\/\/github.com\/prettier\/prettier) from 2.8.8 to 3.2.5.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/prettier\/prettier\/releases\">prettier's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>3.2.5<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#325\">Changelog<\/a><\/p>\n<h2>3.2.4<\/h2>\n<ul>\n<li>Fix <code>.eslintrc.json<\/code> format <a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/15947\">#15947<\/a><\/li>\n<\/ul>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#324\">Changelog<\/a><\/p>\n<h2>3.2.3<\/h2>\n<ul>\n<li>Format <code>tsconfig.json<\/code> file with <code>jsonc<\/code> parser <a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/15927\">#15927<\/a><\/li>\n<\/ul>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#323\">Changelog<\/a><\/p>\n<h2>3.2.2<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#322\">Changelog<\/a><\/p>\n<h2>3.2.1<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#321\">Changelog<\/a><\/p>\n<h2>3.2.0<\/h2>\n<p><a href=\"https:\/\/github.com\/prettier\/prettier\/compare\/3.1.1...3.2.0\">diff<\/a><\/p>\n<p>\ud83d\udd17 <a href=\"https:\/\/prettier.io\/blog\/2024\/01\/12\/3.2.0.html\">Release note<\/a><\/p>\n<h2>3.1.1<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#311\">Changelog<\/a><\/p>\n<h2>3.1.0<\/h2>\n<p><a href=\"https:\/\/github.com\/prettier\/prettier\/compare\/3.0.3...3.1.0\">diff<\/a><\/p>\n<p>\ud83d\udd17 <a href=\"https:\/\/prettier.io\/blog\/2023\/11\/13\/3.1.0.html\">Release note<\/a><\/p>\n<h2>3.0.3<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#303\">Changelog<\/a><\/p>\n<h2>3.0.2<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#302\">Changelog<\/a><\/p>\n<h2>3.0.1<\/h2>\n<p>\ud83d\udd17 <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md#301\">Changelog<\/a><\/p>\n<h2>3.0.0<\/h2>\n<p><a href=\"https:\/\/github.com\/prettier\/prettier\/compare\/3.0.0-alpha.6...3.0.0\">diff<\/a><\/p>\n<p>\ud83d\udd17 <a href=\"https:\/\/prettier.io\/blog\/2023\/07\/05\/3.0.0.html\">Release note<\/a><\/p>\n<h2>3.0.0-alpha.6<\/h2>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Update <code>.d.ts<\/code> files of plugins to use <code>export default ...<\/code> by <a href=\"https:\/\/github.com\/fisker\"><code>@\u200bfisker<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/pull\/14435\">prettier\/prettier#14435<\/a><\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/prettier\/prettier\/blob\/main\/CHANGELOG.md\">prettier's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h1>3.2.5<\/h1>\n<p><a href=\"https:\/\/github.com\/prettier\/prettier\/compare\/3.2.4...3.2.5\">diff<\/a><\/p>\n<h4>Support Angular inline styles as single template literal (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/pull\/15968\">#15968<\/a> by <a href=\"https:\/\/github.com\/sosukesuzuki\"><code>@\u200bsosukesuzuki<\/code><\/a>)<\/h4>\n<p><a href=\"https:\/\/blog.angular.io\/introducing-angular-v17-4d7033312e4b\">Angular v17<\/a> supports single string inline styles.<\/p>\n<!-- raw HTML omitted -->\n<pre lang=\"ts\"><code>\/\/ Input\n@Component({\n  template: `&lt;div&gt;...&lt;\/div&gt;`,\n  styles: `h1 { color: blue; }`,\n})\nexport class AppComponent {}\n<p>\/\/ Prettier 3.2.4\n<a href=\"https:\/\/github.com\/Component\"><code>@\u200bComponent<\/code><\/a>({\ntemplate: <code>&amp;lt;div&amp;gt;...&amp;lt;\/div&amp;gt;<\/code>,\nstyles: <code>h1 { color: blue; }<\/code>,\n})\nexport class AppComponent {}<\/p>\n<p>\/\/ Prettier 3.2.5\n<a href=\"https:\/\/github.com\/Component\"><code>@\u200bComponent<\/code><\/a>({\ntemplate: <code>&amp;lt;div&amp;gt;...&amp;lt;\/div&amp;gt;<\/code>,\nstyles: <code>h1 { color: blue; }<\/code>,\n})\nexport class AppComponent {}<\/p>\n<p><\/code><\/pre><\/p>\n<h4>Unexpected embedded formatting for Angular template (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/pull\/15969\">#15969<\/a> by <a href=\"https:\/\/github.com\/JounQin\"><code>@\u200bJounQin<\/code><\/a>)<\/h4>\n<p>Computed template should not be considered as Angular component template<\/p>\n<!-- raw HTML omitted -->\n<pre lang=\"ts\"><code>\/\/ Input\nconst template = &quot;foobar&quot;;\n<p><a href=\"https:\/\/github.com\/Component\"><code>@\u200bComponent<\/code><\/a>({\n[template]: <code>&amp;lt;h1&amp;gt;{{       hello }}&amp;lt;\/h1&amp;gt;<\/code>,\n})\nexport class AppComponent {}\n&lt;\/tr&gt;&lt;\/table&gt;\n<\/code><\/pre><\/p>\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/7142cf354cce2558f41574f44b967baf11d5b603\"><code>7142cf3<\/code><\/a> Release 3.2.5<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/8cbee2e217baad7acf4cb3947834e8c1b41ed647\"><code>8cbee2e<\/code><\/a> chore(deps): update glimmer to v0.88.1 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/15991\">#15991<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/45baee061cb982d9dd298fefa74f4c195a3e0709\"><code>45baee0<\/code><\/a> chore(deps): update dependency magic-string to v0.30.6 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16022\">#16022<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/9fb32a1a6b10bfb6dae317492f10e5e42956cf23\"><code>9fb32a1<\/code><\/a> Minor refactor to property print (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/15924\">#15924<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/08f19401e48ccfeddff4300827da4c8677cb2b79\"><code>08f1940<\/code><\/a> Update install script for husky v9 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16000\">#16000<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/6d0b1d2a935d47e0517721a0d5e62eb79e972b0c\"><code>6d0b1d2<\/code><\/a> Update yarn to v4.1.0 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16021\">#16021<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/c8ba8dbca18858a7962184bbb3898502b9ec7cfb\"><code>c8ba8db<\/code><\/a> chore(deps): update dependency <code>@\u200bangular\/compiler<\/code> to v17.1.2 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16018\">#16018<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/e2250ec6881222a1bb46ef55403067a259d8c7a3\"><code>e2250ec<\/code><\/a> chore(deps): update typescript-eslint to v6.20.0 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16015\">#16015<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/02865f6cc77858b3a4cbaf7d4e2e72a4e88fe175\"><code>02865f6<\/code><\/a> chore(deps): update dependency npm-run-all2 to v6.1.2 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16017\">#16017<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/prettier\/prettier\/commit\/014ee5d47445ae79233291d5b4846b28bedf3601\"><code>014ee5d<\/code><\/a> chore(deps): update dependency hermes-parser to v0.19.0 (<a href=\"https:\/\/redirect.github.com\/prettier\/prettier\/issues\/16014\">#16014<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/prettier\/prettier\/compare\/2.8.8...3.2.5\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=prettier&package-manager=npm_and_yarn&previous-version=2.8.8&new-version=3.2.5)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":[],"labels":["dependencies","javascript"]},{"title":"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)","body":"### What did you do?\r\n\r\nMultiple errors of `ts=2024-02-29T16:40:47.378Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)` in the logs - during normal operation.\r\n\r\n### What did you expect to see?\r\n\r\nNo errors.\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\n_No response_\r\n\r\n### System information\r\n\r\nLinux 4.15.0-142-generic x86_64\r\n\r\n### Prometheus version\r\n\r\n```text\r\nprometheus, version 2.47.2 (branch: HEAD, revision: 3f3172cde1ee37f1c7b3a5f3d9b031190509b3ad)\r\n  build user:       root@79f2ad339b75\r\n  build date:       20231012-16:07:10\r\n  go version:       go1.21.3\r\n  platform:         linux\/amd64\r\n  tags:             netgo,builtinassets,stringlabels\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```global:\r\n  scrape_interval: 1m\r\n  scrape_timeout: 50s\r\n  evaluation_interval: 1m\r\n  external_labels:\r\n    thanos_distinct: lalala\r\nrule_files:\r\n- \/etc\/config\/recording_rules.yml\r\n- \/etc\/config\/alerting_rules.yml\r\nscrape_configs:\r\n- job_name: pushgateway-lalala\r\n  honor_labels: true\r\n  honor_timestamps: true\r\n  params:\r\n    owner:\r\n    - an_owner\r\n    service:\r\n    - serviceA\r\n  scrape_interval: 1m\r\n  scrape_timeout: 50s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  enable_http2: true\r\n  relabel_configs:\r\n  - separator: ;\r\n    regex: (.*)\r\n    target_label: owner\r\n    replacement: devx\r\n    action: replace\r\n  - source_labels: [__address__]\r\n    separator: ;\r\n    regex: (.*)\r\n    modulus: 11\r\n    target_label: __tmp_hash\r\n    replacement: $1\r\n    action: hashmod\r\n  - source_labels: [__tmp_hash]\r\n    separator: ;\r\n    regex: ^10$\r\n    replacement: $1\r\n    action: keep\r\n  metric_relabel_configs:\r\n  - source_labels: [tmp]\r\n    separator: ;\r\n    regex: .*\r\n    target_label: tmp\r\n    action: replace\r\n  - source_labels: [instance]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: exported_instance\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [owner]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: exported_owner\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [instance]\r\n    separator: ;\r\n    regex: ^$\r\n    target_label: instance\r\n    replacement: some_replacement\r\n    action: replace\r\n  static_configs:\r\n  - targets:\r\n    - sometarget.svc.cluster.local:9091\r\n    labels:\r\n      owner: devx\r\n- job_name: fed-target\r\n  honor_labels: true\r\n  honor_timestamps: true\r\n  params:\r\n    match[]:\r\n    - some_metric\r\n  scrape_interval: 1m\r\n  scrape_timeout: 50s\r\n  metrics_path: \/federate\r\n  scheme: http\r\n  follow_redirects: true\r\n  enable_http2: true\r\n  relabel_configs:\r\n  - source_labels: [__address__]\r\n    separator: ;\r\n    regex: (.*)\r\n    modulus: 11\r\n    target_label: __tmp_hash\r\n    replacement: $1\r\n    action: hashmod\r\n  - source_labels: [__tmp_hash]\r\n    separator: ;\r\n    regex: ^10$\r\n    replacement: $1\r\n    action: keep\r\n  metric_relabel_configs:\r\n  - separator: ;\r\n    regex: ^thanos_distinct$\r\n    replacement: $1\r\n    action: labeldrop\r\n  consul_sd_configs:\r\n  - server: sometarget.svc.cluster.local:8500\r\n    datacenter: chidc2\r\n    tag_separator: ','\r\n    scheme: http\r\n    allow_stale: true\r\n    refresh_interval: 1m\r\n    services:\r\n    - serviceA\r\n    - serviceB\r\n    follow_redirects: true\r\n    enable_http2: true\r\n```\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n```text\r\nts=2024-02-29T19:48:50.391Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)\r\nts=2024-02-29T19:49:15.226Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)\r\nts=2024-02-29T19:49:58.233Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)\r\nts=2024-02-29T20:08:02.707Z caller=stdlib.go:105 level=error component=web caller=\"http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98\" msg=)\r\n```\r\n","comments":["Same errors spotted with v2.50.1 as well.","Same error with v2.48.0","I'm looking into this.","Could you provide a simple procedure for reproducing this log message? Using v2.47.2, I've tried accessing various web server paths, and am so far unable to trigger the bug.","Unfortunately now, but it seems to be happening when CPU usage is high.\r\nI'll try to see if I can narrow it down to specific circumstances.\r\n","Thank you. From reading the code in question (wrap.go from the log message), I see they try to protect against superfluous calls to `WriteHeader`, so the bug isn't obvious.","@zeusal do you have the same experience, i.e. that it's non-trivial to reproduce?"],"labels":["kind\/bug","component\/web"]},{"title":"Implement native histograms custom buckets","body":"This is work in progress. Updates are merged to nhcb branch, we're using this PR to run PromBench and eventually merge to main.\r\n\r\nFixes: #13485 ","comments":["\/prombench main","@krajorama is not a org member nor a collaborator and cannot execute benchmarks.","\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13662`**](http:\/\/prombench.prometheus.io\/13662\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13662\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13662\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13662)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213662%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","I don't see any difference.  Also, prombench data doesn't have any native histograms.","Benchmark tests are running for 3 days! If this is intended ignore this message otherwise you can cancel it by commenting: `\/prombench cancel`","\/prombench cancel\r\n","Benchmark cancel is in progress.\n","> I don't see any difference. Also, prombench data doesn't have any native histograms.\r\n\r\nWe made modifications to the engine, was wondering about any regression on the non-experimental part in floats.\r\nAlso my apologies, didn't realize I had to manually cancel. "],"labels":["prombench"]},{"title":"tsdb.IndexReader: Add method PostingsForMatcher","body":"Add method `PostingsForMatcher` to `tsdb.IndexReader`, a method for obtaining a `Postings` iterator over postings matching a matcher, and use it from `tsdb.PostingsForMatchers`. The intention is to optimize regexp matcher paths, especially not having to load all label values before matching on them.\r\n\r\nThis PR is brought over from [mimir-prometheus](https:\/\/github.com\/grafana\/mimir-prometheus\/pull\/586), where the changes have been reviewed in depth by @dimitarvdimitrov and @colega and approved. We (Grafana Labs) came to the conclusion that the changes are best upstreamed directly, instead of going via our fork.\r\n\r\nBenchmarking shows memory reduction up to ~100%, and speedup of up to ~50%.\r\n\r\n**NB**\r\nI move the `Postings` method out of `tsdb.IndexReader` and into a dedicated interface `index.PostingsReader` for technical reasons. Since `tsdb.IndexReader` includes `index.PostingsReader`, it doesn't really change the API.\r\n\r\n#### Benchmarks\r\nBenchmarking stats, label value streaming vs non-streaming fetching of label values, below:\r\n\r\n<details>\r\n<summary>PostingsForMatchers benchmark stats<\/summary>\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/tsdb\r\ncpu: Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz\r\n                                                                      \u2502 main-postings-for-matchers-6.txt \u2502 postings-for-matcher-postings-for-matchers-fast-6.txt \u2502\r\n                                                                      \u2502              sec\/op              \u2502             sec\/op              vs base               \u2502\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\"-48                                                   476.1n \u00b1  2%                     477.4n \u00b1  2%        ~ (p=0.937 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\"-48                                                   466.6n \u00b1  3%                     467.6n \u00b1  2%        ~ (p=0.818 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                           739.0n \u00b1  2%                     754.8n \u00b1  1%   +2.13% (p=0.004 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                           518.6n \u00b1  1%                     530.6n \u00b1  4%        ~ (p=0.180 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                           740.8n \u00b1  2%                     753.7n \u00b1  3%        ~ (p=0.132 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                          718.2n \u00b1  1%                     722.0n \u00b1  3%        ~ (p=0.310 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                            728.2n \u00b1  2%                     762.6n \u00b1  7%        ~ (p=0.065 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                          490.6n \u00b1  2%                     505.0n \u00b1  4%        ~ (p=0.065 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                                5.650m \u00b1  1%                     4.897m \u00b1 11%  -13.32% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"foo|bar\"-48                                            1.008\u00b5 \u00b1  4%                     1.021\u00b5 \u00b1  1%        ~ (p=0.195 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                            884.2n \u00b1  4%                     895.2n \u00b1  2%        ~ (p=0.288 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"X.+\"-48                                                641.9n \u00b1  1%                     643.0n \u00b1  4%        ~ (p=0.699 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                                1.912\u00b5 \u00b1  1%                     1.974\u00b5 \u00b1  3%   +3.24% (p=0.024 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                                2.034\u00b5 \u00b1  1%                     2.058\u00b5 \u00b1  3%        ~ (p=0.240 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                              1.062\u00b5 \u00b1  2%                     1.099\u00b5 \u00b1  7%   +3.48% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                              1.197\u00b5 \u00b1  2%                     1.251\u00b5 \u00b1  2%   +4.47% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*\"-48                                                 73.91m \u00b1  2%                     76.86m \u00b1  2%   +3.99% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1.*\"-48                                                14.24m \u00b1  3%                     13.60m \u00b1  2%   -4.52% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*1\"-48                                                6.350m \u00b1  1%                     4.777m \u00b1  4%  -24.77% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\"-48                                                104.74m \u00b1  1%                     97.75m \u00b1  2%   -6.67% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                       106.45m \u00b1  2%                     97.89m \u00b1  0%   -8.03% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"\"-48                                                   47.14m \u00b1  5%                     47.53m \u00b1  2%        ~ (p=0.394 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i!=\"\"-48                                                   30.36m \u00b1  1%                     30.30m \u00b1  5%        ~ (p=0.937 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                   75.73m \u00b1  3%                     77.04m \u00b1  2%   +1.73% (p=0.015 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                   653.5n \u00b1  0%                     665.7n \u00b1  2%   +1.87% (p=0.041 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                            74.36m \u00b1  2%                     76.37m \u00b1  2%   +2.70% (p=0.015 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                             30.30m \u00b1  2%                     30.14m \u00b1  8%        ~ (p=0.818 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                     30.25m \u00b1  2%                     30.06m \u00b1  2%        ~ (p=0.699 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                    29.86m \u00b1  2%                     29.80m \u00b1  3%        ~ (p=0.937 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                                30.05m \u00b1  1%                     30.17m \u00b1  3%        ~ (p=0.937 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                                1.389\u00b5 \u00b1  2%                     1.418\u00b5 \u00b1  3%   +2.09% (p=0.009 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                                1.561\u00b5 \u00b1  2%                     1.588\u00b5 \u00b1  1%   +1.73% (p=0.006 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                  103.78m \u00b1  2%                     97.90m \u00b1  1%   -5.67% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                  14.18m \u00b1  1%                     13.65m \u00b1  1%   -3.73% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                                74.03m \u00b1  2%                     73.07m \u00b1  4%        ~ (p=0.093 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                           105.66m \u00b1  2%                     97.09m \u00b1  3%   -8.11% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                          121.2m \u00b1  4%                     110.1m \u00b1  1%   -9.18% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                        180.5m \u00b1  1%                     167.5m \u00b1  2%   -7.18% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                        643.7n \u00b1  3%                     683.4n \u00b1  4%   +6.17% (p=0.009 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\"-48                                                  30.21\u00b5 \u00b1  3%                     30.88\u00b5 \u00b1  2%        ~ (p=0.180 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\"-48                                                  527.5n \u00b1  2%                     533.1n \u00b1  2%        ~ (p=0.937 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                          382.2\u00b5 \u00b1  1%                     391.6\u00b5 \u00b1  5%        ~ (p=0.093 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                          583.6n \u00b1  2%                     588.1n \u00b1  2%        ~ (p=1.000 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                          386.7\u00b5 \u00b1  7%                     387.6\u00b5 \u00b1  7%        ~ (p=0.937 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                         389.8\u00b5 \u00b1  5%                     393.8\u00b5 \u00b1  6%        ~ (p=0.240 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                           31.93\u00b5 \u00b1  0%                     31.77\u00b5 \u00b1  4%        ~ (p=0.065 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                         562.4n \u00b1  1%                     558.6n \u00b1  2%        ~ (p=0.416 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                               3.544m \u00b1  4%                     2.909m \u00b1  1%  -17.90% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"foo|bar\"-48                                           864.5\u00b5 \u00b1  8%                     866.9\u00b5 \u00b1  3%        ~ (p=0.818 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                           754.8n \u00b1  1%                     798.3n \u00b1  1%   +5.77% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"X.+\"-48                                              1231.0n \u00b1 27%                     613.3n \u00b1  1%  -50.18% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                               4.981\u00b5 \u00b1  1%                     4.979\u00b5 \u00b1  1%        ~ (p=0.937 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                               879.5\u00b5 \u00b1  6%                     874.2\u00b5 \u00b1  2%        ~ (p=0.589 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                             982.2n \u00b1  1%                    1008.5n \u00b1  4%        ~ (p=0.372 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                             878.7\u00b5 \u00b1  3%                     892.6\u00b5 \u00b1  5%        ~ (p=0.485 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*\"-48                                                65.82m \u00b1  2%                     67.12m \u00b1  4%        ~ (p=0.310 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1.*\"-48                                               11.40m \u00b1  3%                     10.65m \u00b1  4%   -6.53% (p=0.004 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*1\"-48                                               3.034m \u00b1  2%                     2.491m \u00b1  3%  -17.88% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\"-48                                                83.92m \u00b1  6%                     81.39m \u00b1  1%   -3.01% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                       82.53m \u00b1  2%                     80.64m \u00b1  1%   -2.29% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"\"-48                                                  30.85m \u00b1  2%                     30.98m \u00b1  0%        ~ (p=0.310 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i!=\"\"-48                                                  18.68m \u00b1  8%                     18.80m \u00b1  1%        ~ (p=0.589 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                  65.98m \u00b1  4%                     65.87m \u00b1  2%        ~ (p=0.485 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                  719.9n \u00b1  1%                     715.6n \u00b1  1%        ~ (p=0.102 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                           65.38m \u00b1  2%                     65.79m \u00b1  3%        ~ (p=0.937 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                            19.06m \u00b1  1%                     19.16m \u00b1  7%        ~ (p=0.240 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                    19.85m \u00b1  1%                     20.32m \u00b1  2%   +2.35% (p=0.004 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                   18.76m \u00b1  3%                     19.26m \u00b1  2%   +2.66% (p=0.026 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                               18.86m \u00b1  2%                     19.10m \u00b1  3%   +1.24% (p=0.041 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                               31.61\u00b5 \u00b1  4%                     31.93\u00b5 \u00b1  1%        ~ (p=0.394 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                               393.3\u00b5 \u00b1  4%                     402.8\u00b5 \u00b1  4%        ~ (p=0.065 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                  83.81m \u00b1  2%                     82.72m \u00b1  2%        ~ (p=0.310 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                 12.22m \u00b1  4%                     11.46m \u00b1  3%   -6.24% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                               65.82m \u00b1  4%                     63.99m \u00b1  4%   -2.79% (p=0.026 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                           85.41m \u00b1  2%                     81.83m \u00b1  1%   -4.19% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                         99.39m \u00b1  5%                     92.58m \u00b1  1%   -6.86% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                       150.8m \u00b1  6%                     147.8m \u00b1  7%   -1.97% (p=0.041 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                       731.9n \u00b1  4%                     750.5n \u00b1  3%        ~ (p=0.240 n=6)\r\ngeomean                                                                                     424.2\u00b5                           415.4\u00b5         -2.08%\r\n\r\n                                                                      \u2502 main-postings-for-matchers-6.txt \u2502 postings-for-matcher-postings-for-matchers-fast-6.txt \u2502\r\n                                                                      \u2502               B\/op               \u2502            B\/op              vs base                  \u2502\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\"-48                                                     64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\"-48                                                     48.00 \u00b1 0%                    48.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                             176.0 \u00b1 0%                    176.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                             48.00 \u00b1 0%                    48.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                             176.0 \u00b1 0%                    176.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                            176.0 \u00b1 0%                    176.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                              176.0 \u00b1 0%                    176.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                            48.00 \u00b1 0%                    48.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                                1.531Mi \u00b1 0%                  1.531Mi \u00b1 0%    -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"foo|bar\"-48                                              448.0 \u00b1 0%                    448.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                              224.0 \u00b1 0%                    224.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"X.+\"-48                                                  104.0 \u00b1 0%                    104.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                                  848.0 \u00b1 0%                    848.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                                  912.0 \u00b1 0%                    912.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                                344.0 \u00b1 0%                    344.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                                408.0 \u00b1 0%                    408.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*\"-48                                                 1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=1.000 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1.*\"-48                                                3.357Mi \u00b1 0%                  3.185Mi \u00b1 0%    -5.12% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*1\"-48                                                1.531Mi \u00b1 0%                  1.531Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\"-48                                                 20.73Mi \u00b1 0%                  19.20Mi \u00b1 0%    -7.39% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                        20.73Mi \u00b1 0%                  19.20Mi \u00b1 0%    -7.39% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"\"-48                                                   20.73Mi \u00b1 0%                  20.73Mi \u00b1 0%         ~ (p=0.513 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i!=\"\"-48                                                   12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                   1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=1.000 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                     48.00 \u00b1 0%                    48.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                            1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=0.455 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                             12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                     12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                    12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                                12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                                  408.0 \u00b1 0%                    408.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                                  520.0 \u00b1 0%                    520.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                   20.73Mi \u00b1 0%                  19.20Mi \u00b1 0%    -7.39% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                  3.357Mi \u00b1 0%                  3.186Mi \u00b1 0%    -5.12% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                                9.146Mi \u00b1 0%                  8.521Mi \u00b1 0%    -6.84% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                            20.73Mi \u00b1 0%                  19.20Mi \u00b1 0%    -7.39% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                          24.10Mi \u00b1 0%                  22.39Mi \u00b1 0%    -7.07% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                        29.89Mi \u00b1 0%                  27.73Mi \u00b1 0%    -7.21% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                          48.00 \u00b1 0%                    48.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\"-48                                                    80.00 \u00b1 0%                    80.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\"-48                                                    64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                            208.0 \u00b1 0%                    208.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                            64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                            208.0 \u00b1 0%                    208.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                           208.0 \u00b1 0%                    208.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                             208.0 \u00b1 0%                    208.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                           64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                            1605722.00 \u00b1 0%                    88.00 \u00b1 1%   -99.99% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"foo|bar\"-48                                             448.0 \u00b1 0%                    448.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                             192.0 \u00b1 0%                    192.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"X.+\"-48                                               1224.00 \u00b1 0%                    72.00 \u00b1 0%   -94.12% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                                 720.0 \u00b1 0%                    720.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                                 800.0 \u00b1 0%                    800.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                               296.0 \u00b1 0%                    296.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                               376.0 \u00b1 0%                    376.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*\"-48                                                1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=0.333 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1.*\"-48                                               3.357Mi \u00b1 0%                  1.654Mi \u00b1 0%   -50.73% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*1\"-48                                            1605696.00 \u00b1 0%                    64.00 \u00b1 0%  -100.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\"-48                                                20.73Mi \u00b1 0%                  17.67Mi \u00b1 0%   -14.77% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                       20.74Mi \u00b1 0%                  17.67Mi \u00b1 0%   -14.77% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"\"-48                                                  20.73Mi \u00b1 0%                  20.73Mi \u00b1 0%         ~ (p=0.870 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i!=\"\"-48                                                  12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                  1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=0.636 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                    64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                           1.532Mi \u00b1 0%                  1.532Mi \u00b1 0%         ~ (p=0.182 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                            12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                    12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                   12.23Mi \u00b1 0%                  12.22Mi \u00b1 0%    -0.01% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                               12.22Mi \u00b1 0%                  12.22Mi \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                                 376.0 \u00b1 0%                    376.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                                 504.0 \u00b1 0%                    504.0 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                  20.73Mi \u00b1 0%                  17.67Mi \u00b1 0%   -14.77% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                 3.357Mi \u00b1 0%                  1.654Mi \u00b1 0%   -50.73% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                               9.146Mi \u00b1 0%                  6.989Mi \u00b1 0%   -23.58% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                           20.73Mi \u00b1 0%                  17.67Mi \u00b1 0%   -14.77% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                         24.10Mi \u00b1 0%                  19.33Mi \u00b1 0%   -19.78% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                       29.89Mi \u00b1 0%                  24.67Mi \u00b1 0%   -17.46% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                         64.00 \u00b1 0%                    64.00 \u00b1 0%         ~ (p=1.000 n=6) \u00b9\r\ngeomean                                                                                     44.46Ki                       31.80Ki        -28.48%\r\n\u00b9 all samples are equal\r\n\r\n                                                                      \u2502 main-postings-for-matchers-6.txt \u2502 postings-for-matcher-postings-for-matchers-fast-6.txt \u2502\r\n                                                                      \u2502            allocs\/op             \u2502          allocs\/op            vs base                 \u2502\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\"-48                                                     3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\"-48                                                     3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                             7.000 \u00b1 0%                     7.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                             3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                             7.000 \u00b1 0%                     7.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                            7.000 \u00b1 0%                     7.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                              7.000 \u00b1 0%                     7.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                            3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                                  5.000 \u00b1 0%                     5.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"foo|bar\"-48                                              13.00 \u00b1 0%                     13.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                              11.00 \u00b1 0%                     11.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/j=~\"X.+\"-48                                                  5.000 \u00b1 0%                     5.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                                  31.00 \u00b1 0%                     31.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                                  34.00 \u00b1 0%                     34.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                                15.00 \u00b1 0%                     15.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                                18.00 \u00b1 0%                     18.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*\"-48                                                   6.000 \u00b1 0%                     6.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"1.*\"-48                                                 11.14k \u00b1 0%                    11.13k \u00b1 0%   -0.01% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".*1\"-48                                                  4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\"-48                                                  100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                         100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/i=~\"\"-48                                                    100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/i!=\"\"-48                                                    100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                     10.00 \u00b1 0%                     10.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                     3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                              14.00 \u00b1 0%                     14.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                              100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                      100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                     100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                                 100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                                  18.00 \u00b1 0%                     18.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                                  22.00 \u00b1 0%                     22.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                    100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                   11.14k \u00b1 0%                    11.14k \u00b1 0%   -0.01% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                                 40.99k \u00b1 0%                    40.99k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                             100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                           111.2k \u00b1 0%                    111.2k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                         141.1k \u00b1 0%                    141.1k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                          3.000 \u00b1 0%                     3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\"-48                                                    4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\"-48                                                    4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j=\"foo\"-48                                            9.000 \u00b1 0%                     9.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j=\"foo\"-48                                            4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=\"foo\",n=\"1\"-48                                            9.000 \u00b1 0%                     9.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",j!=\"foo\"-48                                           9.000 \u00b1 0%                     9.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"2\"-48                                             9.000 \u00b1 0%                     9.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",j!=\"foo\"-48                                           4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1[0-9]\",j=~\"foo|bar\"-48                                 5.000 \u00b1 0%                     4.000 \u00b1 0%  -20.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"foo|bar\"-48                                             13.00 \u00b1 0%                     13.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"XXX|YYY\"-48                                             9.000 \u00b1 0%                     9.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/j=~\"X.+\"-48                                                 5.000 \u00b1 0%                     4.000 \u00b1 0%  -20.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"(1|2|3|4|5|6|20|55)\"-48                                 23.00 \u00b1 0%                     23.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"(1|2|3|4|5|6|20|55)\"-48                                 27.00 \u00b1 0%                     27.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"X|Y|Z\"-48                                               12.00 \u00b1 0%                     12.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i!~\"X|Y|Z\"-48                                               16.00 \u00b1 0%                     16.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*\"-48                                                  7.000 \u00b1 0%                     7.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"1.*\"-48                                                11.14k \u00b1 0%                    11.13k \u00b1 0%   -0.02% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".*1\"-48                                                 4.000 \u00b1 0%                     3.000 \u00b1 0%  -25.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\"-48                                                 100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\".+\",j=~\"X.+\"-48                                        100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/i=~\"\"-48                                                   100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/i!=\"\"-48                                                   100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",j=\"foo\"-48                                    12.00 \u00b1 0%                     12.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".*\",j=\"foo\"-48                                    4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*\",i!=\"2\",j=\"foo\"-48                             17.00 \u00b1 0%                     17.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\"-48                                             100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=\"foo\"-48                                     100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"X.+\"-48                                    100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!=\"\",j=~\"XXX|YYY\"-48                                100.0k \u00b1 0%                    100.0k \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"X|Y|Z\",j=\"foo\"-48                                 16.00 \u00b1 0%                     16.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i!~\"X|Y|Z\",j=\"foo\"-48                                 21.00 \u00b1 0%                     21.00 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",j=\"foo\"-48                                   100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\"1.+\",j=\"foo\"-48                                  11.14k \u00b1 0%                    11.14k \u00b1 0%   -0.02% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".*1.*\",j=\"foo\"-48                                40.99k \u00b1 0%                    40.99k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!=\"2\",j=\"foo\"-48                            100.0k \u00b1 0%                    100.0k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\"2.*\",j=\"foo\"-48                          111.2k \u00b1 0%                    111.2k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"1\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                        141.1k \u00b1 0%                    141.1k \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/PostingsForMatchers\/n=\"X\",i=~\".+\",i!~\".*2.*\",j=\"foo\"-48                         4.000 \u00b1 0%                     4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\ngeomean                                                                                       268.3                          265.8        -0.94%\r\n\u00b9 all samples are equal\r\n```\r\n<\/details>\r\n\r\n<details>\r\n<summary>labelValuesWithMatchers benchmark stats<\/summary>\r\n\r\n```console\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/tsdb\r\ncpu: Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GHz\r\n                                                                         \u2502 main-labelvalues-6.txt \u2502 postings-for-matcher-fast-labelvalues-6.txt \u2502\r\n                                                                         \u2502         sec\/op         \u2502        sec\/op          vs base              \u2502\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1\"-48                                 1.905m \u00b1  3%             1.913m \u00b1 2%       ~ (p=0.394 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\"-48                                 174.1m \u00b1  2%             174.0m \u00b1 1%       ~ (p=0.818 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                              118.5m \u00b1  2%             119.8m \u00b1 3%       ~ (p=0.240 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                        201.7m \u00b1  4%             197.3m \u00b1 2%       ~ (p=0.093 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                        36.80m \u00b1  1%             35.94m \u00b1 2%  -2.32% (p=0.009 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                    36.78m \u00b1  2%             35.97m \u00b1 1%  -2.20% (p=0.026 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                        136.1m \u00b1  2%             138.4m \u00b1 1%       ~ (p=0.065 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48              352.4m \u00b1  2%             348.1m \u00b1 2%       ~ (p=0.240 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                        1.913m \u00b1  2%             1.901m \u00b1 1%       ~ (p=0.240 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                              12.04m \u00b1  1%             11.90m \u00b1 2%       ~ (p=0.180 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                   18.26m \u00b1  1%             18.23m \u00b1 0%       ~ (p=0.589 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                              218.9m \u00b1  4%             226.2m \u00b1 1%       ~ (p=0.065 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=\"1\"-48                                 8.238\u00b5 \u00b1  2%             8.075\u00b5 \u00b1 3%  -1.98% (p=0.026 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                              28.91m \u00b1 10%             28.41m \u00b1 7%       ~ (p=0.394 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                             270.9n \u00b1  2%             272.9n \u00b1 2%       ~ (p=0.485 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1\"-48                                2.357m \u00b1  2%             2.352m \u00b1 2%       ~ (p=0.937 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\"-48                                176.2m \u00b1  4%             176.9m \u00b1 3%       ~ (p=0.589 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                             484.8m \u00b1  2%             490.4m \u00b1 4%       ~ (p=0.589 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                       192.5m \u00b1  4%             190.9m \u00b1 2%       ~ (p=0.394 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                       77.16m \u00b1  1%             77.43m \u00b1 3%       ~ (p=0.485 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                   76.91m \u00b1  1%             77.30m \u00b1 2%       ~ (p=0.394 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                       137.4m \u00b1  2%             135.6m \u00b1 1%  -1.31% (p=0.015 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48             314.7m \u00b1  1%             323.9m \u00b1 1%  +2.92% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                       2.363m \u00b1  1%             2.342m \u00b1 1%       ~ (p=0.394 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                             9.473m \u00b1  1%             9.488m \u00b1 3%       ~ (p=0.589 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                  15.48m \u00b1  4%             15.45m \u00b1 1%       ~ (p=0.818 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                             238.1m \u00b1  3%             240.4m \u00b1 5%       ~ (p=0.093 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=\"1\"-48                                904.3\u00b5 \u00b1  5%             940.7\u00b5 \u00b1 6%  +4.03% (p=0.026 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                             12.80m \u00b1  2%             12.43m \u00b1 3%  -2.90% (p=0.026 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                            246.2n \u00b1  3%             241.8n \u00b1 2%       ~ (p=0.093 n=6)\r\ngeomean                                                                              13.44m                   13.42m       -0.15%\r\n\r\n                                                                         \u2502 main-labelvalues-6.txt \u2502 postings-for-matcher-fast-labelvalues-6.txt \u2502\r\n                                                                         \u2502          B\/op          \u2502        B\/op         vs base                 \u2502\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1\"-48                               1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\"-48                               16.14Mi \u00b1 0%           16.14Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                            16.15Mi \u00b1 0%           16.14Mi \u00b1 0%   -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                      16.14Mi \u00b1 0%           16.14Mi \u00b1 0%        ~ (p=0.455 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                      10.70Mi \u00b1 0%           10.70Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                  10.70Mi \u00b1 0%           10.70Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                      16.14Mi \u00b1 0%           16.14Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48            17.67Mi \u00b1 0%           17.67Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                      1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                            1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=0.080 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                 1.532Mi \u00b1 0%           1.532Mi \u00b1 0%        ~ (p=0.641 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                            5.782Ki \u00b1 0%           5.782Ki \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=\"1\"-48                               4.695Ki \u00b1 0%           4.695Ki \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                            3.363Mi \u00b1 0%           3.192Mi \u00b1 0%   -5.11% (p=0.002 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                             0.000 \u00b1 0%             0.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1\"-48                              1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\"-48                              17.66Mi \u00b1 0%           17.66Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                           17.67Mi \u00b1 0%           17.67Mi \u00b1 0%   -0.01% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                     17.66Mi \u00b1 0%           17.66Mi \u00b1 0%        ~ (p=1.000 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                     12.22Mi \u00b1 0%           12.22Mi \u00b1 0%   -0.01% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                 12.22Mi \u00b1 0%           12.22Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                     17.66Mi \u00b1 0%           17.66Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48           19.20Mi \u00b1 0%           19.20Mi \u00b1 0%        ~ (p=1.000 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                     1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                           1.531Mi \u00b1 0%           1.531Mi \u00b1 0%        ~ (p=0.102 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                1.532Mi \u00b1 0%           1.532Mi \u00b1 0%        ~ (p=0.913 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                           7.501Ki \u00b1 0%           7.501Ki \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=\"1\"-48                              6.398Ki \u00b1 0%           6.398Ki \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                           3.365Mi \u00b1 0%           1.662Mi \u00b1 0%  -50.61% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                            0.000 \u00b1 0%             0.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\ngeomean                                                                                         \u00b2                        -2.50%               \u00b2\r\n\u00b9 all samples are equal\r\n\u00b2 summaries must be >0 to compute geomean\r\n\r\n                                                                         \u2502 main-labelvalues-6.txt \u2502 postings-for-matcher-fast-labelvalues-6.txt \u2502\r\n                                                                         \u2502       allocs\/op        \u2502      allocs\/op       vs base                \u2502\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1\"-48                                 1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\"-48                                200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                             200.1k \u00b1 0%             200.1k \u00b1 0%  -0.00% (p=0.002 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                       200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                       200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                   200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                       200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48             200.0k \u00b1 0%             200.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                        1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                              1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                   1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                              98.00 \u00b1 0%              98.00 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=\"1\"-48                                 87.00 \u00b1 0%              87.00 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Head\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                             11.23k \u00b1 0%             11.23k \u00b1 0%  -0.01% (p=0.002 n=6)\r\nQuerier\/Head\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                             0.000 \u00b1 0%              0.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1\"-48                                1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\"-48                               300.0k \u00b1 0%             300.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"^.+$\"-48                            300.1k \u00b1 0%             300.1k \u00b1 0%  -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j!=\"foo\"-48                      300.0k \u00b1 0%             300.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"X.+\"-48                      300.0k \u00b1 0%             300.0k \u00b1 0%  -0.00% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",j=~\"XXX|YYY\"-48                  300.0k \u00b1 0%             300.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"X\",j!=\"foo\"-48                      300.0k \u00b1 0%             300.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_n=\"1\",i=~\"^.*$\",j!=\"foo\"-48            300.0k \u00b1 0%             300.0k \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=\"1aaa...ddd\"-48                       1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\"-48                             1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/i_with_i=~\"1.+\",i=~\".+ddd\"-48                  1.000 \u00b1 0%              1.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_j!=\"foo\"-48                             140.0 \u00b1 0%              140.0 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=\"1\"-48                                128.0 \u00b1 0%              128.0 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\nQuerier\/Block\/labelValuesWithMatchers\/n_with_i=~\"1.+\"-48                            11.27k \u00b1 0%             11.27k \u00b1 0%  -0.02% (p=0.002 n=6)\r\nQuerier\/Block\/labelValuesWithMatchers\/none_with_i=~\"1\"-48                            0.000 \u00b1 0%              0.000 \u00b1 0%       ~ (p=1.000 n=6) \u00b9\r\ngeomean                                                                                         \u00b2                        -0.00%               \u00b2\r\n\u00b9 all samples are equal\r\n\u00b2 summaries must be >0 to compute geomean\r\n```\r\n<\/details>\r\n","comments":["@bwplotka @fpetkovski I hear you might want to review this?","\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13620`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13620\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13620)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213620%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","> I assume given you changed querier's postingForMatcher to used new index method, we should see improvement (or at least no regression, right?)\r\n\r\n@bwplotka there should only be improvement (for regexp matchers (as this should be the affected code path if I remember correctly, although not 100% sure), no longer having to load all label values before matching; if we see regressions I will be surprised. If you see the benchmark stats I included in the PR description, you can see that memory usage drops for one regexp matcher case for both head and block. In mimir-prometheus, there's also a second regexp matcher case that benefits from reduced memory. The stats also show some speedup, but I find that this changes between every time I bench (whereas memory reduction is constant) so I don't trust it.","I pushed a minor change to `TestMemPostings_PostingsForMatcher`, to make it order the series consistently with `TestReader_PostingsForMatcher`. Forgot to do this earlier.","I don't have time to take a deeper look, but generally main looks better than this PR on prombench, unfortunately. Not significantly, but there is some diff:\r\n\r\nCPU is 6% worse with this PR:\r\n\r\n<img width=\"2045\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/6950331\/6e156079-f34f-48ab-bcc0-70a49dd32aac\">\r\n\r\nPlus queries are a bit slower on HTTP query_range (also engine avg):\r\n\r\n<img width=\"2039\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/6950331\/b755989f-8fa2-436a-87e3-b8e7ee8f347a\">\r\n\r\nhttp:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13620 \r\n\r\nAny idea why? cc @bboreham \r\n\r\n\r\n","Benchmark tests are running for 3 days! If this is intended ignore this message otherwise you can cancel it by commenting: `\/prombench cancel`","> Any idea why?\r\n\r\n@bwplotka not really, but I'm also unfamiliar with prombench. Do you have any suggestions for how I might analyze the prombench results, to see e.g. what kind of queries are slower with my PR? Is it possible that calling `ix.PostingsForMatcher` (being an interface method) might be slightly slower than calling the free function `postingsForMatcher`, when one of its fast paths is hit?","@bwplotka I just merged in some changes from main and pushed BTW. Among the changes coming in from main was a change from compress\/gzip to github.com\/klauspost\/compress\/gzip in `util\/httputil`. I wonder if this type of change could make main perform better. Does prombench compare against the latest main, or the main revision that my PR is based on?","\/prombench cancel\r\n","@bwplotka @fpetkovski @bboreham after running `BenchmarkQuerier\/Head\/PostingsForMatchers`  benchmarks on AMD64, I realized there is a performance regression (which doesn't appear on Apple ARM64, which I tried first). What happens is that regular expression label matching (actually calling `strings.Suffix` under the hood), while iterating over the `p.m[m.Name]` map, is slower (on AMD64) than doing the same while iterating over a slice of label values. I don't know why it would matter in practice whether you first make the slice iterating over the map or match while you iterate over the map. I tried benchmarking this particular execution path multiple times, and matching during map iteration was always slower than first making a slice and then matching over that.\r\n\r\nI've tried solving the regression two ways:\r\n1. First make a slice of label values, for fast matching during iteration over slice\r\n1. Convert `MemPostings` to use [SwissMap](https:\/\/github.com\/dolthub\/swiss) for the label value -> series refs map\r\n\r\nThe SwissMap solution is the fastest and uses the least memory, you can see my (AMD64) benchmark results in this [Gist](https:\/\/gist.github.com\/aknuds1\/3ecd314dd889530ec7eeccc4eb34801c) (one file for each solution).\r\n\r\nEdit: I see now there's also a CockroachDB version of SwissMap,which I think is newer?\r\n\r\n**NB:** I've drafted a [PR](https:\/\/github.com\/prometheus\/prometheus\/pull\/13644) for using dolthub SwissMap in `MemPostings` and [one](https:\/\/github.com\/prometheus\/prometheus\/pull\/13646) for the CockroachDB equivalent, so @bboreham can prombench them.","\/prombench cancel\r\n","Benchmark cancel is in progress.\n","@bwplotka @fpetkovski @bboreham after giving it some thought, I decided to go with the first way of solving the performance regression (i.e., copy label values to a slice before matching on them), since it's simple and it just means we won't save as much memory versus the main branch. I've pasted in new benchmark stats from an AMD64 architecture machine, and they're looking all good to me. PTAL.","\/prombench main\r\n","\/prombench main\r\n","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13620`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13620\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13620)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213620%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13620`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13620\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13620\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13620)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213620%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","Thanks for letting me know @jesusvazquez .. This looks amazing!!\r\n\r\n\r\nJust as a side note, I was looking at the parca profiles from the prombench and seems that most (if not all) of the queries are only hitting the head block?\r\n\r\n![Screenshot 2024-02-29 at 4 33 27\u202fPM](https:\/\/github.com\/prometheus\/prometheus\/assets\/4027760\/c7854221-9b3f-42c1-955a-276d7ff90861)\r\n\r\nThis seems to be indeed the case looking at the [prombench config](https:\/\/github.com\/prometheus\/test-infra\/blob\/98e6f9e1376b0731e44fa90b719cb2bb10639a6c\/prombench\/manifests\/prombench\/benchmark\/6_loadgen.yaml#L10-L50)? Would make sense to change the prombench to have longer queries (last 6 hours instead of last 2\/3 hours) to see the impact on the [`File(Index)Reader`](https:\/\/github.com\/prometheus\/prometheus\/pull\/13620\/files#diff-fb0f63e7d2d24fa7aa1c18b2619b10a07c821b38f45c2351458b00edef63f995R1779) as well?\r\n\r\ncc @bboreham Is this the case? and WDYT, should we also create queries that query the already compacted blocks? \r\n\r\n\r\n","@bboreham I refactored `TestReader_PostingsForMatcher` and `TestMemPostings_PostingsForMatcher`, as you requested. Their common test suite is now in a shared function. PTAL.","\/prombench cancel\r\n","Benchmark cancel is in progress.\n","Prombench is not showing any material difference; sometimes one is on top and sometimes the other.\r\n\r\n<img width=\"1315\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/8125524\/64b59f46-5b38-42ab-8223-76361f5320a7\">\r\n\r\n\r\n--\r\n\r\n> Would make sense to change the prombench to have longer queries (last 6 hours instead of last 2\/3 hours) to see the impact on the File(Index)Reader as well?\r\n\r\nYes, I've mentioned that before.  The downside is, if we have to wait for 6 hours of data to populate, it's a very slow cycle.\r\n\r\nStill, this PR will need work somewhere to motivate adding 500+ lines of code to Prometheus.","> Still, this PR will need work somewhere to motivate adding 500+ lines of code to Prometheus.\r\n\r\nbut it may have improvements when querying the blocks (instead of the head block) and regex queries (both cases not covered by prombench) , we are just not seeing it, right ? ","> but it may have improvements when querying the blocks (instead of the head block) and regex queries (both cases not covered by prombench) , we are just not seeing it, right ?\r\n\r\nI'm thinking of trying these changes in Mimir next week, seeing if I can exercise the paths showing improvements in benchmarks.","> Would make sense to change the prombench to have longer queries (last 6 hours instead of last 2\/3 hours) to see the impact on the File(Index)Reader as well?\r\n\r\n> Yes, I've mentioned that before. The downside is, if we have to wait for 6 hours of data to populate, it's a very slow cycle.\r\n\r\ncould the interval of queries be a parameter (off by default) of prombench","> Yes, I've mentioned that before. The downside is, if we have to wait for 6 hours of data to populate, it's a very slow cycle.\r\n\r\nI'm not familiar with prombench, but would it be an option to start prombench with an well known block (like 2020-01-01) and run queries for that date aswell? ","I ran a load test on Grafana Mimir (vendoring Prometheus) with and without my optimizations, issuing a range query including a single regexp matcher on a label with ~59900 different values. The regexp matcher matches _none_ of the label values (I picked this case as the best performing for blocks in the benchmarks).\r\n\r\nThe Mimir version without my changes managed 100.20 requests\/second, about 3.3% of the requests failing. The Mimir version _with_ my changes managed 187.32 requests\/second, with zero failures.\r\n\r\n* Load test results from unoptimized Mimir: [unoptimized-load-test.txt](https:\/\/github.com\/prometheus\/prometheus\/files\/14484304\/unoptimized-load-test.txt)\r\n* Load test results from optimized Mimir: [optimized-load-test.txt](https:\/\/github.com\/prometheus\/prometheus\/files\/14484305\/optimized-load-test.txt)\r\n\r\n~~It's also visible from CPU utilization metrics that the Mimir store-gateway (\"mimir-backend\" in the legend) was much less stressed during the load test of the optimized Mimir version (second screenshot below, the ingester's CPU utilization didn't differ so much)~~ In retrospect I'm not so sure whether the mimir-backend component's CPU utilization is actually directly tied to the query, please disregard.\r\n\r\n![unoptimized](https:\/\/github.com\/prometheus\/prometheus\/assets\/281303\/a4c36d10-ab42-45a3-ad8e-34f462eceac6)\r\n![optimized](https:\/\/github.com\/prometheus\/prometheus\/assets\/281303\/68c24c0f-335c-470e-83c3-744fa15ec6e5)\r\n\r\n**NB**: I plan to follow up with a corresponding test w\/ pure Prometheus, when I have the time. It was quicker for me to test with one of our Mimir instances :)"],"labels":["prombench"]},{"title":"New Prometheus UI","body":"### Proposal\r\n\r\nAs agreed when we decided what was required for the Prometheus 3.0, we want a new UI.\r\n\r\n### What does it mean\r\n\r\n- Functionalities and pages remain the same \r\n- We want to rework each them to have a better display and potentially fix some performance issues. That's specially the case for the pages Alert \/ Rules that we know are frozen when there are too many elements to display even with the current infinite scrolling.\r\n- We also want to add at least two new functionalities coming from PromLens:\r\n   - A concret and smooth metrics finder\r\n   - The nice PromQL debugger\r\n- Specifically for the Alert \/ Rules page we want to use the PromQL highlighter.\r\n- In order to have a new look and feel, we will use [mantine](https:\/\/mantine.dev\/). This decision will impact other UIs like AlertManager.\r\n\r\n### How we will start\r\n\r\nWe are going to work on a separate branch. We will remove immediately the previous UI and start on the fresh one from scratch.\r\nWe will merge the branch on `main` once every previous functionalities are covered.","comments":["Hey, I'm looking to contribute to prometheus. Can I help out with this? I'm familiar with react and mantine","@Maniktherana Gladly! I'm currently working on an initial sketch of the new Mantine-based UI that is coming along nicely. I'll share it as code in the next week(s), and then we can more easily find specific parts to collaborate on - whether those are specific features, design improvements, or code cleanups. Let me know in case you have any specific interests in mind already!"],"labels":["component\/ui"]},{"title":"PromQL\/storage: Require selectors to always return matching results","body":"### Proposal\n\nProbably very surprising for most, there is no requirement that the storage returns matching time series for a given selector. This apparently unlocks certain more or less valid use cases, mostly around remote-read. There is a lengthy discussion in #8053. However, this behavior also prevents a lot of optimizations, and there are implications for the planned migration to UTF-8 capable names. (See [the relevant design doc](https:\/\/github.com\/prometheus\/proposals\/blob\/main\/proposals\/2023-11-13-utf8-migration.md) for starters, but there are implications about rewriting the results that aren't yet reflected in the design doc.)\r\n\r\nMy proposal here is to revisit the assumption that there are valid and useful use cases for selectors returning a result that doesn't match the selector. Almost four years later, we should have a pretty good picture if that is still and actually the case. The Prometheus 3 release is a good time to act on the result:\r\n- If those use cases have turned out to not be very useful after all, we can just mandate that selectors only return matching time series and unlock optimizations etc.\r\n- If those use cases have some use, but are only relevant for a very small fraction of users, we might be able to contain them behind flags or something so that we can still optimize things for the majority of users.\r\n- \"Worst case\" would be that we just have to live with the possibility that selectors return anything.\r\n\r\nI will also submit this to the dev summit because I think it warrants a broader discussion.\r\n\r\nPaging @ywwg @GiedriusS @fpetkovski @charleskorn @bwplotka as this might be relevant for you.","comments":["We have discussed this at the dev-summit. The consensus reached there is the following:\r\n\r\n> Starting with Prometheus 3.0, any querier implementation MUST return something that matches the selectors sent in the request, including remote-read. We are open to adding a flag to optionally reinstate the old expectations should this be required by real-world use cases.\r\n\r\nNote that a dev-summit consensus is not a formal vote and still subject to lazy consensus among the maintainers. If anyone has objections, please add them here for discussion.","In more detail: The idea is that we can do optimizations relying on selectors returning matching results starting from v3.0.0. Should relevant use cases show up that are now broken by this expectation, we can introduce some way for storage implementations (presumably that's affecting mostly remote-read) to communicate that they are not behaving in that way and that their results cannot be subject of these optimizations.","@beorn7 I was looking back at #8053 and thinking about implementing it. Is it unnecessary now that 3.0 is coming or is it still worth adding for the time being?","@LeviHarrison : My understanding is that the mandate we have agreed upon now _unlocks_ what has been proposed in #8053. So v3.0 makes it possible to work on it, while it was impossible previously.\r\nHowever, @bboreham had some plans for farther reaching optimizations. Maybe chat with him first.","#8053 would have allowed this optimization be performed optionally (via a config param) for certain remote backends, so I think it would be compatible with v2.0. However now that we're going to do it for all remote write backends by default in v3.0, I was wondering if it was worth implementing the conditional functionality described in #8053.","To clarify: The optimization described in #8053 hasn't been implemented at all. In the discussion, the concern was brought up that this optimization, should we implement it, would break with remote-read backends that return series that do not match the selectors in the request. So it was concluded that we needed to not apply the optimization for those backends.\r\n\r\nThis issue here intends to make it part of the contract that any storage implementation MUST return series that match the selectors in the request. With that, we can just implement the optimization without worrying about any condition where we cannot apply it. It's first a matter of simplification and second guarantees that we don't under-use the optimization (e.g. by never using it as soon as remote-read is part of the game).\r\n\r\nFurther, this issue also intends to acknowledge that we can _still_ add an optional deactivation of the optimization, should it actually be needed, without creating a breaking change. (But the assumption is that it won't really be needed, so we go down the simple path.)"],"labels":["component\/remote storage","component\/promql","component\/tsdb","kind\/change"]},{"title":"scrape: Add more tests validating HTTP requests made on scrape (and their headers)","body":"### Proposal\n\nThis is to ensure bug like in https:\/\/github.com\/prometheus\/prometheus\/issues\/13401#issuecomment-1931903806 has less chance to be made in the next Prometheus versions.\r\n\r\nIdea by @hacklschorsch\r\n\r\n> Edit: Add something like https:\/\/pypi.org\/project\/httplint\/ maybe?","comments":["I'm sure I ran `2.49.0` and prometheus had no problem scraping itself. Maybe the content negotiation implementation used by golang client is broken\/doesn't follow the RFC? (accepts `q>1`?).\r\n\r\nIt can also be useful to have some e2e tests where we scrape real targets (not just `httptest`'s where a lot of stuff is mocked) of different prom clients...\r\n\r\nEDIT:\r\n\r\nYep, the implementation doesn't seem to check the quality value: https:\/\/github.com\/prometheus\/common\/blob\/773d5664eb8d228d2c6ba235b1af12fbf7ee9020\/internal\/bitbucket.org\/ww\/goautoneg\/autoneg.go#L84 ","> I'm sure I ran 2.49.0 and prometheus had no problem scraping itself. \r\n\r\nYes, Prometheus seems to be rather lenient on what requests it accepts.\r\nWe also were still successfully scraping node-exporter, but not our own service's `\/metrics` endpoint.","Yep, all targets based on the golang client should have a \"broken\" content negotiation implementation, we'll probably need to fix [that](https:\/\/github.com\/prometheus\/common\/blob\/773d5664eb8d228d2c6ba235b1af12fbf7ee9020\/internal\/bitbucket.org\/ww\/goautoneg\/autoneg.go#L84) or move to another library: https:\/\/github.com\/elnormous\/contenttype seems to better stick to the RFC and seems to be better tested.\r\n\r\n(having a standard implementation will be even better https:\/\/github.com\/golang\/go\/issues\/19307)"],"labels":["help wanted"]},{"title":"Create Multiples `fPointPool` to hold slices with with distinct capacities.","body":"This is just a Draft.\r\n\r\nCreating multiples pools to hold the FPoinSlices of different capacities. \r\n\r\n@bboreham can we run prombench to see if this make sense?\r\n","comments":["\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13549`**](http:\/\/prombench.prometheus.io\/13549\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13549\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13549\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13549)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213549%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","There is already a bucketed pool [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/c954cd9d1d4e3530be2939d39d8633c38b70913f\/util\/pool\/pool.go#L22-L24), used in scraping.\r\n\r\nNote I did some experiments with that pool, including taking out the `interface{}` at #12972.\r\nIf you do decide to use it for `fPoint` it could be made into a proper Go generic type.","From Prombench this PR is doing more allocations with a bigger heap, however CPU is lower.  This is confusing.\r\n\r\nIf it wasn't branched off latest main I suggest you rebase and we can re-run.","> From Prombench this PR is doing more allocations with a bigger heap, however CPU is lower. This is confusing.\r\n\r\nMore allocations+heap are to be expected since we store more buckets of pools now? We mandatorily assign only from a particular index, potentially creating a new slice even when there might be available slices in pools at other indices...","There's no value in guessing; we have tools such as profilers to look at the detail.\r\n\r\nHowever, it's a short PR: there is no mechanism by which it can reduce CPU usage, unless by reducing memory allocation.  Since the opposite happened, it's confusing.",">However, it's a short PR: there is no mechanism by which it can reduce CPU usage, unless by reducing memory allocation. Since the opposite happened, it's confusing.\r\n\r\nYeah.. i think this lower CPU its not relevant.. maybe it will go away on the next run\r\n\r\nAnother possibility that may be happening is, after https:\/\/github.com\/prometheus\/prometheus\/pull\/13448 we may allocate a big slice that is then split in multiples smaller ones and added back on the smaller bucket. When we run the same query again we pick from the bigger bucket that is empty.\r\n\r\n>If it wasn't branched off latest main I suggest you rebase and we can re-run.\r\n\r\nIts from main.\r\n\r\nI think for now this seems to not be worth it! ","Benchmark tests are running for 3 days! If this is intended ignore this message otherwise you can cancel it by commenting: `\/prombench cancel`","\/prombench cancel","Benchmark cancel is in progress.\n"],"labels":["prombench"]},{"title":"build(deps): bump reactstrap from 8.10.1 to 9.2.2 in \/web\/ui","body":"Bumps [reactstrap](https:\/\/github.com\/reactstrap\/reactstrap) from 8.10.1 to 9.2.2.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/releases\">reactstrap's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>v9.2.2<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.2.1...v9.2.2\">9.2.2<\/a> (2024-01-19)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li>dropdown with inNavbar bug (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/18cc094afa8d4e95785d823eb735e06c28547d19\">18cc094<\/a>)<\/li>\n<\/ul>\n<h2>v9.2.1<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.2.0...v9.2.1\">9.2.1<\/a> (2023-10-05)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/1680\">#1680<\/a>:<\/strong> arrowClass not applied to arrow (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2772\">#2772<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/aeaf180701511377ab6a00830ccac4fb96e84028\">aeaf180<\/a>)<\/li>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2755\">#2755<\/a>:<\/strong> make offcanvas example code readable (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2768\">#2768<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/b932582798911ebd57ad6735baf9f4fa21d8af7a\">b932582<\/a>)<\/li>\n<\/ul>\n<h2>v9.2.0<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.10...v9.2.0\">9.2.0<\/a> (2023-06-09)<\/h2>\n<h3>Features<\/h3>\n<ul>\n<li><strong>modal:<\/strong> add 'aria-modal=&quot;true&quot;' to modal (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/2a43591f6d6f2fd6e1e5f7f3bd65caf9efd7f7ab\">2a43591<\/a>)<\/li>\n<\/ul>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2662\">#2662<\/a>:<\/strong> remove many instances of default props (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2752\">#2752<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/b7d571c777de6169de01da5cc14f2eb28988071a\">b7d571c<\/a>)<\/li>\n<\/ul>\n<h2>v9.1.10<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.9...v9.1.10\">9.1.10<\/a> (2023-05-13)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2427\">#2427<\/a>:<\/strong> error building esm files (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2748\">#2748<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/04d571dcaa630ccbc8b8363ad06dad436f11780e\">04d571d<\/a>)<\/li>\n<li>properly pass in <code>isOpen<\/code> in modal test (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2745\">#2745<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/8cb051522ddfad48c08651604744caea25b5e68e\">8cb0515<\/a>)<\/li>\n<\/ul>\n<h2>v9.1.9<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.8...v9.1.9\">9.1.9<\/a> (2023-04-11)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2730\">#2730<\/a>:<\/strong> import error (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2733\">#2733<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/2547f3acfd12299dc0f4e2a5863c5cf82a203ef4\">2547f3a<\/a>)<\/li>\n<\/ul>\n<h2>v9.1.8<\/h2>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.7...v9.1.8\">9.1.8<\/a> (2023-03-29)<\/h2>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/blob\/master\/CHANGELOG.md\">reactstrap's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.2.1...v9.2.2\">9.2.2<\/a> (2024-01-19)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li>dropdown with inNavbar bug (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/18cc094afa8d4e95785d823eb735e06c28547d19\">18cc094<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.2.0...v9.2.1\">9.2.1<\/a> (2023-10-05)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/1680\">#1680<\/a>:<\/strong> arrowClass not applied to arrow (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2772\">#2772<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/aeaf180701511377ab6a00830ccac4fb96e84028\">aeaf180<\/a>)<\/li>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2755\">#2755<\/a>:<\/strong> make offcanvas example code readable (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2768\">#2768<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/b932582798911ebd57ad6735baf9f4fa21d8af7a\">b932582<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.10...v9.2.0\">9.2.0<\/a> (2023-06-09)<\/h2>\n<h3>Features<\/h3>\n<ul>\n<li><strong>modal:<\/strong> add 'aria-modal=&quot;true&quot;' to modal (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/2a43591f6d6f2fd6e1e5f7f3bd65caf9efd7f7ab\">2a43591<\/a>)<\/li>\n<\/ul>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2662\">#2662<\/a>:<\/strong> remove many instances of default props (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2752\">#2752<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/b7d571c777de6169de01da5cc14f2eb28988071a\">b7d571c<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.9...v9.1.10\">9.1.10<\/a> (2023-05-13)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2427\">#2427<\/a>:<\/strong> error building esm files (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2748\">#2748<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/04d571dcaa630ccbc8b8363ad06dad436f11780e\">04d571d<\/a>)<\/li>\n<li>properly pass in <code>isOpen<\/code> in modal test (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2745\">#2745<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/8cb051522ddfad48c08651604744caea25b5e68e\">8cb0515<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.8...v9.1.9\">9.1.9<\/a> (2023-04-11)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li><strong><a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2730\">#2730<\/a>:<\/strong> import error (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2733\">#2733<\/a>) (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/2547f3acfd12299dc0f4e2a5863c5cf82a203ef4\">2547f3a<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.7...v9.1.8\">9.1.8<\/a> (2023-03-29)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li>esm and lib folder not included in release (<a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/e974d0c180ebece9e35614da595e9cab2d8684b6\">e974d0c<\/a>)<\/li>\n<\/ul>\n<h2><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v9.1.6...v9.1.7\">9.1.7<\/a> (2023-03-22)<\/h2>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/6085ac0bb340f3e2cdae1aec4e6abec38c37ebad\"><code>6085ac0<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2780\">#2780<\/a> from reactstrap\/release-please--branches--master--co...<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/9d750bfe4283f3d94c3f99a617b46392df118e34\"><code>9d750bf<\/code><\/a> chore(master): release 9.2.2<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/d5ab0f40381f6cb227010a0dccf7e50485aa9e8d\"><code>d5ab0f4<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2787\">#2787<\/a> from reactstrap\/fixMergedIssues<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/18cc094afa8d4e95785d823eb735e06c28547d19\"><code>18cc094<\/code><\/a> fix: dropdown with inNavbar bug<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/5a9526962df5400cc181e9265814fffb47be4e20\"><code>5a95269<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2774\">#2774<\/a> from IntellyCode\/master<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/85cd68b58492876817fd15ba4e1649a08ffdc492\"><code>85cd68b<\/code><\/a> chore(master): release 9.2.1 (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2757\">#2757<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/f2362aaa52afc3e31a8b955afc4885162c4a6ff0\"><code>f2362aa<\/code><\/a> Fixing inNavbar bug<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/aeaf180701511377ab6a00830ccac4fb96e84028\"><code>aeaf180<\/code><\/a> fix(<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/1680\">#1680<\/a>): arrowClass not applied to arrow (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2772\">#2772<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/86eb20ab44e05e17e2162226829b63727d122b6d\"><code>86eb20a<\/code><\/a> Migrate media to rtl (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2715\">#2715<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/commit\/3be3ad16d8b21923e13650af6235f08a3d4461b0\"><code>3be3ad1<\/code><\/a> chore: migrate tests to RTL (<a href=\"https:\/\/redirect.github.com\/reactstrap\/reactstrap\/issues\/2773\">#2773<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/reactstrap\/reactstrap\/compare\/v8.10.1...v9.2.2\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=reactstrap&package-manager=npm_and_yarn&previous-version=8.10.1&new-version=9.2.2)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>\n\n> **Note**\n> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.\n","comments":[],"labels":["dependencies","javascript"]},{"title":"Multiple files for remote_write configs as in scrape_config_files and rule_files","body":"### Proposal\n\nWe are managing multiple instances of Prometheus across different projects. We have been allowing different projects to have custom rules and scrape configs loaded using the glob patterns via `rule_files` and `scrape_config_files`. This is done to make sure the default configuration `prometheus.yml` is untouched in the installations. It works nicely.\r\n\r\nIn some installations, the projects need some extra configs for their internal purpose. Such a custom configuration is `remote_write`. Right now, it looks like `remote_write` cannot be added using such a glob pattern file. \r\n\r\nIt will be nice to have the ability to load configs related to `remote_write` using similar pattern.","comments":[],"labels":["component\/config"]},{"title":"Proposal: deprecate 'consoles' in Prometheus 3","body":"### Proposal\n\nIt's a little-used feature, focus is on Grafana or Perses nowadays.\r\n","comments":["Hello,\r\n\r\nI am using this feature in production.\r\n\r\nI think the templating feature is useful, as well as servicing static assets.\r\n\r\nI think we should deprecate the legacy css and js that we kept for console users."],"labels":["help wanted"]},{"title":"storage: don't wrap single querier in merge-queriers","body":"If given a single querier, just return it instead of constructing a complicated wrapper. The code in `mergeGenericQuerier` which skipped merging when there was only one is not needed any more.\r\n\r\nThis change required a few tests to be tweaked, because they relied on the specific behaviour of `mergeGenericQuerier.Select()`.\r\n\r\nChange suggested by @colega in #13427\r\n","comments":[],"labels":["kind\/optimization"]},{"title":"PromQL: Make durations and number literals the same","body":"### Proposal\n\nThis has been agreed upon a long time ago. Also see [this design doc by @darshanime](https:\/\/docs.google.com\/document\/d\/1LaZfknXuuRWGtQSbULoMtclQhuLUMrdwg15wMvoBvCQ\/edit#) and [this brain dump of mine](https:\/\/docs.google.com\/document\/d\/1jMeDsLvDfO92Qnry_JLAXalvMRzMSB1sBr9V7LolpYM\/edit).\r\n\r\n@darshanime gave it a try in https:\/\/github.com\/prometheus\/prometheus\/pull\/9138 , but the PR fell stale without follow-up to comments.\r\n\r\nv3 would be a great timing to finish this up (either as a new PR or by building on top of #9138), because this changes the syntax of PromQL (in a backwards-compatible way, but still).","comments":["hello! i've resurrected https:\/\/github.com\/prometheus\/prometheus\/pull\/9138, we can continue the discussion there...","Great, thank you very much."],"labels":["component\/promql"]},{"title":"tsdb: need histogram support for created-timestamp handling","body":"### Proposal\n\nOM introduced created timestamps, but doesn't support native histograms yet. However, we have backported created-timestamps to the classic protobuf format, which also supports native histograms. (And of course, OM is supposed to support native histograms soon.)\r\n\r\nCombining both features, we run into the problem that [AppendCTZeroSample](https:\/\/github.com\/prometheus\/prometheus\/blob\/6cd24d87b0d4e3acd4503b061d32c5aa6d8883a9\/tsdb\/head_append.go#L389) will always insert a float zero sample, even into a series of histograms. Which will make rate calculation impossible whenever the zero float sample and any histogram sample is in the range at the same time.\r\n\r\nSo we need to find a way to insert a zero histogram in case the created timestamp comes with a histogram sample.","comments":["@ArthurSens would you be interested in working on this?\r\n\r\n@bwplotka @krajorama FYI.","Yes, that sounds fun :) Finally an excuse for me to dig deeper into native histograms :P\r\n\r\nIs this something that you folks need ASAP? I have an accepted talk at kubecon around something that I haven't finished the implementation yet, I can work on this after a few weeks \ud83d\ude2c ","Given that's only happening if you activate both feature flags at the same time, I doubt it's super urgent. But user reports will tell. :)","I'm just making sure I understand the problem here.\r\n\r\nWe currently have 2 sample types, `float` and `histogram`. The problem is that `AppendCTZeroSample` appends a float type and we don't check if we're appending a float or a histogram before calling it. Is that correct?\r\n\r\nIf yes, what do we want to do here? Just ensure we don't call `AppendCTZeroSample` when iterating over a histogram? Or implement AppendCTZeroSample to HistogramAppender?\r\n\r\nI'm asking because I haven't fully understood how native histograms work under the hood... does created timestamps matter in their context?","If a histogram is a counter-like histogram, CT applies in the same way as for float counters.\r\n\r\nThe same way as a zero float sample is inserted into a series of float counters, we need to insert a zero histogram sample into a series of histogram counters.\r\n\r\nIn the exposition format, we can see if a CT is part of a float sample or a histogram sample, so it should be easy to do the right thing.\r\n\r\nImplementation details have to be researched, but the above describes the desired outcome.","Awesome, thanks for the info!\r\n\r\nAnother question, what is the histogram equivalent to a zero sample?\r\n\r\nDoes this look correct?\r\n```go\r\nhistogram.Histogram{Count: 0, Sum: 0, Schema: 3, ZeroThreshold: 0.0, ZeroCount: 0}\r\n```","yes. just use the Go zero value.\r\n\r\nWe might need to check that a schema change or a change of the zero bucket doesn't trigger a new chunk if it is only for an empty histogram. Once we have made sure of that, even the Schema doesn't matter anymore."],"labels":["priority\/P2","component\/tsdb"]},{"title":"build(deps): bump @forevolve\/bootstrap-dark from 2.1.1 to 4.0.0 in \/web\/ui","body":"Bumps [@forevolve\/bootstrap-dark](https:\/\/github.com\/ForEvolve\/bootstrap-dark) from 2.1.1 to 4.0.0.\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/5b94de18f770a95fb5597573338d9b99b7a35aaf\"><code>5b94de1<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/ForEvolve\/bootstrap-dark\/issues\/66\">#66<\/a> from ForEvolve\/fix-pre-code-color<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/3cfdd6659a3e2d4498f7624eb8b48ea60f818aab\"><code>3cfdd66<\/code><\/a> Version bump and patch notes<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/18640ead9b544962a8f244bfde4b8dd58be095af\"><code>18640ea<\/code><\/a> Fix pre and code color<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/7f2a2db9f53714477bf7261a0348680f7276389c\"><code>7f2a2db<\/code><\/a> Merge pull request <a href=\"https:\/\/redirect.github.com\/ForEvolve\/bootstrap-dark\/issues\/58\">#58<\/a> from ForEvolve\/fix-the-dark-variable-maps<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/f285407cc63d1d0056f1b73fea2e855e63b69eb3\"><code>f285407<\/code><\/a> Bump version to 3.0.0 and update README<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/ca03ef7a1e5a070f161669201243d28b96902d52\"><code>ca03ef7<\/code><\/a> Adjust $yiq-text-dark and $yiq-text-light<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/261407b024a7a1be5749ee098d545305ffdedc86\"><code>261407b<\/code><\/a> Fix the variable maps<\/li>\n<li><a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/commit\/1439fd9410d1d96aed38e167cbd7cd94f32458e0\"><code>1439fd9<\/code><\/a> Delete FUNDING.yml<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/ForEvolve\/bootstrap-dark\/compare\/v2.1.1...v4.0.0\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=@forevolve\/bootstrap-dark&package-manager=npm_and_yarn&previous-version=2.1.1&new-version=4.0.0)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nYou can trigger a rebase of this PR by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>\n\n> **Note**\n> Automatic rebases have been disabled on this pull request as it has been open for over 30 days.\n","comments":[],"labels":["dependencies","javascript"]},{"title":"Migrate golang\/snappy to klauspost\/compress\/snappy","body":"<!--\r\n    Please give your PR a title in the form \"area: short description\".  For example \"tsdb: reduce disk usage by 95%\"\r\n\r\n    If your PR is to fix an issue, put \"Fixes #issue-number\" in the description.\r\n\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --signoff flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n\r\n- Since klauspost\/compress\/snappy is a drop-in replacement, well-maintained and claims to have better performance, it will be good to use it over golang\/snappy. \r\n- I currently don't have a production grade prometheus setup where I can try it but I hope prombench can give us those numbers.\r\n","comments":["\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13330`**](http:\/\/prombench.prometheus.io\/13330\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13330\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13330\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13330)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213330%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","Something went wrong - the Prometheus in prombench for this PR has only 12,000 series (should be millions).","\/prombench cancel","Benchmark cancel is in progress.\n","\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-13330`**](http:\/\/prombench.prometheus.io\/13330\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/13330\/prometheus-release)\n\nAfter successful deployment, the benchmarking results can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-13330\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=13330)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n- [Parca profiles (e.g. in-use memory)](http:\/\/prombench.prometheus.io\/profiles?expression_a=memory%3Ainuse_space%3Abytes%3Aspace%3Abytes%7Bpr_number%3D%2213330%22%7D&time_selection_a=relative:minute|15)\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","Running the benchmark again to see if it was a strange race condition. I'll watch closely to cancel earlier.","Unfortunately, I don't think prombench exercises remote write, or does it? There is this script https:\/\/github.com\/prometheus\/prometheus\/pull\/13102 we developed to benchmark remote write as part of https:\/\/github.com\/prometheus\/prometheus\/issues\/13105 , that may help.\r\n\r\nAlso, we actually tried klauspost\/compress\/snappy as part of that too, but AFAIR results were not really better. I am happy to be proven wrong though. ","\/prombench cancel","Benchmark cancel is in progress.\n","This time the prombench got the timeseries right for both instances of prometheus but as @npazosmendez pointed out the benchmark isnt really going to give us evidence that the new lib performs better.\r\n\r\n@chhetripradeep please have a look and see if you could come up with a meaningful benchmark for this PR. I'm also curious about this comment https:\/\/github.com\/prometheus\/prometheus\/pull\/13330#discussion_r1439279698","> Unfortunately, I don't think prombench exercises remote write\r\n\r\nThis is true, however I believe the WAL is Snappy-compressed by default, so we should be able to see the difference on writing.","Yep prombench doesn't test RW, but as nico pointed out we do have a benchmarking script for remote write.\r\n\r\nHowever, I'd prefer to leave any changes out of remote write for now. Again as Nico mentioned we've tested a number of options, including klaus's snappy\/s2 packages. The main benefit for remote write to change from golang\/snappy is going to be on the receivers side. We're doing some refactoring of how the compression package(s) is used as part of the 2.0 work and so I'd prefer to just continue with those and merge it in separately or as part of 2.0."],"labels":["prombench"]},{"title":"On-scrape conversion of classic histograms to native histograms (opt-in flag)","body":"### Proposal\n\n## Acceptance Criteria\r\n\r\n* Prometheus has a way to convert automatically all scraped classic histograms to native histograms.\r\n\r\n### Open Questions\r\n\r\n* A new have flag  e.g. (`--scrape.convert-histograms-to-native`)? More flags to control some specifics of this behaviour? Field per job in scrape-config? \ud83e\udd14 \r\n* Should we convert histograms in place (essentially use the same family metric name?) and remove classic ones? Or different name and preserving classic ones, so they can be optionally dropped by relabelling?\r\n\r\n  I think it would be good to start with doing this in place and go from there. AFAIK native histograms have different name by design (same metric family name, but in practice different __name__) allowing no clash ([one source of this info](https:\/\/github.com\/prometheus\/prometheus\/issues\/12268))\r\n\r\n## Motivation\r\n\r\nUsing native histograms can be slightly different on PromQL layer (e.g. new [functions](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/operators\/#operators-for-native-histograms)), but they are generally much cheaper for Prometheus and potential remote backends. \r\n\r\n**On top of that (why main rationale) native histograms are superior for remote write cases as they naturally make the streaming more atomic\/transactional on scale (scraped information about histogram are now self-contained in one sample, instead of multiple series that could be send in different remote write streams\/requests). This would a be huge improvement when adopting remote write (both 1.0 and 2.0).**\r\n\r\nHowever, migration to native histograms will take time, mostly due to required instrumentation changes (even if it's as simple as upgrading\/configuring the SDKs).\r\n\r\nDoing automatic migration, ideally in place would be an epic way to have one-off transition to new histograms from certain point of time. This is related to DevSummit topic for [transition strategies](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit#bookmark=id.nsi5eyddtcdr). I don't think we ever had conclusion on this.\r\n\r\n## Alternatives\r\n\r\n* Different type of relabelling action (convert?) that can be applied on histogram metrics. Tricky as relabeling user had to ensure all bucket are covered. Also we never allowed relabelling to affect sample values or timestamps.\r\n\r\ncc @beorn7 @SuperQ @roidelapluie \r\n","comments":["While not as efficient in exposition, this would also allow clients to expose more classic histogram buckets without the down side of increased cardinality on Prometheus.","This only works if the buckets in your classic histogram match some set of native histogram buckets.\r\n\r\nMaybe if you added some error tolerance, like \"convert to native histogram if the maximum mismatch of bucket boundary is <1%\" ?","Yes, I assume there will be some error tolerance, perhaps configurable \ud83d\udc4d\ud83c\udffd ","tl;dr: It was always the plan to do this, but we need custom bucket layouts #11277 first.\r\n\r\nLonger version:\r\n\r\nAs @bboreham has mentioned already, converting a classic histogram into a native one only works well in the (unlikely) case that the bucket layout of the classic histogram closely matches the bucket layout of a native histogram. In practice, this will happen very rarely. The most believable scenario would be bucket boundaries like 1, 2, 4, 8, 16, \u2026 , which is schema 0 in the native histogram world. Even allowing a small-ish error tolerance will not create many more matches. We could use interpolation and use a significantly higher resolution for the native histogram, filling the (many) native buckets that are in the same range as one of the original classic buckets with equal parts of its count. This would create \"equally bad\" quantile estimations, maybe still at a somewhat lower resource cost. I'm not sure if it is worth going down that path. It will also create confusion.\r\n\r\nCustom bucket layouts (see #11277) would solve all the problems. We could just directly emulate the classic histogram. And this very use case was one of the motivating factors of putting custom bucket layouts on the feature list. It is, however, quite involved, and we have many lower hanging fruit to harvest before.","Good points. \r\n\r\nI wonder if despite no custom buckets support we could do some (opt in) translation, with some (big) error tolerance, even accepting all those \"bad\" consequences.\r\n\r\nRationales:\r\n\r\nA) We could do this now.\r\n~B) With custom bucket layouts, many downstream Prometheus users would still have exactly the same problem. That translation will be needed for systems which only support either static or exponential buckets (e.g. Otel and Google, but most likely everybody else who does not directly import Prometheus DB) and did not implement a mix mode (or don't plan to). The difference is that it will be not directly a Prometheus problem.~ \r\n\r\nEDIT: I somehow assumed we want a \"mixed\" histogram, so sample with both exponential and custom buckets \ud83d\ude31 verified with @beorn7  that's not the case, it's either one or another \ud83d\ude48 \r\n\r\n~So my question is.. is there a room for adding no-custom-bucket mode for this conversion for now and perhaps later? Once custom buckets will land in native histograms we could either replace it or have two modes \ud83e\udd14 @beorn7~ \r\n\r\nEDIT: Given above mistake, it might indeed much better to collab on custom bucket work \ud83e\udd14 How I can help \ud83d\ude48 \r\n\r\n\r\n"],"labels":["kind\/feature"]},{"title":"Text parser mistakenly supports missing comma between label terms in the selector","body":"scrape_test.go contains a few tests that contain spaces between label terms: \r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/main\/scrape\/scrape_test.go#L1330\r\nand\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/main\/scrape\/scrape_test.go#L1405\r\n\r\nThis is not allowed in OpenMetrics parsing and does not appear to be intended behavior given the definition of the format as \r\n\r\n```\r\nmetric_name [\r\n  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\r\n] value [ timestamp ]\r\n```\r\n\r\nThis can be fixed but should probably wait until Prom 3.0\r\n","comments":["Historically, the Prometheus text format was created to allow very simple expositions, even makeshift ones (curl a metric or something). With that in mind, the format was already formulated quite tolerantly (whitespace allow almost everywhere etc.), but the parsers was made even more tolerant, arguably following the [robustness principle](https:\/\/en.wikipedia.org\/wiki\/Robustness_principle). The danger of that is that people might test their exposition against Prometheus, and then will wake up to a bad surprise if another scraper tries to parse their exposition. The historical viewpoint was that \"proper\" exposition would be done with the protobuf format anyway. That changed in between, but now (with more efficient protobuf code in Go and native histograms) we might see more protobuf usage again.\r\n\r\nIt was always my opinion that the strictness in the OpenMetrics text format is a very problematic design flaw (for reasons explained elsewhere). However, my worries are more about things like whitespaces, that were part of the Prometheus text format, not the additional tolerance that the parser granted. (Note, however, that even the OpenMetrics parser in Prometheus tolerates a few things that are not specified in OpenMetrics, which has already caused confusion. I generally prefer to have tolerance in the format itself.)\r\n\r\nLong story short: Ideally, the future is OpenMetrics (hopefully in an improved v2.0). Support of the old Prometheus text format will probably have to stay to not break the many legacy endpoints. But for the same reason, I would not change the parser's behavior for it, unless we have to.","So we close this as \"works as designed\" ?","\"As designed\" might be a bit strong, but I would indeed suggest to not change the current behavior, unless it gets in the way of something else (like the problems we ran into with the overly tolerant matcher parser in Alertmanager).","And what are your thoughts on eliminating those spaces in prom 3.0 when 'dumping' the labels?\r\nThis change would align more closely with OpenMetrics.\r\nI made a test https:\/\/github.com\/prometheus\/prometheus\/pull\/13194\/files#r1406080776 and doing so appears to only affect 'promtool tsdb dump' (which lacks a test; I'll be adding one).","It's not about the spaces, it's about having just a space and no comma.\r\nSince the code has moved, the links at the top don't show the problem. Here are links with fixed commit:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/9c4782f1cc34f85f07daaaed14af2e7ad1ff56a0\/scrape\/scrape_test.go#L1330\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/9c4782f1cc34f85f07daaaed14af2e7ad1ff56a0\/scrape\/scrape_test.go#L1405\r\n","@carrieedwards are we going to deprecate the old prometheus metric format in favour of OpenMetric format in prometheus 3.0 ? Can I work on it ? ","I think we need to make a call here. I expressed my opinion above with the rational behind it. If we follow it, we can close this issue (as we will keep the current behavior for the legacy text format parser). Working to improve OpenMetrics and make it the actual de-facto standard is a different thing out of scope of this issue."],"labels":["help wanted"]},{"title":"Make DBReadOnly more RO","body":"<!--\r\n    Please give your PR a title in the form \"area: short description\".  For example \"tsdb: reduce disk usage by 95%\"\r\n\r\n    If your PR is to fix an issue, put \"Fixes #issue-number\" in the description.\r\n\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --signoff flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n\r\nFixes: https:\/\/github.com\/prometheus\/prometheus\/issues\/11618 https:\/\/github.com\/prometheus\/prometheus\/issues\/8086\r\n\r\nThe RO head could alter the mmap chunks while replaying the WAL e.g. https:\/\/github.com\/prometheus\/prometheus\/blob\/e6d3d6abd99d1eda8576809b62d59d606ee1d5ac\/tsdb\/head_wal.go#L594\r\n\r\nThe other possibility is if it tries to repair a chunk file in repairLastChunkFile or when adjusting corrupted chunks.\r\n\r\n---\r\n\r\nThis PR:\r\n\r\nchore(tsdb): add a sandboxDir to DBReadOnly, the directory can be used for transient file writes.\r\n    \r\nuse it in loadDataAsQueryable to make sure the RO Head doesn't truncate or cut new chunks in data\/chunks_head\/.\r\n    \r\nadd a -sandbox-dir-root flag to \"promtool tsdb dump\/dump-openmetrics\" to control the root of that sandbox dirrectory.\r\n\r\n---\r\n\r\n~- [ ] put \"feature behind a flag\"~\r\n- [x] allow setting the tmp path (to be able to use a FS with enough space if needed etc.)\r\n- [ ] doc that extra disk space may be needed\r\n","comments":["@jesusvazquez could you make a call if this approach make sense and @machine424 should continue to refine it?"],"labels":["component\/tsdb"]},{"title":"scrape: Bail out of invalid scrape format rather than silently falling back to the classic text format","body":"### Proposal\r\n\r\nBack in the old days, it was a reasonable assumption that a target that fails to properly negotiate the scrape format is probably an ad-hoc implementation that most likely tries to produce the (classic) text format. Therefore, it was a good idea to fall back to parsing the classic text format whenever anything went wrong.\r\n\r\nHowever, this assumption doesn't hold anymore. It might even be dangerous. That's mostly because now there is OpenMetrics, which looks very similar to the classic Prometheus text format but is not the same. With the current fallback behavior, Prometheus might end up in a situation where it parses OpenMetrics as the classic text format, which - in the worst case - works without error but yielding wrong results (most relevant when it comes to timestamps).\r\n\r\nA recent issue caused by this is #13172. In short, the following happened: With native histograms enabled, Prometheus tries to negotiate protobuf. However, [KSM](https:\/\/github.com\/kubernetes\/kube-state-metrics) wasn't prepared for that and responded with some invalid content type (details to be investigated), which caused Prometheus to parse the KSM scrape as classic text format, while KSM was serving OpenMetrics. Luckily, the metrics exposed by KSM use features of OM that won't parse as the classic text format, so Prometheus error'd out rather than silently ingesting wrong data. However, the error message is still quite confusing (see #13272) and it would have been much clearer if Prometheus had just reported that KSM wasn't negotiating a valid content type.\r\n\r\nSo the proposal here is to simply not do a fallback but report an error in case of invalid content negotiation.\r\n(Relevant code: https:\/\/github.com\/prometheus\/prometheus\/blob\/b8bcaef14da44eb384e8a07655da35614dc97467\/scrape\/scrape.go#L1404-L1410 https:\/\/github.com\/prometheus\/prometheus\/blob\/b8bcaef14da44eb384e8a07655da35614dc97467\/model\/textparse\/interface.go#L81-L91 )\r\n\r\nThis is a breaking change as currently there might be a lot of targets that aren't doing proper content negotiation but \"just work\". This change would break those use cases, but with the danger of ingesting wrong data (as explained above), we should make use of the 3.0.0 release to deliberately expose those broken scrapers.\r\n\r\nAn open question is if we want to retain the simplicity of doing trivial text-format expositions without _any_ kind of content negotiation, i.e. the [case where the content type ends up empty](https:\/\/github.com\/prometheus\/prometheus\/blob\/b8bcaef14da44eb384e8a07655da35614dc97467\/model\/textparse\/interface.go#L84-L86). OpenMetrics has created a dilemma here. The subtle differences between OM and the classic text format create ambiguities (see above). We could retain the feature of using one specific format in case the target doesn't do any content negotiation. But which format would that be? We have only bad choices here:\r\n1. If we stuck with the classic text format, the existing use cases won't break, but we would extend the life of the deprecated classic text format forever.\r\n2. If we fell back to OpenMetrics, we would run into the danger of ingesting old targets in the wrong way (most relevant concern: timestamps).\r\n3. We could only accept a \"safe common subset\" of OM and the classic format in case of missing content negotiation. This would only break a few use cases, but it would be a weird and complicated contraption to cater for a supposedly \"trivial\" use case.\r\n\r\n_(Edited for clarity and correctness. Previously I assumed, Prometheus would also fall back on parsing problem when one content type was negotiated but another one was served, but it doesn't.)_","comments":["Thanks for raising this.\r\n\r\n> However, this assumption doesn't hold anymore. It might even be dangerous. That's mostly because now there is OpenMetrics, which looks very similar to the classic Prometheus text format but is not the same. With the current fallback behavior, Prometheus might end up in a situation where it parses OpenMetrics as the classic text format, which - in the worst case - works without error but yielding wrong results (most relevant when it comes to timestamps).\r\n\r\nAgree, but wonder.. can we finally do the improvements to OM text we always wanted and perhaps \"revert\" or change some ideas which were not ideal at the end (e.g _created series and perhaps timestamp change?) and release OM 2.0?\r\n\r\nAlternatively we could add some required marker for OM? Or perhaps there is way for OM 2.0 to be... backward compatible with Prometheus text (that would be PARADISE \ud83d\ude48 ).\r\n\r\nMaybe I am looking too wide here, but it feels whatever we do here is wrong, without changing OM a bit. But it feels we version OM for a reason, so why being blocked here?"],"labels":["component\/scraping","kind\/change"]},{"title":"promql: Make range selections left-open and right-closed","body":"### Proposal\n\nFor historical reasons, a range selector selects a closed interval, i.e. samples perfectly coinciding with the boundaries of the range are included in the selection.\r\n\r\nFor various reasons, it is more consistent and more helpful in practice to make the selection \"left open\" and \"right closed\", i.e. a sample coinciding with the \"left\" boundary (the one further in the past) is excluded from the selection, while at the \"right\" boundary (the one further towards the future) stays inclusive.\r\n\r\nFor example, with samples perfectly spaced every 1m (very common in practice), a range over 5m might currently select 5 or 6 samples, depending on the exact alignment. In practice, it almost always selects 5 samples, because perfect alignment rarely happens in real-world scenarios. However, in test scenarios, perfect alignment happens easily, even without intending to do so. So test cases can easily represent a case that is very rare in practice. With the proposed change, the range will always select 5 samples in the case of perfectly spaced samples.\r\n\r\nThis is, however, a breaking change, although the impact is mostly academic. Therefore, we should implement this change with the upcoming 3.0.0 release.","comments":["Hello! I am following this change.\r\nMay I ask why the range selector is defined as left-open and right-closed instead of left-closed and right-open?\r\nI think the latter is better because it is consistent with the behavior of range selectors for lists in multiple programming languages.\r\nFor example, in Go, use `[lo:hi]` to operate on array or slice `arr`, the result is:\r\n```go\r\narr[lo], arr[lo+1], arr[lo+2],..., arr[hi-1]\r\n```\r\nIt includes left bound and excludes right bound.\r\nThanks.","Good question. I should have mentioned the rationale for that, too. The short answer is that a range vector should always select the samples that the corresponding instant vector would have selected, too.\r\n\r\nIn more detail:\r\n\r\nAn instant vector will select a sample that is precisely at the evaluation timestamp (if there is one, otherwise it will search for the next sample in the past). So let's say the instant vector `foo` returns a sample that happens to be precisely at the evaluation timestamp. If you change that instant vector to a range vector `foo[1m]`, you should still see that same sample selected as part of the range. That's accomplished with a \"right closed\" range. However, with a \"right open\" range, you would _not_ select that sample.","When the range selection interval becomes smaller and smaller, the result shall converge to the corresponding instant query result - Well designed, thanks\ud83d\ude00","Hey @beorn7, I would like to work on this issue.\r\nSo, in which function are we supposed to make this change?\r\nThe query data flows like this ig - `NewRangeQuery -> Exec -> exec -> execEvalStmt -> InitStepTracking -> Eval -> eval` in `promql\/engine.go`.","> Hey @beorn7, I would like to work on this issue.\r\n\r\nI have assigned this issue to you.\r\n\r\n> So, in which function are we supposed to make this change?\r\n\r\nI don't know the PromQL codebase well enough to answer this question from the top of my head. You have to investigate the codebase yourself (as you have already started to do so). #prometheus-dev on the [CNCF slack](https:\/\/slack.cncf.io\/) might be a good forum to fish for more experienced PromQL coders than me or general help.\r\n\r\nNote that these changes have to be behind a feature flag in v2.x and will only become default in v3.","Hey @beorn7 \ud83d\udc4b I just stumbled upon this issue since I was looking at the same artifact of including more datapoints than I intend to when using `<agg_function>_over_time` for the purpose of down-sampling a gauge metric.\r\n\r\nWhat I was thinking wrt this proposed solution is to do the opposite selection of which side is open and which one is closed. When down-sampling using an aggregation function over time, you would expect the left limit of the range interval to be included in the aggregation but not the right limit, i.e. left-closed and right-open.\r\n\r\nDoing the opposite would mean that in a range between `t1` and `t2` timestamps, the `t1` aggregate doesn't include `t1` but includes `t2` and is tracked under `t2` in the resulting timeseries, which is not accurate to describe which time-bucket a given aggregate represents. In other words, with left-open and right-closed, what you get is the aggregate of the `(t1, t2]` interval timestamped on the right-side limit (`t2`) when the semantic (or intuitive) behavior of down-sampling is to calculate the aggregate over the `[t1, t2)` interval and timestamp it on `t1`.\r\n\r\nMaybe I'm missing something, but \"left-closed and right-open\" is how I've seen all re-sampling\/down-sampling intervals working across multiple time-series tools (database query languages, stream processing, data frame libraries, etc.). Maybe the fundamental behavior here is also based on the range interval in prometheus always looking back as opposed to creating discrete `[t1, t2)` intervals over the overall query time range, since today I can easily achieve the \"left-open and right-closed\" range interval by using a simple `offset 1s` on my range vectors (assuming stored data sampling rate is > 1s), but that looks just odd for example on the last timestamp aggregate when you don't have the full interval and if you're using `sum` aggregation you already see a complete interval total \ud83e\udd14 \r\n\r\nI don't know if this behavior also relates to @KofClubs's [comment](https:\/\/github.com\/prometheus\/prometheus\/issues\/13213#issuecomment-1853147793) or if that one was purely based on array slicing semantics \ud83d\ude05 ","I guess the one important argument would be that an instant selector selects the most recent sample at or before the evaluation timestamp. If a range selector was right-open, it would _not_ select a sample exactly at the evaluation timestamp, so it would be possible that an instant selector selects a sample that a range selector at the same timestamp would not select, which I believe is highly undesirable."],"labels":["component\/promql","kind\/change"]},{"title":"--enable-feature: remove \"promql-at-modifier\" and \"promql-negative-offset\"","body":"### Proposal\n\nBoth features are enabled by default and thus should be dropped from the feature flag list for a major version bump.","comments":["@carrieedwards @jan--f \r\nHi Gophers, \r\nIs this issue still open?","Yes, and it will (only) be closed once v3.0.0 has been released with those feature flags removed.","Sorry, I guess you wanted to know if this is still free to work on\u2026 I guess it is, and it is indeed an easy one. I guess we will keep the PR unmerged until we have a prometheus-3 release branch (and merge it into that one).","Yup, @beorn7 , thank you for understanding. \r\nFor further details, could you please assist me more regarding the PR because this will be my first contribution?","Then take your time and explore the code a bit. You could first search the code base to find where the feature flag mentioned in this issue are processed. From there, it should be easy to see how to remove these flags.","Ummm, now that I look at this issue more closely, there is already a PR doing the thing, see #13456 linked above.\r\nSo I guess that means this issue is already \"solved\", we just need to delay merging the PR and closing the issue up to the point where we have a v3 release branch.","@beorn7 thank you for your collaboration."],"labels":["help wanted","good first issue"]},{"title":"remove the `remote-write-receiver` flag from `enable-feature`","body":"### Proposal\n\nThis flag has been deprecated and user should use `--web.enable-remote-write-receiver` to enable this feature.","comments":["modified the title a little, it was a bit unclear before what the point of this issue was :+1:  ","Hey is anyone working on this? If not, I would like to work on it","Yes there is a PR linked against this issue already.","And we can only merge that one once we actually release v3 (or have a release branch for v3, in which we can merge it)."],"labels":["help wanted","good first issue"]},{"title":"Improve efficiency of histogram decoding","body":"### Proposal\r\n\r\nThe histogram decoder allocates a new object each time `AtHistogram` or `AtFloatHistogram` is called, and this takes a significant amount of CPU when executing queries. Furthermore, calling `ToFloat` on a regular histogram also creates a new object and there is no good way to reuse memory during the execution of a query.\r\n\r\nI've put together a quick benchmark to illustrate this problem: https:\/\/github.com\/prometheus\/prometheus\/pull\/13160. Note that even though the tooltip says 4.6% of all, that also includes the time required to set up the benchmark. The `ToFloat` call actually takes around 30-40% of the query execution time.\r\n\r\n<img width=\"1044\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/1286231\/42f46347-88ae-4e73-804c-cbab0cacb73d\">\r\n\r\n\r\nI think this optimization is worth pursuing because it would lead to significant performance improvements for queries on native histograms. One solution could be to apply the same pattern that is used in `SampleIterable` where the caller can provide an existing histogram object that can be reused. If `nil` is provided, the decoder would allocate a new object.","comments":["As per direct communication, @fpetkovski will work on this."],"labels":["kind\/enhancement","component\/promql","priority\/P3"]},{"title":"Epic: Implement UTF-8 character support for label and metric names","body":"### Tracking Issue\r\n\r\nRelates to:\r\n\r\n[Original discussion](https:\/\/groups.google.com\/g\/prometheus-developers\/c\/-O23C4_9Ijc\/m\/Zrk2UOaNAQAJ)\r\nBrainstorming doc [Quoting Prometheus Names](https:\/\/docs.google.com\/document\/d\/1yFj5QSd1AgCYecZ9EJ8f2t4OgF2KBZgJYVde-uzVEtI\/edit)\r\n[OTEL Attribute compatibility section](https:\/\/docs.google.com\/document\/d\/1epvoO_R7JhmHYsII-GJ6Yw99Ky91dKOqOtZGqX7Bk0g\/edit#heading=h.yqsgk5wz7ww0)\r\n\r\nWe are creating this issue as part of the OTEL Support milestone so it will be OTEL oriented but the problem is general to prometheus and has been around for a long time.\r\n\r\nOTel allows UTF-8 in label names while Prometheus has a much [more restrictive set](https:\/\/prometheus.io\/docs\/concepts\/data_model\/#metric-names-and-labels). This is causing friction for users when using Prometheus. In particular, . (dot) is a very common character in OTel and we convert that to _ when converting to Prometheus. For example, service.version becomes service_version.\r\n\r\nProposals have been accepted to do this work:  \r\n\r\n* https:\/\/github.com\/prometheus\/proposals\/blob\/main\/proposals\/2023-08-21-utf8.md\r\n* https:\/\/github.com\/prometheus\/proposals\/blob\/main\/proposals\/2023-11-13-utf8-migration.md\r\n\r\n\r\n### Write-side tasks:\r\n- [x] prom\/common: update openmetrics_parse, promparse (https:\/\/github.com\/prometheus\/common\/pull\/537)\r\n- [x] prom\/common: content negotiation in scraping (https:\/\/github.com\/prometheus\/common\/pull\/570)\r\n- [x] prom\/client_golang: (https:\/\/github.com\/prometheus\/client_golang\/issues\/1369)\r\n- [x] prom\/prometheus: Update some parsers (https:\/\/github.com\/prometheus\/prometheus\/pull\/13271)\r\n- [ ] prom\/common: update text_parse (go version) (https:\/\/github.com\/prometheus\/common\/issues\/554)\r\n- [ ] prom\/prometheus: content negotiation in scraping \/ remote-write (https:\/\/github.com\/grafana\/mimir-prometheus\/pull\/589)\r\n- [ ] prom\/pushgateway: (https:\/\/github.com\/prometheus\/pushgateway\/issues\/623)\r\n\r\n### Read-side tasks:\r\n- [ ] #13765\r\n- [ ] prom\/prometheus: tsdb meta.json bump\r\n- [ ] prom\/prometheus: tsdb multiquerying for otel and other migration cases\r\n- [ ] prom\/prometheus: content negotiation in querying?\r\n- [ ] prom\/prometheus: update web\/ui parser (https:\/\/github.com\/prometheus\/prometheus\/issues\/13564)\r\n\r\n### Client Libraries:\r\n- [x] [Go](https:\/\/github.com\/prometheus\/client_golang\/issues\/1369): no changes required\r\n- [ ] [Java](https:\/\/github.com\/prometheus\/client_java\/issues\/916) -- in progress, will be implemented in a feature branch pending OTel spec updates\r\n- [ ] https:\/\/github.com\/prometheus\/client_python\/issues\/1013\r\n- [ ] https:\/\/github.com\/prometheus\/client_ruby\/issues\/306\r\n- [ ] https:\/\/github.com\/prometheus\/client_rust\/issues\/190\r\n\r\n### User-visible changes:\r\n- [ ] grafana\/agent: needs minimal config and flags for setting globals\r\n- [ ] grafana\/mimir: flags to enable utf8\r\n- [ ] grafana\/grafana: https:\/\/github.com\/grafana\/grafana\/issues\/74875\r\n\r\n### Other task refs:\r\n- [ ] https:\/\/github.com\/prometheus\/common\/issues\/527\r\n- [ ] update docs, like https:\/\/github.com\/prometheus\/docs\/blob\/main\/content\/docs\/instrumenting\/exposition_formats.md\r\n- [ ] Officially update OpenMetrics spec\r\n- [ ] Officially update [OTEL spec](https:\/\/opentelemetry.io\/docs\/specs\/otel\/compatibility\/prometheus_and_openmetrics\/#metric-metadata-1)\r\n\r\n(This is a clone of https:\/\/github.com\/prometheus\/prometheus\/issues\/12630 that I own so I can edit the top-level comment and track work here)","comments":["Design work will be needed for the tsdb changes -- determining how to query data that was written with old versions of prometheus will be tricky, for instance if an Otel metric goes from _ to . but doesn't use the U__ quoting syntax in the old metrics.","Ran into this issue with Grafana alert labels vs. Prometheus alert labels. Grafana supports `-`, but Prometheus does not. In order to avoid conflicts or multiple label sets, we'll need to replace `-` with `_` in all of our alerts and notification policies next week.\r\n\r\nConsistency between OTel, Prometheus, etc. would be fantastic.","https:\/\/github.com\/prometheus\/common\/pull\/537 in progress (name validation, exposition parsers)","https:\/\/github.com\/ywwg\/common\/pull\/2 in draft (write-side content negotiation)"],"labels":["help wanted"]},{"title":"Enable using a published UI instead of building it","body":"Personally I struggle with the build running `npm`; only one of my machines has that installed and I don't know anything about front-end development. I wish there was a way to skip that, maybe at the cost of a reduced UI experience.\r\n\r\n_Originally posted by @bboreham in https:\/\/github.com\/prometheus\/prometheus\/issues\/12974#issuecomment-1763446234_\r\n            ","comments":["In the release artefacts, there is a tar.gz with the UI. We could have some magic to use this for people who do not need to build the UI."],"labels":["component\/ui"]},{"title":"Website is not responsive for a small devices ","body":"### What did you do?\n\nHello @roidelapluie ,\r\nI found an issue in this website which is not responsive for small device or 320px to 392px \r\nyou can check it ,I want to work on this issue ...assign  me this issue .\n\n### What did you expect to see?\n\n![Screenshot 2023-10-08 110040](https:\/\/github.com\/prometheus\/prometheus\/assets\/125847751\/977dc011-b471-4a73-b16a-1c3aa6a18054)\r\n![Screenshot 2023-10-08 110056](https:\/\/github.com\/prometheus\/prometheus\/assets\/125847751\/6ef1823c-04d1-44a9-9901-68df250ed3e8)\r\n\n\n### What did you see instead? Under which circumstances?\n\nhttps:\/\/prometheus.io\/docs\/introduction\/overview\/#what-is-prometheus\r\n\n\n### System information\n\nLinux\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Thanks! So generally it works quite well for all of the phone types (see the dropdown in \"Dimensions\" column). It feels it's good enough. Note that it takes some time or reload for responsiveness to catch up on Chrome when you change the dimensions.\r\n\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/6950331\/cf2a6345-d05e-475c-befc-489ef8e912db)\r\n\r\nIf you have some reasonable device that is problematic, then we can consider improving - but we could reproduce the problem yet, plus not sure if we want support very old devices with small resolution etc \ud83e\udd14  Unless it would help with accessibility, but not sure - feedback welcome.\r\n\r\n(Reviewed during Prometheus bug scrub) \r\n\r\n\r\n\r\n"],"labels":["kind\/enhancement","component\/documentation"]},{"title":"Error if target-group is empty","body":"### Proposal\n\nI am trying to diagnose at what level my problem is, my ServiceMonitor is not finding any of the target endpoints I have defined to it. But there's no error in the logs. In the list of scrape pools, I can find my service monitor, but when I select it, I can't see any targets.\r\n\r\nWhy that is happening, I can't tell. But I'd like more diagnostic data from Prometheus, that there were no endpoints found (or some other problem).\r\n\r\nIn this code somewhere, it's missing a check for if a scrape-pool is empty.\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a5a4eab679ccf2d432ab34b5143ef9e1c6482839\/scrape\/manager.go#L211\r\nThe code should notice it and print a helpful diagnostic (however helpful it can be) for the problem pool.","comments":["... Or here. If all targets are dropped, give us helpful information.\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a5a4eab679ccf2d432ab34b5143ef9e1c6482839\/scrape\/scrape.go#L535C1-L536C1","What story does the service discovery page tell?","`promtool check service-discovery` to manually re-trigger the discovery, can be helpful.\r\nAlso you can check metrics like `prometheus_sd_discovered_targets` or per SD mechanism metrics to see where the targets are disappearing, if any.\r\n\r\nUnfortunately, with more details, we cannot help. "],"labels":["component\/service discovery","kind\/more-info-needed"]},{"title":"Improve handling of staleness markers around hashmod sharding use cases","body":"### Proposal\n\nCurrently, scaling up or down a Prometheus or Prometheus Agent setup that makes use of Hashmod Sharding leads to staleness markers being inserted for series that get re-assigned to another shard (as the target disappears from its original owner's scrape pool).\r\n\r\nThis leads to gaps in graphs, even if the metric will re-appear in the next scrape interval.\r\n\r\nIt'd be nice if we could have some more fine-grained handling of staleness markers to avoid causing issues in environments who have some automated ways of dynamically scaling their Prometheus instances. (this is also relevant to scraping with Grafana Agent [clustering](https:\/\/grafana.com\/docs\/agent\/latest\/flow\/concepts\/clustering\/)).\r\n\r\nI don't have a proposal ready here, so I wanted to start by asking: Has this come up in the past? Any ideas that we didn't have the time pursuing?","comments":["Nice, definitely valid issue for all scaling (out, back) without consistent hashing.\r\n\r\nOne yolo idea would be detect hashmod or modulus changes, but then there is no guarantee that other Prometheus replicas will actually continue the scrape. Not sure if solvable without some consensus protocol.\r\n\r\n(Triaged during Prometheus bug scrub)."],"labels":["kind\/bug","component\/tsdb"]},{"title":"Support for client_id_file configuration in prometheus oauth configuration","body":"### Proposal\n\nA user has to mention client_id for oauth in string format. This is a sensitive data in the configuration yaml in my firm.\r\n\r\n`client_id: [<string>]`\r\n\r\n**Proposal:**\r\n\r\nWe want to give file for client_id, similar to client_secret_file\r\n\r\n`[ client_secret_file: [<filename>]`\r\n\r\nSimilar to above config:\r\n`client_id_file: [<filename>]`\r\n\r\n\r\n**Contribution:**\r\n\r\nI am working on this and would be happy to contribute on this feature back to prometheus community.","comments":["Hey @AVESH-RAI, I would like to contribute to this.","Hi @nikzayn , the above requirement is to add a secret file `client_id_file`. The content of the file will be `client_id` . \r\nI am already working on this and testing my code changes.","Hi \ud83d\udc4b\ud83c\udffd \r\n\r\nSo generally client_id is [NOT a secret](https:\/\/datatracker.ietf.org\/doc\/html\/rfc6749#section-2.2), but some sources claim it's recommended to [not show it around](https:\/\/www.oauth.com\/oauth2-servers\/client-registration\/client-id-secret\/) (whatever that means). \r\n\r\nWe wonder if there are other companies\/users who would like to see this feature before we accept it?\r\n\r\nAsking as it's yet another duplicated setting and file to be parsed unless we decide to put that as some `client_file` or so with both client ID and secret (probably micro optimization).\r\n\r\n(Triaged during Prometheus bug scrub meeting) \r\n\r\n","@bwplotka  In our company client_id is considered as secret"],"labels":["help wanted","kind\/enhancement","component\/config"]},{"title":"promtool http.config.file format?","body":"### Proposal\r\n\r\nWhen prometheus is using basic auth all promtool commands require \"--http.config.file\".\r\nDocumentation says: \"HTTP client configuration file for promtool to connect to Prometheus.\"\r\nI have been trying to find a sample  or reference to such a file, but no luck. I am unable to connect, neither use promtool in such scenario.\r\nIs this reference missing or is it that obvious that does not require further introduction?","comments":["It should be similar to:\r\n\r\nhttps:\/\/prometheus.io\/docs\/alerting\/latest\/configuration\/#http_config\r\n\r\nBut we should indeed document this.","Hello :wave: \r\nI'll come up with proposal to document it in a PR soon!\r\n"],"labels":["help wanted","component\/documentation"]},{"title":"dependency failed to start","body":"### What did you do?\n\n1. docker compose up\n\n### What did you expect to see?\n\nI would expect the container to start without error.\n\n### What did you see instead? Under which circumstances?\n\ndependency failed to start: container prometheus is unhealthy\n\n### System information\n\nmacOS 14 (Sonoma)\n\n### Prometheus version\n\n```text\nv2.47.0\n```\n\n\n### Prometheus configuration file\n\n```yaml\n# my global config\r\nglobal:\r\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n  # scrape_timeout is set to the global default (10s).\r\n\r\n  external_labels:\r\n    monitor: 'phoenix_app'\r\n\r\n# Alertmanager configuration\r\nalerting:\r\n  alertmanagers:\r\n    - static_configs:\r\n        - targets:\r\n          # - alertmanager:9093\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n  # - \"first_rules.yml\"\r\n  # - \"second_rules.yml\"\r\n\r\n# A example scrape configuration for running Prometheus with Docker.\r\n\r\nscrape_configs:\r\n  # Make Prometheus scrape itself for metrics.\r\n  - job_name: 'prometheus'\r\n    static_configs:\r\n      - targets: ['host.docker.internal:9090']\r\n      # - targets: ['prometheus:9090']\r\n\r\n  # Create a job for Docker daemon.\r\n  #\r\n  # This example requires Docker daemon to be configured to expose\r\n  # Prometheus metrics, as documented here:\r\n  # https:\/\/docs.docker.com\/config\/daemon\/prometheus\/\r\n  - job_name: 'docker'\r\n    static_configs:\r\n      # - targets: ['127.0.0.1:9323']\r\n      - targets: ['host.docker.internal:9323']\r\n\r\n  #\r\n  # Create a job for the Phoenix application.\r\n  #\r\n  # Reference:\r\n  #\r\n  # https:\/\/github.com\/akoutmos\/prom_ex\/blob\/master\/example_applications\/shared_docker\/prometheus\/config.yml#L15-L17\r\n  #\r\n  - job_name: 'phoenix_app'\r\n    scrape_interval: 5s\r\n    static_configs:\r\n      - targets: ['host.docker.internal:4000']\r\n\r\n  # Create a job for Docker Swarm containers.\r\n  #\r\n  # This example works with cadvisor running using:\r\n  # docker run --detach --name cadvisor -l prometheus-job=cadvisor\r\n  #     --mount type=bind,src=\/var\/run\/docker.sock,dst=\/var\/run\/docker.sock,ro\r\n  #     --mount type=bind,src=\/,dst=\/rootfs,ro\r\n  #     --mount type=bind,src=\/var\/run,dst=\/var\/run\r\n  #     --mount type=bind,src=\/sys,dst=\/sys,ro\r\n  #     --mount type=bind,src=\/var\/lib\/docker,dst=\/var\/lib\/docker,ro\r\n  #     google\/cadvisor -docker_only\r\n  # - job_name: 'docker-containers'\r\n  #   docker_sd_configs:\r\n  #     - host: unix:\/\/\/var\/run\/docker.sock # You can also use http\/https to connect to the Docker daemon.\r\n  #   relabel_configs:\r\n  #     # Only keep containers that have a `prometheus-job` label.\r\n  #     - source_labels: [__meta_docker_container_label_prometheus_job]\r\n  #       regex: .+\r\n  #       action: keep\r\n  #     # Use the task labels that are prefixed by `prometheus-`.\r\n  #     - regex: __meta_docker_container_label_prometheus_(.+)\r\n  #       action: labelmap\r\n  #       replacement: $1\r\n# remote_write:\r\n#   - url: '<Your Prometheus remote_write endpoint>'\r\n#     basic_auth:\r\n#       username: '<Your Grafana Username>'\r\n#       password: '<Your Grafana API key>'\n```\n\n\n### Alertmanager version\n\n```text\nN\/A\n```\n\n\n### Alertmanager configuration file\n\n```yaml\nN\/A\n```\n\n\n### Logs\n\n```text\ndocker compose up\r\n[+] Running 2\/0\r\n \u2714 Container prometheus  Created                                                                                                                                                                              0.0s \r\n \u2714 Container grafana     Created                                                                                                                                                                              0.0s \r\nAttaching to grafana, prometheus\r\nprometheus  | ts=2023-10-01T06:08:38.909Z caller=main.go:583 level=info msg=\"Starting Prometheus Server\" mode=server version=\"(version=2.47.0, branch=HEAD, revision=efa34a5840661c29c2e362efa76bc3a70dccb335)\"\r\nprometheus  | ts=2023-10-01T06:08:38.909Z caller=main.go:588 level=info build_context=\"(go=go1.21.0, platform=linux\/arm64, user=root@409eb5e6b30c, date=20230906-10:27:30, tags=netgo,builtinassets,stringlabels)\"\r\nprometheus  | ts=2023-10-01T06:08:38.909Z caller=main.go:589 level=info host_details=\"(Linux 6.3.13-linuxkit #1 SMP PREEMPT Thu Sep  7 07:48:47 UTC 2023 aarch64 39d6d04f566a (none))\"\r\nprometheus  | ts=2023-10-01T06:08:38.909Z caller=main.go:590 level=info fd_limits=\"(soft=1048576, hard=1048576)\"\r\nprometheus  | ts=2023-10-01T06:08:38.909Z caller=main.go:591 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nprometheus  | ts=2023-10-01T06:08:38.910Z caller=web.go:566 level=info component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nprometheus  | ts=2023-10-01T06:08:38.910Z caller=main.go:1024 level=info msg=\"Starting TSDB ...\"\r\nprometheus  | ts=2023-10-01T06:08:38.911Z caller=tls_config.go:274 level=info component=web msg=\"Listening on\" address=[::]:9090\r\nprometheus  | ts=2023-10-01T06:08:38.911Z caller=tls_config.go:277 level=info component=web msg=\"TLS is disabled.\" http2=false address=[::]:9090\r\nprometheus  | ts=2023-10-01T06:08:38.912Z caller=head.go:600 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nprometheus  | ts=2023-10-01T06:08:38.912Z caller=head.go:681 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=1.042\u00b5s\r\nprometheus  | ts=2023-10-01T06:08:38.912Z caller=head.go:689 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nprometheus  | ts=2023-10-01T06:08:38.914Z caller=head.go:760 level=info component=tsdb msg=\"WAL segment loaded\" segment=0 maxSegment=1\r\nprometheus  | ts=2023-10-01T06:08:38.914Z caller=head.go:760 level=info component=tsdb msg=\"WAL segment loaded\" segment=1 maxSegment=1\r\nprometheus  | ts=2023-10-01T06:08:38.914Z caller=head.go:797 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=23.375\u00b5s wal_replay_duration=1.628625ms wbl_replay_duration=41ns total_replay_duration=1.663791ms\r\nprometheus  | ts=2023-10-01T06:08:38.915Z caller=main.go:1045 level=info fs_type=EXT4_SUPER_MAGIC\r\nprometheus  | ts=2023-10-01T06:08:38.915Z caller=main.go:1048 level=info msg=\"TSDB started\"\r\nprometheus  | ts=2023-10-01T06:08:38.915Z caller=main.go:1229 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nprometheus  | ts=2023-10-01T06:08:38.916Z caller=main.go:1266 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=708.334\u00b5s db_storage=959ns remote_storage=1\u00b5s web_handler=334ns query_engine=5.417\u00b5s scrape=161.041\u00b5s scrape_sd=19.542\u00b5s notify=9.792\u00b5s notify_sd=3.125\u00b5s rules=834ns tracing=1.75\u00b5s\r\nprometheus  | ts=2023-10-01T06:08:38.916Z caller=main.go:1009 level=info msg=\"Server is ready to receive web requests.\"\r\nprometheus  | ts=2023-10-01T06:08:38.916Z caller=manager.go:1009 level=info component=\"rule manager\" msg=\"Starting rule manager...\"\r\ndependency failed to start: container prometheus is unhealthy\n```\n","comments":["Hi!\r\n\r\nCould you give us more info on your compose configuration? We don't provide anything in this repo, so it's unclear how you run & configure Prometheus (other than scrape config). You injected some commentary about `# Create a job for Docker Swarm containers.` but it's within scrape config, so it's hard to parse, plus we don't see how you spin up Prometheus containers. Thanks! \ud83e\udd17 \r\n\r\nIt feels you might missed some readiness probe (or healthcheck it's called in Docker swarm?)\r\n\r\n(Triaged during Prometheus bug scrub meeting) ","@bwplotka I have created a sample repository that you can find at the following location:\r\n\r\nhttps:\/\/github.com\/conradwt\/promex-example-using-docker-desktop","Yep, it looks like a docker compose error https:\/\/github.com\/docker\/compose\/blob\/2945532f978ba1fc5b4e448359cf18f2e35f76eb\/pkg\/compose\/convergence.go#L375C1-L375C63\r\n\r\nI suggest you take a look at those healthchecks logs (maybe `docker inspect` may help)"],"labels":["kind\/question"]},{"title":"tsdb: filter label values before appending","body":"This saves a lot of memory in cases where there are a lot of possible values but the matcher discards most of them.\r\nFor example `pod=~\"foo-bar-.*\"`, in a cluster with 200,000 pods.\r\n\r\nExtending the `IndexReader` interface is a drawback, but it was already changed this week (to add Context).\r\nI suspect `SortedLabelValues` could be removed, since #7448 speaks about the same underlying issue.\r\n\r\nEDIT: `MemPostings.LabelValuesFiltered` has grown an ugly batching system, because otherwise it runs 25% slower than before.  This seems weird, but is totally repeatable at least on my M2 laptop.\r\n\r\nI extended the benchmarks too, which threw up some scenarios that needed tweaking:\r\n\r\n```\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com\/prometheus\/prometheus\/tsdb\r\n                                                                            \u2502    before    \u2502               after                \u2502\r\n                                                                            \u2502    sec\/op    \u2502   sec\/op     vs base               \u2502\r\nLabelValuesWithMatchers\/a_unique;[]-8                                         12.98m \u00b1  8%   12.81m \u00b1 1%        ~ (p=1.000 n=6)\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                        40.66m \u00b1  9%   18.13m \u00b1 0%  -55.40% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                          3.181m \u00b1  4%   3.126m \u00b1 0%        ~ (p=0.065 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                         111.3m \u00b1  3%   110.9m \u00b1 0%        ~ (p=1.000 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                            3.106m \u00b1  3%   3.127m \u00b1 0%        ~ (p=0.180 n=6)\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8       40.12m \u00b1  4%   37.85m \u00b1 0%   -5.66% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8         19.53m \u00b1  1%   18.73m \u00b1 0%   -4.07% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[]-8                                     12.54m \u00b1  9%   11.82m \u00b1 9%   -5.79% (p=0.041 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                    51.94m \u00b1  3%   25.23m \u00b1 2%  -51.43% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                      3.025\u00b5 \u00b1  5%   3.021\u00b5 \u00b1 0%        ~ (p=1.000 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                     76.90m \u00b1  3%   75.48m \u00b1 0%   -1.84% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                        3.267\u00b5 \u00b1  2%   3.292\u00b5 \u00b1 1%        ~ (p=0.589 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8   51.88m \u00b1 10%   51.73m \u00b1 2%        ~ (p=0.818 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8     25.38m \u00b1  1%   25.73m \u00b1 4%   +1.35% (p=0.041 n=6)\r\ngeomean                                                                       6.598m         5.836m       -11.55%\r\n\r\n                                                                            \u2502     before      \u2502                after                \u2502\r\n                                                                            \u2502      B\/op       \u2502     B\/op      vs base               \u2502\r\nLabelValuesWithMatchers\/a_unique;[]-8                                            15.27Mi \u00b1 0%   15.27Mi \u00b1 0%        ~ (p=0.994 n=6)\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                        31536.43Ki \u00b1 0%   49.57Ki \u00b1 1%  -99.84% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                             2.578Ki \u00b1 0%   2.586Ki \u00b1 0%   +0.30% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                            2.344Ki \u00b1 0%   2.352Ki \u00b1 0%   +0.33% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                               3.659Ki \u00b1 1%   2.729Ki \u00b1 3%  -25.40% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8        31504.4Ki \u00b1 0%   279.8Ki \u00b1 2%  -99.11% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8          15751.9Ki \u00b1 0%   114.1Ki \u00b1 1%  -99.28% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[]-8                                        15.27Mi \u00b1 0%   15.27Mi \u00b1 0%        ~ (p=1.000 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                      30.775Mi \u00b1 0%   1.579Mi \u00b1 0%  -94.87% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                         1.562Ki \u00b1 0%   1.570Ki \u00b1 0%   +0.50% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                        1.312Ki \u00b1 0%   1.320Ki \u00b1 0%   +0.60% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                           1.625Ki \u00b1 0%   1.648Ki \u00b1 0%   +1.44% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8     30.738Mi \u00b1 0%   3.317Mi \u00b1 0%  -89.21% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8       15.377Mi \u00b1 0%   1.642Mi \u00b1 0%  -89.32% (p=0.002 n=6)\r\ngeomean                                                                          414.4Ki        75.69Ki       -81.73%\r\n\r\n                                                                            \u2502    before    \u2502                after                 \u2502\r\n                                                                            \u2502  allocs\/op   \u2502  allocs\/op   vs base                 \u2502\r\nLabelValuesWithMatchers\/a_unique;[]-8                                           4.000 \u00b1 0%    4.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                        4035.00 \u00b1 0%    16.00 \u00b1 0%  -99.60% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                            44.00 \u00b1 0%    45.00 \u00b1 0%   +2.27% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                           45.00 \u00b1 0%    46.00 \u00b1 0%   +2.22% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                              46.00 \u00b1 0%    47.00 \u00b1 0%   +2.17% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8        4.036k \u00b1 0%   4.046k \u00b1 0%   +0.25% (p=0.002 n=6)\r\nLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8          1.033k \u00b1 0%   1.034k \u00b1 0%   +0.10% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[]-8                                       3.000 \u00b1 0%    3.000 \u00b1 0%        ~ (p=1.000 n=6) \u00b9\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"]-8                    3034.00 \u00b1 0%    17.00 \u00b1 0%  -99.44% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=\"value0\"]-8                        33.00 \u00b1 0%    34.00 \u00b1 0%   +3.03% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety!=\"value0\"]-8                       33.00 \u00b1 0%    34.00 \u00b1 0%   +3.03% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[c_ninety=~\".*0\"]-8                          35.00 \u00b1 0%    37.00 \u00b1 0%   +5.71% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/a_unique;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8    3.033k \u00b1 0%   3.049k \u00b1 0%   +0.53% (p=0.002 n=6)\r\nHeadLabelValuesWithMatchers\/b_tens;[a_unique=~\".*000\"_b_tens=~\"value0\"]-8      1.030k \u00b1 0%   1.035k \u00b1 0%   +0.49% (p=0.002 n=6)\r\ngeomean                                                                         159.1         75.03       -52.83%\r\n\u00b9 all samples are equal\r\n```","comments":[],"labels":["kind\/optimization"]},{"title":"RemoteWriteBehind false alarm when all series dropped via relabel config","body":"### What did you do?\n\nWe tried recently to play around with dropping some metrics that appear to be gathered independently of the scrape phase, such as `up`, and `scrape_duration_seconds`. As in, these metrics will get remote-written even if a relabel config in the scrape phase is set up to drop them.\r\n\r\nSo for this scenario, we configured Prometheus with appropriate `write_relabel_configs` in place to drop those metrics, and also triggered a scenario where 0 other samples would be gathered from `scrape_config` sources, due to e.g. auth revocation on the target side.\r\n\r\n\r\n\r\n\n\n### What did you expect to see?\n\nIt seems like it would be nice to be able to have a timestamp which appropriately accounts for series which make it past relabeling, since if all series end up getting dropped due to write relabeling, then naturally remote writing is not falling behind, there's just nothing new to process.\n\n### What did you see instead? Under which circumstances?\n\nWe found that if 0 other samples are gathered for a processing cycle, we see that `prometheus_remote_storage_highest_timestamp_in_seconds` [from here](https:\/\/github.com\/prometheus\/prometheus\/blob\/aa7bf083e92ff68eeaa61fdd38920d355695f838\/storage\/remote\/write.go#L95) continues to increase, while `prometheus_remote_storage_queue_highest_sent_timestamp_seconds` [from here](https:\/\/github.com\/prometheus\/prometheus\/blob\/aa7bf083e92ff68eeaa61fdd38920d355695f838\/storage\/remote\/queue_manager.go#L219) remains the same.\r\n\r\nThis results in our `RemoteWriteBehind` alert firing, based in part on the difference between these two series over time, which [comes from this mixin alert](https:\/\/github.com\/prometheus\/prometheus\/blob\/aa7bf083e92ff68eeaa61fdd38920d355695f838\/documentation\/prometheus-mixin\/alerts.libsonnet#L196)\r\n\r\nDigging around the areas of code above, it seems like this is almost expected. as far as I can tell, `prometheus_remote_storage_highest_timestamp_in_seconds` is tracking the highest timestamp that enters the remote write system at all, before anything such as relabel configs are evaluated. thus it seems that the internal info metrics like up continue to come into the remote write system and update `prometheus_remote_storage_highest_timestamp_in_seconds`, before relabeling drops them.\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["I agree with your analysis.\r\n\r\nI don't think it will be trivial to fix.  The logic to drop samples is [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/d2ae8dc3cb849b165b02fcfec9fb015903d5e64b\/storage\/remote\/queue_manager.go#L586-L587), intentionally well ahead of the logic to send over remote-write.\r\nSo, if your config is dropping some samples and letting others through, and remote-write is stuck, the timestamps on dropped samples can be expected to be be ahead of sent ones.  This will defeat the alert.\r\n\r\nHowever, once all queues fill up the relabeling part will also stall, so the alert would fire.  Under reasonable scenarios this will not take too long.\r\n\r\nAnother approach would be to detect the case where all series have been dropped, and blank `prometheus_remote_storage_highest_timestamp_in_seconds` so it does not trigger the alert."],"labels":["component\/remote storage","not-as-easy-as-it-looks"]},{"title":"RFE: Log more progress information on startup","body":"### Proposal\n\nIn our environment, Prometheus takes a significant amount of time to start (over a minute) and during much of the time, it is silent in logs and otherwise about what it's doing. One reason for the slow startup and the silence may be that we have an extremely long retention period and a relatively large metrics database (around 4.9 Tbytes at the moment). The long silent gap occurs between loading the tsdb and replaying on-disk memory mappable chunks:\r\n\r\n```\r\nSep 08 09:37:26 aeschylos prometheus[877707]: ts=2023-09-08T13:37:26.123Z caller=main.go:583 level=info msg=\"Starting Prometheus Server\" mode=server version=\"(version=2.47.0, branch=HEAD, revision=efa34a5840661c29c2e362efa76bc3a70dccb335)\"\r\n[...]\r\nSep 08 09:37:26 aeschylos prometheus[877707]: ts=2023-09-08T13:37:26.148Z caller=repair.go:56 level=info component=tsdb msg=\"Found healthy block\" mint=1694167200024 maxt=1694174400000 ulid=01H9TE1NHSC9TCKNM2BF76DWDD\r\n\r\n[no log messages in the big time gap here]\r\n\r\nSep 08 09:38:32 aeschylos prometheus[877707]: ts=2023-09-08T13:38:32.020Z caller=head.go:600 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\n[...]\r\nSep 08 09:37:26 aeschylos prometheus[877707]: ts=2023-09-08T13:37:26.148Z caller=repair.go:56 level=info component=tsdb msg=\"Found healthy block\" mint=1694131200024 maxt=1694152800000 ulid=01H9T76HSJPBAB8ZJJXVZGC360\r\nSep 08 09:38:51 aeschylos prometheus[877707]: ts=2023-09-08T13:38:51.370Z caller=head.go:797 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=2.335225162s wal_replay_duration=16.117092627s wbl_replay_duration=105ns total_replay_duration=19.349808386s\r\n[...]\r\nSep 08 09:38:52 aeschylos prometheus[877707]: ts=2023-09-08T13:38:52.082Z caller=main.go:1009 level=info msg=\"Server is ready to receive web requests.\"\r\nSep 08 09:38:52 aeschylos prometheus[877707]: ts=2023-09-08T13:38:52.082Z caller=manager.go:1009 level=info component=\"rule manager\" msg=\"Starting rule manager...\"\r\n```\r\nI can provide the full startup messages if they would be useful.\r\n\r\nThis isn't a new development in 2.47.0; Prometheus has been doing this for us for years.","comments":["\ud83d\udc4b I just had a quick look at this and although I don't have an answer yet that explains where the time is being spent, I found the code that needs to be studied or instrumented with more log statements.\r\n\r\nHere is it https:\/\/github.com\/prometheus\/prometheus\/blob\/7d7b9eacffbc4da97ecfb77d7d866fd5fff00d50\/tsdb\/db.go#L752-L894\r\n\r\n`Found healthy block` is produced within `repairBadIndexVersion()` and `Replaying on-disk memory mappable chunks if any` is produced within `db.Init()`\r\n\r\n\r\nI have a question though, does it take typically a minute or is it more than that? \r\n\r\nThank you","The time can go beyond a minute, but I haven't been precisely tracking it. The most recent restart had the gap running from 11:25:33 to 11:26:54, although this was during a system reboot; the one before that was 10:13:36 to 10:14:48, then the one in the log above, and before that 13:36:16 to 13:37:19. (We don't restart Prometheus very often, only on version updates and rare system reboots.)"],"labels":["component\/tsdb"]},{"title":"Add a limit parameter to \/api\/v1\/series and \/api\/v1\/labels and \/api\/v1\/label\/{name}\/values","body":"### Proposal\r\n\r\nAdd a `limit` parameter to `\/api\/v1\/series` and `\/api\/v1\/labels` and `\/api\/v1\/label\/{name}\/values`\r\n\r\nThis was agreed at the [dev summit 2023-06-22](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit#bookmark=kix.sof7zj2b5694).\r\n\r\nIn the meeting we agreed that `\/query` and `\/query_range` could use the same feature.\r\n> CONSENSUS: We will have a LIMIT feature in the API. Ordering will not be guaranteed, and we will document this.\r\nCONSENSUS: If a LIMIT is set, and the limit is exceeded, the API will make this obvious in the response.\r\n\r\n### Motivation:\r\nAPIs like `\/labels` and `\/series` are often used to populate autocomplete; in such situations it isn't useful to have 100,000 responses, and wastes memory and CPU dealing with them.\r\nThe caller should be able to specify a maximum number of results to return.\r\n\r\n### Considerations:\r\n * If there are more than N, do we return \"any N\", \"the first N\", or just an error?\r\n\r\n### Previous discussion \r\n * #7485 \r\n * #10996\r\n * Comment recommending a limit on \/series: https:\/\/github.com\/prometheus\/prometheus\/issues\/8286#issuecomment-744824596\r\n * #10107","comments":[],"labels":["kind\/enhancement","component\/api"]},{"title":"Add LookbackDelta to ReadHints","body":"### Proposal\n\nRelated to #4192 & #4122.\r\n\r\nHey, my goal is to implement the Remote Read protocol on top of my own storage and I'm trying to reduce the number of outgoing samples to shrink a response size. Basically, I'd like to push down the downsampling operation to my storage, and apply the logic of vanilla `query_range` to filter out samples that will be excluded by caller itself. \r\n\r\nBut, both `start_timestamp_ms` in `Query` and `start_ms` in `ReadHint` contain the same value. And this value is affected by `lookbackDelta` configured on caller side, so if the original query is in the range `[t0, t1]`, remote read query will be in the range `[t0-lookbackDelta, t1]`. \r\n\r\nUnfortunately, since `lookbackDelta` is a configurable parameter, I have no idea how to calculate the original `t0` here. And without original `t0` it's impossible to reproduce a grid of the original `query_range` and align samples in the same way.  \r\n\r\nSo, the proposal is to pass `lookbackDelta` explicitly on each Remote Read request.\r\n","comments":["This looks good to me","Hi \ud83d\udc4b  first-time contributor here! if no one else is working on this, I'd like to contribute a PR. ","Hi, is this being worked on? If not I'd like to take on this task!"],"labels":["help wanted","component\/remote storage"]},{"title":"Proposal: optionally support underscore in big numbers","body":"### Proposal\n\nRecently i found a merge request to internal monitoring with scientific notation in right side of expression. That is how currently users trying to cope with big numbers to make it \"simple\" to read.\r\nGo and many languages now support input numbers with underscore in it.\r\nPlease make it possible in promql too. \r\n\r\nsupport information\r\nhttps:\/\/go.dev\/ref\/spec#Letters_and_digits\r\nhttps:\/\/peps.python.org\/pep-0515\/\r\nhttps:\/\/www.javascripttutorial.net\/es-next\/javascript-numeric-separator\/","comments":["Where exactly would you like to support them?","in promql query language \r\nexpression like \r\n`expr: metric_name > 10_000_000_000_000` \r\nsimpler to read then\r\n`expr: metric_name > 10e12` \r\n\r\n","This is quite a change, I will gather the team's opinion about it."],"labels":["component\/promql"]},{"title":"Metric proposal: prometheus_sd_dropped_targets","body":"### Proposal\n\nHey folks!\r\n\r\nMaybe it might be something already exists but I don't find anything related.\r\n\r\nWould be nice if Prometheus expose a metric `prometheus_sd_dropped_targets` or something like that which can expose the amount of targets dropped by relabel configs.\r\n\r\nThis might be useful if you're running target sharding and would like to see the target distributions between shards.","comments":["If you subtract `prometheus_target_scrape_pool_targets` from `prometheus_sd_discovered_targets` (with suitable aggregation to make the labels match), does that give you what you wanted?\r\n\r\n(I'm not opposed to adding a new specific metric)"],"labels":["component\/service discovery"]},{"title":"Support OTel delta temporality","body":"### Proposal\n\nI've outlined my thoughts here: https:\/\/docs.google.com\/document\/d\/1vMtFKEnkxRiwkr0JvVOrUrNTogVvHlcEWaWgZIqsY7Q\/edit\r\n\r\nBut I think supporting ingesting deltas instead of cumulatives will open us up to new use-cases.","comments":[],"labels":["help wanted","component\/remote storage","component\/promql","not-as-easy-as-it-looks"]},{"title":"Prometheus config reload takes 2 minutes if you remove an unreachable remote_write endpoint","body":"### What did you do?\n\n- Configure a \"remote_write\" endpoint that is not reachable\r\n- Remove that endpoint from the config\r\n- Reload using API \/reload\r\n\n\n### What did you expect to see?\n\nPrometheus reloads config in a while and web server returns 200 OK\n\n### What did you see instead? Under which circumstances?\n\nPrometheus takes 2 minutes (exactly) to reload. Web server returned 200OK after 2 minutes.\r\nIf you configure 2 endpoints and remove both of them, the reload will take 2x2 minutes.\n\n### System information\n\nLinux 5.4.0-156-generic x86_64 (container)\n\n### Prometheus version\n\n```text\nprometheus, version 2.46.0 (branch: HEAD, revision: cbb69e51423565ec40f46e74f4ff2dbb3b7fb4f0)\r\n  build user:       root@42454fc0f41e\r\n  build date:       20230725-12:31:24\r\n  go version:       go1.20.6\r\n  platform:         linux\/amd64\r\n  tags:             netgo,builtinassets,stringlabels\n```\n\n\n### Prometheus configuration file\n\n```yaml\nglobal:\r\n  scrape_interval: 15s\r\n  evaluation_interval: 15s\r\n  external_labels:\r\nremote_write:\r\n  - name: test\r\n    url: 'http:\/\/172.17.13.200'\r\n    tls_config:\r\n      insecure_skip_verify: false\r\n  \r\nrule_files:\r\n  - \/etc\/prometheus\/conf\/rules\/*.yaml\r\n\r\nalerting:\r\n  alertmanagers:\r\n  - static_configs:\r\n    - targets:\r\n      - '127.0.0.1:9093'\r\n\r\nscrape_configs:\r\n  - job_name: ****\r\n    scrape_interval: 10s\r\n    http_sd_configs:\r\n      - url: http:\/\/127.0.0.1:8089\/prometheus\r\n        refresh_interval: 60s\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nts=2023-08-25T11:56:44.635Z caller=main.go:1231 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/conf\/prometheus.yaml\r\nts=2023-08-25T11:56:44.636Z caller=dedupe.go:112 component=remote level=info remote_name=test url=http:\/\/172.17.13.200 msg=\"Stopping remote storage...\"\r\nts=2023-08-25T11:57:27.015Z caller=dedupe.go:112 component=remote level=warn remote_name=test url=http:\/\/172.17.13.200 msg=\"Failed to send batch, retrying\" err=\"Post \\\"http:\/\/172.17.13.200\\\": dial tcp 172.17.13.200:80: connect: no route to host\"\r\nts=2023-08-25T11:57:44.637Z caller=dedupe.go:112 component=remote level=error remote_name=test url=http:\/\/172.17.13.200 msg=\"non-recoverable error\" count=2000 exemplarCount=0 err=\"context canceled\"\r\nts=2023-08-25T11:57:44.658Z caller=dedupe.go:112 component=remote level=error remote_name=test url=http:\/\/172.17.13.200 msg=\"Failed to flush all samples on shutdown\" count=7999\r\nts=2023-08-25T11:57:44.659Z caller=dedupe.go:112 component=remote level=info remote_name=test url=http:\/\/172.17.13.200 msg=\"WAL watcher stopped\" queue=test\r\nts=2023-08-25T11:57:44.659Z caller=dedupe.go:112 component=remote level=info remote_name=test url=http:\/\/172.17.13.200 msg=\"Stopping metadata watcher...\"\r\nts=2023-08-25T11:58:34.482Z caller=dedupe.go:112 component=remote level=warn remote_name=test url=http:\/\/172.17.13.200 msg=\"Failed to send batch, retrying\" err=\"Post \\\"http:\/\/172.17.13.200\\\": dial tcp 172.17.13.200:80: connect: no route to host\"\r\nts=2023-08-25T11:58:44.659Z caller=dedupe.go:112 component=remote level=error remote_name=test url=http:\/\/172.17.13.200 msg=\"Failed to flush metadata\"\r\nts=2023-08-25T11:58:44.660Z caller=dedupe.go:112 component=remote level=warn remote_name=test url=http:\/\/172.17.13.200 msg=\"Failed to send batch, retrying\" err=\"Post \\\"http:\/\/172.17.13.200\\\": context canceled\"\r\nts=2023-08-25T11:58:44.660Z caller=dedupe.go:112 component=remote level=error remote_name=test url=http:\/\/172.17.13.200 msg=\"non-recoverable error while sending metadata\" count=701 err=\"context canceled\"\r\nts=2023-08-25T11:58:44.660Z caller=dedupe.go:112 component=remote level=info remote_name=test url=http:\/\/172.17.13.200 msg=\"Scraped metadata watcher stopped\"\r\nts=2023-08-25T11:58:44.712Z caller=dedupe.go:112 component=remote level=info remote_name=test url=http:\/\/172.17.13.200 msg=\"Remote storage stopped.\"\r\nts=2023-08-25T11:58:44.714Z caller=manager.go:677 level=warn component=\"rule manager\" file=\/etc\/prometheus\/conf\/rules\/smf.yaml group=smf name=smf-n4c-down index=0 msg=\"Evaluating rule failed\" rule=\"alert: smf-n4c-down\\nexpr: pfcp_node_status{target_type=\\\"smf\\\"} < 1\\nlabels:\\n  severity: warning\\nannotations:\\n  description: SMF unable to get service from UPF {{ $labels.remote_ip }}, due to\\n    connectivity failure or insufficient UPF resources.\\n  summary: SMF unable to get service from UPF {{ $labels.remote_ip }}\\n\" err=\"query timed out in expression evaluation\"\r\nts=2023-08-25T11:58:44.761Z caller=main.go:1268 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/conf\/prometheus.yaml totalDuration=2m0.126625265s db_storage=7.583\u00b5s remote_storage=2m0.076991599s web_handler=2.916\u00b5s query_engine=4.666\u00b5s scrape=401.042\u00b5s scrape_sd=282.625\u00b5s notify=23.219875ms notify_sd=104.417\u00b5s rules=24.544792ms tracing=29.75\u00b5s\n```\n","comments":["Hi @majinzin,\r\n\r\nDid you mean to remove the \"remote_write\" from the config file ? and how to reproduce the issue ?\r\n","Yes exactly, remove the remote_write endpoint from the config or eventually change only the its IP address, the result is the same: Prometheus takes 2 minutes to answer to reload API.","Will commenting this out of the  ``` prometheus\/config\/testdata\/conf.good.yml ``` fix this issue ?\r\n\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/99421476\/e9ce91cf-52c3-4951-b042-45c4a518396c)\r\n","As said, the issue is that Prometheus tries to manage gracefully the removal of remore_write endpoint even if it is not reachable waiting 2 minutes before to answer 200OK to \/reload api call. The issue is not on the config but in the code that manages this graceful shutdown: there is no reason to wait 2 minutes to try a graceful shutdown if the endpoint is non reachable. ","So where do I need to be headed to know more about this issue ? ","I'm not a Golang dev but I suspect this case needs to be handled in queue_manager.go","Alright I think I have found something to fix this but to test the changes I have to recreate this ? how do I recreate this issue so that I can see if the reload time has reduced or not .\r\n","As you can read on \"What did you do?\" section, you have to configure an \"remote_write url\" that is not reachable (use a random IP) and call \/reload API (it takes a while to return 200 code ), then you can edit the config removing remote_write endpoint and call \/reload api again and you will see that takes 2 minutes to answer 200 code. ","Hi @majinzin. I want to work on this issue. I am a first time contributor. Can I pick this up if no one is working on it any longer?\r\n@Sheikh-Abubaker are you still working on this?","@Sheikh-Abubaker go ahead if you want to work on this","Update: Was able to recreate this issue . Working on a solution will propose it soon.","> Update: Was able to recreate this issue . Working on a solution will propose it soon.\n\nI was also facing the same issue","@majinzin hi\uff0ctry turning down the `--storage.remote.flush-deadline` parameter like `--storage.remote.flush-deadline=10s`\uff0cthings should improve. ","@nishantasarma are you still working on this issue ?\r\n","@Sheikh-Abubaker go ahead if you want to pick it up","This is by design; remote write will always try to \"graceful shutdown\" and flush all of it's buffered data to the receiver. This is to avoid data loss, as when it starts up remote write will only start buffering\/sending _newly_ scraped data. The real fix here is the idea of a remote write checkpoint: https:\/\/github.com\/prometheus\/prometheus\/issues\/8809\r\n\r\nAnother potential option is to make remote writes config reload non-blocking for the rest of the config reload except in the case that remote write(s) is already in the process of reloading."," > Another potential option is to make remote writes config reload non-blocking for the rest of the config reload except in the case that remote write(s) is already in the process of reloading.\r\n\r\n@cstyan I didn't get the above part can you please explain again ?","> This is by design; remote write will always try to \"graceful shutdown\" and flush all of it's buffered data to the receiver. This is to avoid data loss, as when it starts up remote write will only start buffering\/sending _newly_ scraped data. The real fix here is the idea of a remote write checkpoint: #8809\r\n\r\nso did you mean that reducing the reload time is not a good practice and we should look for other workarounds to solve this issue ?\r\n","From my point of view, graceful shutdown that waits tcp timeout shall happen only if remote write endpoint has been used at least once since the beginning: if remote endpoint has never been reachable does not make sense to wait to flush enqueue data. ","@Sheikh-Abubaker \r\n\r\n> I didn't get the above part can you please explain again ?\r\n\r\nSure! I don't know for sure, since it's been a while since I read any of these bits of the code but hopefully this gives you some idea of where to start looking. It sounds to me like the reload endpoint is calling a bunch of `ApplyConfig` (at least, that's what remote writes is called) functions for various components, and each of these functions is blocking. Meaning that until the new config is successfully applied, we don't move on to applying the new config to the next component. If we made remote write's `ApplyConfig` function non-blocking, for example introducing a channel that we send the config over and then only blocking if remote write is still currently processing a reload config (here it the `ApplyConfig` function would block until the message over the channel is accepted), we could allow not block reloading of the rest of the components on remote write gracefully shutting down.\r\n\r\n> so did you mean that reducing the reload time is not a good practice and we should look for other workarounds to solve this issue ?\r\n\r\nMeaning the timeout for actual remote write sending; personally no, I don't think this is the correct fix _if_ we're going to make a change here. To me that's just a band aid over the actual cause.\r\n\r\n\r\n@majinzin \r\n> From my point of view, graceful shutdown that waits tcp timeout shall happen only if remote write endpoint has been used at least once since the beginning: if remote endpoint has never been reachable does not make sense to wait to flush enqueue data.\r\n\r\nSorry, but I don't agree. There's all kinds of edge cases and scenarios here. It's possible that as soon as you start the shutdown the endpoint is now suddenly available. Until we have proper checkpointing as described in https:\/\/github.com\/prometheus\/prometheus\/issues\/8809 remote write should always attempt to gracefully shutdown regardless of why it's shutting down."],"labels":["kind\/bug","component\/remote storage"]},{"title":"kubernetes_sd_config __metas for namespaces","body":"### Proposal\n\ni would like to have from namespace the metas like label.\r\n\r\nthere is just the name:\r\n`__meta_kubernetes_namespace`\r\n\r\nmy proposal would be:\r\n * `__meta_kubernetes_namespace`: keep name there for backport, or move to `__meta_kubernetes_namespace_name`\r\n * `__meta_kubernetes_namespace_uid`: The UID of the namespace object.\r\n * `__meta_kubernetes_namespace_label_<labelname>`: Each label from the namespace object.\r\n * `__meta_kubernetes_namespace_labelpresent_<labelname>`: true for each label from the namespace object.\r\n * `__meta_kubernetes_namespace_annotation_<annotationname>`: Each annotation from the namespace object.\r\n * `__meta_kubernetes_namespace_annotationpresent_<annotationname>`: true for each annotation from the namespace object.","comments":["How would you use this?\r\n\r\nWhy is this important for the project?","i would let the prometheus-operator configure relabel that i have on every metrics an `team` label based on the `__meta_kubernetes_namespace_label_team`.\r\n\r\nOn this way, i am able to set alertmanager reciever rules based on this label.\r\n\r\nThis is importent, because all resources in kubernetes are grouped in namespaces, so it would be usefull to have there label or annotations for an there metrics, too.\r\n\r\n\r\n---\r\n\r\nhere an example, howto `relabel` could be used on prometheus-config:\r\n```yaml\r\nscrape_configs:\r\n- job_name: \"example-job-name\"\r\n  honor_labels: true\r\n  honor_timestamps: true\r\n  scrape_interval: 3m\r\n  scrape_timeout: 2m\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  enable_http2: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_kubernetes_namespace_label_team]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: team\r\n    replacement: $1\r\n    action: replace\r\n```\r\n\r\n---\r\n\r\nPS: We have something equal already for services: `__meta_kubernetes_service_label_<labelname>`.\r\n\r\nthis services are also inside of an namespace."],"labels":["component\/service discovery","kind\/feature"]},{"title":"OTLP ingestion creates too many metrics","body":"### What did you do?\n\nIf we advertise that we support OTLP, the logical thing to do is to setup an simple http server and test this out:\r\n\r\nhttps:\/\/gist.github.com\/roidelapluie\/fdb0435b368ba8caba7841012e02bb8e\r\n\r\n`OTEL_EXPORTER_OTLP_PROTOCOL=\"http\/protobuf\" OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=http:\/\/127.0.0.1:9090\/api\/v1\/otlp\/v1\/metrics go run main.go`\r\n\r\n`.\/prometheus --enable-feature=otlp-write-receiver`\r\n\r\n\n\n### What did you expect to see?\n\nReasonable metrics coming into prometheus\n\n### What did you see instead? Under which circumstances?\n\nLabels like `http_user_agent`, `net_sock_peer_port`, which make a unique metric for every request.\r\n\r\n```\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33240\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33248\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33252\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33264\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33280\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33296\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33312\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33320\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33334\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"33344\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"39454\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"39464\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"39466\"}\r\n1\r\nhttp_server_duration_count{http_flavor=\"1.1\", http_method=\"GET\", http_scheme=\"http\", http_status_code=\"200\", http_user_agent=\"curl\/8.1.1\", job=\"unknown_service:otlprom\", net_host_name=\"127.0.0.1\", net_host_port=\"7777\", net_sock_peer_addr=\"127.0.0.1\", net_sock_peer_port=\"39480\"}\r\n```\r\n\r\n\r\n\n\n### System information\n\nLinux 6.3.13 x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.46.0 (branch: main, revision: 6ea6def0d3d31531696010e467bc2fb6c93edfef)\r\n  build user:       roidelapluie@nixos\r\n  build date:       19800101-00:00:00\r\n  go version:       go1.20.2\r\n  platform:         linux\/amd64\r\n  tags:             netgo,builtinassets,stringlabels\n```\n\n\n### Prometheus configuration file\n\n```yaml\nEmpty file (really)\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["https:\/\/github.com\/open-telemetry\/semantic-conventions\/blob\/main\/docs\/http\/http-metrics.md\r\n\r\nThe semantic conventions don't mention those labels, not sure why the instrumentation exports them. Will file an issue upstream.","I think this is known issue for instrumentation library `go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp` for package `net\/http`\r\n\r\nhttps:\/\/github.com\/open-telemetry\/opentelemetry-go-contrib\/issues\/3765\r\n\r\nFor now, we could only use `View` to remove those unnecessary labels which cause high cardinality, which is just like the way that opentelemetry-collector does.\r\n\r\nhttps:\/\/github.com\/open-telemetry\/opentelemetry-collector\/pull\/7543\/files\r\n\r\nIn the longterm, I think the instrumentation library should consider different label sets for span and metric, like this comment said https:\/\/github.com\/open-telemetry\/opentelemetry-go-contrib\/issues\/3765#issuecomment-1532570222","Yes, you almost never want to have the span and metric label sets bet the same. They're just different use cases. But this is also exactly why at my $dayjob we use separate libraries and calls for tracing and metrics. Having them be the same has led to too many problems.","> The semantic conventions don't mention those labels, not sure why the instrumentation exports them. Will file an issue upstream.\r\n\r\nThe [instrumentation](https:\/\/github.com\/open-telemetry\/opentelemetry-go-contrib\/blob\/main\/instrumentation\/net\/http\/otelhttp\/handler.go#L29) implements [`v1.17` of the semantic conventions](https:\/\/github.com\/open-telemetry\/opentelemetry-specification\/blob\/v1.17.0\/specification\/metrics\/semantic_conventions\/http-metrics.md).  Those conventions have been [significantly re-written](https:\/\/github.com\/open-telemetry\/semantic-conventions\/blob\/main\/docs\/http\/http-metrics.md).  The [transition process](https:\/\/github.com\/open-telemetry\/opentelemetry-specification\/pull\/3443) for these conventions directs instrumentation authors to not change their HTTP\/networking attributes until the HTTP semantic conventions are stabilized.","This is what we need to iterate the solution and become a great backend for OTEL.\r\n\r\nI assume the instrumentation SDKs can improve over time to send less high cardinality data based on configuration but on our side we can probably have a similar approach to the metric relabel config during scrapes so if SDK misses a flag or there is an issue the user can solve cardinality with the prometheus config.\r\n\r\nI was chatting with @gouthamve earlier. This is not a new problem, we've seen high cardinality issues repeatedly although from experience is mostly due to mistakes during instrumentation i.e when extracting metrics out of logs for example. This is one of the reasons why we have all the relabel configs.\r\n\r\nWhat I'm noticing here is that OTLP ingestion shares with remote write the nature of being push based and neither ingestion path applies relabeling (maybe I missed it for remote write but I was checking the code and saw nothing). \r\n\r\nSo what does everybody think about having a remote write ingestion relabel config specific to these two paths that users can use in case they have high cardinality writes?","> So what does everybody think about having a remote write ingestion relabel config specific to these two paths that users can use in case they have high cardinality writes?\r\n\r\nThere is no way to do anything useful here with relabeling. It doesn't work to drop a label like `net_sock_peer_port` - you won't get an accumulated count from all the individual metrics.","> There is no way to do anything useful here with relabeling.\r\n\r\nThere is. Drop the metrics altogether, not just a label or two. Not perfect, but it protects Prometheus from blowing up.","> There is no way to do anything useful here with relabeling. It doesn't work to drop a label like net_sock_peer_port - you won't get an accumulated count from all the individual metrics.\r\n\r\nDamn you are right. The only way to solve that is through aggregation but that gets trickier :see_no_evil: "],"labels":["help wanted"]},{"title":"build: Ability to build local arm64 images","body":"### Proposal\n\nThe instructions to build docker images locally is the following:\r\n\r\n```\r\nmake promu\r\npromu crossbuild -p linux\/amd64\r\nmake npm_licenses\r\nmake common-docker-amd64\r\n```\r\n\r\nHowever, this only builds an amd64 image even on an ARM system (like the newer Macs). We should a make target that builds native architecture images.","comments":["I've successfully managed to build the images via:\r\n```\r\nmake promu\r\npromu crossbuild -p linux\/arm64\r\nmake npm_licenses\r\nmake common-docker-arm64\r\n```\r\n\r\nI still think a single make target to do this would be great. Something like `make local-docker-image`.","\/assign"],"labels":["help wanted"]},{"title":"Document OTLP ingestion endpoint","body":"### Proposal\r\n\r\n- reopened #10547\r\n\r\n- Support [OTLP\uff08OpenTelemetry Protocol\uff09](https:\/\/opentelemetry.io\/docs\/specs\/otlp\/) ingestion.\r\n  -  [x] implement #12571 \r\n  -  [x] add docs #12643 \r\n  -  [ ] writing the guide with examples prometheus\/docs#2382\r\n\r\n\r\n\r\n- relates #12633","comments":["@jesusvazquez Can I take this up?","Thanks a lot for offering @prathamesh-sonpatki! See https:\/\/github.com\/prometheus\/prometheus\/pull\/12643 for the basic docs.\r\n\r\nCould you take up the task of writing the guide with examples? I'll be happy to guide you. Once the guide is up, we can link it from the API and featureflag docs.","Sure, let me go through #12643 and get back to you if I have any questions.","## client-side(opentelemetry-go) example\r\n\r\n[MrAlias](https:\/\/github.com\/MrAlias) has a good example.\r\n\r\nhttps:\/\/github.com\/MrAlias\/otel-otlp-metric-example\/blob\/main\/main.go\r\n\r\n## server-side example\r\n\r\n### docker compose.yml\r\n\r\n```\r\nservices:\r\n  prom:\r\n    restart: always\r\n    image: quay.io\/prometheus\/prometheus@sha256:8a1275e99ced4e896279845af495e00fb68708e56d9a857f56f459fa6261a6ec\r\n    command:\r\n      - '--config.file=\/etc\/prometheus\/prometheus.yml'\r\n      - '--enable-feature=otlp-write-receiver'\r\n    ports:\r\n      - '9090:9090'\r\n    configs:\r\n      - source: prometheus\r\n        target: \/etc\/prometheus\/prometheus.yml\r\n        mode: 440\r\n\r\nconfigs:\r\n  prometheus:\r\n    file: .\/prometheus.yml\r\n```\r\n\r\n### prometheus.yml\r\n\r\n```\r\nstorage:\r\n  tsdb:\r\n    # A 10min time window is enough because it can easily absorb retries and network delays.\r\n    out_of_order_time_window: 10m\r\n\r\nremote_write:\r\n  - url: http:\/\/localhost:9090\/api\/v1\/otlp\/v1\/metrics\r\n    queue_config:\r\n      capacity: 1000000\r\n      batch_send_deadline: 10s\r\n      max_samples_per_send: 1000\r\n      max_shards: 3\r\n    remote_timeout: \"30s\"\r\n    follow_redirects: true\r\n```\r\n","Opened https:\/\/github.com\/prometheus\/docs\/pull\/2382\/files to close part of this issue"],"labels":["help wanted","component\/documentation"]},{"title":"Discussion: Support OTLP sending in addition to Remote Write","body":"### Proposal\n\nAs OTel gains more adoption, people might be interested in sending data from Prometheus to remote services through OTLP protocol in addition the Prometheus remote write protocol.\r\n\r\nI think its a valid ask, and something that we should potentially consider. Thoughts?","comments":["+1 to this! I would love to assist in making this possible if this is acceptable. I've been working with many end users who would really love support for this exact use case.","+1 this would be great, would remove a lot of user pain and reduce complexity processing the WAL","> would remove a lot of user pain and reduce complexity processing the WAL\r\n\r\nWhich WAL are you talking about? Could you elaborate?","Ah, I spoke too soon, I was thinking of an old version of this feature that tried to consume the prom WAL directly. But I do think that supporting OTLP directly would make things more flexible for users and folks building compatible systems.","It supports in agent mode?\r\n\r\nAgent receives OTLP or prometheus and writes OTLP remotely","Nice, thanks for raising this!\r\n\r\nI'm supportive, we discussed it a few times already. To be specific--[this means OTLP metric protocol part](https:\/\/github.com\/open-telemetry\/opentelemetry-proto\/tree\/main\/opentelemetry\/proto\/metrics\/v1)(: \r\n\r\nJust note that this might require current WAL tail + remote write sharding implementation to be redesigned for metric atomicity, as histograms are atomic on the OTLP (https:\/\/github.com\/open-telemetry\/opentelemetry-proto\/blob\/main\/opentelemetry\/proto\/metrics\/v1\/metrics.proto#L384). Alternative this might only work with native histograms. \ud83d\ude43 For initial implementation, no sharding implementation might work too.\r\n\r\nNOTE: We wanted to pursue scrape atomicity of remote write anyway.\r\n\r\nWould be curious how this can evolve \ud83d\udcaa\ud83c\udffd ","+1 from someone explaining and implementing with customers this would be a huge positive for enablement.","@gouthamve did someone do this already? if not, id be happy to give it a try!","Hi, this will be discussed tomorrow at the [Prometheus dev summit](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit) as part of \"Prometheus becoming OTel native\".\r\n\r\nWe don't yet have consensus that we want this in Prometheus, but I hope to get it tomorrow. I'll update the issue after the discussion tomorrow.","@gouthamve I have some cycles this week to work on this. I saw the consensus here:\r\n> CONSENSUS: We want to invite the OTel community to contribute patches enabling Prometheus to send OTLP metrics. Ideally some of them would join us as maintainers for the long term\r\n\r\nDoes that mean I'm okay to work on an alpha?","Yes sorry, should have updated y'all here. Yes please, go for it. And tag @jesusvazquez @cstyan and me for the review?","I'm not fully understanding Bartek's comment regarding histogram atomicity, if someone from OTel could elaborate that would be helpful. Is it related to needing all series for a histogram (ie all buckets and their values) to be received at the same time? \r\n\r\nIn general I'd be against a completely separate code path just to support OTLP, and suggest we go with either of his suggested workarounds of either a) only sending native histograms or b) finding a nice hack to disable sharding if we're sending OTLP. Transactional remote write (ie per scrape atomicity of sample sending) is still something we want to implement.","Hey @jaronoff97, you may be interested in the work from https:\/\/github.com\/prometheus\/prometheus\/pull\/11640 . It is the intent to include per-sample metadata in [Remote Write 1.1](https:\/\/docs.google.com\/document\/d\/1PljkX3YLLT-4f7MqrLt7XCVPG3IsjRREzYrUzBxCPV0\/edit#heading=h.m5w2zqslg10y), which I understand should be useful for the work you are doing"],"labels":["priority\/Pmaybe","component\/remote storage"]},{"title":"Discussion: Enable 5 minutes of out-of-order by default in Prometheus","body":"### Proposal\n\nThis is an issue to kick start a discussion to switch the default configuration for the out-of-order window. \r\n\r\nToday the default value of the window is `0` **meaning no out-of-order samples are allowed into Prometheus by default**. it's up to the client to allow this use case and thus its up to the client to toggle the window and set it to a certain amount of minutes.\r\n\r\nIf we use Prometheus in the traditional way were it is pull based and it does scrapes based on a the configured interval this is completely fine and probably the out-of-order feature won't be needed.\r\n\r\nHowever if Prometheus is used with remote-write due to a specific use case, certain network topologies, OTEL ingestion, scale or other reasons then we might run into issues. It's a paradigm change where order is hard to maintain. \r\n\r\nIt is this second use case where enabling out-of-order by default to a small window like `5m` or `10m` starts making a bit of sense.\r\n- An example is using Prometheus with OTEL. Due to how OTEL clients work, encouraging users to batch up writes and then remote writing them all at once to improve compression rate, out-of-order is a natural outcome.\r\n\r\nFor any of these cases, the user needs to know about the existence of this feature and how to tweak it to avoid data loss. Maybe it does make sense to enable it to a small window to solve 99% of the race conditions between the writes?","comments":["I disagree that we should enable this by default. This is still very experimental, in regular usage of Prometheus when you need it it is showing that you have clocks instabilities, and it has bad side effects, such as those described in #11730 that would break some features in prometheus in regular usage, even with 5m delay."],"labels":["help wanted","component\/config","component\/tsdb"]},{"title":"Mark out-of-order as stable","body":"### Proposal\r\n\r\nOut-of-order has been out for a year now, and we've gained enough production experience to consider it as a stable feature.\r\n\r\nAlso out-of-order is a downstream dependency of OTEL Support because of how OTEL clients interact with Prometheus so as part of the initiative to have a better OTEL Support we need to mark this feature as stable.\r\n\r\n### Related issues\r\n\r\n- https:\/\/github.com\/prometheus\/prometheus\/issues\/11329\r\n- https:\/\/github.com\/prometheus\/prometheus\/issues\/11628\r\n- https:\/\/github.com\/prometheus\/prometheus\/pull\/11644\r\n- https:\/\/github.com\/prometheus\/prometheus\/issues\/11837\r\n- https:\/\/github.com\/prometheus\/prometheus\/pull\/12133\r\n- https:\/\/github.com\/prometheus\/prometheus\/issues\/11834\r\n- https:\/\/github.com\/prometheus\/prometheus\/pull\/11847\r\n\r\n### Other actions\r\n\r\n- Write documentation on how to enable out-of-order.\r\n- Write a Prometheus blogpost announcing out-of-order as stable.\r\n- Highlight this on the next release when it happens.","comments":["Requires #11730 "],"labels":["help wanted","component\/tsdb"]},{"title":"Handle OTEL resource attributes","body":"### Problem\r\n\r\nRelates to https:\/\/docs.google.com\/document\/d\/1epvoO_R7JhmHYsII-GJ6Yw99Ky91dKOqOtZGqX7Bk0g\/edit#heading=h.4lju4hrri2ch `Resource Attributes and their mapping` section.\r\n\r\nThis issue is intended as an umbrella issue to address the various problems that come from supporting natively OTEL resource attributes. There are two cases that we want to support:\r\n- Application pushing metrics through the OTEL Collector.\r\n- Application instrumented with OTEL SDK exposing metrics through \/metrics and Prometheus scraping them.\r\n\r\nWe want both cases to be handled equally so that the ingestion method is not part of the problem.\r\n\r\n### Proposal\r\n\r\nThe OTEL spec says that everything that is a resource attribute goes into `target_info`. One potential solution today is that Prometheus copy these attributes into series labels.\r\n\r\nOne way to do this is by setting up a config that specifies which attributes need to be mapped as labels. This config can be used for the both usecases that we have to support.","comments":["To make sure I understand this correctly,\r\n> Application instrumented with OTEL SDK exposing metrics through \/metrics and Prometheus scraping them.\r\n\r\nThis line means that app instrumented with OTEL SDK which use prometheus exporter and export the metric with `\/metrics` path, then let prometheus scrape it, right?\r\n\r\n@jesusvazquez ","I suggest we provide different ways of configuring the mapped resource's attributes\r\n\r\nFor `Application pushing metrics through the OTEL Collector`\r\n\r\nWe support users to configure by prometheus config file, we add a new config section called `otlp` and add a config item called `mapped_resource_attributes`, for example\r\n\r\n```\r\nglobal:\r\n  ...\r\nscrape_configs\r\n  ...\r\notlp:\r\n  mapped_resource_attributes: [\"service.instance.id\", \"service.version\"]\r\n```\r\n\r\nFor `Application instrumented with OTEL SDK exposing metrics through \/metrics and Prometheus scraping them`\r\n\r\nWe could add a new function to prometheus exporter package `` called `WithMappedResourceAttributes` , which support user configure the mapped resource attributes by modifying the code, for example\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\r\n\t\"github.com\/prometheus\/client_golang\/prometheus\/promhttp\"\r\n\t\"go.opentelemetry.io\/otel\/attribute\"\r\n\t\"go.opentelemetry.io\/otel\/exporters\/prometheus\"\r\n\tmetricapi \"go.opentelemetry.io\/otel\/metric\"\r\n\tmetricsdk \"go.opentelemetry.io\/otel\/sdk\/metric\"\r\n)\r\n\r\nfunc main()\r\n\r\n\treader, _ = prometheus.New(prometheus.WithMappedResourceAttributes([]string{\"service.instance.id\"}))\r\n\r\n\tprovider := metricsdk.NewMeterProvider(metricsdk.WithReader(reader))\r\n\tmeter := provider.Meter(\"github.com\/open-telemetry\/opentelemetry-go\/example\/prometheus\")\r\n\r\n\ttestInt64Counter, err := meter.Int64Counter(\"test_int64_counter\", metricapi.WithDescription(\"the counter of http requests\"))\r\n\tif err != nil {\r\n\t\tlog.Panicf(\"new test_int64_counter failed: %v\", err)\r\n\t}\r\n}\r\n\r\n```\r\n\r\nThe benefit we get from this design is that, we could simplify the implementation into just support adding configured resource attributes to other metrics when converting the metric from otlp format to prometheus format, which is easy to implement in both case.\r\n\r\nFor prometheus server, we could just modify the function\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/ffa74eb12db3d46c98033cea42cfdb347e7d1e75\/storage\/remote\/otlptranslator\/prometheusremotewrite\/metrics_to_prw.go#L28\r\n\r\nFor golang prometheus exporter package, we could just modify the function\r\nhttps:\/\/github.com\/open-telemetry\/opentelemetry-go\/blob\/8a923d0c7a7bc67bf4a209826065be0375d0b3de\/exporters\/prometheus\/exporter.go#L146\r\n\r\nI also write a simple implementation in my fork\r\nhttps:\/\/github.com\/open-telemetry\/opentelemetry-go\/commit\/0fe1edd323ac4ee8d87402d452c6c2d6e602a5d5\r\n\r\nWhereas, we could also support the secondary case by modifying the prometheus server, we should add a configuration item in `scrape_config`\r\n\r\n```\r\njob_name: test \r\nscrape_interval: 30s\r\notlp:\r\n  mapped_resource_attributes: [\"service.instance.id\"]\r\n\r\n```\r\n\r\nbut I think this configuration is more difficult to implement for now.\r\n\r\n@jesusvazquez @gouthamve looking forward for your comments!","Hello @fatsheep9146 \r\n\r\nThanks for being so proactive and having a deep look into this.\r\n\r\nYou're definitely after something here. During the last prometheus dev summit [we commented this topic](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit#heading=h.i3e0cwuipgr) and we discussed two approaches.\r\n\r\nOne is definitely what you are describing here or what we call do mapping on ingestion by copying labels. The other approach was to be smart during query time so we could join otel series with target_info data somehow without the user actually having to write the join.\r\n\r\nThe benefits of the former is that its relatively simple to implement but if tomorrow you modify the config you create new series.\r\n\r\nThe benefit of the later is that you have all labels available without creating more series but its harder to implement and perhaps maintain.\r\n\r\nI know that @gouthamve is putting some thought on this topic lately but he is on an offsite this week, we'd have to wait for him until next week.","It seems that by adding `DisableTargetInfo: false`\r\nhere: https:\/\/github.com\/prometheus\/prometheus\/blob\/7153f61790bef8b320be664e16b107a30e34c8ae\/storage\/remote\/write_handler.go#L211\r\n\r\nIt's already supported... Why not add a simple option to start ?","Hello @gillg I believe these are two different things? One is what to do with resource attributes coming from otel metrics which is still WIP in the doc above and another is the configuration option highlighted in your comment which adds suffixes to metrics. OpenMetrics specifies that metric unit needs to be added to the metric name as a suffix, this is why that configuration is set.\r\n\r\nThere is a lot of conversation going around metric names and what the default behavior should be. The most updated doc is https:\/\/docs.google.com\/document\/d\/16Wo-QHZLcKO0uFx97HPUJ-ZMFXSPviQ7Oc4-bShgcto\/edit#heading=h.gc25lho2rm57"],"labels":["help wanted","component\/config","component\/remote storage","component\/scraping","component\/api"]},{"title":"Find a less aggressive way to garbage-collect series with OOO samples only","body":"### Proposal\n\nWhen a customer has OOO-only samples, (i.e., they're too old and they have no in-order chunks in the head), those series are garbage-collected on each compaction.\r\n\r\nHowever, this doesn't mean that the customer wouldn't send the same series the second after compaction, which means that we are constantly deleting and re-creating series on each compaction.\r\n\r\nWe saw an extreme case of a customer who has 95% of their samples OOO, which means that 95% of their series are deleted on each compaction.\r\n\r\nSince deleting and re-creating the series has a cost, it would be nice to find a way to keep those series if they're likely to keep receiving OOO samples after compaction.","comments":["I think this is a valid request despite of being a scenario with such  high rate of out-of-order ingestion.\r\n\r\nWhat I'm thinking is to have an heuristic where we add a metric in the TSDB to make it aware of whats the rate of out-of-order ingestion vs in-oirder ingestion. If its high and if the out-of-order window is larger than the compaction window then retain series references for longer during gc.\r\n\r\nProbably hide this behavior behind a flag. Maybe this would suffice."],"labels":["component\/tsdb"]},{"title":"Performance: when ingesting Otel metrics, use a cache for metric and labels normalization","body":"### Proposal\n\nNow that https:\/\/github.com\/prometheus\/prometheus\/pull\/12571 has been merged, we can ingest Otel metrics directly.\r\n\r\nOtel metric names and labels are normalized with some code from the translator of OpenTelemetry Prometheus exporters. The normalization involves a lot of string operations (matching, replacing, concatenations, etc.) and is performed on-the-fly during the ingestion. This consumes significant processor cycles.\r\n\r\nAs the function that translates (metricName, metricType, metricUnit) always result with the same Prometheus metric name, we could use a cache mechanism to avoid running the normalization again and again.\r\n\r\nA thread-safe dictionary should do the trick.\r\n\r\nPotential problem: the cache could grow in size infinitely. Should it have a limit?","comments":["By normalizing OTel metric names, do you mean `prometheustranslator.BuildCompliantName`? Since this issue was opened, I have [optimized](https:\/\/github.com\/open-telemetry\/opentelemetry-collector-contrib\/pull\/29686) the translation code by generating the Prometheus metric name _per metric_, instead of per sample as it used to be. Therefore, it's less of a bottleneck than it used to be :)","On the topic of OTLP performance; I noticed from profiling that building metric identifiers is a bottleneck, and have [prototyped](https:\/\/github.com\/open-telemetry\/opentelemetry-collector-contrib\/pull\/31385) a version of the `prometheusremotewrite` package that instead hashes label sets in the way that Prometheus does. It gives a nice performance bump. I figure it might be relevant."],"labels":["help wanted","component\/remote storage"]},{"title":"Store metric metadata in TSDB","body":"### Proposal\r\n\r\nToday we store the metadata (i.e help description from metrics scrape) into a memory layer that we call the scrape cache. This implementation has been useful but has some limitations like for example not being persistent.\r\n\r\nAlso the remote write receiver can not store scrape metadata today. It does receive it but because the implementation only has access to the TSDB layer, and the metadata is not stored in the TSDB, we cannot store it.\r\n\r\nThis would also be useful for the OTLP receiver.\r\n\r\n### Prometheus team consensus\r\n\r\n[The prometheus team agreed we want to support more metadata](https:\/\/docs.google.com\/document\/d\/1yuaPKLDvhJNXMF1ubsOOm5kE2_6dvCxHowBQIDs0KdU\/edit) and a design doc to comment on is required. \r\n\r\n### Technical analysis of what needs to be done\r\n\r\nTBD","comments":["I think we store metadata in the memory right now but do not persist it.\r\n\r\nOne idea I have for persisting is, in the [Series entry](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/tsdb\/docs\/format\/index.md#series) of the block index, we can store it with the series labels like this.\r\n\r\n* Current for labels:\r\n  ```\r\n  ... | <labels count> | <key value pairs> | ...\r\n  ```\r\n\r\n* Extending it for metadata:\r\n  ```\r\n  ... | <labels count> | <key value pairs> | <metadata count> | <key value pairs> | ...\r\n  ```\r\n\r\nThis will require an index version bump.","However, metadata can change over time, which (currently) doesn't change the series identity.\r\n\r\nAlso, there might be more use cases for adding more metadata (that might be change over time, too)."],"labels":["help wanted","component\/remote storage","not-as-easy-as-it-looks","kind\/more-info-needed","component\/tsdb"]},{"title":"Caching all symbols read from chunks","body":"## Summary\r\nThis MR adds a performance enhancement- caching _all_ symbols read from a chunk. This was initially designed to benefit reading high-cardinality series (where a relatively small number of symbols would be referenced by many different series) but it seems this even improves performance for querying a single series.\r\n\r\n## Benchmark\r\nA sample benchmark is provided for discussion purposes. It references a chunk from our production Prometheus instances that I have not included in this MR. This chunk contains two hours of data and many metrics- among them is `bid_request_count`, a metric with about 150K series. \r\n\r\n#### Analysis\r\nQuery execution time improvement: 13-27% faster\r\nBytes alloc per query:  6%-10% less\r\n\r\n#### Raw Output\r\nWithout caching all symbols:\r\n```\r\nGOROOT=C:\\Program Files\\Go #gosetup\r\nGOPATH=C:\\Users\\james.luck\\go #gosetup\r\n\"C:\\Program Files\\Go\\bin\\go.exe\" test -c -o C:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___6gobench_example_test_go.test.exe github.com\/prometheus\/prometheus\/promql #gosetup\r\nC:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___6gobench_example_test_go.test.exe -test.v -test.paniconexit0 -test.bench ^\\QBenchmarkCountSingleInstance\\E|\\QBenchmarkCountAll\\E|\\QBenchmarkCountSingleInstanceOver1h\\E|\\QBenchmarkCountAllOver1h\\E$ -test.run ^$ -test.benchtime 45s -test.benchmem #gosetup\r\ngoos: windows\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\ncpu: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz\r\nBenchmarkCountSingleInstance\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nBenchmarkCountSingleInstance-12                    14364           3713313 ns\/op\r\n          960977 B\/op      13273 allocs\/op\r\nBenchmarkCountAll\r\nPeak samples: 311715, Total samples: 155857\r\nPeak samples: 311715, Total samples: 155857\r\nBenchmarkCountAll-12                                  63         714498584 ns\/op\r\n        203373190 B\/op   2651373 allocs\/op\r\nBenchmarkCountSingleInstanceOver1h\r\nPeak samples: 1545, Total samples: 92640\r\nPeak samples: 1545, Total samples: 92640\r\nPeak samples: 1545, Total samples: 92640\r\nBenchmarkCountSingleInstanceOver1h-12               7848           6283421 ns\/op\r\n         1324687 B\/op      14101 allocs\/op\r\nBenchmarkCountAllOver1h\r\nPeak samples: 311715, Total samples: 18700202\r\nPeak samples: 311715, Total samples: 18700202\r\nBenchmarkCountAllOver1h-12                            38        1247905737 ns\/op\r\n        275807519 B\/op   2808082 allocs\/op\r\nPASS\r\n\r\nProcess finished with the exit code 0\r\n```\r\n\r\nWith caching all symbols:\r\n```\r\nGOROOT=C:\\Program Files\\Go #gosetup\r\nGOPATH=C:\\Users\\james.luck\\go #gosetup\r\n\"C:\\Program Files\\Go\\bin\\go.exe\" test -c -o C:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___6gobench_example_test_go.test.exe github.com\/prometheus\/prometheus\/promql #gosetup\r\nC:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___6gobench_example_test_go.test.exe -test.v -test.paniconexit0 -test.bench ^\\QBenchmarkCountSingleInstance\\E|\\QBenchmarkCountAll\\E|\\QBenchmarkCountSingleInstanceOver1h\\E|\\QBenchmarkCountAllOver1h\\E$ -test.run ^$ -test.benchtime 45s -test.benchmem #gosetup\r\ngoos: windows\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\ncpu: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz\r\nBenchmarkCountSingleInstance\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nPeak samples: 1545, Total samples: 772\r\nBenchmarkCountSingleInstance-12                    17977           2917263 ns\/op\r\n          872657 B\/op       7096 allocs\/op\r\nBenchmarkCountAll\r\nPeak samples: 311715, Total samples: 155857\r\nPeak samples: 311715, Total samples: 155857\r\nPeak samples: 311715, Total samples: 155857\r\nBenchmarkCountAll-12                                  98         541630717 ns\/op\r\n        184982530 B\/op   1403638 allocs\/op\r\nBenchmarkCountSingleInstanceOver1h\r\nPeak samples: 1545, Total samples: 92640\r\nPeak samples: 1545, Total samples: 92640\r\nPeak samples: 1545, Total samples: 92640\r\nBenchmarkCountSingleInstanceOver1h-12               9118           5558946 ns\/op\r\n         1235009 B\/op       7923 allocs\/op\r\nBenchmarkCountAllOver1h\r\nPeak samples: 311715, Total samples: 18700202\r\nPeak samples: 311715, Total samples: 18700202\r\nBenchmarkCountAllOver1h-12                            43        1063583444 ns\/op\r\n        257784993 B\/op   1560519 allocs\/op\r\nPASS\r\n\r\nProcess finished with the exit code 0\r\n```\r\n\r\n","comments":["I believe the point of the current design is to balance peak memory usage against speed.\r\nCan you show the peak memory of your two benchmark cases?","Hey @bboreham thanks for the insight. Go isn't my primary language- is there an easy way to measure what you're after? I checked the `go test` flags and saw nothing around peak memory usage. ","On Linux `\/bin\/time` will give you the peak memory.\r\nAnother option would be to set the environment variable `GODEBUG=gctrace=1` which prints out statistics on every garbage-collect, and inspect by eye where the peak is.\r\n\r\nYet another option would be to add at the end of your test:\r\n```\r\n\tvar memstats runtime.MemStats\r\n\truntime.ReadMemStats(&memstats)\r\n\tb.ReportMetric(float64(memstats.Sys), \"sys_memory\")\r\n```\r\n\r\n(There is some chance that the Go runtime will have handed memory back to the OS before this runs, but for a short test I wouldn't expect it)\r\n\r\nIncidentally if you use the `ReportMetric` function instead of `Printf` then your output will be a bit tidier.\r\n```\r\n\tif n == 0 {\r\n\t\tb.ReportMetric(float64(q.Stats().Samples.PeakSamples), \"peak_samples\")\r\n\t\tb.ReportMetric(float64(q.Stats().Samples.TotalSamples), \"total_samples\")\r\n\t}\r\n```","Thanks for the tips, I went with the `ReadMemStats` approach and have pushed an updated version of the benchmark. The results show peak memory usage is lower with my change. \r\n\r\nBaseline:\r\n```\r\nGOROOT=C:\\Program Files\\Go #gosetup\r\nGOPATH=C:\\Users\\james.luck\\go #gosetup\r\n\"C:\\Program Files\\Go\\bin\\go.exe\" test -c -o C:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___1gobench_cache_symbols_test_go.test.exe github.com\/prometheus\/prometheus\/promql #gosetup\r\nC:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___1gobench_cache_symbols_test_go.test.exe -test.v -test.paniconexit0 -test.bench ^\\QBenchmarkCountSingleInstance\\E|\\QBenchmarkCountAll\\E|\\QBenchmarkCountSingleInstanceOver1h\\E|\\QBenchmarkCountAllOver1h\\E$ -test.run ^$ -test.benchtime 45s -test.benchmem #gosetup\r\ngoos: windows\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\ncpu: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz\r\nBenchmarkCountSingleInstance\r\nBenchmarkCountSingleInstance-12                    14420           3766871 ns\/op\r\n              1545 peak_samples   41761448 sys_memory          772.0 total_sampl\r\nes        969158 B\/op      13275 allocs\/op\r\nBenchmarkCountAll\r\nBenchmarkCountAll-12                                  73         752897966 ns\/op\r\n            311715 peak_samples  389749672 sys_memory       155857 total_samples\r\n        204536969 B\/op   2651493 allocs\/op\r\nBenchmarkCountSingleInstanceOver1h\r\nBenchmarkCountSingleInstanceOver1h-12               7291           6506170 ns\/op\r\n              1545 peak_samples  389749672 sys_memory        92640 total_samples\r\n         1342987 B\/op      14105 allocs\/op\r\nBenchmarkCountAllOver1h\r\nBenchmarkCountAllOver1h-12                            40        1241468748 ns\/op\r\n            311715 peak_samples  417089672 sys_memory     18700202 total_samples\r\n        278489576 B\/op   2808500 allocs\/op\r\nPASS\r\n\r\nProcess finished with the exit code 0\r\n\r\n```\r\n\r\nWith my change:\r\n```\r\nGOROOT=C:\\Program Files\\Go #gosetup\r\nGOPATH=C:\\Users\\james.luck\\go #gosetup\r\n\"C:\\Program Files\\Go\\bin\\go.exe\" test -c -o C:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___1gobench_cache_symbols_test_go.test.exe github.com\/prometheus\/prometheus\/promql #gosetup\r\nC:\\Users\\james.luck\\AppData\\Local\\JetBrains\\GoLand2023.1\\tmp\\GoLand\\___1gobench_cache_symbols_test_go.test.exe -test.v -test.paniconexit0 -test.bench ^\\QBenchmarkCountSingleInstance\\E|\\QBenchmarkCountAll\\E|\\QBenchmarkCountSingleInstanceOver1h\\E|\\QBenchmarkCountAllOver1h\\E$ -test.run ^$ -test.benchtime 45s -test.benchmem #gosetup\r\ngoos: windows\r\ngoarch: amd64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\ncpu: Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz\r\nBenchmarkCountSingleInstance\r\nBenchmarkCountSingleInstance-12                    17674           2880361 ns\/op\r\n              1545 peak_samples   41761448 sys_memory          772.0 total_sampl\r\nes        879595 B\/op       7097 allocs\/op\r\nBenchmarkCountAll\r\nBenchmarkCountAll-12                                  97         545361978 ns\/op\r\n            311715 peak_samples  362651336 sys_memory       155857 total_samples\r\n        186201756 B\/op   1403854 allocs\/op\r\nBenchmarkCountSingleInstanceOver1h\r\nBenchmarkCountSingleInstanceOver1h-12               8211           5587753 ns\/op\r\n              1545 peak_samples  362651336 sys_memory        92640 total_samples\r\n         1251706 B\/op       7927 allocs\/op\r\nBenchmarkCountAllOver1h\r\nBenchmarkCountAllOver1h-12                            43        1079410993 ns\/op\r\n            311715 peak_samples  376868152 sys_memory     18700202 total_samples\r\n        260490145 B\/op   1560987 allocs\/op\r\nPASS\r\n\r\nProcess finished with the exit code 0\r\n```","That's certainly interesting. \r\n\r\nI would guess there is some point at which we don't want to cache every string in heap, for the lifetime of a block.\r\n\r\nNapkin math: suppose someone has stored a million different GUIDs in a label. That's 36 bytes of content plus 16 bytes of header, plus some overhead, call it 64 bytes each, so 64MB, which Go will double to 128MB for GC.  Say you have one block for each day going back 15 days is 2GB.\r\n\r\nIt doesn't sound out of the question.\r\n\r\nLet's see what prombench says:\r\n\r\n\/prombench main","\/prombench main","\u23f1\ufe0f Welcome to Prometheus Benchmarking Tool. \u23f1\ufe0f\n\n**Compared versions:** [**`PR-12587`**](http:\/\/prombench.prometheus.io\/12587\/prometheus-pr) and [**`main`**](http:\/\/prombench.prometheus.io\/12587\/prometheus-release)\n\nAfter successful deployment, the benchmarking metrics can be viewed at:\n\n- [Prometheus Meta](http:\/\/prombench.prometheus.io\/prometheus-meta\/graph?g0.expr={namespace%3D\"prombench-12587\"}&g0.tab=1)\n- [Prombench Dashboard](http:\/\/prombench.prometheus.io\/grafana\/d\/7gmLoNDmz\/prombench?orgId=1&var-pr-number=12587)\n- [Grafana Explorer, Loki logs](http:\/\/prombench.prometheus.io\/grafana\/explore?orgId=1&left=[\"now-6h\",\"now\",\"loki-meta\",{},{\"mode\":\"Logs\"},{\"ui\":[true,true,true,\"none\"]}])\n\n**Other Commands:**\nTo stop benchmark: `\/prombench cancel`\nTo restart benchmark: `\/prombench restart main`\n","\/prombench cancel\r\n","Benchmark cancel is in progress.\n","Looks like memory is 15-20% higher.\r\n\r\n<img width=\"404\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/8125524\/aa86e62f-1a4f-462e-807a-5eedba9947de\">\r\n","Hey @bboreham, thanks for the analysis. One thing: I'm seeing lower memory consumption when I look at the \"Used\" node metric:\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/1540536\/fe23d64c-ebf5-4717-9f63-851ea2a1ba0c)\r\n","Hi, I wanted to mention #12304, in which I give every `Labels` a symbol table; this could change the analysis.\r\n\r\nHowever I don't think we would ever want to pre-emptively read every string from every block, because the extreme cases would be too expensive.\r\n\r\nDiscussed at the bug scrub."],"labels":["prombench"]},{"title":"feat: add sample_limit() and sample_ratio() operators","body":"Implement sample_limit(N, metric) and sample_ratio(R, metric):\r\n\r\n* `sample_limit(N, metric)` returns the 1st N samples gathered from metric, _without_ any particular order\r\n* `sample_ratio(R, metric)` with `0 <= R <= 1.0` returns approximately `R` ratio of samples\r\n\r\nSometimes you want to \"sample\" some timeseries, for example to explore\r\ntheir labels names and values, currently the only operator available is\r\n`topk(K, metric)` (and `bottomk() of course), needing to \"pay\" for its\r\nordering feature by visiting all samples, and sorting the topk-s.\r\n\r\nThis change introduces `sample_limit(N, metric)` which _just_ samples\r\nthe first points visited without any further\/guaranteed ordering, and\r\nalso `sample_ratio(R, metric)` for cases where you want a more \"fair\"\r\ndistribution of the samples, using `R` as a \"proxy\" for an approximate\r\nsampling ratio, which uses `Metrics.Hash()\/MaxUint64` as the sample\r\nratio, and select the sample if its ratio is lesser than `R`. \r\n\r\nAn `promql\/engine.go` excerpt to expand on `sample_ratio()`:\r\n(NOTE: `sampleOffset()` returns `sample.Metric.Hash()) \/ MaxUint64`)\r\n```\r\n    switch {\r\n    case ratioLimit >= 0:\r\n        \/\/ If ratioLimit >= 0: add sample if ratiosampler.sampleOffset() is lesser than ratioLimit\r\n        \/\/\r\n        \/\/ 0.0        ratioLimit                1.0\r\n        \/\/  [---------|--------------------------]\r\n        \/\/  [#########...........................]\r\n        \/\/\r\n        \/\/ e.g.:\r\n        \/\/   ratiosampler.sampleOffset()==0.3 && ratioLimit==0.4\r\n        \/\/     0.3 < 0.4 ? --> add sample\r\n        \/\/\r\n        if sampleOffset < ratioLimit {\r\n            heap.Push(h, sample)\r\n        }\r\n    case ratioLimit < 0:\r\n        \/\/ If ratioLimit < 0: add sample if rand() return the \"complement\" of ratioLimit>=0 case\r\n        \/\/ (loosely similar behavior to negative array index in other programming languages)\r\n        \/\/\r\n        \/\/ 0.0       1+ratioLimit               1.0\r\n        \/\/  [---------|--------------------------]\r\n        \/\/  [.........###########################]\r\n        \/\/\r\n        \/\/ e.g.:\r\n        \/\/   ratiosampler.sampleOffset()==0.3 && ratioLimit==-0.6\r\n        \/\/     0.3 >= 0.4 ? --> don't add sample\r\n        if sampleOffset >= (1.0 + ratioLimit) {\r\n            heap.Push(h, sample)\r\n        }\r\n    }\r\n```\r\n\r\nTesting\r\n-------\r\n* added unit tests to promql\/engine_test.go\r\n* added tests to promql\/bench_test.go\r\n\r\nWeb\r\n---\r\n* added web\/ui\/ artifacts\r\n\r\nNB: `sample_ratio()` was previously `sample_random()`, changed to a deterministic implementation, as per PR discussion.\r\n\r\nSigned-off-by: JuanJo Ciarlante <juanjosec@gmail.com>\r\n","comments":["Thanks for your pull request. Deterministic results are a key factor of PromQL, so limit_random does not fit. We decided in our last dev summit to implement limit, but at the API level, for all API endpoints. Not in PromQL.","> Thanks for your pull request. Deterministic results are a key factor of PromQL, so limit_random does not fit. We decided in our last dev summit to implement limit, but at the API level, for all API endpoints. Not in PromQL.\r\n\r\nTo add to that, we decided to make the API-level limits non-deterministic to begin with (i.e. it's ok to return an arbitrary subset), but might potentially make them more deterministic in the future.\r\n\r\nBut your PR's motivations does raise the questions:\r\n\r\n* If such a limit is applied overall to a PromQL query, can we make it affect the PromQL evaluation in such a way that we don't have to compute all the underlying results if most of them are thrown away anyway (important only for queries selecting huge numbers of metrics). At least for bare selectors without anything wrapping them, this should be an easy thing to do.\r\n* Should we add an option to make the results *explicitly* random. I guess your hope for that would be to possibly get a more \"fair\" look at the data e.g. if the ordered results are always not representative in the way you care about, but randomness of course can also never really guarantee anything, so I'm not sure about that yet.","> Thanks for your pull request. Deterministic results are a key factor of PromQL, so limit_random does not fit.\r\n\r\nThanks for the reply, I ab-used the `Randomizer` interface to implement a non-random\/deterministic\r\nHashRandomizer that returns `[0.0, 1.0]` as `labels.Hash() \/ MaxUint64` at 181b34b\r\n\r\n> We decided in our last dev summit to implement limit, but at the API level, for all API endpoints. Not in PromQL.\r\n\r\nIMO there may still be use cases for limit at PromQL, it would also not need any UI artifact to be used (like e.g. an input field to specific such limit value).\r\n\r\n","> > Thanks for your pull request. Deterministic results are a key factor of PromQL, so limit_random does not fit. We decided in our last dev summit to implement limit, but at the API level, for all API endpoints. Not in PromQL.\r\n> \r\n> To add to that, we decided to make the API-level limits non-deterministic to begin with (i.e. it's ok to return an arbitrary subset), but might potentially make them more deterministic in the future.\r\n> \r\n> But your PR's motivations does raise the questions:\r\n> \r\n> * If such a limit is applied overall to a PromQL query, can we make it affect the PromQL evaluation in such a way that we don't have to compute all the underlying results if most of them are thrown away anyway (important only for queries selecting huge numbers of metrics). At least for bare selectors without anything wrapping them, this should be an easy thing to do.\r\n\r\nIf I get you correctly, I've already implemented it at (i.e. `continue` after filling the heap with `k` elements): \r\nhttps:\/\/github.com\/jjo\/prometheus\/blob\/181b34b9a921cbf38040feb2114d3a1f7cdfb338\/promql\/engine.go#L2763-L2772\r\n\r\n> * Should we add an option to make the results _explicitly_ random. I guess your hope for that would be to possibly get a more \"fair\" look at the data e.g. if the ordered results are always not representative in the way you care about, but randomness of course can also never really guarantee anything, so I'm not sure about that yet.\r\n\r\nActually, I went the other way, adding a deterministic HashRandomizer (please mind the Rand in the name :P), that returns a float64 in `[0.0, 0.1]` range as `labels.Hash() \/ MaxUint64`, the used it as implementation of the `Randomizer` interface I had added, see 181b34b.\r\n\r\nAs an interesting feature, also added the possibility to specify _negative_ `p` numbers in `sample_random(p-1.0, metric)`, which will return \"the complement\" (set of timeseries) than its `sample_random(p, metric)` counterpart, excerpt from the code:\r\n\r\n```\r\n    switch {\r\n    case ratioLimit >= 0:\r\n        \/\/ If ratioLimit >= 0: add sample if ratiosampler.sampleOffset() is lesser than ratioLimit\r\n        \/\/\r\n        \/\/ 0.0        ratioLimit                1.0\r\n        \/\/  [---------|--------------------------]\r\n        \/\/  [#########...........................]\r\n        \/\/\r\n        \/\/ e.g.:\r\n        \/\/   ratiosampler.sampleOffset()==0.3 && ratioLimit==0.4\r\n        \/\/     0.3 < 0.4 ? --> add sample\r\n        \/\/\r\n        if sampleOffset < ratioLimit {\r\n            heap.Push(h, sample)\r\n        }\r\n    case ratioLimit < 0:\r\n        \/\/ If ratioLimit < 0: add sample if rand() return the \"complement\" of ratioLimit>=0 case\r\n        \/\/ (loosely similar behavior to negative array index in other programming languages)\r\n        \/\/\r\n        \/\/ 0.0       1+ratioLimit               1.0\r\n        \/\/  [---------|--------------------------]\r\n        \/\/  [.........###########################]\r\n        \/\/\r\n        \/\/ e.g.:\r\n        \/\/   ratiosampler.sampleOffset()==0.3 && ratioLimit==-0.6\r\n        \/\/     0.3 >= 0.4 ? --> don't add sample\r\n        if sampleOffset >= (1.0 + ratioLimit) {\r\n            heap.Push(h, sample)\r\n        }\r\n    }\r\n```\r\n\r\nThis feature could also open some interesting use cases, as it allows subsetting the timeseries by a \"pivot\" value, returning a deterministic \"complement\"  of timeseries (depending on their labels' hash).\r\n\r\nOf course, if this latest change makes sense, I'd rename `sample_random()` to `sample_ratio()` or alike.","> Actually, I went the order way, adding a deterministic HashRandomizer (please mind the Rand in the name :P), that returns a float64 in `[0.0, 0.1]` range as `labels.Hash() \/ MaxUint64`, the used it as implementation of the `Randomizer` interface I had added, see [181b34b](https:\/\/github.com\/prometheus\/prometheus\/commit\/181b34b9a921cbf38040feb2114d3a1f7cdfb338).\r\n> \r\n> As an interesting feature, also added the possibility to specify _negative_ `p` numbers in `sample_random(-p, metric)`, which will return \"the complement\" (set of timeseries) than its `sample_random(p, metric)` counterpart, excerpt from the code:\r\n\r\n@juliusv you can see this \"in action\" at the [promql\/engine_test.go test case](https:\/\/github.com\/jjo\/prometheus\/blob\/181b34b9a921cbf38040feb2114d3a1f7cdfb338\/promql\/engine_test.go#L1724-L1767) I added.","We have discussed this during our weekly bug scrub.\r\n\r\nWe would like to better understand the gap between implementing this in PromQL and the implementation of a limit directly at the API level, maybe in a broader design document gathering use cases.\r\n\r\nFor now, as we agreed with implementing this at the API level, we will leave this PR open until we can better perceive the added value of this pull request.","We will also discuss this topic at our next Dev Summit in Berlin, at the end of September.\r\n\r\nThanks!","> We will also discuss this topic at our next Dev Summit in Berlin, at the end of September.\r\n> \r\n> Thanks!\r\n\r\nThanks Julien!, appreciate the details and context. :) \r\n\r\nNote that API limit would be the ~equivalent of `sample_limit()`, but `sample_ratio(p, metrics)` (as per my update above, using `labels Hash()\/MaxUint64` to compare against against `p`) is quite different.\r\n\r\nDo you think it would be worth fixing this PR to narrow it to `sample_ratio()` implementation only?\r\nCan also work on a design\/use-case docu if needed.","Sadly, we still haven't discussed this at the dev summit, see [the relevant agenda item](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#bookmark=id.pr0js4n43f6z).\r\n\r\nOn a more general not, we now have [a feature flag for experimental PromQL functions](https:\/\/github.com\/prometheus\/prometheus\/pull\/13103), so maybe just put this behind the flag, and we can merge it without a super deep discussion? @roidelapluie WDYT?","Yes, let's do it behind the flag.","> Yes, let's do it behind the flag.\r\n\r\nThanks folks for the feedback, will do.\r\n\/cc @beorn7 ","Update from the dev summit: We discussed this, there are significant doubts about how needed or helpful this would be, but we all agreed that this is why we have the feature flag (so that we can let people try things out and make a call later based on the experience). So we are generally fine merging this once the feature flag is in the code and a code level review has happened.","We have consensus to have this behind flag (on DevSummit), but I think we have some suggestions for the name. To me it seems this mimics \"topk\" just without priority queue. Topk is kind of \"series\" limit not sample \ud83d\ude48 thus \"sample_limit\" might be ambiguous -- does it mean vertical samples (series) or horizontal (e.g. with over_time).\r\n\r\ncc @bboreham @jan--f had some opinions.\r\n\r\nEDIT @beorn7 got me to this update \ud83d\ude48 "],"labels":["priority\/Pmaybe"]},{"title":"Using Prometheus SD as a library did not work as expected","body":"### What did you do?\n\nIn version 2.44.0, service discovery updates the targetGroup for some deleted resources instead of deleting them, which can cause m.targets to grow larger and larger. m.targets is a map type, which can cause the gc scanobject to take longer and longer, leading to an increase in CPU usage.\r\n\r\nThe problem occurred in this line:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/v2.44.0\/discovery\/manager.go#L390\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/3193b5e4-9b41-47ac-88e3-baf5e40a3cc7)\r\nFor deleted resources, imformer will return and only update but not delete in the updateGroup function\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/v2.44.0\/discovery\/kubernetes\/pod.go#L161\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/e9c95c3a-4df7-4c33-a22c-84abb16ba9fb)\r\n\n\n### What did you expect to see?\n\n_No response_\n\n### What did you see instead? Under which circumstances?\n\nAs you can see, the CPU and memory are constantly increasing\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/83611bf0-d955-40b5-948e-66fee0ce9440)\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/2e5d3608-6c6d-438c-a8a0-a2485fedaa66)\r\n\r\nBy printing m.targets with dlv, it was found that there are many deleted resources\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/10435981-f69b-49da-9661-e6b6685f7c71)\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/c5aaca36-935b-4709-b7c3-07f336a45bb0)\r\n\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\nv2.44.0\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Are you saying this changed in 2.44?  The lines you point to have been the same for 6 years and 3 years respectively.\r\n\r\nWhat exactly do you mean when you say \"deleted\"? \r\nWhat kind of service discovery are you using?","> Are you saying this changed in 2.44? The lines you point to have been the same for 6 years and 3 years respectively.\r\n> \r\n> What exactly do you mean when you say \"deleted\"? What kind of service discovery are you using?\r\n\r\nI am currently using version 2.44, and using kubernetes pod service discovery. I found the same issue in other versions, such as 2.35.0, which are the two versions I am currently using.\r\n\r\nBy 'deleted', I mean the k8s pod is deleted, and the deletion event can also be sensed by the pod service discovery. At this time, the `updateGroup` function in `discovery\/manager.go` will update `m.targets`. In my understanding, should we remove it from `m.targets`? If not, frequent deletion and rebuilding of pods will cause `m.targets` to keep growing. For example:\r\n\r\nI have a \"test\" scrape job here that uses k8s pod service discovery to find the target of pods in the `dev2` namespace. Currently, there is only one pod named `mc-gz-dev2-trtc-aggregate-mc-9d545bbb7-2nfzf` in `dev2`. The data structure in the `m.targets` of the `updateGroup` function like:\r\n```\r\n{setName:\"test\", provider: \"kubernetes\/1\": [\r\n \t\"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-9d545bbb7-2nfzf\": {\r\n \t\tTargets: [...],\r\n \t\tLabels: [...],\r\n \t\tSource: \"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-9d545bbb7-2nfzf\"\r\n\t}\r\n ]}\r\n```\r\n\r\nAt this point, I deleted this pod, and k8s will create a new pod named `mc-gz-dev2-trtc-aggregate-mc-58df787744-wzgzw`. The data structure of `m.targets` at this time like:\r\n```\r\n{setName:\"test\", provider: \"kubernetes\/1\": [\r\n \t\"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-9d545bbb7-2nfzf\": {\r\n \t\tTargets: nil,\r\n \t\tLabels: nil,\r\n \t\tSource: \"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-9d545bbb7-2nfzf\"\r\n \t},\r\n \t\"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-58df787744-wzgzw\": {\r\n \t\tTargets: [...],\r\n \t\tLabels: [...],\r\n \t\tSource: \"pod\/dev2\/mc-gz-dev2-trtc-aggregate-mc-58df787744-wzgzw\"\r\n \t},\r\n ]}\r\n```\r\nIn fact, the deleted pod still exists in `m.targets`. Currently, it seems that `m.targets` only increases and does not decrease.\r\n\r\n\r\n\r\n","Thank you for including more details.  I retitled the issue as far as I understand your description.\r\n\r\nI tried to find the same symptom in a Prometheus running at my work, but did not.\r\nI note that the first line you point to is not used unless `new-service-discovery-manager` is specified.\r\nBy default a different implementation in [discovery\/legacymanager\/manager.go](https:\/\/github.com\/prometheus\/prometheus\/blob\/2f58be840d06e647dfdbfa952e3451c0b6afee50\/discovery\/legacymanager\/manager.go) is used.\r\n\r\nPlease can you supply more details about your config.","> Thank you for including more details. I retitled the issue as far as I understand your description.\r\n> \r\n> I tried to find the same symptom in a Prometheus running at my work, but did not. I note that the first line you point to is not used unless `new-service-discovery-manager` is specified. By default a different implementation in [discovery\/legacymanager\/manager.go](https:\/\/github.com\/prometheus\/prometheus\/blob\/2f58be840d06e647dfdbfa952e3451c0b6afee50\/discovery\/legacymanager\/manager.go) is used.\r\n> \r\n> Please can you supply more details about your config.\r\n\r\nThanks for your response. I'm not directly using Prometheus, but importing the [Prometheus package](https:\/\/pkg.go.dev\/github.com\/prometheus\/prometheus) in my project and using its service discovery capabilities.\r\n\r\nI have tried [discovery\/manager.go](https:\/\/github.com\/prometheus\/prometheus\/blob\/v0.44.0\/discovery\/manager.go) and [legacymanager\/manager.go](https:\/\/github.com\/prometheus\/prometheus\/blob\/v0.44.0\/discovery\/legacymanager\/manager.go), but both have the same issue. I'll share my code, configuration, and the steps to reproduce the issue. My code is as follows:\r\n```\r\nimport (\r\n    prom_discovery \"github.com\/prometheus\/prometheus\/discovery\"\r\n)\r\n...\r\ndiscoveryManagerScrape := prom_discovery.NewManager(ctx, log.With(logger, \"component\", \"discovery manager scrape\"), prom_discovery.Name(\"scrape\"))\r\n\r\ng := errgroup.Group{}\r\ng.Go(func() error {\r\n    lg.Infof(\"SD start\")\r\n    return discoveryManagerScrape.Run()\r\n})\r\n\r\ng.Go(func() error {\r\n    lg.Infof(\"targetDiscovery start\")\r\n    return target.Run(ctx, discoveryManagerScrape.SyncCh())\r\n})\r\n...\r\n```\r\nThe steps to reproduce are as follows:\r\n- I created a scrape job `test-grafana`, this job will scrape pods with the namespace `infrastore-common` and the label `name=infrastore-grafana`. The configuration is as follows:\r\n```\r\n- job_name: test-grafana\r\n  honor_timestamps: true\r\n  scrape_interval: 15s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n    namespaces:\r\n      names:\r\n      - infrastore-common\r\n  relabel_configs:\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_label_name\r\n    separator: ;\r\n    regex: infrastore-grafana\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: namespace\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: pod\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_node_name\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: node\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: workload_type\r\n    replacement: deployment\r\n    action: replace\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: workload_name\r\n    replacement: infrastore-grafana\r\n    action: replace\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_ip\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: __address__\r\n    replacement: $1:3000\r\n    action: replace\r\n```\r\nThe namespace `infrastore-common` currently has two pods:\r\n```\r\n# kubectl get pod -n infrastore-common -l name=infrastore-grafana\r\nNAME                                  READY   STATUS    RESTARTS   AGE\r\ninfrastore-grafana-5c7c8468c8-f4v7l   1\/1     Running   0          66s\r\ninfrastore-grafana-5c7c8468c8-hl6s2   1\/1     Running   0          28h\r\n```\r\nBy using `dlv` to inspect the memory stack of service discovery, there are also two pods:\r\n```\r\n(dlv) b discovery\/manager.go:397\r\n(dlv) c\r\n(dlv) list\r\n   392:\t\t\t\t} else {\r\n   393:\t\t\t\t\tm.targets[poolKey][tg.Source] = tg\r\n   394:\t\t\t\t}\r\n   395:\t\t\t}\r\n   396:\t\t}\r\n=> 397:\t}\r\n   398:\t\r\n   399:\tfunc (m *Manager) allGroups() map[string][]*targetgroup.Group {\r\n   400:\t\ttSets := map[string][]*targetgroup.Group{}\r\n   401:\t\tn := map[string]int{}\r\n(dlv) p m.targets\r\n{setName: \"test-grafana\", provider: \"kubernetes\/3\"}: [\r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-hl6s2\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc000677d70), \r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-f4v7l\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc001ce6b40), \r\n\t], \r\n```\r\nAdding one more pod first, there are now three pods, which is as expected:\r\n```\r\n# kubectl get pod -n infrastore-common -l name=infrastore-grafana\r\nNAME                                  READY   STATUS    RESTARTS   AGE\r\ninfrastore-grafana-5c7c8468c8-f4v7l   1\/1     Running   0          4m26s\r\ninfrastore-grafana-5c7c8468c8-hl6s2   1\/1     Running   0          28h\r\ninfrastore-grafana-5c7c8468c8-lm7gz   1\/1     Running   0          25s\r\n```\r\n```\r\n(dlv) b discovery\/manager.go:397\r\n(dlv) c\r\n(dlv) p m.targets\r\n{setName: \"test-grafana\", provider: \"kubernetes\/3\"}: [\r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-hl6s2\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc002455d70), \r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-f4v7l\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc002455e30), \r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-lm7gz\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc00218d560), \r\n\t],\r\n```\r\n- Now deleting two pods, leaving only one pod:\r\n```\r\n# kubectl get pod -n infrastore-common -l name=infrastore-grafana\r\nNAME                                  READY   STATUS    RESTARTS   AGE\r\ninfrastore-grafana-5c7c8468c8-hl6s2   1\/1     Running   0          28h\r\n```\r\nHowever, by using `dlv` to inspect the memory stack of service discovery, there are still records of three pods even though only one pod is left:\r\n```\r\n(dlv) b discovery\/manager.go:397\r\n(dlv) c\r\n(dlv) p m.targets\r\n{setName: \"test-grafana\", provider: \"kubernetes\/3\"}: [\r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-hl6s2\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc002455d70), \r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-f4v7l\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc001ce64e0), \r\n\t\t\"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-lm7gz\": *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc00253d1a0), \r\n\t], \r\n```\r\nInspecting the data of the deleted pods, it appears that it has already been update:\r\n```\r\n(dlv) p *(*\"github.com\/prometheus\/prometheus\/discovery\/targetgroup.Group\")(0xc001ce64e0)\r\ngithub.com\/prometheus\/prometheus\/discovery\/targetgroup.Group {\r\n\tTargets: []github.com\/prometheus\/common\/model.LabelSet len: 0, cap: 0, nil,\r\n\tLabels: github.com\/prometheus\/common\/model.LabelSet nil,\r\n\tSource: \"pod\/infrastore-common\/infrastore-grafana-5c7c8468c8-f4v7l\",}\r\n```\r\nTherefore, even though the pods have been deleted, they still remain in `m.targets`. For businesses like yours where pod destruction and reconstruction occur frequently, this can cause `m.targets` to grow larger and larger. This has been confirmed through analysis using pprof:\r\n```\r\n(pprof) top20\r\nShowing nodes accounting for 790.02MB, 96.97% of 814.74MB total\r\nDropped 187 nodes (cum <= 4.07MB)\r\nShowing top 20 nodes out of 72\r\n      flat  flat%   sum%        cum   cum%\r\n  230.52MB 28.29% 28.29%   230.52MB 28.29%  github.com\/prometheus\/prometheus\/discovery.(*Manager).updateGroup\r\n   85.10MB 10.45% 38.74%    85.10MB 10.45%  github.com\/prometheus\/prometheus\/model\/labels.(*Builder).Labels\r\n      84MB 10.31% 49.05%       84MB 10.31%  github.com\/prometheus\/prometheus\/discovery\/kubernetes.podSourceFromNamespaceAndName (inline)\r\n      73MB  8.96% 58.01%   171.02MB 20.99%  github.com\/prometheus\/prometheus\/discovery\/kubernetes.(*Pod).process\r\n   71.26MB  8.75% 66.76%    71.26MB  8.75%  reflect.unsafe_NewArray\r\n   50.38MB  6.18% 72.94%    50.38MB  6.18%  github.com\/prometheus\/prometheus\/discovery.(*Manager).allGroups\r\n   48.57MB  5.96% 78.90%    48.57MB  5.96%  k8s.io\/apimachinery\/pkg\/apis\/meta\/v1.(*FieldsV1).UnmarshalJSON\r\n   36.50MB  4.48% 83.38%       37MB  4.54%  github.com\/json-iterator\/go.(*Iterator).ReadString\r\n   19.01MB  2.33% 85.72%    19.01MB  2.33%  reflect.mapassign\r\n      13MB  1.60% 87.31%       13MB  1.60%  github.com\/modern-go\/reflect2.(*unsafeType).UnsafeNew\r\n   12.51MB  1.53% 88.85%   132.12MB 16.22%  tkestack.io\/kvass\/pkg\/discovery.targetsFromGroup\r\n   11.51MB  1.41% 90.26%    11.51MB  1.41%  github.com\/prometheus\/prometheus\/model\/labels.New\r\n    9.51MB  1.17% 91.43%     9.51MB  1.17%  reflect.New\r\n    9.51MB  1.17% 92.59%    14.51MB  1.78%  github.com\/prometheus\/prometheus\/discovery\/kubernetes.(*Pod).buildPod\r\n       9MB  1.10% 93.70%        9MB  1.10%  github.com\/prometheus\/prometheus\/scrape.NewTarget (inline)\r\n    7.51MB  0.92% 94.62%     7.51MB  0.92%  github.com\/prometheus\/prometheus\/scrape.(*Target).DiscoveredLabels\r\n       7MB  0.86% 95.48%        7MB  0.86%  github.com\/prometheus\/prometheus\/model\/labels.NewBuilder (inline)\r\n    6.04MB  0.74% 96.22%     7.04MB  0.86%  tkestack.io\/kvass\/pkg\/scrape.StatisticSeries\r\n    4.50MB  0.55% 96.77%     4.50MB  0.55%  github.com\/prometheus\/prometheus\/discovery\/kubernetes.podLabels\r\n    1.59MB  0.19% 96.97%   141.21MB 17.33%  tkestack.io\/kvass\/pkg\/discovery.(*TargetsDiscovery).translateTargets\r\n```","Suggest you inspect the way Prometheus uses this code to identify what you are doing differently. \r\n\r\nAnd please answer questions in the issue template next time to make the context clear. ","I tried prometheus kubernetes service discovery and found the same issue.","@bboreham \r\nI found that whether it is importing Prometheus Kubernetes service discovery as a package or using Prometheus Kubernetes service discovery itself, there is this issue that deleted resources will still be updated to [m.targets](https:\/\/github.com\/prometheus\/prometheus\/blob\/v2.44.0\/discovery\/legacymanager\/manager.go#L126) by service discovery and will not be deleted until Prometheus is reloaded.\r\n\r\nEspecially when collecting without configuring label selector or field selector, the service discovery will cover the entire namespace. If the resources in this namespace are frequently destroyed and rebuilt, m.targets will become larger and larger, which will cause CPU and memory to continuously increase.\r\n\r\nWhat other information do you need? I am happy to provide it.\r\n\r\n","Did you find the cause for the bug? Do you have a reproducer ?\r\n\r\nLe dim. 2 juil. 2023, 15:21, haleyao ***@***.***> a \u00e9crit :\r\n\r\n> @bboreham <https:\/\/github.com\/bboreham>\r\n> I found that whether it is importing Prometheus Kubernetes service\r\n> discovery as a package or using Prometheus Kubernetes service discovery\r\n> itself, there is this issue that deleted resources will still be updated to\r\n> m.targets\r\n> <https:\/\/github.com\/prometheus\/prometheus\/blob\/v2.44.0\/discovery\/legacymanager\/manager.go#L126>\r\n> by service discovery and will not be deleted until Prometheus is reloaded.\r\n>\r\n> Especially when collecting without configuring label selector or field\r\n> selector, the service discovery will cover the entire namespace. If the\r\n> resources in this namespace are frequently destroyed and rebuilt, m.targets\r\n> will become larger and larger, which will cause CPU and memory to\r\n> continuously increase.\r\n>\r\n> What other information do you need? I am happy to provide it.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/12436#issuecomment-1616659770>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AACHHJQDP6MU7GADGBZZIZ3XOFYUNANCNFSM6AAAAAAY4NQQ3M>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n","@roidelapluie \r\nThank you for your reply. The cause for this is that Kubernetes service discovery uses `informer` to watch resources, and deleted resources cannot be obtained from the `informer` cache store, but they will still be added to the [scrape manager](https:\/\/github.com\/prometheus\/prometheus\/blob\/v2.44.0\/discovery\/legacymanager\/manager.go#L126). Taking pod discovery as an example:\r\n```\r\nfunc (p *Pod) process(ctx context.Context, ch chan<- []*targetgroup.Group) bool {\r\n\t. . . \r\n\r\n\to, exists, err := p.store.GetByKey(key)\r\n\tif err != nil {\r\n\t\treturn true\r\n\t}\r\n\tif !exists {     \/\/ If the pod is deleted, exists is false\r\n\t\tsend(ctx, ch, &targetgroup.Group{Source: podSourceFromNamespaceAndName(namespace, name)})\r\n\t\treturn true\r\n\t}\r\n\t. . . \r\n}\r\n```\r\nYou can reproduce it simply by deleting a resource, such as a pod. [The reproduction steps](https:\/\/github.com\/prometheus\/prometheus\/issues\/12436#issuecomment-1585752974)\r\n\r\nI have currently fixed the issue by forking the Prometheus repository, code is as follows:\r\n\r\n\/\/ discovery\/legacymanager\/manager.go\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/31c40c3e-5be6-46d0-80ca-2c0b0cab4915)\r\n\r\nThe effect after the fix is as follows:\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/45335494\/5716b0e9-adb7-4e9d-a7c1-fb5f2c6adc78)\r\n\r\nIf there is any other information needed to provide, I am more than happy to do so.\r\n","Would you like to push your branch as a PR?\r\nThat would be easier than trying to copy-type from a screen grab. ","I'll give this a try."],"labels":["kind\/more-info-needed"]},{"title":"fatal error: mSpanList.insertruntime","body":"### What did you do?\n\nNo action taken.\n\n### What did you expect to see?\n\nNo Panic and restart.\n\n### What did you see instead? Under which circumstances?\n\nBellow panic has been seen in Prometheus server.\r\n\r\n```\r\nts=2023-05-25T23:04:31.935Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=562 last=574 duration=22.790495137s\r\nts=2023-05-25T23:08:35.308Z caller=compact.go:459 level=info component=tsdb msg=\"compact blocks\" count=3 mint=1685016000003 maxt=1685037600000 ulid=01H1AJG6Y0TYHBP7102HXQDKW7 sources=\"[01H19PS0DDYG7JEE7CSER9PQDY 01H19XMQNDGJEWMS83NP2R59NY 01H1A4GEYRSEH1QEZJT4AH4K9R]\" duration=4m3.372621194s\r\nts=2023-05-25T23:08:35.626Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H1A4GEYRSEH1QEZJT4AH4K9R\r\nts=2023-05-25T23:08:36.303Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H19XMQNDGJEWMS83NP2R59NY\r\nts=2023-05-25T23:08:36.538Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H19PS0DDYG7JEE7CSER9PQDY\r\nruntime: failed mSpanList.insert 0x7f24db941328 0x5dd3f3b80 0x0 0x0\r\nfatal error: mSpanList.insertruntime stack:\r\nruntime.throw({0x2bc50b4?, 0x0?})\r\n    \/usr\/local\/go\/src\/runtime\/panic.go:992 +0x71\r\nruntime.(*mSpanList).insert(0x40cc310?, 0x7f24db941328)\r\n    \/usr\/local\/go\/src\/runtime\/mheap.go:1612 +0xe5\r\nruntime.stackfree({0x0?, 0xc0000c0f00?})\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:513 +0x148\r\nruntime.copystack(0xc32d33c820, 0x800000002?)\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:936 +0x307\r\nruntime.newstack()\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:1110 +0x497\r\nruntime.morestack()\r\n```\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\n2.31.1\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nts=2023-05-25T23:08:35.626Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H1A4GEYRSEH1QEZJT4AH4K9R\r\nts=2023-05-25T23:08:36.303Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H19XMQNDGJEWMS83NP2R59NY\r\nts=2023-05-25T23:08:36.538Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01H19PS0DDYG7JEE7CSER9PQDY\r\nruntime: failed mSpanList.insert 0x7f24db941328 0x5dd3f3b80 0x0 0x0\r\nfatal error: mSpanList.insertruntime stack:\r\nruntime.throw({0x2bc50b4?, 0x0?})\r\n    \/usr\/local\/go\/src\/runtime\/panic.go:992 +0x71\r\nruntime.(*mSpanList).insert(0x40cc310?, 0x7f24db941328)\r\n    \/usr\/local\/go\/src\/runtime\/mheap.go:1612 +0xe5\r\nruntime.stackfree({0x0?, 0xc0000c0f00?})\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:513 +0x148\r\nruntime.copystack(0xc32d33c820, 0x800000002?)\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:936 +0x307\r\nruntime.newstack()\r\n    \/usr\/local\/go\/src\/runtime\/stack.go:1110 +0x497\r\nruntime.morestack()\r\n    \/usr\/local\/go\/src\/runtime\/asm_amd64.s:547 +0x8bgoroutine 9502 [copystack]:\r\ngithub.com\/golang\/snappy.encodeBlock({0xc5d6eca003, 0x23fcb2, 0x23fcb2}, {0xc4e9c6a000, 0x60a3, 0x18000})\r\n    \/root\/go\/pkg\/mod\/github.com\/golang\/snappy@v0.0.4\/encode_amd64.s:265 +0x380 fp=0xc077fd52a0 sp=0xc077fd5298 pc=0x209f460\r\ngithub.com\/golang\/snappy.Encode({0xc5d6eca000?, 0xc1c07d50d8?, 0x4?}, {0xc4e9c6a000?, 0x39?, 0xc1c07d50dc?})\r\n    \/root\/go\/pkg\/mod\/github.com\/golang\/snappy@v0.0.4\/encode.go:39 +0x245 fp=0xc077fd5328 sp=0xc077fd52a0 pc=0x209eac5\r\ngithub.com\/prometheus\/prometheus\/tsdb\/wal.(*WAL).log(0xc000438240, {0xc4e9c6a000, 0x60a3, 0x18000}, 0x1)\r\n    \/home\/jenkins\/workspace\/CTO\/CSF\/FaultAndPerformanceManagement\/CSF-PROMETHEUS\/CSF-PROMETHEUS-PATCHES\/21.11FP1PP\/CSF-PROMETHEUS-PROMETHEUS-MIRROR\/LX\/src\/github.com\/prometheus\/prometheus\/tsdb\/wal\/wal.go:627 +0x136 fp=0xc077fd53f0 sp=0xc077fd5328 pc=0x20a66b6\r\ngithub.com\/prometheus\/prometheus\/tsdb\/wal.(*WAL).Log(0xc000438240, {0xc077fd5548, 0x1, 0x46a21b?})\n```\n","comments":["Please supply system information.\r\nNote that version 2.31.1 is quite old.","I am also seeing the issue in the customer premises. Initially Prometheus gave OOM errors and also runtime panic issues (GC errors) occassionally. Further, we reduced the jobs being scrapped by prometheus to one-third. The OOM error is not seen anymore but runtime panic error is still seen. We are giving 2VCPU and 48 GB memory to prometheus. Disk is 50% full out of 104GB. \r\n\r\nPrometheus-2.31.1 is compiled with go version 1.18.1. K8s version is \"v1.19.5\" on centos 7.9. \r\n\r\nAttached file contains the backtrace. \r\n[prometheus panic issues files](https:\/\/drive.google.com\/drive\/u\/0\/folders\/13yuaDNTGqEM1U3h-eXMeIPnNPxciZjOZ)\r\n\r\n","After adding this environment in prometheus container. The error is not seen in last 13 days after this change. Prior to this change it used to restart once in a day or two.\r\n        - name: GODEBUG\r\n          value: \"gctrace=2\"","Thanks for the update.  Can you please clarify are you working together with SarthakSahu or yours is a separate report?\r\n\r\nI think if you are sometimes OOM-killed it is plausible that these errors from inside the Go runtime are also caused by running out of memory.\r\n\r\nMany people report that Prometheus 2.44 and newer use much less memory; I recommend you upgrade."],"labels":["kind\/more-info-needed"]},{"title":"docs: Add auto-generated page for current metrics Prometheus exposes about itself.","body":"### Proposal\n\nIt would be amazing to have list of exact metric names and their help that Prometheus gives about itself. This is common for many products that expose Prometheus metrics, so would be nice to have Prometheus ones too (e.g head compactions). Perhaps we could expand on each HELP to tell more.\r\n\r\nExtra: Have it autogenerate from source code would be amazing, so it's not requiring manual changes (or validation) on every PR.","comments":["A quick hack is to give link to demo instance: https:\/\/prometheus.demo.do.prometheus.io\/metrics","Hi @bwplotka  I'd like to work on this issue (#12397). Can you assign the issue to me? Thanks!\r\n","Hi @bwplotka, I can work on this issue. ","@bwplotka I would like to work on this can you give some more information","Sure, help wanted. We don't assign issues, so anybody is free to make PR, no need to ask for permission. The fact PR was made with the link to this issue will show others that the issue is worked on. \ud83e\udd17\n\nLet us know what \"more information\" mean. What questions do you have?\n\nNote that no Prometheus maintainer (other than me), commented or shared opinion on this one, so it's unsure yet if we want that."],"labels":["help wanted","component\/documentation"]},{"title":"Add match[] param to \/api\/v1\/status\/tsdb API","body":"### Proposal\n\nTSDB status API https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#tsdb-stats returns various cardinality statistics.\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/11945 `topN` param got added to return more info we need (default is 10).\r\n\r\nHowever, if users want to check the cardinality of a specifc target or job, if it is not at high cardinality we cannot include those information in response.\r\n\r\nThe proposal is to add `match[]=<series_selector>` param similar to the GetSeries API and we can look at cardinality of specific series\/jobs.","comments":["This would be a great proposal.","Thanks @roidelapluie.\r\n\r\nI am thinking about supporting only equal matcher `=` for now. \r\nAs MemPostings is a nested key-value map https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/tsdb\/index\/postings.go#L174. `=` matcher only can simplify the implementation. WDYT?\r\n\r\n","I would accept that approach as a first step and let further contributors contribute regex and negation support if they need it.","Hi \ud83d\udc4b\ud83c\udffd i am new to prometheus, i would like to  contribute a PR, if no one else is working on this.","@palash25 I am not actively working on this issue right now so please go ahead.","i won't be able to do it since I am taking a break from typing because of RSI if someone else is interested in this then feel free to work on it","Hi, I would love to take a go at this. I wrote a basic implementation but wanted to make sure I understand the requirement correctly.\r\n\r\nThis is how the current algorithm works per my understanding:\r\n```\r\nfunc stats(labelName, limit) {\r\n  \/\/ for labelName only\r\n  get number of series grouped by value\r\n  \/\/ across all labels\r\n  get number of unique values per label\r\n  get bytes required to represent labelValue to seriesRef relationship\r\n  get number of series grouped by labelValue pair\r\n  get total number of labelValue pairs\r\n}\r\n```\r\n\r\n If we use the following data as an example:\r\n```\r\n{__name__=\"requests_total\", path=\"\/status\", job=\"static\u201d} -> series1\r\n{__name__=\"requests_total\", path=\"\/\", job=\"static\u201d} -> series2\r\n{__name__=\"requests_total\", path=\"\/\", job=\"pod\u201d} -> series3\r\n{__name__=\"memory_bytes\", job=\"pod\u201d} -> series4\r\n```\r\nThe current stats method returns the following (if labelName == `__name__`) :\r\n<img width=\"684\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/12442986\/ff38635a-14ee-4d9c-b1e5-f6b3713ee1e9\">\r\n\r\n## Newer version\r\n\r\n1. If we want the statistics for `job==\"pod\"` we want the method to return the following (if labelName == `__name__`):\r\n<img width=\"654\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/12442986\/6d77548b-e239-4c80-a540-3be6b576fd4e\">\r\n\r\n2. If we want the statistics for `job==\"pod\",__name__==\"requests_total\"` we want the method to return the following (if labelName == `__name__`):\r\n<img width=\"680\" alt=\"image\" src=\"https:\/\/github.com\/prometheus\/prometheus\/assets\/12442986\/d0b12b07-01cf-4003-a1df-bccc4a9ef5ec\">\r\n"],"labels":["help wanted","priority\/P3","component\/tsdb","kind\/feature"]},{"title":"Prometheus doesnot log any error when service discovery file is not present at startup","body":"### What did you do?\r\n\r\nRan Prometheus with the attached configuration to scrape targets using `file_sd_configs`. I forgot to place the ecs_sd_target.json file at Prometheus startup and noticed that Prometheus didn't log any messages that indicate the sd_file was missing. I was also running Prometheus with log level mode as debug. \r\n\r\nUpon triaging noticed that, the intent of code [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/3c4802635d05af47ddf7358c0c079961b011c47b\/discovery\/file\/file.go#L343) might be to log message if the sd_file is not present, but its not getting logged - as upon absence of sd_file the listFiles() [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/3c4802635d05af47ddf7358c0c079961b011c47b\/discovery\/file\/file.go#L338) doesn't return any path and hence code doesn't enter [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/3c4802635d05af47ddf7358c0c079961b011c47b\/discovery\/file\/file.go#L338) for loop. However, Prometheus does log error message when the path is not watchable, but does not log message if `sd_file` is missing in that path.\r\n\r\nSimilarly, also found if the `sd_file` is not empty but the `targetgroups` is not present inside the `sd_file`, the Prometheus does not log the message intended [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/3c4802635d05af47ddf7358c0c079961b011c47b\/discovery\/file\/file.go#L412), b\/c code doesn't evaluate this condition as `tg == nil`. \r\n\r\n### What did you expect to see?\r\n\r\nI expected to see some logs printed out to indicate if `sd_file` is missing.\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nI didn't see any logs regarding this - and usual operation.\r\n\r\n### System information\r\n\r\nmacos ventura arm64\r\n\r\n### Prometheus version\r\n\r\n```text\r\n2.43.0\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```yaml\r\nglobal:\r\n  scrape_interval: 15s \r\n  evaluation_interval: 15s \r\n\r\nalerting:\r\n  alertmanagers:\r\n    - static_configs:\r\n        - targets:\r\n\r\nrule_files:\r\n\r\nscrape_configs:\r\n  - job_name: \"prometheus\"\r\n    file_sd_configs:\r\n    - files:\r\n        - 'ecs_sd_target.json'\r\n```\r\n\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n_No response_","comments":["This is expected because the file path is a glob. prometheus can start without files, and in production setup, I have seen that happening a log. I am unsure logging (which would happen on every reload) is a good solution here.","The ecs_sd_target.yaml file is not included already. How did you add it to your host?\r\n\r\nI tried to import the image of the adot collector and add files using the ADD command, but it is not working well."],"labels":["kind\/enhancement","component\/service discovery"]},{"title":"Log output doesn't need to be UTC","body":"### What did you do?\n\nHello all.\r\n\r\nI understand that the collected data is in UTC (https:\/\/prometheus.io\/docs\/introduction\/faq\/#can-i-change-the-timezone-why-is-everything-in-utc and https:\/\/github .com\/prometheus\/prometheus\/issues\/500), but I don't see why the logs need to be too.\r\n\r\nI believe it is because of this line: https:\/\/github.com\/prometheus\/prometheus\/blob\/3c4802635d05af47ddf7358c0c079961b011c47b\/documentation\/examples\/custom-sd\/adapter-usage\/main.go#L256\r\n\r\nI'm asking this because I live in a country that is UTC -3, and the logs are being collected with the wrong time.\r\n\r\nI'm using prometheus from helm kube-prometheus-stack, but I think the issue is in the prometheus binary itself.\r\n\r\nI mounted a volume by creating an \/etc\/localtime with:\r\n```\r\nprometheus:\r\n   prometheusSpec:\r\n     volumeMounts:\r\n       - name: tz-config\r\n         mountPath: \/etc\/localtime\r\n     volumes:\r\n       - name: tz-config\r\n         hostPath:\r\n           path: \/usr\/share\/zoneinfo\/America\/Sao_Paulo\r\n```\r\nInside the pod the time was correct, but in the logs it still appeared with UTC.\r\n\r\nThanks in advance, Marcelo.\n\n### What did you expect to see?\n\nThe timestamp in the logs in the correct timezone\n\n### What did you see instead? Under which circumstances?\n\nThe timestamp is in UTC\n\n### System information\n\nBusyBox v1.34.1 (2022-10-25 00:07:12 UTC) multi-call binary.\n\n### Prometheus version\n\n```text\nprometheus --version\r\nprometheus, version 2.40.5 (branch: HEAD, revision: 44af4716c86138869aa621737139e6dacf0e2550)\r\n  build user:       root@70f803b28803\r\n  build date:       20221201-12:50:06\r\n  go version:       go1.19.3\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["@raxidex Did you create a PR about it?\r\n\r\nIf no, let me know as it's something we need too and I would like to contribute :)","Hello @zigelboim-misha. I didn't create, in fact I kind of left it aside for now as our project with prometheus stopped."],"labels":["kind\/feature"]},{"title":"Arithmetic on the argument of the @ (at) modifier","body":"### What did you do?\n\nI was trying to retrieve specific metrics at yesterdays midnight (23:59:59) by using something like this:\r\n```\r\nsome_custom_metric @ <timestamp>\r\n```\r\nwhile \r\n```\r\n(time() - (hour(vector(time())) * 60 * 60 + minute(vector(time())) * 60) - 60)\r\nand\r\n0+(time() - (hour(vector(time())) * 60 * 60 + minute(vector(time())) * 60) - 60)\r\n```\r\nreturns an OK timestamp, but while trying to use it like\r\n\r\n```\r\nsome_custom_metric @ (time() - (hour(vector(time())) * 60 * 60 + minute(vector(time())) * 60) - 60)\r\nor\r\nsome_custom_metric @ 0+(time() - (hour(vector(time())) * 60 * 60 + minute(vector(time())) * 60) - 60)\r\n```\r\nI'm receiving these kinds of errors\r\n\r\n```\r\nError executing query: 1:10: parse error: unexpected \"(\" in @, expected timestamp\r\nor\r\nError executing query: 1:1: parse error: @ modifier must be preceded by an instant vector selector or range vector selector or a subquery\r\n```\n\n### What did you expect to see?\n\nMetric at a specific time.\n\n### What did you see instead? Under which circumstances?\n\nErrors:\r\n```\r\nError executing query: 1:10: parse error: unexpected \"(\" in @, expected timestamp\r\nor\r\nError executing query: 1:1: parse error: @ modifier must be preceded by an instant vector selector or range vector selector or a subquery\r\n```\n\n### System information\n\nPrometheus in K8s via Helm (2.43.0)\n\n### Prometheus version\n\n```text\nVersion\t2.43.0\r\nRevision\tedfc3bcd025dd6fe296c167a14a216cab1e552ee\r\nBranch\tHEAD\r\nBuildUser\troot@8a0ee342e522\r\nBuildDate\t20230321-12:56:07\r\nGoVersion\tgo1.19.7\n```\n\n\n### Prometheus configuration file\n\n```yaml\n-\n```\n\n\n### Alertmanager version\n\n```text\n-\n```\n\n\n### Alertmanager configuration file\n\n```yaml\n-\n```\n\n\n### Logs\n\n```text\n-\n```\n","comments":["There have been thoughts around this for a long time, see [this exploratory document](https:\/\/docs.google.com\/document\/d\/1jMeDsLvDfO92Qnry_JLAXalvMRzMSB1sBr9V7LolpYM\/edit#), some parts of which are already implemented, some are [being worked on](https:\/\/github.com\/prometheus\/prometheus\/pull\/9138), and some are somewhat out in the distance, among them arithmetic on durations and on the argument of the `@` modifier."],"labels":["component\/promql","not-as-easy-as-it-looks","priority\/P3","kind\/feature"]},{"title":"WAL Checkpoint holds deleted series for 1 extra compaction cycle","body":"EDIT: the problem has been substantially improved by #12297; check out the write-up there for details.\r\n\r\n### What did you do?\r\n\r\nI observed a Prometheus where `prometheus_tsdb_head_series` varied from 4 million to 7 million over each compaction cycle.\r\n\r\nThe number of series in the WAL checkpoint is not observable via a metric, so I downloaded it and used the following code, adapted from `TestReadCheckpointMultipleSegments` to see the number of series:\r\n\r\n```\r\n\twt := newWriteToMock()\r\n\twatcher := NewWatcher(wMetrics, nil, nil, \"\", wt, dir, false, false)\r\n\twatcher.MaxSegment = -1\r\n\twatcher.setMetrics()\r\n\r\n\tlastCheckpoint, _, err := LastCheckpoint(watcher.walDir)\r\n\terr = watcher.readCheckpoint(lastCheckpoint, (*Watcher).readSegment)\r\n\tfmt.Println(len(wt.seriesSegmentIndexes))\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nAbout 7 million series in the WAL checkpoint (same as the max observed number of series).\r\n\r\nReasoning: a WAL checkpoint is generated every head compaction* (= 2 hours with default settings) and the checkpoint covers about the same time, so the checkpoint should have about the same number of series as I observe in the head.\r\n\r\n\\* unless the amount of data being collected is small, in which case it's every 2 head compactions.\r\n\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nThe WAL checkpoint had 18 million series in it.\r\n\r\n### Prometheus version\r\n\r\n```text\r\nI was looking at 2.41 but further tests show the same thing happens in 2.43.\r\n```\r\n","comments":["I tried to write out step-by-step what happens to cause this.\r\nIllustrated is a Prometheus TSDB that has been collecting data since 10:00 UTC.\r\nWAL segments are named A, B, C, ...\r\n\r\nConsider a series `foo` which received a few samples at 10:15, then stopped.\r\nThe samples for series `foo` are in WAL segment C.\r\n\r\n```\r\n           10:00        12:00\r\nHead       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL        A- B- C- D- E-- F- G-\r\n```\r\n\r\nAt approx 13:00, head compaction runs.  A block is created from 10-12, and\r\nthat data is dropped from the head. Series `foo` is garbage-collected, but\r\nthe head notes in its 'deleted' map that it might be needed until WAL segment G has\r\nbeen dropped.\r\n\r\nA WAL checkpoint is created from the first two thirds of the segments, A-D.\r\nThis checkpoint has no samples, since any samples before 12:00 are excluded.\r\nWAL segments A-D are removed from disk.\r\n\r\n```\r\n           10:00        12:00\r\nHead                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nBlocks     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL                    E-- F- G-\r\nCheckpoint            X\r\n```\r\n\r\nAfter two more hours, the head and WAL have built up:\r\n\r\n```\r\n           10:00        12:00        14:00\r\nHead                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nBlocks     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL                    E-- F- G- H I- J-- K-\r\nCheckpoint            X\r\n```\r\n\r\nAt approx 15:00, head compaction runs again.\r\nA WAL checkpoint is created covering segments E-H.\r\nSeries `foo` is retained in the checkpoint, since it is in the 'deleted' map.\r\nThe 'deleted' map is cleaned of any series needed until segment 'E', so series `foo` remains in the map.\r\n\r\n```\r\n           10:00        12:00        14:00\r\nHead                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nBlocks     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL                                I- J-- K-\r\nCheckpoint                        X\r\n```\r\n\r\nAfter two more hours, the head and WAL have built up:\r\n\r\n```\r\n           10:00        12:00        14:00        16:00\r\nHead                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nBlocks     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL                                I- J-- K- L M N-- O- P-\r\nCheckpoint                        X\r\n```\r\n\r\nAt approx 17:00, head compaction runs again.\r\nA WAL checkpoint is created covering segments I-L.\r\nSeries `foo` is retained in the checkpoint, since it is still in the 'deleted' map.\r\nNow, series `foo` is dropped from the 'deleted' map since segment G is before I.\r\n\r\n```\r\n           10:00        12:00        14:00        16:00\r\nHead                                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n                                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nBlocks     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nWAL                                            M N-- O- P-\r\nCheckpoint                                    X\r\n```\r\n\r\nOnly at the next head compaction at 19:00 will series `foo` be dropped from\r\nthe checkpoint, since it is no longer in the 'deleted' map.\r\n\r\nIn this way, a series which stopped receiving data at 10:45 is retained in the WAL until 19:00.\r\n","This is a nice find. We probably have to dive in to see why is this happening. Just want to leave a comment here saying that whatever is affecting the WAL is probably affecting the WBL too since they both rely on the same implementation with minor differences.","Thanks @jesusvazquez : I don't see anything about a checkpoint in the WBL code I looked at:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/5442d7e524834a8815775a1b07bb69550647a3af\/tsdb\/head.go#L1247-L1261\r\n\r\nIs there some other mechanism to ensure there is a series record for all samples in the remaining part of the WBL?\r\n\r\nBTW it would be nice to have the WBL mentioned in the docs https:\/\/github.com\/prometheus\/prometheus\/blob\/a0f7c31c2666dc45f8006ee66395b5409a59a2b9\/tsdb\/docs\/","Following up my last comment: it turns out every sample in the WBL is first written to the WAL, so the series records in the WAL will work for the WBL too. \r\n\r\nNow that #12297 is merged, the problem is reduced: we are holding series for 1 extra compaction cycle.\r\nIn terms of the example we will remove the record of series `foo` at 15:00."],"labels":["kind\/enhancement","component\/tsdb"]},{"title":"how can i reduce the time load the wal when prometheus oom restart","body":"### Proposal\n\nhow can i reduce the time load the wal when prometheus oom restart. when i restart vm-srorage it only takes a few time","comments":["If your Prometheus is bottlenecked on memory-mapped IO, then adding more RAM should help.\r\nOtherwise you might like to shorten the time between WAL truncations (default 2h) using `--storage.tsdb.min-block-duration`.\r\n\r\nSome relevant discussion at #6934 "],"labels":["kind\/question","component\/tsdb"]},{"title":"Recording rule groups don't have consistent view of data","body":"### What did you do?\n\nWe run a number of recording rules that are used to calculate SLOs within rule groups. Each rule group performs these calculations:\r\n- Count the number of good events (`good_events`), e.g. `count(my_metric{result=\"ok\"})`\r\n- Count the number of total events (`total_events`), e.g. `count(my_metric)`\r\n- Calculate the ratio (`ratio`), e.g. `good_events \/ total_events`\r\n\r\n*The rules are defined in this order within a single group*.\n\n### What did you expect to see?\n\nAs the service was healthy and queries to `my_metric` verified no errors had occurred, we expected that `good_events` would equal `total_events` and the `ratio` would equal 1. \n\n### What did you see instead? Under which circumstances?\n\nWe were frequently saw on 1 of our two Prometheus HA server pairs that the ratio was not 100%. Looking at the raw values of the `good_events` and `total_events` rules, I saw that often `total_events` had a value + 1 greater than `good_events`. After digging in a bit more, I think I can see what's happening: the scrape interval for the underlying metric and the rule evaluation schedules are almost identical- this means that when a rule group evaluates, we have the potential for part of the rule group to see \"more\" samples after a scrape completes midway through a rule group evaluation. \r\n\r\nI can see that in the rules manager we pass a timestamp that _should_ limit later samples from arriving and being considered but this doesn't seem to prevent the issue from occurring. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1540536\/233249868-b970e56b-4965-4d21-803e-f9a252eb7507.png)\r\nThe graph above shows the result of running `timestamp(<problematic_metric_name>) - (prometheus_rule_group_last_evaluation_timestamp_seconds{rule_group=\"<problematic_rule_group>\"})`. When we saw the value of 15s, we saw no issue. When we saw the value of 30s (this is the value of both our scrape interval _and_ the rule group evaluation frequency), this is when the issue occurred- to me this demonstrates that the rule group evaluation and scrape is happening simultaneously. \r\n\n\n### System information\n\nLinux 5.11.0-1022-aws x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.29.1 (branch: HEAD, revision: dcb07e8eac34b5ea37cd229545000b857f1c1637)\r\n  build user:       root@364730518a4e\r\n  build date:       20210811-14:48:27\r\n  go version:       go1.16.7\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["I believe the current implementation aims for consistent results within a single query but not within a rule group.\r\nAll targets scheduled to scrape at the same time will get the same timestamp, so that isn't enough.\r\nThere is an isolation mechanism that excludes all changes made to TSDB after a query starts.\r\n\r\nIt seems a plausible enhancement to do this for rule groups.","Hello, this would be a huge burden to keep the same querier across different rules, especially since you have to be able to query samples from the previous rules, which is something a single querier would not be able to do.\r\n\r\nOne solution is possibly https:\/\/github.com\/prometheus\/prometheus\/issues\/11807","Thanks for your input guys. So @roidelapluie it seems that using `offset 5s` (or some similar value) on recording rules that reference scraped metrics would be a potential fix.","Yes you could try that "],"labels":["kind\/enhancement","component\/rules"]},{"title":"`remote_read` should support exemplars","body":"### Proposal\n\nI was playing with an in-house `remote_read` server PoC (from which Prometheus can read) and noticed exemplars don't show up on Prometheus' React UI. (Discussion moved from https:\/\/cloud-native.slack.com\/archives\/C167KFM6C\/p1680885285678839 )\r\n\r\nIt seems like `storage\/remote` package focuses on `Sample`s from `TimeSeries` although the type includes `Exemplar`s as well.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/8dba9163f1e923ec213f0f4d5c185d9648e387f0\/storage\/remote\/read.go#L170\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/8dba9163f1e923ec213f0f4d5c185d9648e387f0\/storage\/remote\/codec.go#L169\r\n\r\nSince the reference implementation of exemplars in this repository is queried via `\/query_exemplars` REST anyways, I think it makes sense to have an option of delegating the work to a remote implementation which might be based on Postgres, InfluxDB or https:\/\/github.com\/etcd-io\/bbolt (if I'm not mistaken, Thanos and Mimir just inherited the reference implementation of Prometheus exemplars? But I don't have any first hand experiences with them)","comments":[],"labels":["kind\/enhancement","component\/remote storage"]},{"title":"I hope to be able to see the number of all series and the total number of samples.","body":"### Proposal\n\nThe values \u200b\u200bof series and chunk in the tsdb status seen in prometheus ui are only the active data in the last few hours, not all the data summary.\r\n\r\nIs there any way to see the total number of series and samples in the entire prometheus tsdb, as well as the write rate of samples\uff1f","comments":["Without having tried it:\r\n\r\nNumber of samples: `count_over_time({__name__=\".+\"}[my_retention_period])` \r\nNumber of series: `count(max_over_time({__name__=\".+\"}[my_retention_period]))` \r\n\r\n(replace `my_retention_period` with `14d` or however your Prometheus is configured).\r\n\r\nMay be resource-intensive to execute these queries.\r\n\r\nWrite rate is simpler: `rate(prometheus_tsdb_head_samples_appended_total[1m])`\r\n","> Without having tried it:\r\n> \r\n> Number of samples: `count_over_time({__name__=\".+\"}[my_retention_period])` Number of series: `count(max_over_time({__name__=\".+\"}[my_retention_period]))`\r\n> \r\n> (replace `my_retention_period` with `14d` or however your Prometheus is configured).\r\n> \r\n> May be resource-intensive to execute these queries.\r\n> \r\n> Write rate is simpler: `rate(prometheus_tsdb_head_samples_appended_total[1m])`\r\n\r\nThank you for your answer, I have tried to count {__name__=\".+\"}, but in the case of a large amount of data, the execution will hang and consume a lot of resources."],"labels":["kind\/question"]},{"title":"promtool: Add support for native histograms","body":"### Proposal\n\nVarious parts of `promtool` require proper support for native histograms, for example the rules unit tests (but there are many more, once we have an overview, let's create a checklist here).","comments":["@beorn7 I would like to work on this issue\r\n","@ashutosh887 Awesome. The first step would be, as said, to do some research and find out all the features of `promtool` that need work to support native histograms.\r\n\r\nA bit of prior experience with using `promtool` would definitely be helpful here.","When it comes to the actual implementation of the items on the (still to be created) checklist, it should be noted that the rules unit tests use the same testing framework as the PromQL tests we have in the code. That testing framework needs to support native histograms first, which is a huge effort tackled by @hdost , see https:\/\/github.com\/prometheus\/prometheus\/issues\/11170 .\r\n\r\nBut there are probably other items that are easier to implement and don't have dependencies to wait for.","The rules unit tests now support native histograms. :tada: ","After I have researched promtool, I think the all the subcommands already support native histogram or do not need to support native histogram are \r\n\r\n- check \r\n   - service-discovery: do not need support \r\n   - config: do not need support \r\n   - web-config: do not need support \r\n   - healthy: do not need support \r\n   - ready: do not need support \r\n   - rules: already native histogram\r\n- query\r\n   - instance: already support native histogram\r\n   - range: already support native histogram\r\n   - series: already support native histogram\r\n   - labels: already support native histogram\r\n- debug\r\n   - pprof: do not need support \r\n- test\r\n   - rule: already support native histogram\r\n- promql: all subcommands support native histogram\r\n- tsdb\r\n\t- analyze: already support native histogram\r\n\t- list: already support native histogram\r\n\t- create-blocks-from: do not need support \r\n\r\nSo all the TODOs need to be done\r\n\r\n- [ ] **check metrics**: This command only supports check metrics in text format for now, but the native histogram do not support text format.\r\n- [ ] **debug metrics**: This commond only supports fetch metric int text format for now, but the native histogram do not support text format.\r\n- [ ] **push metrics**: This command only supports load metrics in text format for now, but the native histogram do not support text format.\r\n- [ ] **tsdb bench write**: This command only write float value for bench test, not write native histogram\r\n- [x] **tsdb dump**: This command only dump float type value, not native histogram value\r\n\r\n@beorn7 \r\n\r\nAnd I would love to help with it.","Thanks for the great analysis. Note that there is also #12331 (which is more about creating a whole new sub-command and not really about adding support for native histograms to an existing command).\r\n\r\nI think `tsdb analyze` still needs an update because we will now have chunks with more than 120 samples or deliberately less. That's not only caused by native histograms, but by other recent changes, too. See #12054 and #12055. So we need to handle that variable number of samples better, plus we should probably add some specific stuff for native histograms. A single histogram sample can have many buckets (or very few), and it would be good to see some stats about that, too. Seeing the size of chunks in bytes would be cool, too. So maybe have separate histograms for float chunks and histogram chunks, and each of them should have a histogram for sample count, chunk size in bytes, and for histogram number of buckets in the chunk (maybe even \"represented buckets\" vs. \"populated buckets\").\r\n\r\nAbout the TODOs:\r\n\r\nI agree that a lot depends on availability of text format support (which we'll probably only add to OpenMetrics rather than to the old text format \u2013 see #11265 ). So that means for `check metrics`, `debug metrics`, and `push metrics`, we simply have to wait and see what the OpenMetrics text format will look like. (I guess we could also utilize the protobuf format somehow for these commands, but I don't think the need is urgent enough. Better to get the OM text format rolling first.)\r\n\r\nI think `tsdb bench write` would just be \"nice to have\". I'm not sure how much it is used right now. So I would only invest work into it if somebody explicitly requests it.\r\n\r\nWhich leaves us with `tsdb dump`: That would indeed be good to support native histograms. The dump uses pretty much an ad hoc format. That's a problem of its own, see discussion in #8281 in #8320 , but for now, it simplifies things here. You can essentially make something up (which could be using the `String` methods that are also used in the internal web UI).\r\n\r\nSo in summary, that's what you could work on right naw:\r\n- `tsdb dump` should be pretty straight forward. \r\n-  `tsdb analyze` needs an update for variable chunk sizes anyway, plus more data for histograms. So that would be a bit more involved, but probably very helpful.","> Thanks for the great analysis. Note that there is also #12331 (which is more about creating a whole new sub-command and not really about adding support for native histograms to an existing command).\r\n> \r\n> I think `tsdb analyze` still needs an update because we will now have chunks with more than 120 samples or deliberately less. That's not only caused by native histograms, but by other recent changes, too. See #12054 and #12055. So we need to handle that variable number of samples better, plus we should probably add some specific stuff for native histograms. A single histogram sample can have many buckets (or very few), and it would be good to see some stats about that, too. Seeing the size of chunks in bytes would be cool, too. So maybe have separate histograms for float chunks and histogram chunks, and each of them should have a histogram for sample count, chunk size in bytes, and for histogram number of buckets in the chunk (maybe even \"represented buckets\" vs. \"populated buckets\").\r\n> \r\n> About the TODOs:\r\n> \r\n> I agree that a lot depends on availability of text format support (which we'll probably only add to OpenMetrics rather than to the old text format \u2013 see #11265 ). So that means for `check metrics`, `debug metrics`, and `push metrics`, we simply have to wait and see what the OpenMetrics text format will look like. (I guess we could also utilize the protobuf format somehow for these commands, but I don't think the need is urgent enough. Better to get the OM text format rolling first.)\r\n> \r\n> I think `tsdb bench write` would just be \"nice to have\". I'm not sure how much it is used right now. So I would only invest work into it if somebody explicitly requests it.\r\n> \r\n> Which leaves us with `tsdb dump`: That would indeed be good to support native histograms. The dump uses pretty much an ad hoc format. That's a problem of its own, see discussion in #8281 in #8320 , but for now, it simplifies things here. You can essentially make something up (which could be using the `String` methods that are also used in the internal web UI).\r\n> \r\n> So in summary, that's what you could work on right naw:\r\n> \r\n> * `tsdb dump` should be pretty straight forward.\r\n> * `tsdb analyze` needs an update for variable chunk sizes anyway, plus more data for histograms. So that would be a bit more involved, but probably very helpful.\r\n\r\nThanks! I will start working as you suggests","@beorn7 @dgl\r\nI think this issue is only partly resolved, should be opened again.","@fatsheep9146 ack (I missed that GitHub had parsed your \"partly fix #id\" comment as \"fixes\" and so automatically closed this).","> @fatsheep9146 ack (I missed that GitHub had parsed your \"partly fix #id\" comment as \"fixes\" and so automatically closed this).\r\n\r\nThanks!","> So we need to handle that variable number of samples better.\r\n\r\nI think for now we are not able to deduce the samples-per-chunk from the chunk file, could we just optimize the display format of histogram for compact analysis? \r\nInstead of displaying the percentage, we just display the absolute value of the sample count (but with a step size of 10 units.)\r\n\r\nFor example, for now the display format is like\r\n```\r\nCompaction analysis:\r\nFullness: Amount of samples in chunks (100% is 120 samples)\r\n     10%:\r\n     20%:\r\n     30%:\r\n     40%:\r\n     50%:\r\n     60%:\r\n     70%:\r\n     80%:\r\n     90%: ####################\r\n    100%: ################################################################################\r\n```\r\n\r\nWe change to\r\n```\r\n     10:\r\n     20:\r\n     30:\r\n     40:\r\n     50:\r\n     60:\r\n     70:\r\n     80:\r\n     90: \r\n    100: ####################\r\n    110:\r\n    120: ################################################################################\r\n```\r\n\r\n@beorn7 WDYT?","Yeah, sounds good. This change should also apply to conventional float chunks because their max chunk number can be configured by now. See #12055 .","Hi @beorn7 am I correct in understanding based off of this issue promtool is not capable of correctly running unit tests against expressions which leverage histograms? ","https:\/\/github.com\/prometheus\/prometheus\/pull\/12668 should have added that capability. So my expectation is that `promtool` will run unit tests involving native histograms just fine. If not, that would be a bug."],"labels":["priority\/P2","component\/promtool"]},{"title":"add new functions 'consecutive_gt' and 'consecutive_lt'","body":"### Proposal\r\n\r\nWe set an alert for the latency of our system (something like max(job:latency_quantile95) > 100 ). \r\nWe soon found it was too sensitive because our system would experience inherent latency spikes periodically, I can't explain the details about the spike here, we just can't get ride of it for now. \r\n\r\n![img_2](https:\/\/user-images.githubusercontent.com\/7302803\/223084582-6b1912f3-198f-432c-ba89-93b84cc68a0c.png)\r\n\r\n\r\nWe don't wanna be bothered by those spikes and we decide only to alert if the spike persists for a period of time, say 5 min.\r\nWe try to do it like: `(sum_over_time(((sum_over_time((max(job:lantency_quantile95) > bool 100 ) [3m:1m])) >= bool 5)[1d:5m])`. It worked but we found that we are not alarmed when the spike persists 5min and spanned across two 5-min-window. \r\n\r\nWe actually want a function to check whether the series satisfies a certain condition consecutively for a range of points.\r\nI checked the code and it turns out that it's quite convenient to add these new functions.\r\n```golang\r\n\t\"consecutive_gt\": {\r\n\t\tName:       \"consecutive_gt\",\r\n\t\tArgTypes:   []ValueType{ValueTypeMatrix, ValueTypeScalar, ValueTypeScalar},\r\n\t\tReturnType: ValueTypeVector,\r\n\t},\r\n\t\"consecutive_lt\": {\r\n\t\tName:       \"consecutive_lt\",\r\n\t\tArgTypes:   []ValueType{ValueTypeMatrix, ValueTypeScalar, ValueTypeScalar},\r\n\t\tReturnType: ValueTypeVector,\r\n\t},\r\n```\r\nWe don't wanna maintain our own repo, @roidelapluie plz kindly help review whether it's worth adding these functions to the official repo.\r\nIf so, plz help review my pr: ","comments":["Let's continue the discussion in the pull request and figure out the best way to move forward."],"labels":["priority\/Pmaybe","component\/promql","kind\/feature"]},{"title":"Update tsdb\/docs\/\u2026 for native histograms and out-of-order ingestion","body":"### Proposal\n\nBoth native histograms and out-of-order ingestion added various things to TSDB. Those update still needs to be reflected in https:\/\/github.com\/prometheus\/prometheus\/tree\/main\/tsdb\/docs\/format ","comments":[],"labels":["help wanted","priority\/P2","component\/documentation"]},{"title":"Aggregations like avg, min and max produce incorrect results for native histograms","body":"### What did you do?\r\n\r\nAt the moment, there are many aggregations which produce either incorrect or inconsistent behavior when evaluated against a vector of mixed types (floats + native histograms). For example, the `avg` aggregation is calculated as a `sum \/ count` of all samples in a vector. The count is incremented once for each sample, but only float samples are added to the sum. \r\n\r\nThis is optimized in the engine to use a moving mean instead of a sum, but the principle is the same:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/8a8f594b16847831972dd41c9ae606b6dec5afc4\/promql\/engine.go#L2466-L2486\r\n\r\n### What did you expect to see?\r\n\r\nI would expect aggregations that are not supported on native histograms to not take such samples into account.\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nI noticed the issue while working on https:\/\/github.com\/thanos-community\/promql-engine\/pull\/177. It is fairly easy to reproduce using a test by calculating one of the unsupported aggregations against a mixed type vector: https:\/\/github.com\/thanos-community\/promql-engine\/pull\/177\/files#diff-cbf9ce27965b15f1e13be995f164fd6b33a19381364c4be3e8c047d528994495R3195-R3200\r\n\r\nI can help with resolving this edge cases once we have consensus on the expected outcome.\r\n\r\n### System information\r\n\r\n_No response_\r\n\r\n### Prometheus version\r\n\r\n```text\r\n0.42.0\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n_No response_\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n_No response_","comments":["I think with #11687 this should behave more reasonably. I'll keep an eye on it while finishing up that PR.","@fpetkovski now that #11687 is merged, could you verify that the new behavior matches your expectations?","This is the behavior I am seeing for the `avg` function:\r\n\r\n* With histogram only samples, the aggregation returns \r\n```\r\npromql.Vector{promql.Sample{T:50000, F:0, H:(*histogram.FloatHistogram)(nil), Metric:labels.Labels{}}\r\n```\r\nwhich indicates that we don't apply it on histogram samples. Here I would expect it to return nothing instead of a float 0.\r\n\r\n* With mixed types where the sum of floats is 9, count of floats is 2 and count of histograms is 1, the aggregation returns:\r\n```\r\npromql.Vector{promql.Sample{T:50000, F:3.000000000000001, H:(*histogram.FloatHistogram)(nil), Metric:labels.Labels{}}}\r\n```\r\n\r\nHere I would expect it to either return a `NaN`, or if we only include floats in the calculation, it might be better to return 9 \/ 2. It seems that we sum only floats but normalize by the count of both floats and histograms.\r\n\r\nI can fix this behavior if we agree on the expected output.","OK, I guess we count histogram samples for purposes of the averaging, but we should completely ignore them.\r\n\r\nHowever, since we are about to implement multiplication and division for histograms, see #12262, I think we should just handle `avg` fully, i.e. create an \"average histogram\" for the \"histograms only\" case and return nothing for mixed cases (corresponding to `sum`, which also supports both histograms and floats \u2013 this case will also return a warning once #12152 is in).","Added avg to the open PR according to @beorn7 's comment above.","I'll have a look ASAP (apologies again for the delays - trying to get through my backlog in the limited time I have).","@zenador @beorn7 PR #12262 merged. Seems like this should have addressed the avg computation too?\n@fpetkovski any chance you could check if you still see inconsistent behavior?","I have an (ancient) TODO on my list to go through all functions and aggregators and make sure they behave as expected (and add tests using the fancy test framework we have). I have some vague memory that some cases aren't covered yet. "],"labels":["help wanted","priority\/P2","component\/promql"]},{"title":"Help\/Description entry for recording rules","body":"### Proposal\n\nToday we have\r\n- `HELP` entry for metrics\r\n- `Description` and `Summary` for Alerting rules\r\n\r\nBut nothing for Recording rules, I think it will be beneficial to have\/support a description entry for it\r\n\r\nExample:\r\n```\r\ngroups:\r\n  - name: rules\r\n    rules:\r\n      - record: xxx:yyyy_yyy:zzzz\r\n        expr: <PromQL expression>\r\n        description: \"foo bar\"\r\n```\r\n\r\nThis will help understanding the rule, generating automatic documentations etc","comments":["Just that right now, we have nowhere to store this.\r\n\r\nBut I also think we should treat the help string of metrics in a 1st class way (i.e. store them properly in the TSDB instead of treating them in the ephemeral way we do it now). Once we do that, having a help string for recording rules would just be consistent."],"labels":["component\/rules"]},{"title":"support aliyun ecs discovery","body":"### Proposal\r\n\r\nHi~ We are alibaba aliyun team, we have a aliyun ecs discovery implement.\r\nhttps:\/\/github.com\/AliyunContainerService\/prometheus\/tree\/master\/discovery\/ecs\r\n\r\nhttps:\/\/github.com\/AliyunContainerService\/prometheus\/blob\/feature\/aliyun-ecs-discovery\/discovery\/aliyun\/README.md\r\n\r\nCan we take a merge-request of our commit to master branch?\r\n\r\n- pressure test: this implement support pressure test with 1K+ ECS, and every ECS have 20 tags, support frequency ECS scale scenario.\r\n- support filter with ECS label.\r\n- use scrolling API to discovery ecs to support frequency ECS scale change scenario.\r\n- we can do some translation to English.","comments":["A quick glance at the link shows me that we could accept it if there are enough tests and if we do not take configuration and secrets from environment variables. We'd also need an alibaba account to test this.","@roidelapluie \r\nok, Thanks~\r\nI will make a merge request with configuration refine and support enough test case, and refine comments and README.\r\nWe can provide alibaba account for testing.","Is there any progress? I am very looking forward to the pr being merged","WIP, I m trying to make it out in this month.\r\n\r\n> Is there any progress? I am very looking forward to the pr being merged\r\n\r\n","Any updates on this? We are awaiting for this to be merged. ","> Any updates on this? We are awaiting for this to be merged.\r\n\r\nSorry i will give a merge request recently.\r\n\r\nThis is our implement, but need some refine like translation, config refactor,  and refine test ut.\r\nhttps:\/\/github.com\/AliyunContainerService\/prometheus\/blob\/feature\/aliyun-ecs-discovery\/discovery\/aliyun\/README.md\r\n\r\nif you need in hurry, u can use it as a temporary plan. "],"labels":["component\/service discovery","priority\/P3","kind\/feature"]},{"title":"histograms: Support expansion of native histogram values in templating","body":"In particular alert templates (i.e. what to do if an alert description contains `{{ $value }}` and the value is a native histogram), but let's not forget console templates.","comments":["Thoughts:\r\n\r\nA pure text representation of the values will be difficult to read for a human. We could additionally add functions that render it in different ways \u2013 like `$value | histogramASCII` to build a graphical approximation out of ASCII art, `$value | histogramD3` for a context where d3.js is available to do the rendering. If we want to go that far, even one to produce a Base64 encoded PNG for inline `data:` URLs.\r\n\r\nWe need a safe default behavior, but most of the time the user will know that they expect a histogram as `$value`. Different representations will work in different contexts. Functions to transform the representation let the user choose one that is appropriate in the context.\r\n\r\nThis may break down when the same text (alert description) is used in different contexts (email, Slack, Pagerduty notification). For these cases it might be helpful if the default text representation is uniquely identifiable, so that a filter in the receiver specific template can walk over the whole text and do something to any histogram it finds.","@marctc plans to work on this.\r\n","Now @suntala is giving it a try.","Hey @beorn7, \r\n@britishrum and I would like to be assigned to this one please.","@britishrum apparently GH requires you to actually comment on this issue before I can assign it to you. :thinking: ","@beorn7 commenting now, could you try again please? "],"labels":["priority\/P2"]},{"title":"Synthetic metric (e.g.`scrape_samples_scraped`) missing HELP metadata","body":"### What did you do?\r\n\r\nI downloaded and ran the Prometheus binary with the default config to scrape itself.\r\n\r\nI found that:\r\n- I could graph `scrape_samples_scraped` at `\/graph` endpoint\r\n- But viewing the metrics metadata at `\/api\/v1\/metadata` there was no entry for `scrape_samples_scraped`\r\n\r\nI believe the same is true of:\r\n- `scrape_series_added`\r\n- `scrape_samples_post_metric_relabeling`\r\n\r\nI'm trying to both:\r\n- Understand better what this metric is, and\r\n- Help other users of Grafana to gain the same understanding.\r\n\r\nWithin Grafana you can typically see the HELP part of the metric-metadata on hover in the metrics browser:\r\n![image](https:\/\/user-images.githubusercontent.com\/2903904\/214284449-a83dd69d-3fc3-4f14-8b3f-f8f0021b1904.png)\r\n\r\nBut for this metric, there's nothing useful:\r\n![image](https:\/\/user-images.githubusercontent.com\/2903904\/214263887-87e63d97-511c-4da8-ada9-8bbadea6d2df.png)\r\n\r\nI'm interested in this metric in particular because I think it can help diagnose problems with people setting their scrape_interval much smaller than they need (and shipping data far too frequently). For example, as described here:\r\nhttps:\/\/grafana.com\/docs\/grafana-cloud\/billing-and-usage\/control-prometheus-metrics-usage\/changing-scrape-interval\/#grafana-agent\r\n\r\n### What did you expect to see?\r\n\r\nHELP metadata for this metric both on the \/api\/v1\/metadata endpoint and in Grafana (on hover, in the metrics-browser).\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nNo metadata\r\n\r\n### System information\r\n\r\nDarwin 21.6.0 arm64\r\n\r\n### Prometheus version\r\n\r\n```text\r\nprometheus, version 2.40.4 (branch: HEAD, revision: 414d31aee6586a5f29e755ae059b7d7131f1c6c8)\r\n  build user:       root@45956a3006ca\r\n  build date:       20221129-11:04:07\r\n  go version:       go1.19.3\r\n  platform:         darwin\/arm64\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```yaml\r\n# my global config\r\nglobal:\r\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n  # scrape_timeout is set to the global default (10s).\r\n\r\n# Alertmanager configuration\r\nalerting:\r\n  alertmanagers:\r\n    - static_configs:\r\n        - targets:\r\n          # - alertmanager:9093\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n  - \"first_rules.yml\"\r\n  # - \"second_rules.yml\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n  - job_name: \"prometheus\"\r\n\r\n    # metrics_path defaults to '\/metrics'\r\n    # scheme defaults to 'http'.\r\n\r\n    static_configs:\r\n      - targets: [ \"localhost:9090\" ]\r\n```\r\n\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n_No response_","comments":["The `scrape_...` metrics are so-called synthetic metrics. They are not exposed by the target, but they are created by Prometheus itself during scraping based on the scrape. Another synthetic metric (and the most famous of all) is the `up` metric.\r\n\r\nMetadata is still not a native concept of the internal Prometheus data model. It is only really present in the exposition format (from where it is quite superficially handled and made available via the metadata API). But since synthetic metrics are only created internally and never exposed, they never get metadata attached.\r\n\r\nOne might argue the metadata should still be created for the sake of the metadata API. To me, that would feel like patching an already quite patchy API even more. It would be much better to finally fully embrace metadata in the internal data model (as it is beginning right now, out of necessity, with native histograms).","To get back to your original concern, you can estimate the scrape interval by counting the number of times the `up` metrics were inserted over the last hour with the following query:\r\n\r\n```\r\ncount_over_time(up[1h])\r\n```\r\n\r\nThis query returns the number of times the targets were scraped over the last hour.","I get that this is 'not as easy as it looks', but can someone confirm the metric type of `scrape_samples_scraped`? Am I correct that its a gauge and not a counter?\r\n\r\nI'm guessing its a gauge based [on this blog post](https:\/\/valyala.medium.com\/prometheus-storage-technical-terms-for-humans-4ab4de6c3d48), which says\r\n\r\n```\r\nIngestion rate can be calculated from scrape_samples_scraped metric exposed by Prometheus using the following PromQL query:\r\n\r\nsum_over_time(scrape_samples_scraped[5m]) \/ 300\r\n```","Yes it is a gauge ","Thanks! "],"labels":["not-as-easy-as-it-looks","priority\/P3","component\/scraping","kind\/feature"]},{"title":"Simplify `chunks.Meta` by removing the OOO fields","body":"### Proposal\n\nWith OOO support, we introduced 3 new fields to the chunk meta:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/f88a0a7d83bb9ef4387ff158d43f5a0c580fffdf\/tsdb\/chunks\/chunks.go#L125-L132\r\n\r\nThis unnecessarily bloats the chunk meta because these fields are only required by the Head block's OOO Head reader (the part that reads the OOO data from the Head), and are not required everywhere else. We can likely find a way that does not require these fields to be inside the meta. For example, store that information with the Head block's OOO reader wrapper.\r\n\r\nHow to do this is still TBD. Ideas are welcome :)\r\n\r\n---\r\n\r\nThis issue is broken down from https:\/\/github.com\/prometheus\/prometheus\/issues\/11329","comments":["I want to work on this issue. Can you please assign it to me?","Hi @codesome , I'm currently working on this issue.\r\nCould you please give more clarification on the example solution you've mentioned?\r\n\r\nAlternate solution:\r\nAfter going through the code, for a `Series()` call we are storing the same markers data for the overlapping chunks.\r\nAs these markers is mainly used in `oooMergedChunk` method, we can move it to `memSeriesOOOFields` struct.\r\n\r\nPlease do let me know your thoughts on this."],"labels":["component\/tsdb"]},{"title":"Remove OOO Head compaction's dependency on the in-order Head compaction","body":"### Proposal\n\nIt is possible that the in-order Head stopped getting data for a while, while the OOO Head kept getting a lot of old data. In which case, we should compact the OOO data eventually.\r\n\r\nCurrently, we attempt OOO compaction only if the in-order Head gets compacted. We should remove that dependency to attempt OOO compaction every 2h as well. Irrespective of block duration here, we can attempt OOO compaction every 2h because there is no cap on how much OOO data you can ingest.","comments":["Hi, I would like to take on this issue and try to submit a PR by this weekend.","@MarkintoshZ I see that your PR is not yet merged. Are you still actively working on this? If not I would like to pick this up.","Hi @saurabhsuniljain, I have completed the PR and now I am awaiting feedback from @codesome. Thank you for your assistance though!","**@MarkintoshZ** is the issue **completed**","I assume the issue is completed. Is it?\r\n","I'm sorry for not getting back to you guys sooner. I am done with the PR but still waiting for it to be merged."],"labels":["help wanted","low hanging fruit","component\/tsdb"]},{"title":"Option to delay rule evaluations","body":"### Proposal\n\nWith out-of-order ingestion, now time series can get old data anytime. There could be recording rules that need to be re-calculated for a timestamp because they got new samples. But it is also possible that you can identify a set of rule groups that tend to get delayed samples (especially when coupled with the remote-write receiver). In this case, having the ability to delay the evaluation of rule groups can keep the recording rule results more accurate.\r\n\r\nThe proposal is to add a new field `evaluation_delay` to the rule group. And let's say the delay is `d`, whenever the rule has to evaluate for time `now()`, it would just evaluate for time `now()-d` and produce samples for that time.\r\n\r\nThis is something done in Mimir and works well there. A reference implementation of how it will look in Prometheus is here https:\/\/github.com\/grafana\/mimir-prometheus\/pull\/155\/.\r\n\r\ncc @roidelapluie @beorn7 @juliusv\r\n\r\n","comments":["Sounds reasonable to me. I mean I'm always more on the conservative side when adding new options since they come with lots of costs (docs + learning + code + mental overhead), but since we already have out-of-order ingestion now, we probably need to allow delayed rule evaluations as well.","We could wait for some time and see if the community asks for it.","Yup, either is ok with me.","This also could help with the [ingestion delay issue discussed at the dev summit last April](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#heading=h.pnxuet28ppx3) (search for \"ingestion delay\" to find the relevant section).\r\n\r\nIt's not a perfect solution (and the discussion at the dev-summit suggested there might not be one), but depending on your realtime requirements, it might actually be a good practice to add a bit of delay to most rule groups.","That makes sense. I used offset in PromQL queries in the past to simulate this. I propose we validate that OOO window is larger than the delay.","> I propose we validate that OOO window is larger than the delay.\r\n\r\nI think we should not validate it, because it gets complicated quickly. For example, even without OOO window set, you can still get samples up to 1hr old that is not out-of-order. And some backfilling could be going on behind the scenes. So I would not add restrictions.","From the above comments, I think we don't need to wait for more ask from the community. Maybe they are already using a workaround as Julien mentioned.","The restriction is to make sure that we can ingest the result in the tsdb.","> The restriction is to make sure that we can ingest the result in the tsdb.\r\n\r\nOh yeah, did not think about that. max_delay=max(1h, OOO window).","If Prometheus is used only for ingestion and not scraping, should there be a global delay setting that will be applied to all the rule groups?"],"labels":["component\/rules","kind\/feature"]},{"title":"Deleting data points older than TSDB min date with 'delete_series' API endpoint fails","body":"### What did you do?\n\n1. Set `out_of_order_time_window` in storage configuration to allow the storage of data points that are older than the TSDB's minimum timestamp by a certain amount of time.\r\n2. Generate and store some data points with timestamps older than the TSDB's minimum timestamp.\r\n3. Use the` \/api\/v1\/admin\/tsdb\/delete_series` API endpoint to delete the data points, specifying a match parameter that matches the data points you created.\r\n4. Observe that the data points are not deleted.\n\n### What did you expect to see?\n\nWhen I use the `\/api\/v1\/admin\/tsdb\/delete_series` API endpoint to delete the data points I created, I expect Prometheus to delete all data points matching the specified match parameters, regardless of their timestamps.\n\n### What did you see instead? Under which circumstances?\n\nThe `delete_series` API does not delete data points with timestamps older than the TSDB's minimum timestamp.\n\n### System information\n\nLinux 6.0.11-300.fc37.x86_64 x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.40.7 (branch: HEAD, revision: ab239ac5d43f6c1068f0d05283a0544576aaecf8)\r\n  build user:       root@afba4a8bd7cc\r\n  build date:       20221214-08:49:43\r\n  go version:       go1.19.4\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n```yaml\nstorage:\r\n  tsdb:\r\n    out_of_order_time_window: 1w\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nts=2022-12-31T02:45:19.628Z caller=main.go:512 level=info msg=\"No time or size retention was set so using the default time retention\" duration=15d\r\nts=2022-12-31T02:45:19.628Z caller=main.go:556 level=info msg=\"Starting Prometheus Server\" mode=server version=\"(version=2.40.7, branch=HEAD, revision=ab239ac5d43f6c1068f0d05283a0544576aaecf8)\"\r\nts=2022-12-31T02:45:19.628Z caller=main.go:561 level=info build_context=\"(go=go1.19.4, user=root@afba4a8bd7cc, date=20221214-08:49:43)\"\r\nts=2022-12-31T02:45:19.628Z caller=main.go:562 level=info host_details=\"(Linux 6.0.11-300.fc37.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Dec 2 20:47:45 UTC 2022 x86_64 localhost-live.Home (none))\"\r\nts=2022-12-31T02:45:19.628Z caller=main.go:563 level=info fd_limits=\"(soft=524288, hard=524288)\"\r\nts=2022-12-31T02:45:19.628Z caller=main.go:564 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nts=2022-12-31T02:45:19.629Z caller=web.go:559 level=info component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nts=2022-12-31T02:45:19.630Z caller=main.go:993 level=info msg=\"Starting TSDB ...\"\r\nts=2022-12-31T02:45:19.630Z caller=tls_config.go:232 level=info component=web msg=\"Listening on\" address=[::]:9090\r\nts=2022-12-31T02:45:19.630Z caller=tls_config.go:235 level=info component=web msg=\"TLS is disabled.\" http2=false address=[::]:9090\r\nts=2022-12-31T02:45:19.633Z caller=head.go:562 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nts=2022-12-31T02:45:19.633Z caller=head.go:606 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=1.581\u00b5s\r\nts=2022-12-31T02:45:19.633Z caller=head.go:612 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nts=2022-12-31T02:45:19.633Z caller=head.go:683 level=info component=tsdb msg=\"WAL segment loaded\" segment=0 maxSegment=0\r\nts=2022-12-31T02:45:19.634Z caller=head.go:711 level=info component=tsdb msg=\"WBL segment loaded\" segment=0 maxSegment=0\r\nts=2022-12-31T02:45:19.634Z caller=head.go:720 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=25.653\u00b5s wal_replay_duration=290.747\u00b5s wbl_replay_duration=123.648\u00b5s total_replay_duration=461.634\u00b5s\r\nts=2022-12-31T02:45:19.635Z caller=main.go:1014 level=info fs_type=9123683e\r\nts=2022-12-31T02:45:19.635Z caller=main.go:1017 level=info msg=\"TSDB started\"\r\nts=2022-12-31T02:45:19.635Z caller=main.go:1197 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nts=2022-12-31T02:45:21.019Z caller=main.go:1234 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=1.384764925s db_storage=746ns remote_storage=1.026\u00b5s web_handler=285ns query_engine=536ns scrape=1.384541556s scrape_sd=28.161\u00b5s notify=3.212\u00b5s notify_sd=9.203\u00b5s rules=5.028\u00b5s tracing=20.258\u00b5s\r\nts=2022-12-31T02:45:21.020Z caller=main.go:978 level=info msg=\"Server is ready to receive web requests.\"\r\nts=2022-12-31T02:45:21.020Z caller=manager.go:944 level=info component=\"rule manager\" msg=\"Starting rule manager...\"\r\nts=2022-12-31T02:45:26.561Z caller=compact.go:519 level=info component=tsdb msg=\"write block\" mint=1672436696000 maxt=1672437600000 ulid=01GNK13RWKHAQGK0TCB7FXNG8J duration=13.847104ms\r\nts=2022-12-31T02:45:26.562Z caller=head.go:1213 level=info component=tsdb msg=\"Head GC completed\" caller=truncateMemory duration=663.29\u00b5s\n```\n","comments":["Hi @codesome @jesusvazquez, I would like to work on this.\r\n\r\nI tried to reproduce the issue and was able to successfully do reproduce it. I was going through the design document - [Support for out-of-order samples in the TSDB ](https:\/\/docs.google.com\/document\/d\/1Kppm7qL9C-BJB1j6yb6-9ObG3AbdZnFUBYPNNWwDBYM\/edit#heading=h.unv3m5m27vuc), I read under \"**Non goals for the initial version**\" that deletion is not supported for OOO data. Also, while going through the code and one of your blog which quoted \"_any deletions to the data are stored as tombstones in a separate file while the data still stays on disk. So when the tombstones are touching more than some % of the series, we need to remove that data from the disk_\" and Prometheus doesn't support **Tombstones for out of order metrics**. \r\n\r\nCould you provide some insight into why the support for tombstones for out-of-order metrics has not been implemented, particularly if there are any logical complications that have made this challenging? Also, can you please clarify whether if there are any plans to implement it in the future? ",":wave: @nidhey27 \r\n\r\n> Could you provide some insight into why the support for tombstones for out-of-order metrics has not been implemented\r\n\r\nIn all honestly it was left out at the time. The remaining parts of the out-of-order feature had already quite enough complexity and we needed to decide what to do. So we chased read, write and compaction support and then production experience with those.\r\n\r\n> Also, can you please clarify whether if there are any plans to implement it in the future?\r\n\r\nWe definitely want deletion for out-of-order samples. But we do not have any plans for doing so in the sort term so your contribution would be really appreciated.\r\n\r\nI know some bits of the TSDB better than others and tombstones and the deletion features is not one of my strengths. Maybe @codesome can help us here. In the meantime I'll try to investigate this whenever I get some spare time so I can also provide guidance."],"labels":["component\/tsdb"]},{"title":"Simplify TSDB Appender interface","body":"### Proposal\r\n\r\nI am specifically talking about this interface https:\/\/github.com\/prometheus\/prometheus\/blob\/e1b708200853371517480176da0a03437b3bb2c2\/storage\/interface.go#L219\r\n\r\nWith every new data type, we are requiring breaking changes in the interface. For example recently the introduction of `AppendHistogram()`. #11522 again extends `AppendHistogram()` to accept two possible types of histograms.\r\n\r\nThis requires all downstream consumers to keep changing the code everywhere. Also, the interface is getting overloaded IMO.\r\n\r\nI propose this simplification which will reduce the refactoring required in the code.\r\n\r\n---\r\n\r\nFor starters, we could combine `Append()` and `AppendHistogram()` as:\r\n\r\n```go\r\ntype AppendSample struct {\r\n\t\/\/ Checked in the order as present here. If h and fh are nil, f is used.\r\n\th  *histogram.Histogram\r\n\tfh *histogram.FloatHistogram\r\n\tf  float64\r\n}\r\n\r\ntype Appender interface {\r\n\r\n\tAppend(ref SeriesRef, l labels.Labels, t int64, v AppendSample) (SeriesRef, error)\r\n\r\n\t\/\/ ...\r\n}\r\n```\r\n\r\n---\r\n\r\nWe could go further by moving timestamp into `AppendSample`.\r\n\r\n```go\r\ntype AppendSample struct {\r\n\t\/\/ Checked in the order as present here. If h and fh are nil, f is used.\r\n\th  *histogram.Histogram\r\n\tfh *histogram.FloatHistogram\r\n\tf  float64\r\n\tt  int64\r\n}\r\n\r\ntype Appender interface {\r\n\r\n\tAppend(ref SeriesRef, l labels.Labels, s AppendSample) (SeriesRef, error)\r\n\r\n\t\/\/ ...\r\n}\r\n```\r\n\r\n---\r\n\r\nIf allocations are not a problem here, we could even go further and combine exemplars in this, and get rid of `AppendExemplar()`.\r\n\r\n```go\r\ntype AppendSample struct {\r\n\t\/\/ Checked in the order as present here. If h, fh, and e are nil, f is used.\r\n\th  *histogram.Histogram\r\n\tfh *histogram.FloatHistogram\r\n\te  *exemplar.Exemplar \/\/ If this is present, t is ignored.\r\n\tf  float64\r\n\tt  int64\r\n}\r\n\r\ntype Appender interface {\r\n\r\n\tAppend(ref SeriesRef, l labels.Labels, s AppendSample) (SeriesRef, error)\r\n\r\n\t\/\/ ...\r\n}\r\n```\r\n---\r\ncc @bwplotka ","comments":["As a follow-up, I have opened https:\/\/github.com\/prometheus\/prometheus\/issues\/11764. ","Nitpick: this sounds interesting, however if you add a new kind of value to AppendSample - what compiler mechanism alerts you to the new type? With the current way you get a compiler error since not all methods would be implemented. Or if we had an enum in AppendSample to explicitly say the type, you could use the linter `exhaustive` to check for it.","@krajorama you make a good point. Here is my take on that.\r\n\r\nWhat I have seen usually is that if Prometheus added something new, both Prometheus and downstream users do not need to support it in all the components upfront. The only thing the compile error achieves is that you end up satisfying the interface at all places with an empty function for a start so that you can compile, and not necessarily that all the components support this new datatype (for example exemplars or native histograms). We will need to go through all the un-implemented components again later anyway to support new things.\r\n\r\nAnd usually, Prometheus adds new things such that it does not break the old things. So the addition of a new data type in the AppendSample won't break anything that exists right now.\r\n\r\nWith this interface change, it removes the requirement of refactoring all the code up front. And add support gradually as required (some of them may never get the support). From the above, I infer that even with a compilation error, eventually, you will have to search for those functions later anyway to add support one by one. Similarly, with this change, you just search for Append() and add support where required.\r\n\r\nAlso, that said, I am sure downstream consumers will have an eye on what new data types are incoming if they are using the TSDB code closely and won't miss it."],"labels":["kind\/enhancement","component\/tsdb"]},{"title":"Proposal: Concurrent evaluation of alert rules in an alert group","body":"### Context\r\n\r\nAccording to [the documentation](https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/recording_rules\/#recording-rules) all rules within an alert group are evaluated sequentially. This makes sense since an alert group could contain one or more recording rules that have to be evaluated before other alert rules can subsequently refer to the recording.\r\n\r\nThis feels like an artificial restriction on the concurrent evaluation of rules within an alert group. \r\n\r\nCurrently, if we'd want alert rules to be evaluated concurrently they must exist in different alert groups, leading to the creation of _many_ different alert groups at scale.\r\n\r\nIt becomes tricky to find the rule group you need since \"grouping\" implies the rules belong together, but we've placed them in _different_ groups because having too many rules in a single group has adverse effects on evaluation.\r\n\r\n### Proposal\r\n\r\nIn order not to introduce any breaking changes to the existing evaluation behavior I'd like to propose a `concurrent: <true|false>` property that can be configured for a `<rule_group>` to opt-in to concurrent evaluation mode.\r\n\r\nWith concurrent alert rule evaluation all recording rules would be \"hoisted\" automatically to be evaluated first (and also concurrently) so other alert rules can still use the recorded series and all subsequently alert rules can be evaluated concurrently.\r\n\r\nThis means alert groups can group a large amount of rules without having adverse effects on the evaluation duration for the entire group.\r\n\r\n```\r\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2524 Alert Group \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u250c \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2510 \u2502\r\n\u2502 \u250c \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502      \u2502 \u2502   Alert Rule #1   \u2502 \u2502 \u2502\r\n\u2502 \u2502\u2502Recording Rule #1 \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502      \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\r\n\u2502 \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2500\u2500\u2500\u2500\u2500\u25b6  \u2502   Alert Rule #2   \u2502   \u2502\r\n\u2502  \u2502Recording Rule #2 \u2502\u2502      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\r\n\u2502 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\r\n\u2502  \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2518      \u2502 \u2502   Alert Rule #3   \u2502 \u2502 \u2502\r\n\u2502                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\r\n\u2502                             \u2514 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2518 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\r\n\u2551 Dashed line means evaluated concurrently \u2551\r\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\r\n```\r\n\r\nLastly, I think hoisting recording rules automatically would be a nice quality-of-life improvement for the existing (sequential) alert group behavior so alert group authors don't have to think about or manually specify the order of alert rules within a group. \r\n\r\nIf this would be a breaking change perhaps another property like `hoist_recording_rules: <true|false>` could be an option.","comments":["is this ticket actively being worked on?","related https:\/\/github.com\/prometheus\/prometheus\/pull\/12946"],"labels":["component\/rules"]},{"title":"TSDB: No warning or error when creating series collisions via metric relabeling","body":"### What did you do?\n\nRun Prometheus like this to drop the disambiguating `quantile` label from the `go_gc_duration_seconds` summary:\r\n\r\n```\r\nglobal:\r\n  scrape_interval: 5s\r\n    \r\nscrape_configs:\r\n- job_name: prometheus\r\n  static_configs:\r\n  - targets:\r\n    - localhost:9090\r\n  metric_relabel_configs:\r\n  - action: keep\r\n    source_labels: [__name__]\r\n    regex: go_gc_duration_seconds\r\n  - action: labeldrop\r\n    target_label: quantile\r\n```\r\n\r\nFor reference, the underlying metrics are:\r\n\r\n```bash\r\n# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.\r\n# TYPE go_gc_duration_seconds summary\r\ngo_gc_duration_seconds{quantile=\"0\"} 6.6283e-05\r\ngo_gc_duration_seconds{quantile=\"0.25\"} 7.8936e-05\r\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000126586\r\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.000198849\r\ngo_gc_duration_seconds{quantile=\"1\"} 0.000247392\r\ngo_gc_duration_seconds_sum 0.001408357\r\ngo_gc_duration_seconds_count 10\r\n```\n\n### What did you expect to see?\n\nSome kind of warning about multiple series being mapped into one, like an out-of-order \/ duplicate timestamp ingestion error and \/ or a scrape failure.\n\n### What did you see instead? Under which circumstances?\n\nNo error or warning. The first series is ingested (with the dropped label removed), but the others are not. Or at least they are not visible in queries. This way I can lose a bunch of series without ever knowing.\r\n\r\nThis is due to the fact that multiple series from one scrape are written to the same Appender, but that Appender does not compare timestamps within its own batched set of series, only with its own initial minimum valid time:\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/c3fac587ef3c7d515e319844042ee0dddab54cbc\/tsdb\/head_append.go#L344\r\n\r\nMaybe the Appender could be made to check samples within one append?\n\n### System information\n\nLinux 6.0.12-arch1-1 x86_64\n\n### Prometheus version\n\n```text\n2.40.5\n```\n\n\n### Prometheus configuration file\n\n```yaml\nglobal:\r\n  scrape_interval: 5s\r\n    \r\nscrape_configs:\r\n- job_name: prometheus\r\n  static_configs:\r\n  - targets:\r\n    - localhost:9090\r\n  metric_relabel_configs:\r\n  - action: keep\r\n    source_labels: [__name__]\r\n    regex: go_gc_duration_seconds\r\n  - action: labeldrop\r\n    target_label: quantile\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["@juliusv I can able to recreate the issue and below I have provided my observations and proposal to solve it.\r\n\r\n**Problem**: As you mentioned, when scraping, multiple samples with the same label set but different values are added to the series sample (a.samples) inside the Append() call of (a *headAppender). This happens because the headMaxt is not updated while processing the samples in the given scrape. However, during the commit call [scrapeAndReport](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/scrape\/scrape.go#L1330), samples with the same timestamp but different values are detected, and ErrDuplicateSampleForTimestamp is returned by [appendable](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/tsdb\/head_append.go#L925), as the series time is updated for every sample. This error is not handled, which is why it is silently removed in the given scenario.\r\n\r\n**Solution**: It appears that this inconsistency between the commit and append methods has already been [discussed](https:\/\/github.com\/prometheus\/prometheus\/discussions\/10305). Based on that discussion and my code walkthrough, I suggest the following steps to mitigate this issue:\r\n\r\nInside the commit() method, we can handle the error returned by appendable by counting it for every sample.\r\nAt the end of the method, if the count is non-zero, we can log a warning message and also increment a new metric called 'prometheus_tsdb_conflicting_samples_for_timestamp_total'.\r\nThis approach ensures that there are no changes to the commit() method signature, which would affect other dependencies. Additionally, it provides users with visibility of the issue through logs and metrics. Please let me know if this solution is satisfactory.\r\n\r\n@bboreham @codesome \r\n\r\n","@jesusvazquez "],"labels":["kind\/bug","component\/tsdb"]},{"title":"Promtool documentation","body":"### Proposal\n\nI don't believe we have any complete documentation of Promtool and its subcommands anywhere besides the help flag of the binary itself, save for a few snippets in various places. I think it might be useful to put together a full page in the docs both to help with the use of the tool and also the discovery of its features.","comments":["+1.\r\n\r\nA quick list of things we do have:\r\n- https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/unit_testing_rules\/\r\n- https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/recording_rules\/#syntax-checking-rules\r\n- https:\/\/prometheus.io\/docs\/prometheus\/latest\/storage\/#backfilling-from-openmetrics-format\r\n- https:\/\/prometheus.io\/docs\/prometheus\/latest\/migration\/#recording-rules-and-alerts \"update rules\" -- which has been removed now...\r\n- https:\/\/prometheus.io\/docs\/guides\/basic-auth\/#creating-web-yml\r\n\r\nI think it makes sense to keep the main docs for some of those where they are, but maybe link from an index \/ other commands page.","Agreed, sounds good.","Thanks to @roidelapluie, we have https:\/\/prometheus.io\/docs\/prometheus\/latest\/command-line\/promtool\/ (generated) now.\r\n\r\nWe can enrich it and add links to relevant materials you shared. I think we can reword this issue."],"labels":["component\/promtool","priority\/P3","component\/documentation"]},{"title":"Intermittent \"sample too old\" errors in logs (producing ~2h gaps on all charts)","body":"### What did you do?\r\n\r\nSpin-up of https:\/\/github.com\/prometheus\/prometheus\/issues\/7669.\r\n\r\nAfter installing and configuring Prometheus as data receiver and otel-collector as data producer, ~2-hour gaps occur on all charts. \"too old sample\" debug messages are generated in logs during gaps.\r\n\r\nAnother exporter (local file on disk) contains all metrics with no gaps.\r\n\r\n### What did you expect to see?\r\n\r\nCharts with no gaps.\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\n\"too old sample\" debug messages in logs and gaps on charts:\r\n![image](https:\/\/user-images.githubusercontent.com\/4700523\/202786207-8b0bb0c2-7347-4cee-b015-710d4d8d9906.png)\r\n\r\n### System information\r\n\r\nLinux 5.10.0-8-amd64 x86_64 (Debian Bullseye)\r\n\r\n### Prometheus version\r\n\r\n```text\r\nprometheus, version 2.40.2 (branch: HEAD, revision: a07a94a5abb8a979d8aa87297f77f3979148b2da)\r\n  build user:       root@1b4b53e3f125\r\n  build date:       20221117-13:40:12\r\n  go version:       go1.19.3\r\n  platform:         linux\/amd64\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```yaml\r\nglobal:\r\n  scrape_interval: 45s\r\n  evaluation_interval: 30s\r\n  scrape_timeout: 30s\r\n\r\nscrape_configs:\r\n  - job_name: prometheus\r\n    honor_labels: true\r\n    static_configs:\r\n      - targets: [\"otel-gw:1234\"]\r\n\r\nstorage:\r\n  tsdb:\r\n    out_of_order_time_window: 30m\r\n```\r\n\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n```text\r\nLogs before gap:\r\n\r\nprometheus_1  | ts=2022-11-18T17:01:26.752Z caller=compact.go:519 level=info component=tsdb msg=\"write block\" mint=1668780041728 maxt=1668787200000 ulid=01GJ5V08AJWAM6BEYKYR7E1VYV duration=13.595184ms\r\nprometheus_1  | ts=2022-11-18T17:01:26.752Z caller=head.go:1212 level=info component=tsdb msg=\"Head GC completed\" caller=truncateMemory duration=606.169\u00b5s\r\nprometheus_1  | ts=2022-11-18T17:01:26.753Z caller=checkpoint.go:100 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=20 to_segment=21 mint=1668787200000\r\nprometheus_1  | ts=2022-11-18T17:01:26.756Z caller=head.go:1184 level=info component=tsdb msg=\"WAL checkpoint complete\" first=20 last=21 duration=3.576443ms\r\nprometheus_1  | ts=2022-11-18T17:01:26.756Z caller=db.go:1203 level=info component=tsdb msg=\"compact ooo head resulted in no blocks\" duration=604ns\r\nprometheus_1  | ts=2022-11-18T17:01:26.772Z caller=compact.go:460 level=info component=tsdb msg=\"compact blocks\" count=3 mint=1668751215789 maxt=1668772800000 ulid=01GJ5V08B4ZKGZ48ES2TC5WEB2 sources=\"[01GJ4ZFZCA7X56K3EP9DBYPCSR 01GJ56BPM9VAR1TGARJW4S61W1 01GJ5D8STGXF47Q5BX1BZ1EB58]\" duration=15.962322ms\r\nprometheus_1  | ts=2022-11-18T17:01:26.773Z caller=db.go:1541 level=info component=tsdb msg=\"Deleting obsolete block\" block=01GJ4ZFZCA7X56K3EP9DBYPCSR\r\nprometheus_1  | ts=2022-11-18T17:01:26.774Z caller=db.go:1541 level=info component=tsdb msg=\"Deleting obsolete block\" block=01GJ56BPM9VAR1TGARJW4S61W1\r\nprometheus_1  | ts=2022-11-18T17:01:26.775Z caller=db.go:1541 level=info component=tsdb msg=\"Deleting obsolete block\" block=01GJ5D8STGXF47Q5BX1BZ1EB58\r\n```\r\n\r\nLogs during gap:\r\n```\r\nprometheus_1  | ts=2022-11-18T17:34:04.415Z caller=scrape.go:1633 level=debug component=\"scrape manager\" scrape_pool=otel-gw-consumer target=http:\/\/otel-gw:1234\/metrics msg=\"Unexpected error\" series=\"system_network_connections{env=\\\"prod\\\",host_name=\\\"trafdp101\\\",job=\\\"trafd-group\\\",os_type=\\\"linux\\\",protocol=\\\"tcp\\\",service_name=\\\"trafd-group\\\",state=\\\"FIN_WAIT1\\\"}\" err=\"too old sample\"\r\nprometheus_1  | ts=2022-11-18T17:34:04.415Z caller=scrape.go:1358 level=debug component=\"scrape manager\" scrape_pool=otel-gw-consumer target=http:\/\/otel-gw:1234\/metrics msg=\"Append failed\" err=\"too old sample\"\r\nprometheus_1  | ts=2022-11-18T17:34:49.418Z caller=scrape.go:1633 level=debug component=\"scrape manager\" scrape_pool=otel-gw-consumer target=http:\/\/otel-gw:1234\/metrics msg=\"Unexpected error\" series=\"system_network_connections{env=\\\"prod\\\",host_name=\\\"trafdp101\\\",job=\\\"trafd-group\\\",os_type=\\\"linux\\\",protocol=\\\"tcp\\\",service_name=\\\"trafd-group\\\",state=\\\"FIN_WAIT1\\\"}\" err=\"too old sample\"\r\nprometheus_1  | ts=2022-11-18T17:34:49.418Z caller=scrape.go:1358 level=debug component=\"scrape manager\" scrape_pool=otel-gw-consumer target=http:\/\/otel-gw:1234\/metrics msg=\"Append failed\" err=\"too old sample\"\r\n```\r\n\r\nLogs after gap:\r\n```\r\nprometheus_1  | ts=2022-11-18T19:00:41.752Z caller=compact.go:519 level=info component=tsdb msg=\"write block\" mint=1668787215789 maxt=1668794400000 ulid=01GJ61TKM9J135G2PFWZVFX4NH duration=15.701434ms\r\nprometheus_1  | ts=2022-11-18T19:00:41.753Z caller=head.go:1212 level=info component=tsdb msg=\"Head GC completed\" caller=truncateMemory duration=480.317\u00b5s\r\nprometheus_1  | ts=2022-11-18T19:00:41.753Z caller=db.go:1203 level=info component=tsdb msg=\"compact ooo head resulted in no blocks\" duration=469ns\r\n```\r\n```\r\n","comments":["cc @codesome ","@codesome any idea of what is going on here?","I plan to check this next week. Apologies for the delay. On the top of my head, I don't know what might be the issue. @rayrapetyan do the gaps go away if you do not set `out_of_order_time_window`?","Also, @rayrapetyan do you see the gaps for all the metrics or only a subset of metrics?","@codesome, sorry for a long delay. Yes, it happens with all metrics, and seems not setting \"out_of_order_time_window\" resolves the issue!"],"labels":["kind\/bug","priority\/P1","component\/tsdb"]},{"title":"Histograms: Document final spec","body":"### Proposal\n\nTo declare native histograms stable, we also need to document the final state of things, the detailed spec etc. so that 3rd parties can interact with native histograms with confidence.\r\n\r\nThis overlaps with documenting the exposition formats, but there's more to it. It could still all happen in one document, though, if that's the most helpful way.","comments":["Hi, I want to work on this issue.","Thanks for the offer @Himanshu-Vishwas . \r\n\r\nCurrently, we have quite a few moving parts (see all the other open issues in the [milestone](https:\/\/github.com\/prometheus\/prometheus\/milestone\/10)). I would actually wait a bit until the dust has settled before we can really start with a (more or less) final spec. (Some parts might still change, and others, like the text format, don't even exist yet.)","We really need a spec, or at least a candidate thereof, so people can look at it for advice, rather than searching all over the documentation and the code.\r\n\r\nI'm probably the only one who can do this right now, so I'm working on it\u2026"],"labels":["priority\/P2","component\/documentation"]},{"title":"Panic during WAL replay after downgrading from 0.39.1 to 0.38.0","body":"### What did you do?\n\nWe downgraded a tsdb from version 0.39.1 to 0.38.0. It's important to note that the memory-snapshot-on-shutdown feature is enabled.\r\n\r\n_For some additional context: we import tsdb as a library similar to projects like Thanos, Cortex, and Mimir._\n\n### What did you expect to see?\n\nTSDB should restart successfully after downgrading.\n\n### What did you see instead? Under which circumstances?\n\nWe encounter this panic during WAL replay:\r\n```\r\npanic: runtime error: integer divide by zero\r\n\r\ngoroutine 299 [running]:\r\ngithub.com\/prometheus\/prometheus\/tsdb.rangeForTimestamp(...)\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/db.go:1643\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*memSeries).cutNewHeadChunk(0xc181294a20, 0x18444cb5382, 0xc0541d0710?)\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/head_append.go:644 +0x1b2\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*memSeries).append(0xc181294a20, 0x18444cb5382, 0x3ff0000000000000, 0x0, 0xc1953c3bc0?)\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/head_append.go:601 +0x1c5\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*walSubsetProcessor).processWALSamples(0xc00111e9a8, 0xc00014ae00)\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/head_wal.go:449 +0x357\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*Head).loadWAL.func7(0xc1a4bd7fa0?)\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/head_wal.go:110 +0x2f\r\ncreated by github.com\/prometheus\/prometheus\/tsdb.(*Head).loadWAL\r\n\t\/go\/pkg\/mod\/github.com\/prometheus\/prometheus@v0.38.0\/tsdb\/head_wal.go:109 +0x405\r\n```\r\n\r\nI did some digging and I believe the cause for this is due to [this change](https:\/\/github.com\/prometheus\/prometheus\/commit\/d0607435a29b542a03d5c83f89cf17f0447ecacf#diff-e55e448a1ce649167194e1ced19ab2009f422ad678011e333414b8de8e62d311R790), since versions prior to `0.39.0` are assuming that `chunkRange` can never be `0`, which makes sense.\r\n\r\nI'm not sure if this is worth fixing or not considering the memory snapshot stuff is still considered experimental. If there is actually a use-case where a downgrade is needed all you have to do is disable the feature or delete the chunk snapshot directory.\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Since memory snapshot is experimental, and we promise that you can upgrade prometheus without breakage, not downgrade, we should not fix this.\r\n\r\nBut, we would need to add a note to the 2.39 release that users using that features will not be able to downgrade."],"labels":["component\/tsdb"]},{"title":"Reduce default overhead of out-of-order feature when not enabled","body":"Currently we are allocating space for these 3 variables for all series even if out-of-order ingestion is not enabled.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/04dd6be1a0f14b77c78199f4ab502d70207fdee8\/tsdb\/head.go#L1798-L1800\r\n\r\nWe can improve this situation by putting it inside a struct and replacing these 3 variables by a pointer. (We can re-use the new struct for [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/04dd6be1a0f14b77c78199f4ab502d70207fdee8\/tsdb\/head.go#L1794-L1796) as well)","comments":[],"labels":["kind\/enhancement","component\/tsdb"]},{"title":"Stop removing __name__ from results: prevent `vector cannot contain metrics with the same labelset`","body":"This issue was previously named as:\r\n> Identify highest DPM series via PromQL\r\n\r\n## Problem \/ Use-Case\r\n\r\nAs an Admin\/Manager\/Operator of a Prometheus instance one use-case is to understand, then manage-down the usage on the instance and the cost of running it. Sometimes there are a few metrics with very high DPM (ie. high frequency of scraping \/ small scrape-interval), which increase \"usage\" on the instance, and increase costs. For a multi-tenant project which implements PromQL (such as Mimir) this appears as a DPM overage and increase in the dollar costs for that tenant.\r\nThe Admin then needs to identify **which** metrics are responsible for the high DPM; which are the worst offenders. I've tried to find these with the following query:\r\n\r\n`topk(10, count_over_time({__name__!=\"\"}[1m]))`\r\n\r\nUnfortunately this fails with:\r\n\r\n`execution: vector cannot contain metrics with the same labelset`\r\n\r\nBoiling this example down a bit, this error is present with the query `count_over_time({__name__!=\"\"}[1m])`. And it's present for my small instance which only has 4,200 metrics. Google suggests to me a typical workaround for this error is to [use label_replace to move the original metric-name into a temporary label](https:\/\/stackoverflow.com\/questions\/68944000\/how-to-avoid-vector-cannot-contain-metrics-with-the-same-labelset-error-when-p). So incorporating this fix I get to this query:\r\n\r\n`topk(10, count_over_time(label_replace({__name__!=\"\"},\"name_label\",\"$1\",\"__name__\", \"(.+)\")[1m:]))`\r\n\r\nBut I've now introduced another problem. (Can you spot it?) I had to switch from `[1m]` to `[1m:]` ie. convert to a subquery, to avoid `parse error: ranges only allowed for vector selectors`. But by switching to a subquery, `count_over_time` no longer counting the DPM of the underlying metrics\/series, but instead counts the subquery-evaluations during the interval in question!\r\n\r\n**So my question is this:**\r\nHow can I identify the series that have the highest DPM **via PromQL**, so I can effectively manage cost and usage on my instance? (Or if it weren't via PromQL, how would we change Prometheus to better support this use-case, and how would operators of multi-tenant Prometheus services such as Mimir support their tenants to achieve the same).\r\n\r\nCan we make changes to Prometheus to better support this use-case?\r\n\r\n**Note:** I'm aware that the query I'm trying to run can be very computationally expensive, and may fail if the result-set is too big. But I'm not aware of alternatives for the case I'm describing.\r\n\r\n## Proposal \/ Ideas\r\nI believe `vector cannot contain metrics with the same labelset` happens because Prometheus removes the metric-name when performing `count_over_time`. It does this for any function which changes the 'dimensions' of the metric. This is an opinionated decision taken by the project in the past - I think the opinion is that 'no metric name is better than the wrong metric name', and metrics names typically do include dimensions (such as `\u2026_seconds`). But personally (as an instance admin, and as someone who builds features for the instance-admin persona) I've found myself hitting this error message a few times, and found it a barrier to learning and using Prometheus.\r\n\r\nSome ideas then, to start some discussion:\r\n- Somehow always stop users from hitting the error `vector cannot contain metrics with the same labelset`, while also retaining the rule that \"we mustn't show the wrong metric name\".\r\n   - Retain the metric name, but add a suffix \"\u2026DIMS_CHANGED\" (either under `__name__` or possibly a different label-key, such as `original_name`)\r\n\r\n- Somehow always stop users from the hitting the error `parse error: ranges only allowed for vector selectors` when they have been forced to use `label_replace` (by the metric-name being removed). ie. Have Prometheus consider `label_replace` not as a true part of the query which needs to be evaluated, but as a simple rename which can happen somehow differently at the start\/end of the querying process...? I lack the words to describe this as I don't know the codebase, but I discussed this with @beorn7 a few months ago, and he thought it was a possibility (although maybe not until Prom v3).\r\n\r\n- Could we implement the [keep_metric_names](https:\/\/github.com\/VictoriaMetrics\/VictoriaMetrics\/wiki\/MetricsQL#keep_metric_names) option that VictoriaMetrics has implemented? \r\n\r\n- \u201cextend the cardinality API to cover time ranges\u201d (Bryan Boreham\u2019s suggestion - I think for a Mimir implementation)","comments":["Thanks, @samjewell. As you might have guessed from our personal communication, this touches two areas I have been thinking a bit about in the past, but never refined my thoughts enough to propose them in a design doc or to the dev summit. Let's take your issue as an opportunity to lay them out in their current rough form:\r\n\r\nThe first is essentially what you have proposed as your first idea above: Don't drop the name but prefix or suffix it somehow. My thought was to use a prefix that expresses the operation, and also prefix the prefix with a double underscore to mark the internal nature. In your example, the prefix would come out as something like `__count_over_time:the_original_metricname`.\r\n\r\nPlaying devil's advocate, here are concerns (with counter arguments in parentheses):\r\n* It's kind of verbose and clunky. And those artificial names are visible in the query results. Dashboards that use the bare labelset in the legend will now see the clunky names. (Yeah, that would render the whole idea a breaking change. A feature flag could mitigate that, though.)\r\n* Name collisions will still happen if the query is used in a recording rule because the name then changes to the name of the rule for all metrics in the result vector. (That's true, but then just don't use it in a recording rule. Further thought: Maybe  even create a placeholder syntax to fill in the original metric name when naming the output of recording rules.)\r\n* How to deal with binary operators? (Vector to vector matching requires unique labelsets anyway. Vector to scalar could be mapped into the proposed system, e.g. `__2_times:prometheus_build_info`.)\r\n\r\n\r\nThe second thought is about using the timestamp of the evaluation even when the operation is essentially \"in place\", only affecting a single sample, related to your second idea. It's kind of funny how `timestamp(prometheus_build_info)` gives you actual timestamps from the TSDB, but `timestamp(+prometheus_build_info)` gives you the evaluation timestamp. Maybe we would only really need to pick the evaluation timestamp when we are actually aggregating samples with different timestamps. This would be a huge change to the evaluation model and certainly only something to consider for Prometheus v3. I would also not really consider it a good solution to your problem here, but this has come up quite often in different contexts, e.g. `timestamp(last_over_time(prometheus_build_info[10m]))` or generally to enable range selectors on this whole class of \"non-aggregating operations\", avoiding sub queries there.\r\n\r\n\r\nAbout the ViMe option for that: Adding a separate option and thus mental overhead for everyone reading the docs for such a niche use case would make me really sad. If that first idea of yours (and mine) flew, it would be almost invisible for the naive user. Things would \"just work\".","Inspired by this issue, I had added an agenda item for the dev-summit many months ago. However, it wasn't discussed so far as other agenda items were deemed more pressing.\r\n\r\nThis makes it hard to go forward with any of the more invasive approaches. However, as already discussed in the dev-summit agenda item (search for \"Revisit metric name removal during PromQL evaluation\" in the [dev summit document](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1), sorry, no deep linking possible), there are less invasive approaches, and I think we can go forward with one of them in the meantime, to at least have some way of addressing the problem at hand.\r\n\r\nThe idea I propose here is to only drop the metric name as the very last step, before the result is returned to the outside world. This would not change the outcome of any working query, but it would allow to apply the `label_replace` trick later. @samjewell's problem could be solved by writing:\r\n```PromQL\r\ntopk(10, label_replace(count_over_time({__name__!=\"\"}[1m]), \"name_label\", \"$1\", \"__name__\", \"(.+)\"))\r\n```\r\n\r\nAnd in cases, where the ambiguity gets aggregated away anyway, formerly broken queries would just start to work. E.g. to answer the question how many metrics have had more than 10 samples in the last minute:\r\n```PromQL\r\ncount(count_over_time({__name__!=\"\"}[1m]) > 10)\r\n```\r\n\r\nThe only trick is how to track that a metric name has to be removed before returning the result. But that's an implementation detail we should be able to solve.","> The Admin then needs to identify which metrics are responsible for the high DPM; which are the worst offenders. I've tried to find these with the following query:\r\n> `topk(10, count_over_time({__name__!=\"\"}[1m]))`\r\n\r\nI've since found a better way to identify the sources of high resolution metrics (high _[datapoints per minute per series](https:\/\/grafana.com\/docs\/grafana-cloud\/billing-and-usage\/active-series-and-dpm\/#data-points-per-minute)_):\r\n\r\nQuery with `sort_desc(count_over_time(scrape_samples_scraped[1m]))` to get the _data points per minute per series_ by target. This works a treat in most cases.\r\n\r\nThere's an exception for metrics which weren't created from a scrape at all, for example those created by recording rules. For those we'd still benefit from the improvement requested here.\r\n\r\n","I would like to note that the concrete use case that served as an example here is by far not the only one where users run into the dreaded `vector cannot contain metrics with the same labelset` error. I would still like to see a solution around the \"drop names as the last step\" idea. (And in Prom 3.x or something, I would like to see a solution around (not) changing the timetamps to the evaluation time.)","> The idea I propose here is to only drop the metric name as the very last step, before the result is returned to the outside world.\r\n\r\nThis doesn't resolve the case when metric names should be returned in the response. For example, the following query could be used for returning resident and virtual memory usage:\r\n\r\n```promql\r\nmax_over_time({__name__=~\"process_(resident|virtual)_memory_bytes\"}[5m])\r\n```\r\n\r\nUnfortunately, it doesn't work in Prometheus now and will not work after the proposed idea is implemented. The [keep_metric_names](https:\/\/docs.victoriametrics.com\/MetricsQL.html#keep_metric_names) modifier from VictoriaMetrics elegantly resolves this issue. Note that the `keep_metric_names` modifier doesn't complicate queries by default. Users **may** add it if they **really** want keeping metric names in the results of some PromQL or [MetricsQL](https:\/\/docs.victoriametrics.com\/MetricsQL.html) function.","> Unfortunately, it doesn't work in Prometheus now and will not work after the proposed idea is implemented.\r\n\r\nIf the metric name is dropped as the last thing, you can still use `label_replace` to move the `__name__` label into a normal label.\r\n\r\n> The [keep_metric_names](https:\/\/docs.victoriametrics.com\/MetricsQL.html#keep_metric_names) modifier from VictoriaMetrics elegantly resolves this issue.\r\n\r\nI don't think it's a good idea. From the dev-summit notes: \"Firstly, it exposes everybody to yet another knob they have to think about, while being only relevant for very few (the use case is a niche in a niche). Secondly, it ignores the problem that motivated the name removal in the first place (you are now creating misleading metric names).\"","We [discussed this topic during the last dev-summit](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#bookmark=kix.1yn3rd8rsq8u). I try to summarize the results here:\r\n\r\n### About allowing `label_replace` to work on range vectors\r\n\r\nThis would solve most use cases in a seemingly easy way, but at a closer look, it comes with a lot of implications:\r\n1. As already discussed, it would require overloaded parameter types (`label_replace` would have to work on both instant vectors and range vectors).\r\n2. Worse: It would also require variable return types (`label_replace` acting on an instant vector returns an instant vector, a future `label_replace` acting on a range vector would need to return a range vector).\r\n3. And finally, while currently an instant vector returned by a function has the evaluation time as its timestamp, the returned range vector had to retain all its timestamps.\r\n\r\nEach of the above is something that we can very well discuss, and it might become a new feature in Prometheus 3.x, maybe being beneficial in many different ways, not just for this one feature request. However, to just solve the feature request at hand, it feels like a PromQL change way too fundamental and invasive.\r\n\r\nA more moderate variant would be to create a new function `label_replace_range_vector`. A function returning a range vector with unmodified timestamps would still be a new concept, but it would be much less invasive change to PromQL. _However_, this would add a new function for a use case that is in a niche of a niche, but it would add mental overhead for everyone (adds to the documentation, and is leaving many with the question \"What is this function even for?\"). Same concern as for the ViMe approach above.\r\n\r\n### About removing the `__name__` label as the last step\r\n\r\nUnfortunately, the discussion exposed that this would _not_ be a transparent change. Certain \"weird\" queries would behave differently, so it would be technically a breaking change. Take the following query as an example:\r\n\r\n```PromQL\r\nsum by (__name__) (rate({foo=\"bar\"}[5m]))\r\n```\r\n\r\nIt is currently equivalent to `sum(rate({foo=\"bar\"}[5m]))`, while it would (ironically) create the `vector cannot contain metrics with the same labelset` error message _with_ the proposed change. (The `sum` would be partitioned by the not-yet-removed `__name__` label, which would then be removed, creating the labelset collision.) One might argue the query above is nonsensical, and while a human would probably never type such a query, queries are often created via some templating so that the no-op addition of `by (__name__)` can actually happen in practice, and we really cannot break those queries. (Especially autogenerated ones will wreak havoc in subtle ways.)\r\n\r\nMore generally, every query dealing with `__name__` explicitly has the potential of behaving differently.\r\n\r\nIn that same line of thought, it should be noted that a `__name__` label marked for removal has to be un-marked if it is explicitly overridden later via `label_replace`. This also allows to create a new meaningful name, e.g.\r\n\r\n```PromQL\r\nlabel_replace(rate({foo=\"bar\"}[5m]), \"__name__\", \"rate5m_of_$1\", \"__name__\", \"(.*)\")\r\n```\r\n\r\nOverall, I don't think the caveats found kill the approach, but it has to stay behind a feature flag until the next major release after all, which reduces its elegance quite a bit. (We would have initially put it behind a feature flag anyway just to be cautious, but we could have removed the flag eventually if the change had been non-breaking.)\r\n\r\nPending any other new ideas or further evidence of implications, I would propose to give the \"late `__name__` removal\" approach a try, even though it's now a bit more complicated than it appeared initially.","This may be getting out of hand in complexity, but could we detect the\r\n\r\n* aggregate `by (__name__)`\r\n  * function that used to remove `__name__`\r\n\r\nstacking in the AST and rewrite the query to drop the `__name__` aggregation? Would that always be compatible? Then we could drop _this_ compatibility shim in 3.0 without having to gate all the good effects of this change.","+1 to this issue. I ran into a similar problem where I was trying to count samples ingested broken down by a label key of interest. Specifically, I have a label `aws_account` which gets applied to all my time series and helps me determine which time series are being scraped from which `aws_account`. I then want to count the rate of samples being sent from each account so I can detect if any single account suddenly starts massively increasing the sample rate it sends (e.g., maybe someone misconfigured something). \r\n\r\nI was thinking I could do something like `sum by(aws_account) (count_over_time( {__name__ =~ \".+\" } ) )` but then I ran into the same `execution: vector cannot contain metrics with the same labelset` issue as above. ","With v3.0 projected for 2024, the above annoyance about requiring a feature flag before 3.0 has become less bad, because 3.0 is going to happen quite soon rather than \"at some point in the distant future\". So it would indeed be good to tackle this long enough before 3.0 to get an idea if it works as expected.","I added this to the Prometheus 3 project, as the mild breakage caused by the proposed solution might be nicely dealt with by the major version bump.","@suntala is currently working on this.","@beorn7 pinging you to get assigned to the issue."],"labels":["kind\/enhancement","priority\/Pmaybe","component\/promql","not-as-easy-as-it-looks"]},{"title":"Create a metric that counts total labels per scrape ","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\nIf someone is running Observability as a Service and doesn't have access or knowledge of the labels per team then they can run into an issue where they are blind to the number of labels compared to the \"label_limit\" set in the scrape config. If there was a metric that counted the number of labels per scrape and that was exposed at every scrape then the central team could understand at a high-level where each user\/team sits compared to their rate limits. This would ultimately help the observability team control cardinality and costs per team.\r\n","comments":["There is the `prometheus_target_scrape_pool_exceeded_label_limits_total` metric, which counts how many times the limit has been exceeded. ","Added a PR which implements the metric in question. We'll see if this gets accepted...","Each time series can have a different set of labels, so all you can do really is export a single metric showing what's the maximum number of labels exposed.\r\nI don't think that #11394 works as you expect, if I read it correctly it will set that metric to the number of labels on the last metric in each scrape.\r\n\r\nThe problem here, from my experience, is that trying this sort of accounting tends to be very expensive with Prometheus.\r\nPlus one could make an argument that no team should be blind to the number of labels they have on metrics - it's produced by code they wrote or manage. Label values might be dynamic but the list of labels and their meaning should be fairly static. I set all limits to high values that no reasonable scrape job would ever exceed since they are mostly aimed to be the last line of defence - to stop Prometheus from ingesting metrics from buggy code that generated 100s of labels of put MBs of error blobs into label values. Any fine tuning and trying to keep those limits close to usage is often very difficult in all the environments I work with."],"labels":["kind\/enhancement","priority\/Pmaybe","not-as-easy-as-it-looks","component\/scraping"]},{"title":"Gate needs a waiting duration metric","body":"## Proposal\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/main\/util\/gate\/gate.go this is used in the remote read handler to limit number of concurrent requests. However, if we are close to that limit then currently it is impossible to know that. Ideally, we'd know if requests are being held up at the gate. I suggest adding a \"waiting duration\" histogram metric there so that users could understand whether they are hitting that limit.\r\n","comments":["Hi does this issue still make sense? I'd like to work on it if possible","Is this issue still persisting as it seems to still be open","Hi! There are no open PRs for this Issue, so I would like to take on this issue. I've pushed the PR."],"labels":["help wanted","low hanging fruit","kind\/enhancement","component\/remote storage"]},{"title":"Add service discovery metadata to a single metric","body":"Use case :\r\n\r\nService discovery like Consul can expose a whole lot of useful information using tags and metadata (host flavor, host os, runtime version, ...) however adding dozens of meta label to every metrics is unreadable and a waste of resources. Adding them to a single metric like the target_info proposed in the OpenTelemetry standard is sufficient as PromQL is well able to join labels from a metadata series on the instance label on demand.\r\n\r\nHowever Service Discovery data is only available during _relabel_config_ so it's not possible to filter out on which metric to add labels as __name__ is only available during the _metrics_relabel_config_ phase.\r\n\r\nThe only trick we found is to add the Service Discovery labels to all metrics with a _tmpsd_ prefix during the relabel_config phase\r\n\r\n```\r\n{% macro relabel_metadata_metric(target, source, regex=\"(.*)\", replacement=\"$1\") -%}\r\n- source_labels: ['{{source}}']\r\n  regex: '{{regex}}'\r\n  target_label: '_tmpsd_{{target}}'\r\n  replacement: '{{replacement}}'\r\n{% endmacro %}\r\n\r\n# SERVER INFO\r\n{{relabel_metadata_metric(\"device_serial\",\"__meta_consul_metadata_serial_number\")}}\r\n{{relabel_metadata_metric(\"device_flavor\",\"__meta_consul_metadata_criteo_flavor\")}}\r\n{{relabel_metadata_metric(\"device_rack\",\"__meta_consul_metadata_rack_name\")}}\r\n{{relabel_metadata_metric(\"device_os\",\"__meta_consul_metadata_os_platform_version\")}}\r\n{{relabel_metadata_metric(\"device_os_family\",\"__meta_consul_metadata_os_platform_family\")}}\r\n{{relabel_metadata_metric(\"device_os_version\",\"__meta_consul_metadata_os_platform_version\")}}\r\n{{relabel_metadata_metric(\"device_placement_information\",\"__meta_consul_metadata_placement_information\")}}\r\n```\r\n\r\nAnd then to relabel them to remove the prefix on the desired _target_info_ metric\r\n\r\n```\r\n{% macro relabel_metadata_metric(label) -%}\r\n- source_labels: ['name','_tmpsd_{{label}}']\r\n  regex: '^target_info;(.*)$'\r\n  target_label: '{{label}}'\r\n{% endmacro %}\r\n\r\n# SERVER INFO\r\n{{relabel_metadata_metric(\"device_serial\")}}\r\n{{relabel_metadata_metric(\"device_flavor\")}}\r\n{{relabel_metadata_metric(\"device_rack\")}}\r\n{{relabel_metadata_metric(\"device_os\")}}\r\n{{relabel_metadata_metric(\"device_os_family\")}}\r\n{{relabel_metadata_metric(\"device_os_version\")}}\r\n{{relabel_metadata_metric(\"device_placement_information\")}}\r\n```\r\n\r\nAnd finally to drop them on every other metrics\r\n\r\n```\r\n- action: 'labeldrop'\r\n  regex: '_tmpsd_.*'\r\n```\r\n\r\nHowever this resulted in a 10%-15% increase of the memory usage to add the same 17 labels from service discovery to this single metric per target (study over 1700+ Prometheus instance of different size ranging from 500MB to 80GB) which is a significant infra cost for a single metric per target. Also the _tmpsd_ prefixed labels are not removed on _up_ and _scrape_sample_scraped_ metrics because they are special metrics which are not affected by by _metrics_relabel_config_.\r\n\r\nWould it be possible to provide a way to add some labels only to the up or target_info metric during the _relabel_config_ phase or to have access all Service Discovery attributes during the _metrics_relabel_config_ phase ?\r\n\r\nWe would be happy to contribute to such feature if needed.","comments":["Thank you for the feature request. It would certainly be useful to find a way to add labels only to e.g. a new `target_info` metric during the `relabel_config phase`, or to have access to all Service Discovery attributes during the `metrics_relabel_config` phase. I can understand the increase in memory usage and infra cost when adding the same labels from service discovery to every metric per target.\r\n\r\nI'm not sure how we could implement this currently, but I would be happy to look into it further. If you have any specific ideas or a design document outlining your proposal, I would be happy to review and provide feedback. We welcome contributions and would be glad to work with you on finding a solution to this issue.\r\n\r\n*Note: generated with the help of chatgpt*"],"labels":["priority\/Pmaybe","not-as-easy-as-it-looks","kind\/feature"]},{"title":"Idea: add a mode where Prometheus will do head compaction\/WAL truncation before starting scraping","body":"## Proposal\r\nAs @mattburgess said at https:\/\/github.com\/prometheus\/prometheus\/issues\/6934#issuecomment-945739611\r\n>would it be possible to run a compaction\/WAL cleanup as soon as the replay has been completed? That would help us avoid the crash-loop we find ourselves in, whereby a pod restart causes WAL replay to happen, but then they never get cleaned up, so when the OOM-killer comes along the same WALs get replayed on the next restart and on and on we go in circles until we resign ourselves to removing the WALs and thereby losing data.\r\n\r\nI found myself wanting this today.\r\n\r\nI will add, I think it is important that we _not_ start scraping until after this head compaction has finished, to keep memory usage down and avoid making the WAL any bigger, because if Prometheus OOMs again it will restart with a worse problem.\r\n\r\nIt could be a CLI flag to Prometheus, like `--force-head-compaction-at-start`.","comments":["Related idea: #7575, #7939.","Sounds good to me, but what would be the effect if someone uses this routinely? Would it still create non overlapping aligned blocks or would that create new blocks every time?","During research for #12286 I realised that doing WAL checkpoint and truncation to the end of the last real block, before doing anything else, will be a benefit. \r\nPerhaps also scan the WAL to see which series have samples after that time, so that the checkpoint can be built with the minimum set of series."],"labels":["priority\/Pmaybe","component\/tsdb","kind\/feature"]},{"title":"ui (histograms): Prettify rendering of Native Histograms in Table view","body":"The text rendering of Native Histograms in the Table view is quite verbose, but it's currently the only way of displaying Native Histograms in the built-in UI.\r\n\r\nMaybe a small graphical representation of each Histogram would be much more consumable for humans, with some way of retrieving the exact text representation of all the numbers (maybe via a tool tip, but that might have accessibility problems).","comments":["Thanks for bringing this to my attention, I'm working on this right now!\r\n\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/538008\/215611917-24d4b689-dfde-4bc0-bb42-977584d8605c.mp4\r\n\r\n","different width bars are even more :exploding_head: when you have a dynamic\/adaptive x axis.\r\n\r\nin one table a 30px wide bar can be 0.1-0.2 and the same width bar, in the same hz position in the next table can be 500-1000 :). i'm still entirely unconvinced by the utility of different bar widths here :sweat_smile:. at minumum the x scale should be synchronized across all histograms to bring some level of sanity to this.","Hmm yeah, it's definitely a mind-altering experience \ud83e\udd14\ud83e\udd23\r\n\r\nSynching the X-axis up is an interesting idea, I could try that. Of course it would mean that if only one out of many histograms has one super high-ranged bucket, all other histogram displays will be squeezed together as well.","> Of course it would mean that if only one out of many histograms has one super high-ranged bucket, all other histogram displays will be squeezed together as well.\r\n\r\nhttps:\/\/getyarn.io\/yarn-clip\/b1c103b1-9629-48b4-aa03-4318ce00422d","Maybe I'm missing something, but shouldn't the x axis just be exponential as the bucket width is also exponential?","> Maybe I'm missing something, but shouldn't the x axis just be exponential as the bucket width is also exponential?\r\n\r\nthat's essentially what i'm suggesting. when the axis scale matches the bucket progression, you get uniform widths and spacing. when the axis scale is linear but the bucket progression is exponential, you get the varying widths as in the screen rec above.\r\n\r\nwe don't have individually hoverable bars in grafana's Histogram panel currently, but the UX would be pretty crappy if it was nearly impossible to hover\/click the 100 bar but 100k bar took up most of the screen space. the same is true for the Heatmap (2d histogram) -- exponential cells that follow a linear scale would be ungreat. they certainly exist. good luck hovering anything \"hot\" or interesting > 30.\r\n\r\n![e4e6abf6fb9b6c7e107f5f9ae9effa1543774b38_2_399x375](https:\/\/user-images.githubusercontent.com\/43234\/216050775-bf877f80-6735-426f-9420-d0f8419819af.png)\r\n\r\n(yes, this is a log scale with a linear bucket progression. just imagine it being inverted for linear scale and exponential progression, where the 1 is impossible to click)","Yeah, for interaction the log scale definitely is better. OTOH human brains are bad at grokking exponentials or logarithms, so I think having an option to also display buckets on a linear axis would be good too. Just viewing things linearly made me appreciate much more just how tiny the initial buckets are, and how large the last ones are \u2013 that's something that gets way harder to picture mentally with a log-scale axis. So I could implement both as well.","A switch would certainly make sense.\r\n\r\nBTW, for the funny PNG renderings in native histograms, I used an exponential y axis. But a heatmap is essentially a series of color-coded histograms rotated by 90\u00b0, so the exponential y axis in the heatmap corresponds to an exponential x axis in the table view.\r\n","@juliusv have you made any progress on this recently?\r\nThere might be a new contributor interested in this (@Maniktherana). If you don't plan to work on this any time soon, perhaps you could share your work so far?","hey @juliusv I'm looking to work on this\r\nHow did you achieve the histogram renders? was it using heatmaps or did you make a custom flot chart?","@Maniktherana Oh, I actually didn't use any library at all and drew everything with pure HTML, which ended up being only a few lines actually :) And yes, since I have been shamefully neglecting this and mostly working on the new UI now, feel free to take this over. I just pushed what I had back then to this branch: https:\/\/github.com\/prometheus\/prometheus\/tree\/native-histogram-charts (with this latest commit containing all the relevant changes: https:\/\/github.com\/prometheus\/prometheus\/commit\/662fa3871b538541ec7651c1cc61789831c75e80)\r\n\r\nI had some other work on top of that to start adding optionality for exponential \/ non-exponential display as requested here, but it was still so broken that I removed it for now and just pushed the latest working state before that.\r\n\r\nFeel free to either base your work on that branch or build something better :)","Just so I'm following, we're looking to essentially make the width of the bars equal? After piggybacking off of @juliusv's work I managed to figure something out but there's a weird double rendering of the tooltip that I need to figure out. That and how to generate the proper x labels. \r\n\r\nIf you're wondering about the negative ranges in the tables, I'm using the `client_golang` random example and I just filtered out the negative bars for now.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/assets\/14011425\/7e4ca64b-3ffe-4619-a4eb-1939c08e240c\r\n\r\n","@Maniktherana Yes that's right about the equal width, and that looks really good to me! Modulo the double tooltip and the \"BRUH\" at the top of course :)\r\n\r\nDo you think it's worth making the X axis configurable between exponential vs. linear (what I had previously)?\r\n\r\nAnother consideration is whether we will always want to display that much for each histogram sample. Makes the table quite long and the DOM will probably get expensive at some point as well. So at least at some point we may want to add an option to cut down on the level of detail, but I think we can add that later too.","> Modulo the double tooltip and the \"BRUH\" at the top of course :)\r\n\r\nFixed the double tooltip issue, it was caused by clashing ```id={`bucket-${index}-${bIdx}`}```.\r\nAnd yeah I'll remember to remove that \ud83d\ude05 \r\n\r\n> Do you think it's worth making the X axis configurable between exponential vs. linear (what I had previously)?\r\n\r\nAs long its meaningful to users I don't see an issue. Can make it a toggle like how it is between line chart\/stacked chart. Might have to remove x-axis ticks\/grid for every quarter in exponential mode tho as it doesn't seem to make much sense. It can be saved for the linear scale.\r\n\r\n> Another consideration is whether we will always want to display that much for each histogram sample. Makes the table quite long and the DOM will probably get expensive at some point as well. So at least at some point we may want to add an option to cut down on the level of detail, but I think we can add that later too.\r\n\r\nYes that's also a concern I share. I'm looking into strategies for this, ig we can simply not render bars with a count <1% as they can refer to the table for granular data. It's more apparent on linear scales as bars start having a width of 1px","@beorn7 ig we can close this under #13658","Thanks for working on this. It looks great.\r\nHowever, the current solution doesn't seem to render the zero bucket and negative buckets.\r\n\r\nHere you can see a histogram with a populated zero bucket and many negative buckets, they simply don't show up:\r\n\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/5609886\/93c2a776-f036-42d2-b10d-cf579155b8a5)\r\n","Also, it would be nice to improve the sorting of the negative buckets and the zero buckets. The negative buckets that are \"most negative\" should come first, and the zero bucket should be in between the negative and positive buckets.","> Thanks for working on this. It looks great. However, the current solution doesn't seem to render the zero bucket and negative buckets.\r\n> \r\n> Here you can see a histogram with a populated zero bucket and many negative buckets, they simply don't show up:\r\n> \r\n> ![image](https:\/\/private-user-images.githubusercontent.com\/5609886\/309007513-93c2a776-f036-42d2-b10d-cf579155b8a5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDkyMjYwNDksIm5iZiI6MTcwOTIyNTc0OSwicGF0aCI6Ii81NjA5ODg2LzMwOTAwNzUxMy05M2MyYTc3Ni1mMDM2LTQyZDItYjEwZC1jZjU3OTE1NWI4YTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDIyOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAyMjlUMTY1NTQ5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE4YzUxYTgxODBjNjhhMWQ1NWRlZjVlM2ZiNThiMzU2ZjhjMDYwYWY0ODdlYTkyZmEwNDFmMWVkOWIzODI0NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.v_obH0wegaJGfkDXEYecKnqRO-ya8q7FK_81SCQ7SYA)\r\n\r\nWhat should behavior look like for negative buckets? with current implementation, they overflow behind y axis (of course they do) but it would lead to a rather cramped graph in table view ","Yeah, the negative buckets have to be \"left of zero\". But the X axis can start at negative values.\r\nMore of a challenge is how to render the zero at all because you'll never reach it on a truly exponential scale. I guess you just start rendering at the first populated bucket closest to zero and render the zero bucket at the same width as all the other buckets, and then just label the x axis accordingly.\r\n","Maybe leave one standard bucket width empty if the zero bucket isn't directly touching the first populated negative or positive bucket. (In most practical cases, there will be very many empty buckets between the zero bucket and the first populated bucket. There is even the case where the zero bucket has a width of zero, i.e. it only takes observations of exactly zero. That's a valid use case and has to be rendered somehow.)","I'm trying to keep up here, but can you please explain the zero bucket?\r\nIs it just a bucket where the range has zero in it?","From the [design doc](https:\/\/docs.google.com\/document\/d\/1cLNv3aufPZb3fNfaJgdaRBZsInZKKIHo9E6HinJVbpM\/edit) (which is quite outdated by now, so take things there with a grain of salt): \"The zero-bucket-width defines a special bucket around zero. All observations with an absolute value below or equal the zero-bucket-width are counted in this special bucket. This avoids an explosion of the bucket counts for observations very close to zero and allows observations of the value zero in the first place. The precise value for the zero-bucket-width is arbitrary (and in particular not related to the resolution). In practice, it doesn\u2019t even matter that much. It should be large enough to not let noisy observations very close to zero create a lot of buckets, and small enough to not count observations into the zero bucket that are meant to be seen as non-zero.\"","I'll try to draw a few sketches once I find the time.","There is another aspect of histogram rendering: The population in a bucket should be represented by the _area_, not by the _height_ of the bar. This works just fine with the exponential rendering because all bars have the same width. But for  the linear rendering, we needed to adjust the height (or we decide to not render a \"real\" histogram).\r\n\r\nBut let's first get the exponential rendering right, that will be most common anyway.","> There is another aspect of histogram rendering: The population in a bucket should be represented by the _area_, not by the _height_ of the bar. This works just fine with the exponential rendering because all bars have the same width. But for the linear rendering, we needed to adjust the height (or we decide to not render a \"real\" histogram).\r\n> \r\n> But let's first get the exponential rendering right, that will be most common anyway.\r\n\r\nRough draft, but I came up with this for exponential scales:\r\nwidth: `100 \/ numBuckets + '%';`\r\nheight: `((count \/ range) \/ (countMax \/ rangeOfCountMax)) * 100 + '%'`\r\n\r\nfor linear scales, we use:\r\nwidth: `(range \/ (rangeMax - rangeMin)) * 100 + '%'`\r\nheight: same as above\r\n\r\nThe y-axis ticks are just quartiles of `[0 -> countMax \/ rangeOfCountMax]`\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/assets\/14011425\/16de754f-2e02-4b2e-a023-33838dddbb2b\r\n\r\n\r\nThis would, at the very least, look somewhat consistent and you could calculate to get the area equal to the count\r\n\r\nfor example in the tallest bar, \r\nthe range is `[9.5367431640625e-07 -> 0.0000019073486328125)`\r\nand count is `1`\r\n\r\nOn dividing count by range we get our height of `1,048,576` or ~1M\r\n\r\nThe renders are consistent between both scales as we're not really marking x-axis ticks \r\n\r\nEdit: the position of x-axis 0 is incorrect in this video but I've managed to fix that\r\n\r\n\r\n","@beorn7 Let me know if this is on the right path. I think the zero bucket bit may not have been addressed yet","It looks very promising. Could you put your current code state into a WIP PR? Then I can play with it. (But I won't get there before Tuesday.)"],"labels":["component\/ui","priority\/P3"]},{"title":"ui (histograms): Render Native Histograms in Graph view","body":"The text rendering of Native Histograms in the Table view is quite verbose, but it's currently the only way of displaying Native Histograms in the built-in UI.\r\n\r\nIt would be cool to also display them in the Graph view rather than only in the Table view. The most obvious way of representing them would be a heat map, but maybe there are others.\r\n\r\nA heat map can only show one histogram series at a time, so a query that yields multiple histogram series needed to produce a number of heat maps, which is distinctly different from what we do so far, namely redering many graphs in a single panel. Plus, there is the case of mixed histogram and conventional float series, for which we probably would need to render the conventional graph panel followed by _n_ heat maps.","comments":["Maybe we can render multiple heat maps in one panel after all. Options:\r\n- Rather than rendering one heat map with a \"rainbow\" style color schema, we could render it in one color from dark to light. Then we could overlay multiple heatmaps, each in a different color. With the right transparency, this might be actually useful (cf. astronomical pictures where very different parts of the spectrum, e.g. infrared, xray, are mapped to different colors). This would, in a way, correspond to how we render different time series with a different color each.\r\n- Rotate through the different heat maps over time.\r\n\r\nI can imagine that each mode of rendering (side-by-side, color-coded, rotate-over-time) has its pros and cons and it depends on the concrete case which one will fare best. So perhaps give the user an option to easily switch.","Is this covered by #13096 ?","#13096 is only for classic histograms.","#13680 is still being worked on to get the table view right for native histograms. Once that is done, it should be relatively straight-forward to extend the approach to the graph view AKA heat maps."],"labels":["help wanted","component\/ui","priority\/P3"]},{"title":"histograms: Implement final OM text format","body":"Once OM has decided how the text format representation of Native Histograms should look like (likely in OM 2.0, cf. https:\/\/github.com\/OpenObservability\/OpenMetrics\/issues\/237 for tracking), we need to implement it here in prometheus\/prometheus.\r\n\r\nThis is needed to declare Native Histograms a stable feature.\r\n\r\nSee also #11172, which will inform how to design the text format.","comments":["For reference: https:\/\/docs.google.com\/document\/d\/1w6GhLmDYKkkNLsyPWkC3TGhW3Rs1G73j7E_nG7bhlw0\/edit# enumerates ideas how an OM text format could look like","Are there any decisions made on the OM text format?\r\n\r\nI would interested in implementing the text format for the OpenTelemetry Prometheus exporter in `otel-js`.","No decisions made yet. The next step would be to create a prototype in any of the instrumentation libraries based on the brainstorming doc linked above. https:\/\/github.com\/prometheus\/client_python comes to mind, and @csmarchbanks seemed interesting in exploring the topic. Not sure how soon he'll have time (or if he is ready to receive PRs from others)."],"labels":["priority\/P2","component\/scraping"]},{"title":"histograms: Implement OpenMetrics protobuf parsing","body":"OpenMetrics always had an optional protobuf format, see https:\/\/github.com\/OpenObservability\/OpenMetrics\/blob\/main\/proto\/openmetrics_data_model.proto . Prometheus has no support for it yet. However, with Native Histograms, the protobuf option might become much more attractive. See https:\/\/github.com\/prometheus\/client_model\/issues\/60 for an effort to port the Histogram extensions to the old protobuf format to the OM protobuf format. If that works out, we need OM protobuf parsing in Prometheus. Maybe it will be as easy as slightly modifying the protobuf parsing code created as part of the histogram experiment (to parse the old protobuf format including the histogram extensions).\r\n\r\nThis is important to enable instrumentation that is not based on the old protobuf format (as still supported by client_golang, but hardly by anyone else). See also #11172 for getting text format support of histograms.","comments":["I think we can demote this to P2:\r\n\r\n- client_golang already works fine with the old protobuf format.\r\n- client_java has support for the old protobuf format in a feature branch.\r\n- I know of no other instrumentation library with native histogram support in progress.\r\n\r\nTherefore, supporting the proposed OM protobuf format doesn't unblock anything directly. We need to see what direction other instrumentation library maintainers would like to take (e.g. support old protobuf, use the [proposed OM text format experiment](https:\/\/github.com\/OpenObservability\/OpenMetrics\/issues\/247), use [newly proposed OM protobuf](https:\/\/github.com\/OpenObservability\/OpenMetrics\/pull\/256), something completely different\u2026). Once we know that, we can take appropriate steps on the Prometheus server side. But this should not block merging native histograms into the main branch."],"labels":["help wanted","priority\/P2","component\/scraping"]},{"title":"Support out of order ingestion for native histograms","body":"This feature will be released soon for normal float samples, and it would be really weird if we did not support it for histogram samples.\r\n\r\nUpdate 14th June 2023: @krajorama and @jesusvazquez are starting to work on this issue.","comments":["Overall task list from discussion:\r\nScope: everything is in the tsdb package: db.go, head_append.go, ooo_*.\r\n\r\nWrite path: start from headAppender.Append function\r\nNeed ooo_head.go OOOChunk.InsertHistogram, OOOChunk.ToHistogramChunk, modify writing ooo head to disk to account for different encodings.\r\nNeed head.go insertHistogram - keep it simple for now, no handling of counter reset, just use the 32 sample OOOchunks.\r\n\r\n@jesusvazquez notes:\r\nooo_head.go\r\n - Creeate new InsertHistogram() method\r\n - Create new ToHistogramFloatHistogram() ToHistogram()\r\n\r\nhead_append.go\r\n - Modify AppendHistogram() with ooo logic\r\n - Modify appendableHistogram() with ooo logic\r\n - Create insertHistogram()\r\n - Modify mmapCurrentOOOHeadChunk() with new chunk encoding logic. The method currently works with ooo chunks as if they always were XOR chunks.\r\n","Also assigning to @zenador and @krajorama to mark all the collaborators on this issue.","Testing to see if this grants permission to assign","From IRL discussion:  we've decided that we do **not need** to support the of mixing different sample types (float\/integer histogram\/float histogram) in the OOO headchunk. This should make testing a bit simpler.","From IRL discussion:\r\n@carrieedwards @fionaliao @jesusvazquez (cc @zenador )\r\n\r\nImmediate goals in priority order for the code:\r\n* add native histograms to existing unit test happy paths\r\n* add tests around counter reset\/multiple chunk handling\r\n* tests for float <-> native histogram series transitions\r\n","Counter reset between chunks test case discussion:\r\n\r\nIt is theoretically possible to write such sequence of histogram samples:\r\n- append and commit a recent histogram sample to initialize the normal in order head\/WAL\r\n- append and commit `opts.OutOfOrderCapMax` number of out of order histogram samples without counter reset\r\n- append and commit 2 x `opts.OutOfOrderCapMax` more histograms that are out of order, don't have counter reset between them and later then the previous out of order samples AND the first sample in this batch has lower (bucket) counters than the last sample in the previous batch\r\n\r\nWhen we query the samples back, something should detect that now the samples have a counter reset - I'm not sure if this should happen on chunk \/ chunk iterator level or simply somewhere in the PromQL engine (cc @beorn7 )","The PromQL engine simply looks at the `CounterResetHint` as returned from the query to TSDB. If you want the PromQL engine to do the (expensive) traditional counter reset detection, just set the `CounterResetHint` in the returned histogram to `UnknownCounterReset`.","Generally, it would be cool if we never end up with (counter) histogram chunks that have counter resets in the middle. (We have ruled that out so far, assuming that a histogram will change its bucket layout a lot after a reset so that handling that all in the same chunk is more expensive than cutting a new chunk. I'm not completely sure if it is even technically possible to encode a chunk with a counter reset in the middle. We definitely have created a lot of code around the invariant that there is never a counter reset within the same chunk. So keeping it that way would definitely be much much preferred.)\r\n\r\nBut if all of the above is just about returning histogram samples from a state where things haven't been consolidated into one chunk, and they come from overlapping chunks or buffers, you could indeed simply return a `CounterResetHint` as `UnknownCounterReset` whenever you want PromQL to detect the counter reset itself."],"labels":["priority\/P2","component\/tsdb"]},{"title":"promql (histograms): Final decision about histogram related extensions of the language","body":"Before declaring native histograms a stable feature, we need to be reasonably confident that we won't need breaking changes in PromQL anymore.\r\n\r\nPoints to consider:\r\n* Are we happy with the semantics and the functions introduced?\r\n* What other functions do we still need to add and, perhaps more importantly, which existing functions and operators need to be extended to also support native histograms?\r\n* Should we add new concepts (like a dot accessor, e.g. `my_histogram.count`)? (This could maybe done later as a non-breaking change, but at least we need an informed decision. Also, this would replace functions like histogram_count and histougram_sum, which we might not want to introduce into the stable feature set.)\r\n* This should be accompanied by a proper design doc based on our experience so far and the [brainstorming doc](https:\/\/docs.google.com\/document\/d\/1ch6ru8GKg03N02jRjYriurt-CZqUVY09evPg6yKTA1s\/edit).","comments":[],"labels":["help wanted","priority\/P2","component\/promql"]},{"title":"promql (histograms): Refine interpolation behavior","body":"Native histograms use interpolation to estimate quantiles (with `histogram_quantile`) or to estimate the fraction of observations falling within an interval (with `histogram_fraction`). Currently, the interpolation is linear, which is almost certainly suboptimal for the exponential bucketing schema we are using. Instead, it should be some kind of exponential interpolation (have a look at [beta distribution](https:\/\/en.wikipedia.org\/wiki\/Beta_distribution) perhaps?). Or even no interpolation (arguably, the harmonic mean of a bucket minimizes the maximum possible relative error, while interpolation minimizes the expectation value of the error, but maximizes the theoretical maximum error). Or even make it configurable (even more relevant for custom bucket layouts, e.g. for linear buckets, linear interpolation would be a good fit).\r\n\r\nAlso, we have to come to terms if it is fine to assume the zero bucket is only filled with positive (negative) observations if all other observations are in positive (negative) buckets (respectively), which is the currently implemented behavior.\r\n\r\nFinally, with float counters, we make sure to never extrapolate \"below zero\", see [code](https:\/\/github.com\/prometheus\/prometheus\/blob\/c92673fb1480cd6e3404a004522cbb851be4115d\/promql\/functions.go#L92-L104). We could\/should implement the same for histograms. (Presumably by making sure that no bucket and no count is ever extrapolated below zero.)","comments":["Not to clutter this issue, but I've noticed two practical issues with quantiles that are loosely related to interpolation.[^1]\r\nWould appreciate your thoughts on these.\r\n\r\n## Should pick last, not first, matching bucket in run of equal counts\r\n\r\nConsider the following state in `bucketQuantile`[^2]:\r\n\r\n```\r\nbucket counts = [10, 20, 20, 20, 31], rank=18 (i.e. q=18\/31)\r\n```\r\n\r\nThe `sort.Search` will then select the bucket `b=1` (i.e. the first `20`), but it ought to have selected the last one (`b=3`), and use its bucket boundary for the interpolation. (Which would have run into additional problems due to a divide-by-zero since we'll end up with `count=0` near the end of the method). As is, we get a result that's too small, and drastically too small in the common case of exponential bucket spacing.\r\n\r\nThe most extreme example is that of equal bucket counts: `counts = [10, 10, 10, 10, ...]` and, say, the first bucket boundary is `0.0` and the last one `10.0 <= x <= +Inf`. Every measurement added to this histogram went into the `>= 10` bucket - and yet we're returning `0` as the `histogram_quantile(1.0, _)`. It should return `10`.\r\n\r\nI have a change for this coded up locally but unfortunately it modifies some of the existing test cases (since they have a repeating adjacent bucket count[^3]) so I wanted to check if there's something I'm missing before I finish it up.\r\n\r\n## Run of empty buckets before +Inf prevents proper interpolation\r\n\r\nThe second issue isn't exactly easy to fix, and has to do with omitting buckets with a zero count. I seem to frequently see in our (CockroachDB's) internal monitoring that high quantiles over rates of latency histograms return bogus low values. I extracted one such instance, which is abbreviated below:\r\n\r\n```\r\n# index_in_buckets_slice \/ upper_bucket_boundary \/ count\r\n[...]\r\nidx 94: <=22.020095ms: 142.240000\r\nidx 95: <=23.068671ms: 142.240000\r\nidx 96: <=24.117247ms: 142.320000\r\n[...]\r\nidx 144: <=192.937983ms: 142.320000\r\nidx 145: <=201.326591ms: 142.320000\r\nidx 146: <=+Inf: 142.320000\r\n```\r\n\r\nFrom what's described above, one problem is that we'll get a `p100` of `24.117247ms` instead of `201.326591ms`. But the true `p100` should be `10s`, since I happen to know that the histogram has its last bucket boundary there. However, it seems that very few measurements ever fall into the bucket(s) between `201.326591ms` and `10s` and instead fall into the `+Inf` bucket, and of course that bucket doesn't lend itself to interpolation. Since the empty buckets between idx 145 and the `+Inf` bucket are omitted, `histogram_quantiles` couldn't return anything more useful than `201.326591ms` even if we fixed the first problem (as is, it returns `24.117247ms`, despite a constant stream of 10s+ measurements flowing into this latency metric!)\r\n\r\nI doubt that this second problem is easy to fix - or rather, it should be fixed on our end by picking better bucket boundaries, which to our credit we have[^4] though I have yet to check that these buckets actually prevent the problem in practice.\r\n\r\n[^1]: https:\/\/github.com\/cockroachdb\/cockroach\/pull\/88127#issuecomment-1251087403\r\n[^2]: https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/promql\/quantile.go#L100-L101\r\n[^3]: https:\/\/github.com\/prometheus\/prometheus\/blob\/2bb457dcce500053e115a6fc05984aee7fdf321a\/promql\/testdata\/histograms.test#L14-L15\r\n[^4]: https:\/\/github.com\/cockroachdb\/cockroach\/pull\/85990","I think some confusion has happened here.\r\n\r\nThis issue is about the new Native Histograms. Currently, all related code exclusively lives in a [branch](https:\/\/github.com\/prometheus\/prometheus\/tree\/sparsehistogram). The code in [main](https:\/\/github.com\/prometheus\/prometheus\/tree\/main) is only dealing with the conventional Histograms.\r\n\r\nIf there is really an issue with the code for conventional Histograms, it should go into a separate issue.\r\n\r\nHaving said that, I don't really see the described issue. Conventional Histograms have _cumulative_ buckets. So the described buckets `[10, 20, 20, 20, 31]` would look like `[10, 10, 0, 0, 11]` as regular buckets (and `[10, 10, 10, 10, ...]` would look like `[10, 0, 0, 0, ...]`). So in the given example, the observation with rank=18 is definitely in bucket #1 (zero based counting), and the question is merely if we should estimate the quantile value as the arithmetic mean of the bucket boundaries, the harmonic mean of the bucket boundaries, or some linear or exponential interpolation.","Thanks @beorn7 for the prompt reply - definitely not the right thread to be discussing this here, I'll mull your reply over and open a new issue. I'm hiding my comment as \"off topic\" to avoid distracting future readers of this issue.\r\n\r\nedit: I understand my mistake now - thank you for clearing that up - here's the root cause, entirely on our end: https:\/\/github.com\/cockroachdb\/cockroach\/pull\/88331"],"labels":["help wanted","kind\/enhancement","priority\/P2","component\/promql"]},{"title":"codemirror-promql - \"outdated\" dependencies ","body":"### What did you do?\n\nInstalled codemirror-promql and codemirror packages in the latest version. \n\n### What did you expect to see?\n\nIt should work with the latest version. \n\n### What did you see instead? Under which circumstances?\n\nFirst of all, that is no real bug, but a current limitation: \r\n\r\nIf you have multiple versions of @codemirror\/state, codemirror will start to complain and stop working <- this is intended. The main issue is that codemirror-promql uses an \"outdated\" version of codemirror and is due to this no longer being compatible with the latest version. \r\n\r\nI don't know how dependency updates are handled currently within your organization, but it would be cool to automate those updates, or at least check on them frequently. \r\n\r\nThanks for your time, and sorry for missusing the Bug section for this. \n\n### System information\n\n_No response_\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["cc @Nexucis ","`codemirror-promql` or `lezer-promql` use `peerDependencies` to depend on `codemirror` or `lezer`. \r\n\r\nFor example: \r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/main\/web\/ui\/module\/codemirror-promql\/package.json#L48-L54\r\n\r\nHopefully because we said `codemirror-promql` is compatible with any version above `6.0.0` you can install the latest version of codemirror without waiting an upgrade of the `codemirror-promql` dependencies. \r\n\r\nAs you can see by using this tool https:\/\/semver.npmjs.com\/, if you enter `@codemirror\/autocomplete` and the range `^6.0.0`, `6.1.0` is available.","What npm version are you using ?\r\nWhat dependencies do you have in your `package.json` ? ","I have a project where I'm using `@prometheus-io\/codemirror-promql` and I am using it with the last version of `codemirror` with no issue.\r\n\r\nhere an extract of the `package.json`. \r\n\r\n```json\r\n{\r\n[...]\r\n    \"@codemirror\/autocomplete\": \"6.2.0\",\r\n    \"@codemirror\/commands\": \"6.1.0\",\r\n    \"@codemirror\/language\": \"6.2.1\",\r\n    \"@codemirror\/lint\": \"6.0.0\",\r\n    \"@codemirror\/search\": \"6.2.0\",\r\n    \"@codemirror\/state\": \"6.1.1\",\r\n    \"@codemirror\/view\": \"6.2.4\",\r\n    \"@lezer\/common\": \"1.0.1\",\r\n    \"@lezer\/highlight\": \"1.0.0\",\r\n    \"@prometheus-io\/codemirror-promql\": \"0.38.0\",\r\n}\r\n```"],"labels":["kind\/more-info-needed"]},{"title":"Metric explorer in Prometheus","body":"## Proposal\r\n\r\nWith large Prometheus and Prometheus based deployments, exploring metrics with PromQL can be difficult since entire series with their samples need to be fetched from the TSDB. We run a large Thanos deployment and autocomplete for labels and values coming from the `api\/v1\/labels` API would often time out due to the large amount of data that we store.\r\n\r\nIt would be great if Prometheus had an API that exposes all metrics, together with the labels and values, through some endpoint like `api\/v1\/metrics`. As a follow up, this information can also be exposed directly in the Prometheus UI.\r\n\r\nHere is an example of what the metrics explorer in DataDog looks like: \r\n<img width=\"550\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/1286231\/185309031-d6ebe89f-bf4d-447f-bc6f-8630c8063fb9.png\">\r\n\r\n","comments":[],"labels":["priority\/Pmaybe","component\/ui","component\/api","kind\/feature"]},{"title":"Prometheus GUI (the Targets view) is not usable.","body":"### What did you do?\n\n1. Open a Prometheus GUI web.\r\n2. Go to the `Status \/ Targets` view.\r\n3. Expand a group with more than 10 items (?, just a guess, don't know the real threshold for showing the group within its own view).\r\n4. Move the mouse to the `Label` block to see the `Before relabeling` hint (showing the labels before relabeling).\n\n### What did you expect to see?\n\nA `Before relabeling` hint showing the labels (for the selected row).\n\n### What did you see instead? Under which circumstances?\n\nNothing.\r\nBoth the selected row and the mouse are just blinking.\n\n### System information\n\nUbuntu 22.04.1 LTS, Linux 5.15.0-46-generic x86_64\n\n### Prometheus version\n\n```text\n2.37.0\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["`Firefox 103.0.2 (64-bit)`\r\n(a new, clean profile (no addons))\r\n\r\nSee the screencast for more info:\r\n[220815-120251.prometheus-gui.targets.webm](https:\/\/user-images.githubusercontent.com\/25924718\/184616731-09dec900-f112-4ab7-9db3-bf569495414a.webm)\r\n\r\nNote that it's also impossible to highlight any text with the mouse (to copy it).","Facing the same issue in Chrome. Was trying to copy the labels and it just wasn't working.\r\n\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/7354143\/185085936-2a4380b6-893b-45db-8d6e-db148c09dc3e.mov\r\n\r\n\r\n\r\n"," 2 issues here:\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/10894\r\n\r\nAnd the fact that labels can't be selected","I did a little bit backwards testing.  I can confirm:\r\n\r\n2.33.0: Does not work\r\n2.32.1: Works","`2.39.1` - not working\r\nAny chance to fix it?\r\n","`2.42.0` - a problem persists."],"labels":["help wanted","kind\/bug","component\/ui","priority\/P3"]},{"title":"Scrape duration longer than Head Compaction can cause Overlapping Blocks Error","body":"### What did you do?\n\nConfigured a Scrape target that takes several minutes to respond. The metrics include timestamps from the start of the scrape running.\r\n\r\n\n\n### What did you expect to see?\n\n_No response_\n\n### What did you see instead? Under which circumstances?\n\nIf a Head compaction runs and completes whilst the scrape is in progress - when finished the scrape appends data to HEAD that includes a mint that overlaps with the newly created block from Head Compaction. This results in a failure state in which head compaction no longer works failing with the error message ```\"compaction failed\", \"compact head: reloadBlocks blocks: invalid block sequence: block time ranges overlap: [....```\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\n2.36.1\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["cc @codesome ","@richardjennings does your scrape target specify timestamps for the metric? If not, several minutes of scrape should not create this issue. Is your scrape scraping samples of nearly 1 hour old?\r\n\r\nBut what you said is indeed possible. If Prometheus was trying to scrape something that is nearly 1 hour old, it could face this issue. At this point I wonder if we should enable handling of overlapping blocks by default, it is not experimental anymore.","@codesome yes, the scrape target specified timestamps and those timestamps could be quite old. We changed the way this particular scrape job worked and have not seen the issue since. \r\n\r\n> But what you said is indeed possible. If Prometheus was trying to scrape something that is nearly 1 hour old, it could face this issue. At this point I wonder if we should enable handling of overlapping blocks by default, it is not experimental anymore.\r\n\r\nThank you. "],"labels":["kind\/bug","not-as-easy-as-it-looks","priority\/P3","component\/tsdb"]},{"title":"Improve large number readability","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n\r\nOften I have queries like `foo > 1000000`.  This is hard to read and very easy to make a mistake between 100k, 1M, and 10M.  I would love to have some way formatting large numbers for increased clarity.\r\n\r\nOption 1 - digit grouping - eg `1_000_000` or `1.000.000`\r\nOption 2 - engineering SI notation  - eg `1M` or `900k` or `2Ti`\r\n\r\n\r\n","comments":["You can do `foo > 10^6` or `foo > 1000 * 1000`","Absolutely there are some work around but one you move off easy powers of 10.  Let's say I wanted to alert on 90% of 2TiB,  2TiB != 2TB and there is a huge difference between the two.  \r\n\r\nMy big concern is the readability and maintenance rather than strict compactness.  `0.90 * 2TiB` is clearer than `0.9 * 2 * 1_099_511_627_776` is clearer than `1.979 * 1000 * 1000 * 1000 * 1000` even though they're all equivalent.\r\n\r\nA built in function like `pow` would also be pretty good and wouldn't require changing the parsing engine.  So something like `0.9 * parse_number(\"2Ti\")`","Indeed this can be done with `0.9*2*2^40`. My concern with introducing a specific function for bytes would be where to draw the line: should we also parse centimeters, milliliters, tons, .. ? I think we can still push this to the user. Note that PromQL expression also support comments with `#` which could help reading queries."],"labels":["kind\/enhancement","priority\/Pmaybe","component\/promql"]},{"title":"breaking change: `promtool check rules` now exits with code 0 on failures","body":"### What did you do?\n\nUpdated promtool on our CI(Github Action) from v2.36.0 to v2.37.0\r\n\r\nGithub actions fail the pipeline depending on the exit code of the execution:\r\n* 0 = success\r\n* anything else = failure\r\n\r\n\n\n### What did you expect to see?\n\nFailed checks to keep exiting with something different from 0\n\n### What did you see instead? Under which circumstances?\n\nThe same CI now succeeds even with invalid rules, because promtool now exits with exit code 0\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n_No response_\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Can you please provide an example?","Ah yes of course, sorry for the incomplete issue.\r\n\r\nBy accident, we ended up with duplicated alerts (Same alertname, same labels, different expressions):\r\n```\r\ngroups:\r\n- name: node-exporter\r\n  rules:\r\n    - alert: NodeFilesystemAlmostOutOfSpace\r\n      annotations:\r\n        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.\r\n        runbook_url: https:\/\/github.com\/gitpod-io\/runbooks\/blob\/main\/runbooks\/NodeFilesystemAlmostOutOfSpace.md\r\n        summary: Filesystem has less than 5% space left.\r\n      expr: |\r\n        (\r\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"shiftfs\"} \/ node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"shiftfs\"} * 100 < 5\r\n        and\r\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"shiftfs\"} == 0\r\n        )\r\n      for: 15m\r\n      labels:\r\n        severity: critical\r\n    - alert: NodeFilesystemAlmostOutOfSpace\r\n      annotations:\r\n        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.\r\n        runbook_url: https:\/\/github.com\/gitpod-io\/runbooks\/blob\/main\/runbooks\/NodeFilesystemAlmostOutOfSpace.md\r\n        summary: Filesystem has less than 3% space left.\r\n      expr: |\r\n        (\r\n          node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"shiftfs\"} \/ node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"shiftfs\"} * 100 < 3\r\n        and\r\n          node_filesystem_readonly{job=\"node-exporter\",fstype!=\"shiftfs\"} == 0\r\n        )\r\n      for: 15m\r\n      labels:\r\n        severity: critical\r\n```\r\n\r\nWith promtool v2.36.0:\r\n<details><summary>Logs<\/summary>\r\n\r\n```console\r\ngitpod \/workspace\/observability (main) $ promtool --version\r\npromtool, version 2.36.0 (branch: HEAD, revision: d48f381d9a4e68c83283ce5233844807dfdc5ba5)\r\n  build user:       root@b3126bd1c115\r\n  build date:       20220530-13:56:56\r\n  go version:       go1.18.2\r\n  platform:         linux\/amd64\r\ngitpod \/workspace\/observability (main) $ promtool check rules promrules.yml \r\nChecking promrules.yml\r\n  FAILED:\r\nlint error 1 duplicate rule(s) found.\r\nMetric: NodeFilesystemAlmostOutOfSpace\r\nLabel(s):\r\n        severity: critical\r\nMight cause inconsistency while recording expressions\r\n\r\ngitpod \/workspace\/observability (main) $ echo $?\r\n3\r\n```\r\n\r\n<\/details>\r\n\r\nWith promtool v2.37.0:\r\n<details><summary>Logs<\/summary>\r\n\r\n```console\r\ngitpod \/workspace\/observability (main) $ promtool --version\r\npromtool, version 2.37.0 (branch: HEAD, revision: b41e0750abf5cc18d8233161560731de05199330)\r\n  build user:       root@0ebb6827e27f\r\n  build date:       20220714-15:13:18\r\n  go version:       go1.18.4\r\n  platform:         linux\/amd64\r\ngitpod \/workspace\/observability (main) $ promtool check rules promrules.yml \r\nChecking promrules.yml\r\n  FAILED:\r\nlint error 1 duplicate rule(s) found.\r\nMetric: NodeFilesystemAlmostOutOfSpace\r\nLabel(s):\r\n        severity: critical\r\nMight cause inconsistency while recording expressions\r\n\r\ngitpod \/workspace\/observability (main) $ echo $?\r\n0\r\n```\r\n\r\n<\/details>","By default, `promtool check rules` does not consider linting problems to be errors anymore. This can be changed by adding the `--lint-fatal` flag, which restores the previous behavior.","@ArthurSens Did you check @rgroothuijsen comment ? Does that fit your need ?\r\nSeems fine to me\r\n```\r\npromtool37 check rules --lint-fatal promrules.yml\r\necho $?\r\n1\r\n```"],"labels":["help wanted","kind\/bug","component\/promtool"]},{"title":"ExternalLabels should error or drop empty values","body":"### What did you do?\n\nRun `.\/prometheus --config.file .\/documentation\/examples\/prometheus.yml`\r\nwith this addition to the config:\r\n```\r\n   evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n   # scrape_timeout is set to the global default (10s).\r\n+  external_labels:\r\n+    blank:\r\n \r\n # Alertmanager configuration\r\n```\r\n\n\n### What did you expect to see?\n\nAn warning and\/or no `external_labels`.\r\n\r\nThis would be in line with the [docs](https:\/\/prometheus.io\/docs\/concepts\/data_model\/#metric-names-and-labels), which say:\r\n\r\n> A label with an empty label value is considered equivalent to a label that does not exist.\r\n\n\n### What did you see instead? Under which circumstances?\n\nPrometheus reports this at `\/config`:\r\n```\r\nglobal:\r\n  scrape_interval: 15s\r\n  scrape_timeout: 10s\r\n  evaluation_interval: 15s\r\n  external_labels:\r\n    blank: \"\"\r\n```\r\n\r\nWorse, there is even a test to ensure it works:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/97d7e09e0b639efd3f9d69197df26e6b608e671c\/config\/config_test.go#L1668\n\n### System information\n\nLinux 5.4.0-72-generic x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.37.0-rc.0 (branch: main, revision: 1ba2bbc5fe6af8a0af8f8f489505fef2770701ea)\r\n  build user:       vagrant@vagrant\r\n  build date:       20220714-16:23:07\r\n  go version:       go1.18.3\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n```yaml\n# my global config\r\nglobal:\r\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n  # scrape_timeout is set to the global default (10s).\r\n  external_labels:\r\n    blank:\r\n\r\n# Alertmanager configuration\r\nalerting:\r\n  alertmanagers:\r\n    - static_configs:\r\n        - targets:\r\n          # - alertmanager:9093\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n  # - \"first_rules.yml\"\r\n  # - \"second_rules.yml\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n  - job_name: \"prometheus\"\r\n\r\n    # metrics_path defaults to '\/metrics'\r\n    # scheme defaults to 'http'.\r\n\r\n    static_configs:\r\n      - targets: [\"localhost:9090\"]\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["This can be used as part of e.g. federation:\r\n\r\nhttp:\/\/127.0.0.1:9090\/federate?match[]=up\r\n\r\n```\r\n# TYPE up untyped\r\nup{instance=\"127.0.0.1:9100\",job=\"node\",cluster=\"\"} 0 1657816800550\r\n```\r\n\r\nIt is possible to expose empty labels and then use honor_label to suppress target labels.\r\n","Suggest you should document this usage, if it is intentional.\r\nThe docs on `honor_label` tend in the opposite direction:\r\n```\r\n# Note that any globally configured \"external_labels\" are unaffected by this\r\n# setting. In communication with external systems, they are always applied only\r\n# when a time series does not have a given label yet and are ignored otherwise.\r\n```","Also document whether remote-write is supposed to send empty external labels, because I'm pretty sure Cortex and Mimir are broken if you do that.","the documentation is fine, this is for the scraping side, not the emitting one","My first quote is from \u201cData Model\u201d; no mention of scraping or emitting. \r\n\r\nSeriously, I don\u2019t understand what you meant about federation and honor_label; I quoted what I could find in the documentation, so it doesn\u2019t seem fine to me. ","> A label with an empty label value is considered equivalent to a label that does not exist.\r\n\r\nThis is correct, but it does not mean that empty labels can't be emitted. IIRC the pushgateway is emitting empty job and instance labels, so that when used with `honor_labels: true`, the pushed metrics do not get the Pushgateway's job & label.\r\n\r\nIf Prometheus finds a metric exposed `foo{job=\"\",bar=\"foobar\"}` and `honor_label: true`, it will not add the local job's label when ingesting the metric. Therefore in TSDB the metric will be `foo{bar=\"foobar\"}`.\r\n\r\nTherefore, we need the ability to emit empty labels on Prometheus' \/federate endpoint. I have to admit that it is a pretty niche use case, but possibly some users rely on this behaviour.\r\n","OK I think what you are describing is:\r\n * The client side of `\/federate` is normal scraping.  \r\n * Scraping adds labels like `job` and `instance`. Federation docs tell you to add `honor_labels: true` to avoid overwriting these.\r\n * The Prometheus you are scraping from may have series with no `job` or `instance` labels, due to some other cause such as Pushgateway.\r\n * Users in this situation should set `external_labels: [job: \"\", instance: \"\"]` when configuring federation, so those series with no `job` and\/or `instance` get blank ones when scraped, and they are not added in scraping.\r\n\r\nI think my preferred way forward now would be:\r\n * Document under `external_labels` that values can be empty, and are supplied in federation.\r\n * Document under federation to add `external_labels: [job: \"\", instance: \"\"]`.\r\n * Drop empty `external_labels` in remote_write and document this.\r\n\r\nThe last one is because I don't think this is a valid use-case, and it simplifies things for the user, although it's inconsistent internally.\r\n\r\nI believe the one other place `external_labels` are used is when talking to Alertmanager, but I haven't thought about empty labels in that case."],"labels":["component\/config","component\/remote storage","priority\/P3","component\/documentation","kind\/change"]},{"title":"Series un-isolated when toggling local time","body":"### What did you do?\n\nI clicked on an individual series below the graph in the graph view, isolating that series, and then toggled the \"Use local time\" feature\n\n### What did you expect to see?\n\nThe series continue to be isolated on the graph like before:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54278938\/177073003-73dcfc0c-1bf4-4082-800b-66f29101295a.png)\n\n### What did you see instead? Under which circumstances?\n\nThe series was no longer isolated on the graph, although on the list below it showed it was still selected:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54278938\/177072953-98cc3bfa-a4d8-49e1-abf8-64cc48ee7be4.png)\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\nTested on v2.27 and v2.36 in Microsoft Edge\/Chromium\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Checked this on the latest version, and noticed that there was a \"show exemplars\" button which has similar \"remove filters when toggled\" behavior. For the sake of consistency: should filters be maintained after toggling for both, neither, or should they have different behavior?","The filters should be maintained on both, so that needs to be fixed too. Thanks for pointing it out."],"labels":["kind\/bug","component\/ui"]},{"title":"More labels regarding docker compose for docker_sd_configs","body":"# More Compose Meta Data Please\r\n\r\nSome more labels would be useful for docker_sd_configs, Mostly Docker compose information  such as:\r\n\r\n```yaml\r\n__meta_docker_compose_project_name\r\n__meta_docker_compose_config_hash\r\n...\r\n```\r\n\r\nThere are alot of use cases for such information. ","comments":["If this is available to the API calls we are using now, we could expose it. Is it the case? We should in general try to avoid extra calls.","It looks like these are already available as container labels.  \r\n\r\nWith just a default docker sd scrape config, and docker version `20.10.18+azure-2` I found meta labels called\r\n```\r\n__meta_docker_container_label_com_docker_compose_config_hash=\"7cd900f6f4df1b0cdf6a9f2865c7a4434d851e631975a5b786366d14b87d61e8\"\r\n__meta_docker_container_label_com_docker_compose_container_number=\"1\"\r\n__meta_docker_container_label_com_docker_compose_oneoff=\"False\"\r\n__meta_docker_container_label_com_docker_compose_project=\"hello-prometheus\"\r\n__meta_docker_container_label_com_docker_compose_project_config_files=\"\/workspaces\/hello-prometheus\/docker-compose.yml\"\r\n__meta_docker_container_label_com_docker_compose_project_working_dir=\"\/workspaces\/hello-prometheus\"\r\n__meta_docker_container_label_com_docker_compose_service=\"apache\"\r\n__meta_docker_container_label_com_docker_compose_version=\"2.11.2\"\r\n```\r\n\r\nIf this is the data you are looking for, then a relabel config would be able to expose them.\r\n\r\nAre you able to check if this data already exists from the SD?","Taking a look at this issue now, will follow up once I find these labels! ","Is this issue still persisting as it seems to still be open. I would like to work on it if possible \r\n","> Is this issue still persisting as it seems to still be open. I would like to work on it if possible\r\n\r\nI am working on it right now and I think it is still a persisting issue ","@MadVikingGod I just confirmed that these labels already exist in SD. I will give the relabel config a try as per your suggestion. Thank you!! \r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/52259856\/62534d7a-7523-4f06-828b-6c3753b9fc0d)\r\n"],"labels":["help wanted","low hanging fruit","kind\/enhancement","component\/service discovery"]},{"title":"performance improvement idea: drop metric names later","body":"Currently, many simple functions such as `abs()` call `DropMetricName()` on every sample.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/0906f2eafa3506d411f2053ecba160a6c5ca0387\/promql\/functions.go#L536-L539\r\n\r\nIt would be more efficient to leave till the end and do once per series.  `DropMetricName` is hashed, so the additional work on every step of a range query is not huge, but we could eliminate that work and the hash table.\r\n\r\nThis is not a trivial change: there are about four different ways that functions get called and it took me a while just to follow all the logic.  I do think it should be possible to refactor this area more simply and understandably.\r\n\r\nFor functions that take a vector, any metric name in the output is ignored!\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/0906f2eafa3506d411f2053ecba160a6c5ca0387\/promql\/engine.go#L1400-L1405\r\n\r\n","comments":["Some discussion about not dropping the metric name: #11397 "],"labels":["kind\/enhancement","priority\/Pmaybe","priority\/P3"]},{"title":"Consider warning users that label-propagation optimisations that seem 'trivial' are not done","body":"## Proposal\r\n\r\nI think it's fair to say developers tend to expect things to be automatically optimised - see any modern compiler or DBMS, and there's generally an assumption that they will spot obvious inefficiencies and fix them for you at 'build' time (whether that's building code or preparing queries for execution) before consuming computational time.\r\n\r\nI noticed today that Prometheus doesn't optimise queries along the lines of: `a{b=\"c\"} + d` into the form `a{b=\"c\"} + d{b=\"c\"}`, which can produce very significant (15x for my query, though this presumably differs wildly) performance improvements, and I searched around looking for the rationale for not optimising this, and stumbled upon an old issue (#8053) suggesting this exact optimisation. There's very good reasons enumerated there why this optimisation can't be taken, but I would wager that most users of Prometheus don't realise these use cases exist and would assume the same way I did that there is no need to manually optimise these queries.\r\n\r\nAs a consequence, I wonder whether it might be worth adding \"hints\" to the UI warning users that Prometheus doesn't optimise these things? I imagine, for example, if you input a query in the graph view along the lines of:\r\n\r\n`a{b=\"c\"} <bin-op> d`\r\n\r\nand assuming the query takes more than some fixed time - say, 1s - to execute, then there could be a one-time dismissible warning that you may benefit from including your label selectors on both sides of the query string if you are querying \"normal\" metrics stored in the TSDB, and not a dynamic remote endpoint that might do something particularly clever with `d`.\r\n\r\n**To be clear** -- I totally understand the rationale why this isn't optimised, and I'm not criticising or suggesting that changes! Just suggesting there might be a large volume of users who don't realise this and get bitten by this issue unknowingly on the regular. Perhaps a dynamic detection for these things in the UI isn't the correct path, and emphasising it in the documentation might be better? Very open to discussion here of course and I'm happy to help the implementation if others agree with this!","comments":[],"labels":["kind\/enhancement","component\/promql"]},{"title":"UI: Target order not sorted","body":"### What did you do?\n\nI've just upgraded to 2.36.2. In the Classic UI the order of the targets on the \"Status -> Targets\" page was sorted, probably based on the Endpoint URL. With the \"new\" UI, it's not sorted at all in my case. We have groups with over 100 targets, and you find nothing without sorting. Is this a bug or expected behaviour? It was always an issue for me with the new UI and now the classic UI is gone....\n\n### What did you expect to see?\n\nBest option would be to have the header where you can click to sort (asc\/desc) one or multiple specific columns. Because if one of the 100 targets is down, you have to scroll and search manually without an option to sort.\r\nHowever it would be fine for me to just get the sorted output based on endpoint again.\n\n### What did you see instead? Under which circumstances?\n\nTarget Output isn't sorted at all.\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\n2.36.2\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n```text\n0.24.0\n```\n\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["> and you find nothing without sorting.\r\n\r\nAnd using the search bar doesn't help you to find your targets ? ","Of course the search bar helps to filter. But without sorting I've to use the search bar always, I can't just scroll down and check without typing. What speaks agains sorting? Today it's just a listing of all targets in an unsorted way.\r\n ","I'm not against the sorting feature, I was just wondering if the search bar could help you while waiting for the fix. Because if you are searching for a specific target and you know a partial name, then the search bar should help you to find it quickly.","I agree, searching helps a lot, but as human its sometimes easier to scan the list with your eyes. But today it's even with a few targets complicated to find it without using the search bar.","Hi,I want to try this work","sure @lushaorong go ahead :)","> sure @lushaorong go ahead :)\n\nShall I refer to Classic UI or add a sort button","perhaps like suggested in the issue, we could have a sort per column header. So likely you have the sort arrow per column.","@lushaorong I may have stepped on your toes here, sorry! Happy to collaborate on this as needed.","@hazzadous I see you made some commits but they are not in the main branch yet. Any ideas why?","@josefzahner not sure, I saw some reviewer assignments on the PR, but I guess not a high priority. Please feel free to pick up, I don't really have any skin in the game as I don't use prom UI so much."],"labels":["low hanging fruit","kind\/enhancement","component\/ui","priority\/P3"]},{"title":"Before relabeling Grid keep flicker","body":"### What did you do?\n\nI notice that `before relabeling` grid of the `target` tab keep flicker. \r\n\n\n### What did you expect to see?\n\nWhen I move mouse on the lables, `before relabeling` grid  should show up\n\n### What did you see instead? Under which circumstances?\n\nNow when I move mouse on the lables, `before relabeling` grid keep flicker.\r\nit's hard to describe,  my browers horizontal and vertical scroll bars keep flicker too. But when I zoom out the web page, the issue maybe gone.\r\n\r\nI deploy prometheus in the K8S as a `statefulset`, and the target is discover by `kubernetes_sd_configs endpoint`\r\n\r\n\r\nI test on firefox, chrome and edge, and on the different computers, same thing happend\r\nI test some prometheus version, here is the result :\r\n\r\n- v2.20       good\r\n- v2.34       not good\r\n- v2.36.1    not good\r\n- v2.36.2    not good\r\n\r\n\r\nPerhaps there are too many `before relabeling` items cause this issue.\r\n\n\n### System information\n\nLinux 4.15.0-158-generic x86_64\n\n### Prometheus version\n\n```text\nSee above\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nThere are no error logs of prometheus pods\n```\n","comments":["Thanks for your report. I can reproduce it on  http:\/\/prombench.prometheus.io\/prometheus-meta\/targets","ah yes, I thought I had fixed this issue, but it seems not. It happens when the `tooltip` is displayed between the cursor and the Grid. The cursor is not on the grid anymore, so the tooltip disappear. And so on ...","Can we close this one in favor of #11158 ?"],"labels":["help wanted","kind\/bug","component\/ui","priority\/P3"]},{"title":"tls_config.server_name documentation is misleading","body":"### What did you do?\n\nI was expecting the `server_name` to pass hostname as an SNI extension with scrape requests, at least this is what documentation suggests https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#tls_config:\r\n\r\n```\r\n# ServerName extension to indicate the name of the server.\r\n# https:\/\/tools.ietf.org\/html\/rfc4366#section-3.1\r\n[ server_name: [<string>](https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#string) ]\r\n```\n\n### What did you expect to see?\n\nI spent some time debugging why `server_name` isn't sent with scrape requests, e.g. I enabled logging of the `$host` variables in my Nginx and it was empty. I could see the $host value when I used SNI with curl, so I believe my Nginx config was correct.\n\n### What did you see instead? Under which circumstances?\n\nI checked the code just to find out that this field is only used to verify TLS connection\r\nhttps:\/\/github.com\/prometheus\/common\/blob\/3763a1ded109d63be8a27824983c41413e056e1b\/config\/http_config.go#L778-L779:\r\n\r\n```\r\n\/\/ TLSConfig configures the options for TLS connections.\r\ntype TLSConfig struct {\r\n...\r\n\t\/\/ Used to verify the hostname for the targets.\r\n\tServerName string `yaml:\"server_name,omitempty\" json:\"server_name,omitempty\"`\r\n```\r\n\r\nI'd either made it clear in the documentation that the fields has nothing to do with SNI, or maybe implement the SNI support for scrapers?\n\n### System information\n\nLinux 4.14.273-207.502.amzn2.x86_64 x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.33.4 (branch: HEAD, revision: 83032011a5d3e6102624fe58241a374a7201fee8)\r\n  build user:       root@d13bf69e7be8\r\n  build date:       20220222-16:51:28\r\n  go version:       go1.17.7\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["```\r\n\t\/\/ ServerName is used to verify the hostname on the returned\r\n\t\/\/ certificates unless InsecureSkipVerify is given. It is also included\r\n\t\/\/ in the client's handshake to support virtual hosting unless it is\r\n\t\/\/ an IP address.\r\n```\r\n\r\nIt should be also SNI.","Do you have more feedback here ? Did you run a test about this or should we clarify the documentation *in the code*?","Hi @roidelapluie -- the thing is it wasn't included in the client's handshake.\r\n\r\nI don't have any problems with this anymore, because I've implemented a workaround already, but I had to do an extra research to understand it's not working as expected and thought I would raise this as an issue so you could clarify the behaviour in documentation for future users.\r\n\r\nOr are you saying it should pass the `server_name` already in version 2.33.4? If so -- I couldn't get it working, I didn't have `InsecureSkipVerify`, and I also cannot find the comment you're citing in this git repo. I guess I'm confused now :) ","The comment is from https:\/\/pkg.go.dev\/crypto\/tls , sorry :-) please note that in TLS1.3, SNI is also encrypted I think.","@roidelapluie encrypted SNI is still in the draft according to https:\/\/datatracker.ietf.org\/doc\/draft-ietf-tls-esni\/ -- are you saying Prometheus supports\/uses the draft version of ESNI by default?\r\n\r\nMaybe it was encrypted indeed. As I mentioned in the original post above I only checked the setup with Nginx and `curl`, I also used Wireshark sniffer on the server-side: Wireshark was able to show `server_name` from `curl`, but not from Prometheus, also Nginx couldn't \"see\" the `server_name` from Prometheus, but worked fine with `curl`.\r\n\r\nSo not really sure, are there any tests on the Prometheus that can prove SNI actually works as expected?\r\n\r\nMy workaround was to set up a dedicated `server` block with a dedicated `default_server` for requests that didn't match any other server names. I still have the `server_name` directive in my Prometheus scrapers, so I'm certain that at least the verification works. I'm also sure that the `server_name` field is not passed (at least as cleartext SNI that Nginx would recognize) because I disabled the metrics endpoints on other server blocks and I wouldn't have any metrics if it worked.","I have the same problem doing a mTLS scrape configuration with kubernetes service discovery (prometheus v2.37)\r\n\r\n`curl --cacert myCAchain.pem --cert mycert.pem --key mykey.key --resolve original-sni:443:pod.namespace https:\/\/original-sni:443\/metrics `\r\n\r\nworks, but putting the same in prometheus configuration\r\n```yaml  \r\n  - job_name: kubernetes-pods-mtls\r\n    tls_config:\r\n      ca_file: \/etc\/prometheus\/certs\/myCAchain.pem\r\n      cert_file: \/etc\/prometheus\/certs\/mycert.pem\r\n      key_file: \/etc\/prometheus\/certs\/mykey.key\r\n      server_name: original-sni\r\n    kubernetes_sd_configs:\r\n    ...\r\n```\r\n\r\nresults in:\r\n**server returned HTTP status 400 Host does not match SNI**\r\n\r\nadditionally it is no option to set \r\n```yaml  \r\n      insecure_skip_verify: true\r\n```\r\nas it also prevents prometheus to send the client certificate.\r\n\r\n`curl -k --cacert myCAchain.pem --cert mycert.pem --key mykey.key https:\/\/pod.namespace\/metrics `\r\nworks!\r\n"],"labels":["kind\/more-info-needed"]},{"title":"Add various warnings (and maybe even other info) to PromQL evaluation results","body":"_This is meant as an umbrella issues for adding warnings (and possibly non-warning just-informational annotations) to PromQL results. (I failed to find an existing issue.) If parts of this turn out to be more involved, separate issues might be filed for it._\r\n\r\nThis topic was discussed at the dev summit, as can be seen in [the meeting notes](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#heading=h.9dxdqy30r96p). Very generally, there is a long-standing desire to \u201cexplain\u201d the results of a PromQL query better. Most commonly, this would affect things that went wrong in a way that isn't an outright error. That's where the term \u201cwarning\u201d is coming from. But there are also cases where the explanation isn't even a warning, e.g. performance stats about the evaluation or (relevant with the new histograms) information about the accuracy of a result (based on the resolution of the histograms involved).\r\n\r\nThe effort has two parts: (1) Providing the plumbing to deliver the warnings (or \u201cannotations\u201d or whatever we call them). (2) Actually issuing warnings\/annotations.\r\n\r\n### Plumbing\r\n\r\nThe query API already has a warnings field (only used for remote-read issues so far) and returns stats. What's missing is support in the UI to display warnings and stats, and maybe a notion of \u201cannotations\u201d for valuable information that isn't supposed to flag anything \u201cwrong\u201d so that framing it as a warning would be confusing.\r\n\r\nA concern is that the [`promql.Result` type](https:\/\/github.com\/prometheus\/prometheus\/blob\/e4a09f2b4b4772a886a694eb29a90712d41d63df\/promql\/value.go#L195-L201) contains the warnings as `storage.Warnings`, presumably because the warnings are currently only used for issues coming from remote-read. With more warnings (or even \u201cannotations\u201d), they will also come from other sources, like the PromQL engine itself, and the `storage.Warnings` type would be misleading.\r\n\r\n### Actual warnings\/annotations to add\r\n\r\nReasons for warnings expressed in the past include:\r\n- Explain a failed label match (or any label match).\r\n- Warn about technically correct but nonsensical usage (e.g. `quantile(10, foo)`).\r\n- Warn about applying gauge functions to counters or counter function to gauges (based on the naming (`..._total`) or even based on what's stored in the metadata buffer, it would of course be better to have a proper persistent metadata storage).\r\n- Warn about `rate` and similar calculations that fail for a lack of samples covered by the used range.\r\n\r\nSome of those are not trivial. E.g. the last point about not enough samples to calculate a rate shouldn't warn if it happens for a \u201clegitimate\u201d reason (e.g. the series ends or starts within the range or even outright before or after the range), but if \u201csimply\u201d extending the range a bit would allow the calculation to succeed, a warning would be helpful. Finding out about that might be costly, so the we might not want those warnings to be \u201con by default\u201d\u2026 (Again, a fully-fledged metadata storage would help, which could know when a series starts or ends and what intended scrape interval it has.)\r\n\r\nThe new sparse histograms add a whole lot of more opportunities to warn:\r\n- New histogram samples are mixed up with conventional samples.\r\n- Incompatible bucket layouts prevent an aggregation (over time or between different histograms).\r\n- \u2026\r\n\r\n\r\n\r\n","comments":["#12152 is merged now and a huge leap forward.\r\n\r\nWe still need to figure out how to warn about `rate` and similar calculations that fail for a lack of samples covered by the used range. (We don't want a huge number of false positives here.)\r\n\r\nAlso note that rule evaluations have ignored warnings so far, which was probably fine as they only came from remote read. Now that warnings AKA annotations are much more useful for recording rules, we need to log and count the annotations (and maybe even display them in the rules page of the web UI). The starting point is probably the [EngineQueryFunc](https:\/\/github.com\/prometheus\/prometheus\/blob\/156222cc509d2f234589ba0aac6ae8f673f54a85\/rules\/manager.go#L196), where we need to extract the annotations from the `res` and return it as an additional return argument."],"labels":["priority\/P2","component\/ui","component\/promql","component\/api","kind\/feature"]},{"title":"config memory-snapshot-on-shutdown but wal replay still exist","body":"### What did you do?\n\nI install prometheus with docker and config memory-snapshot-on-shutdown\n\n### What did you expect to see?\n\nThe prometheus docs tell me with this config prometheus will reduce the startup time since the memory state can be restored with this snapshot and m-mapped chunks without the need of WAL replay.\r\n\r\nhttps:\/\/prometheus.io\/docs\/prometheus\/2.34\/feature_flags\/#memory-snapshot-on-shutdown\r\n\r\n\n\n### What did you see instead? Under which circumstances?\n\nI reboot prometheus and found from the log that  prometheus start still need wal relay, and this is not the same as prometheus docs\n\n### System information\n\nLinux 5.10.83 x86_64\n\n### Prometheus version\n\n```text\nprometheus, version 2.34.0 (branch: HEAD, revision: 881111fec4332c33094a6fb2680c71fffc427275)\r\n  build user:       root@121ad7ea5487\r\n  build date:       20220315-15:18:00\r\n  go version:       go1.17.8\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n```yaml\n--enable-feature=memory-snapshot-on-shutdown \r\n--log.level=debug\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\nts=2022-06-08T06:11:26.188Z caller=head.go:493 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nts=2022-06-08T06:11:26.188Z caller=head.go:500 level=info component=tsdb msg=\"Chunk snapshot is enabled, replaying from the snapshot\"\r\nts=2022-06-08T06:11:27.222Z caller=head_wal.go:1044 level=info component=tsdb msg=\"chunk snapshot loaded\" dir=\/prometheus\/chunk_snapshot.004003.0031424512 num_series=136898 duration=1.033069595s\r\nts=2022-06-08T06:11:27.222Z caller=head.go:514 level=info component=tsdb msg=\"Chunk snapshot loading time\" duration=1.034020665s\r\nts=2022-06-08T06:11:27.371Z caller=head.go:536 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=148.755762ms\r\nts=2022-06-08T06:11:27.371Z caller=head.go:542 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nts=2022-06-08T06:11:27.400Z caller=head.go:613 level=info component=tsdb msg=\"WAL segment loaded\" segment=4003 maxSegment=4004\r\nts=2022-06-08T06:11:27.401Z caller=head.go:613 level=info component=tsdb msg=\"WAL segment loaded\" segment=4004 maxSegment=4004\r\nts=2022-06-08T06:11:27.401Z caller=head.go:619 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=163.422\u00b5s wal_replay_duration=29.367864ms total_replay_duration=1.212414046s\r\nts=2022-06-08T06:11:27.548Z caller=main.go:958 level=info fs_type=EXT4_SUPER_MAGIC\r\nts=2022-06-08T06:11:27.548Z caller=main.go:961 level=info msg=\"TSDB started\"\r\n```\n```\n","comments":["Your complaint is about 29.36 milliseconds?","yes, there are two questions about this\r\n1.  On large producion environment, wal files are large and wal replay takes a long time\r\n2.  I found without memory-snapshot-on-shutdown config checkpoint and wal replay duration all need long time. is that this config reduce the time of this two phases?","Yes, the function of `--enable-feature=memory-snapshot-on-shutdown` is to snapshot the active memory Prometheus had when you shut it down.  It dumps this snapshot to disk as an m-mapped structure.  It still does a WAL replay, but does so by reading in this m-map file instead of fetching up to 2h of metrics by scanning through the disk.  So, the debug output you listed is correct.  Prometheus did a WAL replay and the replay took just under 30 milliseconds because it came from the m-map.\r\n\r\nI think this issue can be closed; it seems to be a slight misunderstanding of wording in the docs.","The WAL replay takes 30 milliseconds, which means that indeed it is not really \"done\". Maybe we can indeed clarify the docs or the logs @codesome ","Somehow I missed this issue. Yes the snapshot does not mean it won't touch the WAL. There could be many reasons why new WAL files came in later without a new snapshot. A docs thing yes.","can any one provide the prometheus  values files. or where should i mention this value --enable-feature=memory-snapshot-on-shutdown "],"labels":["priority\/P3","component\/documentation"]},{"title":"Creating remote.Storage twice results in duplicate metrics collector registration","body":"### What did you do?\n\nWe vendor the `remote.Storage` implementation in Grafana Tempo. If we receive data for a tenant, we create a new `remote.Storage` for that tenant and apply its remote write config. If this fails (because the config happens to be invalid), we will retry creating the `remote.Storage` later resulting in the process panicking.\r\n\r\nIn this specific case, applying the config failed because `tls_config.ca_file` was invalid.\n\n### What did you expect to see?\n\nThe process should not panic when `remote.Storage` is created a second time with the same `prometheus.Registerer`.\r\n\r\nIf it's not possible to guarantee this, a validate function would also be really nice, this would allows us to fail while the process is still starting.\n\n### What did you see instead? Under which circumstances?\n\nThe process panicked with\r\n\r\n```\r\npanic: duplicate metrics collector registration attempted\r\n\r\ngoroutine 270 [running]:\r\ngithub.com\/prometheus\/client_golang\/prometheus.(*wrappingRegisterer).MustRegister(0xc00108c2a0, {0xc000a15eb0, 0x1, 0x1dc9cb4})\r\n\t\/drone\/src\/vendor\/github.com\/prometheus\/client_golang\/prometheus\/wrap.go:104 +0x151\r\ngithub.com\/prometheus\/prometheus\/tsdb\/wal.NewWatcherMetrics({0x2259ca8, 0xc00108c2a0})\r\n\t\/drone\/src\/vendor\/github.com\/prometheus\/prometheus\/tsdb\/wal\/watcher.go:139 +0x374\r\ngithub.com\/prometheus\/prometheus\/storage\/remote.NewWriteStorage({0x223be60, 0xc00043d040}, {0x2259ca8, 0xc00108c2a0}, {0xc000acfb20, 0x1c}, 0xdf8475800, {0x223af00, 0x32cf470})\r\n\t\/drone\/src\/vendor\/github.com\/prometheus\/prometheus\/storage\/remote\/write.go:77 +0xa5\r\ngithub.com\/prometheus\/prometheus\/storage\/remote.NewStorage({0x223a940, 0xc0006a4230}, {0x2259ca8, 0xc00108c2a0}, 0x1e6b688, {0xc000acfb20, 0x1c}, 0x0, {0x223af00, 0x32cf470})\r\n\t\/drone\/src\/vendor\/github.com\/prometheus\/prometheus\/storage\/remote\/storage.go:75 +0x116\r\ngithub.com\/grafana\/tempo\/modules\/generator\/storage.New(0xc000b5cfa0, {0x1dce4fd, 0xd}, {0x2259c78, 0xc0000bca50}, {0x223a940, 0xc0002f7360})\r\n\t\/drone\/src\/modules\/generator\/storage\/instance.go:60 +0x4b6\r\n...\r\n```\r\n\r\nA minimal test case to reproduce the error:\r\n\r\n```go\r\npackage modules\r\n\r\nimport (\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com\/go-kit\/log\"\r\n\t\"github.com\/pkg\/errors\"\r\n\t\"github.com\/prometheus\/client_golang\/prometheus\"\r\n\t\"github.com\/prometheus\/common\/model\"\r\n\t\"github.com\/prometheus\/prometheus\/scrape\"\r\n\t\"github.com\/prometheus\/prometheus\/storage\/remote\"\r\n\t\"github.com\/stretchr\/testify\/assert\"\r\n)\r\n\r\nfunc Test(t *testing.T) {\r\n\treg := prometheus.NewRegistry()\r\n\tstartTimeCallback := func() (int64, error) {\r\n\t\treturn int64(model.Latest), nil\r\n\t}\r\n\tremoteStorage := remote.NewStorage(log.NewNopLogger(), reg, startTimeCallback, \"\/var\/wal\", time.Minute, &noopScrapeManager{})\r\n\tassert.NoError(t, remoteStorage.Close())\r\n\r\n\tremoteStorage = remote.NewStorage(log.NewNopLogger(), reg, startTimeCallback, \"\/var\/wal\", time.Minute, &noopScrapeManager{})\r\n\tassert.NoError(t, remoteStorage.Close())\r\n}\r\n\r\ntype noopScrapeManager struct{}\r\n\r\nfunc (noop *noopScrapeManager) Get() (*scrape.Manager, error) {\r\n\treturn nil, errors.New(\"scrape manager not implemented\")\r\n}\r\n```\n\n### System information\n\n_No response_\n\n### Prometheus version\n\n```text\n2.34.0-rc.0 (v1.8.2-0.20220228151929-e25a59925555)\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["I don't mind contributing a PR to ensure we can create `remote.Storage` twice by calling `Close` in between. Just want to check first if this would be accepted \ud83d\ude42 ","You could create a new registry each time and register it to the main registry after successful creation. ","We could look into my solution or yours, I think however it is reasonable to expect a new registerer each time.","Is there any progress? This happens also in [https:\/\/github.com\/grafana\/grafana\/issues\/68213](https:\/\/github.com\/grafana\/grafana\/issues\/68213)"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/remote storage"]},{"title":"Fix notation of `storage.tsdb.retention.size` to match kubernetes notation","body":"## Proposal\r\nLet `storage.tsdb.retention.size` take values in the same format as k8s does, specifically:\r\n- support both binary and decimal formats\r\n- interpret unitless values as decimal bytes\r\n\r\nCurrently, the `storage.tsdb.retention.size` flag is expected to be specified in decimal notation (e.g. 500MB), but is interpreted as binary notation (i.e. 500MiB):\r\n\r\n> supported units: B, KB, MB, GB, TB, PB, EB. Ex: \\\"512MB\\\". Based on powers-of-2, so 1KB is 1024B.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/57f4aab27d53872b109c8513846765df6b51eb57\/cmd\/prometheus\/main.go#L304-L307\r\n\r\nCompare to [k8s spec](https:\/\/kubernetes.io\/docs\/concepts\/configuration\/manage-resources-containers\/#meaning-of-memory):\r\n\r\n> You can express memory as a plain integer or as a fixed-point number using one of these [quantity](https:\/\/kubernetes.io\/docs\/reference\/kubernetes-api\/common-definitions\/quantity\/) suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:\r\n>\r\n> 128974848, 129e6, 129M,  128974848000m, 123Mi\r\n\r\nThis is actually a standard ([ref](https:\/\/en.wikipedia.org\/wiki\/Byte#History_of_the_conflicting_definitions)):\r\n\r\n> In December 1998, the IEC addressed such multiple usages and definitions by adopting the IUPAC's proposed prefixes (kibi, mebi, gibi, etc.) to unambiguously denote powers of 1024. Thus one kibibyte (1 KiB) is 1024^1 bytes = 1024 bytes, one mebibyte (1 MiB) is 1024^2 bytes = 1,048,576 bytes, and so on.","comments":["I do not think we should support unitless values. For decimal notation we should be very careful about backwards compatibility, and get it upstream first: https:\/\/github.com\/alecthomas\/units","My bad, seems like `storage.tsdb.retention.size` is currently using **legacy binary** notation. Is that correct?\r\n![image](https:\/\/user-images.githubusercontent.com\/82407168\/171034717-5625b570-2ed6-490f-a23a-127e1eb39611.png)\r\n\r\nbut seems like `alecthomas\/units` supports all three to some extent.\r\n\r\nOther than \"kB\" and \"KB\", all the others (MB, GB, TB) are identical, so for backwards compatibility would probably have to have a new flag and \"Overrides _ if this flag is set to anything other than default.\"..?","Apart from the configuration not yet allowing the IEC (https:\/\/en.wikipedia.org\/wiki\/Binary_prefix#IEC_prefixes) prefixed to indicate binary units, ( as in `GiB`) there already seems to be a display inconsistency when looking at the flags at runtime:\r\n\r\nThe documentation at https:\/\/prometheus.io\/docs\/prometheus\/latest\/storage\/#operational-aspects clearly states that --storage.tsdb.retention.size is \"Based on powers-of-2, so 1KB is 1024B. \".\r\n\r\nI therefore set `--storage.tsdb.retention.size=750GB`, the only supported format for the Gigabyte unit. When looking at `prometheus_tsdb_retention_limit_bytes` being `805306368000` which is 750*1024*1024*1024 this also seems accurate.\r\n\r\nBut when then looking at \"http:\/\/localhost:9090\/flags\" is see that this is \"converted\" to `--storage.tsdb.retention.size 750GiB` using `GiB`. In any case, while potentially painful once when asking folks to change their config, it would be great to align any uses of units to avoid further confusion.\r\n\r\nAdding \/ allowing scientific units might also be nice, sometimes that's what you got from your configuration management or whatever data source you use to set the `tsdb.retention.size` value ...\r\n\r\n@sed-i  maybe the title of this issue should better ask for the alignment with the standardized \"IEC units\" (https:\/\/en.wikipedia.org\/wiki\/Binary_prefix#IEC_prefixes) than pointing to \"kubernetes sizes\" which is just another piece of software using them.\r\n\r\n\r\n\r\nBTW, this issue seems to be a duplicate of https:\/\/github.com\/prometheus\/prometheus\/issues\/9406."],"labels":["kind\/enhancement","not-as-easy-as-it-looks","priority\/P3"]},{"title":"Kubernetes service discovery continues to scrape metrics from the IP of pods in an Error state","body":"### What did you do?\n\nWe have configured Prometheus with Kubernetes service discovery. We have a job named `kubernetes-pods` which uses the `pod` role for service discovery.\r\n\r\nWe had a pod named `ibv-instructions-svc-111-4gvk8` running with pod IP `100.99.29.17` exposing metrics on port 8009 under the path `\/metrics`. This pod failed (due to excess memory usage), and went into the `Error` state:\r\n\r\n```\r\n$ k get po -o wide\r\nNAME                                  READY   STATUS    RESTARTS         AGE     IP                NODE   NOMINATED NODE   READINESS GATES\r\n[\u2026]\r\nibv-instructions-svc-111-4gvk8        0\/1     Error     0                42h     100.99.29.17      kn5    <none>           <none>\r\nibv-instructions-svc-111-58gs5        1\/1     Running   3 (22h ago)      42h     100.97.22.212     kn3    <none>           <none>\r\nibv-instructions-svc-111-7d28m        1\/1     Running   0                42h     100.97.52.60      kn4    <none>           <none>\r\nibv-instructions-svc-111-ctwvv        1\/1     Running   0                22h     100.112.31.6      kn2    <none>           <none>\r\n```\r\n\r\nAfter some time, this pod IP was re-used by another pod `admin-checks-3-p45lk` (actually in a different namespace, though that is not relevant for the ticket):\r\n\r\n```\r\n$ k get po -o wide --namespace admin\r\nNAME                              READY   STATUS    RESTARTS   AGE   IP               NODE   NOMINATED NODE   READINESS GATES\r\n[\u2026]\r\nadmin-checks-3-m2mnr              1\/1     Running   0          64m   100.97.96.24     kn7    <none>           <none>\r\nadmin-checks-3-p45lk              1\/1     Running   0          64m   100.99.29.17     kn5    <none>           <none>\r\nadmin-checks-3-qpmg4              1\/1     Running   0          65m   100.97.23.24     kn3    <none>           <none>\r\n```\r\n\r\nThis new pod also exposes metrics on port 8009, under the same path `\/metrics`.\n\n### What did you expect to see?\n\nWe expected to no longer see any metrics from the pod `ibv-instructions-svc-111-4gvk8`, once it had entered an Error state. We did not expect to see it in the Targets view of the prometheus agent that is scraping it. \n\n### What did you see instead? Under which circumstances?\n\nMetrics emitted by the new pod `admin-checks-3-p45lk` are visible, but a _copy_ of those metrics labelled as though they come from the old pod `ibv-instructions-svc-111-4gvk8` are also visible.\r\n\r\nWith our relabelling config (attached below) we see otherwise-duplicate metrics with the following labels:\r\n- `{cluster_loc=\"stg1\", instance=\"100.99.29.17:8009\", job=\"kubernetes-pods\", kubernetes_cluster=\"app-clusterx1.stg-ld5.gbr\", kubernetes_namespace=\"docs\", kubernetes_pod_name=\"ibv-instructions-svc-111-4gvk8\", monitor=\"kubernetes\", serviceName=\"ibv-instructions-svc\"}`\r\n- `{cluster_loc=\"stg1\", instance=\"100.99.29.17:8009\", job=\"kubernetes-pods\", kubernetes_cluster=\"app-clusterx1.stg-ld5.gbr\", kubernetes_namespace=\"admin\", kubernetes_pod_name=\"admin-checks-3-p45lk\", monitor=\"kubernetes\", serviceName=\"admin-checks\"}`\n\n### System information\n\nKubernetes 1.22.5\n\n### Prometheus version\n\n```text\nprometheus, version 2.35.0 (branch: HEAD, revision: 6656cd29fe6ac92bab91ecec0fe162ef0f187654)\r\n  build user:       root@cf6852b14d68\r\n  build date:       20220421-09:53:42\r\n  go version:       go1.18.1\r\n  platform:         linux\/amd64\n```\n\n\n### Prometheus configuration file\n\n```yaml\nglobal:\r\n  scrape_interval: 15s\r\n  external_labels:\r\n    monitor: 'kubernetes'\r\n    cluster_loc: \"{{cluster_loc}}\"\r\n    kubernetes_cluster: \"{{cluster_name}}\"\r\n\r\nremote_write: [] # elided for brevity\r\n\r\nscrape_configs:\r\n# several elided for brevity\r\n\r\n#\r\n# Scrape from Kubernetes pods\r\n#\r\n- job_name: 'kubernetes-pods'\r\n  label_limit: 100\r\n  label_name_length_limit: 100\r\n  label_value_length_limit: 2000\r\n\r\n  # ask k8s for a list of pods\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n\r\n  relabel_configs:\r\n  # Pod scraping config comes from annotations on the pod.\r\n  #  \u2022 `prometheus.io\/scrape`: Only scrape pods that have a value of `true`\r\n  #  \u2022 `prometheus.io\/path`: If the metrics path is not `\/metrics` override this.\r\n  #  \u2022 `prometheus.io\/port`: If the metrics are exposed on a different port to the\r\n  #    pod then set this appropriately.\r\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\r\n    action: keep\r\n    regex: true\r\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\r\n    action: replace\r\n    target_label: __metrics_path__\r\n    regex: (.+)\r\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\r\n    action: replace\r\n    target_label: __address__\r\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n    replacement: $1:$2\r\n\r\n  # Add pod name and namespaces labels to the time series.\r\n  - source_labels: [__meta_kubernetes_namespace]\r\n    action: replace\r\n    target_label: kubernetes_namespace\r\n  - source_labels: [__meta_kubernetes_pod_name]\r\n    action: replace\r\n    target_label: kubernetes_pod_name\r\n\r\n  # Connect team used the \"serviceName\" label in some dashboards; keep it in case\r\n  # it's useful elsewhere.\r\n  - source_labels: [__meta_kubernetes_pod_label_serviceName]\r\n    action: replace\r\n    target_label: serviceName\n```\n\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n_No response_","comments":["Please note: I don't believe this is necessarily a duplicate of #10257, because that is using the `endpoints` role and not the `pods` role.\r\n\r\nAs far as I can see, #10257 is being caused by the Endpoints object (maintained by k8s) retaining a reference to the old pod IP, whereas this issue is because Prometheus is not considering the fact that the pod has exited (state `Error`) before it decides to scrape, and it just so happens to pick up some other unrelated pod that has since come along on the same virtual IP.","I've captured the YAML for the pod, in case it's useful to see how to extract the status: [pod.yaml.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8780317\/pod.yaml.gz)\r\n","The underlying reason here is actually very similar to #10257, pods are kept around after termination; this is dependent on kube-controller-manager GC flags and the amount of pod churn in the environment.\r\n\r\nAs mentioned in https:\/\/github.com\/prometheus\/prometheus\/issues\/10257#issuecomment-1046784527 for endpoints the desired behaviour can depend on the environment. Pods are similar to endpoints in this respect and there is a meta label available to relabel on ` __meta_kubernetes_pod_phase`.\r\n\r\nA relabel like:\r\n\r\n```yaml\r\n- source_labels: [__meta_kubernetes_pod_phase]\r\n  action: drop\r\n  regexp: Failed|Succeeded\r\n```\r\n\r\nWould fix this (the pod yaml you provided -- thanks! -- shows `phase: Failed` for this case). It probably would make sense to suggest this in the Kubernetes example config.","Thank you for the response.\r\n\r\nI agree that it would be sensible to provide this in the example config, as we did use that as the starting basis for our own config. But it is rather surprising that we need to explicitly filter out pods in this status (I believe that it is only in Kubernetes v1.22 onwards that pods that have exited retain their original `IP` field value in the `Status` section \u2014 in earlier releases, the pod objects still existed but they had no `IP` value).\r\n\r\nI have added the following `relabel_configs` entry to the job:\r\n\r\n```yaml\r\n  # Only scrape pods that are in the Running phase.\r\n  #  https:\/\/github.com\/prometheus\/prometheus\/issues\/10755#issuecomment-1139248161\r\n  #  https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.22\/#podstatus-v1-core\r\n  #    Pending \u2014 not all containers running yet. Pod may not even be scheduled yet.\r\n  #    Running \u2014 all containers created and at least one still running.\r\n  #    Succeeded \u2014 all containers exited with status=0, pod will not restart.\r\n  #    Failed \u2014 one or more containers exited with status\u22600, pod will not restart.\r\n  #    Unknown \u2014 we lost contact with the host's kubelet. We have seen this persist for\r\n  #              days after a node machine dies.\r\n  - source_labels: [__meta_kubernetes_pod_phase]\r\n    action: keep\r\n    regex: Running\r\n```\r\n\r\nI chose to only keep pods in the `Running` phase \u2014 each of the other phases I have observed to overlap with cases where I know metrics scraping would fail and I'd rather not have the error messages \/ metrics from Prometheus in this case. However, perhaps others may evaluate the trade-offs differently.\r\n\r\nI've verified this on the cluster where we still have the same status as in the bug report and it does indeed just drop the one target that was being scraped erroneously.","sorry guys, \r\n I have the same issue, basicly it is scraping (**sysdig**-> **prometheus**-> **pods** ) also pods which are in **Terminating** phase.\r\nI thought that the following should address what the author of this issue is asking, is there any solution for that?\r\n\r\n\r\n\r\n\r\n```\r\n    prometheus.yaml: |\r\n      global:\r\n        scrape_interval: 1m\r\n      - job_name: 'kubernetes-pods'\r\n        kubernetes_sd_configs:\r\n        - role: pod\r\n        relabel_configs:\r\n        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\r\n          action: keep\r\n          regex: true\r\n        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\r\n          action: replace\r\n          target_label: __metrics_path__\r\n          regex: (.+)\r\n        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\r\n          action: replace\r\n          target_label: __address__\r\n          regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n          replacement: $1:$2\r\n        - source_labels: [__meta_kubernetes_namespace]\r\n          action: replace\r\n          target_label: kubernetes_namespace\r\n        - source_labels: [__meta_kubernetes_pod_name]\r\n          action: replace\r\n          target_label: kubernetes_pod_name\r\n        - source_labels: [__meta_kubernetes_pod_label_serviceName]\r\n          action: replace\r\n          target_label: serviceName\r\n        - source_labels: [__meta_kubernetes_pod_phase]\r\n          action: keep\r\n          regex: Running\r\n```\r\n\r\n\r\nthis seems not working properly. @lwithers any help?\r\n"],"labels":["component\/service discovery","kind\/more-info-needed"]},{"title":"IONOS cloud pagination","body":"The IONOS cloud integration does not use pagination, limiting to 1000 the number of servers discovered per datacenter.\r\n\r\nWe should integrate pagination in case a user has more than 1000 servers.\r\n","comments":[],"labels":["kind\/enhancement","component\/service discovery","priority\/P3"]},{"title":"Snapshot folder catched by prometheus","body":"### What did you do?\n\nHi, I created some snapshot data with that api:\r\n curl -XPOST http:\/\/localhost:9090\/api\/v1\/admin\/tsdb\/snapshot\r\nThe folder was created successfully \r\nNow I wanted to copy the folder to side folder with cp command.\r\n\n\n### What did you expect to see?\n\nI excpect to be able to copy the snapshot folder\n\n### What did you see instead? Under which circumstances?\n\nIt seems that I cannot copy the folder as long as the prometheus is running.\r\nI am getting the error:\r\ncp: cannot open '20220511T121232Z-57f36c7e91baff60\/01G2SCD9E9V9HWHVXYC2J1XFYS\/chunks\/000001' for reading: Invalid argument\r\nBut removing the folder is allowed.\r\nIt seems that the Prometheus is keeping some links to the snapshot folder files.\r\nCan it be fixed ?\r\n\r\nThanks in advance\n\n### System information\n\nLinux 4.15.0-175-generic x86_6\n\n### Prometheus version\n\n```text\nv2.33.0\n```\n\n\n### Prometheus configuration file\n\n_No response_\n\n### Alertmanager version\n\n_No response_\n\n### Alertmanager configuration file\n\n_No response_\n\n### Logs\n\n```text\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33P08RW5C8VRCGPAAQ9HTAE\/chunks\/000001' for reading: Invalid argument\r\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33P08RW5C8VRCGPAAQ9HTAE\/index' for reading: Invalid argument\r\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33F49TJNC1K7JQHAYJY6ASM\/chunks\/000001' for reading: Invalid argument\r\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33F49TJNC1K7JQHAYJY6ASM\/index' for reading: Invalid argument\r\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33P010MTKTG5FNHBEXSFJS2\/chunks\/000001' for reading: Invalid argument\r\ncp: cannot open '20220515T122526Z-4f97da5d35e0955c\/01G33P010MTKTG5FNHBEXSFJS2\/index' for reading: Invalid argument\n```\n","comments":["Can you please provide the logs of Prometheus, especially the startup logs?\r\n\r\nThanks","Tried to reproduce this behavior.\r\n\r\n### What did you do?\r\n1. Executed `curl -XPOST http:\/\/localhost:9090\/api\/v1\/admin\/tsdb\/snapshot`.\r\n2. Executed `cp -r <<snapshot_path>> <<destination_path>>`.\r\n\r\n### What did you expect to see?\r\nI expected to be able to copy the snapshot folder.\r\n\r\n### What did you see instead? Under which circumstances?\r\nI was indeed able to copy the snapshot folder without issues.\r\n\r\n### Prometheus version\r\n> 2.35.0"],"labels":["kind\/more-info-needed"]},{"title":"Makefile vulnerability ","body":"\n<!--\n\n    Please do *NOT* ask support questions in Github issues.\n\n    If your issue is not a feature request or bug report use our\n    community support.\n\n    https:\/\/prometheus.io\/community\/\n\n    There is also commercial support available.\n\n    https:\/\/prometheus.io\/support-training\/\n\n-->\n## Proposal\n**Use case. Why is this important?**\nCurrently the Makefile pipes a download from curl directly to tar without verifying checksums. \n\nThis makes the build vulnerable to man in the middle attacks, and prevents the build from being reproducible. \n\nVerifying checksums will fix the security vulnerability. Additionally verifying checksums makes builds reproducible and cachable.\n","comments":["Thanks. Indeed it would be a nice enhancement. Do you want to submit a pull request?","I can cook one up. The one I wasn't sure about was if locking on particular versions of the curled files was suitable for your usage.\r\n\r\nFor example:\r\n```sh\r\ncurl -sfL https:\/\/raw.githubusercontent.com\/golangci\/golangci-lint\/$(GOLANGCI_LINT_VERSION)\/install.sh \\\r\n  ...\r\n```\r\nIn this case you'll want a big case statement for any possible versions:\r\n```sh\r\nexpected_sha=;  \\\r\ncase \"$GOLANGCI_LINT_VERSION\" in  \\\r\n  0.0.0)  expected_sha=\"XXXXXXXX...\"; ;;\r\n  0.0.1)  expected_sha=\"XXXXXXXX...\"; ;;\r\n  ...\r\nesac;  \\\r\nif test \"$( shasum -a 256 FILE|awk '{print $1}'; )\" != \"$expected_sha\"; then  \\\r\n  rm FILE;  \\\r\n  echo \"UHOH!\" > \/dev\/stderr;  \\\r\n  exit 1;  \\\r\nfi;  \\\r\n...\r\n```\r\n\r\nSo knowing which versions you require or accept is important. If you're cool with any version I can just set it to explicitly download the most recent release and hard code the SHA verification; but you'll obviously need to update that in the future if you ever need to bump the linter version. Otherwise a list of possible versions so I can look up the checksums and make a `case` routine.","The version is fixed in the makefile.\n\nOn 02 May 14:09, aameen-tulip wrote:\n> I can cook one up. The one I wasn't sure about was if locking on particular versions of the curled files was suitable for your usage.\n> \n> For example:\n> ```sh\n> \tcurl -sfL https:\/\/raw.githubusercontent.com\/golangci\/golangci-lint\/$(GOLANGCI_LINT_VERSION)\/install.sh \\\n> ```\n> In this case you'll want a big case statement for any possible versions:\n> ```sh\n> expected_sha=;  \\\n> case \"$GOLANGCI_LINT_VERSION\" in  \\\n>   0.0.0)  expected_sha=\"XXXXXXXX...\"; ;;\n>   0.0.1)  expected_sha=\"XXXXXXXX...\"; ;;\n>   ...\n> esac;  \\\n> if test \"$( shasum -a 256 FILE|awk '{print $1}'; )\" != \"$expected_sha\"; then  \\\n>   rm FILE;  \\\n>   echo \"UHOH!\" > \/dev\/stderr;  \\\n>   exit 1;  \\\n> fi;  \\\n> ...\n> ```\n> \n> So knowing which versions you require or accept is important. If you're cool with any version I can just set it to explicitly download the most recent release and hard code the SHA verification; but you'll obviously need to update that in the future if you ever need to bump the linter version. Otherwise a list of possible versions so I can look up the checksums and make a `case` routine.\n> \n> -- \n> Reply to this email directly or view it on GitHub:\n> https:\/\/github.com\/prometheus\/prometheus\/issues\/10660#issuecomment-1115370650\n> You are receiving this because you commented.\n> \n> Message ID: ***@***.***>\n\n-- \nJulien Pivotto\nO11y - https:\/\/o11y.eu\/\n","Cool I'll PR tomorrow.","Morning @SuperQ  @roidelapluie @aameen-tulip \r\nI pushed a new PR at https:\/\/github.com\/prometheus\/prometheus\/pull\/13328 to continue @juanrh work","I question the validity of this security claim. The current download method uses https for chain of trust to github.com. If https is MitM'd, with a valid cert, I doubt anything local could be trusted.","The chance that `github.com` and its `certs` are compromised is almost unlikely to happen, IMHO"],"labels":["help wanted","kind\/enhancement"]},{"title":"Getting CPU spikes on 2.19.3","body":"### What did you do?\r\n\r\nWe are seeing CPU spikes and slow query execution when we load via grafana dashboards. Till now we were only using it for monitoring via grafana alerts. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/57655135\/166132081-a3c6363c-20ea-4144-9a82-05d46a093655.png)\r\n\r\nAccording to newrelic, memory is about 70-80% free\r\n\r\n### What did you expect to see?\r\n\r\nCPU spikes shouldn't happen\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nEven with a single query, I am seeing CPU spikes. I am running it in docker on an m5.xlarge EC2 machine.\r\n\r\nThe series which I am querying have high cardinality, they are among the highest cardinality series ingested\r\n\r\nI am attaching screenshots of the benchmark dashboard\r\n<img width=\"1403\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/57655135\/166132267-30f0b347-238b-4586-a914-e690e1963511.png\">\r\n\r\n<img width=\"1406\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/57655135\/166132271-1935adea-97f2-4079-8b4c-203f553cd437.png\">\r\n\r\n<img width=\"1406\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/57655135\/166132286-43ef5018-5d7e-4a32-8a3f-1087f413ab17.png\">\r\n\r\n<img width=\"1404\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/57655135\/166132292-05d3dd61-cdb3-4f55-87a5-57ae8f100e8f.png\">\r\n\r\n\r\n### System information\r\n\r\nLinux 4.9.0-8-amd64 x86_64\r\n\r\n### Prometheus version\r\n\r\n```text\r\nprometheus, version 2.19.3 (branch: HEAD, revision: 657ba532e42f1db8d7c77bf802378643da0d3118)\r\n  build user:       root@b99e50bced7c\r\n  build date:       20200724-12:20:53\r\n  go version:       go1.14.6\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n```yaml\r\n#global config\r\nglobal:\r\n  scrape_interval:     5m # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n  # scrape_timeout is set to the global default (10s).\r\n\r\n# Alertmanager configuration\r\nalerting:\r\n  alertmanagers:\r\n  - static_configs:\r\n    - targets:\r\n      # - alertmanager:9093\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n  # - \"first_rules.yml\"\r\n  # - \"second_rules.yml\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n  - job_name: 'telegraf'\r\n\r\n    # metrics_path defaults to '\/metrics'\r\n    # scheme defaults to 'http'.\r\n\r\n    static_configs:\r\n    - targets: ['telegraf:9273']\r\n\r\n  - job_name: 'monitoring-metric-collector'\r\n\r\n    # metrics_path defaults to '\/metrics'\r\n    # scheme defaults to 'http'.\r\n    scrape_interval: 15s\r\n    static_configs:\r\n    - targets: ['monitoring-metrics-collector:8001']\r\n\r\n  - job_name: 'aggregated-metric-collector'\r\n    scrape_interval: 15s\r\n    static_configs:\r\n    - targets: ['aggregated-metrics-collector:8001']\r\n\r\n  - job_name: 'prometheus'\r\n    scrape_interval: 15s\r\n    static_configs:\r\n    - targets: ['localhost:9090']\r\n\r\nremote_write:\r\n  - url: http:\/\/remote_write:9090\/api\/v1\/write\r\n    queue_config:\r\n      max_samples_per_send: 1000\r\n      max_shards: 200\r\n      capacity: 2500\r\n```\r\n\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n```\r\nlevel=info ts=2022-04-30T05:45:56.110Z caller=head.go:645 component=tsdb msg=\"Replaying WAL and on-disk memory mappable chunks if any, this may take a while\"\r\nlevel=info ts=2022-04-30T05:45:57.793Z caller=head.go:682 component=tsdb msg=\"WAL checkpoint loaded\"\r\nlevel=info ts=2022-04-30T05:45:58.287Z caller=head.go:706 component=tsdb msg=\"WAL segment loaded\" segment=7731 maxSegment=7734\r\nlevel=info ts=2022-04-30T05:45:58.816Z caller=head.go:706 component=tsdb msg=\"WAL segment loaded\" segment=7732 maxSegment=7734\r\nlevel=info ts=2022-04-30T05:45:59.134Z caller=head.go:706 component=tsdb msg=\"WAL segment loaded\" segment=7733 maxSegment=7734\r\nlevel=info ts=2022-04-30T05:45:59.134Z caller=head.go:706 component=tsdb msg=\"WAL segment loaded\" segment=7734 maxSegment=7734\r\nlevel=info ts=2022-04-30T05:45:59.134Z caller=head.go:709 component=tsdb msg=\"WAL replay completed\" duration=3.023948227s\r\nlevel=info ts=2022-04-30T05:45:59.827Z caller=main.go:694 fs_type=EXT4_SUPER_MAGIC\r\nlevel=info ts=2022-04-30T05:45:59.827Z caller=main.go:695 msg=\"TSDB started\"\r\nlevel=info ts=2022-04-30T05:45:59.827Z caller=main.go:799 msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nts=2022-04-30T05:45:59.828Z caller=dedupe.go:112 component=remote level=info remote_name=94c0c2 url=http:\/\/172.31.118.81:9090\/api\/v1\/write msg=\"Starting WAL watcher\" queue=94c0c2\r\nts=2022-04-30T05:45:59.828Z caller=dedupe.go:112 component=remote level=info remote_name=94c0c2 url=http:\/\/172.31.118.81:9090\/api\/v1\/write msg=\"Replaying WAL\" queue=94c0c2\r\nlevel=info ts=2022-04-30T05:45:59.828Z caller=main.go:827 msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nlevel=info ts=2022-04-30T05:45:59.828Z caller=main.go:646 msg=\"Server is ready to receive web requests.\"\r\nts=2022-04-30T05:46:12.053Z caller=dedupe.go:112 component=remote level=info remote_name=94c0c2 url=http:\/\/172.31.118.81:9090\/api\/v1\/write msg=\"Done replaying WAL\" duration=12.225549766s\r\nlevel=info ts=2022-04-30T07:00:16.322Z caller=compact.go:495 component=tsdb msg=\"write block\" mint=1651291200000 maxt=1651298400000 ulid=01G1WMA56ZD0Y0C05N98JM6PCV duration=4.195571528s\r\nlevel=info ts=2022-04-30T07:00:17.197Z caller=head.go:792 component=tsdb msg=\"Head GC completed\" duration=266.076165ms\r\nlevel=info ts=2022-04-30T07:00:18.521Z caller=head.go:869 component=tsdb msg=\"WAL checkpoint complete\" first=7731 last=7732 duration=1.309271398s\r\nlevel=info ts=2022-04-30T09:00:18.946Z caller=compact.go:495 component=tsdb msg=\"write block\" mint=1651298400000 maxt=1651305600000 ulid=01G1WV5WFD14B9BCCM46XBW7D1 duration=6.805652591s\r\nlevel=info ts=2022-04-30T09:00:20.018Z caller=head.go:792 component=tsdb msg=\"Head GC completed\" duration=426.295034ms\r\nlevel=info ts=2022-04-30T09:00:25.553Z caller=compact.go:441 component=tsdb msg=\"compact blocks\" count=3 mint=1651276800000 maxt=1651298400000 ulid=01G1WV64RTF42B2SGZ3A2FN91D sources=\"[01G1W6JPKA9APSGP9Q0FV7N1P9 01G1WDEDW5NKDTSZWF6A8TFAZJ 01G1WMA56ZD0Y0C05N98JM6PCV]\" duration=4.918738151s\r\nlevel=info ts=2022-04-30T09:00:40.631Z caller=compact.go:441 component=tsdb msg=\"compact blocks\" count=3 mint=1651233600000 maxt=1651298400000 ulid=01G1WV6APY9GBMQWHH3NHFM20M sources=\"[01G1VHZQ609TNAV9WPVWPKCHDY 01G1W6JR66184W42MGR2BGHF7Z 01G1WV64RTF42B2SGZ3A2FN91D]\" duration=13.913392203s\r\n```","comments":["Could you provide us with CPU profiling data (see https:\/\/jvns.ca\/blog\/2017\/09\/24\/profiling-go-with-pprof\/ )? Thanks!"],"labels":["kind\/more-info-needed"]},{"title":"Issues with promtool and metric cardinality analysis","body":"### What did you do?\r\n\r\n\r\nI've attempted to use the recent promtool cardinality check feature in order to gain the ability to spot high-cardinality endpoints without having to lookup these stats in prometheus or via the tsdb tool.\r\nUnfortunately, it appears that the tool fails to report cardinality stats in the situation where the metrics do not respect the naming convention best practices.  \r\n\r\nFor example, say we examine a subset of the node_exporter metrics (v1.3.1) and only include the series prefixed with \"node_memory_\".   \r\n```bash\r\ncurl -s http:\/\/myhost.local:9100\/metrics | grep node_memory_ > node_exporter-node_memory.prom\r\n```\r\n\r\nWhen placing those into a file and piping them to the check metrics command, it fails because of styling warnings (linting):\r\n```bash\r\n$ cat node_exporter-node_memory.prom | promtool-2.34 check metrics --extended\r\n\r\nnode_memory_AnonHugePages_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_AnonPages_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_CmaFree_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_CmaTotal_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_CommitLimit_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_DirectMap1G_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_DirectMap2M_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_DirectMap4k_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_HardwareCorrupted_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_HugePages_Free metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_HugePages_Rsvd metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_HugePages_Surp metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_HugePages_Total metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_KernelStack_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_MemAvailable_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_MemFree_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_MemTotal_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_PageTables_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_SwapCached_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_SwapFree_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_SwapTotal_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_VmallocChunk_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_VmallocTotal_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_VmallocUsed_bytes metric names should be written in 'snake_case' not 'camelCase'\r\nnode_memory_WritebackTmp_bytes metric names should be written in 'snake_case' not 'camelCase'\r\n```\r\n\r\nTo solve this, I've lower-cased all characters in the file (while keeping the upper case HELP\/TYPE strings):\r\n```bash\r\nsed 's\/.*\/\\L&\/g' < node_exporter-node_memory.prom > node_exporter-node_memory-valid.prom\r\nsed -i 's\/help \/HELP \/g' node_exporter-node_memory-valid.prom\r\nsed -i 's\/type \/TYPE \/g' node_exporter-node_memory-valid.prom\r\n```\r\n\r\nAlthough this also fails it fails due to a metric with a \"_total\" suffix.\r\n```bash\r\n$ cat node_exporter-node_memory-valid.prom | promtool-2.34 check metrics --extended\r\nnode_memory_hugepages_total non-counter metrics should not have \"_total\" suffix\r\n```\r\n\r\nOnce that metric is updated to remove the \"_total\" suffix, the linting now passes and consequently we can finally view the cardinality analysis (output truncated):\r\n```bash\r\n$ cat node_exporter-node_memory-valid.prom | promtool-2.34 check metrics --extended\r\nMetric                                 Cardinality    Percentage\r\nnode_memory_hugepages_free             1              2.13%\r\nnode_memory_inactive_file_bytes        1              2.13%\r\nnode_memory_sunreclaim_bytes           1              2.13%\r\nnode_memory_swapcached_bytes           1              2.13%\r\nnode_memory_active_anon_bytes          1              2.13%\r\nnode_memory_bounce_bytes               1              2.13%\r\n```\r\n\r\nWhile I completely understand it's important to encourage users to follow best practices, it should be possible to obtain a cardinality analysis of series regardless of styling warnings.  I believe that notices such as snake case vs. camel case shouldn't cause the this command to [exit with an error](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/cmd\/promtool\/main.go#L714) if the metrics are still valid as per the [official metrics exposition format](https:\/\/github.com\/prometheus\/docs\/blob\/main\/content\/docs\/instrumenting\/exposition_formats.md).   What I find a bit more confusing is this promtool command enforces styling which even the node_exporter doesn't appear to follow.     The reality is that there are various exporters out there that may not present metrics in the most \"appropriate\" format although as long as prometheus continues to ingest them,  promtool should at least be consistent with what it accepts.\r\n\r\nAs a reasonable solution, I propose adding additional flags such as these ones:\r\n1. `--no-strict-linting`  :  Allow the check command to continue to the cardinality analysis as long as  the metrics are still valid with regards to the exposition format.\r\n2. `--show-linting-notices` : Specify whether or not to display linting notices.  In some cases, users may only be interested in seeing the cardinality analysis. \r\n\r\n\r\n\r\n### What did you expect to see?\r\n\r\n_No response_\r\n\r\n### What did you see instead? Under which circumstances?\r\n\r\nI would expect to view the cardinality analysis regardless of linting notices as long as the metrics naming convention is technically valid.\r\n\r\n### System information\r\n\r\n_No response_\r\n\r\n### Prometheus version\r\n\r\n```text\r\nVersion\t2.27.1\r\nRevision\tdb7f0bcec27bd8aeebad6b08ac849516efa9ae02\r\nBranch\tHEAD\r\nBuildUser\troot@fd804fbd4f25\r\nBuildDate\t20210518-14:17:54\r\nGoVersion\tgo1.16.4\r\n```\r\n\r\n\r\n### Prometheus configuration file\r\n\r\n_No response_\r\n\r\n### Alertmanager version\r\n\r\n_No response_\r\n\r\n### Alertmanager configuration file\r\n\r\n_No response_\r\n\r\n### Logs\r\n\r\n_No response_","comments":["~You should be able to use `--lint=none` to disable linting.~\r\n\r\nNever mind, that flag is for rule linting not metric linting.","The issue I see here is that the cardinality analysis functionality is merely a flag under `check metrics`, not a full sub-command[^1]. And given that the stated point of `check metrics` is to \"lint [metrics] for consistency and correctness\", it might be a little confusing to offer a flag disabling the linting. The solutions to this would be either modifying the documentation of the `check metrics` to be looser or spinning off the `--extended` flag into another subcommand. I could also be overthinking.\r\n\r\n[^1]: We originally wanted it to be a sub-command, but couldn't implement it that way due to library constraints.","Is there any possibility this could be addressed in an upcoming release or is it more realistic to just extract the relevant code and create an external tool to accomplish this functionality? ","This could definitely end up in an upcoming release if we reach a consensus on the best way forward. It shouldn't be too complicated to actually code.\r\n\r\nOpinions @roidelapluie or anyone else?","I ran into this problem as well today and I'd be in favor of adding a sub-command for checking the cardinality of metrics. I'd be up for creating a PR if there is a consensus on the best way forward (as you mentioned as well @LeviHarrison).\r\n\r\n**Example**\r\n```bash\r\n$ promtool check cardinality\r\n```\r\n\r\nWhat do you think?\r\n\r\n\/\/cc @roidelapluie "],"labels":["kind\/enhancement","component\/promtool","priority\/P3"]},{"title":"Create a major.minor tag for latest of each version (i.e. 2.35 tag to represent the latest 2.35.x)","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n\r\nThe proposal is to add an additional tag for versioning that allows developers consuming this docker image to specify a specific version in their configuration that will be updated with patches\/bug fixes but will not accidentally pull breaking changes with a new release. In other words, tag with major.minor and major.minor.build.\r\n\r\nCurrently, the only option is to specify the latest tag, which could introduce breaking changes.  Instead, if there was a tag 2.35 that was updated with each 2.35.x update, the developer could safely use the 2.35 tag.\r\n","comments":["Sure, we should have something like that but it does not work: https:\/\/github.com\/prometheus\/prometheus\/pull\/8026"],"labels":["kind\/enhancement","priority\/P3"]},{"title":"@prometheus-io\/codemirror-promql: When can we give an example of Vue\uff1f","body":null,"comments":["Well on my side I am not familiar with VueJS, but if you have any example showing an integration of this package in a Vue, then I will be happy to reference it :).","me too, expect vue","Thank you for your work!\r\nExpect vue","this is how i'm using codemirror-promql with vue composition\r\n```\r\n<template>\r\n    <div id=\"editor\" class=\"editor page-panel\" \/>\r\n<\/template>\r\n\r\n<script>\r\n\r\nimport { PromQLExtension } from 'codemirror-promql';\r\nimport { basicSetup } from '@codemirror\/basic-setup';\r\nimport { EditorState } from '@codemirror\/state';\r\nimport { EditorView, keymap } from '@codemirror\/view';\r\nimport { reactive, onMounted } from '@vue\/composition-api';\r\n\r\nexport default {\r\n    name: 'PromQLEditor',\r\n    setup() {\r\n        function enterKey() {\r\n            return keymap.of([{\r\n                key: 'Enter',\r\n                run() { console.log('this is to prevent a new line, and trigger search'); return true; },\r\n            }]);\r\n        }\r\n\r\n        onMounted(() => {\r\n            const data = reactive({\r\n                editorState: null,\r\n                editorView: null,\r\n            });\r\n            const promQL = new PromQLExtension();\r\n\r\n            data.editorState = EditorState.create({\r\n                extensions: [\r\n                    enterKey(),\r\n                    basicSetup,\r\n                    promQL.asExtension()\r\n                ]\r\n            });\r\n\r\n            data.editorView = new EditorView({\r\n                state: data.editorState,\r\n                parent: document.getElementById('editor'),\r\n            });\r\n        });\r\n\r\n        return {};\r\n    },\r\n};\r\n<\/script>\r\n\r\n<style lang=\"scss\" scoped>\r\n.editor {\r\n    height: 40px;\r\n    width: 100%;\r\n    margin: 0 25 10;\r\n}\r\n<\/style>\r\n```"],"labels":["help wanted","component\/ui"]},{"title":"Add recovery rules for alerting","body":"Hi.\r\nPleasy add recovery rule for allerts.\r\n\r\nexampl\r\ndisk < 10 %  - ALLER FIRE\r\nDISK > 30% - ALLERT OK\r\n\r\nIt is impotent for prevent multi firing allerting\r\n\r\nThanks you\r\n","comments":["This is a hysteresis style alert. We discussed it at the dev summit, see [meeting notes](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#heading=h.ro1r0q736gqw) (search for \"hysteresis\" to find the relevant section). tl;dr: We like it, and I think the notes even spell out an option how to implement this. Someone just has to do it.","Thear write - CONSENSUS: We want to enable this behind an experimental feature flag\r\n\r\nVery Whell !!!\r\n\r\nWait someone just to do it !!!","```\r\nRichiH: Hysteresis: Allow a secondary PromQL query to disable alerts\r\nCONSENSUS: We intend to support hysteresis based on both query and\/or cooldown time\r\n```","> ```\r\n> RichiH: Hysteresis: Allow a secondary PromQL query to disable alerts\r\n> CONSENSUS: We intend to support hysteresis based on both query and\/or cooldown time\r\n> ```\r\n\r\nHi, all\r\nIt is different cases\r\n\r\nOne - have Recovery PromQL query\r\n\r\nAnd any have freeze Allert at ON style.\r\n\r\nI think  that RichH right: \" Allow a secondary PromQL query to disable alerts\"","I believe my proposal in the doc is able to satisfy all the proposed use cases:\r\n\r\n> Model it that way: Every alert now has two queries and two \u201cfor\u201ds, one to switch it on, one to switch it off. If only one query is given, the same query is (logically) used for both. The default for the \u201coff-for\u201d is zero, same as we already have it for the \u201con-for\u201d\r\n> \r\n> - Alert starts to fire if both queries return elements with the same label set for the duration of \u201con-for\u201d.\r\n> - Alert stops firing if both queries do not return elements with the same label set for the duration of \u201coff-for\u201d.\r\n> - (And I think you could extend this to any number of queries, if that\u2019s what RichiH wanted\u2026)\r\n\r\nThat should be fairly easy to add, and then we can play with it and see if my believe is true. \r\n\r\n","> That should be fairly easy to add, and then we can play with it and see if my believe is true.\r\n\r\nOf course","> ```\r\n> RichiH: Hysteresis: Allow a secondary PromQL query to disable alerts\r\n> CONSENSUS: We intend to support hysteresis based on both query and\/or cooldown time\r\n> ```\r\n\r\nHi !\r\n\r\nWhat about hysteresis  now?","I have open a pull request: #11827 ","To illustrate the differences between the \"multi expression approach\" I have suggested above and the \"keep-firing-for approach\" implemented by #11827, I have created a little drawing:\r\n\r\n![PXL_20230117_135951901](https:\/\/user-images.githubusercontent.com\/5609886\/212919646-8ba76e22-a447-4327-85ce-875213fb5464.jpg)\r\n\r\nBox I shows the \"keep-firing-for approach\". Box II contains the \"multi expression approach\", in this case with two expressions, but it could be any number. The alert starts to fire once it is returned by all expressions alike, and it stops only after it is _not_ returned by _any_ expression anymore.\r\n\r\nI don't think that either of the two approaches is better or worse. But they do different things, and one might fit your actual needs better than the other.\r\n\r\nThere are three different scenarios in the graph:\r\n- The red one probably shows the most proverbial scenario: The CPU temperature reaches the alerting threshold (400K) and then hovers around it for a while until it drops for good. Both approaches deal with this equally well. Approach I stops firing 3m after the temperature has dropped below the threshold for good. Approach II stops firing immediately once the temperature has dropped below the 2nd threshold (375K). As I have drawn the graph, approach II stops firing earlier and therefore shows the faster (and thus \"better\") reset behavior. But if the temperature dropped more slowly, approach I could be faster.\r\n- The green line shows a single temperature peak. Arguably, approach II deals better with this, as it stops firing earlier, while approach I will always need the keep-firing-for time until \"realizing\" that the alerting condition is over, even if the temperature has already dropped dramatically.\r\n- The blue line shows an initial peak that only goes down slightly below the threshold and lingers there for a while. Now approach I stops firing earlier. But is that \"better\"? It depends on your idea of the alerting condition. Is it fine if the temperature lingers for quite a while just below the alerting threshold? Or is this still a condition that justifies investigation or even action?\r\n\r\nSo the answer is that it really depends.\r\n\r\nMaybe it is even worth combining both approaches by not just allowing multiple expressions, but multiple tuples of expr\/for\/keep_firing_for. For example:\r\n\r\n```yaml\r\n- alert: CPUTemperatureTooHigh\r\n  conditions:\r\n    - expr: cpu_temp_kelvin > 400\r\n      keep_firing_for: 3m\r\n    - expr: cpu_temp_kelvin > 375\r\n  labels:\r\n    severity: warning\r\n```\r\n\r\nThis alert would start firing once the CPU temperature reaches 400K and only stop once the temperature is below 375K _and_ has been below 400K for at least 3m.\r\n\r\nThis could be a non-breaking change by still allowing top-level one of each top level keys `expr`\/`for`\/`keep_firing`  (which would constitute one of the conditions).","> This is a hysteresis style alert. We discussed it at the dev summit, see [meeting notes](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#heading=h.ro1r0q736gqw) (search for \"hysteresis\" to find the relevant section). tl;dr: We like it, and I think the notes even spell out an option how to implement this. Someone just has to do it.\r\n\r\nI reviewed the notes from [meeting notes](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1#heading=h.ro1r0q736gqw) and found the proposed implementation complex and error-prone from the user perspective. In my opinion, the weak points of the proposed implementation are the following:\r\n* Using two queries increases the load on the database;\r\n* Both queries should produce the same label sets, which may not be trivial for users to perceive and configure correctly;\r\n* There is no simple way to validate whether both queries are correct, so the alert won't remain in the FIRING state forever.\r\n\r\nAs an alternative, I propose the approach discussed [here](https:\/\/github.com\/VictoriaMetrics\/VictoriaMetrics\/issues\/2857#issuecomment-1210755685). The idea is to have an ability for alerting engine to compare values received in results. When we have this, the alerting rule configuration can be extended with an extra piece of config:\r\n```\r\n      - alert: TemperatureTooHigh\r\n        expr: my_temperature_metric > 75 # filter out series with temp < 75\r\n        thresholds:\r\n          trigger: gt:80 # fire if my_temperature_metric is > 80\r\n          resolve: lt:75 # resolve if my_temperature_metric < 75\r\n```\r\n\r\nPros of such an approach:\r\n* No need for extra query;\r\n* Simpler to configure and maintain, since the rule definition has all the needed information;\r\n* Rule can be validated before using: simple checking for `trigger` and `resolve` boundaries can exclude misconfiguration.\r\n\r\nCons of such an approach:\r\n* Trigger and resolve conditions are bound to the same query. It makes it simple, but not as flexible as the proposed approach by Prometheus maintainers.\r\n\r\nI'm keen to discuss this approach in more detail and would like to contribute its implementation to Prometheus if it is acceptable.\r\n","> In my opinion, the weak points of the proposed implementation are the following:\r\n\r\nI don't think these points are very relevant.\r\n\r\n> * Using two queries increases the load on the database;\r\n\r\nAlerting rules are usually just a tiny fraction of the query load. They tend to be simple. And should they ever be expensive to evaluate, it's easy to put the expensive part into a separate recording rule.\r\n\r\nFurthermore, I don't expect that hysteresis alerts will be common. We are indeed talking about optimizing query load that's inherently just a tiny fraction of a tiny fraction.\r\n\r\n> * Both queries should produce the same label sets, which may not be trivial for users to perceive and configure correctly;\r\n\r\nIt will be trivial in simple cases. It won't be trivial in complex cases, but the fact that my proposal even supports complex cases is a pro, not a con.\r\n\r\nIn general, I would expect users that feel the need for hysteresis alerts to be in a situation where they are likely to have quite complex requirements anyway. I think we end up in a \"worst of both worlds\" situation if we try to satisfy a power user feature request with a solution that is limited to simple use cases.\r\n\r\nWhich is not to say that my proposal isn't easy and elegant in easy cases, see example below.\r\n\r\n> * There is no simple way to validate whether both queries are correct, so the alert won't remain in the FIRING state forever.\r\n\r\nAt least then you will notice. :o)\r\n\r\nBut seriously, I'm not sure I buy this. You can simply try out the query in the expression browser. (There is a more general issue that Prometheus currently doesn't make it easy to retroactively try out alerts on historical data, but the problem here is really the `FOR` clause. Cf. https:\/\/github.com\/prometheus\/prometheus\/pull\/4277 .)\r\n\r\nWhat rubs me the wrong way about @hagen1778's proposal:\r\n- It re-implements parts of PromQL in YAML. In other words: Instead of formulating thresholds in PromQL, now we have to use a makeshift tiny expression language and fill them into a number of newly defined configuration keys.\r\n- But it only re-implements a tiny part of PromQL. In other words: It ignores that Prometheus alerts don't have to be threshold based.\r\n\r\nAs it crosses the formerly well defined domains of responsibility between PromQL and YAML, it convolutes things, and makes them complicated and harder to maintain (to directly contradict one of the Pros stated above).\r\n\r\nHead to head comparison (note that the first example isn't only more verbose, it also needs many new elements in our implicit \"configuration language\", like `trigger`, `resolve`, `gt`, `lt`, and presumably `ge`, `le`, and maybe more, while the second example adds just one more thing, namely multiple expressions after `exprs`):\r\n\r\n```yaml\r\n      - alert: TemperatureTooHigh\r\n        expr: my_temperature_metric > 75 # filter out series with temp < 75\r\n        thresholds:\r\n          trigger: gt:80 # fire if my_temperature_metric is > 80\r\n          resolve: lt:75 # resolve if my_temperature_metric < 75\r\n```\r\n\r\n```yaml\r\n      - alert: TemperatureTooHigh\r\n        exprs:\r\n          - my_temperature_metric > 80 # Alert fires here.\r\n          - my_temperature_metric > 75 # And will continue to fire until this isn't true anymore.\r\n```\r\n\r\n\r\n","> Alerting rules are usually just a tiny fraction of the query load. They tend to be simple. And should they ever be expensive to evaluate, it's easy to put the expensive part into a separate recording rule.\r\n\r\nI agree it should be so. But it doesn't mean it is so. Once fast and inexpensive, alerting rule can quickly become more expensive with workload growth. Especially for k8s. So we at least should try to minimize the penalty of using alerts with hysteresis conditions.\r\n\r\n> But seriously, I'm not sure I buy this. You can simply try out the query in the expression browser. \r\n\r\nI'm not so optimistic here. You now need to try two expressions in browser and correlate the output to understand whether alert should or shouldn't trigger. \r\n\r\nThe ability to debug alerting rules is what concerns me the most. It wasn't always easy with one expression. And with two expressions situation may become much worse. Imagine that both expressions return labelsets that are different by one label, which is a common situation when `without` is used.\r\n\r\n> As it crosses the formerly well defined domains of responsibility between PromQL and YAML, it convolutes things, and makes them complicated and harder to maintain (to directly contradict one of the Pros stated above).\r\n\r\nThis is something I'm not aware of. But if there is such a rule and it pays it off - I understand.\r\nFrom my perspective, having a general list of comparison operations (which is pretty limited) is quite easy to maintain. Especially, when we know exactly that there will be only one value of type `float64` per-series. \r\n\r\nBut again, I intended to have a concept simple for using and understanding for end users. Reducing the possibility to misconfigure such important thing as alerting is what matters, in my opinion.\r\n\r\n> Head to head comparison\r\n\r\nThe comparison would quickly become irrelevant if we used more complex queries. ","With my proposal, you have to understand (and debug) two expressions, both written in PromQL.\r\n\r\nWith your proposal, you have to understand (and debug) three expressions written in two different expression languages.\r\n\r\n> The comparison would quickly become irrelevant if we used more complex queries.\r\n\r\nOn the contrary, I believe the comparison would be even more striking.\r\n\r\nWe really seem to think in completely different patterns here, arriving at opposing conclusion when following the same line of argumant.\r\n\r\nI guess we put this as an item on the dev-summit agenda to make a call.","Really appreciate your time reviewing this!"],"labels":["priority\/Pmaybe","component\/rules","kind\/feature"]},{"title":"Sudden unexplained drop in scrape_samples_scraped within every 24 hours","body":"What did you do?\r\n\r\n--> I have 4 services running in my docker stack, namely, Prometheus, Promscale, Postgres\/Timescaledb and Grafana to monitor my django application. Currently I am working on testing APIs RPM and latency metrics, and accordingly I have a \"api_latency_bucket\" histogram and \"api_hit_rate\" counter metrics. Both metrics have a label called \"apis\" and values are the api name in my code. (Example: api_hit_rate.label(func_name).inc())\r\n\r\nWhat did you expect to see?\r\n\r\n--> I expected too see my API RPM and compare it with my elastic apm that I previously have been using. And if they show different metrics trend, I shall use logger info to resolve who shows the truth.\r\n\r\nWhat did you see instead? Under which circumstances?\r\n\r\n-->  I noticed that API RPM get sudden dips and it deviates from what my elastic apm shows. So I decided to check the samples scraped at the time of dip and it matched. Whenever there is a sudden drop in samples scraped, there is drop in my metrics too.\r\n\r\nEnvironment\r\n\r\nDocker-compose:\r\n\r\nversion: \u201c3.3\u201d\r\n\r\nservices:\r\n\r\npostgres-db:\r\nimage: [Package dev_promscale_extension \u00b7 GitHub](http:\/\/ghcr.io\/timescale\/dev_promscale_extension:develop-ts2-pg14)\r\nports:\r\n- 5432:5432\/tcp\r\nnetworks:\r\n- rexnet\r\nenvironment:\r\n- POSTGRES_PASSWORD=***\r\n- POSTGRES_USER=***\r\n- POSTGRES_DB=***\r\ndeploy:\r\nplacement:\r\nconstraints: [node.labels.trex.type ==mon]\r\n\r\npromscale:\r\nimage: timescale\/promscale:latest\r\nports:\r\n- 9201:9201\/tcp\r\n- 9202:9202\/tcp\r\ndepends_on:\r\n- postgres-db\r\n- prometheus\r\nnetworks:\r\n- rexnet\r\nenvironment:\r\n- PROMSCALE_DB_CONNECT_RETRIES=10\r\n- PROMSCALE_DB_HOST=postgres-db\r\n- PROMSCALE_DB_PORT=5432\r\n- PROMSCALE_DB_NAME=***\r\n- PROMSCALE_DB_SSL_MODE=allow\r\n- PROMSCALE_DB_USER=***\r\n- PROMSCALE_DB_PASSWORD=***\r\n\r\ndeploy:\r\n  placement:\r\n    constraints: [node.labels.trex.type ==mon]\r\nprometheus:\r\nimage: prom\/prometheus:main\r\nuser: ***\r\nports:\r\n- 9090:9090\r\nnetworks:\r\n- rexnet\r\ncommand: --web.enable-lifecycle --config.file=\/etc\/prometheus\/prometheus.yml\r\nvolumes:\r\n- .\/prometheus:\/etc\/prometheus\r\n- prometheus-data:\/prometheus\r\n- \/var\/run\/docker.sock:\/var\/run\/docker.sock:ro\r\ndeploy:\r\nplacement:\r\nconstraints: [node.labels.trex.type ==mon]\r\n\r\ngrafana:\r\nimage: grafana\/grafana:7.5.7\r\nports:\r\n- 3000:3000\r\nnetworks:\r\n- rexnet\r\nrestart: unless-stopped\r\nvolumes:\r\n- .\/grafana:\/etc\/grafana\/provisioning\/datasources\r\n- grafana-data:\/var\/lib\/grafana\r\ndeploy:\r\nplacement:\r\nconstraints: [node.labels.trex.type == mon]\r\n\r\nnetworks:\r\nrexnet:\r\ndriver: overlay\r\nattachable: true\r\nexternal:\r\nname: rexnet\r\n\r\nvolumes:\r\nprometheus-data:\r\ngrafana-data:\r\n\r\n\r\nPrometheus.yml:\r\n\r\n-->\r\nglobal:\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  evaluation_interval: 1m\r\nscrape_configs:\r\n- job_name: **\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - **:***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - lobby-***:***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***:***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_dockerswarm_network_name]\r\n    separator: ;\r\n    regex: rexnet\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_dockerswarm_service_label_prometheus_cashtable_job]\r\n    separator: ;\r\n    regex: .+\r\n    replacement: $1\r\n    action: keep\r\n  dockerswarm_sd_configs:\r\n  - follow_redirects: true\r\n    host: unix:\/\/\/var\/run\/docker.sock\r\n    role: tasks\r\n    port: ***\r\n    filters: []\r\n    refresh_interval: 1m\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_dockerswarm_network_name]\r\n    separator: ;\r\n    regex: rexnet\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [***]\r\n    separator: ;\r\n    regex: .+\r\n    replacement: $1\r\n    action: keep\r\n  dockerswarm_sd_configs:\r\n  - follow_redirects: true\r\n    host: unix:\/\/\/var\/run\/docker.sock\r\n    role: tasks\r\n    port: ***\r\n    filters: []\r\n    refresh_interval: 1m\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_dockerswarm_network_name]\r\n    separator: ;\r\n    regex: rexnet\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [****]\r\n    separator: ;\r\n    regex: .+\r\n    replacement: $1\r\n    action: keep\r\n  dockerswarm_sd_configs:\r\n  - follow_redirects: true\r\n    host: unix:\/\/\/var\/run\/docker.sock\r\n    role: tasks\r\n    port: ***\r\n    filters: []\r\n    refresh_interval: 1m\r\n- job_name: ***\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_dockerswarm_network_name]\r\n    separator: ;\r\n    regex: rexnet\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [***]\r\n    separator: ;\r\n    regex: .+\r\n    replacement: $1\r\n    action: keep\r\n  dockerswarm_sd_configs:\r\n  - follow_redirects: true\r\n    host: unix:\/\/\/var\/run\/docker.sock\r\n    role: tasks\r\n    port: ***\r\n    filters: []\r\n    refresh_interval: 1m\r\n**_- job_name: monitor-django\r\n  honor_timestamps: true\r\n  scrape_interval: 10s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  authorization:\r\n    type: HTTP_X_METRICS_AUTH\r\n    credentials: <secret>\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - ***_**\r\nremote_write:\r\n- url: http:\/\/promscale:9201\/write\r\n  remote_timeout: 30s\r\n  follow_redirects: true\r\n  queue_config:\r\n    capacity: 2500\r\n    max_shards: 200\r\n    min_shards: 1\r\n    max_samples_per_send: 500\r\n    batch_send_deadline: 5s\r\n    min_backoff: 30ms\r\n    max_backoff: 5s\r\n  metadata_config:\r\n    send: true\r\n    send_interval: 1m\r\n    max_samples_per_send: 500\r\n\r\nThe last job named \"monitor-django\" is the one that I am having problem in.\r\n\r\nHere is the image of samples scraped graph:\r\n\r\n\r\n![](https:\/\/aws1.discourse-cdn.com\/standard20\/uploads\/prometheus\/original\/1X\/450e0c8a7aeb098c884d61b52af607b739be581a.jpeg)\r\n\r\n\r\nI am unable to figure out where the problem lies in prometheus server.\r\n\r\n","comments":["You could look at the missing metrics at that time. is your application restarting?"],"labels":["kind\/more-info-needed"]},{"title":"Remote-Read Performance","body":"**What did you do?**\r\nWe have a promethues instance that monitors 3,500 nodes, and provides global queries through the thanos sidecar mode. During use, we found that the query time of the same PromQL in thanos and prometheus differs by 2 times or more. Further monitoring found that \/api\/v1\/read of prometheus has relatively large time delay\r\n\r\n**What did you expect to see?**\r\nI would like to be able to improve remote read performance and improve query response time\r\n\r\n**Environment:**\r\nSystem information:\r\nLinux 3.10.0-514.el7.x86_64\r\n\r\nPrometheus version:\r\nprometheus, version 2.33.1 (branch: HEAD, revision: 4e08110891fd5177f9174c4179bc38d789985a13)\r\n  build user:       root@37fc1ebac798\r\n  build date:       20220202-15:23:18\r\n  go version:       go1.17.6\r\n  platform:         linux\/amd64\r\n\r\nThe node has 16 CPUs 64GB memroy 4TB SSD, CPU and Mem usage:\r\n<img width=\"1358\" alt=\"2022-03-17 13 42 19\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756389-e583d52b-83aa-4b90-befd-6c4a5c1ee6de.png\">\r\n\r\n4million series :\r\n<img width=\"1355\" alt=\"2022-03-17 13 14 47\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756554-990971da-6b34-4c88-a2bb-c3c8533f3fd8.png\">\r\n\r\nResponse time, P99 > 2s:\r\n<img width=\"1361\" alt=\"2022-03-17 13 13 41\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756611-fbe52466-42ca-4470-a650-90dd6269dba3.png\">\r\n\r\nprometheus pprof profile:\r\n<img width=\"1431\" alt=\"2022-03-17 15 20 06\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158757139-5afcdc9e-a815-454a-bb4d-f29bbd26e563.png\">\r\n\r\npprof file:\r\n[prometheus-profile.pprof.zip](https:\/\/github.com\/prometheus\/prometheus\/files\/8282284\/prometheus-profile.pprof.zip)\r\n","comments":["Can we get more details into the query and the version of thanos you are comparing to? it is not really a surprise that thanos is doing better.","Thanos version: 0.25.0\r\n  I don't know what more information to provide \ud83d\ude41\r\n\r\nPreviously all record rules were worked by thanos rule, I estimate that the series queried by several record rules have high cardinality, which is inefficient when going through the \/api\/v1\/read interface, for example: `sum (rate(node_cpu_seconds_total{mode!=\" idle\"}[1m])) without(cpu,mode)`\r\nTherefore, I configured some high cardinality record rules in prometheus, then deleted the corresponding rules in thanos rule, and finally deleted some unused labels using relabeling in prometheus.\r\nThe adjusted result shows that the P99 latency is reduced by 100%\r\n\r\n<img width=\"1373\" alt=\"\u622a\u5c4f2022-06-09 20 05 12\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/172845267-6acc0884-2744-4fd7-86f7-d33a7fe6d12e.png\">\r\n\r\n","hello MR.wu,I'm also worried about monitoring Prometheus. Then I see that most of the indicators of Prometheus are provided on your grafana monitoring panel. Can you provide your grafana panel for me to use for reference? be deeply grateful","> **What did you do?** We have a promethues instance that monitors 3,500 nodes, and provides global queries through the thanos sidecar mode. During use, we found that the query time of the same PromQL in thanos and prometheus differs by 2 times or more. Further monitoring found that \/api\/v1\/read of prometheus has relatively large time delay\r\n> \r\n> **What did you expect to see?** I would like to be able to improve remote read performance and improve query response time\r\n> \r\n> **Environment:** System information: Linux 3.10.0-514.el7.x86_64\r\n> \r\n> Prometheus version: prometheus, version 2.33.1 (branch: HEAD, revision: [4e08110](https:\/\/github.com\/prometheus\/prometheus\/commit\/4e08110891fd5177f9174c4179bc38d789985a13)) build user: root@37fc1ebac798 build date: 20220202-15:23:18 go version: go1.17.6 platform: linux\/amd64\r\n> \r\n> The node has 16 CPUs 64GB memroy 4TB SSD, CPU and Mem usage: <img alt=\"2022-03-17 13 42 19\" width=\"1358\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756389-e583d52b-83aa-4b90-befd-6c4a5c1ee6de.png\">\r\n> \r\n> 4million series : <img alt=\"2022-03-17 13 14 47\" width=\"1355\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756554-990971da-6b34-4c88-a2bb-c3c8533f3fd8.png\">\r\n> \r\n> Response time, P99 > 2s: <img alt=\"2022-03-17 13 13 41\" width=\"1361\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158756611-fbe52466-42ca-4470-a650-90dd6269dba3.png\">\r\n> \r\n> prometheus pprof profile: <img alt=\"2022-03-17 15 20 06\" width=\"1431\" src=\"https:\/\/user-images.githubusercontent.com\/3648074\/158757139-5afcdc9e-a815-454a-bb4d-f29bbd26e563.png\">\r\n> \r\n> pprof file: [prometheus-profile.pprof.zip](https:\/\/github.com\/prometheus\/prometheus\/files\/8282284\/prometheus-profile.pprof.zip)\r\n\r\nhello MR.wu,I'm also worried about monitoring Prometheus. Then I see that most of the indicators of Prometheus are provided on your grafana monitoring panel. Can you provide your grafana panel for me to use for reference? be deeply grateful"],"labels":["priority\/Pmaybe","component\/remote storage","kind\/more-info-needed"]},{"title":"UI: Filter in service discovery page has a weird behaviour","body":"**What did you do?**\r\n\r\nWe were trying to filter some targets in the service discovery page, when we found that using a pattern shouldn't match anything, is still returning\/displaying the dropped target\r\n\r\n<img width=\"565\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/4548045\/158649646-1d1ac207-e4d9-4527-8736-eb8d6bdbeb7c.png\">\r\n\r\n**What did you expect to see?**\r\n\r\nWhen the pattern doesn't match any target, then it shouldn't display anything\r\n\r\n\r\n**Environment**\r\n\r\n* Prometheus version:\r\n\r\n\tv2.33.5\r\n\r\n\/cc @AntoineThebaud\r\n","comments":["Hmmm, it's unclear what the expected behavior *should* be. I can well imagine that for some people it might be useful to search dropped ones as well, especially if they want to confirm whether they existed at all before dropping if they are unexpectedly missing.","That's actually the point of this proposal @juliusv, today you can not easily search among dropped targets because the filter is ignored, so when you have thousands of dropped targets like in the example above & pagination applying, it's really difficult to narrow down to a specific dropped target.","Ah I misunderstood. Sounds good!","#10668 partially fixed the issue, but it is still present :\/"],"labels":["kind\/bug","component\/ui"]},{"title":"Feature request: set target limits via service discovery labels","body":"## Proposal\r\nTL;DR: User should be able to set various scrape-related limits (`body_size_limit`, `sample_limit`, `label_limit`, etc) for individual targets based on its discovered labels.\r\n\r\n## Use case\r\nConsider a typical prometheus config:\r\n```\r\nscrape_configs:\r\n- job_name: all-the-targets\r\n  sample_limit: 100\r\n  <some_discovery_config>\r\n```\r\nThis is perfectly readable and works just fine up until some special important target appears. This target contains more than 100 important metrics, and setting different `sample_limit` is a necessity. However, you have an understanding that there is still a reasonable upper limit. The config grows:\r\n```\r\nscrape_configs:\r\n- job_name: all-the-targets-but-target1\r\n  sample_limit: 100\r\n  relabel_config:\r\n  - action: drop\r\n    source_labels: [__sd_target_name]\r\n    regex: target1\r\n  <some_discovery_config>\r\n- job_name: target1\r\n  sample_limit: 200\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_name]\r\n    regex: target1\r\n  <some_discovery_config>\r\n```\r\nNot so pretty, but still readable. Time passes, and important endpoint appears. It contains even more metrics. Something like this happens:\r\n```\r\nscrape_configs:\r\n- job_name: all-the-targets-but-target1-or-target2\r\n  sample_limit: 100\r\n  relabel_config:\r\n  - action: drop\r\n    source_labels: [__sd_target_name]\r\n    regex: target1|target2\r\n  <some_discovery_config>\r\n- job_name: target1\r\n  sample_limit: 200\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_name]\r\n    regex: target1\r\n- job_name: target2\r\n  sample_limit: 10000\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_name]\r\n    regex: target2\r\n  <some_discovery_config>\r\n```\r\n\r\nThen prometheus 2.27 comes along, and prometheus admin tries to enforce `label_value_length_limit`. However, there are several endpoints that already have weirdly long labels. They should be allowed to function, but no new endpoints should be allowed to behave that way. This is when hell brakes loose:\r\n```\r\nscrape_configs:\r\n- job_name: all-the-targets-but-target1-or-target2-and-longlabeltargets\r\n  sample_limit: 100\r\n  label_value_length_limit: 20\r\n  relabel_config:\r\n  - action: drop\r\n    source_labels: [__sd_target_name]\r\n    regex: target1|target2\r\n  - action: drop\r\n    source_labels: [__sd_target_has_long_labels]\r\n    regex: true\r\n  <some_discovery_config>\r\n- job_name: longlabeltargets\r\n  sample_limit: 100\r\n  label_value_length_limit: 100\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_has_long_labels]\r\n    regex: true\r\n  <some_discovery_config>\r\n- job_name: target1\r\n  sample_limit: 200\r\n  label_value_length_limit: 20\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_name]\r\n    regex: target1\r\n- job_name: target2\r\n  sample_limit: 10000\r\n  label_value_length_limit: 20\r\n  relabel_config:\r\n  - action: keep\r\n    source_labels: [__sd_target_name]\r\n    regex: target2\r\n  <some_discovery_config>\r\n```\r\npod_name is used just as an example. various labels would be used in production system, making changes more and more difficult with every change. At some point operator forgets to set some new limit. Those are details. The essence is: config becomes complicated, hard to read and to maintain.\r\n\r\nAdjusting some per-target limits via labels will make config dramatically simpler and easier to maintain:\r\n```\r\nscrape_configs:\r\n- job_name: all-the-targets\r\n  sample_limit: 10\r\n  label_value_length_limit: 100\r\n  <some_discovery_config>\r\n```\r\nAnd some target-specific setting will be set via its labels:\r\n```\r\n[\r\n  {\"targets\": [ \"target1\"], \"labels\": {\"__sample_limit__\": \"200\"}},\r\n  {\"targets\": [ \"target2\"], \"labels\": {\"__sample_limit__\": \"10000\"}},\r\n  {\"targets\": [ \"longlabeltarget1\"], \"labels\": {\"__label_value_length_limit__\": \": 100\"}},\r\n  {\"targets\": [ \"defaultlimitstarget1\"], \"labels\": {}}\r\n  {\"targets\": [ \"defaultlimitstarget2\"], \"labels\": {}}\r\n ]\r\n```\r\n\r\nAdding a new limit globally will be as easy as it is supposed to be - just add it to a single scrape_config. Adding exceptions to those default limits would only require setting those labels via SD, which could be done without redeploying new prometheus config.\r\n\r\n**Similar feature is already present for `interval` and `timeout`:** https:\/\/github.com\/prometheus\/prometheus\/issues\/8895\r\nI believe this can be done for other scraper \/ scrape loop limits and `honor*` settings as well: https:\/\/github.com\/prometheus\/prometheus\/blob\/38ef68e27f565b4183b98d5e080076870f99ac65\/scrape\/scrape.go#L540-L550","comments":["A WIP PR to show this is feasible: https:\/\/github.com\/prometheus\/prometheus\/pull\/10432"],"labels":["component\/scraping","kind\/feature"]},{"title":"Prometheus agent mode using more heap memory than regular mode.","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nConfigured the prometheus in agent mode.\r\n**What did you expect to see?**\r\nExpect to see less memory foot print compared to regular mode.\r\n**What did you see instead? Under which circumstances?**\r\nRather we see agent mode using more memory than that of regular mode. After further debugging with go pprof we see remote.processExternalLabels is using more than 50% of the heap, which is not the case in regular mode.\r\n```\r\n\r\n(pprof) top\r\nShowing nodes accounting for 776.43MB, 91.73% of 846.41MB total\r\nDropped 200 nodes (cum <= 4.23MB)\r\nShowing top 10 nodes out of 45\r\n      flat  flat%   sum%        cum   cum%\r\n  428.31MB 50.60% 50.60%   428.31MB 50.60%  github.com\/prometheus\/prometheus\/storage\/remote.processExternalLabels\r\n  110.50MB 13.06% 63.66%   539.43MB 63.73%  github.com\/prometheus\/prometheus\/storage\/remote.(*QueueManager).StoreSeries\r\n   76.06MB  8.99% 72.65%    76.06MB  8.99%  github.com\/prometheus\/prometheus\/model\/labels.(*Builder).Labels\r\n   53.34MB  6.30% 78.95%    53.34MB  6.30%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n   38.77MB  4.58% 83.53%    38.77MB  4.58%  github.com\/prometheus\/prometheus\/tsdb\/agent.(*DB).gc\r\n   29.01MB  3.43% 86.95%    29.01MB  3.43%  github.com\/prometheus\/prometheus\/model\/textparse.(*PromParser).Metric\r\n   10.73MB  1.27% 88.22%    10.73MB  1.27%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).trackStaleness\r\n   10.46MB  1.24% 89.46%    10.46MB  1.24%  github.com\/golang\/snappy.Encode\r\n   10.25MB  1.21% 90.67%    10.25MB  1.21%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).addRef\r\n       9MB  1.06% 91.73%        9MB  1.06%  github.com\/prometheus\/prometheus\/tsdb\/agent.seriesHashmap.Set\r\n\r\n```\r\nBelow the heap foot print of the same for non agent mode for the exact same environment.\r\n```\r\n(pprof) top\r\nShowing nodes accounting for 403.64MB, 74.21% of 543.94MB total\r\nDropped 149 nodes (cum <= 2.72MB)\r\nShowing top 10 nodes out of 94\r\n      flat  flat%   sum%        cum   cum%\r\n  112.58MB 20.70% 20.70%   112.58MB 20.70%  github.com\/prometheus\/prometheus\/pkg\/labels.(*Builder).Labels\r\n   83.06MB 15.27% 35.97%    83.06MB 15.27%  github.com\/prometheus\/prometheus\/storage\/remote.processExternalLabels\r\n   42.01MB  7.72% 43.69%    42.01MB  7.72%  github.com\/prometheus\/prometheus\/pkg\/textparse.(*PromParser).Metric\r\n   37.22MB  6.84% 50.53%    37.22MB  6.84%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n   35.51MB  6.53% 57.06%    51.01MB  9.38%  github.com\/prometheus\/prometheus\/tsdb.newMemSeries\r\n   30.52MB  5.61% 62.67%    30.52MB  5.61%  github.com\/prometheus\/prometheus\/tsdb\/chunkenc.(*bstream).writeByte\r\n   16.24MB  2.99% 65.66%    16.24MB  2.99%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).trackStaleness\r\n   15.50MB  2.85% 68.51%    15.50MB  2.85%  github.com\/prometheus\/prometheus\/tsdb\/chunkenc.NewXORChunk\r\n   15.50MB  2.85% 71.36%    15.50MB  2.85%  github.com\/prometheus\/prometheus\/tsdb.newTxRing (inline)\r\n   15.50MB  2.85% 74.21%    22.02MB  4.05%  github.com\/prometheus\/prometheus\/tsdb.(*memSeries).mmapCurrentHeadChunk\r\n```\r\n**Environment**\r\nKubernetes\r\n* System information:\r\nLinux 3.10.0-1160.53.1.el7.x86_64 x86_64\r\n\r\n* Prometheus version:\r\nprometheus, version 2.33.4 (branch: HEAD, revision: 83032011a5d3e6102624fe58241a374a7201fee8)\r\n  build user:       root@d13bf69e7be8\r\n  build date:       20220222-16:51:28\r\n  go version:       go1.17.7\r\n  platform:         linux\/amd64\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\nglobal:\r\n  evaluation_interval: 1m\r\n  external_labels:\r\n    cluster_name: 172.16.20.154\r\n    monitor: prometheus\r\n  scrape_interval: 1m\r\n  scrape_timeout: 10s\r\nremote_write:\r\n- url: http:\/\/thanos-receive:19291\/api\/v1\/receive\r\nscrape_configs:\r\n- job_name: prometheus\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: external\r\n    source_labels:\r\n    - __meta_kubernetes_pod_annotation_prometheus\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_pod_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: kubernetes_pod_name\r\n  - action: keep\r\n    regex: 9090\r\n    source_labels:\r\n    - __meta_kubernetes_pod_container_port_number\r\n  scrape_interval: 10s\r\n- honor_labels: true\r\n  job_name: kafka-metrics\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: broker\r\n    source_labels:\r\n    - __meta_kubernetes_pod_annotation_prometheus\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_pod_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: kubernetes_pod_name\r\n  - action: keep\r\n    regex: 5556\r\n    source_labels:\r\n    - __meta_kubernetes_pod_container_port_number\r\n  scrape_interval: 30s\r\n  scrape_timeout: 30s\r\n- honor_labels: true\r\n  job_name: linstor-metrics\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: linstor\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n  - action: keep\r\n    regex: .*-op-cs\r\n    source_labels:\r\n    - __meta_kubernetes_endpoints_name\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_pod_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: kubernetes_pod_name\r\n  scrape_interval: 30s\r\n  scrape_timeout: 30s\r\n- honor_labels: true\r\n  job_name: drbd-metrics\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: linstor\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n  - action: keep\r\n    regex: .*-op-ns-monitoring\r\n    source_labels:\r\n    - __meta_kubernetes_endpoints_name\r\n  - action: keep\r\n    regex: prometheus\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_pod_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: kubernetes_pod_name\r\n  scrape_interval: 30s\r\n  scrape_timeout: 30s\r\n- bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  job_name: kubernetes-apiservers\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: default;kubernetes;https\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    - __meta_kubernetes_service_name\r\n    - __meta_kubernetes_endpoint_port_name\r\n  scheme: https\r\n  tls_config:\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n    insecure_skip_verify: true\r\n- bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  job_name: kubernetes-nodes\r\n  kubernetes_sd_configs:\r\n  - role: node\r\n  relabel_configs:\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_node_label_(.+)\r\n  - replacement: kubernetes.default.svc:443\r\n    target_label: __address__\r\n  - regex: (.+)\r\n    replacement: \/api\/v1\/nodes\/$1\/proxy\/metrics\r\n    source_labels:\r\n    - __meta_kubernetes_node_name\r\n    target_label: __metrics_path__\r\n  scheme: https\r\n  tls_config:\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n    insecure_skip_verify: true\r\n- bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  job_name: kubernetes-nodes-cadvisor\r\n  kubernetes_sd_configs:\r\n  - role: node\r\n  relabel_configs:\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_node_label_(.+)\r\n  - replacement: kubernetes.default.svc:443\r\n    target_label: __address__\r\n  - regex: (.+)\r\n    replacement: \/api\/v1\/nodes\/$1\/proxy\/metrics\/cadvisor\r\n    source_labels:\r\n    - __meta_kubernetes_node_name\r\n    target_label: __metrics_path__\r\n  scheme: https\r\n  tls_config:\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n    insecure_skip_verify: true\r\n- job_name: kubernetes-service-endpoints\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: true\r\n    source_labels:\r\n    - __meta_kubernetes_service_annotation_prometheus_io_scrape\r\n  - action: replace\r\n    regex: (https?)\r\n    source_labels:\r\n    - __meta_kubernetes_service_annotation_prometheus_io_scheme\r\n    target_label: __scheme__\r\n  - action: replace\r\n    regex: (.+)\r\n    source_labels:\r\n    - __meta_kubernetes_service_annotation_prometheus_io_path\r\n    target_label: __metrics_path__\r\n  - action: replace\r\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n    replacement: $1:$2\r\n    source_labels:\r\n    - __address__\r\n    - __meta_kubernetes_service_annotation_prometheus_io_port\r\n    target_label: __address__\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_service_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: kubernetes_name\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_node_name\r\n    target_label: kubernetes_node\r\n- honor_labels: true\r\n  job_name: prometheus-pushgateway\r\n  kubernetes_sd_configs:\r\n  - role: service\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: pushgateway\r\n    source_labels:\r\n    - __meta_kubernetes_service_annotation_prometheus_io_probe\r\n- job_name: kubernetes-services\r\n  kubernetes_sd_configs:\r\n  - role: service\r\n  metrics_path: \/probe\r\n  params:\r\n    module:\r\n    - http_2xx\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: true\r\n    source_labels:\r\n    - __meta_kubernetes_service_annotation_prometheus_io_probe\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __param_target\r\n  - replacement: blackbox\r\n    target_label: __address__\r\n  - source_labels:\r\n    - __param_target\r\n    target_label: instance\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_service_label_(.+)\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: kubernetes_name\r\n- job_name: kubernetes-pods\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n  relabel_configs:\r\n  - action: keep\r\n    regex: true\r\n    source_labels:\r\n    - __meta_kubernetes_pod_annotation_prometheus_io_scrape\r\n  - action: replace\r\n    regex: (.+)\r\n    source_labels:\r\n    - __meta_kubernetes_pod_annotation_prometheus_io_path\r\n    target_label: __metrics_path__\r\n  - action: replace\r\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n    replacement: $1:$2\r\n    source_labels:\r\n    - __address__\r\n    - __meta_kubernetes_pod_annotation_prometheus_io_port\r\n    target_label: __address__\r\n  - action: labelmap\r\n    regex: __meta_kubernetes_pod_label_(.+)\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: kubernetes_namespace\r\n  - action: replace\r\n    source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: kubernetes_pod_name\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["cc @rfratto ","Hi, just to make sure: is the config file you shared being used for both Prometheus and Prometheus Agent?","Hi.\r\nBoth use the same config file except that non agent has rules files in config.\r\n","I have tried to reproduce this but have not been able to. Can you share details on the number of series, the timeframe you saw this, and any other details?","@mattdurham \r\nNumber of Series: 140620\r\nWe see the agent mode using the high heap memory(As described remote.processExternalLabels using more than 50% of total heap) all the time compared to that of server mode. Please let us know if you need any more specific info.","Below might be useful. The first go profile output ran on Mar 11, 2022 at 9:38pm (PST); the prometheus was using 332.41MB, and 2nd output is taken on Mar 12, 2022 at 5:26pm (PST) and we can see the heap usage is increased to 939.79MB of which processExternalLabels using more than 50% of heap.\r\n`~\/ws\/prometheus$ go tool pprof pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz\r\nFile: prometheus\r\nType: inuse_space\r\nTime: Mar 11, 2022 at 9:38pm (PST)\r\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\r\n(pprof) top\r\nShowing nodes accounting for 276.11MB, 83.06% of 332.41MB total\r\nDropped 137 nodes (cum <= 1.66MB)\r\nShowing top 10 nodes out of 94\r\n      flat  flat%   sum%        cum   cum%\r\n   77.06MB 23.18% 23.18%    77.06MB 23.18%  github.com\/prometheus\/prometheus\/storage\/remote.processExternalLabels\r\n   76.07MB 22.88% 46.06%    76.07MB 22.88%  github.com\/prometheus\/prometheus\/model\/labels.(*Builder).Labels\r\n   35.66MB 10.73% 56.79%    35.66MB 10.73%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n   25.01MB  7.52% 64.31%    25.01MB  7.52%  github.com\/prometheus\/prometheus\/model\/textparse.(*PromParser).Metric\r\n   13.81MB  4.16% 68.47%    90.87MB 27.34%  github.com\/prometheus\/prometheus\/storage\/remote.(*QueueManager).StoreSeries\r\n   10.60MB  3.19% 71.66%    10.60MB  3.19%  github.com\/golang\/snappy.Encode\r\n   10.49MB  3.16% 74.82%    10.49MB  3.16%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).trackStaleness\r\n   10.11MB  3.04% 77.86%    10.11MB  3.04%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).addRef\r\n       9MB  2.71% 80.57%        9MB  2.71%  github.com\/prometheus\/prometheus\/tsdb\/agent.seriesHashmap.Set\r\n    8.30MB  2.50% 83.06%     8.30MB  2.50%  github.com\/golang\/snappy.Decode\r\n(pprof) exit\r\n~\/ws\/prometheus$ go tool pprof pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz\r\nFile: prometheus\r\nType: inuse_space\r\nTime: Mar 12, 2022 at 5:26pm (PST)\r\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\r\n(pprof) top\r\nShowing nodes accounting for 844.99MB, 89.91% of 939.79MB total\r\nDropped 168 nodes (cum <= 4.70MB)\r\nShowing top 10 nodes out of 49\r\n      flat  flat%   sum%        cum   cum%\r\n  495.37MB 52.71% 52.71%   495.37MB 52.71%  github.com\/prometheus\/prometheus\/storage\/remote.processExternalLabels\r\n  110.50MB 11.76% 64.47%   605.87MB 64.47%  github.com\/prometheus\/prometheus\/storage\/remote.(*QueueManager).StoreSeries\r\n   84.58MB  9.00% 73.47%    84.58MB  9.00%  github.com\/prometheus\/prometheus\/model\/labels.(*Builder).Labels\r\n   56.68MB  6.03% 79.50%    56.68MB  6.03%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n   25.51MB  2.71% 82.21%    25.51MB  2.71%  github.com\/prometheus\/prometheus\/model\/textparse.(*PromParser).Metric\r\n   21.29MB  2.27% 84.48%    21.29MB  2.27%  github.com\/prometheus\/prometheus\/tsdb\/agent.(*DB).gc\r\n   19.77MB  2.10% 86.58%    19.77MB  2.10%  github.com\/prometheus\/prometheus\/tsdb\/encoding.(*Encbuf).PutString\r\n   10.69MB  1.14% 87.72%    10.69MB  1.14%  github.com\/golang\/snappy.Encode\r\n   10.49MB  1.12% 88.84%    10.49MB  1.12%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).trackStaleness\r\n   10.11MB  1.08% 89.91%    10.11MB  1.08%  github.com\/prometheus\/prometheus\/scrape.(*scrapeCache).addRef\r\n(pprof) exit\r\n~\/ws\/prometheus$ `\r\n\r\nAlso attached the respective pprof output files.\r\n[pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8255692\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz)\r\n[pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8255693\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz)\r\n","[pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8255692\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz)\r\n[pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8255693\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz)\r\n\r\n","@mattdurham Do you need any more info\/logs on this issue?","@roidelapluie @mattdurham  Please let me know what additional info is needed from me.","Hi\r\nOn the screenshot you can find memory usage from prometheus container (v2.36.0)\r\nFirst \"half\" - agent mode turned on\r\nSecond \"half\" - agent mode turned off\r\n\r\n(metrics from cadvisor -> \"container_memory_rss\")\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/44766208\/173801897-02d5e33f-305e-4435-ae42-5806fada7f5b.png)\r\n","First \"half\" - agent mode turned on\r\nSecond \"half\" - agent mode turned off\r\n@vaprzemek so this means agent mode is using significantly more memory than that regular mode, right?. ","@srimuvva Yes, as you can see in agent mode we can observe a peaks on memory usage. ","Update on this: this is something we've looked into a few times but haven't been able to reproduce ourselves, and I haven't been able to find anything in the code that might explain this which gives me confidence. \r\n\r\nMy working theory so far is that Prometheus Agent uses more memory because it has more in-memory series, where some series has more than one ref ID associated with it. \r\n\r\nThe only difference that I found is how the TSDB has an [extra check](https:\/\/github.com\/prometheus\/prometheus\/blob\/6767f6e1a946af41f83ca233c6fbfc94a928abee\/tsdb\/head.go#L1266-L1275) to ensure that concurrent writers don't create the same series with two different ref IDs (if I'm understanding the code correctly). Though this shouldn't be common: two concurrent scrape jobs writing the same series would likely be incorrect and produce unexpected results while graphing. \r\n\r\nI don't have a lot of confidence in that extra check being the cause, though that doesn't necessarily discount the theory that Prometheus Agent has more active in-memory series. ","@rfratto  Can you check the below issue and let us know if you need any help from me?\r\nhttps:\/\/github.com\/prometheus\/prometheus\/discussions\/10979","For the moment, our biggest issue is being able to reproduce this so we can investigate and fix. I think finding the smallest reproducible example which shows the memory difference (using targets which are either public or can be reproduced locally) would be the most helpful for getting this resolved.","We also observe the issue in our environment\r\n```\r\n\u276f go tool pprof -symbolize=remote -inuse_space http:\/\/<IP>:9090\/debug\/pprof\/heap\r\nFetching profile over HTTP from http:\/\/<IP>:9090\/debug\/pprof\/heap\r\nSaved profile in \/Users\/levar\/pprof\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gz\r\nFile: prometheus\r\nType: inuse_space\r\nTime: Sep 29, 2022 at 11:50am (CEST)\r\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\r\n(pprof) top\r\nShowing nodes accounting for 10.66GB, 91.72% of 11.62GB total\r\nDropped 241 nodes (cum <= 0.06GB)\r\nShowing top 10 nodes out of 56\r\n      flat  flat%   sum%        cum   cum%\r\n    4.63GB 39.84% 39.84%     4.63GB 39.84%  github.com\/prometheus\/prometheus\/storage\/remote.processExternalLabels\r\n    1.67GB 14.38% 54.22%     1.67GB 14.38%  github.com\/prometheus\/prometheus\/tsdb\/encoding.(*Decbuf).UvarintStr (inline)\r\n    1.50GB 12.93% 67.15%     3.16GB 27.23%  github.com\/prometheus\/prometheus\/tsdb\/record.(*Decoder).Series\r\n    1.12GB  9.61% 76.76%     1.12GB  9.61%  github.com\/prometheus\/prometheus\/tsdb\/index.(*MemPostings).Delete\r\n    0.85GB  7.28% 84.04%     0.85GB  7.30%  github.com\/prometheus\/prometheus\/model\/textparse.(*PromParser).Metric\r\n    0.23GB  1.98% 86.03%     4.87GB 41.89%  github.com\/prometheus\/prometheus\/storage\/remote.(*QueueManager).StoreSeries\r\n    0.22GB  1.92% 87.95%     0.22GB  1.92%  github.com\/prometheus\/prometheus\/tsdb.seriesHashmap.set\r\n    0.20GB  1.70% 89.65%     0.20GB  1.70%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n    0.14GB  1.19% 90.84%     0.19GB  1.61%  github.com\/prometheus\/prometheus\/tsdb.newMemSeries\r\n    0.10GB  0.88% 91.72%     0.10GB  0.88%  github.com\/prometheus\/prometheus\/tsdb\/chunkenc.NewXORChunk\r\n(pprof)\r\n```\r\nWhen can we expect a solving to this issue?","I also found this problem in my environment and have not found a solution so far","This is also an issue for me.\r\n\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/8452446\/e5521b8a-b70f-4b0a-9f88-abe45579e6ad)\r\n","\u4f60\u597d\uff0c\u4f60\u7684\u6765\u4fe1\u5b59\u5176\u541b\u5df2\u6536\u5230\uff0c\u82e5\u6709\u6025\u4e8b\u8bf7\u7535\u8bdd\u8054\u7cfb"],"labels":["kind\/more-info-needed"]},{"title":"TSDB DB.Delete does not work","body":"**What did you do?**\r\nI am using the TSDB package for storing metrics. Later the metrics are read from TSDB and written to a remote server.\r\nI tried using the function [DB.Delete](https:\/\/github.com\/prometheus\/prometheus\/blob\/fdb6916baff47c7d07c05bfc588c063e1cb9903f\/tsdb\/db.go#L1640) for deleting range.\r\nI even tested deleting everything from 'DB.StartTime()' to 'DB.Head().MaxTime()'.\r\nI even waited for the retention and compaction goroutine to operate.\r\nHowever, the range I deleted is still there.\r\n\r\n**What did you expect to see?**\r\nThe deleted range not appearing in queries nor on disk\r\n\r\n**What did you see instead? Under which circumstances?**\r\nMetrics in range are still there.\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n\t`Linux 5.14.10-300.fc35.x86_64 x86_64`\r\n\r\n* Prometheus version:\r\n\r\n\tfrom go.mod: `github.com\/prometheus\/prometheus v1.8.2-0.20211217191541-41f1a8125e66`\r\n","comments":["@bwplotka would appreciate your thoughts on this","Hi, \r\n\r\n1. What matchers you provide?\r\n2. Can you share how you are using the code? How you are creating DB etc? There might some wrong usage etc - it should work as you expect if you create DB correctly and if `func (db *DB) Delete(mint, maxt int64, ms ...*labels.Matcher) error {` does not return any errors. ","> Hi,\r\n> \r\n> 1. What matchers you provide?\r\n> 2. Can you share how you are using the code? How you are creating DB etc? There might some wrong usage etc - it should work as you expect if you create DB correctly and if `func (db *DB) Delete(mint, maxt int64, ms ...*labels.Matcher) error {` does not return any errors.\r\n\r\n1. I do not use matches. Just range. I want to delete the entire time range.\r\n2. [Here](https:\/\/github.com\/project-flotta\/flotta-device-worker\/blob\/8b3cf5a8dd5cf4f38e3a704d62db7ed0bb2cc20f\/internal\/metrics\/metrics.go#L65) you can see how we open the DB.\r\nThe last piece of code was this:\r\n```go\r\nminTime, err := db.StartTime()\r\nif err != nil {\r\n    log.Error(err)\r\n    return\r\n}\r\nlog.Info(\"TSDB min max\", minTime, db.Head().MaxTime())\r\nerr = db.Delete(minTime, db.Head().MaxTime())\r\nif err != nil {\r\n    log.Error(err)\r\n}\r\n\r\nerr = db.CleanTombstones()\r\nif err != nil {\r\n    log.Error(err)\r\n}\r\n```\r\ndb is an instance of 'tsdb.DB'.\r\nAt first, I did not use CleanTombstones.","> I do not use matches. Just range. I want to delete the entire time range.\r\n\r\nI think that is the reason there - you have to provide matches. Even if it is `__name__ != \"\"` (not sure if that will work, we block this in some places)","> > I do not use matches. Just range. I want to delete the entire time range.\r\n> \r\n> I think that is the reason there - you have to provide matches. Even if it is `__name__ != \"\"` (not sure if that will work, we block this in some places)\r\n\r\n@bwplotka it is not working. I used `db.Delete(tMin, tMax, &labels.Matcher{\r\n\t\tType: labels.MatchNotEqual,\r\n\t})`\r\nit seems that it should match any non-empty label.\r\nEven if adding matchers will work it will still be a bug. Because, when querying a range I do not have to provide a matcher.","Then we need to look closer, PRs welcome too! (: "],"labels":["kind\/question","component\/tsdb"]},{"title":"Proposal: add query sharding support to TSDB","body":"## Proposal\r\nIn November 2021, Grafana Labs [announced query sharding](https:\/\/grafana.com\/blog\/2021\/11\/09\/observabilitycon-announcements\/) for Grafana Enterprise Metrics (GEM).\r\n\r\nThe query sharding implementation leverages some changes we did in TSDB to be able to selectively query a given shard, so that multiple processes (running on different machines) can query different shards from the same copy of a TSDB block.\r\n\r\nA shard is a consistent set of series. The same series belongs to the same shard across different blocks. To do it, we use a simple hash mod algorithm to find out which shard each series belongs to.\r\n\r\nWe propose to upstream to Prometheus TSDB the changes we did to support query sharding. \r\n\r\nThe proposed changes target TSDB as a library. They can be used by other OSS projects as a foundation to build query sharding capabilities.\r\n\r\n**Use case. Why is this important?**\r\n\r\nQuery sharding can significantly reduce the time required to execute high cardinality and\/or CPU intensive queries. Running GEM at Grafana Labs, we\u2019ve seen a 10x reduction in query latency for high cardinality queries (up to 30x for some special cases).\r\n\r\n### How query sharding works in GEM\r\n\r\nLet\u2019s start with an example. Consider this simple query:\r\n\r\n```\r\nsum(rate(metric[1m]))\r\n```\r\n\r\nQuery sharding splits the query into N partial queries, where N is the number of shards (3 in the example) and each partial query runs the `sum(rate())` on a different set of series (a shard). Each partial query is executed by a different process (typically running on different machines) and the result of each partial query is then concatenated and a final `sum()` aggregation is run on top of it:\r\n\r\n```\r\nsum(\r\n  concat(\r\n    sum(rate(metric{__query_shard__=\"1_of_3\"}[1m]))\r\n    sum(rate(metric{__query_shard__=\"2_of_3\"}[1m]))\r\n    sum(rate(metric{__query_shard__=\"3_of_3\"}[1m]))\r\n  )\r\n)\r\n```\r\n\r\n> Note: the `concat()` function doesn\u2019t exist in PromQL and is not a real function. I\u2019ve used it to give an idea of the series concatenation process.\r\n\r\n### How GEM queries different shards from the same TSDB block\r\n\r\nFor a given block (containing all series for a time period, eg. 2h), a partial query needs to fetch chunks only from a subset of the series, which are the series matching the given shard. To do it, we added a couple of options to `storage.SelectHints`:\r\n\r\n- ShardIndex: the ID of the shard to query from the block (ranges between 0 and ShardCount-1). Only series whose hash mod matches the ShardIndex will be considered when running the `Select()`.\r\n- ShardCount: Total number of shards.\r\n\r\n### Proposed changes in Prometheus TSDB\r\n\r\nTo add sharding capability to TSDB querier\u2019s `Select()`, we propose the following changes.\r\n\r\nAdd `ShardIndex` and `ShardCount` to `storage.SelectHints`:\r\n\r\n```golang\r\ntype SelectHints struct {\r\n\t\/\/ \u2026\r\n\r\n\tShardIndex uint64 \/\/ Current shard index (starts from 0 and up to ShardCount-1).\r\n\tShardCount uint64 \/\/ Total number of shards (0 means sharding is disabled).\r\n}\r\n```\r\n\r\nAdd `ShardedPostings()` to `tsdb.IndexReader`. `ShardedPostings()` works similar to `SortedPostings()` but, instead of sorting postings, it filters postings by the given shard index and count:\r\n\r\n```golang\r\ntype IndexReader interface {\r\n\t\/\/ \u2026\r\n\r\n\t\/\/ ShardedPostings returns a postings list filtered by the provided shardIndex\r\n\t\/\/ out of shardCount. For a given posting, its shard MUST be computed hashing\r\n\t\/\/ the series labels mod shardCount (eg. `labels.Hash() % shardCount == shardIndex`).\r\n\tShardedPostings(p index.Postings, shardIndex, shardCount uint64) index.Postings\r\n}\r\n```\r\n\r\nFinally, call `ShardedPostings()` from `blockQuerier.Select()` and `blockChunkQuerier.Select()`.\r\n\r\nTo give a you a better understanding of this proposal, I\u2019ve opened a PR showing a **reference implementation**: https:\/\/github.com\/prometheus\/prometheus\/pull\/10421\r\n\r\n### Follow up work\r\n\r\nIn case this proposal is accepted, there\u2019s some follow up work we commit to upstream too, in particular:\r\n\r\n#### Series hash caching\r\n\r\nWhen querying the TSDB head, the series hash is already in memory, but when querying a TSDB block the series hash needs to be computed each time. Computing the series hash is an expensive operation.\r\n\r\nTo overcome this, we\u2019ve built an optional, configurable and fast in-memory cache for the series hashes, which we\u2019ll propose as a follow up PR.\r\n","comments":["Love this idea!\r\n\r\nThis is something we are discussing to leverage in Thanos too cc @moadz @fpetkovski \r\n\r\nDetails make sense, except for one thing that might matter for Thanos systems only: We don't do write deduplication of remote writes, so we can have Prometheus\/agent replicas in the receiver\/block TSDB code. This means that we might want to ensure that all replicas are in the same shard. The way we could it is to hash labels all but configured `replica` labels. Still I think we could implement the design proposed by @pracucci and then allow swapping `hash` function with some special hash that ignores `replica` label.\r\n\r\nTo sum, I don't see any objections to this design - maybe I would propose adding sharded method as separate interface to not overcrowd main one? \r\n\r\nAnyway, we can discuss this on PR","Thanks Bartek for the feedback!\r\n\r\n> Still I think we could implement the design proposed by @pracucci and then allow swapping hash function with some special hash that ignores replica label.\r\n\r\nAgree. I think being able to customise the hashing function could allow a good degree of customisation.\r\n\r\n> maybe I would propose adding sharded method as separate interface to not overcrowd main one?\r\n\r\nI'm totally up for that if helps to simplify downstream projects vendor\/ upgrades or TSDB maintenance. I just have a question to better understand the feedback: the `ShardedPostings()` function is directly used by the queriers `Select()`. Are there significant uses cases of the index that don't hit `Select()`? Maybe labels querying?","Who would be responsible to error when a user has the same label set in rate({}) output?\r\n\r\nWrong query example: `sum(rate({__name__=~\"foo|bar\"}[5m]))`. If you have foo and bar metrics (e.g. with no labels), it will fail in prometheus but not in your example."],"labels":["priority\/Pmaybe","component\/tsdb","kind\/feature"]},{"title":"PromQL: scalar comparison for range selector","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nIt would be great if we could do: `avg_over_time(probe_duration_seconds[10m] != 0)`. This could happen between a range selector and scalars. This would work directly on the samples of the selection, samples not matching would be dropped. This could be combined: `avg_over_time(probe_duration_seconds[10m] > 0 < 10)`\r\n\r\nThis is more effective that doing a subquery, which, additionally, requires you to know the scrape interval.","comments":["cc @juliusv ","No strong opinion on this yet, but once we go that direction, we could also consider a whole lot more variations:\r\n\r\n- The same between a range vector and an instant vector (I don't see how the other way around would work).\r\n- The same for arithmetic and set ops (for example, you could also have used an *and* operator, like `probe_duration_seconds[10m] and probe_success == 1`, or maybe in other cases you'll want to multiply or add an offset to all values before averaging them.","What would it change for instant vector? For instant vector this should already work with current promql.","I mean between a range vector and an instant vector: `foo[5m] > bar`","I think we can limit this to scalar for now.","@roidelapluie I would like to work on this proposal. \r\nCould you provide some more expressions I can analyse with, especially combination of operators  like the one you put in the description above. \r\nAs I have only come across operator combination in logical expressions, would that suffice here. e.g. `avg_over_time(probe_duration_seconds[10m] > 0 < 10)` be equivalent to `avg_over_time(probe_duration_seconds[10m] > 0 ) and avg_over_time(probe_duration_seconds[10m] < 10)`","@roidelapluie, as per my understanding, when applying a scalar on to a range vector,  does this illustration below depict the correct behavior \/ results or the expected outcome is something else. Request your feedback here, please.\r\n\r\nInput: `prometheus_http_requests_total[2m] ` actual samples\r\n\r\n- prometheus_http_requests_total{code=\"200\", handler=\"\/api\/v1\/query\", instance=\"localhost:9090\", job=\"prometheus\"}\r\n    ```\r\n    5 @123456\r\n    3 @123476\r\n    ```\r\n- prometheus_http_requests_total{code=\"200\", handler=\"\/metrics\", instance=\"localhost:9090\", job=\"prometheus\"}\r\n   ```\r\n    5 @123456\r\n    2 @123476\r\n    3 @123496\r\n    23 @123116\r\n    13 @123136\r\n    ```\r\n\r\nexpected outcome `prometheus_http_requests_total[2m] > 4`\r\n- prometheus_http_requests_total{code=\"200\", handler=\"\/api\/v1\/query\", instance=\"localhost:9090\", job=\"prometheus\"}\r\n    ```\r\n    5 @123456\r\n    ```\r\n- prometheus_http_requests_total{code=\"200\", handler=\"\/metrics\", instance=\"localhost:9090\", job=\"prometheus\"}\r\n   ```\r\n    5 @123456\r\n    23 @123116\r\n    13 @123136\r\n    ```\r\n"],"labels":["help wanted","priority\/P2","component\/promql","kind\/feature"]},{"title":"Patch log level config on runtime","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nIn some cases we would like to know what's going on with prometheus in production, we would like to see prometheus log change to debug. I thought it would be nice to have a way to change the log level in runtime without interrupting prometheus work (such as deployment).\r\n\r\nSo I'm thinking something like adding `PATCH \/config` endpoint to prometheus could be nice, or maybe just specific only for the log level\r\n","comments":["Yes, that's something I would like to add to Prometheus. it would involve moving the log level to file. this should be discussed in the dev summit because I don't know if everyone is happy with deprecating the command line flag."],"labels":["priority\/Pmaybe","component\/config"]},{"title":"ECR Public Gallery","body":"Would you consider also hosting your container image on ECR Public? This would be beneficial to AWS customers who are constrained by Dockerhub rate limiting.","comments":["This would be a question for [the developer community](https:\/\/prometheus.io\/community\/). Supporting this would mean adding it to all of our repositories. Worth considering, but it would require updating a number of common components.","For reference, this is about adding Prometheus to https:\/\/gallery.ecr.aws\/","Another way we could handle this is to setup something like https:\/\/github.com\/xelalexv\/dregsy. ","As a workaround, we also publish (most?) images to quay.io, which AFAIK does not have rate limits for pulling.","This is an example of one of the projects that's only on dockerhub: https:\/\/github.com\/prometheus\/statsd_exporter\r\n\r\nI can log an issue for that repo specifically if that makes more sense. I wasn't sure if this was a repo-specific policy or not.","@mike-stewart That project is on quay.io.\r\n\r\nhttps:\/\/quay.io\/repository\/prometheus\/statsd-exporter","Publishing to ECR Public Gallery would also help for when Quay.io goes down like it did earlier today"],"labels":["priority\/Pmaybe"]},{"title":"Implement oauth2 using refresh_token","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nCurrent implementation of oauth2 uses client_credentials (client_id and client_secret) https:\/\/www.oauth.com\/oauth2-servers\/access-tokens\/client-credentials\/ but for oauth2 authentication using refresh_token requires (refresh_token secret) https:\/\/www.oauth.com\/oauth2-servers\/access-tokens\/refreshing-access-tokens\/. \r\nWhen the endpoint has oauth2 authentication using refresh_token grant type, then we need to fetch access_token from user provided refresh_token secret and then hit the endpoint url.\r\nHence we shall add the support for oauth2 authentication using refresh_token along with client_credentials.","comments":["I am working on this enhancement and the proposed design is to introduce **refresh_token** and **grant_type** attributes in the oauth2 - https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#oauth2 and fetch access_token from given refresh_token or client_credentials ","What is your usecase? refresh_token is something that a client gets when it authenticates for the first time. This should not be used in a configuration file. Additionally, I'd like to keep the fact that our oauth implementation is only using golang.org\/x\/ and stdlib libraries.","Usecase -\n\nWe have an endpoint with oauth2 authentication which needs to be scraped by\nprometheus serviceMonitor.  This oauth2 authentication is based on\nrefresh_token\nhttps:\/\/www.oauth.com\/oauth2-servers\/access-tokens\/refreshing-access-tokens\/\nThis refresh_token(long lived) generation is one time effort but the\nendpoint is only accessed using the access_token which is generated by\nrefresh_token and token_url.\nHere we don't use client credentials for auth, hence we need oauth2 to\nsupport refresh_token based authentication.\n\nWe have to first generate *access_token* from the token_url using above\ndescribed refresh_token, then use this access_token to access the\nauthenticated endpoint which needs to be scraped. Please let me know if\nthis is possible.\n\nThanks,\nSindhura\n\n\n\nOn Wed, Feb 23, 2022 at 1:43 AM Julien Pivotto ***@***.***>\nwrote:\n\n> What is your usecase? refresh_token is something that a client gets when\n> it authenticates for the first time. This should not be used in a\n> configuration file. Additionally, I'd like to keep the fact that our oauth\n> implementation is only using golang.org\/x\/ and stdlib libraries.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/10339#issuecomment-1048172890>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AFOOSHI3IPPEIGYLRXX7IQ3U4PU53ANCNFSM5PCCREDQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n","Process - We have an authenticated (oauth2 - refresh-token based - https:\/\/www.oauth.com\/oauth2-servers\/access-tokens\/refreshing-access-tokens\/ ) endpoint that needs to be scraped by prometheus-operator.\u00a0We actually get the refresh_token for the first time from the client_credentials. Using this refresh_token alone we need to hit the token_url to get  the access_token using which we can access the authenticated endpoint. I was thinking if we can use below library for our usecase. Please correct me if I am wrong.\r\nhttps:\/\/pkg.go.dev\/golang.org\/x\/oauth2#Token\r\n","As suggested to use stdlib libraries, I looked at the oauth2 token library and I believe that we can use https:\/\/pkg.go.dev\/golang.org\/x\/oauth2#Token for implementation of refreshing access_tokens for oauth2 auth in prometheus repo.\r\n\r\n@roidelapluie could you please confirm if we can use above go oauth token library https:\/\/github.com\/golang\/oauth2\/blob\/2e8d9340160224d36fd555eaf8837240a7e239a7\/internal\/token.go for our usecase  (refreshing access_tokens).\r\n\r\nAlso please let me know if I can proceed with implementing this change in this repo.\r\nThanks.","This is not a roundtripper. It sounds like to me that you are not using oauth2 as intended. You could try using a proxy.","We believe that we are using OAuth2 as intended. We have used https:\/\/datatracker.ietf.org\/doc\/html\/rfc6749#section-6 logic in our service.\r\nAfter the service is deployed, we generate long-lived refresh_token (one time effort) using client_id. We use this refresh_token to regenerate access_token and this short lived access_token is used to hit the authenticated endpoint for metrics.\r\n\r\nTo clarify, we generate refresh_token as part of our deployment and store it in secret (one-time) - this transaction is done by our service. Only the generation of access_token using the provided refresh_token and hitting the endpoint needs to be taken care by the prometheus.\r\n\r\n@roidelapluie Please let me know if you have any queries\/ need more information. Also can you please describe more on the proxy that you have mentioned in the previous comment.","I am afraid that it's not possible to follow only tiny part of the RFC.\r\nIt sounds like you are ignoring the protocol flow section: https:\/\/datatracker.ietf.org\/doc\/html\/rfc6749#section-1.2\r\nYour usecase does not match what is in the RFC because you have multiple clients. Sadly, we can't support that in Prometheus."],"labels":["kind\/more-info-needed"]},{"title":"Handle saves\/deletes on \/api\/v1\/rules","body":"## Proposal\r\n### Is your proposal related to a problem?\r\n\r\nGrafana since v8 introduced a very useful alerting UI for end users.\r\nThey rely in background on an internal alertmanager not able to receive alerts from outside, native Alertmanager APIs to manage a remote instance, and some Cortex metrics APIs to manage all alerting from 1 place.\r\n\r\n**Using Grafana to evaluate alerting rules is a SPOF and late in alerting stack**, so you can also use this API natively https:\/\/cortexmetrics.io\/docs\/api\/#set-rule-group\r\nIs there any plans to implement this endpoint ?\r\n\r\n### Describe the solution you'd like\r\n\r\nThe best would be to handle POST and DELETE verbs on the existing rules API `\/api\/v1\/rules`.\r\nWe should be able to configure prometheus config file to check rules in a folder like `\/etc\/prometheus\/remote-rules.d\/*.yml`\r\nAt command-line args we could have `--rules.remote-management.store-path=\/etc\/prometheus\/remote-rules.d\/` then one yaml file is created by \"namespace\" (see cortexmetrics API doc)\r\n\r\n### Describe alternatives you've considered\r\n\r\nThe only homemade alternative is a reverse proxy with handles POST \/ DELETEs on `\/api\/v1\/rules` and a tool in charge to create files in a `\/etc\/prometheus\/remote-rules.d\/*.yml` folder\r\n\r\nThis proposal partialy duplicates https:\/\/github.com\/thanos-io\/thanos\/issues\/5168 but both are usefull if you want to dispatch alerts at different levels.\r\n","comments":["hello,\r\n\r\nthanks for your message. I plan to propose optionally backing up Prometheus config with a consistent storage, e.g. etcd, at the next dev summit. We'll see what happens then but it could be a first step to address this issue.","Great ! That's true it could be better for a k8s model where write config files will not persist...\r\n\r\nWhy not use a dedicated rules system folder in the data dir as first approach also ? We know data will persist in any architecture. And if for any reason it's not the case, you just use regular configs and not \"remote rules management\".\r\n\r\nAn additional flag --alerts.remote-management-enabled=true would be useful","I'd like to avoid mangling files on disk. If we were to support something like this, it would be hard to remove it later.\n\nI think it makes more sense to go directly to a database rather than letting the users access the local filesystem.","Sounds good.","I'd like to know is there any roadmap to support post\/put\/get\/delete api for rules in Prometheus natively? Managing rule files is kind of hard and inconvenient. \r\n\r\n@roidelapluie "],"labels":["priority\/Pmaybe"]},{"title":"EKS deployment of Prom: remote_write using sigv4 defaults to the wrong STS endpoint for private VPCs","body":"**What did you do?**\r\nAWS:  Inside of a private VPC ( by default no outbound access, but has VPC PrivateLink endpoints configured for AWS services.) we've deployed Prometheus into and EKS cluster using the kube-prometheus-stack Helm chart. \r\n\r\nConfigured prometheus to remote_write to a AWS Managed Prometheus endpoint using IaM Roles for Service Accounts as per the official documentation:  https:\/\/docs.aws.amazon.com\/prometheus\/latest\/userguide\/AMP-onboard-ingest-metrics-new-Prometheus.html\r\n\r\n**What did you expect to see?**\r\nExpected to see prometheus server running in my EKS cluster and sending metrics to the AWS Managed Prometheus ingest endpoint.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nThe Prometheus server pod started up and its readiness check was in a failed state.  The readiness check which hits \/-\/ready was returning a 503 error.  Then after about 10 mins the pod would be restarted.\r\n\r\nThe server's pod logs showed the following right before it was restarted by Kubernetes:\r\n```\r\nts=2022-02-16T21:13:29.514Z caller=main.go:1129 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml\r\nts=2022-02-16T21:22:13.151Z caller=main.go:1155 level=error msg=\"Failed to apply configuration\" err=\"could not get SigV4 credentials: WebIdentityErr: failed to retrieve credentials\\ncaused by: RequestError: send request failed\\ncaused by: Post \\\"https:\/\/sts.amazonaws.com\/\\\": dial tcp 209.54.180.124:443: connect: connection timed out\"\r\n```\r\nBased on the Prometheus remote_write configuration (shown in the section below) the AWS authentication library used by Prometheus should have connected to the regional endpoint for STS (sts.us-east-1.amazonaws.com) vs the global endpoint (sts.amazonaws.com) for sigv4 authentication.  \r\n\r\nI did find a workaround:\r\nI ended up doing some research and found this document: https:\/\/docs.aws.amazon.com\/cli\/latest\/topic\/config-vars.html#aws-sts\r\nYou can set the env var \"**AWS_STS_REGIONAL_ENDPOINTS**\" to the value \"**regional**\" which the AWS library Prometheus is using picks up and then connects to the correct endpoint.  \r\n\r\nFor reference, this is the value I had to pass the kube-prometheus-stack helm chart to fix the issue:\r\n```\r\nprometheus:\r\n  prometheusSpec:\r\n    containers:\r\n      - name: prometheus\r\n         env:\r\n           - name: AWS_STS_REGIONAL_ENDPOINTS\r\n              value: regional      \r\n```\r\n\r\nSo I'm not sure if the AWS auth code can be updated to account for potentially choosing the sts endpoint? or maybe this needs to potentially be better documented?\r\n\r\nIt's also worth noting that it took just shy of ten minutes for this failure to show up in the logs, even with the log level set to debug.  See the timestamps in the logs below.\r\n\r\n\r\n**Environment**\r\n\r\n* Prometheus version:\r\n\r\nprometheus, version 2.32.1 (branch: HEAD, revision: 41f1a8125e664985dd30674e5bdf6b683eff5d32)\r\n  build user:       root@54b6dbd48b97\r\n  build date:       20211217-22:08:06\r\n  go version:       go1.17.5\r\n  platform:         linux\/amd64\r\n\r\n* Prometheus configuration file:\r\n```\r\nremote_write:\r\n- url: https:\/\/aps-workspaces.us-east-1.amazonaws.com\/workspaces\/ws-1234abcd-1234abcd-1-2-3\/api\/v1\/remote_write\r\n  remote_timeout: 30s\r\n  sigv4:\r\n    region: us-east-1\r\n  queue_config:\r\n    capacity: 2500\r\n    max_shards: 200\r\n    max_samples_per_send: 1000\r\n```\r\n\r\n\r\n\r\n* Logs:\r\n```\r\nts=2022-02-16T21:13:29.514Z caller=main.go:1129 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml\r\nts=2022-02-16T21:22:13.151Z caller=main.go:1155 level=error msg=\"Failed to apply configuration\" err=\"could not get SigV4 credentials: WebIdentityErr: failed to retrieve credentials\\ncaused by: RequestError: send request failed\\ncaused by: Post \\\"https:\/\/sts.amazonaws.com\/\\\": dial tcp 209.54.180.124:443: connect: connection timed out\"\r\n```\r\n","comments":["I believe this is happening because by default the AWS SDK Go V1 SDK uses the global STS endpoint: https:\/\/github.com\/aws\/aws-sdk-go\/issues\/4154\r\n\r\n> By default the AWS SDK for Go V1 SDK uses the global STS access point. To change this behavior you should set aws.Config#STSRegionalEndpoint to endpoints.RegionalSTSEndpoint. You should provide this value when calling session.NewSession.\r\n\r\nMaybe it's just a matter of set the STSRegionalEndpoint here: https:\/\/github.com\/prometheus\/common\/blame\/main\/sigv4\/sigv4.go#L62-L68","Adding to the conversation that STSRegionalEndpoints can be activated\/deactivated https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_temp_enable-regions.html#sts-regions-activate-deactivate so once can probably not only use regional endpoints ","@jeromeinsf @marianafranco what is the correct action that we should take here?"],"labels":["kind\/more-info-needed"]},{"title":"feature request: extend agent to write\/read to\/from filesystem instead of in-memory with retention","body":"## Background of the problem\r\nHi.\r\nWe are working on managing edge devices from a central management application. Management application executes in k8s.\r\n\r\nAt the moment, we collect\/scrape metrics ourselves in the device. Metrics are saved in TSDB format.\r\nWe need to send these metrics to the k8s cluster.\r\nIn the cluster, metrics should be available for querying. Either by user or Grafana.\r\nWe also need to be able to forward the metrics from the cluster to a multi-cluster management application like Stolostron.\r\n\r\nWe were made aware of Prometheus Agent Mode and thought of using that as the collector and forwarder in the devices (replace our implementation). It will write to a Prometheus\/Thanos server in the cluster.\r\n## Request\/Proposal\r\nThe agent is missing a few features:\r\n- write from TSDB files and not from memory\r\n- disable in-memory metrics (default 2 hours) for lowering the memory usage\r\n- delete\/forget whatever was sent from TSDB files\r\n- apply retention rules. delete metrics older than the configured threshold.\r\n\r\nExisting features that might need verification or enhancements:\r\n- Agent should retry endlessly to send the data. Our use case is 10000s of devices and not all can write at once.\r\n## Links\r\n[feature request: Add agent support for beyond WAL persistent storage and metric export](https:\/\/github.com\/prometheus\/prometheus\/issues\/9607)","comments":["@bwplotka here's a new issue following our discussion. Thanks for all your help. appreciate it.","Prometheus agent is read\/writing metrics from filesystem already, I am not sure what you mean with this issue. The agent is using a Write-Ahead-Log on disk.","cc @rfratto ","It sounds like the proposal is for Prometheus Agent to write scraped metrics directly to the WAL and hold absolutely nothing in memory long-term (including from remote_write which also holds series in memory). Am I following correctly?","@rfratto @roidelapluie \r\nPerhaps I am not aware of how agent works. I thought that it saves up to 2 hours of metrics. All of it as part of WAL and may be persisted to filesystem.\r\nWhat I need is to save more than 2 hours. Days. Write the metrics to remote server. Delete sent metrics.\r\nPerhaps my problem can be addressed with a regular Prometheus. How heavy is the Prometheus server? If configuration includes only a few targets and no alerts are required , can it be considered a small server?","Hm let's clarify this issue @ydayagi \r\n\r\n> write from TSDB files and not from memory\r\n\r\nDo you mean remote write from TSBD blocks? I believe this ask is covered by https:\/\/github.com\/prometheus\/prometheus\/issues\/9607\r\n\r\n> disable in-memory metrics (default 2 hours) for lowering the memory usage\r\n\r\nI guess this is the core of this issue and it means what @rfratto tried to rephrase?\r\n\r\n> It sounds like the proposal is for Prometheus Agent to write scraped metrics directly to the WAL and hold absolutely nothing in memory long-term (including from remote_write which also holds series in memory). Am I following correctly?\r\n\r\nIs this what we are discussing here?\r\n\r\n> Perhaps I am not aware of how agent works. I thought that it saves up to 2 hours of metrics. All of it as part of WAL and may be persisted to filesystem.\r\nWhat I need is to save more than 2 hours. Days. \r\n\r\nI don't think that is physically possible because some code has to operate that WAL and that needs memory. Plus, I don't think we can have days of data in WAL format as it much less consistent \r\n\r\n> How heavy is the Prometheus server? If configuration includes only a few targets and no alerts are required , can it be considered a small server?\r\n\r\nYes, it can be considered \"small\". But there are things we can optimize further for your case:\r\n\r\n* Removal of any memory structures required for efficient querying capabilities, since you probably don't want that\r\n* Removal of data right after a remote write was successful for such batch of data.\r\n\r\nYet, there is nothing wrong in using it first and trying how it goes. Still, I don't think it matches your other point: the ability to remote write older data like \"days\", since we can only remote write data from first 2h. And even if we could remote write older data in the event of this binary being out of network for days, most of the backends\/remote write receivers does not accept writes older than 2h.\r\n\r\nWe choose to instead of trimming Prometheus from those features, we will implement an agent mode that starts from the above points. Then we can slowly add things like persistent storage with blocks and ability to forward that data (and remove after forward). This is what https:\/\/github.com\/prometheus\/prometheus\/issues\/9607 asks for.\r\n\r\n> delete\/forget whatever was sent from TSDB files\r\n\r\nI think this is also covered by https:\/\/github.com\/prometheus\/prometheus\/issues\/9607, please join discussion there.\r\n"],"labels":["kind\/more-info-needed"]},{"title":"Support percentage values in size-based retention","body":"## Proposal\r\n\r\nI'd love to see percentage values supported in size-based retenation. Prometheus might determine disk size on startup, and configure the disk size to n percent of this. For example: `--storage.tsdb.retention.size=90%` would configure a limit of 9GiB for a 10GiB disk.\r\n\r\nThis seems rather easily possible by using the syscall directly, or import Minio libraries (didn't check what license that code is under, though): https:\/\/stackoverflow.com\/q\/20108520\/695343\r\n\r\n**Use case. Why is this important?**\r\n\r\nWe're operating hundreds of Prometheus instances for differently sized Kubernetes clusters using prometheus-operator. We're actively monitoring for Prometheus instances not having a sufficiently large disk and increase their sizes. Currently, we need to not only resize the volume, but also reconfigure our prometheus operator CR, allowing diverging configuration. A percentage-based size definition would allow us to simply configure retention to use up \"most of the disk\".\r\n\r\n<sup>Jens Erat <jens.erat@daimler.com>, Daimler TSS GmbH, [imprint](https:\/\/github.com\/mercedes-benz\/daimler-foss\/blob\/master\/PROVIDER_INFORMATION.md)<\/sup>","comments":["Can the total disk size change while Prometheus is running?","> Can the total disk size change while Prometheus is running?\r\n\r\nyes a mounted partition can be resized while mounted (like remote mounted partition or LVM based partition).","This sounds like a good option to have if we can find the disk size in a reliable way. Additionally, instead of just determining it on startup, we may also update the limit during runtime at intervals in case the underlying disk size was increased without a restart, hence not requiring a restart to detect this change.","We can check the disk size every time the BeyondSizeRetention function is called when a percentage is given. This way, we can ensure that we always have an up-to-date value for the disk size without having to store it.","One thing to consider is that there are special cases returning inconsistent values for used and available storage space, especially some network filesystems. I specifically remember NFS filers with snapshots enabled magically reducing available storage space and transparent compression enabling to store more data than the volume theoretically has.\r\n\r\nI'd propose to just ignore and potentially document this issue, or maybe add some safeguards when reading free disk space.","> One thing to consider is that there are special cases returning inconsistent values for used and available storage space, especially some network filesystems. I specifically remember NFS filers with snapshots enabled magically reducing available storage space and transparent compression enabling to store more data than the volume theoretically has.\r\n> \r\n> I'd propose to just ignore and potentially document this issue, or maybe add some safeguards when reading free disk space.\r\n\r\nI would make this parameter optional (obvious) and document it with a warning about remote or exotic filesystems. Prefer using it on local disks or use it at your own risks.","I'm open to review \/ comment on my PR \ud83d\ude03"],"labels":["help wanted","priority\/P3","component\/tsdb","kind\/feature"]},{"title":"Missing docs for `__meta_kubernetes_endpoints_label*` labels","body":"The commit https:\/\/github.com\/prometheus\/prometheus\/commit\/617c56f55a14b7f6ed4f0f4402c1e18aaf3b396c adds new labels for `role: endpoints`: `__meta_kubernetes_endpoints_label_*` and `__meta_kubernetes_endpoints_labelpresent_*`. But these labels aren't documented at https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#kubernetes_sd_config . This may prevent from the usage of these labels.\r\n\r\nSide notes:\r\n1) Why these labels weren't added to `role: endpointslice`?\r\n2) Why `__meta_kubernetes_endpoints_annotation*` labels weren't added to `role: endpoints` and `role: endpointslice` to be consistent with other `role`s in `kubernetes_sd_config`?","comments":["cc'ing @fcddk and @like-inspur , the authors of https:\/\/github.com\/prometheus\/prometheus\/commit\/617c56f55a14b7f6ed4f0f4402c1e18aaf3b396c "," @roidelapluie I see #10911 was merged and `__meta_kubernetes_endpoints_label_<labelname>` and `__meta_kubernetes_endpoints_labelpresent_<labelname>` are documented in https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#endpoints, so this issue should be ok to close, right? Or am I missing something?","any updates?"],"labels":["help wanted","component\/service discovery","priority\/P3","component\/documentation"]},{"title":"Flaky test on windows","body":"https:\/\/app.circleci.com\/pipelines\/github\/prometheus\/prometheus\/15920\/workflows\/9d30d9e2-35ea-47e6-95a2-440976b3d6ed\/jobs\/75648\r\n\r\n```\r\n=== RUN   TestDBReadOnly\/chunk_querier\r\nlevel=info msg=\"Replaying on-disk memory mappable chunks if any\"\r\nlevel=info msg=\"On-disk memory mappable chunks replay completed\" duration=0s\r\nlevel=info msg=\"Replaying WAL, this may take a while\"\r\nlevel=info msg=\"WAL segment loaded\" segment=0 maxSegment=1\r\nlevel=info msg=\"WAL segment loaded\" segment=1 maxSegment=1\r\nlevel=info msg=\"WAL replay completed\" checkpoint_replay_duration=4.8002ms wal_replay_duration=2.4152ms total_replay_duration=7.2154ms\r\n    db_test.go:2353: \r\n        \tError Trace:\tdb_test.go:2353\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: map[string][]chunks.Meta{\"{foo=\\\"bar\\\"}\":[]chunks.Meta{chunks.Meta{Ref:0x2000000, Chunk:(*chunkenc.XORChunk)(0xc000095d60), MinTime:18, MaxTime:18}}, \"{labelName=\\\"0\\\"}\":[]chunks.Meta{chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f0080), MinTime:10, MaxTime:11}, chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f0000), MinTime:12, MaxTime:13}, chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f0060), MinTime:14, MaxTime:15}, chunks.Meta{Ref:0x1000000, Chunk:(*chunkenc.XORChunk)(0xc0021f0020), MinTime:16, MaxTime:17}}}\r\n        \t            \tactual  : map[string][]chunks.Meta{\"{foo=\\\"bar\\\"}\":[]chunks.Meta{chunks.Meta{Ref:0x2000000, Chunk:(*chunkenc.XORChunk)(0xc0021f0700), MinTime:18, MaxTime:18}}, \"{labelName=\\\"0\\\"}\":[]chunks.Meta{chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f0a80), MinTime:10, MaxTime:11}, chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f09c0), MinTime:12, MaxTime:13}, chunks.Meta{Ref:0x8, Chunk:(*chunkenc.XORChunk)(0xc0021f0a60), MinTime:14, MaxTime:15}, chunks.Meta{Ref:0x1000000, Chunk:(*chunkenc.XORChunk)(0xc0021f09e0), MinTime:16, MaxTime:17}}}\r\n        \t            \t\r\n        \t            \tDiff:\r\n        \t            \t--- Expected\r\n        \t            \t+++ Actual\r\n        \t            \t@@ -36,4 +36,4 @@\r\n        \t            \t      stream: ([]uint8) (len=21) {\r\n        \t            \t-      00000000  00 00 00 00 00 00 30 fe  39 f5 cb 01 00 00 01 00  |......0.9.......|\r\n        \t            \t-      00000010  00 00 00 00 00                                    |.....|\r\n        \t            \t+      00000000  00 02 18 3f e8 49 b4 73  25 04 a6 01 d5 af 33 f1  |...?.I.s%.....3.|\r\n        \t            \t+      00000010  f1 ba 42 03 c0                                    |..B..|\r\n        \t            \t      },\r\n        \t            \t@@ -50,4 +50,4 @@\r\n        \t            \t      stream: ([]uint8) (len=21) {\r\n        \t            \t-      00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\r\n        \t            \t-      00000010  00 00 00 00 00                                    |.....|\r\n        \t            \t+      00000000  00 02 1c 3f ea 22 94 0d  e5 13 42 01 d5 b4 8c bc  |...?.\"....B.....|\r\n        \t            \t+      00000010  44 86 0e f2 60                                    |D...`|\r\n        \t            \t      },\r\n        \tTest:       \tTestDBReadOnly\/chunk_querier\r\n        \tMessages:   \tseries chunks mismatch\r\n--- FAIL: TestDBReadOnly (0.19s)\r\n    --- PASS: TestDBReadOnly\/blocks (0.00s)\r\n    --- PASS: TestDBReadOnly\/querier (0.02s)\r\n    --- FAIL: TestDBReadOnly\/chunk_querier (0.02s) \r\n```","comments":["@replay is it possible that it is happening because of the queue?"],"labels":["kind\/bug","kind\/cleanup","component\/tsdb"]},{"title":"promtool to report at least warning when scrape interval is greater than 4 minutes","body":"## Proposal\r\n**Use case:** \r\nThere are some product related metrics that are collected by running some scripts. As the scripts puts heavy load on system, they are run only once per day during night time. These metrics are exported via a custom exporter which is scrapped from Prometheus once in 23 hours.\r\n**Issue:**\r\nBecause of 5minute staleness, any metric that is reported with more than 4 minute scrape interval will essentially be marked as stale. \r\n**Proposal:**\r\nWhile there are other mechanisms like using push gateway or implementing our own metric cache and scrapping the cache for every say 4 minutes, it is good if promtool reports larger scrape intervals as warning so that new bees like us are made aware of things then and there. \r\n**References:**\r\nhttps:\/\/stackoverflow.com\/questions\/57663315\/how-to-correctly-scrape-and-query-metrics-in-prometheus-every-hour\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/9250\r\n","comments":["Or even 2m, because that's generally the max recommended scrape interval (so you can deal with a missed scrape without running into the lookback delta).","I'm happy to work on this if some direction could be provided for this feature request.","I would think a warning should be issues if the scrape interval is larger than 2m (but it shouldn't be a hard error as there are special scenarios where larger scrape intervals make sense).\r\n\r\nNot even sure if the `check config` command has a concept of warnings.\r\n\r\nPaging @dgl and @JessicaGreben as the promtool maintainers.","I think a similar feature in `check config` to the idea for `--lint` outlined in #8613 would work. As @beorn7 points out there isn't really the concept of warnings, so this --lint option would add the ability to do that.\r\n\r\nOne minor complexity for this is the lookback delta can be set as a command line flag, which isn't a command line flag for promtool. Having the ability to disable the relevant lint check is probably enough. In general I think we should discourage people changing the lookback delta (and note promtool rule tests can't change it currently, so those tests can't check rules designed for a different lookback delta)."],"labels":["kind\/enhancement","component\/promtool","priority\/P3"]},{"title":"Provide opencontainers labels in Dockerfile","body":"It could be a nice addition to provide official opencontainers labels in the Dockerfile of prometheus:\r\n\r\n```\r\nLABEL org.opencontainers.image.authors=\"xxxx?\" \\\r\n      org.opencontainers.image.source=\"https:\/\/github.com\/prometheus\/prometheus\" \\\r\n      org.opencontainers.image.description=\"The Prometheus monitoring system and time series database\" \\\r\n      org.opencontainers.image.documentation=\"https:\/\/prometheus.io\/docs\" \\\r\n      org.opencontainers.image.licenses=\"Apache License 2.0\" \\\r\n      org.opencontainers.image.title=\"prometheus\" \\\r\n      org.opencontainers.image.url=\"https:\/\/github.com\/prometheus\/prometheus\" \\\r\n      org.opencontainers.image.vendor=\"xxx?\"\r\n```\r\n\r\nOne example why this is useful is dependabot or renovatebot. When these tools update the dockerimage version, they can extract the release notes from the `source` label and provide it in the pullrequest. [See the renovate docs](https:\/\/github.com\/renovatebot\/renovate\/blob\/4b6270b981612f1108d23c3fbe0f85ed28f2ca18\/lib\/datasource\/docker\/readme.md)\r\n\r\nWhat do you think?","comments":["That sounds find to me, but it would be great to discuss this in the prometheus-developers mailing list, as all projects would be impacted."],"labels":["kind\/enhancement","priority\/Pmaybe"]},{"title":"Native Service Discovery for Oracle Cloud Infrastructure","body":"Hello Prometheus Community,\r\nI am with OCI(Oracle Cloud Infrastructure). \r\n\r\nUse Case: I am reaching out to you on behalf of joint OCI and Prometheus customers. These joint customers are asking for support for native Service Discovery(SD) in Prometheus for their OCI resources(similar to the ones listed [here](https:\/\/github.com\/prometheus\/prometheus\/tree\/main\/discovery)). Since Prometheus is widely used as a one-stop-shop for monitoring resources across different environments(cloud providers and on-prem), native SD for OCI will really be useful for many OCI and Prometheus joint users.\r\n\r\nMy team is planning to develop this new SD for OCI, following all the guidelines and processes set by the community. We will maintain it on an ongoing basis as well. I think we meet the criteria for new native SD, as laid down in the documentation [here](https:\/\/github.com\/prometheus\/prometheus\/tree\/main\/discovery). \r\n\r\nBefore my team embarks on building SD for OCI,  I wanted to know if Prometheus Community is open to accepting new SD for OCI, given the information above.\r\n","comments":["Can we get more details about OCI, mainly:\r\n- Which services would be implemented?\r\n- Is there any rate limits that users might hit?","Hi @roidelapluie \r\n\r\n1. We are starting with Compute Service...discovering compute nodes on OCI. These nodes will have Prometheus NodeExporter running on them. Other services might come in the future.\r\n\r\n2. we have a rate limit of 1 TPS\/api\/Region\/CustomerAccount for most of the OCI SDK APIs which are for fetching metadata info(for nodes, vcn etc). In order to avoid hitting limits, we can leverage retries and exponential backoff built into OCI SDK.\r\n\r\nThanks\r\nMayur","Even if vendors commit to support for their SD, it's critical as Prometheus maintainer to have access to the infrastructure. Would it be possible to get a test account to validate and maintain the SD over the long term? We have a few SD's where I use personal account to develop\/maintain and I like to avoid it for new SD's.\r\n\r\nAlso  the fact that the second message already speaks about hitting limits seems to be a red flag to me. Do you have an idea of the number of api call's needed\/allowed for this SD to work? ","Hi @roidelapluie \r\nDefinitely. What are your requirements for the test account in terms of what services\/service limits are required?\r\n\r\nI would love to understand more about the red flag you mentioned. For context, each discovery job starts at a region and a compartment in a tenancy. Each of the calls has its own rate limits per region. We will have recommendations in service discovery documentation to mitigate issues around throttling, for example, recommended values for refresh interval and right defaults, etc.\r\nI will get back to you with API calls calculations, to make crystal the number of calls to different OCI APIs we will make in SD.","@roidelapluie gentle reminder","Yes, I would like to know how many targets you aim at supporting with default values, without throttling.","Friendly ping.","@roidelapluie I plan to support ~300 targets in 1 sd job, without throttling and with a default refresh_interval of 5minutes. \r\nI will be done with coding the functionality in 1 or 2 days and will perform testing and see how it performs. Will soon get back to you with results in 2\/3 days.  Does this help?\r\nAlso, can you please share your requirements for the test account on OCI in terms of what services\/service limits are required? ","We need to be able to power small vms', to test the SD. This is only debugging and ad-hoc, when we have users requests.","Hi @roidelapluie \r\nI have got pulled into some other work but will get back to OCI-SD development and testing in a week.\r\n\r\nFor the OCI Cloud account for testing, I think Oracle Cloud \"Always Free\" tier might do the job. Here are the relevant links for the specifics.\r\nhttps:\/\/docs.oracle.com\/en-us\/iaas\/Content\/FreeTier\/freetier_topic-Always_Free_Resources.htm#freetier_topic_Always_Free_Resources_Infrastructure\r\n\r\nhttps:\/\/www.oracle.com\/cloud\/free\/#always-free\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/93163855\/159537488-68e2a8f4-b3df-4811-8254-51593771d043.png)\r\n\r\n\r\n","Okay. One last think would be to see how much bigger the prometheus binary gets with that extra SD.","for Prometheus binary: it is 116.5 MB after building locally with new OCI SD on mac vs 104.7MB I downloaded from https:\/\/prometheus.io\/download\/ for darwin.","Hi @roidelapluie \r\nI am code complete and have done testing with 1000 targets on OCI across 3 jobs. The results look good. I will create PR after completing some internal processes.","Hi @roidelapluie \r\n\r\nWhile working through some internal processes, a question came up with regards to the copyright notice format. Our standard when contributing code to a third-party open-source project is to just include \"Copyright 2022 Oracle and\/or its affiliates\u201d (with the applicable year(s)) in the file headers.\r\nHowever, since we\u2019d be contributing Oracle code under the Apache 2.0 license, it may make more sense and be easier for you in the long run for Oracle to include in the file headers of Oracle code the notice. \r\nThis is as per the recommendation of Apache under the How To Apply The Apache License To Your Work section at https:\/\/www.apache.org\/licenses\/LICENSE-2.0 (\"Copyright 2022 Oracle and\/or its affiliates. Licensed under the Apache License, Version 2.0 . . .\").\r\nDo you have a preference as to which form of the header we include in the Oracle-authored files? Do you also want us to include a NOTICE.txt file for the Oracle folder (in keeping with Apache 2.0 application guidance)? Since the same license (Apache 2.0) that applies to the Prometheus project would also apply to Oracle\u2019s contribution, a LICENSE.txt file in the Oracle folder is not necessary, but, if you would prefer a more built-out folder, we can accommodate. Please advise. ","re: copyright headers, we prefer what is here:\r\nhttps:\/\/github.com\/cncf\/foundation\/blob\/main\/copyright-notices.md#copyright-notices\r\n\r\nre: NOTICE.txt, it's fine to include\r\n\r\nWe also prefer a LICENSE file at the root of the project","In other cases i had added original copyright to files, like here:\r\nhttps:\/\/github.com\/prometheus\/exporter-toolkit\/blob\/9f7b277231e81a5529f560c0389d764cc0ac7de3\/web\/handler.go#L1-L3\r\n\r\nMaybe a double-copyright attribution on the files would be the simplest option here?","dear guys, i have written oci_exporter which is capable of exporting all metrics to prometheus, i am trying to contribute to exporters  https:\/\/github.com\/munger1985\/OCI-Auto-Scripts\/tree\/main\/oci_exporter"],"labels":["priority\/Pmaybe","component\/service discovery","kind\/feature"]},{"title":"Discovery.Manager not closing `syncCh` after sending stopped","body":"**What did you do?**\r\n\r\nWe quotes packages of `prometheus\/discovery` for service discovery in `loki` project, \r\n\r\nand finds that after `discovery.Manager` exit, there is a possible memory leak when quoating `manager.Sync()` outside manager's lifecycle.  \r\n\r\nThis is due to an unclosed channel in Manager struct.\r\n\r\n\r\n**What did you expect to see?**\r\n\r\nAfter discussing with contributors in loki project(see more in[loki-pr-5238](https:\/\/github.com\/grafana\/loki\/pull\/5238)), \r\n\r\nwe think it would be better the channel is closed within lifecycle of `discovery.Manager`, preferably when write process is over, for two reasons:\r\n\r\n1.  in `documentation\/examples\/custom-sd\/adapter\/adapter.go` Prometheus gives an official example of reading from `Manager.Synch()`,  and it will only exit after the channel is closed(which didn't, and this caused confusion), see below\r\n\r\n``` golang\r\nfunc (a *Adapter) runCustomSD(ctx context.Context) {\r\n\tupdates := a.manager.SyncCh()\r\n\tfor {\r\n\t\tselect {\r\n\t\tcase <-ctx.Done():\r\n\t\tcase allTargetGroups, ok := <-updates:\r\n\t\t\t\/\/ Handle the case that a target provider exits and closes the channel\r\n\t\t\t\/\/ before the context is done.\r\n\t\t\tif !ok {\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\ta.refreshTargetGroups(allTargetGroups)\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n2. If the channel is not closed, readers will have no choice but to shutdown goroutine via `ctx.Done()`, consider this situation causing messages losing:\r\n\r\n* `ctx` is cancelled before readers read in all messages\r\n\r\n \r\n\r\n\r\n\r\n**To Reproduce:**\r\n\r\n1. call `discovery.NewManager(ctx)` and name it `m` in project\r\n2. call `m.Synch()` to get a read copy of channel, and read from it \r\n3. call `ctx.CancelFunc()` \r\n4. find that `m.sender()`is stopped while not close the channel.","comments":["I agree with you. however, we have to be very careful with this as we ave to check we never send anything to that closed channel.","> \r\n\r\n@roidelapluie  Yes, I totally agree with your prudence  and thanks for your attention :) !  \r\nI checked related code and found that there is a great design for this channel\u2014\u2014send ops only happens within  goroutine of `sender()`\uff0cany other place references the channel is via read-mode, see below :\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/4cc25c0cb0b96042a7d36a0dd53dc6970ad607fd\/cmd\/prometheus\/main.go#L1542\r\n\r\n so I suggested we can close it after `sender()` is stopped naturally( in `defer` process,so there is no worry about sending to a closed channel). see in  #10218 \r\n\r\nIf there is anything I can do to follow up plz let me know","I have been investigating this deeply and this change is not good to me:\r\n\r\nPrometheus is using this channel into the NotifyManager and the ScrapeManager. Both are reading from this channel. If this channel is closed, they would receive in loops zero-values from that channel.\r\n\r\nThe Prometheus server is using https:\/\/github.com\/oklog\/run to manage those goroutines. This packages does not guarantee any shutdown order, which means that the chances that Notify\/Scrape manager might read un-necessarily zeroed values is not nil, leading to unknown behavior.\r\n\r\nTherefore, I'd prefer to stick to the current behavior."],"labels":["kind\/cleanup","component\/service discovery","priority\/P3"]},{"title":"Memory spikes killing Prometheus","body":"\r\n**What did you do?**\r\n\r\n    Prometheus restart due to wal folder size increase \r\n\r\n**What did you expect to see?**\r\n   Prometheus shoudl clear the wal folder over a period of time and getting crash\r\n\r\n**What did you see instead? Under which circumstances?**\r\n   \r\n     when WAL folder reaches > 4gb , container will restart or crash\r\n\r\n**Environment**\r\n\r\n   Production \r\n\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n         Prometheus 2.21\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n\r\nTo solve this issue I am using below sidecar container script\r\n\r\nPrerequisite:\r\n     The wal folder location should be accessed by sidecar container\r\n\r\n ```\r\n  package main\r\n\r\n        import (\r\n        \t\"fmt\"\r\n        \t\"log\"\r\n        \t\"net\/http\"\r\n        \t\"os\"\r\n        \t\"os\/exec\"\r\n        \t\"strconv\"\r\n        \t\"strings\"\r\n        \t\"time\"\r\n        )\r\n\r\n        func clear() {\r\n        \tfolder := []string{\"\/data\/wal\", \"\/data\/chunks_head\"}\r\n        \tfor i := 0; i < len(folder); i++ {\r\n        \t\te := os.RemoveAll(folder[i])\r\n        \t\tfmt.Println(\" Removed \", e)\r\n        \t}\r\n        \tfor i := 0; i < len(folder); i++ {\r\n        \t\terr1 := os.MkdirAll(folder[i], os.FileMode(0777))\r\n        \t\terr := os.Chown(folder[i], 65534, 65534)\r\n        \t\tif err != nil || err1 != nil {\r\n        \t\t\tlog.Println(err, err1)\r\n        \t\t}\r\n        \t}\r\n        }\r\n        func main() {\r\n        \tfor {\r\n        \t\ttime.Sleep(1 * time.Second)\r\n        \t\tout, err := exec.Command(\"du\", \"-sk\", \"\/data\/wal\").Output()\r\n        \t\tif err == nil {\r\n        \t\t\td := strings.Fields(string(out))[0]\r\n        \t\t\tf := strings.Replace(d, \"K\", \"\", 1)\r\n        \t\t\tif f1, e := strconv.Atoi(f); f1 > 4194304 && e == nil {\r\n        \t\t\t\tclear()\r\n\r\n        \t\t\t} else {\r\n        \t\t\t\tfmt.Println(\"Size is less \"+d+\" ==>  %q\", (time.Now()))\r\n        \t\t\t}\r\n\r\n        \t\t\turl := \"http:\/\/localhost:9090\/graph\"\r\n\r\n        \t\t\treq, _ := http.NewRequest(\"GET\", url, nil)\r\n\r\n        \t\t\tres, _ := http.DefaultClient.Do(req)\r\n\r\n                                     if res == nil {\r\n             \t\t\t\t            clear()\r\n             \t\t\t         }\r\n\r\n\r\n\r\n        \t\t} else {\r\n        \t\t\tfmt.Printf(\"Folder %q is not exists  ==>  %q\"+\"\\n\", (\"\/data\/wal\"), (time.Now()))\r\n        \t\t}\r\n\r\n        \t}\r\n\r\n        }\r\n\r\n```\r\n\r\n**Note:** The above script clear the logs if the size is more then 4gb or if the wal folder corrupt\r\n","comments":["@praveensams16 the WAL files are automatically cleaned up every 2 hours and corrupt files are repaired or removed on startup. What is the exact issue that you are facing? Can you share the logs around the crash?","PS: Manually deleting the WAL files is not a good idea, you might face data loss on any restart.","Hi @codesome \r\n\r\nPlease find the logs below\r\n\r\n```\r\nts=2022-02-01T01:17:34.902Z caller=main.go:444 level=info msg=\"Starting Prometheus\" version=\"(version=2.31.0, branch=HEAD, revision=d4c83da6d252d4edfdbd639fb817ebdb8e9ab2e4)\"\r\nts=2022-02-01T01:17:34.902Z caller=main.go:449 level=info build_context=\"(go=go1.17.2, user=root@395d507f537a, date=20211102-10:28:54)\"\r\nts=2022-02-01T01:17:34.902Z caller=main.go:450 level=info host_details=\"(Linux 4.14.243-185.433.amzn2.x86_64 #1 SMP Mon Aug 9 05:55:52 UTC 2021 x86_64 prometheus-server-8478db6d78-944g7 (none))\"\r\nts=2022-02-01T01:17:34.902Z caller=main.go:451 level=info fd_limits=\"(soft=1048576, hard=1048576)\"\r\nts=2022-02-01T01:17:34.902Z caller=main.go:452 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nts=2022-02-01T01:17:34.907Z caller=web.go:542 level=info component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nts=2022-02-01T01:17:34.907Z caller=main.go:839 level=info msg=\"Starting TSDB ...\"\r\nts=2022-02-01T01:17:34.909Z caller=tls_config.go:195 level=info component=web msg=\"TLS is disabled.\" http2=false\r\nts=2022-02-01T01:17:34.917Z caller=head.go:479 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nts=2022-02-01T01:17:35.119Z caller=head.go:513 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=201.587779ms\r\nts=2022-02-01T01:17:35.119Z caller=head.go:519 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nts=2022-02-01T01:18:03.261Z caller=head_wal.go:360 level=warn component=tsdb msg=\"Unknown series references\" samples=3774 exemplars=0\r\nts=2022-02-01T01:18:03.261Z caller=head.go:555 level=info component=tsdb msg=\"WAL checkpoint loaded\"\r\nts=2022-02-01T01:18:03.263Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=614 maxSegment=621\r\nts=2022-02-01T01:18:03.266Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=615 maxSegment=621\r\nts=2022-02-01T01:18:03.271Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=616 maxSegment=621\r\nts=2022-02-01T01:18:03.275Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=617 maxSegment=621\r\nts=2022-02-01T01:18:03.278Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=618 maxSegment=621\r\nts=2022-02-01T01:18:03.282Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=619 maxSegment=621\r\nts=2022-02-01T01:18:03.662Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=620 maxSegment=621\r\nts=2022-02-01T01:18:03.662Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=621 maxSegment=621\r\nts=2022-02-01T01:18:03.662Z caller=head.go:596 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=28.142507098s wal_replay_duration=400.604107ms total_replay_duration=28.744749931s\r\nts=2022-02-01T01:18:11.811Z caller=main.go:866 level=info fs_type=EXT4_SUPER_MAGIC\r\nts=2022-02-01T01:18:11.811Z caller=main.go:869 level=info msg=\"TSDB started\"\r\nts=2022-02-01T01:18:11.811Z caller=main.go:996 level=info msg=\"Loading configuration file\" filename=\/etc\/config\/prometheus.yml\r\nts=2022-02-01T01:18:11.816Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2022-02-01T01:18:11.836Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2022-02-01T01:18:11.836Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2022-02-01T01:18:11.837Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2022-02-01T01:18:11.852Z caller=main.go:1033 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/config\/prometheus.yml totalDuration=41.035182ms db_storage=1.048\u00b5s remote_storage=1.462\u00b5s web_handler=372ns query_engine=927ns scrape=2.080747ms scrape_sd=20.97451ms notify=26.125\u00b5s notify_sd=11.519\u00b5s rules=14.991234ms\r\nts=2022-02-01T01:18:11.852Z caller=main.go:811 level=info msg=\"Server is ready to receive web requests.\"\r\nts=2022-02-01T01:18:19.837Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1643666400107 maxt=1643673600000 ulid=01FTSDRQ9389EBTFAD7F91V4YD duration=6.554784618s\r\nts=2022-02-01T01:18:19.855Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FTSDRQ9389EBTFAD7F91V4YD\r\nts=2022-02-01T01:18:20.411Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=556.056078ms\r\nts=2022-02-01T01:18:20.411Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=614 to_segment=618 mint=1643673600000\r\nts=2022-02-01T01:18:41.380Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=614 last=618 duration=20.968527514s\r\n```\r\n\r\n\r\nThe pods getting restarts due to memory spike and eviction. we see the spike when the wal folder size reach 2 GB ","> we see the spike when the wal folder size reach 2 GB\r\n\r\n> restarts due to memory spike\r\n\r\nIndicates that you need more memory provisioned to the Prometheus and has nothing to do with the WAL. WAL reaching 2GB during spike might just be the side effect of some background task that runs periodically that is taking a little more memory.\r\n\r\nHow bad is the spike? Do you have any dashboard to show the spike?","Hi Venkat ,\n\n    The node limitations are 16 gb , but you can see spike go beyond 19GB\n\n[image: image.png]\n\nOn Wed, Feb 9, 2022 at 5:11 PM Ganesh Vernekar ***@***.***>\nwrote:\n\n> we see the spike when the wal folder size reach 2 GB\n>\n> restarts due to memory spike\n>\n> Indicates that you need more memory provisioned to the Prometheus and has\n> nothing to do with the WAL. WAL reaching 2GB during spike might just be the\n> side effect of some background task that runs periodically that is taking a\n> little more memory.\n>\n> How bad is the spike? Do you have any dashboard to show the spike?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/10211#issuecomment-1033745839>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ADO6FZ2IKY7CWCMSTX4BY23U2JRYTANCNFSM5M2FSPFQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n\n\n-- \nThanks--\nPraveen Sam\n","<img width=\"1354\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/14541543\/153252423-5485a36d-5f1c-46b3-8a59-8a3434ad649d.png\">\r\n\r\nPlease find the spike in the snap","I have modified the title since it is not confirmed if WAL is the issue. Can you try giving more memory to Prometheus and see if it is able to handle the spikes? It would be useful to get a memory profile and the logs exactly when the spikes happen."],"labels":["kind\/more-info-needed"]},{"title":"Backfilling recording rules via API","body":"\r\n## Use case. Why is this important?\r\n\r\nFor the Service Level Objective project [Pyrra](https:\/\/github.com\/pyrra-dev\/pyrra) we are adding [support for availability recording rules](https:\/\/github.com\/pyrra-dev\/pyrra\/pull\/56) that pre-aggregate the number of failed and total requests over the objective's window (often like 4w).\r\n\r\nNow, if the Prometheus server is running for more than 4w and there's plenty of data available for the objective the displayed graph will only have the recording rule's data since it was created. This leaves users with a seemingly broken graph. \r\n\r\n![errorbudget](https:\/\/user-images.githubusercontent.com\/872251\/150957667-d11c80fe-fe8a-4cc2-81d7-7bbfcd247327.png)\r\n\r\n_Note: This Prometheus has all the data available, it's just that the recording rule was created less than 2 days ago._ \r\n\r\nThis experiences gets worse if users were to chance the objective's window from 4w to something else like 2w, since the underlying recording rules changes from `increase(http_requests_total[4w])` to `increase(http_requests_total[2w])` and it's name from `http_requests:increase4w` to `http_requests:increase2w`.\r\n\r\n## Proposal\r\n\r\nRather than creating some extra tooling as part of Pyrra to call the existing `promtool tsdb create-blocks-from rules` I want to propose adding a API to Prometheus itself to create these back filling blocks. \r\nIn my particular use case the back filling doesn't need to be instantly either and is fine to take a couple of minutes (depending on the amount of data). \r\n\r\nAs for the API I can imagine a RESTful admin API call containing such payload:\r\n```\r\n> GET \/api\/v1\/admin\/tsdb\/blocks\/rules HTTP\/1.1\r\n> Host: localhost:9090\r\n> User-Agent: curl\/7.81.0\r\n> Accept: *\/*\r\n> {\"start\": 1643106335, \"rules\": [\"http_requests:increase4w\"]}\r\n```\r\nThe response wouldn't be blocking until the blocks are created but could return `201 CREATED` and thus signal that the request was successfully queued.\r\nObviously the actual implementation is up for discussion and I don't have a strong opinion.\r\n\r\ncc @JessicaGreben @GayathriVenkatesh @roidelapluie ","comments":["Hey \ud83d\udc4b \r\nAre there any ongoing plans of implementing this backfill API?"],"labels":["priority\/Pmaybe","component\/rules","kind\/feature"]},{"title":"sigv4 remote write does not deduplicate slashes","body":"**What did you do?**\r\n\r\nConfigured Remote Write to Amazon Managed Prometheus using sigv4 creds presented by EKS.\r\n\r\n**What did you expect to see?**\r\n\r\nRemote writes to go through.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nRemote write failed 100% of the time with a sigv4 signature failure. I noticed that the URL had two slashes in it (due to templating). Once I fixed this, it worked. \r\n\r\nAccording to the sigv4 docs, this URI should have the slashes deduplicated.\r\n\r\n**Environment**\r\n\r\nKubernetes (EKS) running linux amd64\r\n\r\n* System information:\r\n\r\n```\r\nuname -srm\r\nLinux 5.4.162-86.275.amzn2.x86_64 x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\nBroken in build from `main`:\r\n```\r\n# git describe --tag\r\nv2.33.0-rc.1-10-ga4d320661\r\n```\r\n\r\nAnd at least\r\n`quay.io\/prometheus\/prometheus:v2.32.1@sha256:82252381f9645bfdb0d0cee6dbe05505d86b9a784d4d0004e8b731705fb0ce11`\r\n\r\n* Prometheus configuration file:\r\n```\r\n...\r\nremote_write:\r\n- sigv4:\r\n    region: us-east-2\r\n    url: https:\/\/aps-workspaces.us-east-2.amazonaws.com\/workspaces\/<REDACTED WORKSPACE ID>\/\/api\/v1\/remote_write\r\n...\r\n```\r\n\r\nNotice the double slash in the path.\r\n\r\n* Logs:\r\n```\r\nts=2022-01-25T00:11:44.163Z caller=dedupe.go:112 component=remote level=error remote_name=ddbb0e url=https:\/\/aps-workspaces.us-east-2.amazonaws.com\/workspaces\/<REDACTED WORKSPACE ID>\/\/api\/v1\/remote_write msg=\"non-recoverable error while sending metadata\" count=204 err=\"server returned HTTP status 403 Forbidden: {\\\"message\\\":\\\"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\\\n\\\\nThe Canonical String for this request should have been\\\\n'POST\\\\n\/workspaces\/<REDACTED WORKSPACE ID\/api\/v1\/remote_write\\\\n\\\\ncontent-encoding:snappy\\\\ncontent-type:application\/x-protobuf\\\\nhost:aps-workspaces.us-east-2.amazonaws.com\\\\nx-amz-date:20220125T001144Z\\\\nx-amz-security-token:<REDACTED>\"\r\n```\r\n\r\n* Hacks:\r\n\r\nI patched `common\/sigv4\/sigv4.go` to enable extra AWS SDK debugging:\r\n\r\n```\r\n\tsigner := signer.NewSigner(signerCreds)\r\n\tsigner.Logger = aws.NewDefaultLogger()\r\n\tsigner.Debug = aws.LogDebugWithSigning\r\n```\r\n\r\n**Guesses**\r\n\r\nCould be a bug in the SDK? I had a quick look and couldn't find any test cases specifically for duplicated slashes. https:\/\/github.com\/aws\/aws-sdk-go\/tree\/main\/aws\/signer\/v4 \r\n","comments":["The [docs](https:\/\/docs.aws.amazon.com\/sdk-for-go\/api\/aws\/signer\/v4\/#pkg-overview) seem to suggest some amount of escaping is up to the caller, but prometheus probably wouldn't be able to determine how much escaping to do generally :\/ So I guess it is kind of up to the user? Maybe just a warning in the docs would suffice?","According to the documentation, we should call http.EscapePath ourselves when doing the sigv4 sign.\r\n\r\nI am moving this issue to prometheus\/common as that is where the code lives.","Hello! Can I pick this up? @roidelapluie ","Yes! The code is at https:\/\/github.com\/prometheus\/common\/tree\/main\/sigv4","Don't we want to remove the extra double slash instead of escaping it? \r\nFrom the [AWS doc](https:\/\/docs.aws.amazon.com\/sdk-for-go\/api\/aws\/signer\/v4\/#pkg-overview), wouldn't `\"\/\/<hostname>\/<path>\"` correspond to `\/\/aps-workspaces.us-east-2.amazonaws.com\/workspaces\/...`? , or does it refer to `\/\/api\/v1\/remote_write...` instead?\r\n\r\nOr do we want to just escape the extra backslash in `<REDACTED WORKSPACE ID>\/\/api\/...`? as per this line from the docs:\r\n\r\n> AWS v4 signature validation requires that the canonical string's URI path element must be the URI escaped form of the HTTP request's path.\r\n\r\nIs this backslash coming from AWS itself and hence we don't want to remove it, but escape it instead? ","Right, my issue is specifically about deduplicating those double slashes, but [the AWS docs](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sigv4-create-canonical-request.html) give a more thorough explanation, and it's not limited to escaping:\r\n\r\n> Normalize URI paths according to [RFC 3986](https:\/\/tools.ietf.org\/html\/rfc3986). Remove redundant and relative path components.\r\n\r\n> Is this backslash coming from AWS itself and hence we don't want to remove it, but escape it instead?\r\n\r\nThis string comes from the user's config file, not AWS.\r\n\r\n\r\n","Not too experienced with AWS or Prometheus, so I looked at the [prometheus remote write docs](https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/configuration\/#remote_write), I'm guessing the URL that is provided is not generated via Prometheus itself, correct? So either the double slash would have to be removed in the config file itself, or we would normalize the URI as per the AWS docs. Don't think we should outright deduplicate the double slashes. \r\n\r\nPlease correct my understanding if wrong. In the end though I think the code change would be, as suggested by Julien, escaping while doing the sigv4 sign.","> So either the double slash would have to be removed in the config file itself, or we would normalize the URI as per the AWS docs. Don't think we should outright deduplicate the double slashes.\r\n\r\nYeah, I would say to be totally correct, there should be some sort of URI normalization, which includes normalizing the path (e.g.double slash deduplication, `\/blah\/.\/blah` to `\/blah\/blah`, etc.).\r\n\r\nHonestly, as a maintainer I might be tempted to say that the user should normalize the path in the config file. In this case, it'd be nice if the resultant error in the logs was a little more obvious and legible."],"labels":["kind\/enhancement","component\/remote storage","priority\/P3"]},{"title":"Proposal: report sample and series statistics to help analyze and monitor query complexity","body":"Hey Prometheus community, @alanprot and I work on Amazon Managed Service for Prometheus. We're trying to improve our own and customer visibility into how expensive queries are to execute.\r\n\r\nPrometheus already returns timing statistics about the query execution when the `stats=true` parameter is provided. When running many queries in parallel, especially in a multi-tenant environment on Cortex or Thanos, that timing can vary a lot for the same query. We're interested in metrics that should be the same across query runs for the same underlying data.\r\n\r\n## Proposal\r\n\r\nRecord and add two new fields to the statistics response to help understand the query complexity and the amount of work done by the Prometheus engine to run a query.\r\n\r\n* **Series Processed:** the number of series processed by the engine\r\n* **Samples Processed:** the number of samples processed by the engine\r\n\r\nThese new fields are stable - not affected by any external factor other than the query and samples ingested - and can be used to better explain the cost of distinct queries.\r\n\r\nWe'd also like to understand these stats over time in a range query, so we propose adding a new `stats=detailed` option that returns stats by step. This is especially helpful in a context like Cortex where a subset of steps of a cached query can contribute to a query result.\r\n\r\nWhat are your thoughts?\r\n\r\n## Examples\r\n\r\nLet's assume SerieA and SerieB has 5 labels and one sample per minute:\r\n\r\n    * q=SerieA\r\n        * InstantQuery: SeriesProcessed 5, SamplesProcessed=5\r\n        * Range Query with 10 steps: SeriesProcessed 50, SamplesProcessed=50\r\n    * q=max_over_time(SerieA[10m])\r\n        * SeriesProcessed 5, SamplesProcessed=50\r\n        * Range Query with 10 steps: SeriesProcessed 50, SamplesProcessed=500\r\n    * q=sum by (label) (max_over_time(SerieA}[10m]))\r\n        * SeriesProcessed 5, SamplesProcessed=50\r\n        * Range Query with 10 steps: SeriesProcessed 50, SamplesProcessed=500\r\n    * q=max_over_time(SerieA{label=\"unique\"}[10m])\r\n        * SeriesProcessed 1, SamplesProcessed=10\r\n        * Range Query with 10 steps: SeriesProcessed 10, SamplesProcessed=100\r\n    * q=sum(max_over_time(SerieA[10m])) + sum(max_over_time(SerieA[10m]))\r\n        * SeriesProcessed 10, SamplesProcessed=100\r\n        * Range Query with 10 steps: SeriesProcessed 100, SamplesProcessed=1000\r\n\r\n### range queries\r\n\r\nFor range queries with `stats=detailed` with the following underlying data:\r\n\r\nMetricA\r\n* Number of Series: 1 at 1642118820 and 4 after that\r\n* 2 samples per minute (scrape_interval=30s)\r\n\r\nOutput:\r\n```\r\nq=sum(serieA)&start=1642119240&end=1642119360&step=60\r\n\r\n{\r\n  \"status\": \"success\",\r\n  \"data\": {\r\n    \"resultType\": \"matrix\",\r\n    \"result\": [\r\n      {\r\n        \"metric\": {},\r\n        \"values\": [\r\n          [\r\n            1642118820,\r\n            \"6\"\r\n          ],\r\n          [\r\n            1642118880,\r\n            \"7\"\r\n          ],\r\n          [\r\n            1642118940,\r\n            \"6\"\r\n          ]\r\n        ]\r\n      }\r\n    ],\r\n    \"stats\": {\r\n      \"timings\": {\r\n        \"evalTotalTime\": 0.00502032,\r\n        \"resultSortTime\": 7.56e-7,\r\n        \"queryPreparationTime\": 0.000021759,\r\n        \"innerEvalTime\": 0.004988867,\r\n        \"execQueueTime\": 0.000044814,\r\n        \"execTotalTime\": 0.005070935\r\n      },\r\n      \"query\": {\r\n        \"steps\": {\r\n          \"1642118820\": {\r\n            \"samples\": 2,\r\n            \"series\": 1\r\n          },\r\n          \"1642118880\": {\r\n            \"samples\": 8,\r\n            \"series\": 4\r\n          },\r\n          \"1642118940\": {\r\n            \"samples\": 8,\r\n            \"series\": 4\r\n          }\r\n        },\r\n        \"total\": {\r\n          \"samples\": 18,\r\n          \"series\": 9\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nNote that there's a bit of subtlety to work out in the total semantics if we move forward here: should series be the number of unique series or the total for each bucket?","comments":["It looks like there are some past efforts in this space as well:\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/6668\r\nhttps:\/\/github.com\/prometheus\/prometheus\/pull\/6890","I'm always a fan of having more data. The step-level stats look a bit too much on the face of it, but as you say it would be optional.\r\n\r\nRelated: working with a different back-end as I do (Cortex), I also want to bring stats out which would not be known to Prometheus - e.g. how much data came from Cortex' cache.  So I would like the mechanism to be extensible and not tied to one struct as it is today.","@bboreham Yeah, agreed, extensibility seems important here. I'm planning on writing up a Cortex-side issue for this work as well (since of course we use Cortex too) and will have further thoughts there.","Hi, \r\n\r\nI just pushed a draft PR with the samples processed changes.","cc @moadz as you were working on a similar thing in Thanos","Hey @aughr, I've written a proposal for introducing a partitioned histogram metric that represents this exactly:\r\nhttps:\/\/github.com\/thanos-io\/thanos\/pull\/4932 \r\nhttps:\/\/github.com\/thanos-io\/thanos\/issues\/4895\r\n\r\nWhere my proposal differs is the need to record these stats as a metric for the purpose of measuring and enforcing response time SLO's on the `\/query_range` and `\/query` endpoints.  One key thing is accounting for the Proxy Store API's fan out of series selects across targets. Thanos will dedup timeseries post-select before promql eval, so it's important these stats reflect the true reach of a query. ","Interesting, thank you for pointing me to that, @moadz! Beyond the per-query \/ metric distinction, I think our goals for sample stats differ, but they can both co-exist. For our own operations at AWS, we might be interested in the true reach sample count. For our customers, though, we want to provide a consistent sample count for a given query whether an ingester in Cortex is down, compaction has happened, etc. That's the focus of this proposal.\r\n\r\nThat said, do you think it would be valuable for your use case to have both kinds of sample stats (true reach and de-duped) be in query stats if this proposal were to move forward? Would that make recording the metrics you want easier?","Without any opinion on the json payload format, having this stat makes sense.\r\n\r\nAlthough, I am not sure about the `Range Query with 10 steps: SeriesProcessed 50, SamplesProcessed=50` in the issue description. Intuitively feels like `SeriesProcessed` should be number of unique series touched overall during the range query. And not a sum of series touched across steps. Because the range query is internally optimised to only fetch the series once per range query. So here I would expect `SeriesProcessed` to be 5 if there were 5 distinct series.\r\n\r\ncc @roidelapluie","@aughr \r\n> That said, do you think it would be valuable for your use case to have both kinds of sample stats (true reach and de-duped) be in query stats if this proposal were to move forward? Would that make recording the metrics you want easier?\r\nNot quite as these would be changes to the Query API, not the Store API (Cortex & Thanos make use of direct selects on the TSDB as opposed to via the query path) \r\n\r\nThat being said, I do want to look into exposing these query stats via the Thanos query API as well. So any standard set here should be followed upstream in Cortex\/Thanos. \r\n\r\nI would also like to hear more about how you intend to use these additional stats. Will they be parsed\/persisted for analysis? Or is it simply for additional transparency to the user wrt query performance? \r\n"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/promql"]},{"title":"Tracking issue for promotion\/demotion of experimental features","body":"_The idea of this issue is to assign it to the upcoming release shepherd and use it to hand over state from one release shepherd to the next, updating this original post (rather than adding comments). If you know a better way of doing this, feel free to implement it and port the information here over to the better way._\r\n\r\n### `--scrape.adjust-timestamps` and `--scrape.timestamp-tolerance`\r\n\r\n__State:__ Blocked on decision for default value.\r\n\r\nThese are not feature flags, but regular, albeit hidden, flags, marked as \"Experimental. This flag will be removed in a future release.\". However, as per the dev-summit, the plan is to remove only `--scrape.adjust-timestamps`, as `--scrape.timestamp-tolerance` alone is enough to control the feature (`--scrape.timestamp-tolerance=0` will disable timestamp adjustment). Before we can do so, we want to find out a good default value (as the feature should be \"on by default\"). From the dev-summit notes:\r\n\r\n> CONSENSUS: We will promote `-scrape.timestamp-tolerance` to stable and remove `-scrape.adjust-timestamps`. Before we do so, we will try to find a reasonable default value and reach out to users about donating real-world data.\r\n\r\n### Feature flag `remote-write-receiver`\r\n\r\n__State:__ Deprecated.\r\n\r\nThis feature flag should be removed in v2.34. In v2.33, it creates a warning to use `--web.enable-remote-write-receiver` instead, but it still takes effect. In v2.34, it should be a no-op (i.e. the remote write receiver will be inactive if the feature flag is set but _not_ `--web.enable-remote-write-receiver`), and a corresponding warning should be issued.\r\n\r\n### Feature flag `memory-snapshot-on-shutdown`\r\n\r\n__State:__ To be vetted.\r\n\r\nTalk to @codesome . It was still too new to promote in v2.33. Also, it looks this should not just be promoted to \"always on\", but it might be useful to still give the user a choice (via a regular flag).\r\n\r\n### Feature flag `expand-external-labels`\r\n\r\n__State:__ To be vetted. \r\n\r\nNo longer blocked on #10122\/#10129. Probably should be deprecated in v2.35.\r\n\r\nOnce the issue above is resolved, this can be promoted to stable. However, as the behavior with the feature enabled can break existing setups, it still needs to be opted in explicitly (via a regular flag -- which can only be removed in v3.0.0).\r\n\r\n### Feature flag `new-service-discovery-manager`\r\n\r\n__State:__ To be vetted.\r\n\r\nThis feels fairly new still. We have to find out if it is stable enough. Once it is, we can probably promote this to \"always on\".\r\n\r\n_(Note that there are more experimental features, but for those, there is not state to hand over.)_","comments":["> If you know a better way of doing this, feel free to implement it and port the information here over to the better way.\r\n\r\nWe could do it as a project with different columns for \"To be vetted,\" \"Blocked,\" \"Deprecated,\" but this is probably fine.","\r\n> ### Feature flag `remote-write-receiver`\r\n> **State:** Deprecated.\r\n> \r\n> This feature flag should be removed in v2.34. In v2.33, it creates a warning to use `--web.enable-remote-write-receiver` instead, but it still takes effect. In v2.34, it should be a no-op (i.e. the remote write receiver will be inactive if the feature flag is set but _not_ `--web.enable-remote-write-receiver`), and a corresponding warning should be issued.\r\n\r\nThis seems too soon to me, we should wait a couple of releases before removing the feature flag.","> ### Feature flag `new-service-discovery-manager`\r\n> **State**: To be vetted.\r\n>\r\n> This feels fairly new still. We have to find out if it is stable enough. Once it is, we can probably promote this to \"always on\".\r\n\r\nMy impression of this flag was that it was more of a minor protection for a mechanism we didn't consider was too prone to failure, and given that we've only seen one bug (https:\/\/github.com\/prometheus\/prometheus\/issues\/9654) a day after the release, I think we should consider promoting this to stable sooner rather than later. Of course, I don't know how extensively it's been tested.","WRT using a GH project: Good idea. We can see how convoluted things get here.\r\n\r\nWRT deprecation timing (in this case remote-write-receiver): Given that feature flags are inherently volatile, I don't think we need to wait for longer than one minor release. IMHO it just increases the management overhead for us, while it won't really significantly reduce the number of users hit by surprise. Perhaps others could chime in on this so that we get an idea what practice to follow.","Since there was concern expressed around removing the remote-write-receiver flag in v2.34 I will leave it for another release. Does removing it in v2.35 sound ok to everyone? If so I will update this issue.\r\n\r\nI believe the bug @codesome needs to fix is related to memory-snapshot-on-shutdown so that should be left for at least another release as well."],"labels":["kind\/cleanup"]},{"title":"Store metadata for OpenMetrics _created metrics","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nDeployed an [example application](https:\/\/github.com\/yuriatgoogle\/stack-doctor\/blob\/master\/oss-o11y\/python\/frontend\/frontend.py) emitting Prometheus metrics in a Kubernetes cluster alongside a Prometheus deployment.\r\n\r\n**What did you expect to see?**\r\nAll metrics with properly formatted `HELP` and `TYPE` strings have their metadata available in the Prometheus UI and API.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nWhen querying for metrics in the graph UI, I am able to see the metric time series plotted. However, the metric metadata in the autocomplete tooltip is [missing](https:\/\/gist.github.com\/pintohutch\/35a15994510d3f5cccbf4cf5755a07b2#file-prom-graph-png).\r\n\r\nI went to the metadata API at `\/api\/v1\/metadata` and saw the metadata was [missing](https:\/\/gist.github.com\/pintohutch\/35a15994510d3f5cccbf4cf5755a07b2#file-prom-metadata-png) there as well.\r\n\r\nI verified the target metrics endpoint [looks to be](https:\/\/gist.github.com\/pintohutch\/35a15994510d3f5cccbf4cf5755a07b2#file-target-metrics-png) exporting properly-formatted metrics.\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n`Linux 5.4.144+ x86_64`\r\n\r\n* Prometheus version:\r\n\r\n```\r\nprometheus, version 2.32.1 (branch: HEAD, revision: 41f1a8125e664985dd30674e5bdf6b683eff5d32)\r\n  build user:       root@54b6dbd48b97\r\n  build date:       20211217-22:08:06\r\n  go version:       go1.17.5\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Alertmanager version:\r\n\r\nN\/A\r\n\r\n* Prometheus configuration file:\r\n```\r\nglobal:\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  evaluation_interval: 1m\r\nscrape_configs:\r\n- job_name: prometheus\r\n  honor_timestamps: true\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  static_configs:\r\n  - targets:\r\n    - localhost:9090\r\n- job_name: prom-example\r\n  honor_timestamps: true\r\n  scrape_interval: 30s\r\n  scrape_timeout: 10s\r\n  metrics_path: \/metrics\r\n  scheme: http\r\n  follow_redirects: true\r\n  relabel_configs:\r\n  - source_labels: [__meta_kubernetes_pod_label_app]\r\n    separator: ;\r\n    regex: o11y-demo\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_kubernetes_namespace]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: namespace\r\n    replacement: $1\r\n    action: replace\r\n  - source_labels: [__meta_kubernetes_pod_name, __meta_kubernetes_pod_container_port_name]\r\n    separator: ;\r\n    regex: (.+);(.+)\r\n    target_label: instance\r\n    replacement: $1:$2\r\n    action: replace\r\n  - source_labels: [__meta_kubernetes_pod_container_port_name]\r\n    separator: ;\r\n    regex: metrics\r\n    replacement: $1\r\n    action: keep\r\n  kubernetes_sd_configs:\r\n  - role: pod\r\n    kubeconfig_file: \"\"\r\n    follow_redirects: true\r\n```\r\n\r\n* Alertmanager configuration file:\r\nN\/A\r\n\r\n* Logs:\r\nN\/A\r\n\r\nPlease let me know if there's any more detail I can provide and thank you!","comments":["If a metric name ends with _created, it is very possible that we ignore it or remove the prefix. _created is a special suffix in OpenMetrics and is threated in a different way. Could you use another name and see if it improves the situation?","I'm the \"author\" of the sample app.  The metric name is `frontend_request_count_otel` - I'm not sure where the `_created` suffix is coming from....","In this case, this is exposing using the OpenMetrics format. In OpenMetrics, histograms get an extra metric which indicates the creation timestamp. We do not store metadata for _created metrics, which are metrics coming from a metricfamily. This is an enhancement, I will clarify the issue title.","So, I would be willing to work to add the `_created` timestamp into the `scrapeCache`: https:\/\/github.com\/prometheus\/prometheus\/blob\/0fa8469f76107dc12e949164fc174a4098aedb19\/scrape\/scrape.go#L879-L905\r\n\r\nAnd add a method to get it back for each metric. This is useful downstream (in OTel for ex.) and they currently maintain a separate map of the previous datapoint and current datapoint to figure out when a reset happened. I think us maintaining it `seriesCur` and `seriesPrev` would be much more efficient.\r\n\r\nA side-effect of this would be that `_created` wouldn't double the number of active series. If there is interest for this, I'd be happy to write up a design doc.","Sounds like a good idea in general.\r\n\r\nHowever, Prometheus has so far ingested the `_created` series is if they were normal series. While that's of little use generally, some users might, by now, rely on this behavior. So we need to gate the proposed behavior (store it in the scrape cache) behind a feature flag.\r\n\r\nI'm also a bit concerned to add yet another ad-hoc approach to metadata, still without any high-level plan how we want to treat metadata in general. We already have the [\/api\/v1\/metadata](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#querying-metric-metadata) and the [\/api\/v1\/targets\/metadata](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#querying-target-metadata) endpoints. The former is per metric name and per target. The latter is per metric name (but for all targets summarized). The `_created` timestamps needed to be retrieved per series, which is yet another way of accessing metadata. Would we need to add another endpoint? Then we had three, but none of those endpoints would be able to address _historical_ metadata (if I'm interested in the metadata of a metric long gone, or where the metadata has changed over time \u2013 with the latter being particularly interesting for the `_created` timestamp, which might change multiple times during the lifetime of a series).\r\n","This feels related to https:\/\/github.com\/prometheus\/prometheus\/issues\/6541\r\n\r\nIf we agree on using _created timestamp metrics in OM text format specially (and ignoring series), this issue is solved I presume."],"labels":["kind\/enhancement","priority\/P3","component\/scraping"]},{"title":"Error sending alert: http2: unsupported scheme","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nI've updated prometheus and alertmanager.\r\n\r\n**What did you expect to see?**\r\nalerts being sent to alertmanager.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nno alerts are sent to alertmanager and and error logged by prometheus.\r\n\r\n**Environment**\r\ndocker + docker-compose in an Ubuntu 20.04.3 LTS VM running on VMware vSphere ESXi.\r\n\r\nThere is a nginx installed on the Ubuntu host machine which provides https + reverse proxy for internal and external requests to service-name.domain.tld.\r\n\r\n* System information:\r\n\r\n\tLinux 5.4.0-94-generic x86_64\r\n\r\n* Prometheus version:\r\n\r\n\tfrom version v2.28.1 to v2.32.1\r\n\r\n* Alertmanager version:\r\n\r\n\tfrom version v0.22.2 to v0.23.0\r\n\r\n* Docker Compose file:\r\n```\r\nversion: '3.7'\r\n\r\nvolumes:\r\n  prometheus_data: {}\r\n\r\nnetworks:\r\n  default:\r\n    driver: bridge\r\n    ipam:\r\n      driver: default\r\n      config:\r\n        - subnet: 172.19.13.0\/24\r\n\r\n  front-tier:\r\n  default:\r\n    driver: bridge\r\n    ipam:\r\n      driver: default\r\n      config:\r\n        - subnet: 172.27.173.0\/24\r\n\r\n  back-tier:\r\n  default:\r\n    driver: bridge\r\n    ipam:\r\n      driver: default\r\n      config:\r\n        - subnet: 172.31.191.0\/24\r\n\r\nservices:\r\n  prometheus:\r\n#   https:\/\/hub.docker.com\/r\/prom\/prometheus\/tags\r\n    image: prom\/prometheus:v2.32.1\r\n    volumes:\r\n      - .\/prometheus\/:\/etc\/prometheus\/\r\n      - prometheus_data:\/prometheus\r\n    command:\r\n      - '--config.file=\/etc\/prometheus\/prometheus.yml'\r\n      - '--storage.tsdb.path=\/prometheus'\r\n      - '--storage.tsdb.retention.size=200GB'\r\n      - '--storage.tsdb.retention.time=5y'\r\n      - '--web.console.libraries=\/usr\/share\/prometheus\/console_libraries'\r\n      - '--web.console.templates=\/usr\/share\/prometheus\/consoles'\r\n      - '--web.external-url=https:\/\/prometheus.domain.tld\/'\r\n      - '--web.enable-lifecycle'\r\n      - '--web.enable-admin-api'\r\n      - '--enable-feature=promql-at-modifier'\r\n    ports:\r\n      - 9090:9090\r\n    networks:\r\n      - back-tier\r\n    restart: always\r\n    logging:\r\n      driver: json-file\r\n      options:\r\n        max-size: \"10m\"\r\n        max-file: \"3\"\r\n\r\n  alertmanager:\r\n#   https:\/\/hub.docker.com\/r\/prom\/alertmanager\/tags\r\n    image: prom\/alertmanager:v0.23.0\r\n    ports:\r\n      - 9093:9093\r\n    volumes:\r\n      - .\/alertmanager\/etc\/alertmanager\/:\/etc\/alertmanager\/\r\n      - .\/alertmanager\/data\/:\/data\/\r\n    networks:\r\n      - back-tier\r\n    restart: always\r\n    command:\r\n      - '--config.file=\/etc\/alertmanager\/config.yml'\r\n      - '--storage.path=\/data'\r\n      - '--web.external-url=https:\/\/alertmanager.domain.tld\/'\r\n```\r\n\r\n* Prometheus configuration file:\r\n```\r\nglobal:\r\n  scrape_interval:     15s # By default, scrape targets every 15 seconds.\r\n  evaluation_interval: 15s # By default, scrape targets every 15 seconds.\r\n  # scrape_timeout is set to the global default (10s).\r\n\r\n  # Attach these labels to any time series or alerts when communicating with\r\n  # external systems (federation, remote storage, Alertmanager).\r\n#  external_labels:\r\n#      monitor: 'my-project'\r\n\r\n# Load and evaluate rules in this file every 'evaluation_interval' seconds.\r\nrule_files:\r\n  - 'alerts\/*.yml'\r\n  # - \"first.rules\"\r\n  # - \"second.rules\"\r\n\r\n# alert\r\nalerting:\r\n  alertmanagers:\r\n  - scheme: http\r\n    proxy_url: \"https:\/\/alertmanager.domain.tld\"\r\n    static_configs:\r\n    - targets:\r\n      - \"alertmanager:9093\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n\r\n  - job_name: 'prometheus'\r\n    # Override the global default and scrape targets from this job every 5 seconds.\r\n    scrape_interval: 5s\r\n    scrape_timeout: 4s\r\n    static_configs:\r\n      - targets: ['localhost:9090']\r\n\r\n  - job_name: 'alertmanager'\r\n    scrape_interval: 10s\r\n    scrape_timeout: 9s\r\n    static_configs:\r\n      - targets: ['alertmanager:9093']\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\n# https:\/\/www.prometheus.io\/docs\/alerting\/latest\/configuration\r\n\r\nglobal:\r\n  smtp_smarthost: 'mailserver.domain.tld:25'\r\n  smtp_from: 'alertmanager@domain.tld'\r\n  smtp_hello: 'redacted.domain.tld'\r\n  smtp_require_tls: false\r\n  http_config:\r\n    proxy_url: 'https:\/\/alertmanager.domain.tld'\r\n\r\ntemplates:\r\n  - '\/etc\/alertmanager\/template\/*.tmpl'\r\n\r\n# A route block defines a node in a routing tree and its children.\r\n#\r\n# Every alert enters the routing tree at the configured top-level route, \r\n# which must match all alerts (i.e. not have any configured matchers). \r\n# It then traverses the child nodes. If continue is set to false, it stops \r\n# after the first matching child. \r\n# If continue is true on a matching node, the alert will continue matching \r\n# against subsequent siblings. \r\n# If an alert does not match any children of a node \r\n# (no matching child nodes, or none exist), the alert is handled based \r\n# on the configuration parameters of the current node.\r\nroute:\r\n\r\n  # The labels by which incoming alerts are grouped together. For example,\r\n  # multiple alerts coming in for cluster=A and alertname=LatencyHigh would\r\n  # be batched into a single group.\r\n  #\r\n  # To aggregate by all possible labels use the special value '...' as the sole label name, for example:\r\n  # group_by: ['...']\r\n  # This effectively disables aggregation entirely, passing through all\r\n  # alerts as-is. This is unlikely to be what you want, unless you have\r\n  # a very low alert volume or your upstream notification system performs\r\n  # its own grouping.\r\n  group_by: ['alertname', 'instance', 'service']\r\n\r\n  # How long to initially wait to send a notification for a group of alerts. \r\n  # Allows to wait for an inhibiting alert to arrive or collect more initial \r\n  # alerts for the same group. (Usually ~0s to few minutes.)\r\n  group_wait: 30s\r\n\r\n  # How long to wait before sending a notification about new alerts that are \r\n  # added to a group of alerts for which an initial notification has \r\n  # already been sent.\r\n  group_interval: 5m\r\n\r\n  # How long to wait before sending a notification again if it has \r\n  # already been sent successfully for an alert.\r\n  #repeat_interval: 8736h\r\n  receiver: mail-alerts\r\n\r\n  # Zero or more child routes.<LeftMouse>\r\n  # \r\n  # Routing tree visulizer \/ debugger:\r\n  # https:\/\/www.prometheus.io\/webtools\/alerting\/routing-tree-editor\/\r\n  routes:\r\n  - matchers:\r\n      - 'severity=\"critical\"'\r\n      - 'alertname=~\"HP_iLO_Unhealthy_Temparature|Temperature_RZ1|Temperature_RZ2|Temperature_RZ3|Temperature_USV_Room\"'\r\n    receiver: mail-alerts-critical-temperature\r\n  - match:\r\n      severity: critical\r\n    receiver: mail-alerts-critical\r\n  - match:\r\n      severity: warning\r\n    receiver: mail-alerts-warning\r\n  - match:\r\n      severity: test\r\n    receiver: tils-test-mail\r\n    repeat_interval: 3000m\r\n\r\n# An inhibition rule mutes an alert (target) matching a set of matchers \r\n# when an alert (source) exists that matches another set of matchers. \r\n# Both target and source alerts must have the same label values for \r\n# the label names in the equal list.\r\n#\r\n# Semantically, a missing label and a label with an empty value \r\n# are the same thing. \r\n# Therefore, if all the label names listed in equal are missing from \r\n# both the source and target alerts, the inhibition rule will apply.\r\n#\r\n# To prevent an alert from inhibiting itself, an alert that matches \r\n# both the target and the source side of a rule cannot be inhibited \r\n# by alerts for which the same is true (including itself). \r\n# However, we recommend to choose target and source matchers \r\n# in a way that alerts never match both sides. \r\n# It is much easier to reason about and does not trigger this special case.\r\ninhibit_rules:\r\n  # We use this to mute any warning-level notifications if the same alert is \r\n  # already critical.\r\n  - source_match:\r\n      severity: 'critical'\r\n    target_match:\r\n      severity: 'warning'\r\n    # Apply inhibition if the alertname is the same.\r\n    # CAUTION: \r\n    #   If all label names listed in `equal` are missing \r\n    #   from both the source and target alerts,\r\n    #   the inhibition rule will apply!\r\n    equal: ['alertname', 'instance', 'service']\r\n\r\n# Receiver is a named configuration of one or more notification integrations.\r\nreceivers:\r\n  - name: 'mail-alerts'\r\n    email_configs:\r\n      - to: 'redacted@domain.tld'\r\n        send_resolved: true\r\n  - name: 'mail-alerts-critical'\r\n    email_configs:\r\n      - to: 'redacted@domain.tld'\r\n        send_resolved: true\r\n  - name: 'mail-alerts-critical-temperature'\r\n    email_configs:\r\n      - to: 'alerts-critical@domain.tld'\r\n        from: 'redacted@domain.tld'\r\n        send_resolved: true\r\n  - name: 'mail-alerts-warning'\r\n    email_configs:\r\n      - to: 'redacted@domain.tld'\r\n        send_resolved: true\r\n  - name: 'tils-test-mail'\r\n    email_configs:\r\n     - to: 'redacted@domain.tld' \r\n```\r\n\r\n\r\n* Logs:\r\n** Alertmanager\r\n```\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.667Z caller=main.go:225 msg=\"Starting Alertmanager\" version=\"(version=0.23.0, branch=HEAD, revision=61046b17771a57cfd4c4a51be370ab930a4d7d54)\"\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.667Z caller=main.go:226 build_context=\"(go=go1.16.7, user=root@e21a959be8d2, date=20210825-10:48:55)\"\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.668Z caller=cluster.go:184 component=cluster msg=\"setting advertise address explicitly\" addr=172.18.0.7 port=9094\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.671Z caller=cluster.go:671 component=cluster msg=\"Waiting for gossip to settle...\" interval=2s\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.713Z caller=coordinator.go:113 component=configuration msg=\"Loading configuration file\" file=\/etc\/alertmanager\/config.yml\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.714Z caller=coordinator.go:126 component=configuration msg=\"Completed loading of configuration file\" file=\/etc\/alertmanager\/config.yml\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.717Z caller=main.go:518 msg=Listening address=:9093\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:01.718Z caller=tls_config.go:191 msg=\"TLS is disabled.\" http2=false\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:03.672Z caller=cluster.go:696 component=cluster msg=\"gossip not settled\" polls=0 before=0 now=1 elapsed=2.001061322s\r\nalertmanager_1              | level=info ts=2022-01-13T10:52:11.674Z caller=cluster.go:688 component=cluster msg=\"gossip settled; proceeding\" elapsed=10.002424442s\r\n```\r\n\r\n** Prometheus\r\n```\r\nprometheus_1                | ts=2022-01-12T14:02:20.367Z caller=main.go:171 level=info msg=\"Experimental promql-at-modifier enabled\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.388Z caller=main.go:515 level=info msg=\"Starting Prometheus\" version=\"(version=2.32.1, branch=HEAD, revision=41f1a8125e664985dd30674e5bdf6b683eff5d32)\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.388Z caller=main.go:520 level=info build_context=\"(go=go1.17.5, user=root@54b6dbd48b97, date=20211217-22:08:06)\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.388Z caller=main.go:521 level=info host_details=\"(Linux 5.4.0-94-generic #106-Ubuntu SMP Thu Jan 6 23:58:14 UTC 2022 x86_64 7388d9fb57e3 (none))\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.388Z caller=main.go:522 level=info fd_limits=\"(soft=1048576, hard=1048576)\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.388Z caller=main.go:523 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.399Z caller=web.go:570 level=info component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nprometheus_1                | ts=2022-01-12T14:02:20.420Z caller=main.go:924 level=info msg=\"Starting TSDB ...\"\r\nprometheus_1                | ts=2022-01-12T14:02:20.422Z caller=tls_config.go:195 level=info component=web msg=\"TLS is disabled.\" http2=false\r\nprometheus_1                | ts=2022-01-12T14:02:20.453Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1628877600000 maxt=1630627200000 ulid=01FEMSK0DPKSPK61B3N5NV81KW\r\nprometheus_1                | ts=2022-01-12T14:02:20.472Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1630627200798 maxt=1632376800000 ulid=01FG95GBM38BPARV9N0KYM4W4C\r\nprometheus_1                | ts=2022-01-12T14:02:20.487Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1632376800041 maxt=1634126400000 ulid=01FHX9SBN5YPSY96SHT79TC5N2\r\nprometheus_1                | ts=2022-01-12T14:02:20.504Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1634126400238 maxt=1635876000000 ulid=01FKHE8NED6Z7C93GAATRBERZX\r\nprometheus_1                | ts=2022-01-12T14:02:20.525Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1635876000053 maxt=1637625600000 ulid=01FN5JSJEB0H78ZT4SWX6RAC7N\r\nprometheus_1                | ts=2022-01-12T14:02:20.537Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1637625600217 maxt=1639375200000 ulid=01FPSQB7WB1R16X4DZJTXQY5QA\r\nprometheus_1                | ts=2022-01-12T14:02:20.545Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1639375200239 maxt=1641124800000 ulid=01FRDVWP47TH57SG8EMB9PT9NC\r\nprometheus_1                | ts=2022-01-12T14:02:20.555Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641124800283 maxt=1641708000000 ulid=01FRZ7ZRC6NS8NXNVPT38T2XRJ\r\nprometheus_1                | ts=2022-01-12T14:02:20.565Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641708000283 maxt=1641902400000 ulid=01FS51BDM8HAEJ11VM5YPADC22\r\nprometheus_1                | ts=2022-01-12T14:02:20.578Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641967200250 maxt=1641974400000 ulid=01FS6R7WXGQAWGHR896BP9DYC1\r\nprometheus_1                | ts=2022-01-12T14:02:20.592Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641974400250 maxt=1641981600000 ulid=01FS6Z3M5FS2Z37NHJ4755MR0Z\r\nprometheus_1                | ts=2022-01-12T14:02:20.597Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641902400250 maxt=1641967200000 ulid=01FS6Z4FNG91MT4RD6NGPHV8Z9\r\nprometheus_1                | ts=2022-01-12T14:02:20.604Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1641981600250 maxt=1641988800000 ulid=01FS75ZBE92JMGSDY3SD7VQB3S\r\nprometheus_1                | ts=2022-01-12T14:02:25.347Z caller=head.go:488 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nprometheus_1                | ts=2022-01-12T14:02:25.806Z caller=head.go:522 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=459.603976ms\r\nprometheus_1                | ts=2022-01-12T14:02:25.806Z caller=head.go:528 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nprometheus_1                | ts=2022-01-12T14:04:40.966Z caller=head.go:564 level=info component=tsdb msg=\"WAL checkpoint loaded\"\r\nprometheus_1                | ts=2022-01-12T14:04:46.848Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17459 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:04:54.006Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17460 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:04:59.462Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17461 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:05:08.929Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17462 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:05:10.172Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17463 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:05:10.173Z caller=head.go:599 level=info component=tsdb msg=\"WAL segment loaded\" segment=17464 maxSegment=17464\r\nprometheus_1                | ts=2022-01-12T14:05:10.173Z caller=head.go:605 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=2m15.159957052s wal_replay_duration=29.20616551s total_replay_duration=2m44.825789043s\r\nprometheus_1                | ts=2022-01-12T14:05:10.908Z caller=main.go:945 level=info fs_type=EXT4_SUPER_MAGIC\r\nprometheus_1                | ts=2022-01-12T14:05:10.909Z caller=main.go:948 level=info msg=\"TSDB started\"\r\nprometheus_1                | ts=2022-01-12T14:05:10.909Z caller=main.go:1129 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nprometheus_1                | ts=2022-01-12T14:05:11.125Z caller=main.go:1166 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=216.873395ms db_storage=1.723\u00b5s remote_storage=1.968\u00b5s web_handler=680ns query_engine=1.303\u00b5s scrape=291.882\u00b5s scrape_sd=591.151\u00b5s notify=28.149\u00b5s notify_sd=16.694\u00b5s rules=212.727311ms\r\nprometheus_1                | ts=2022-01-12T14:05:11.125Z caller=main.go:897 level=info msg=\"Server is ready to receive web requests.\"\r\nprometheus_1                | ts=2022-01-12T14:06:21.791Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:07:36.787Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:08:51.787Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:10:06.786Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:10:39.063Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:11:14.319Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:11:21.788Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:11:54.068Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:12:29.319Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:12:36.787Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:12:36.790Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:13:09.064Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:13:44.319Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:13:51.788Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:13:51.791Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\nprometheus_1                | ts=2022-01-12T14:14:24.064Z caller=notifier.go:526 level=error component=notifier alertmanager=http:\/\/alertmanager:9093\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/alertmanager:9093\/api\/v2\/alerts\\\": http2: unsupported scheme\"\r\n```","comments":["Could you try with Prometheus running with the environment variable `PROMETHEUS_COMMON_DISABLE_HTTP2=x` ? That would help a lot.","Thanks, that resolved the issue for me.\r\nThe error message does not appear anymore and alerts are delivered to altermanager again.","This is more a workaround that a solution. Is there any reverse proxies between Prometheus and alertmanager?","No reverse proxy. From Prometheus to Alertmanager it's a direct call. From one docker container to the other.\r\n\r\nDoesn't the use of HTTP\/2 imply \/ require TLS in this case?","proxy_url: \"https:\/\/alertmanager.domain.tld\"\r\n\r\nYou are using TLS with the proxy.","I've removed proxy_url and the PROMETHEUS_COMMON_DISABLE_HTTP2 env variable. Everything is still working as expected. Thanks :)","This still requires investigation from my side, reopening.","OK, let me know if i can help with anything :)"],"labels":["kind\/more-info-needed"]},{"title":"Docker SD missing service: and container: networking modes","body":"**What did you do?**\r\nUsing Docker Compose, I have two containers, summarized here:\r\n\r\n```yaml\r\n- container_name: mysql\r\n  hostname: mysql\r\n  image: mysql:5\r\n  networks:\r\n    private: {}\r\n\r\n- container_name: exporter__mysql\r\n  depends_on:\r\n    - mysql\r\n  image: prom\/mysqld-exporter\r\n  network_mode: service:mysql\r\n```\r\n\r\nMy goal is to have a Pod-like ability to hit the `mysql` container on both ports 3306 (MySQL traffic), and 9104 (exporter traffic).\r\n\r\n**What did you expect to see?**\r\nThat `exporter__mysql` would show up as discoverable within Prometheus's Service Discovery, which I could then further filter labels and select.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nWhen I view all containers discovered by Prometheus using the Docker socket, it does not include the `exporter__mysql` container. I've tried both `network_mode: service:mysql`, as well as `network_mode: container:mysql` and neither work. If I move `exporter__mysql` to the `private` network in which `mysql` is running (`network: private`), then it shows up in Prometheus Docker socket discovery.\r\n\r\nI thought I might be running into #7684, which indicates that containers need to expose ports:\r\n> It looks like your services are not exporting any ports, which is why they do not appear in service discovery.\r\n\r\nBut I confirmed that the running container is exposing 9104 and that I could hit it using the `mysql` hostname:\r\n```shell\r\n$ docker inspect exporter__mysql | jq -r '.[].Config.ExposedPorts'\r\n{\r\n  \"9104\/tcp\": {}\r\n}\r\n\r\n$ docker run -it --rm --net private alpine wget -qO- mysql:9104\/metrics | wc -l\r\n1920\r\n```\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n```shell\r\n$ uname -srm\r\nLinux 5.4.0-91-generic x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```shell\r\n$ docker exec -it prometheus prometheus --version\r\nprometheus, version 2.32.1 (branch: HEAD, revision: 41f1a8125e664985dd30674e5bdf6b683eff5d32)\r\n  build user:       root@54b6dbd48b97\r\n  build date:       20211217-22:08:06\r\n  go version:       go1.17.5\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Prometheus configuration file:\r\n\r\n```yaml\r\nglobal:\r\n  scrape_interval:     15s\r\n  evaluation_interval: 15s\r\n\r\nscrape_configs:\r\n  - job_name: 'docker-containers'\r\n    docker_sd_configs:\r\n      - host: unix:\/\/\/var\/run\/docker.sock\r\n    relabel_configs:\r\n      # Select containers whose name is prefixed by `exporter__`.\r\n      - source_labels: [__meta_docker_container_name]\r\n        regex: \/exporter__.+ # For some reason, container names begin with a '\/' character.\r\n        action: keep\r\n      # Create new label named 'container' with the value from the meta container name label.\r\n      - source_labels: [__meta_docker_container_name]\r\n        target_label: container\r\n        regex: \/(.+)\r\n```\r\n\r\nNote: The above config works for other exporters which aren't tied to specific services. I am successfully running and scraping metrics for the Node and cAdvisor exporters. The issue is that the Docker socket service discovery is outright missing containers who share network namespaces with other containers.\r\n\r\n* Logs:\r\n\r\nNo issues:\r\n```\r\nts=2022-01-08T08:49:17.468Z caller=main.go:1129 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nts=2022-01-08T08:49:17.469Z caller=main.go:1166 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=773.879\u00b5s db_storage=1.63\u00b5s remote_storage=1.563\u00b5s web_handler=679ns query_engine=1.434\u00b5s scrape=333.957\u00b5s scrape_sd=74.472\u00b5s notify=11.014\u00b5s notify_sd=6.806\u00b5s rules=1.385\u00b5s\r\n```\r\n","comments":["Is there any chance that you could dump the http requests between Docker and Prometheus?","@roidelapluie, I can try. Do you have thoughts on this? I'm thinking to just make either a `nc` or `socat` socket, change Prometheus to point at that TCP endpoint, dump the data to a file, and then filter the data in Wireshark or something. Is there anything more straight forward? I was looking at just `strace`'ing the `prometheus` process, but I saw a bunch of byte-like data and I'm not sure how to interpret it, or if it's something that `strace` can output correctly.","You could enable the http listener and use Wireshark","I finally got around to dumping this information. I will skip the `tcpdump` set up for now as I believe we can focus on the relevant data in the response from the Docker daemon. (Let me know if you want these details.)\r\n\r\n---\r\n\r\nI am including three containers from the Docker daemon response to Prometheus:\r\n* `mysql`: Actual MySQL container.\r\n* `exporter__mysqld`: The Prometheus MySQL exporter, associated with the `mysql` container's network.\r\n* `exporter__mysql`: A simple container that proxies connections to `exporter__mysqld`.\r\n\r\nRestating the problem:\r\n* `exporter__mysqld` does expose scraped metrics at `http:\/\/mysql:9104\/metrics`, but Prometheus doesn't list it in Service Discovery (or Targets).\r\n* `exporter__mysql` is a separate container in my `private` network which just proxies to `http:\/\/mysql:9104\/metrics`, and Prometheus does list it in Service Discovery (and Targets).\r\n\r\nThis is how I've been able to monitor my DB up until now, I've just been using `exporter__mysql`. I'd like to remove this container and have Prometheus \"see\" `exporter__mysqld`.\r\n\r\nDocker daemon response to Prometheus:\r\n\r\n<details>\r\n  <summary><code>mysql<\/code><\/summary>\r\n\r\n```json\r\n{\r\n  \"Id\": \"b3141ee0925b9075c75dfce62673ef8dd17f60d15f65cf34b670fd8d9010bc42\",\r\n  \"Names\": [\r\n    \"\/mysql\"\r\n  ],\r\n  \"Image\": \"mysql:5\",\r\n  \"ImageID\": \"sha256:42f82e150ec28054e528d2de42299225b0985530bc7e2f688941e5e323b372f3\",\r\n  \"Command\": \"docker-entrypoint.sh mysqld\",\r\n  \"Created\": 1643090826,\r\n  \"Ports\": [\r\n    {\r\n      \"PrivatePort\": 3306,\r\n      \"Type\": \"tcp\"\r\n    },\r\n    {\r\n      \"PrivatePort\": 33060,\r\n      \"Type\": \"tcp\"\r\n    }\r\n  ],\r\n  \"Labels\": {\r\n    \"_ServiceDirectory\": \"mysql\",\r\n    \"com.docker.compose.config-hash\": \"d73015df2d6348ce790e352370149c2201d917e07be58d035dccc8592cf75e5a\",\r\n    \"com.docker.compose.container-number\": \"1\",\r\n    \"com.docker.compose.oneoff\": \"False\",\r\n    \"com.docker.compose.project\": \"REDACTED\",\r\n    \"com.docker.compose.project.config_files\": \"REDACTED\",\r\n    \"com.docker.compose.project.working_dir\": \"REDACTED\",\r\n    \"com.docker.compose.service\": \"mysql\",\r\n    \"com.docker.compose.version\": \"1.28.0\"\r\n  },\r\n  \"State\": \"running\",\r\n  \"Status\": \"Up 10 days\",\r\n  \"HostConfig\": {\r\n    \"NetworkMode\": \"private\"\r\n  },\r\n  \"NetworkSettings\": {\r\n    \"Networks\": {\r\n      \"private\": {\r\n        \"IPAMConfig\": null,\r\n        \"Links\": null,\r\n        \"Aliases\": null,\r\n        \"NetworkID\": \"94389f46d90210bf321c13edef8290ad64d177f385affdd614951a55795dd0b8\",\r\n        \"EndpointID\": \"421981ddc5ea37102ceaacded364c53332a4e1d16ba4802301b39bb56d2ca211\",\r\n        \"Gateway\": \"192.168.224.1\",\r\n        \"IPAddress\": \"192.168.224.44\",\r\n        \"IPPrefixLen\": 24,\r\n        \"IPv6Gateway\": \"\",\r\n        \"GlobalIPv6Address\": \"\",\r\n        \"GlobalIPv6PrefixLen\": 0,\r\n        \"MacAddress\": \"02:42:c0:a8:e0:2c\",\r\n        \"DriverOpts\": null\r\n      }\r\n    }\r\n  },\r\n  \"Mounts\": [\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/docker\/mysql\/data\",\r\n      \"Destination\": \"\/var\/lib\/mysql\",\r\n      \"Mode\": \"rw\",\r\n      \"RW\": true,\r\n      \"Propagation\": \"rprivate\"\r\n    }\r\n  ]\r\n}\r\n```\r\n<\/details>\r\n\r\n<details>\r\n  <summary><code>exporter__mysqld<\/code><\/summary>\r\n\r\n```json\r\n{\r\n  \"Id\": \"b2fe1b66a5a5dc474e74697f83ed47a80e57c6c50f94d0c2b2b250dd31bfb698\",\r\n  \"Names\": [\r\n    \"\/exporter__mysqld\"\r\n  ],\r\n  \"Image\": \"prom\/mysqld-exporter\",\r\n  \"ImageID\": \"sha256:6af217645de36e69042e63c3f784f10fcee300b5fa9ee768c55f5f0a08a85bd6\",\r\n  \"Command\": \"\/bin\/mysqld_exporter\",\r\n  \"Created\": 1643090868,\r\n  \"Ports\": [],\r\n  \"Labels\": {\r\n    \"_ServiceDirectory\": \"prometheus\",\r\n    \"com.docker.compose.config-hash\": \"54109a67f77211352685164f37f391d3a80ecc2197e0111815603b951e17930a\",\r\n    \"com.docker.compose.container-number\": \"1\",\r\n    \"com.docker.compose.oneoff\": \"False\",\r\n    \"com.docker.compose.project\": \"REDACTED\",\r\n    \"com.docker.compose.project.config_files\": \"REDACTED\",\r\n    \"com.docker.compose.project.working_dir\": \"REDACTED\",\r\n    \"com.docker.compose.service\": \"exporter__mysqld\",\r\n    \"com.docker.compose.version\": \"1.28.0\",\r\n    \"maintainer\": \"The Prometheus Authors <prometheus-developers@googlegroups.com>\"\r\n  },\r\n  \"State\": \"running\",\r\n  \"Status\": \"Up 13 minutes\",\r\n  \"HostConfig\": {\r\n    \"NetworkMode\": \"container:b3141ee0925b9075c75dfce62673ef8dd17f60d15f65cf34b670fd8d9010bc42\"\r\n  },\r\n  \"NetworkSettings\": {\r\n    \"Networks\": {}\r\n  },\r\n  \"Mounts\": []\r\n}\r\n```\r\n<\/details>\r\n\r\n<details>\r\n  <summary><code>exporter__mysql<\/code><\/summary>\r\n\r\n```json\r\n{\r\n  \"Id\": \"821c10c99be8b22748f5d89a16a14f51589f305fac6eb079a8f12f2276eeef23\",\r\n  \"Names\": [\r\n    \"\/exporter__mysql\"\r\n  ],\r\n  \"Image\": \"alpine\/socat\",\r\n  \"ImageID\": \"sha256:747019af87763d9bbe3e1645e0a714828176314503224192132b1648ccd2e991\",\r\n  \"Command\": \"socat TCP-LISTEN:9104,reuseaddr,fork,su=nobody TCP:mysql:9104\",\r\n  \"Created\": 1643090825,\r\n  \"Ports\": [\r\n    {\r\n      \"PrivatePort\": 9104,\r\n      \"Type\": \"tcp\"\r\n    }\r\n  ],\r\n  \"Labels\": {\r\n    \"_ServiceDirectory\": \"prometheus\",\r\n    \"com.docker.compose.config-hash\": \"93a18b0f7981b48eb7e6228343ac7aea84d0d7bb9d631131c3f3608cb4b6eb10\",\r\n    \"com.docker.compose.container-number\": \"1\",\r\n    \"com.docker.compose.oneoff\": \"False\",\r\n    \"com.docker.compose.project\": \"REDACTED\",\r\n    \"com.docker.compose.project.config_files\": \"REDACTED\",\r\n    \"com.docker.compose.project.working_dir\": \"REDACTED\",\r\n    \"com.docker.compose.service\": \"exporter__mysql\",\r\n    \"com.docker.compose.version\": \"1.28.0\"\r\n  },\r\n  \"State\": \"running\",\r\n  \"Status\": \"Up 10 days\",\r\n  \"HostConfig\": {\r\n    \"NetworkMode\": \"private\"\r\n  },\r\n  \"NetworkSettings\": {\r\n    \"Networks\": {\r\n      \"private\": {\r\n        \"IPAMConfig\": null,\r\n        \"Links\": null,\r\n        \"Aliases\": null,\r\n        \"NetworkID\": \"94389f46d90210bf321c13edef8290ad64d177f385affdd614951a55795dd0b8\",\r\n        \"EndpointID\": \"9869dba83d73006c0f55660c6d91744dc05e8bdc0af47c263fdafb4055714251\",\r\n        \"Gateway\": \"192.168.224.1\",\r\n        \"IPAddress\": \"192.168.224.41\",\r\n        \"IPPrefixLen\": 24,\r\n        \"IPv6Gateway\": \"\",\r\n        \"GlobalIPv6Address\": \"\",\r\n        \"GlobalIPv6PrefixLen\": 0,\r\n        \"MacAddress\": \"02:42:c0:a8:e0:29\",\r\n        \"DriverOpts\": null\r\n      }\r\n    }\r\n  },\r\n  \"Mounts\": []\r\n}\r\n```\r\n<\/details>\r\n\r\n---\r\n\r\nSo the fact that the Docker daemon is responding with (accurate) data for `exporter__mysqld`, I think it's either a bug with Prometheus, or something else in my config is causing this to not show up under Service Discovery (or Targets).","@roidelapluie Would you take a look at https:\/\/github.com\/prometheus\/prometheus\/pull\/10490 which should closes this issue?","I experience the same issue, are there any news?","I have the same issue.Is the issue fixed?"],"labels":["kind\/bug","component\/service discovery","priority\/P3"]},{"title":"Add visual emphasis to hovered-over lines in graphs","body":"This idea stems from the open (as of Jan 6 2022) issue about [some colors not being visible in the React UI](https:\/\/github.com\/prometheus\/prometheus\/issues\/8256) when it comes to graphs.  \r\n\r\nWhen a line in the graph is hovered over, there would be added emphasis such as\r\n\r\n- making the line thicker\r\n- adding a shadow or glow to the line that is a stark contrast to the background\r\n\r\n### Why is this important?\r\nThis ensures that despite potential issues with generated colors being not visible enough against the background, colors being too similar in lines, or having a large amount of lines, a line can still become clearly visible if someone hovers over it.\r\n\r\nThis is very important for accessibility, as simply having a better color palette may not be enough for those with color blindness or other vision problems.","comments":["Regarding https:\/\/github.com\/prometheus\/prometheus\/issues\/8256#issuecomment-1007518397: Correct, Flot (https:\/\/github.com\/flot\/flot, the charting library we use) uses `<canvas>` to draw things, so we basically have to go through Flot's plugin and event handling system to change things in there. You could register an event handler for the `plothover` event on a series like we do for the `plotclick` and `plotselected` events here: https:\/\/github.com\/prometheus\/prometheus\/blob\/931acc3ee8f0f4e0eac68b4a2c4d880df7d82944\/web\/ui\/react-app\/src\/pages\/graph\/Graph.tsx#L109-L132. When hovering over a series, the third argument to the event handler will contain some information about the series (you'll have to check out with `console.log(...)` etc. what exactly is in there and how to correlate it with our series data).\r\n\r\nI'd imagine you could then somehow modify the chart data based on the hover info somehow, but I'm not sure if we already make it easy to access a Flot series' line width. I see some mentions of width on http:\/\/www.flotcharts.org\/flot\/API\/, but haven't looked closer yet, to be honest.","@juliusv Thank you so much!  This helped me a lot.  \r\nSo far I've successfully been able to add the event handler in the way you described which calls this function I added to prometheus\/web\/ui\/react-app\/src\/pages\/graph\/GraphHelpers.ts.\r\n\r\nCurrently this is able to make the line that is being hovered over bolder than the others.\r\n\r\n```typescript\r\nexport const emphasizeLine = (chart: jquery.flot.plot, item: any): void => {\r\n  const defaultLineWidth = chart.getOptions().series.points.lineWidth;\r\n  const boldLineWidth = defaultLineWidth + 3;\r\n  \/\/ Revert any lines that are currently emphasized\r\n  chart.getData().forEach((line) => {\r\n    if (line.lines) {\r\n      line.lines.lineWidth = defaultLineWidth;\r\n    }\r\n  });\r\n  \/\/ If a line is being hovered over, emphasize it\r\n  if (item) {\r\n    item.series.lines.lineWidth = boldLineWidth;\r\n  }\r\n  chart.draw();\r\n};\r\n```\r\nThe code design & organization is not ideal but it runs smoothly.  For example, `defaultLineWidth` and `boldLineWidth` probably don't need to be defined every time this is run but I'm not 100% sure on where it should be moved to so I'm leaving it as is for now.\r\n\r\nNext I'm going to work on figuring out how to add a shadow\/highlight to the emphasized line.","I've come to a crossroads with this issue and I'm not sure what to do.  The lines in the graphs are all part of a single HTML `canvas` element.  It is [possible to add a shadow to a line drawn](https:\/\/www.w3resource.com\/html5-canvas\/html5-canvas-shadow.php) in a `canvas` element, but I would have to add the shadow _after_ the line has been started to be drawn and _prior_ to the finalization of the line being drawn.  In our case, both of these actions are done in the jquery.flot.js file.  I'm not sure if I would be able to do this outside of the jquery.flot.js file unless I write a lot of redundant code.  \r\n\r\nI will continue to mull it over this weekend, perhaps I can draw a transparent line with a shadow over the \"emphasized\" line?  There's a bit of math that's done in jquery.flot.js so again this may still add some redundancy.  \u00af\\_(\u30c4)_\/\u00af ","@alisakotliarova Oh that's great that you already got the line width working! One thing to test is whether it still feels roughly as fast with that change as without, for a graph that shows ~100 series or so. We want to be careful about introducing anything that slows things down too much, but hopefully it'll be fine.\r\n\r\nAs for the extra emphasis, I agree that's more tricky. If we have to actually do custom canvas operations (not through Flot), I would tend to say that the extra code complexity would not be worth it. But emphasis through line width (and maybe highlighting the series color somewhat as well) might be enough?","@juliusv Thank you!  I have a couple of updates\/responses to your comment.\r\n\r\nI noticed that there is actually a mechanism that emphasizes lines already.  It's triggered when you hover over a series from the list that appears below the graph.  It appears that this emphasis makes the line corresponding to the series be the topmost line and it dims the other lines in the graphs.  As it is, this is certainly helpful for finding a line (especially when multiple lines are identical) but I still find myself struggling to find the highlighted line sometimes.  So I think it would still be helpful to add bolding to the lines.\r\nMy next goal is to try to track down this behavior (in the code) so I can add line-boldening to it and see if I can trigger it when the line is being hovered over as well.\r\n\r\nAs far as testing with ~100 series, do you have any pointers on how I can test that?  I honestly have never used Prometheus before so I don\u2019t have anything readily available to test with, I\u2019ve just been using the setup I made with the tutorial which only shows a few lines.","> I noticed that there is actually a mechanism that emphasizes lines already. It's triggered when you hover over a series from the list that appears below the graph.\r\n\r\nRight! Forgot about that :)\r\n\r\n> So I think it would still be helpful to add bolding to the lines.\r\nMy next goal is to try to track down this behavior (in the code) so I can add line-boldening to it and see if I can trigger it when the line is being hovered over as well.\r\n\r\nThat behavior is at https:\/\/github.com\/prometheus\/prometheus\/blob\/2f4289a3bfaca725a14f41e30f1c91de92936242\/web\/ui\/react-app\/src\/pages\/graph\/Graph.tsx#L187-L197 and https:\/\/github.com\/prometheus\/prometheus\/blob\/2f4289a3bfaca725a14f41e30f1c91de92936242\/web\/ui\/react-app\/src\/pages\/graph\/GraphHelpers.ts#L70-L78\r\n\r\nIt just sets the `color`, so maybe it's doable to add setting the `width` in there as well.\r\n\r\n> As far as testing with ~100 series, do you have any pointers on how I can test that? I honestly have never used Prometheus before so I don\u2019t have anything readily available to test with, I\u2019ve just been using the setup I made with the tutorial which only shows a few lines.\r\n\r\nYes, you can set your Prometheus config to this, scraping some public demo service instances:\r\n\r\n```yml\r\nglobal:\r\n  scrape_interval: 5s\r\n\r\nscrape_configs:\r\n- job_name: 'demo'\r\n  static_configs:\r\n    - targets:\r\n      - 'demo.promlabs.com:10000'\r\n      - 'demo.promlabs.com:10001'\r\n      - 'demo.promlabs.com:10002'\r\n```\r\n\r\n...and then you could query for example for:\r\n\r\n```\r\nrate(demo_api_request_duration_seconds_bucket[5m])`\r\n```\r\n\r\nThat gives you 702 series.","@juliusv Thank you for the help!  I ran it with that and it's working just fine on my end.  \r\n\r\nLet me know if there's a better place to ask about this but how can I make branch off of main?  I tried making one called `alisakotliarova\/bolden-graph-lines` so I could create a PR and I'm getting a permissions denied error.  Based on the [Contributing doc](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/CONTRIBUTING.md) I got the impression that I should be able to just make one so I'm not sure what I would be doing wrong.  Thanks :)","> Thank you for the help! I ran it with that and it's working just fine on my end.\r\n\r\nGreat!\r\n\r\n> Let me know if there's a better place to ask about this but how can I make branch off of main?\r\n\r\nIf you're getting a permissions error, I guess you tried creating one in the main `prometheus\/prometheus` repo directly on GitHub? That repo is only writeable by Prometheus team members, so the usual contribution flow (if you're not yet on team) is to [fork the repo](https:\/\/docs.github.com\/en\/get-started\/quickstart\/fork-a-repo) first into your own GitHub account and then create the branch in there. And then later, create a pull request back to `prometheus\/prometheus` from there. Let me know if I can help with any of that!","@juliusv Thank you!  I've submitted a PR with a question about unit testing."],"labels":["kind\/enhancement","component\/ui","priority\/P3"]},{"title":"new PromConsole.Graph time zone ","body":"how to set time zone in PromConsole.Graph,When the environment is not UTC\/GMT time\u3002","comments":["\/\/Add xAxis timeFixture in line 537 (\/classic\/static\/js\/prom_console.js)\r\nvar xAxis = new Rickshaw.Graph.Axis.Time({\r\n    graph: graph,\r\n    timeFixture: new Rickshaw.Fixtures.Time.Local()\r\n});\r\n\/\/but,another time is not changed\r\n![image](https:\/\/user-images.githubusercontent.com\/96378503\/148364826-771e1de6-b9d9-4235-a7fd-eaf4f3669600.png)\r\n","@juliusv would know better, but I don't think there is a way to set in PromConsole.Graph.","So that JS library (https:\/\/github.com\/prometheus\/prometheus\/blob\/05dba96ebfff955a2cce32e47240e405116d780b\/web\/ui\/static\/js\/prom_console.js) is only used for console templates, which I've never worked with... as far as I know, it doesn't currently allow any timezone control, but it's served as part of the console template examples from the user-specified path provided via `--web.console.libraries`, so it's possible to serve any custom version of it in case you want to change it.\r\n\r\nThe hover detail popup code is here: https:\/\/github.com\/prometheus\/prometheus\/blob\/05dba96ebfff955a2cce32e47240e405116d780b\/web\/ui\/static\/js\/prom_console.js#L511-L531\r\n\r\nI think it's possible to provide a custom formatter (that could convert the timezone) like in this example: https:\/\/tech.shutterstock.com\/rickshaw\/examples\/formatter.html","I also encountered this problem. My current temporary solution is to put this code in my own template file.\r\n```html\r\n<script>\r\nPromConsole.Graph.prototype._render = function(data) {\r\n  var self = this;\r\n  var palette = new Rickshaw.Color.Palette({scheme: this.params.colorScheme});\r\n  var series = [];\r\n\r\n  \/\/ This will be used on resize.\r\n  this.rendered_data = data;\r\n\r\n\r\n  var nameFuncs = [];\r\n  if (this.params.name === null) {\r\n    var chooser = PromConsole._chooseNameFunction(data);\r\n    for (var i = 0; i < this.params.expr.length; i++) {\r\n      nameFuncs.push(chooser);\r\n    }\r\n  } else {\r\n    for (var i = 0; i < this.params.name.length; i++) {\r\n      if (typeof this.params.name[i] == \"string\") {\r\n        nameFuncs.push(function(i, metric) {\r\n          return PromConsole._interpolateName(this.params.name[i], metric);\r\n        }.bind(this, i));\r\n      } else {\r\n        nameFuncs.push(this.params.name[i]);\r\n      }\r\n    }\r\n  }\r\n\r\n  \/\/ Get the data into the right format.\r\n  var seriesLen = 0;\r\n\r\n  for (var e = 0; e < data.length; e++) {\r\n    for (var i = 0; i < data[e].data.result.length; i++) {\r\n      series[seriesLen] = {\r\n            data: data[e].data.result[i].values.map(function(s) { return {x: s[0], y: self._parseValue(s[1])}; }),\r\n            color: palette.color(),\r\n            name: self._escapeHTML(nameFuncs[e](data[e].data.result[i].metric)),\r\n      };\r\n\t\t\t\/\/ Insert nulls for all missing steps.\r\n\t\t\tvar newSeries = [];\r\n\t\t\tvar pos = 0;\r\n\t\t\tvar start = self.params.endTime - self.params.duration;\r\n      var step = Math.floor(self.params.duration \/ this.graphTd.offsetWidth * 1000) \/ 1000;\r\n\t\t\tfor (var t = start; t <= self.params.endTime; t += step) {\r\n\t\t\t\t\/\/ Allow for floating point inaccuracy.\r\n\t\t\t\tif (series[seriesLen].data.length > pos && series[seriesLen].data[pos].x < t + step \/ 100) {\r\n\t\t\t\t\tnewSeries.push(series[seriesLen].data[pos]);\r\n\t\t\t\t\tpos++;\r\n\t\t\t\t} else {\r\n\t\t\t\t\tnewSeries.push({x: t, y: null});\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tseries[seriesLen].data = newSeries;\r\n      seriesLen++;\r\n    }\r\n  }\r\n  this._clearGraph();\r\n  if (!series.length) {\r\n    var errorText = document.createElement(\"div\");\r\n    errorText.className = 'prom_graph_error';\r\n    errorText.textContent = 'No timeseries returned';\r\n    this.graphTd.appendChild(errorText);\r\n    return;\r\n  }\r\n  \/\/ Render.\r\n  var graph = new Rickshaw.Graph({\r\n          interpolation: \"linear\",\r\n          width: this.graphTd.offsetWidth,\r\n          height: this.params.height,\r\n          element: this.graphTd,\r\n          renderer: this.params.renderer,\r\n          max: this.params.max,\r\n          min: this.params.min,\r\n          series: series\r\n  });\r\n  var hoverDetail = new Rickshaw.Graph.HoverDetail({\r\n    graph: graph,\r\n    onRender: function() {\r\n      var xLabel = this.element.getElementsByClassName(\"x_label\")[0];\r\n      \/\/ --- Add ---\r\n      if(xLabel.textContent.endsWith(\"GMT\")) {\r\n        var timeStamp = Date.parse(xLabel.textContent);\r\n        var date = new Date(timeStamp)\r\n        xLabel.textContent = date.toString();\r\n      }\r\n      \/\/ --- Add ---\r\n      var item = this.element.getElementsByClassName(\"item\")[0];\r\n      if (xLabel.offsetWidth + xLabel.offsetLeft + this.element.offsetLeft > graph.element.offsetWidth ||\r\n        item.offsetWidth + item.offsetLeft + this.element.offsetLeft > graph.element.offsetWidth) {\r\n        xLabel.classList.add(\"prom_graph_hover_flipped\");\r\n        item.classList.add(\"prom_graph_hover_flipped\");\r\n      } else {\r\n        xLabel.classList.remove(\"prom_graph_hover_flipped\");\r\n        item.classList.remove(\"prom_graph_hover_flipped\");\r\n      }\r\n    },\r\n    yFormatter: function(y) {\r\n      if (y === null) {\r\n        return \"\";\r\n      }\r\n      return this.params.yHoverFormatter(y) + this.params.yUnits;\r\n    }.bind(this)\r\n});\r\nvar yAxis = new Rickshaw.Graph.Axis.Y({\r\n    graph: graph,\r\n    tickFormat: this.params.yAxisFormatter,\r\n    ticks: this.params.yAxisTicks\r\n});\r\nvar xAxis = new Rickshaw.Graph.Axis.Time({\r\n    graph: graph,\r\n    \/\/ --- Add ---\r\n    timeFixture: new Rickshaw.Fixtures.Time.Local()\r\n    \/\/ --- Add ---\r\n});\r\nvar legend = new Rickshaw.Graph.Legend({\r\n    graph: graph,\r\n    element: this.legendDiv\r\n});\r\nxAxis.render();\r\nyAxis.render();\r\ngraph.render();\r\n\r\nthis.rickshawGraph = graph;\r\n};\r\n<\/script>\r\n```"],"labels":["kind\/question","component\/ui"]},{"title":"Add `limit` parameters  to \/api\/v1\/querying_exemplars","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\nThe exemplars api may response many exemplars. which cause grafana dashboard get stuck. could we add a limit to control it?\r\nhttps:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#querying-exemplars\r\n\r\n*\u201cNice to have\u201d is not a good use case. :)*\r\n","comments":["Could you please be more explicit? What would the limit do? Render the last examplars, a random subset, the first ones?\r\n\r\ncc @cstyan ","We also don't have any kind of query result limit parameter for the regular query APIs.","There is a hard-coded limit of 11,000 points for range queries.","> There is a hard-coded limit of 11,000 points for range queries.\r\n\r\nThis is a limit of steps.","> We also don't have any kind of query result limit parameter for the regular query APIs.\r\n\r\nDo I understand correctly that it's impossible to limit the number of raw samples returned by the [`query`](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#instant-queries) endpoint?\r\n\r\nHow can pagination of [raw samples](https:\/\/www.robustperception.io\/extracting-raw-samples-from-prometheus) to be done (in chunks of equal size, as with `LIMIT` in InfluxQL)? Here's the [SO question](https:\/\/stackoverflow.com\/questions\/71849368\/return-a-limited-number-of-prometheus-samples-between-start-and-end-timestamps).","> > We also don't have any kind of query result limit parameter for the regular query APIs.\r\n> \r\n> Do I understand correctly that it's impossible to limit the number of raw samples returned by the [`query`](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#instant-queries) endpoint?\r\n> \r\n> How can pagination of [raw samples](https:\/\/www.robustperception.io\/extracting-raw-samples-from-prometheus) to be done (in chunks of equal size, as with `LIMIT` in InfluxQL)? Here's the [SO question](https:\/\/stackoverflow.com\/questions\/71849368\/return-a-limited-number-of-prometheus-samples-between-start-and-end-timestamps).\r\n\r\nCorrect. Prometheus' server side config contains fields to limit the number of samples that can be loaded into memory at once during the execution of a query, but there's no `limit` parameters in the `query` or similar endpoints.","Thanks for clarifying. Coming from Influx, this seems like a major oversight. Am I missing something?","If reading a lot of raw samples in stages is what you want, then the [remote-read API](https:\/\/prometheus.io\/blog\/2019\/10\/10\/remote-read-meets-streaming\/) is more suitable.\r\nBrian's blog post which you linked to is more about seeing some underlying data for debugging purposes.\r\n\r\nAs an Open Source project, Prometheus has those features that someone considered important enough to code up and contribute. I don't think \"major oversight\" applies.","Recently I have hit this myself: if Prometheus returns thousands of exemplars then the client library will spend several seconds parsing them.\r\n\r\nThe client I'm using, Grafana, then down-samples the exemplars because it doesn't want to paint thousands of dots on the screen. So I start to think it would be better to down-sample on the server.","+1 on this. Limit the number of returned exemplars would be nice.\r\nThere are several ways to do this. What I have in mind:\r\n1. Introduce step to downsample exemplars?\r\n2. Return K exemplars sorted by an attribute. Like most recent K exemplars, or top K exemplars with largest value.","Without additional parameters or sampling I feel like a blanket hard limit on the # of exemplars returned by the query API would make them even less useful unless we also introduced pagination to that API endpoint. Not blocking someone from making improvements, but I still don't think a `limit` parameter is the right approach."],"labels":["priority\/Pmaybe","component\/api","kind\/feature"]},{"title":"Add ability to specify some results as precomputed in PromQL expression's AST","body":"Hello,\r\n\r\nOn Thanos side, we've been working on trying to fix one of the oldest tickets, [query pushdown](https:\/\/github.com\/thanos-io\/thanos\/issues\/305). Currently how Thanos works is that we always send data i.e. metrics data that matches given matchers via Select(), to a central location before executing the query. This can get quite inefficient. _A lot_ of resources can be saved by executing part of any original query on the leaf nodes instead of sending raw data. For example, we've recently implemented a feature for pushing down `max`, `max_over_time`, and other functions: https:\/\/github.com\/thanos-io\/thanos\/pull\/4917. It works well for these functions because you can apply them multiple times and get the same result. But, the current PromQL engine is not flexible enough for the other case where results are already precomputed and we don't want the PromQL engine to compute them again. So, in theory, we could calculate `rate(foo[5m])` on leaf nodes but then the \"global\" PromQL engine would apply `rate` again. Thus, I think some improvements need to be made on the PromQL engine's side.\r\n\r\nI am not the first one to come up with this. Here is some of the previous work that I am aware of:\r\n\r\n- promxy which has its own fork of Prometheus with what is called the NodeReplacer API (https:\/\/github.com\/jacksontj\/prometheus\/commits\/release-2.30_fork_promxy) (https:\/\/github.com\/jacksontj\/prometheus\/commit\/3db17800ebc40c3f6abe215a1112fececa495044)\r\n- https:\/\/github.com\/thanos-io\/thanos\/pulls?q=is%3Apr+is%3Aopen+pushdown\r\n- https:\/\/docs.google.com\/document\/d\/1ajMPwVJYnedvQ1uJNW2GDBY6Q91rKAw3kgA5fykIRrg\/edit?usp=sharing\r\n- https:\/\/github.com\/prometheus\/prometheus\/pull\/3631\r\n\r\nSome options to solve this problem that I can think of:\r\n\r\n- Having [Select()](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/storage\/generic.go#L25) return some kind of `SelectHints` which would indicate whether the result has already been computed. Then, the PromQL engine could remove the function call one layer above. Probably the easiest to implement but does this make sense? \r\n- Separate function for doing what `NodeReplacer` is doing. This would give the maximum flexibility to downstream users but probably the hardest to implement. Maybe it could even be called that way?\r\n- Adding the same NodeReplacer to Walk\/Inspect.\r\n\r\nAll of this extra code would only be used by downstream users so all in all it should be relatively easy to add this kind of \"internal API\" to the PromQL engine.","comments":["> Probably the easiest to implement but does this make sense?\r\n\r\nIt doesn't, as leaf nodes don't have access to enough data to correctly evaluate a function like rate() as they don't know what data other leaves may also have for the same time series. It's basically only correct to do this sort of thing for min\/max.","But things such as `rate` only modify each matching time series in place, right? Could you please elaborate? One blocker that I can think of is how from the result returned by `query_range` it is not clear whether there were any gaps in the data so it is impossible to deduplicate correctly without this information."],"labels":["component\/remote storage","component\/promql","not-as-easy-as-it-looks"]},{"title":"Persistent data problem of Prometheus","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nThe persistent data of Prometheus only lasts for one day. Prometheus Prometheus The configuration of yaml file is retention: 7d, which is still useless\r\n\r\n\r\n**What did you expect to see?**\r\nYou can see the data of Prometheus for 7 days\r\n\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n\tcentos 7.6\r\n\r\n* Prometheus version:\r\n\r\n\tv2.20.0\r\n\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\n\/root\/kube-prometheus\/manifests\/prometheus-prometheus.yaml\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\nThe persistent data of Prometheus only lasts for one day. Prometheus Prometheus The configuration of yaml file is retention: 7d, which is still useless\r\n\r\n","comments":["@wizard2318 Have you checked the datetime on your system?","@wizard2318 the retention can only be set via command line flags at the moment and cannot be done by the yaml config. Are you missing that flag? `--storage.tsdb.retention.time=7d`. And do you have size retention enabled by any chance?"],"labels":["kind\/more-info-needed"]},{"title":"Define writing style","body":"The [Go Code Review Comments](https:\/\/github.com\/golang\/go\/wiki\/CodeReviewComments) are cited in CONTRIBUTING.MD as a reference for code style, and they define two rules for writing:\r\n\r\n1. [Comments should be capitalized and end with a period](https:\/\/github.com\/golang\/go\/wiki\/CodeReviewComments#comment-sentences).\r\n2. [Error strings should not be capitalized and end in a period](https:\/\/github.com\/golang\/go\/wiki\/CodeReviewComments#error-strings).\r\n\r\nGenerally, these are followed, especially the first.\r\n\r\nThere are three other areas that don't have clearly defined rules:\r\n\r\n1. Log messages. Capitalization? Periods? The style is pretty varying throughout the codebase.\r\n2. Flags. Yes capitalization, but periods? Most have them but not all.\r\n3. Test error strings. Capitalization? Periods? Unlike regular error strings, they're almost always printed directly instead of in a log message, which is the [reason cited in the wiki for not capitalizing and adding periods to error strings](https:\/\/github.com\/golang\/go\/wiki\/CodeReviewComments#error-strings). The style is pretty varying throughout the codebase.\r\n\r\nOverall this is a nitpick, and I don't think we should go back and fix things, but it would be nice to have something clearly defined in CONTRIBUTING.MD to use as a reference when reviewing new code.","comments":["I remember these to be accepted conventions in Prometheus:\r\n\r\nError strings: All lower case letter unless mentioning some variable and acronym of upper case letter.\r\n\r\nLog messages: Starts with capital letter unless mentioning some variable of lower case. I don't think it needs to end with a period.\r\n\r\nComments: Correct, as you pointed out, starts with capital letter unless mentioning some variable of lower case. Ends with a period.\r\n\r\nNot sure about flags. Following the same thing as comments makes sense here.","> Error strings should not be capitalized and end in a period.\r\n\r\nThat's kind of ambiguous. I would write: \"Error strings should not be capitalized and not end in a period.\" (With the exception of \"naturally\" capitalized words an the beginning of the error string, like quoting a variable name or using a proper noun.)","About the other points:\r\n\r\nI think whenever a text string is supposed to be \"proper English\" to be consumed by humans in \"I'm reading proper English\" mode, then it should follow the usual English punctuation and capitalization rules. Following from that:\r\n\r\n- Log messages: Capitalized as @codesome said. I would end with a period whenever you end something like a sentence, but not if you end with a quoted error message or filename or something, e.g.\r\n  - `Notify discovery manager stopped.` (We don't use a period here in the current code.)\r\n  - `Program exited: SIGKILL` (No period here.)\r\n- Flag help messages are IMHO normal English and should start with a capital letter, end with a period, and ideally read like more or less normal sentences. Brevity should not come at the cost of readability. GOOD: `How long to wait for flushing samples on shutdown or config reload.` \u2013 BAD: `flushing sample wait time on shutdown\/config reload`\r\n- Test error strings: I would study the test framework we use and see how the error strings are represented. Whatever reads most easily should go into the recommendation.","I agree with all of these.\r\n\r\n> Test error strings: I would study the test framework we use and see how the error strings are represented. Whatever reads most easily should go into the recommendation.\r\n\r\n```\r\n--- FAIL: Test1 (0.00s)\r\n    main_test.go:10: \r\n        \tError Trace:\tmain_test.go:10\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: true\r\n        \t            \tactual  : false\r\n        \tTest:       \tTest1\r\n        \tMessages:   \tThere was an error.\r\n--- FAIL: Test2 (0.00s)\r\n    main_test.go:14: \r\n        \tError Trace:\tmain_test.go:14\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: true\r\n        \t            \tactual  : false\r\n        \tTest:       \tTest2\r\n        \tMessages:   \tThere was an error\r\n--- FAIL: Test3 (0.00s)\r\n    main_test.go:18: \r\n        \tError Trace:\tmain_test.go:18\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: true\r\n        \t            \tactual  : false\r\n        \tTest:       \tTest3\r\n        \tMessages:   \tthere was an error\r\nFAIL\r\nFAIL\ttest-errors\t0.005s\r\nFAIL\r\n```\r\n\r\nI think we should handle test error strings like we do log messages: \"end with a period whenever you end something like a sentence, but not if you end with a quoted error message or filename or something\" (also capitalized). Most of them will have a period, but a few won't.","Side note: maybe it's time to have a script like @bwplotka describes in his [excellent blog post](https:\/\/www.bwplotka.dev\/2020\/how-to-became-oss-maintainer):\r\n\r\n> We were spending too much time during a review process to ensure proper commentaries \ud83d\udd75\ufe0f e.g. sentence have to be started with capital letters and finished with a period. We were super strict about it. But sometimes instead of focusing on the essence of the PR, I was spending time commenting on wrong comments. That\u2019s why we created CI script that verifies this for us in the CI pipeline.","@LeviHarrison  Some tools like https:\/\/github.com\/tetafro\/godot may help on this, and it has been integrated into golangci-lint, which is used by this project's CI.\r\n\r\n","Hi @LeviHarrison, if this issue needs to be worked upon, can I try to do it?\r\n","@chenlujjj great idea. Although https:\/\/github.com\/tetafro\/godot only seems to handle comments, that accounts for the majority of issues anyway, and we can always expand our checks later.","@jayesh-srivastava thanks for your offer. I think I'd rather handle the documentation part myself because I have some specific ideas on how I'd like it to be written, but you're free to work on adding the comment check to our golangci-lint configuration.","Actually, I think it might be best to put this in a page on our own wiki, and just link to them in CONTRIBUTING.MD."],"labels":["component\/documentation"]},{"title":"Optimisation: do instant query count() without fetching samples","body":"EDIT: the motivation for this was an instant query (from a recording rule) `count by(job, env, team, cluster) ({__name__=~\".+\"})`.\r\nI think most administrators will run something similar any time they feel the number of metrics is too high, and want to know where they are coming from.\r\n\r\nCurrently `count()` is implemented as an aggregation, which is called within `rangeEval()` here: https:\/\/github.com\/prometheus\/prometheus\/blob\/d677aa4b29a8a0cf9f61af04bbf5bfdce893cf23\/promql\/engine.go#L1191-L1197\r\n\r\n`aggregation()` takes as input `[]Sample`, so the storage layer must fetch samples.\r\n\r\nIn the case it has been called from an instant query, the start and end of the range are equal, but nothing changes, it still fetches samples from storage.\r\n\r\nIt will be cheaper to compute `count()` the same way we do `\/series`, without fetching samples: https:\/\/github.com\/prometheus\/prometheus\/blob\/c954cd9d1d4e3530be2939d39d8633c38b70913f\/tsdb\/querier.go#L140-L143\r\n\r\nHowever this requires some refactoring to be able to run the aggregation (`count by (foo,bar)`, etc.) outside of a function taking `[]Sample`.\r\n\r\nI suggest doing this only for the instant-query case, because over a time-range series can come and go, so we need per-timestamp information and may as well get that via samples.","comments":["I am not sure that this optimization is worth it. We have the risk of introducing bugs for a very niche usecase. The complexity seems too high to me.","I disagree this is \"very niche\"; my guess is every Prometheus admin runs such queries. I edited the description to add that info.\r\n\r\nAs a data point, on a v2.31.1 server with 2 million series `count({__name__=~\".+\"})` took 24 seconds; profile says extracting the samples took 8.5 seconds of that.","Running count() is something I do a lot when auditing metrics usage, especially with big instances shared by many teams.\r\nIt is expensive, especially counting by `__name__` so I mostly rely on `job` label and scrape metrics provided by Prometheus, but would love if `count({__name__=~\".+\"})` was cheaper as it would help a lot.","Do we plan to implement this? I can try an implementation as a PR.","Now I think about it some more, I'm not sure we can get the same result without samples.\r\nI think in the current code it will look back the stale time (5 minutes), and if there is no sample then the series will not appear.\r\nWhereas the alternative code path will return every series in the head, typically anything that had a sample in the last 2-3 hours.\r\n\r\nSo while I still want the cheaper implementation, we would need to address backwards-compatibility.","> we would need to address backwards-compatibility\r\n\r\nThis came up again; we could have it as a different function name, e.g. `count_for_block`.  \r\nNaming is hard."],"labels":["kind\/enhancement","component\/promql"]},{"title":"Possible goroutine leaks at TestScrapeLoopRun","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nUsing our tool to trigger TestScrapeLoopRun and TestScrapeLoopForcedErr\r\n\r\n**What did you expect to see?**\r\n\r\n**What did you see instead? Under which circumstances?**\r\n```\r\nfunc TestScrapeLoopRun(t *testing.T) {\r\n   var (\r\n       signal = make(chan struct{}, 1)\r\n       errc   = make(chan error) \/\/ line 1\r\n   \u2026\u2026\u2026\u2026.\r\n   go func() {\r\n       sl.run(errc) \/\/ line 2\r\n       signal <- struct{}{}\r\n   }()\r\n   select {\r\n   case err := <-errc:  \/\/ line 3\r\n       if err != context.DeadlineExceeded {\r\n           t.Fatalf(\"Expected timeout error but got: %s\", err)\r\n       }\r\n   case <-time.After(3 * time.Second): \/\/ line 5\r\n       t.Fatalf(\"Expected timeout error but got none\")\r\n   }\r\n \r\n\r\nfunc (sl *scrapeLoop) run(errc chan<- error) {\r\n  For {\r\n     \u2026\r\n   last = sl.scrapeAndReport(last, scrapeTime, errc)\r\n\r\n  }\r\n}\r\n\r\nfunc (sl *scrapeLoop) scrapeAndReport(last, appendTime time.Time, errc chan<- error) time.Time {\r\n\u2026\r\nif forcedErr := sl.getForcedError(); forcedErr != nil {\r\n       scrapeErr = forcedErr\r\n       \/\/ Add stale markers.\r\n       if _, _, _, err := sl.append(app, []byte{}, \"\", appendTime); err != nil {\r\n           app.Rollback()\r\n           app = sl.appender(sl.parentCtx)\r\n           level.Warn(sl.l).Log(\"msg\", \"Append failed\", \"err\", err)\r\n       }\r\n       if errc != nil {\r\n           errc <- forcedErr  \/\/ line 4\r\n       }\r\n \r\n       return start\r\n   }\r\n\u2026\r\n}\r\n```\r\n\r\nThe channel is created at line 1, and passed in to function at line 2. Receiver is at line 3.if test failed at line 5, then line 4\u2019s sending operation will be blocked.\r\n```\r\ngoroutine 58 [chan send]:\r\ngithub.com\/prometheus\/prometheus\/scrape.(*scrapeLoop).scrapeAndReport(0xc00039a200, 0x3b9aca00, 0x5f5e100, 0x0, 0x0, 0x0, 0xc0641f7b9561e3fe, 0x3c3c2212, 0x10dc4a0, 0xc00002a680, ...)\r\n   \/repos\/prometheus\/scrape\/scrape.go:1338 +0x12b9\r\ngithub.com\/prometheus\/prometheus\/scrape.(*scrapeLoop).run(0xc00039a200, 0x3b9aca00, 0x5f5e100, 0xc00002a680)\r\n   \/repos\/prometheus\/scrape\/scrape.go:1189 +0x590\r\ngithub.com\/prometheus\/prometheus\/scrape.TestScrapeLoopRun.func4(0xc00000e028, 0xc00002a680, 0xc00002a600)\r\n   \/repos\/prometheus\/scrape\/scrape_test.go:1089 +0x4e\r\ncreated by github.com\/prometheus\/prometheus\/scrape.TestScrapeLoopRun\r\n   \/repos\/prometheus\/scrape\/scrape_test.go:1088 +0x6c8\r\n```\r\n\r\nSimiliar situation also happened in TestScrapeLoopForcedErr\r\n**Environment**\r\n\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["We have tests for go routine leaks. What are we missing, why aren't those tests failing?","In normal situation, test will be fine. The test will have goroutine leak if line 5 got triggered.\r\n\r\n> We have tests for go routine leaks. What are we missing, why aren't those tests failing?\r\n\r\n"],"labels":["kind\/more-info-needed"]},{"title":"Propagate more hints through PromQL Select","body":"I would love to propose pushing more hints to storage.\r\n\r\n#### Proposal\r\n\r\nLet's take an example query: `avg(sum(rate(abc{}[5m])) by (pod)) by (cluster)`\r\n\r\nCurrently such query will ask storage using `Querier` interface `Select` method (`Select(sortSeries bool, hints *SelectHints, matchers ...*labels.Matcher) SeriesSet`)  with `__name__ = abc` matcher and following select hints:\r\n\r\n```\r\nSelectHints{\r\n\tStart: ..., End: ...,  Step: ...,\r\n\tFunc: \"rate\",\r\n\tRange: 5 * 60 * 1000,\r\n}\r\n```\r\n\r\nI would love to propose to pass all hints that are scoped for given matchers FROM the deepest. So after the change storage would have something like:\r\n\r\n```\r\n[]SelectHints{\r\n{ \r\n        Start: ..., End: ...,  Step: ...,\r\n\tFunc: \"rate\",\r\n\tRange: 5 * 60 * 1000,\r\n}. {\r\n\tFunc: \"sum\"\r\n        Grouping []string{\"pod\"},\r\n\tBy: true.\r\n}, {\r\n\tFunc: \"avg\"\r\n        Grouping []string{\"cluster\"},\r\n\tBy: true.\r\n}\r\n```\r\n\r\n### Motivation\r\n\r\nSimilar to https:\/\/github.com\/prometheus\/prometheus\/issues\/10040 such change will enormously improve how in Thanos we can improve query evaluation, how much we can pushdown to leafs and how much we can parallelize the query. \r\n\r\nIn theory we could parallize such query in Prometheus too and perform sub PromQL eval on disjont blocks too for safe query concurrency. This is however outside of this proposal. What we want here is just to pass more information (select hints) to storage and remote read counterparts.\r\n\r\n","comments":["That avg hint is not safe to send, as it's impossible to do things with it correctly in the general case. Thus far the only hints sent have been for things where it is possible to do correct things with it, so that sum hint is fine for example as sum is associative. min\/max are also safe.","@brian-brazil Average can be associative if you pushdown(up) the sum\/count, correct? ","> so that sum hint is fine for example as sum is associative.\r\n\r\nI should correct myself here, this is not safe as different remote read endpoints could return overlapping data so you could end up over-counting.\r\n\r\n> Average can be associative if you pushdown(up) the sum\/count, correct?\r\n\r\nNo as a) you can only return one value and b) the above reason.\r\n\r\nMin\/max remain safe I believe.","Is this a duplicate of #9979 ?","Thanks for feedback!\r\n\r\n#9979 is connected, but not the same. #9978 is about extending hints with more parameters from different functions like `top`. Issue #10041 is about passing a slice of hints to have info about all nested aggr\/func in play.\r\n\r\n"],"labels":["component\/promql","kind\/feature"]},{"title":"Streaming Result for Query\/QueryRange HTTP API","body":"In Thanos we are moving forward with query sharding, parallelization and pushdown ideas \ud83d\udcaa\ud83c\udffd Recently, we pushed a change that is capable of safely pushdown certain aggregations to Prometheus (https:\/\/github.com\/thanos-io\/thanos\/pull\/4917).\r\n\r\nWhat we do in sidecar now, for certain \"safe\" functions like `min, max, group, min_over_time, and max_over_time` instead of fetching all series through remote read, we change to HTTP Query API. We build PromQL from select hints we get from root Querier and perform query. Then we transform the Query result to Thanos GRPC StoreAPI.\r\n\r\nNow, we have efficiency problem with Query API, because it's not streamed (something I was moaning about for long time \ud83d\ude48). This means we unnecessary wait for full response payload, instead of streaming each resulted series by series like we do with remote read. AFAIK for most cases PromQL performs series by series calculations, so first series is calculated before next one, so that fits server part of this API too.\r\n\r\nNow, to solve our use case, there are two options we could do to improve Prometheus for these use cases and actually win in other fields too.\r\n\r\nA) Introduce streaming (per series) for query and query range Prometheus HTTP APIs. We could do this exactly as we did with remote read. Don't stream by default and opt-in using header negotiation for streamed JSON using chunked JSON logic we use with profobuf in new remote read. It's not straightforward as our current JSON result with this `data` format was never attempted to be streamed.\r\nB) Allow remote-read to perform PromQL using hints. It's not hard to implement, but quite weird as there is strong overlap now between Query + Query range and remote write that can do PromQL. Plus it's easy right now to make mistake and pass wrong hints (aggregations\/function that are NOT safe to be pushed down).\r\n\r\nI would vote for A. I believe streaming response on query would help with many other use cases (e.g Prometheus -> Grafana interaction and Querier -> Query frontend in Thanos). \r\n\r\nIf we are ok to pursue A I could try to work with community on proposing exact format of JSON streamed API.\r\n\r\ncc @tomwilkie @roidelapluie @juliusv @fpetkovski @brian-brazil @moadz @RichiH \r\n","comments":["> AFAIK for most cases PromQL performs series by series calculations,\r\n\r\nThat's not the case. This is true for range vector functions internally, however basically everything else is step by step including the functions you list.\r\nSo there's not much to be streamed, and you'd have to rewrite a large chunk of PromQL to make it happen - plus ensure that failure semantics are dealt with correctly, and somehow ensure that resource usage isn't adversely affected given you're now doing more work and keeping more intermediate results in memory.","Yeah, making the PromQL computation itself streamable on a per-series basis sounds hard, but the final JSON result could of course be streamed quite easily. Maybe that's still worth it for large payloads in some use cases?","Once we have all the result samples, streaming from there makes sense. I think I ended up not doing it when making the json iterator changes as it ended up worse performance wise, but that could have changed.","Thanks for the input. Looks like we could descope it to just an API and check if it helps with anything (it helps for Thanos sidecar case for sure).\r\n\r\nOn separate note then...\r\n\r\n> and you'd have to rewrite a large chunk of PromQL to make it happen - plus ensure that failure semantics are dealt with correctly, and somehow ensure that resource usage isn't adversely affected given you're now doing more work and keeping more intermediate results in memory.\r\n\r\n@brian-brazil you made me curious now! So do you think there is room to make PromQL computing as much series by series internally, even if it's not currently? \ud83e\udd14 \r\n\r\n","It's already doing as much as it can as clearly makes sense. Beyond that\nwould require a big redesign, and it's unclear that it'd result in any\nperformance gains. Keeping maximum memory down is tricky.\n\nOn Fri 17 Dec 2021, 21:59 Bartlomiej Plotka, ***@***.***>\nwrote:\n\n> Thanks for the input. Looks like we could descope it to just an API and\n> check if it helps with anything (it helps for Thanos sidecar case for sure).\n>\n> On separate note then...\n>\n> and you'd have to rewrite a large chunk of PromQL to make it happen - plus\n> ensure that failure semantics are dealt with correctly, and somehow ensure\n> that resource usage isn't adversely affected given you're now doing more\n> work and keeping more intermediate results in memory.\n>\n> @brian-brazil <https:\/\/github.com\/brian-brazil> you made me curious now!\n> So do you think there is room to make PromQL computing as much series by\n> series internally, even if it's not currently? \ud83e\udd14\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/10040#issuecomment-997055475>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABWJG5TZIVQ5ROL6K63QZYTUROXDZANCNFSM5KIRQ6YQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"],"labels":["component\/api","kind\/feature"]},{"title":"Panic in TestChunkDiskMapper_WriteChunk_Chunk_IterateChunks","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nTrigger TestChunkDiskMapper_WriteChunk_Chunk_IterateChunks\r\n**What did you expect to see?**\r\nPASS\r\n**What did you see instead? Under which circumstances?**\r\npanic as follows:\r\n\r\n```\r\npanic: runtime error: slice bounds out of range [:104326] with capacity 104293 [recovered]\r\n\tpanic: runtime error: slice bounds out of range [:104326] with capacity 104293\r\n\r\ngoroutine 6 [running]:\r\ntesting.tRunner.func1.2(0x6451c0, 0xc000014108)\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1143 +0x335\r\ntesting.tRunner.func1(0xc00024db80)\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1146 +0x4c2\r\npanic(0x6451c0, 0xc000014108)\r\n\t\/usr\/local\/go\/src\/runtime\/panic.go:965 +0x1b9\r\ngithub.com\/prometheus\/prometheus\/tsdb\/chunks.TestChunkDiskMapper_WriteChunk_Chunk_IterateChunks(0xc00024db80)\r\n\t\/repos\/prometheus\/tsdb\/chunks\/head_chunks_test.go:125 +0xcf2\r\ntesting.tRunner(0xc00024db80, 0x671d80)\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1193 +0xef\r\ncreated by testing.(*T).Run\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1238 +0x2b5\r\n```\r\n**Environment**\r\n\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["@charlesxsh what Prometheus version\/commit are you testing this on?","@codesome e0f1506254688cec85276cc939aeb536a4e029d1"],"labels":["kind\/cleanup","priority\/P3","kind\/more-info-needed"]},{"title":"Feature request: Federation support  exemplars.","body":"## Proposal\r\nI am using federation to scrape selected time series from another Prometheus server, but when the series is updated with exemplars, I can't get the exemplars by the federation.\r\n","comments":["This feature is indeed not supported yet. Federation does not use OpenMetrics at the moment at all.","> This feature is indeed not supported yet. Federation does not use OpenMetrics at the moment at all.\r\n\r\nat present, any plan to support this feautre ?","It can be added relatively easily for federation requests negotiating the old protobuf protocol. (And since Prometheus can understand protobuf again, as a byproduct of native histograms, this might be a viable option.)\r\n\r\nTo enable federation for OpenMetrics scrapers, we indeed need better OpenMetrics support for federation."],"labels":["priority\/P3","component\/api","kind\/feature"]},{"title":"Feature request: add a \"features supported\" API","body":"Arising from https:\/\/github.com\/grafana\/grafana\/issues\/33487, where we try to figure out whether the backend supports label-values requests with matchers.\r\n\r\nSince there are several implementations of the Prometheus API, looking at the Prometheus version in `\/status\/buildinfo` is suboptimal, also some features like Exemplars are enabled by flag.\r\n\r\nProposal: have a `\/status\/features` endpoint that returns a bag of strings saying which things are supported.","comments":["In the context of yesterday's Prometheus dev summit, we talked about if Grafana could ping back some basic information like Prometheus version and enabled feature flags; this could help inform Prometheus-team about what features are in use.","\"Prometheus version\" is less helpful in a world where multiple backends try to serve the Prometheus API (Cortex, Chronosphere, etc).\r\nIf you came up with a \"Prometheus API version\" that would be a bit better, but right now I prefer my design of a bag of feature names that is not constrained to change in one set of steps.","Fair, I did mean \"the version of the thing\", not \"the version on Prometheus binaries only\". Would you be interested in creating a design doc of what Prometheus endpoints should offer in such an API?"],"labels":["priority\/P3","component\/api","kind\/feature"]},{"title":"Prometheus startup should discard data beyond the retention time","body":"**What did you do?**\r\n\r\nCame across a Prometheus that has been crashlooping for a month because its disk is full.\r\n\r\n**What did you expect to see?**\r\n\r\nAt startup, delete WAL and storage blocks that are beyond retention time, before starting to write any new data.\r\n\r\n**What did you see instead?**\r\n\r\nThe data is from well before the retention time (1635876000 =  02 Nov 2021) so can all be deleted, but it tries to start writing before doing any deleting and is crashlooping.\r\n\r\n* Logs:\r\n```\r\nlevel=info ts=2021-12-14T14:44:04.005Z caller=main.go:400 msg=\"No time or size retention was set so using the default time retention\" duration=15d\r\nlevel=info ts=2021-12-14T14:44:04.005Z caller=main.go:438 msg=\"Starting Prometheus\" version=\"(version=2.30.3, branch=HEAD, revision=f29caccc42557f6a8ec30ea9b3c8c089391bd5df)\"\r\nlevel=info ts=2021-12-14T14:44:04.005Z caller=main.go:443 build_context=\"(go=go1.17.1, user=root@5cff4265f0e3, date=20211005-16:10:52)\"\r\nlevel=info ts=2021-12-14T14:44:04.005Z caller=main.go:444 host_details=\"(Linux 5.4.120+ #1 SMP Wed Aug 18 10:20:32 PDT 2021 x86_64 billing-prometheus-1 (none))\"\r\n[...]\r\nlevel=info ts=2021-12-14T14:44:04.007Z caller=main.go:822 msg=\"Starting TSDB ...\"\r\nlevel=info ts=2021-12-14T14:44:04.007Z caller=web.go:575 component=web msg=\"Router prefix\" prefix=\/billing-prometheus\r\nlevel=info ts=2021-12-14T14:44:04.008Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1635811200079 maxt=1635876000000 ulid=01FKHE3PR8GXJ2F155YHP1QEV9\r\nlevel=info ts=2021-12-14T14:44:04.008Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1635876000001 maxt=1635940800000 ulid=01FKKBXK4ZCAD0TFQWZVKRAKHQ\r\n[...]\r\nlevel=info ts=2021-12-14T14:44:04.009Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637150400000 maxt=1637157600000 ulid=01FMQ6V1SNNTAGTSJXNVTBEG56\r\nlevel=error ts=2021-12-14T14:44:04.059Z caller=main.go:915 err=\"opening storage failed: open \/prometheus\/data\/wal\/00002813: no space left on device\"\r\n```\r\n","comments":["Can I know what was the retention time here? Since TSDB does not use wall clock, it takes the time relative to the last block. And in this case the first block is under 15d of the last block, so I guess retention is 15d.\r\n\r\nWe could however consider the Head block as well, but a buggy sample with timestamp too far in future can cause issues (it can cause now too btw, but we get _some_ time to delete the buggy series before it's late, or maybe take snapshot of the data while we fix it).\r\n\r\nA solution that works best for both problems is considering the Head block for retention _only_ on startup.","So IIUC, the problem is with new data being only in the Head block.","Retention time is 15d. \r\n\r\nThere is no new data; it\u2019s trying to read the WAL. (I guess that\u2019s newer than the blocks, if that\u2019s what you meant).\r\n\r\nTo clarify: my objection is that it _would_ have some new data, if it would only delete some older data and start scraping. But it doesn\u2019t; it had been crash-looping for a month. \r\n\r\nMaybe there could be an additional option to delete data older than wall-clock?","If WAL has new data, then that makes it easier to handle from TSDB. For option to delete beyond some wall-clock time:\r\n\r\nSince this is only during startup, and ideally would like to discard data even from WAL, we could extend the `tsdb.Open()` to accept the min time for the DB and delete blocks before that and ignore samples from WAL before that (truncating WAL on startup is not a good idea since it will slow down startup and make code more complex). And then Prometheus passes the time to `tsdb.Open` since it uses the wall-clock for timestamp.\r\n\r\nEven then, I think it should not be optional and should be always enabled.\r\n","Your point about not looking at wall-clock made me think: if what you describe were added and non-optional maybe there should be an option to say \"I want to run read-only with no scraping and no deletion of old data, just to look at the 15 days of data I have on disk from two months ago\".","Is it possible if we can give an optional parameter to override tsdb.Open(), and in that time, it can delete old data, prevent it from such CrashLoopBackOff","TSDB already runs the retention logic before on startup, before replaying the WAL. The Problem here is with the time retention logic, which is w.r.t. the last persistent block and not the Head block, hence has to replay the WAL.\r\n\r\nThere is no easy way to know the time range of the Head block until full WAL replay at the moment. Time retention w.r.t. head block would be acceptable on startup (but not after that when it starts accepting samples).\r\n\r\n> \"I want to run read-only with no scraping and no deletion of old data, just to look at the 15 days of data I have on disk from two months ago\".\r\n\r\nYou can set a super high retention with no scraping config, and that's the read-only Prometheus that you want I believe? (does not prevent the WAL replay here btw)","I think the problem I encountered is a bit simpler: all the persistent blocks, head and WAL probably do fit into 15 days, but it was 15 days a month ago when the disk filled up.\r\nIt won't hit the retention time limit until some new data is scraped.  But it never gets there.\r\n\r\nWhat we probably should have done is set a retention size limit as well, a bit below the disk volume size, then it wouldn't fill the disk."],"labels":["kind\/enhancement","priority\/P3","component\/tsdb"]},{"title":"maximum resolution of 11,000 is not applied to interpolated timeseries","body":"**What did you do?**\r\nI queried for an interpolated time series\r\n```shell\r\n$ curl -sg \"http:\/\/localhost:9090\/api\/v1\/query?query=rate(go_gc_duration_seconds_sum[5m])[60m:10ms]\"\r\n```\r\n\r\n**What did you expect to see?**\r\nI expected the \"exceeded maximum resolution of 11,000 points per timeseries\" error message. @brian-brazil \r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/6f3e664ae712850b020d95c5c8b8a6ff841803bd\/web\/api\/v1\/api.go#L434-L438\r\n\r\n**What did you see instead? Under which circumstances?**\r\nMore than 11k points were returned:\r\n```\r\n$ curl -sg \"http:\/\/localhost:9090\/api\/v1\/query?query=rate(go_gc_duration_seconds_sum[5m])[60m:10ms]\" | jq '.data.result[].values | keys' | wc -l\r\n217020\r\n```\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n```\r\nLinux 5.11.0-1022-gcp x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```\r\nprometheus, version 2.28.1 (branch: HEAD, revision: b0944590a1c9a6b35dc5a696869f75f422b107a1)\r\n  build user:       root@lcy01-amd64-016\r\n  build date:       20210724-22:57:51\r\n  go version:       go1.16.6\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Alertmanager version: N\/A\r\n\r\n* Prometheus configuration file:\r\n```\r\n# my global config\r\nglobal:\r\n  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n  # scrape_timeout is set to the global default (10s).\r\n\r\n# Alertmanager configuration\r\nalerting:\r\n  alertmanagers:\r\n  - static_configs:\r\n    - targets:\r\n      # - alertmanager:9093\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n  # - \"first_rules.yml\"\r\n  # - \"second_rules.yml\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n  - job_name: 'prometheus'\r\n\r\n    # metrics_path defaults to '\/metrics'\r\n    # scheme defaults to 'http'.\r\n\r\n    static_configs:\r\n    - targets: ['localhost:9090']\r\n```\r\n\r\n* Alertmanager configuration file: N\/A\r\n\r\n\r\n* Logs: None\r\n","comments":["The error you are pointing at is for `query_range`, while you are calling `query`.  This is why it is not applied.\r\n\r\nI agree a similar limit should apply to vector results from instant queries.\r\n\r\nThe effect of the test in `query_range` is to bail out before starting work, but this may be difficult to achieve for the sort of case you have illustrated: a subquery can be inside an aggregation which makes the final result quite small.\r\nShould be fairly easy to check the number after computing the result and error instead of sending it.","I think we could avoid running subqueries that large, by looking at subquery step and duration. We don't need to do anything fancier than that, and it is better to stop working before hitting the TSDB.","I was going on the comment which says \"For safety, limit the number of _returned_ points per timeseries.\"\r\nIt does not say the objective is to avoid doing large queries.","Historically this limit was to avoid running with a step of 1s since 1970\nwith query_range, which I did once by accident.\n\nSamples per timeseries internally is different, and may break quite a few\nusers as this hardcoded value was chosen to be enough for realistic query\nrange use cases, not all range vectors or sub queries.\n\nImproving the memory sample limits may be a better approach, for example it\ncurrently handwaves some stuff as small enough not to matter due to the 11k\nas that code predates subqueries.\n\nOn Fri 17 Dec 2021, 16:39 Bryan Boreham, ***@***.***> wrote:\n\n> I was going on the comment which says \"For safety, limit the number of\n> *returned* points per timeseries.\"\n> It does not say the objective is to avoid doing large queries.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/9999#issuecomment-996861737>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABWJG5W5D2JOR4VT46KV35LURNRTLANCNFSM5J2A7P7A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"],"labels":["kind\/enhancement","component\/promql","priority\/P3"]},{"title":"Update code comments for DB.CompactHead() and Head.gc() for append vs gc race","body":"https:\/\/github.com\/prometheus\/prometheus\/blob\/c965a7555b7ffcee1a127d782abd5bb478a16750\/tsdb\/head_append.go#L453\r\n\r\n```\r\n\tfor i, s := range a.samples {\r\n\t\tseries = a.sampleSeries[i]\r\n\t\tseries.Lock()\r\n\t\tok, chunkCreated := series.append(s.T, s.V, a.appendID, a.head.chunkDiskMapper)\r\n\t\tseries.cleanupAppendIDsBelow(a.cleanupAppendIDsBelow)\r\n\t\tseries.pendingCommit = false\r\n```\r\n^  **pendingCommit should be set after all series is appended.Gc process may be error **\r\n```\r\n\t\tseries.Unlock()\r\n\r\n\t\tif !ok {\r\n\t\t\ttotal--\r\n\t\t\ta.head.metrics.outOfOrderSamples.Inc()\r\n\t\t}\r\n\t\tif chunkCreated {\r\n\t\t\ta.head.metrics.chunks.Inc()\r\n\t\t\ta.head.metrics.chunksCreated.Inc()\r\n\t\t}\r\n\t}\r\n```\r\n\r\n## process below may be error \r\n\r\n```\r\nfunc (s *stripeSeries) gc(mint int64) (map[uint64]struct{}, int, int64) {\r\n\tvar (\r\n\t\tdeleted                  = map[uint64]struct{}{}\r\n\t\tdeletedForCallback       = []labels.Labels{}\r\n\t\trmChunks                 = 0\r\n\t\tactualMint         int64 = math.MaxInt64\r\n\t)\r\n\t\/\/ Run through all series and truncate old chunks. Mark those with no\r\n\t\/\/ chunks left as deleted and store their ID.\r\n\tfor i := 0; i < s.size; i++ {\r\n\t\ts.locks[i].Lock()\r\n\r\n\t\tfor hash, all := range s.hashes[i] {\r\n\t\t\tfor _, series := range all {\r\n\t\t\t\tseries.Lock()\r\n\t\t\t\trmChunks += series.truncateChunksBefore(mint)\r\n\r\n\t\t\t\tif len(series.mmappedChunks) > 0 || series.headChunk != nil || series.pendingCommit {\r\n\t\t\t\t\tseriesMint := series.minTime()\r\n\t\t\t\t\tif seriesMint < actualMint {\r\n\t\t\t\t\t\tactualMint = seriesMint\r\n\t\t\t\t\t}\r\n\t\t\t\t\tseries.Unlock()\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n```","comments":["Thanks for this. I will let @codesome comment on this.","@RayYe586 could you please describe what kind of error it can cause?","@codesome If data commit is in progress,gc may delete these data.I find the error by code analysis.","That's true, if we have samples incoming for the gc range, then gc can delete it. The way we take care of it is to not append any samples that gc would delete eventually. It is taken care by [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/e226d1f95c8abeb7138e065bde13d9f70971288a\/tsdb\/head_append.go#L138-L142).\r\n\r\nThe `CompactHead()`, if being used as a library, is required to be used only after stopping all the ingestion. I have updated the issue title since this is more of updating the code comments for DB.CompactHead() and Head.gc()"],"labels":["priority\/Pmaybe","component\/documentation","component\/tsdb"]},{"title":"SelectHints should contain function parameters in addition to the function name","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nWe would like to optimize the retrieval of SeriesSets by pre-computing the result for certain aggregations. \r\nThis is currently possible to do for aggregations like `max` and `min`, but not for `topk` and `bottomk` because the latter two aggregations have an additional parameter which is not provided in the SelectHints: https:\/\/github.com\/prometheus\/prometheus\/blob\/655e2d28795ff5b1532778d7ce2f014e4a76da6f\/promql\/engine.go#L777-L783\r\n\r\nI am more than happy to make the contribution myself, I just wanted to know whether there are any objections to having the parameters in addition to the function name.","comments":["As per previous dev summit, I'd prefer to have a design doc about how we would shard promql queries in the future, instead of changing the protocol right away. I do not know when\/who would work on that design doc.\r\n\r\nIf we do not foresee this in a relativaly close future, we could add this.","Ack, we can try combining all we need to add to hints into one proposal to make sure Thanos can use the same APIs, PromQL engine and Prometheus engine can responds to those hints results."],"labels":["not-as-easy-as-it-looks"]},{"title":"go install promtool not working","body":"**What did you do?**\r\n```\r\nGO111MODULE=on go install github.com\/prometheus\/prometheus\/cmd\/promtool@latest\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nSuccessful installation of promtool.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n```\r\ngo: finding module for package github.com\/prometheus\/client_golang\/api\r\ngo: finding module for package gopkg.in\/alecthomas\/kingpin.v2\r\ngo: finding module for package github.com\/google\/pprof\/profile\r\ngo: finding module for package github.com\/prometheus\/common\/model\r\ngo: finding module for package github.com\/prometheus\/client_golang\/api\/prometheus\/v1\r\ngo: finding module for package github.com\/prometheus\/common\/config\r\ngo: finding module for package gopkg.in\/yaml.v2\r\ngo: finding module for package github.com\/prometheus\/common\/version\r\ngo: finding module for package github.com\/cespare\/xxhash\r\ngo: finding module for package github.com\/pkg\/errors\r\ngo: finding module for package github.com\/go-kit\/kit\/log\r\ngo: finding module for package github.com\/go-kit\/kit\/log\/level\r\ngo: finding module for package github.com\/opentracing\/opentracing-go\r\ngo: finding module for package github.com\/prometheus\/client_golang\/prometheus\r\ngo: finding module for package github.com\/prometheus\/client_model\/go\r\ngo: finding module for package github.com\/prometheus\/common\/expfmt\r\ngo: finding module for package github.com\/Azure\/azure-sdk-for-go\/arm\/compute\r\ngo: finding module for package github.com\/Azure\/azure-sdk-for-go\/arm\/network\r\ngo: finding module for package github.com\/Azure\/go-autorest\/autorest\r\ngo: finding module for package github.com\/Azure\/go-autorest\/autorest\/adal\r\ngo: finding module for package github.com\/Azure\/go-autorest\/autorest\/azure\r\ngo: finding module for package github.com\/hashicorp\/consul\/api\r\ngo: finding module for package github.com\/mwitkow\/go-conntrack\r\ngo: finding module for package github.com\/miekg\/dns\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/aws\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/aws\/credentials\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/aws\/credentials\/stscreds\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/aws\/ec2metadata\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/aws\/session\r\ngo: finding module for package github.com\/aws\/aws-sdk-go\/service\/ec2\r\ngo: finding module for package gopkg.in\/fsnotify\/fsnotify.v1\r\ngo: finding module for package golang.org\/x\/oauth2\r\ngo: finding module for package golang.org\/x\/oauth2\/google\r\ngo: finding module for package google.golang.org\/api\/compute\/v1\r\ngo: finding module for package k8s.io\/api\/core\/v1\r\ngo: finding module for package k8s.io\/api\/extensions\/v1beta1\r\ngo: finding module for package k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\r\ngo: finding module for package k8s.io\/apimachinery\/pkg\/runtime\r\ngo: finding module for package k8s.io\/apimachinery\/pkg\/watch\r\ngo: finding module for package k8s.io\/client-go\/kubernetes\r\ngo: finding module for package k8s.io\/client-go\/rest\r\ngo: finding module for package k8s.io\/client-go\/tools\/cache\r\ngo: finding module for package k8s.io\/client-go\/tools\/metrics\r\ngo: finding module for package k8s.io\/client-go\/util\/workqueue\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\/openstack\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/extensions\/floatingips\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/extensions\/hypervisors\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/servers\r\ngo: finding module for package github.com\/gophercloud\/gophercloud\/pagination\r\ngo: finding module for package github.com\/samuel\/go-zookeeper\/zk\r\ngo: finding module for package github.com\/prometheus\/tsdb\r\ngo: finding module for package github.com\/prometheus\/tsdb\/labels\r\ngo: found github.com\/google\/pprof\/profile in github.com\/google\/pprof v0.0.0-20211204230040-2007db6d4f53\r\ngo: found github.com\/prometheus\/client_golang\/api in github.com\/prometheus\/client_golang v1.11.0\r\ngo: found github.com\/prometheus\/client_golang\/api\/prometheus\/v1 in github.com\/prometheus\/client_golang v1.11.0\r\ngo: found github.com\/prometheus\/common\/config in github.com\/prometheus\/common v0.32.1\r\ngo: found github.com\/prometheus\/common\/model in github.com\/prometheus\/common v0.32.1\r\ngo: found github.com\/prometheus\/common\/version in github.com\/prometheus\/common v0.32.1\r\ngo: found gopkg.in\/alecthomas\/kingpin.v2 in gopkg.in\/alecthomas\/kingpin.v2 v2.2.6\r\ngo: found gopkg.in\/yaml.v2 in gopkg.in\/yaml.v2 v2.4.0\r\ngo: found github.com\/cespare\/xxhash in github.com\/cespare\/xxhash v1.1.0\r\ngo: found github.com\/pkg\/errors in github.com\/pkg\/errors v0.9.1\r\ngo: found github.com\/go-kit\/kit\/log in github.com\/go-kit\/kit v0.12.0\r\ngo: found github.com\/go-kit\/kit\/log\/level in github.com\/go-kit\/kit v0.12.0\r\ngo: found github.com\/opentracing\/opentracing-go in github.com\/opentracing\/opentracing-go v1.2.0\r\ngo: found github.com\/prometheus\/client_golang\/prometheus in github.com\/prometheus\/client_golang v1.11.0\r\ngo: found github.com\/prometheus\/client_model\/go in github.com\/prometheus\/client_model v0.2.0\r\ngo: found github.com\/prometheus\/common\/expfmt in github.com\/prometheus\/common v0.32.1\r\ngo: found github.com\/Azure\/go-autorest\/autorest in github.com\/Azure\/go-autorest\/autorest v0.11.22\r\ngo: found github.com\/Azure\/go-autorest\/autorest\/adal in github.com\/Azure\/go-autorest\/autorest\/adal v0.9.17\r\ngo: found github.com\/Azure\/go-autorest\/autorest\/azure in github.com\/Azure\/go-autorest\/autorest v0.11.22\r\ngo: found github.com\/hashicorp\/consul\/api in github.com\/hashicorp\/consul\/api v1.11.0\r\ngo: found github.com\/mwitkow\/go-conntrack in github.com\/mwitkow\/go-conntrack v0.0.0-20190716064945-2f068394615f\r\ngo: found github.com\/miekg\/dns in github.com\/miekg\/dns v1.1.43\r\ngo: found github.com\/aws\/aws-sdk-go\/aws in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found github.com\/aws\/aws-sdk-go\/aws\/credentials in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found github.com\/aws\/aws-sdk-go\/aws\/credentials\/stscreds in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found github.com\/aws\/aws-sdk-go\/aws\/ec2metadata in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found github.com\/aws\/aws-sdk-go\/aws\/session in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found github.com\/aws\/aws-sdk-go\/service\/ec2 in github.com\/aws\/aws-sdk-go v1.42.20\r\ngo: found gopkg.in\/fsnotify\/fsnotify.v1 in gopkg.in\/fsnotify\/fsnotify.v1 v1.4.7\r\ngo: found golang.org\/x\/oauth2 in golang.org\/x\/oauth2 v0.0.0-20211104180415-d3ed0bb246c8\r\ngo: found golang.org\/x\/oauth2\/google in golang.org\/x\/oauth2 v0.0.0-20211104180415-d3ed0bb246c8\r\ngo: found google.golang.org\/api\/compute\/v1 in google.golang.org\/api v0.61.0\r\ngo: found k8s.io\/api\/core\/v1 in k8s.io\/api v0.22.4\r\ngo: found k8s.io\/api\/extensions\/v1beta1 in k8s.io\/api v0.22.4\r\ngo: found k8s.io\/apimachinery\/pkg\/apis\/meta\/v1 in k8s.io\/apimachinery v0.22.4\r\ngo: found k8s.io\/apimachinery\/pkg\/runtime in k8s.io\/apimachinery v0.22.4\r\ngo: found k8s.io\/apimachinery\/pkg\/watch in k8s.io\/apimachinery v0.22.4\r\ngo: found k8s.io\/client-go\/kubernetes in k8s.io\/client-go v0.22.4\r\ngo: found k8s.io\/client-go\/rest in k8s.io\/client-go v0.22.4\r\ngo: found k8s.io\/client-go\/tools\/cache in k8s.io\/client-go v0.22.4\r\ngo: found k8s.io\/client-go\/tools\/metrics in k8s.io\/client-go v0.22.4\r\ngo: found k8s.io\/client-go\/util\/workqueue in k8s.io\/client-go v0.22.4\r\ngo: found github.com\/gophercloud\/gophercloud in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/gophercloud\/gophercloud\/openstack in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/extensions\/floatingips in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/extensions\/hypervisors in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/gophercloud\/gophercloud\/openstack\/compute\/v2\/servers in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/gophercloud\/gophercloud\/pagination in github.com\/gophercloud\/gophercloud v0.23.0\r\ngo: found github.com\/samuel\/go-zookeeper\/zk in github.com\/samuel\/go-zookeeper v0.0.0-20201211165307-7117e9ea2414\r\ngo: found github.com\/prometheus\/tsdb in github.com\/prometheus\/tsdb v0.10.0\r\ngo: found github.com\/prometheus\/tsdb\/labels in github.com\/prometheus\/tsdb v0.10.0\r\ngo: finding module for package github.com\/Azure\/azure-sdk-for-go\/arm\/compute\r\ngo: finding module for package github.com\/Azure\/azure-sdk-for-go\/arm\/network\r\n..\/..\/..\/..\/workspace\/pkg\/mod\/github.com\/prometheus\/prometheus@v2.5.0+incompatible\/discovery\/azure\/azure.go:24:2: module github.com\/Azure\/azure-sdk-for-go@latest found (v60.0.0+incompatible), but does not contain package github.com\/Azure\/azure-sdk-for-go\/arm\/compute\r\n..\/..\/..\/..\/workspace\/pkg\/mod\/github.com\/prometheus\/prometheus@v2.5.0+incompatible\/discovery\/azure\/azure.go:25:2: module github.com\/Azure\/azure-sdk-for-go@latest found (v60.0.0+incompatible), but does not contain package github.com\/Azure\/azure-sdk-for-go\/arm\/network\r\n```\r\n\r\n**Environment**\r\n\r\n```\r\ngo version go1.17.2 darwin\/amd64\r\n```\r\n\r\n* System information:\r\n\r\n```\r\nDarwin 21.1.0 x86_64\r\n```","comments":["Also seeing this issue with environment `go version go1.17.3 darwin\/amd64` and system information `Darwin 20.6.0 x86_64`","Could you try GO111MODULE=on go install github.com\/prometheus\/prometheus\/cmd\/promtool@main ?","> Could you try GO111MODULE=on go install github.com\/prometheus\/prometheus\/cmd\/promtool@main ?\r\n\r\n@roidelapluie \r\n```\r\ngo install github.com\/prometheus\/prometheus\/cmd\/promtool@main: github.com\/prometheus\/prometheus@v1.8.2-0.20211208101659-655e2d28795f\r\n\tThe go.mod file for the module providing named packages contains one or\r\n\tmore replace directives. It must not contain directives that would cause\r\n\tit to be interpreted differently than if it were the main module.\r\n```","This is a warning, I expect.","> This is a warning, I expect.\r\n\r\n@roidelapluie but not able to install `promtool` with above command. Return code by the `go install` is `1`.","This indeed seems to be a tricky one. We use replace to map kubernetes logging with our logging.","Has anyone found a fix for this yet? I'm getting the same error.","I am trying to find a solution at #10881. @roidelapluie can you help me checking out if that's still keeping the features form the log replacement? ","i am automating by using promtool in our build system, and seeing this issue. Any updates anyone has for this issue ? Thanks in advance.","Not possible to fix this issue without getting rid of all the `replace` clauses in in `go.mod` file. And it's not easy to get rid of those replaces, they are needed to patch k8s' logging system.\r\n\r\nThis limitation of `go install` is a common issue that annoys people: https:\/\/github.com\/golang\/go\/issues\/44840\r\n\r\nThere's a related proposal to allow `go install` from forks that could help: https:\/\/github.com\/golang\/go\/issues\/50278","This is becoming more urgent now that go 1.18 refuses to install via `go get`, which is the workaround we've been using (e.g. `GO111MODULE=\"on\" go get github.com\/prometheus\/prometheus\/cmd\/promtool`). Unless I'm missing an alternative, we are effectively locked onto go 1.17 for certain operations as long as this remains unfixed."],"labels":["not-as-easy-as-it-looks"]},{"title":"Memory leak on 2.31.1?","body":"**What did you do?**\r\n\r\nI installed kube-promethues-stack with thanos side car enabled and disabled compaction. \r\n\r\n**What did you expect to see?**\r\n\r\nI expected for seamless installation process.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nI watched prometheus memory consumption get higher and higher from day to day.  \r\n\r\n**Environment**\r\nkubernetets\r\n\r\n* System information:\r\n\r\n\tkubernetes version 1.19\r\n\r\n* Prometheus version:\r\n\r\n       2.31.1\r\n       \r\n<img width=\"889\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/92375231\/144750786-dfd14a7f-06f2-4401-9118-ab80a579aa6d.png\">\r\n\r\n* Note\r\n\r\nI restarted the pod with lower retention of 6h\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/92375231\/144751872-a7bf6f9e-5ef1-447a-9b26-c1ea3450583d.png)\r\n","comments":["config\uff1f","@wanrui \r\n\r\n<details>\r\n  <summary>The config (click to expand)<\/summary>\r\n\r\n```\r\nglobal:\r\n  evaluation_interval: 30s\r\n  scrape_interval: 30s\r\n  external_labels:\r\n    prometheus: monitoring\/kube-prometheus-stack-prometheus\r\n    prometheus_replica: prometheus-kube-prometheus-stack-prometheus-0\r\nrule_files:\r\n- \/etc\/prometheus\/rules\/prometheus-kube-prometheus-stack-prometheus-rulefiles-0\/*.yaml\r\nscrape_configs:\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-alertmanager\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  metrics_path: \/metrics\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-alertmanager;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_self_monitor\r\n    - __meta_kubernetes_service_labelpresent_self_monitor\r\n    regex: true;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: web\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: web\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-apiserver\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - default\r\n  scheme: https\r\n  tls_config:\r\n    insecure_skip_verify: false\r\n    server_name: kubernetes\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_component\r\n    - __meta_kubernetes_service_labelpresent_component\r\n    regex: apiserver;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_provider\r\n    - __meta_kubernetes_service_labelpresent_provider\r\n    regex: kubernetes;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: https\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_component\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: https\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-coredns\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-coredns;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http-metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-grafana\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  metrics_path: \/metrics\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: grafana;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: service\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: service\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kube-controller-manager\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-kube-controller-manager;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http-metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kube-etcd\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-kube-etcd;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http-metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kube-proxy\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-kube-proxy;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http-metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kube-scheduler\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-kube-scheduler;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http-metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kube-state-metrics\/0\r\n  honor_labels: true\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: kube-state-metrics;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kubelet\/0\r\n  honor_labels: true\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  scheme: https\r\n  tls_config:\r\n    insecure_skip_verify: true\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    - __meta_kubernetes_service_labelpresent_k8s_app\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: https-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: https-metrics\r\n  - source_labels:\r\n    - __metrics_path__\r\n    target_label: metrics_path\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kubelet\/1\r\n  honor_labels: true\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  metrics_path: \/metrics\/cadvisor\r\n  scheme: https\r\n  tls_config:\r\n    insecure_skip_verify: true\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    - __meta_kubernetes_service_labelpresent_k8s_app\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: https-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: https-metrics\r\n  - source_labels:\r\n    - __metrics_path__\r\n    target_label: metrics_path\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-kubelet\/2\r\n  honor_labels: true\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - kube-system\r\n  metrics_path: \/metrics\/probes\r\n  scheme: https\r\n  tls_config:\r\n    insecure_skip_verify: true\r\n    ca_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/ca.crt\r\n  bearer_token_file: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\/token\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    - __meta_kubernetes_service_labelpresent_k8s_app\r\n    regex: kubelet;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: https-metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_k8s_app\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: https-metrics\r\n  - source_labels:\r\n    - __metrics_path__\r\n    target_label: metrics_path\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-node-exporter\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: prometheus-node-exporter;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - source_labels:\r\n    - __meta_kubernetes_service_label_jobLabel\r\n    target_label: job\r\n    regex: (.+)\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-operator\/0\r\n  honor_labels: true\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  scheme: https\r\n  tls_config:\r\n    insecure_skip_verify: false\r\n    ca_file: \/etc\/prometheus\/certs\/secret_monitoring_kube-prometheus-stack-admission_ca\r\n    server_name: kube-prometheus-stack-operator\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-operator;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: https\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: https\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/kube-prometheus-stack-prometheus\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - monitoring\r\n  metrics_path: \/metrics\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: kube-prometheus-stack-prometheus;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_release\r\n    - __meta_kubernetes_service_labelpresent_release\r\n    regex: kube-prometheus-stack;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_self_monitor\r\n    - __meta_kubernetes_service_labelpresent_self_monitor\r\n    regex: true;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: web\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: web\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-0-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-0;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-1-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-1;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-2-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-2;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-3-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-3;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-4-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-4;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-5-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-5;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-6-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-6;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-7-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-7;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-8-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-8;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/nginx-9-ingress-nginx-controller\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - network\r\n  scrape_interval: 30s\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_component\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_component\r\n    regex: controller;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_instance\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_instance\r\n    regex: nginx-9;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app_kubernetes_io_name\r\n    - __meta_kubernetes_service_labelpresent_app_kubernetes_io_name\r\n    regex: ingress-nginx;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: metrics\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: metrics\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\n- job_name: serviceMonitor\/monitoring\/squadron-node-configurator-sm\/0\r\n  honor_labels: false\r\n  kubernetes_sd_configs:\r\n  - role: endpoints\r\n    namespaces:\r\n      names:\r\n      - customers\r\n  scrape_interval: 15s\r\n  metrics_path: \/metrics\r\n  relabel_configs:\r\n  - source_labels:\r\n    - job\r\n    target_label: __tmp_prometheus_job_name\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_service_label_app\r\n    - __meta_kubernetes_service_labelpresent_app\r\n    regex: squadron-node-configurator;true\r\n  - action: keep\r\n    source_labels:\r\n    - __meta_kubernetes_endpoint_port_name\r\n    regex: http\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Node;(.*)\r\n    replacement: ${1}\r\n    target_label: node\r\n  - source_labels:\r\n    - __meta_kubernetes_endpoint_address_target_kind\r\n    - __meta_kubernetes_endpoint_address_target_name\r\n    separator: ;\r\n    regex: Pod;(.*)\r\n    replacement: ${1}\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_namespace\r\n    target_label: namespace\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: service\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_name\r\n    target_label: pod\r\n  - source_labels:\r\n    - __meta_kubernetes_pod_container_name\r\n    target_label: container\r\n  - source_labels:\r\n    - __meta_kubernetes_service_name\r\n    target_label: job\r\n    replacement: ${1}\r\n  - target_label: endpoint\r\n    replacement: http\r\n  - source_labels:\r\n    - __address__\r\n    target_label: __tmp_hash\r\n    modulus: 1\r\n    action: hashmod\r\n  - source_labels:\r\n    - __tmp_hash\r\n    regex: 0\r\n    action: keep\r\n  metric_relabel_configs: []\r\nalerting:\r\n  alert_relabel_configs:\r\n  - action: labeldrop\r\n    regex: prometheus_replica\r\n  alertmanagers:\r\n  - path_prefix: \/\r\n    scheme: http\r\n    kubernetes_sd_configs:\r\n    - role: endpoints\r\n      namespaces:\r\n        names:\r\n        - monitoring\r\n    api_version: v2\r\n    relabel_configs:\r\n    - action: keep\r\n      source_labels:\r\n      - __meta_kubernetes_service_name\r\n      regex: kube-prometheus-stack-alertmanager\r\n    - action: keep\r\n      source_labels:\r\n      - __meta_kubernetes_endpoint_port_name\r\n      regex: web\r\n  \r\n```\r\n<\/details>","Could you please also share the surrounding logs and memory profiles?","@codesome \r\n\r\n```\r\nts=2021-12-05T13:01:56.932Z caller=main.go:444 level=info msg=\"Starting Prometheus\" version=\"(version=2.31.1, branch=HEAD, revision=411021ada9ab41095923b8d2df9365b632fd40c3)\"\r\nts=2021-12-05T13:01:56.932Z caller=main.go:449 level=info build_context=\"(go=go1.17.3, user=root@9419c9c2d4e0, date=20211105-20:35:02)\"\r\nts=2021-12-05T13:01:56.932Z caller=main.go:450 level=info host_details=\"(Linux 5.4.129+ #1 SMP Wed Aug 18 19:58:18 PDT 2021 x86_64 prometheus-kube-prometheus-stack-prometheus-0 (none))\"\r\nts=2021-12-05T13:01:56.932Z caller=main.go:451 level=info fd_limits=\"(soft=1048576, hard=1048576)\"\r\nts=2021-12-05T13:01:56.932Z caller=main.go:452 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nts=2021-12-05T13:01:56.936Z caller=web.go:542 level=info component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nts=2021-12-05T13:01:56.937Z caller=main.go:839 level=info msg=\"Starting TSDB ...\"\r\nts=2021-12-05T13:01:56.939Z caller=tls_config.go:231 level=info component=web msg=\"TLS is disabled.\" http2=false\r\nts=2021-12-05T13:01:56.940Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638369893402 maxt=1638374400000 ulid=01FNVHJTNCY16FSJHHPKRR5CVD\r\nts=2021-12-05T13:01:56.942Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638374400021 maxt=1638381600000 ulid=01FNVNWAX80DCV3AZXVZRYYAKN\r\nts=2021-12-05T13:01:56.943Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638381600036 maxt=1638388800000 ulid=01FNVWR2H6CK6NXXZS513HEQT5\r\nts=2021-12-05T13:01:56.944Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638388800002 maxt=1638396000000 ulid=01FNW3KSD76798QD2Q1YJRFGWA\r\nts=2021-12-05T13:01:56.946Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638396000000 maxt=1638403200000 ulid=01FNWAFGN7RQV9RJ0X9XA1Y80C\r\nts=2021-12-05T13:01:56.954Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638403200003 maxt=1638410400000 ulid=01FNWHB7X7FWXXDCFCTJ4D4M71\r\nts=2021-12-05T13:01:56.956Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638410400022 maxt=1638417600000 ulid=01FNWR6Z57PZVHFADD7Q1W63D7\r\nts=2021-12-05T13:01:56.966Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638417600006 maxt=1638424800000 ulid=01FNWZ2PD8FBKZKEHZKG9FKR19\r\nts=2021-12-05T13:01:56.968Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638424800003 maxt=1638432000000 ulid=01FNX5YDN8TMRA4TMRZJD27QF3\r\nts=2021-12-05T13:01:56.968Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638432000019 maxt=1638439200000 ulid=01FNXCT4X8AC6D50BB8R6NRFD6\r\nts=2021-12-05T13:01:56.970Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638439200036 maxt=1638446400000 ulid=01FNXKNWJ9R56Q6A2J9P91HNE8\r\nts=2021-12-05T13:01:56.973Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638446400036 maxt=1638453600000 ulid=01FNXTHKRVEYDFWSW49TM25VWR\r\nts=2021-12-05T13:01:56.974Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638453600000 maxt=1638460800000 ulid=01FNY1DAN887Y6XXDT79FVZGKR\r\nts=2021-12-05T13:01:56.976Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638460800036 maxt=1638468000000 ulid=01FNY89292VH72GGAGCJE08Q5X\r\nts=2021-12-05T13:01:56.977Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638468000036 maxt=1638475200000 ulid=01FNYF4SHB3RFHVRN824J33789\r\nts=2021-12-05T13:01:56.979Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638475200030 maxt=1638482400000 ulid=01FNYP0GD7CJKVARG374C8W4DQ\r\nts=2021-12-05T13:01:56.979Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638482400020 maxt=1638489600000 ulid=01FNYWW7N7DTVM7Y64YVGK9A9P\r\nts=2021-12-05T13:01:56.980Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638489600021 maxt=1638496800000 ulid=01FNZ3QYX7ZC8G8PM52FTE1JGY\r\nts=2021-12-05T13:01:56.981Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638496800010 maxt=1638504000000 ulid=01FNZAKP57P6Z0ZT48Y9X2EGVD\r\nts=2021-12-05T13:01:56.982Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638504000011 maxt=1638511200000 ulid=01FNZHFDD87FG3EF9CV4ZHP3P7\r\nts=2021-12-05T13:01:56.983Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638511200036 maxt=1638518400000 ulid=01FNZRB50TTNF96P4MCQ643AVT\r\nts=2021-12-05T13:01:56.984Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638518400033 maxt=1638525600000 ulid=01FNZZ6VX791X75AHWQAQG55E4\r\nts=2021-12-05T13:01:56.986Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638525600012 maxt=1638532800000 ulid=01FP062K58AVJADXD7641Y3Q4C\r\nts=2021-12-05T13:01:56.988Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638532800015 maxt=1638540000000 ulid=01FP0CYAD7QWMAHWK9QXMA4BSX\r\nts=2021-12-05T13:01:56.990Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638540000036 maxt=1638547200000 ulid=01FP0KT21JTW15AW9W4PCRRWT3\r\nts=2021-12-05T13:01:56.990Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638547200001 maxt=1638554400000 ulid=01FP0TNRX8ZQG2EP5AZP0ZDSP9\r\nts=2021-12-05T13:01:56.991Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638554400027 maxt=1638561600000 ulid=01FP11HG57YSQXVTC5RKZAD1CF\r\nts=2021-12-05T13:01:56.991Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638561600014 maxt=1638568800000 ulid=01FP18D7D7SB64N6NHDYRMY2T8\r\nts=2021-12-05T13:01:56.992Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638568800036 maxt=1638576000000 ulid=01FP1F8Z1X5ND5VRZC3B5W1AJV\r\nts=2021-12-05T13:01:56.992Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638576000017 maxt=1638583200000 ulid=01FP1P4NX70SN667MA8R16FQ9B\r\nts=2021-12-05T13:01:56.993Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638583200002 maxt=1638590400000 ulid=01FP1X0D57N73232ZR2SES3VK1\r\nts=2021-12-05T13:01:56.993Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638590400002 maxt=1638597600000 ulid=01FP23W4D71R1T16J7MMT72WJ7\r\nts=2021-12-05T13:01:56.994Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638597600007 maxt=1638604800000 ulid=01FP2AQVN8109PZXKBKFN88XTK\r\nts=2021-12-05T13:01:56.995Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638604800009 maxt=1638612000000 ulid=01FP2HKJX7GYZK4ABGVEX9TKW5\r\nts=2021-12-05T13:01:56.996Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638612000011 maxt=1638619200000 ulid=01FP2RFA58HSAE3TBPGXVNGEMA\r\nts=2021-12-05T13:01:56.996Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638619200025 maxt=1638626400000 ulid=01FP2ZB1D8P7A6NT2QRQWS1MVF\r\nts=2021-12-05T13:01:56.997Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638626400036 maxt=1638633600000 ulid=01FP366S1F776V38WNF214F0A5\r\nts=2021-12-05T13:01:56.997Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638633600036 maxt=1638640800000 ulid=01FP3D2G8WFPC4M1YR5522H75X\r\nts=2021-12-05T13:01:56.998Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638640800036 maxt=1638648000000 ulid=01FP3KY7HF26CSVJM311JCZ3B8\r\nts=2021-12-05T13:01:56.999Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638648000013 maxt=1638655200000 ulid=01FP3TSYD8K4NVPYEPCRJNKE0J\r\nts=2021-12-05T13:01:57.000Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638655200027 maxt=1638662400000 ulid=01FP41NNN8K2PSFRV7M7G0FTFK\r\nts=2021-12-05T13:01:57.002Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638662400003 maxt=1638669600000 ulid=01FP48HCX831TE2YGKH2HTPFT1\r\nts=2021-12-05T13:01:57.003Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638669600030 maxt=1638676800000 ulid=01FP4FD45870K1TMB6CCSSB3TM\r\nts=2021-12-05T13:01:57.005Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638676800023 maxt=1638684000000 ulid=01FP4P8VD7E8AKVKFWXFB2TRSH\r\nts=2021-12-05T13:01:57.007Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638684000014 maxt=1638691200000 ulid=01FP4X4JN8VACE7ZPBZZNDM7VJ\r\nts=2021-12-05T13:01:57.009Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638691200002 maxt=1638698400000 ulid=01FP5409X7845HSG378RZK425Q\r\nts=2021-12-05T13:01:57.011Z caller=repair.go:57 level=info component=tsdb msg=\"Found healthy block\" mint=1638698400009 maxt=1638705600000 ulid=01FP5AW157G7PQPKHWAEJZTAXJ\r\nts=2021-12-05T13:01:58.331Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP2AQVN8109PZXKBKFN88XTK\r\nts=2021-12-05T13:01:58.337Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP1P4NX70SN667MA8R16FQ9B\r\nts=2021-12-05T13:01:58.344Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNZZ6VX791X75AHWQAQG55E4\r\nts=2021-12-05T13:01:58.349Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNXKNWJ9R56Q6A2J9P91HNE8\r\nts=2021-12-05T13:01:58.360Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP3TSYD8K4NVPYEPCRJNKE0J\r\nts=2021-12-05T13:01:58.366Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP3D2G8WFPC4M1YR5522H75X\r\nts=2021-12-05T13:01:58.371Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP18D7D7SB64N6NHDYRMY2T8\r\nts=2021-12-05T13:01:58.374Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP0KT21JTW15AW9W4PCRRWT3\r\nts=2021-12-05T13:01:58.378Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNY1DAN887Y6XXDT79FVZGKR\r\nts=2021-12-05T13:01:58.383Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNXCT4X8AC6D50BB8R6NRFD6\r\nts=2021-12-05T13:01:58.389Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP23W4D71R1T16J7MMT72WJ7\r\nts=2021-12-05T13:01:58.393Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP1X0D57N73232ZR2SES3VK1\r\nts=2021-12-05T13:01:58.398Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNWHB7X7FWXXDCFCTJ4D4M71\r\nts=2021-12-05T13:01:58.403Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNZRB50TTNF96P4MCQ643AVT\r\nts=2021-12-05T13:01:58.408Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNZ3QYX7ZC8G8PM52FTE1JGY\r\nts=2021-12-05T13:01:58.412Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNYF4SHB3RFHVRN824J33789\r\nts=2021-12-05T13:01:58.418Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP366S1F776V38WNF214F0A5\r\nts=2021-12-05T13:01:58.423Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP2ZB1D8P7A6NT2QRQWS1MVF\r\nts=2021-12-05T13:01:58.428Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP3KY7HF26CSVJM311JCZ3B8\r\nts=2021-12-05T13:01:58.432Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP2RFA58HSAE3TBPGXVNGEMA\r\nts=2021-12-05T13:01:58.436Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP0CYAD7QWMAHWK9QXMA4BSX\r\nts=2021-12-05T13:01:58.440Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNZHFDD87FG3EF9CV4ZHP3P7\r\nts=2021-12-05T13:01:58.444Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNY89292VH72GGAGCJE08Q5X\r\nts=2021-12-05T13:01:58.450Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP4FD45870K1TMB6CCSSB3TM\r\nts=2021-12-05T13:01:58.455Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP41NNN8K2PSFRV7M7G0FTFK\r\nts=2021-12-05T13:01:58.459Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP062K58AVJADXD7641Y3Q4C\r\nts=2021-12-05T13:01:58.464Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNYP0GD7CJKVARG374C8W4DQ\r\nts=2021-12-05T13:01:58.468Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNX5YDN8TMRA4TMRZJD27QF3\r\nts=2021-12-05T13:01:58.472Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNVNWAX80DCV3AZXVZRYYAKN\r\nts=2021-12-05T13:01:58.475Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNVHJTNCY16FSJHHPKRR5CVD\r\nts=2021-12-05T13:01:58.480Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP2HKJX7GYZK4ABGVEX9TKW5\r\nts=2021-12-05T13:01:58.486Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP11HG57YSQXVTC5RKZAD1CF\r\nts=2021-12-05T13:01:58.489Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNZAKP57P6Z0ZT48Y9X2EGVD\r\nts=2021-12-05T13:01:58.495Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNYWW7N7DTVM7Y64YVGK9A9P\r\nts=2021-12-05T13:01:58.500Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNWZ2PD8FBKZKEHZKG9FKR19\r\nts=2021-12-05T13:01:58.504Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNWR6Z57PZVHFADD7Q1W63D7\r\nts=2021-12-05T13:01:58.508Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNVWR2H6CK6NXXZS513HEQT5\r\nts=2021-12-05T13:01:58.512Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP48HCX831TE2YGKH2HTPFT1\r\nts=2021-12-05T13:01:58.519Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP0TNRX8ZQG2EP5AZP0ZDSP9\r\nts=2021-12-05T13:01:58.523Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNWAFGN7RQV9RJ0X9XA1Y80C\r\nts=2021-12-05T13:01:58.526Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNW3KSD76798QD2Q1YJRFGWA\r\nts=2021-12-05T13:01:58.530Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP1F8Z1X5ND5VRZC3B5W1AJV\r\nts=2021-12-05T13:01:58.534Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FNXTHKRVEYDFWSW49TM25VWR\r\nts=2021-12-05T13:01:58.534Z caller=head.go:479 level=info component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nts=2021-12-05T13:01:59.265Z caller=head.go:513 level=info component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=731.535219ms\r\nts=2021-12-05T13:01:59.266Z caller=head.go:519 level=info component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nts=2021-12-05T13:02:01.529Z caller=head.go:555 level=info component=tsdb msg=\"WAL checkpoint loaded\"\r\nts=2021-12-05T13:02:03.084Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=186 maxSegment=190\r\nts=2021-12-05T13:02:04.962Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=187 maxSegment=190\r\nts=2021-12-05T13:02:06.647Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=188 maxSegment=190\r\nts=2021-12-05T13:02:07.017Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=189 maxSegment=190\r\nts=2021-12-05T13:02:07.017Z caller=head.go:590 level=info component=tsdb msg=\"WAL segment loaded\" segment=190 maxSegment=190\r\nts=2021-12-05T13:02:07.017Z caller=head.go:596 level=info component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=2.263342856s wal_replay_duration=5.488213842s total_replay_duration=8.483174259s\r\nts=2021-12-05T13:02:07.589Z caller=main.go:866 level=info fs_type=EXT4_SUPER_MAGIC\r\nts=2021-12-05T13:02:07.589Z caller=main.go:869 level=info msg=\"TSDB started\"\r\nts=2021-12-05T13:02:07.589Z caller=main.go:996 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml\r\nts=2021-12-05T13:02:07.597Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.598Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.599Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.600Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.601Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.602Z caller=kubernetes.go:284 level=info component=\"discovery manager notify\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.669Z caller=main.go:1033 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml totalDuration=79.600055ms db_storage=1.576\u00b5s remote_storage=1.726\u00b5s web_handler=542ns query_engine=1.39\u00b5s scrape=294.436\u00b5s scrape_sd=4.680093ms notify=24.752\u00b5s notify_sd=1.073879ms rules=65.782914ms\r\nts=2021-12-05T13:02:07.669Z caller=main.go:811 level=info msg=\"Server is ready to receive web requests.\"\r\nts=2021-12-05T13:02:07.669Z caller=main.go:996 level=info msg=\"Loading configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml\r\nts=2021-12-05T13:02:07.676Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.677Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.678Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.679Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.679Z caller=kubernetes.go:284 level=info component=\"discovery manager scrape\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.680Z caller=kubernetes.go:284 level=info component=\"discovery manager notify\" discovery=kubernetes msg=\"Using pod service account via in-cluster config\"\r\nts=2021-12-05T13:02:07.763Z caller=main.go:1033 level=info msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/config_out\/prometheus.env.yaml totalDuration=93.947555ms db_storage=1.61\u00b5s remote_storage=2.509\u00b5s web_handler=809ns query_engine=1.193\u00b5s scrape=65.525\u00b5s scrape_sd=4.52332ms notify=21.06\u00b5s notify_sd=1.004681ms rules=81.358175ms\r\nts=2021-12-05T15:00:10.367Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638705600000 maxt=1638712800000 ulid=01FP5HQRD96EK21P2H8NPVKGG1 duration=10.325432427s\r\nts=2021-12-05T15:00:10.377Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP4P8VD7E8AKVKFWXFB2TRSH\r\nts=2021-12-05T15:00:10.818Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=441.024004ms\r\nts=2021-12-05T15:00:10.893Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=186 to_segment=190 mint=1638712800000\r\nts=2021-12-05T15:00:16.447Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=186 last=190 duration=5.55512674s\r\nts=2021-12-05T17:00:10.371Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638712800008 maxt=1638720000000 ulid=01FP5RKFN816ZTJXM0HJ3FP90Q duration=10.330706293s\r\nts=2021-12-05T17:00:10.380Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP4X4JN8VACE7ZPBZZNDM7VJ\r\nts=2021-12-05T17:00:10.869Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=488.716594ms\r\nts=2021-12-05T17:00:10.923Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=191 to_segment=194 mint=1638720000000\r\nts=2021-12-05T17:00:16.466Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=191 last=194 duration=5.543885866s\r\nts=2021-12-05T19:00:10.645Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638720000030 maxt=1638727200000 ulid=01FP5ZF6X8C2A32J8929BEH85C duration=10.605235861s\r\nts=2021-12-05T19:00:10.657Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP5409X7845HSG378RZK425Q\r\nts=2021-12-05T19:00:10.963Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=306.710545ms\r\nts=2021-12-05T19:00:11.028Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=195 to_segment=198 mint=1638727200000\r\nts=2021-12-05T19:00:16.224Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=195 last=198 duration=5.195600088s\r\nts=2021-12-05T21:00:10.880Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638727200036 maxt=1638734400000 ulid=01FP66AYG86T0RYYP8DCSF80RD duration=10.487931253s\r\nts=2021-12-05T21:00:10.889Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP5AW157G7PQPKHWAEJZTAXJ\r\nts=2021-12-05T21:00:11.261Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=371.811949ms\r\nts=2021-12-05T21:00:11.329Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=199 to_segment=202 mint=1638734400000\r\nts=2021-12-05T21:00:16.484Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=199 last=202 duration=5.155226571s\r\nts=2021-12-05T23:00:10.660Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638734400036 maxt=1638741600000 ulid=01FP6D6NRJBXWD8DBX3Z2707K0 duration=10.258157984s\r\nts=2021-12-05T23:00:10.687Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP5HQRD96EK21P2H8NPVKGG1\r\nts=2021-12-05T23:00:11.093Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=405.65128ms\r\nts=2021-12-05T23:00:11.157Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=203 to_segment=206 mint=1638741600000\r\nts=2021-12-05T23:00:16.689Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=203 last=206 duration=5.532479806s\r\nts=2021-12-06T01:00:10.083Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638741600013 maxt=1638748800000 ulid=01FP6M2CN8EWDQ2GKHJTPR5SFF duration=10.043253579s\r\nts=2021-12-06T01:00:10.111Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP5RKFN816ZTJXM0HJ3FP90Q\r\nts=2021-12-06T01:00:10.512Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=401.523366ms\r\nts=2021-12-06T01:00:10.575Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=207 to_segment=210 mint=1638748800000\r\nts=2021-12-06T01:00:16.172Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=207 last=210 duration=5.597546299s\r\nts=2021-12-06T03:00:10.563Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638748800004 maxt=1638756000000 ulid=01FP6TY3X89QGHC17TZ012R8J7 duration=10.523754896s\r\nts=2021-12-06T03:00:10.590Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP5ZF6X8C2A32J8929BEH85C\r\nts=2021-12-06T03:00:11.064Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=473.11636ms\r\nts=2021-12-06T03:00:11.127Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=211 to_segment=214 mint=1638756000000\r\nts=2021-12-06T03:00:16.340Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=211 last=214 duration=5.212537638s\r\nts=2021-12-06T05:00:11.050Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638756000017 maxt=1638763200000 ulid=01FP71SV58H5YVNDT24BT6MM1Q duration=11.009744578s\r\nts=2021-12-06T05:00:11.078Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP66AYG86T0RYYP8DCSF80RD\r\nts=2021-12-06T05:00:11.693Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=615.253422ms\r\nts=2021-12-06T05:00:11.758Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=215 to_segment=218 mint=1638763200000\r\nts=2021-12-06T05:00:17.024Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=215 last=218 duration=5.265973462s\r\nts=2021-12-06T07:00:10.513Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638763200036 maxt=1638770400000 ulid=01FP78NJSCD1EB1S7PPHDF8S26 duration=10.084672874s\r\nts=2021-12-06T07:00:10.540Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP6D6NRJBXWD8DBX3Z2707K0\r\nts=2021-12-06T07:00:11.135Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=594.290849ms\r\nts=2021-12-06T07:00:11.213Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=219 to_segment=222 mint=1638770400000\r\nts=2021-12-06T07:00:16.827Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=219 last=222 duration=5.61445242s\r\nts=2021-12-06T09:00:10.535Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638770400012 maxt=1638777600000 ulid=01FP7FH9N97WQMG6B2PSMMHKPN duration=10.494236091s\r\nts=2021-12-06T09:00:10.563Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP6M2CN8EWDQ2GKHJTPR5SFF\r\nts=2021-12-06T09:00:11.025Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=461.329595ms\r\nts=2021-12-06T09:00:11.087Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=223 to_segment=226 mint=1638777600000\r\nts=2021-12-06T09:00:16.436Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=223 last=226 duration=5.348777399s\r\nts=2021-12-06T11:00:10.505Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638777600020 maxt=1638784800000 ulid=01FP7PD0X877NVGHKV1RQZH5B1 duration=10.465027362s\r\nts=2021-12-06T11:00:10.532Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP6TY3X89QGHC17TZ012R8J7\r\nts=2021-12-06T11:00:10.957Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=424.223991ms\r\nts=2021-12-06T11:00:11.024Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=227 to_segment=230 mint=1638784800000\r\nts=2021-12-06T11:00:16.334Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=227 last=230 duration=5.310170206s\r\nts=2021-12-06T13:00:10.306Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638784800016 maxt=1638792000000 ulid=01FP7X8R598S1ZDJV15VT5BAPM duration=10.265545807s\r\nts=2021-12-06T13:00:10.334Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP71SV58H5YVNDT24BT6MM1Q\r\nts=2021-12-06T13:00:10.785Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=450.939426ms\r\nts=2021-12-06T13:00:10.849Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=231 to_segment=234 mint=1638792000000\r\nts=2021-12-06T13:00:16.495Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=231 last=234 duration=5.646499187s\r\nts=2021-12-06T15:00:11.199Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638792000036 maxt=1638799200000 ulid=01FP844FSPTDDHJ34JQZ9X6P90 duration=10.761770884s\r\nts=2021-12-06T15:00:11.228Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP78NJSCD1EB1S7PPHDF8S26\r\nts=2021-12-06T15:00:11.610Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=382.185973ms\r\nts=2021-12-06T15:00:11.676Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=235 to_segment=238 mint=1638799200000\r\nts=2021-12-06T15:00:16.889Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=235 last=238 duration=5.212671533s\r\nts=2021-12-06T17:00:10.428Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638799200008 maxt=1638806400000 ulid=01FP8B06N8S6GVCQND85X6HZPV duration=10.387880498s\r\nts=2021-12-06T17:00:10.457Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP7FH9N97WQMG6B2PSMMHKPN\r\nts=2021-12-06T17:00:10.874Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=416.873781ms\r\nts=2021-12-06T17:00:10.949Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=239 to_segment=242 mint=1638806400000\r\nts=2021-12-06T17:00:16.160Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=239 last=242 duration=5.211381434s\r\nts=2021-12-06T19:00:10.394Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638806400028 maxt=1638813600000 ulid=01FP8HVXX9QG832PVCKDVE266X duration=10.35360559s\r\nts=2021-12-06T19:00:10.421Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP7PD0X877NVGHKV1RQZH5B1\r\nts=2021-12-06T19:00:10.829Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=407.677725ms\r\nts=2021-12-06T19:00:10.891Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=243 to_segment=246 mint=1638813600000\r\nts=2021-12-06T19:00:16.445Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=243 last=246 duration=5.554492467s\r\nts=2021-12-06T21:00:10.800Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638813600036 maxt=1638820800000 ulid=01FP8RQNGWMPMJKJGW7B92QGK8 duration=10.388072794s\r\nts=2021-12-06T21:00:10.828Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP7X8R598S1ZDJV15VT5BAPM\r\nts=2021-12-06T21:00:11.191Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=363.709544ms\r\nts=2021-12-06T21:00:11.253Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=247 to_segment=250 mint=1638820800000\r\nts=2021-12-06T21:00:16.529Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=247 last=250 duration=5.276070393s\r\nts=2021-12-06T23:00:10.504Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638820800002 maxt=1638828000000 ulid=01FP8ZKCD8QHVZ4633DVVP5FPJ duration=10.463640891s\r\nts=2021-12-06T23:00:10.531Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP844FSPTDDHJ34JQZ9X6P90\r\nts=2021-12-06T23:00:10.965Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=433.872144ms\r\nts=2021-12-06T23:00:11.029Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=251 to_segment=254 mint=1638828000000\r\nts=2021-12-06T23:00:16.221Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=251 last=254 duration=5.192241192s\r\nts=2021-12-07T01:00:10.119Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638828000014 maxt=1638835200000 ulid=01FP96F3N8AJHXYQE83EFJJ5P2 duration=10.078392133s\r\nts=2021-12-07T01:00:10.146Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP8B06N8S6GVCQND85X6HZPV\r\nts=2021-12-07T01:00:10.510Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=364.837627ms\r\nts=2021-12-07T01:00:10.571Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=255 to_segment=258 mint=1638835200000\r\nts=2021-12-07T01:00:16.138Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=255 last=258 duration=5.567873095s\r\nts=2021-12-07T03:00:10.444Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638835200018 maxt=1638842400000 ulid=01FP9DATX7J4Z05K1X90E5H528 duration=10.404608809s\r\nts=2021-12-07T03:00:10.471Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP8HVXX9QG832PVCKDVE266X\r\nts=2021-12-07T03:00:10.968Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=497.413371ms\r\nts=2021-12-07T03:00:11.027Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=259 to_segment=262 mint=1638842400000\r\nts=2021-12-07T03:00:16.800Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=259 last=262 duration=5.773542633s\r\nts=2021-12-07T05:00:11.460Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638842400014 maxt=1638849600000 ulid=01FP9M6J58N1SWVDTHM3YX4MN8 duration=11.420630778s\r\nts=2021-12-07T05:00:11.488Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP8RQNGWMPMJKJGW7B92QGK8\r\nts=2021-12-07T05:00:12.100Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=611.829173ms\r\nts=2021-12-07T05:00:12.165Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=263 to_segment=266 mint=1638849600000\r\nts=2021-12-07T05:00:17.417Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=263 last=266 duration=5.251904131s\r\nts=2021-12-07T07:00:10.711Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638849600020 maxt=1638856800000 ulid=01FP9V29D7DFZ1C5WXQ2B59KNT duration=10.671890739s\r\nts=2021-12-07T07:00:10.740Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP8ZKCD8QHVZ4633DVVP5FPJ\r\nts=2021-12-07T07:00:11.261Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=521.421055ms\r\nts=2021-12-07T07:00:11.328Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=267 to_segment=270 mint=1638856800000\r\nts=2021-12-07T07:00:16.673Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=267 last=270 duration=5.345246154s\r\nts=2021-12-07T09:00:11.048Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638856800036 maxt=1638864000000 ulid=01FPA1Y11EDBRN7X6ZXRJV64JP duration=10.618018322s\r\nts=2021-12-07T09:00:11.075Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP96F3N8AJHXYQE83EFJJ5P2\r\nts=2021-12-07T09:00:11.454Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=378.524477ms\r\nts=2021-12-07T09:00:11.511Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=271 to_segment=274 mint=1638864000000\r\nts=2021-12-07T09:00:16.914Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=271 last=274 duration=5.403376257s\r\nts=2021-12-07T11:00:11.102Z caller=compact.go:518 level=info component=tsdb msg=\"write block\" mint=1638864000036 maxt=1638871200000 ulid=01FPA8SR8XPS8A01H6SGJ390PS duration=10.688475179s\r\nts=2021-12-07T11:00:11.132Z caller=db.go:1293 level=info component=tsdb msg=\"Deleting obsolete block\" block=01FP9DATX7J4Z05K1X90E5H528\r\nts=2021-12-07T11:00:11.650Z caller=head.go:803 level=info component=tsdb msg=\"Head GC completed\" duration=517.708252ms\r\nts=2021-12-07T11:00:11.732Z caller=checkpoint.go:97 level=info component=tsdb msg=\"Creating checkpoint\" from_segment=275 to_segment=278 mint=1638871200000\r\nts=2021-12-07T11:00:17.798Z caller=head.go:972 level=info component=tsdb msg=\"WAL checkpoint complete\" first=275 last=278 duration=6.066522095s\r\n```","@codesome \r\n\r\nHow can I provide the memory profile? ","@MalulDekel `<prometheus-url>\/debug\/pprof\/heap` and `<prometheus-url>\/debug\/pprof\/allocs` for heap and alloc profiles respectively. It should get downloaded automatically if you open this URL directly (and not clicking `...?debug=1` URLs on the `\/debug\/pprof` UI)","> @MalulDekel `<prometheus-url>\/debug\/pprof\/heap` and `<prometheus-url>\/debug\/pprof\/allocs` for heap and alloc profiles respectively. It should get downloaded automatically if you open this URL directly (and not clicking `...?debug=1` URLs on the `\/debug\/pprof` UI)\r\n\r\nWhat is the file ending this file needs?\r\nI can't upload them in here as is\r\n@codesome ","You can use `.gz` if github allows it.","You can also share it on https:\/\/share.polarsignals.com\/ and provide us the link.","@codesome \r\n[allocs.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/7670365\/allocs.gz)\r\n[heap.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/7670368\/heap.gz)\r\n","@MalulDekel  It does not look like an archive","I'm also running 2.31.1\r\nWe tried increasing the RAM memory for the pod and that gave the pod some time to load all the WAL files. But even after loading all WAL files it was killed by the kubernetes OOMKiller. \r\n\r\nWe then tried to clear the entire data folder and this did show a decrease in memory usage, but eventually it ran out of memory again.\r\n\r\nLogs do not show any errors.","I can also confirm that Prometheus v2.31.1 is OOMing running in Kubernetes v1.19.13.\r\n\r\nI can reproduce the scenario every time by running a specific Grafana Dashboard query against my Prometheus v2.31.1 instance.\r\nThe result is identical to @MalulDekel  and @JoostvdB94, Prometheus instantly OOMs and is killed by the Kubernetes OOMKiller controller and logs nothing before OOM.\r\n\r\nNote, I have never run into this issue with Prometheus in Kubernetes and I even increased the pod container's limited RAM from 1GB to 8GB. The same OOM occurred, so it appears to be quite severe.\r\n\r\nOut of fun I actually downgraded my Prometheus pod to v2.30.3 and experienced the exact same OOM scenario with the exact same query.\r\n\r\nHere is the query that is causing the OOM as logged by Prometheus pod container after it starts back up after crashing. Note that only this query from a Grafana dashboard that is set to query greater than 2 days of Prometheus data OOM's Prometheus v2.31.1. 6 hour queries work fine. Also, previous version of Prometheus v2.2x.x worked fine with this query as well:\r\n\r\n```\r\nts=2021-12-13T21:25:06.082Z caller=query_logger.go:79 level=info component=activeQueryTracker msg=\"These queries didn't finish in prometheus' last run:\" queries=\"[{\\\"query\\\":\\\"sum(phpfpm_process_last_request_memory{pool=\\\\\\\"www\\\\\\\"}) by (pod)\\\",\\\"timestamp_sec\\\":1639430699},{\\\"query\\\":\\\"sum(phpfpm_process_requests{pool=\\\\\\\"www\\\\\\\"}) by (pod)\\\",\\\"timestamp_sec\\\":1639430699},{\\\"query\\\":\\\"avg(phpfpm_process_request_duration{pool=\\\\\\\"www\\\\\\\"})\\\",\\\"timestamp_sec\\\":1639430694},{\\\"query\\\":\\\"avg(phpfpm_process_request_duration{pool=\\\\\\\"www\\\\\\\", pod=~\\\\\\\"(foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-5jpzn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-7gxgb|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-7qznj|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-92r4s|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-9xnvm|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-b46rm|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-bmvwr|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-c4z7n|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-g9dsd|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-nb8dn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-nmg9f|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-qvpwf|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-vxmbb|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-vzrpx|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-wgsrk|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-xmjzn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-cf5d7656f-2cjjj|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-cf5d7656f-469ld|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-,{\\\"query\\\":\\\"sum(phpfpm_process_state{pool=\\\\\\\"www\\\\\\\", pod=~\\\\\\\"(foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-5jpzn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-7gxgb|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-7qznj|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-92r4s|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-9xnvm|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-b46rm|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-bmvwr|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-c4z7n| foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-g9dsd|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-nb8dn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-nmg9f|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-qvpwf|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-vxmbb|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-vzrpx|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-wgsrk|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.81-1-6546b4f6bb-xmjzn|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-cf5d7656f-2cjjj|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-cf5d7656f-469ld|foobar-php-fpm-v1\\\\\\\\\\\\\\\\.1\\\\\\\\\\\\\\\\.83-1-cf5d7656f-4]\"\r\n```","@codesome can you please look at this when you are back? Thanks!","@siris Can you try to find  the first version that caused the problem?","@testn absolutely. I did some testing and here are the results:\r\nProm versions able to reproduce this memory leak.  All test queries ran are on a 7 day time range:\r\n* `v2.32.1`\r\n* `v2.30.3`\r\n* `v2.29.2`\r\n* `v2.28.1`\r\n* `v2.27.1`\r\n* `v2.26.1`\r\n* `v2.25.2`\r\n* `v2.24.1`\r\n* `v2.23.0`\r\n* `v2.22.2`\r\n* `v2.21.0`\r\n\r\nLast know version not able to reproduce this memory leak:\r\n* I couldn't find one. I went back to 11 latest minor version releases from `v2.32.1` to `v2.21.0`, all exhibited the same memory leak behavior running in Kubernetes.\r\n\r\nHere is the PromQL in my Grafana Dashboard that is reproducing this memory leak:\r\n* Dashboard panel has two time series queries it runs:\r\n  1. `sum(phpfpm_process_state{pool=\"www\", pod=~\"$pod\"}) by (pod,state)`\r\n  2. `sum(phpfpm_process_state{pool=\"www\"})`\r\n  3. The `$pod` label value dashboard variable is populated by this query `label_values(phpfpm_up,pod)`\r\n\r\nMy Prometheus Kubernetes pod container has these resource specs:\r\n```\r\n        resources:\r\n          limits:\r\n            cpu: 1\r\n            memory: 1Gi\r\n          requests:\r\n            cpu: 100m\r\n            memory: 256Mi\r\n```\r\nStartup args for those pod:\r\n```\r\n        args:\r\n        - --config.file=\/etc\/prometheus\/conf\/prometheus.yaml\r\n        - --storage.tsdb.path=\/data\r\n        - --storage.tsdb.retention.time=90d\r\n        - --web.enable-lifecycle\r\n        - --web.external-url=https:\/\/prometheus.foobar.com\r\n```\r\n\r\nI went ahead and generated the debug heap and alloc dumps for the `v2.32.1` instance I am testing on:\r\n * [heap_v2.32.1.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/7748134\/heap_v2.32.1.gz)\r\n * [allocs_v2.32.1.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/7748135\/allocs_v2.32.1.gz)\r\n\r\nHopefully all this helps in isolating the issue. If there is a newly tagged Docker image published sometime in the future with a fix I can test it in my Kubernetes environment.","@MalulDekel: The profiles you provided seems to be html Google login page :) Can you make sure the `\/debug\/pprof\/heap` and `\/debug\/pprof\/allocs` route is authenticated before getting the profile?\r\n\r\n@siris:\r\nYour profiles are looking normal and as expected. `loadMmappedChunks` is taking some heap because when you query it loads some chunks from the disk into the memory to read the samples. \r\nWhat do you get for `count(phpfpm_process_state{pool=\"www\"})` (hopefully this does not OOM), or what is the active series count (via `prometheus_tsdb_head_series`)?\r\n1Gi limit could also be the issue and not necessarily a memory leak, because you faced the OOM on running queries, which is expected for low memory limits. I suggest increasing the memory limits until you are able to run a query successfully.\r\nIf the memory keeps increasing for some time (say 3+ hours) then that can tell that there is a memory leak and profiles of that moment will be helpful in debugging.","@siris How big are your wal files? Did you also see the same behavior if you start from scratch (i.e. no existing data files)?","@codesome how do you know if query can make overload on the memory? Is there a way to check top 10 overload queries that I am doing on Grafana?","we are experiencing exactly the same issue , working with bitnami Prometheus operator "],"labels":["kind\/bug","kind\/more-info-needed"]},{"title":"snapshot will lead memory leaks?","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nrun cmd  \r\n```\r\ncurl -XPOST http:\/\/xx.xx.xx.xx:9090\/api\/v1\/admin\/tsdb\/snapshot\r\n```\r\n12h, memory increased very fast!\r\n\r\nmay this cmd load all blocks in to the memory ,then i kill the cmd backend. but memory still stay high.\r\n\r\n**What did you expect to see?**\r\nwhen i kill the cmd , the memory usage will go low level!\r\n\r\n**What did you see instead? Under which circumstances?**\r\n memory still stay high.\r\n\r\n**Environment**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/12728608\/144533864-1397ba28-33a9-4bee-b2e5-8284ba97d5bb.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/12728608\/144534521-f189c2dc-f44e-439a-a07d-41c6a83cc898.png)\r\n\r\n\r\n* System information:\r\n\r\nLinux 3.10.0-327.el7.x86_64 x86_64\r\n\r\n\r\n\r\n* Prometheus version:\r\n```\r\nprometheus, version 2.27.0 (branch: HEAD, revision: 24c9b61221f7006e87cd62b9fe2901d43e19ed53)\r\n  build user:       root@f27daa3b3fec\r\n  build date:       20210512-18:04:51\r\n  go version:       go1.16.4\r\n  platform:         linux\/amd64\r\n\r\n```\r\n\r\n* Alertmanager version:\r\n\r\nno use\r\n\r\n* Prometheus configuration file:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/12728608\/144543919-b897b44c-eb76-4ab6-906e-f2521a116938.png)\r\n\r\n\r\n\r\n\r\n* Alertmanager configuration file:\r\nno use\r\n\r\n\r\n\r\n* Logs:\r\n```\r\nDec 02 15:24:23 xxx prometheus[207705]: level=warn ts=2021-12-02T07:24:23.827Z caller=main.go:377 deprecation_notice=\"'storage.tsdb.retention' flag is deprecated use 'storage.tsdb.retention.\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.827Z caller=main.go:426 msg=\"Starting Prometheus\" version=\"(version=2.27.0, branch=HEAD, revision=24c9b61221f7006e87\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.827Z caller=main.go:431 build_context=\"(go=go1.16.4, user=root@f27daa3b3fec, date=20210512-18:04:51)\"\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.827Z caller=main.go:432 host_details=\"(Linux 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 ops.a3\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.827Z caller=main.go:433 fd_limits=\"(soft=65535, hard=65535)\"\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.827Z caller=main.go:434 vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.831Z caller=web.go:540 component=web msg=\"Start listening for connections\" address=10.32.214.10:9090\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.832Z caller=web.go:574 component=web msg=\"Router prefix\" prefix=\/prometheus\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.833Z caller=main.go:803 msg=\"Starting TSDB ...\"\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.834Z caller=tls_config.go:191 component=web msg=\"TLS is disabled.\" http2=false\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.846Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1622462400000 maxt=1623045600000 ulid=01F7K3R\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.846Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1623045600000 maxt=1623628800000 ulid=01F84FV\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.846Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1623628800000 maxt=1624212000000 ulid=01F8NW0\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.846Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1624212000000 maxt=1624795200000 ulid=01F9786\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.846Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1624795200000 maxt=1625378400000 ulid=01F9RMK\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1625378400000 maxt=1625961600000 ulid=01FAA1C\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1625961600000 maxt=1626544800000 ulid=01FAVE1\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1626544800000 maxt=1627128000000 ulid=01FBCTH\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1627128000000 maxt=1627711200000 ulid=01FBY73\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1627711200000 maxt=1628294400000 ulid=01FCFKF\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.847Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1628294400000 maxt=1628877600000 ulid=01FD10B\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1628877600000 maxt=1629460800000 ulid=01FDJCN\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1629460800000 maxt=1630044000000 ulid=01FE3RW\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1630044000000 maxt=1630627200000 ulid=01FEN54\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1630627200000 maxt=1631210400000 ulid=01FF6GV\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1631210400000 maxt=1631793600000 ulid=01FFQXF\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1631793600000 maxt=1632376800000 ulid=01FG99H\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.848Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1632376800000 maxt=1632960000000 ulid=01FGTQ2\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1632960000000 maxt=1633543200000 ulid=01FHC3A\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1633543200000 maxt=1634126400000 ulid=01FHXFT\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1634126400000 maxt=1634709600000 ulid=01FJEW8\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1634709600000 maxt=1635292800000 ulid=01FK08F\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1635292800000 maxt=1635876000000 ulid=01FKHK6\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.849Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1635876000000 maxt=1636459200000 ulid=01FM30V\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1636459200000 maxt=1637042400000 ulid=01FMMCZ\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637042400000 maxt=1637625600000 ulid=01FN5SJ\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637625600000 maxt=1637820000000 ulid=01FNCR2\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637820000000 maxt=1637884800000 ulid=01FNJPK\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637884800000 maxt=1637949600000 ulid=01FNJRK\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1637949600000 maxt=1638014400000 ulid=01FNJTH\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1638014403522 maxt=1638093600000 ulid=01FNV63\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1638403200000 maxt=1638410400000 ulid=01FNWHB\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1638410400000 maxt=1638417600000 ulid=01FNWR6\r\nDec 02 15:24:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:24:23.850Z caller=repair.go:57 component=tsdb msg=\"Found healthy block\" mint=1638343891328 maxt=1638403200000 ulid=01FNWT7\r\nDec 02 15:28:53 xxx prometheus[207705]: level=info ts=2021-12-02T07:28:53.483Z caller=head.go:741 component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nDec 02 15:29:24 xxx prometheus[207705]: level=info ts=2021-12-02T07:29:24.367Z caller=head.go:755 component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=30.883051769s\r\nDec 02 15:29:24 xxx prometheus[207705]: level=info ts=2021-12-02T07:29:24.367Z caller=head.go:761 component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nDec 02 15:33:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:33:50.333Z caller=head.go:787 component=tsdb msg=\"WAL checkpoint loaded\"\r\nDec 02 15:33:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:33:54.986Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483478 maxSegment=1483767\r\nDec 02 15:33:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:33:59.394Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483479 maxSegment=1483767\r\nDec 02 15:34:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:03.652Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483480 maxSegment=1483767\r\nDec 02 15:34:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:08.072Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483481 maxSegment=1483767\r\nDec 02 15:34:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:12.782Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483482 maxSegment=1483767\r\nDec 02 15:34:17 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:17.171Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483483 maxSegment=1483767\r\nDec 02 15:34:21 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:21.554Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483484 maxSegment=1483767\r\nDec 02 15:34:26 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:26.056Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483485 maxSegment=1483767\r\nDec 02 15:34:30 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:30.424Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483486 maxSegment=1483767\r\nDec 02 15:34:34 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:34.667Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483487 maxSegment=1483767\r\nDec 02 15:34:38 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:38.950Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483488 maxSegment=1483767\r\nDec 02 15:34:43 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:43.249Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483489 maxSegment=1483767\r\nDec 02 15:34:47 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:47.528Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483490 maxSegment=1483767\r\nDec 02 15:34:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:51.524Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483491 maxSegment=1483767\r\nDec 02 15:34:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:34:55.678Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483492 maxSegment=1483767\r\nDec 02 15:35:00 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:00.234Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483493 maxSegment=1483767\r\nDec 02 15:35:07 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:07.362Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483494 maxSegment=1483767\r\nDec 02 15:35:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:11.485Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483495 maxSegment=1483767\r\nDec 02 15:35:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:15.610Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483496 maxSegment=1483767\r\nDec 02 15:35:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:19.636Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483497 maxSegment=1483767\r\nDec 02 15:35:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:23.809Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483498 maxSegment=1483767\r\nDec 02 15:35:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:28.020Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483499 maxSegment=1483767\r\nDec 02 15:35:32 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:32.137Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483500 maxSegment=1483767\r\nDec 02 15:35:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:36.449Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483501 maxSegment=1483767\r\nDec 02 15:35:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:40.552Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483502 maxSegment=1483767\r\nDec 02 15:35:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:44.584Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483503 maxSegment=1483767\r\nDec 02 15:35:48 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:48.703Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483504 maxSegment=1483767\r\nDec 02 15:35:52 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:52.743Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483505 maxSegment=1483767\r\nDec 02 15:35:57 xxx prometheus[207705]: level=info ts=2021-12-02T07:35:57.220Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483506 maxSegment=1483767\r\nDec 02 15:36:01 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:01.537Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483507 maxSegment=1483767\r\nDec 02 15:36:05 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:05.764Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483508 maxSegment=1483767\r\nDec 02 15:36:09 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:09.761Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483509 maxSegment=1483767\r\nDec 02 15:36:13 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:13.832Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483510 maxSegment=1483767\r\nDec 02 15:36:17 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:17.931Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483511 maxSegment=1483767\r\nDec 02 15:36:22 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:22.011Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483512 maxSegment=1483767\r\nDec 02 15:36:26 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:26.151Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483513 maxSegment=1483767\r\nDec 02 15:36:30 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:30.253Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483514 maxSegment=1483767\r\nDec 02 15:36:34 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:34.423Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483515 maxSegment=1483767\r\nDec 02 15:36:41 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:41.463Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483516 maxSegment=1483767\r\nDec 02 15:36:45 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:45.716Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483517 maxSegment=1483767\r\nDec 02 15:36:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:50.001Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483518 maxSegment=1483767\r\nDec 02 15:36:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:54.085Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483519 maxSegment=1483767\r\nDec 02 15:36:58 xxx prometheus[207705]: level=info ts=2021-12-02T07:36:58.361Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483520 maxSegment=1483767\r\nDec 02 15:37:02 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:02.540Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483521 maxSegment=1483767\r\nDec 02 15:37:06 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:06.584Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483522 maxSegment=1483767\r\nDec 02 15:37:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:10.791Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483523 maxSegment=1483767\r\nDec 02 15:37:14 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:14.933Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483524 maxSegment=1483767\r\nDec 02 15:37:18 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:18.904Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483525 maxSegment=1483767\r\nDec 02 15:37:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:23.007Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483526 maxSegment=1483767\r\nDec 02 15:37:26 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:26.790Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483527 maxSegment=1483767\r\nDec 02 15:37:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:31.162Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483528 maxSegment=1483767\r\nDec 02 15:37:35 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:35.383Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483529 maxSegment=1483767\r\nDec 02 15:37:39 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:39.431Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483530 maxSegment=1483767\r\nDec 02 15:37:43 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:43.697Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483531 maxSegment=1483767\r\nDec 02 15:37:47 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:47.932Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483532 maxSegment=1483767\r\nDec 02 15:37:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:51.956Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483533 maxSegment=1483767\r\nDec 02 15:37:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:37:55.991Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483534 maxSegment=1483767\r\nDec 02 15:38:00 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:00.142Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483535 maxSegment=1483767\r\nDec 02 15:38:07 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:07.443Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483536 maxSegment=1483767\r\nDec 02 15:38:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:11.535Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483537 maxSegment=1483767\r\nDec 02 15:38:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:15.810Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483538 maxSegment=1483767\r\nDec 02 15:38:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:19.712Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483539 maxSegment=1483767\r\nDec 02 15:38:24 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:24.062Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483540 maxSegment=1483767\r\nDec 02 15:38:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:28.119Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483541 maxSegment=1483767\r\nDec 02 15:38:32 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:32.217Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483542 maxSegment=1483767\r\nDec 02 15:38:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:36.182Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483543 maxSegment=1483767\r\nDec 02 15:38:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:40.156Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483544 maxSegment=1483767\r\nDec 02 15:38:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:44.497Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483545 maxSegment=1483767\r\nDec 02 15:38:48 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:48.672Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483546 maxSegment=1483767\r\nDec 02 15:38:52 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:52.628Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483547 maxSegment=1483767\r\nDec 02 15:38:56 xxx prometheus[207705]: level=info ts=2021-12-02T07:38:56.619Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483548 maxSegment=1483767\r\nDec 02 15:39:00 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:00.709Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483549 maxSegment=1483767\r\nDec 02 15:39:05 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:05.084Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483550 maxSegment=1483767\r\nDec 02 15:39:09 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:09.244Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483551 maxSegment=1483767\r\nDec 02 15:39:13 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:13.061Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483552 maxSegment=1483767\r\nDec 02 15:39:17 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:17.128Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483553 maxSegment=1483767\r\nDec 02 15:39:21 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:21.363Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483554 maxSegment=1483767\r\nDec 02 15:39:25 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:25.324Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483555 maxSegment=1483767\r\nDec 02 15:39:29 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:29.371Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483556 maxSegment=1483767\r\nDec 02 15:39:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:36.571Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483557 maxSegment=1483767\r\nDec 02 15:39:41 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:41.167Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483558 maxSegment=1483767\r\nDec 02 15:39:45 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:45.378Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483559 maxSegment=1483767\r\nDec 02 15:39:49 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:49.640Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483560 maxSegment=1483767\r\nDec 02 15:39:53 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:53.653Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483561 maxSegment=1483767\r\nDec 02 15:39:57 xxx prometheus[207705]: level=info ts=2021-12-02T07:39:57.782Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483562 maxSegment=1483767\r\nDec 02 15:40:01 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:01.809Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483563 maxSegment=1483767\r\nDec 02 15:40:05 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:05.978Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483564 maxSegment=1483767\r\nDec 02 15:40:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:10.250Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483565 maxSegment=1483767\r\nDec 02 15:40:14 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:14.360Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483566 maxSegment=1483767\r\nDec 02 15:40:18 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:18.450Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483567 maxSegment=1483767\r\nDec 02 15:40:22 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:22.888Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483568 maxSegment=1483767\r\nDec 02 15:40:26 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:26.980Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483569 maxSegment=1483767\r\nDec 02 15:40:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:31.065Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483570 maxSegment=1483767\r\nDec 02 15:40:34 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:34.898Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483571 maxSegment=1483767\r\nDec 02 15:40:39 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:39.087Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483572 maxSegment=1483767\r\nDec 02 15:40:43 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:43.315Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483573 maxSegment=1483767\r\nDec 02 15:40:47 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:47.347Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483574 maxSegment=1483767\r\nDec 02 15:40:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:51.484Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483575 maxSegment=1483767\r\nDec 02 15:40:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:40:55.843Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483576 maxSegment=1483767\r\nDec 02 15:41:04 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:04.123Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483577 maxSegment=1483767\r\nDec 02 15:41:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:10.128Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483578 maxSegment=1483767\r\nDec 02 15:41:14 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:14.592Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483579 maxSegment=1483767\r\nDec 02 15:41:18 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:18.656Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483580 maxSegment=1483767\r\nDec 02 15:41:22 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:22.978Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483581 maxSegment=1483767\r\nDec 02 15:41:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:27.300Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483582 maxSegment=1483767\r\nDec 02 15:41:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:31.717Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483583 maxSegment=1483767\r\nDec 02 15:41:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:36.091Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483584 maxSegment=1483767\r\nDec 02 15:41:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:40.507Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483585 maxSegment=1483767\r\nDec 02 15:41:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:44.665Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483586 maxSegment=1483767\r\nDec 02 15:41:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:50.768Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483587 maxSegment=1483767\r\nDec 02 15:41:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:55.079Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483588 maxSegment=1483767\r\nDec 02 15:41:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:41:59.493Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483589 maxSegment=1483767\r\nDec 02 15:42:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:03.817Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483590 maxSegment=1483767\r\nDec 02 15:42:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:08.229Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483591 maxSegment=1483767\r\nDec 02 15:42:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:12.500Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483592 maxSegment=1483767\r\nDec 02 15:42:16 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:16.917Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483593 maxSegment=1483767\r\nDec 02 15:42:21 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:21.128Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483594 maxSegment=1483767\r\nDec 02 15:42:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:28.365Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483595 maxSegment=1483767\r\nDec 02 15:42:32 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:32.658Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483596 maxSegment=1483767\r\nDec 02 15:42:37 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:37.171Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483597 maxSegment=1483767\r\nDec 02 15:42:41 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:41.435Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483598 maxSegment=1483767\r\nDec 02 15:42:45 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:45.730Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483599 maxSegment=1483767\r\nDec 02 15:42:49 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:49.921Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483600 maxSegment=1483767\r\nDec 02 15:42:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:54.481Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483601 maxSegment=1483767\r\nDec 02 15:42:58 xxx prometheus[207705]: level=info ts=2021-12-02T07:42:58.948Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483602 maxSegment=1483767\r\nDec 02 15:43:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:03.064Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483603 maxSegment=1483767\r\nDec 02 15:43:07 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:07.164Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483604 maxSegment=1483767\r\nDec 02 15:43:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:11.327Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483605 maxSegment=1483767\r\nDec 02 15:43:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:15.373Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483606 maxSegment=1483767\r\nDec 02 15:43:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:19.508Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483607 maxSegment=1483767\r\nDec 02 15:43:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:23.727Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483608 maxSegment=1483767\r\nDec 02 15:43:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:27.955Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483609 maxSegment=1483767\r\nDec 02 15:43:32 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:32.090Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483610 maxSegment=1483767\r\nDec 02 15:43:35 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:35.924Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483611 maxSegment=1483767\r\nDec 02 15:43:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:40.028Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483612 maxSegment=1483767\r\nDec 02 15:43:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:44.212Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483613 maxSegment=1483767\r\nDec 02 15:43:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:51.607Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483614 maxSegment=1483767\r\nDec 02 15:43:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:55.765Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483615 maxSegment=1483767\r\nDec 02 15:43:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:43:59.902Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483616 maxSegment=1483767\r\nDec 02 15:44:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:03.917Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483617 maxSegment=1483767\r\nDec 02 15:44:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:08.045Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483618 maxSegment=1483767\r\nDec 02 15:44:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:12.214Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483619 maxSegment=1483767\r\nDec 02 15:44:16 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:16.327Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483620 maxSegment=1483767\r\nDec 02 15:44:20 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:20.452Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483621 maxSegment=1483767\r\nDec 02 15:44:24 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:24.597Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483622 maxSegment=1483767\r\nDec 02 15:44:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:28.676Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483623 maxSegment=1483767\r\nDec 02 15:44:33 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:33.082Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483624 maxSegment=1483767\r\nDec 02 15:44:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:36.926Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483625 maxSegment=1483767\r\nDec 02 15:44:41 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:41.113Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483626 maxSegment=1483767\r\nDec 02 15:44:45 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:45.363Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483627 maxSegment=1483767\r\nDec 02 15:44:49 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:49.360Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483628 maxSegment=1483767\r\nDec 02 15:44:53 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:53.632Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483629 maxSegment=1483767\r\nDec 02 15:44:57 xxx prometheus[207705]: level=info ts=2021-12-02T07:44:57.910Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483630 maxSegment=1483767\r\nDec 02 15:45:02 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:02.122Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483631 maxSegment=1483767\r\nDec 02 15:45:06 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:06.178Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483632 maxSegment=1483767\r\nDec 02 15:45:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:10.300Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483633 maxSegment=1483767\r\nDec 02 15:45:14 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:14.268Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483634 maxSegment=1483767\r\nDec 02 15:45:18 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:18.328Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483635 maxSegment=1483767\r\nDec 02 15:45:26 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:26.210Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483636 maxSegment=1483767\r\nDec 02 15:45:30 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:30.193Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483637 maxSegment=1483767\r\nDec 02 15:45:34 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:34.305Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483638 maxSegment=1483767\r\nDec 02 15:45:38 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:38.371Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483639 maxSegment=1483767\r\nDec 02 15:45:42 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:42.503Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483640 maxSegment=1483767\r\nDec 02 15:45:46 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:46.624Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483641 maxSegment=1483767\r\nDec 02 15:45:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:50.962Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483642 maxSegment=1483767\r\nDec 02 15:45:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:55.042Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483643 maxSegment=1483767\r\nDec 02 15:45:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:45:59.070Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483644 maxSegment=1483767\r\nDec 02 15:46:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:03.217Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483645 maxSegment=1483767\r\nDec 02 15:46:07 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:07.589Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483646 maxSegment=1483767\r\nDec 02 15:46:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:11.567Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483647 maxSegment=1483767\r\nDec 02 15:46:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:15.614Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483648 maxSegment=1483767\r\nDec 02 15:46:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:19.694Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483649 maxSegment=1483767\r\nDec 02 15:46:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:23.684Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483650 maxSegment=1483767\r\nDec 02 15:46:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:27.971Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483651 maxSegment=1483767\r\nDec 02 15:46:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:31.974Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483652 maxSegment=1483767\r\nDec 02 15:46:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:36.063Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483653 maxSegment=1483767\r\nDec 02 15:46:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:40.198Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483654 maxSegment=1483767\r\nDec 02 15:46:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:44.379Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483655 maxSegment=1483767\r\nDec 02 15:46:52 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:52.219Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483656 maxSegment=1483767\r\nDec 02 15:46:56 xxx prometheus[207705]: level=info ts=2021-12-02T07:46:56.311Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483657 maxSegment=1483767\r\nDec 02 15:47:00 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:00.459Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483658 maxSegment=1483767\r\nDec 02 15:47:04 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:04.576Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483659 maxSegment=1483767\r\nDec 02 15:47:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:08.707Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483660 maxSegment=1483767\r\nDec 02 15:47:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:12.978Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483661 maxSegment=1483767\r\nDec 02 15:47:17 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:17.116Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483662 maxSegment=1483767\r\nDec 02 15:47:21 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:21.200Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483663 maxSegment=1483767\r\nDec 02 15:47:25 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:25.322Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483664 maxSegment=1483767\r\nDec 02 15:47:29 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:29.602Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483665 maxSegment=1483767\r\nDec 02 15:47:33 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:33.671Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483666 maxSegment=1483767\r\nDec 02 15:47:37 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:37.772Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483667 maxSegment=1483767\r\nDec 02 15:47:42 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:42.091Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483668 maxSegment=1483767\r\nDec 02 15:47:46 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:46.226Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483669 maxSegment=1483767\r\nDec 02 15:47:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:50.365Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483670 maxSegment=1483767\r\nDec 02 15:47:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:54.586Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483671 maxSegment=1483767\r\nDec 02 15:47:58 xxx prometheus[207705]: level=info ts=2021-12-02T07:47:58.778Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483672 maxSegment=1483767\r\nDec 02 15:48:02 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:02.840Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483673 maxSegment=1483767\r\nDec 02 15:48:07 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:07.253Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483674 maxSegment=1483767\r\nDec 02 15:48:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:11.314Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483675 maxSegment=1483767\r\nDec 02 15:48:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:19.171Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483676 maxSegment=1483767\r\nDec 02 15:48:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:23.321Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483677 maxSegment=1483767\r\nDec 02 15:48:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:27.260Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483678 maxSegment=1483767\r\nDec 02 15:48:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:31.321Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483679 maxSegment=1483767\r\nDec 02 15:48:35 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:35.464Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483680 maxSegment=1483767\r\nDec 02 15:48:39 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:39.678Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483681 maxSegment=1483767\r\nDec 02 15:48:47 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:47.108Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483682 maxSegment=1483767\r\nDec 02 15:48:56 xxx prometheus[207705]: level=info ts=2021-12-02T07:48:56.380Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483683 maxSegment=1483767\r\nDec 02 15:49:00 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:00.286Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483684 maxSegment=1483767\r\nDec 02 15:49:06 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:06.875Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483685 maxSegment=1483767\r\nDec 02 15:49:11 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:11.254Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483686 maxSegment=1483767\r\nDec 02 15:49:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:15.388Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483687 maxSegment=1483767\r\nDec 02 15:49:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:19.440Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483688 maxSegment=1483767\r\nDec 02 15:49:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:23.326Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483689 maxSegment=1483767\r\nDec 02 15:49:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:27.297Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483690 maxSegment=1483767\r\nDec 02 15:49:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:31.529Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483691 maxSegment=1483767\r\nDec 02 15:49:35 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:35.710Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483692 maxSegment=1483767\r\nDec 02 15:49:39 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:39.734Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483693 maxSegment=1483767\r\nDec 02 15:49:43 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:43.689Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483694 maxSegment=1483767\r\nDec 02 15:49:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:51.452Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483695 maxSegment=1483767\r\nDec 02 15:49:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:55.670Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483696 maxSegment=1483767\r\nDec 02 15:49:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:49:59.665Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483697 maxSegment=1483767\r\nDec 02 15:50:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:03.778Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483698 maxSegment=1483767\r\nDec 02 15:50:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:08.045Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483699 maxSegment=1483767\r\nDec 02 15:50:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:12.097Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483700 maxSegment=1483767\r\nDec 02 15:50:16 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:16.321Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483701 maxSegment=1483767\r\nDec 02 15:50:20 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:20.067Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483702 maxSegment=1483767\r\nDec 02 15:50:24 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:24.279Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483703 maxSegment=1483767\r\nDec 02 15:50:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:28.476Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483704 maxSegment=1483767\r\nDec 02 15:50:32 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:32.612Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483705 maxSegment=1483767\r\nDec 02 15:50:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:36.613Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483706 maxSegment=1483767\r\nDec 02 15:50:40 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:40.775Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483707 maxSegment=1483767\r\nDec 02 15:50:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:44.991Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483708 maxSegment=1483767\r\nDec 02 15:50:49 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:49.102Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483709 maxSegment=1483767\r\nDec 02 15:50:53 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:53.518Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483710 maxSegment=1483767\r\nDec 02 15:50:57 xxx prometheus[207705]: level=info ts=2021-12-02T07:50:57.420Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483711 maxSegment=1483767\r\nDec 02 15:51:01 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:01.604Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483712 maxSegment=1483767\r\nDec 02 15:51:05 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:05.907Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483713 maxSegment=1483767\r\nDec 02 15:51:09 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:09.922Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483714 maxSegment=1483767\r\nDec 02 15:51:17 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:17.471Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483715 maxSegment=1483767\r\nDec 02 15:51:21 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:21.486Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483716 maxSegment=1483767\r\nDec 02 15:51:25 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:25.760Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483717 maxSegment=1483767\r\nDec 02 15:51:29 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:29.944Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483718 maxSegment=1483767\r\nDec 02 15:51:33 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:33.936Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483719 maxSegment=1483767\r\nDec 02 15:51:38 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:38.094Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483720 maxSegment=1483767\r\nDec 02 15:51:41 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:41.909Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483721 maxSegment=1483767\r\nDec 02 15:51:46 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:46.018Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483722 maxSegment=1483767\r\nDec 02 15:51:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:50.138Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483723 maxSegment=1483767\r\nDec 02 15:51:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:54.159Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483724 maxSegment=1483767\r\nDec 02 15:51:58 xxx prometheus[207705]: level=info ts=2021-12-02T07:51:58.225Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483725 maxSegment=1483767\r\nDec 02 15:52:02 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:02.308Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483726 maxSegment=1483767\r\nDec 02 15:52:06 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:06.585Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483727 maxSegment=1483767\r\nDec 02 15:52:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:10.770Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483728 maxSegment=1483767\r\nDec 02 15:52:15 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:15.275Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483729 maxSegment=1483767\r\nDec 02 15:52:19 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:19.481Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483730 maxSegment=1483767\r\nDec 02 15:52:28 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:28.906Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483731 maxSegment=1483767\r\nDec 02 15:52:36 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:36.959Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483732 maxSegment=1483767\r\nDec 02 15:52:45 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:45.058Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483733 maxSegment=1483767\r\nDec 02 15:52:49 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:49.442Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483734 maxSegment=1483767\r\nDec 02 15:52:53 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:53.696Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483735 maxSegment=1483767\r\nDec 02 15:52:58 xxx prometheus[207705]: level=info ts=2021-12-02T07:52:58.188Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483736 maxSegment=1483767\r\nDec 02 15:53:02 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:02.463Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483737 maxSegment=1483767\r\nDec 02 15:53:06 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:06.594Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483738 maxSegment=1483767\r\nDec 02 15:53:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:10.665Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483739 maxSegment=1483767\r\nDec 02 15:53:14 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:14.868Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483740 maxSegment=1483767\r\nDec 02 15:53:18 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:18.971Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483741 maxSegment=1483767\r\nDec 02 15:53:23 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:23.344Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483742 maxSegment=1483767\r\nDec 02 15:53:27 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:27.472Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483743 maxSegment=1483767\r\nDec 02 15:53:31 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:31.768Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483744 maxSegment=1483767\r\nDec 02 15:53:35 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:35.993Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483745 maxSegment=1483767\r\nDec 02 15:53:39 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:39.944Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483746 maxSegment=1483767\r\nDec 02 15:53:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:44.013Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483747 maxSegment=1483767\r\nDec 02 15:53:50 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:50.510Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483748 maxSegment=1483767\r\nDec 02 15:53:54 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:54.902Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483749 maxSegment=1483767\r\nDec 02 15:53:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:53:59.249Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483750 maxSegment=1483767\r\nDec 02 15:54:03 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:03.727Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483751 maxSegment=1483767\r\nDec 02 15:54:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:08.019Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483752 maxSegment=1483767\r\nDec 02 15:54:12 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:12.556Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483753 maxSegment=1483767\r\nDec 02 15:54:20 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:20.943Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483754 maxSegment=1483767\r\nDec 02 15:54:25 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:25.314Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483755 maxSegment=1483767\r\nDec 02 15:54:29 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:29.675Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483756 maxSegment=1483767\r\nDec 02 15:54:34 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:34.030Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483757 maxSegment=1483767\r\nDec 02 15:54:38 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:38.489Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483758 maxSegment=1483767\r\nDec 02 15:54:42 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:42.896Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483759 maxSegment=1483767\r\nDec 02 15:54:47 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:47.104Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483760 maxSegment=1483767\r\nDec 02 15:54:51 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:51.284Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483761 maxSegment=1483767\r\nDec 02 15:54:55 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:55.493Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483762 maxSegment=1483767\r\nDec 02 15:54:59 xxx prometheus[207705]: level=info ts=2021-12-02T07:54:59.767Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483763 maxSegment=1483767\r\nDec 02 15:55:04 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:04.003Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483764 maxSegment=1483767\r\nDec 02 15:55:08 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:08.033Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483765 maxSegment=1483767\r\nDec 02 15:55:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:10.559Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483766 maxSegment=1483767\r\nDec 02 15:55:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:10.560Z caller=head.go:813 component=tsdb msg=\"WAL segment loaded\" segment=1483767 maxSegment=1483767\r\nDec 02 15:55:10 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:10.560Z caller=head.go:818 component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=4m25.966435153s wal_rep\r\nDec 02 15:55:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:44.430Z caller=main.go:828 fs_type=XFS_SUPER_MAGIC\r\nDec 02 15:55:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:44.430Z caller=main.go:831 msg=\"TSDB started\"\r\nDec 02 15:55:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:44.430Z caller=main.go:957 msg=\"Loading configuration file\" filename=\/opt\/prometheus\/prometheus\/prometheus.yml\r\nDec 02 15:55:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:44.447Z caller=main.go:988 msg=\"Completed loading of configuration file\" filename=\/opt\/prometheus\/prometheus\/prometheus\r\nDec 02 15:55:44 xxx prometheus[207705]: level=info ts=2021-12-02T07:55:44.447Z caller=main.go:775 msg=\"Server is ready to receive web requests.\"\r\nDec 02 16:11:56 xxx prometheus[207705]: level=info ts=2021-12-02T08:11:56.126Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638417600000 maxt=1638419400000 ulid=01FNX28XF47G1\r\nDec 02 16:12:32 xxx prometheus[207705]: level=info ts=2021-12-02T08:12:32.557Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=36.255042288s\r\nDec 02 16:27:40 xxx prometheus[207705]: level=info ts=2021-12-02T08:27:40.447Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638419400000 maxt=1638421200000 ulid=01FNX37GYKVQ7\r\nDec 02 16:28:14 xxx prometheus[207705]: level=info ts=2021-12-02T08:28:14.882Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=34.231570978s\r\nDec 02 16:44:09 xxx prometheus[207705]: level=info ts=2021-12-02T08:44:09.662Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638421200000 maxt=1638423000000 ulid=01FNX449AJABE\r\nDec 02 16:44:42 xxx prometheus[207705]: level=info ts=2021-12-02T08:44:42.206Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=32.342311934s\r\nDec 02 17:00:17 xxx prometheus[207705]: level=info ts=2021-12-02T09:00:17.136Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638423000000 maxt=1638424800000 ulid=01FNX52DS2WED\r\nDec 02 17:01:00 xxx prometheus[207705]: level=info ts=2021-12-02T09:01:00.023Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=42.57921971s\r\nDec 02 17:15:18 xxx prometheus[207705]: level=info ts=2021-12-02T09:15:18.441Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638424800000 maxt=1638426600000 ulid=01FNX609DW7XT\r\nDec 02 17:15:56 xxx prometheus[207705]: level=info ts=2021-12-02T09:15:56.430Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=37.687711673s\r\nDec 02 17:30:59 xxx prometheus[207705]: level=info ts=2021-12-02T09:30:59.314Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638426600000 maxt=1638428400000 ulid=01FNX6VKP55AX\r\nDec 02 17:31:35 xxx prometheus[207705]: level=info ts=2021-12-02T09:31:35.222Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=35.75274751s\r\nDec 02 17:45:13 xxx prometheus[207705]: level=info ts=2021-12-02T09:45:13.889Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638428400000 maxt=1638430200000 ulid=01FNX7R90Y3MG\r\nDec 02 17:45:52 xxx prometheus[207705]: level=info ts=2021-12-02T09:45:52.124Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=38.05721395s\r\nDec 02 17:59:12 xxx prometheus[207705]: level=info ts=2021-12-02T09:59:12.021Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638431491328 maxt=1638432000000 ulid=01FNX8JEG7JG9\r\nDec 02 17:59:40 xxx prometheus[207705]: level=info ts=2021-12-02T09:59:40.929Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=28.688825481s\r\nDec 02 18:12:18 xxx prometheus[207705]: level=info ts=2021-12-02T10:12:18.575Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638432000000 maxt=1638433800000 ulid=01FNX9BPY2RCS\r\nDec 02 18:12:53 xxx prometheus[207705]: level=info ts=2021-12-02T10:12:53.303Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=34.467119785s\r\nDec 02 18:25:43 xxx prometheus[207705]: level=info ts=2021-12-02T10:25:43.225Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638433800000 maxt=1638435600000 ulid=01FNXA3WW1CZP\r\nDec 02 18:26:44 xxx prometheus[207705]: level=info ts=2021-12-02T10:26:44.644Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=1m1.212018138s\r\nDec 02 18:38:57 xxx prometheus[207705]: level=info ts=2021-12-02T10:38:57.002Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638435600000 maxt=1638437400000 ulid=01FNXAX8V1J8B\r\nDec 02 18:39:35 xxx prometheus[207705]: level=info ts=2021-12-02T10:39:35.569Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=38.065348161s\r\nDec 02 18:51:38 xxx prometheus[207705]: level=info ts=2021-12-02T10:51:38.391Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638437400000 maxt=1638439200000 ulid=01FNXBMSVH6J4\r\nDec 02 18:52:09 xxx prometheus[207705]: level=info ts=2021-12-02T10:52:09.891Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=31.314942083s\r\nDec 02 19:04:26 xxx prometheus[207705]: level=info ts=2021-12-02T11:04:26.187Z caller=compact.go:513 component=tsdb msg=\"write block\" mint=1638439200000 maxt=1638441000000 ulid=01FNXCBT7P1K4\r\nDec 02 19:05:01 xxx prometheus[207705]: level=info ts=2021-12-02T11:05:01.167Z caller=head.go:925 component=tsdb msg=\"Head GC completed\" duration=34.825086786s\r\nDec 02 19:05:02 xxx prometheus[207705]: level=info ts=2021-12-02T11:05:02.035Z caller=checkpoint.go:97 component=tsdb msg=\"Creating checkpoint\" from_segment=1483478 to_segment=1483869 mint=1\r\nDec 02 19:09:24 xxx prometheus[207705]: level=info ts=2021-12-02T11:09:24.745Z caller=main.go:957 msg=\"Loading configuration file\" filename=\/opt\/prometheus\/prometheus\/prometheus.yml\r\nDec 02 19:09:24 xxx prometheus[207705]: level=info ts=2021-12-02T11:09:24.886Z caller=main.go:988 msg=\"Completed loading of configuration file\" filename=\/opt\/prometheus\/prometheus\/prometheus\r\nDec 02 19:20:47 xxx prometheus[207705]: level=info ts=2021-12-02T11:20:47.347Z caller=head.go:1022 component=tsdb msg=\"WAL checkpoint complete\" first=1483478 last=1483869 duration=15m45.3147\r\nDec 02 19:20:47 xxx prometheus[207705]: level=warn ts=2021-12-02T11:20:47.347Z caller=db.go:888 component=tsdb msg=\"Head compaction took longer than the block time range, compactions are fal\r\nDec 02 19:44:11 xxx prometheus[207705]: level=info ts=2021-12-02T11:44:11.007Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=3 mint=1638419400000 maxt=1638424800000 ulid=01\r\nDec 02 19:44:12 xxx prometheus[207705]: level=info ts=2021-12-02T11:44:11.966Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX37GYKVQ7W1QT864WKMFSW\r\nDec 02 19:44:12 xxx prometheus[207705]: level=info ts=2021-12-02T11:44:12.864Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX52DS2WEDFK01BG5VPGY94\r\nDec 02 19:44:13 xxx prometheus[207705]: level=info ts=2021-12-02T11:44:13.866Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX449AJABE8K1HS3RRZ7MMC\r\nDec 02 20:07:53 xxx prometheus[207705]: level=info ts=2021-12-02T12:07:53.647Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=3 mint=1638424800000 maxt=1638430200000 ulid=01\r\nDec 02 20:07:55 xxx prometheus[207705]: level=info ts=2021-12-02T12:07:55.045Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX7R90Y3MG2MN5RFSP1X983\r\nDec 02 20:07:56 xxx prometheus[207705]: level=info ts=2021-12-02T12:07:55.999Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX6VKP55AXJ87VWBGN5J5VX\r\nDec 02 20:07:56 xxx prometheus[207705]: level=info ts=2021-12-02T12:07:56.819Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX609DW7XTHGR82VWT6ZGCR\r\nDec 02 20:31:18 xxx prometheus[207705]: level=info ts=2021-12-02T12:31:18.439Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=3 mint=1638431491328 maxt=1638435600000 ulid=01\r\nDec 02 20:31:19 xxx prometheus[207705]: level=info ts=2021-12-02T12:31:19.839Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX9BPY2RCS59RYKRAN17GE1\r\nDec 02 20:31:20 xxx prometheus[207705]: level=info ts=2021-12-02T12:31:20.611Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX8JEG7JG9W9SGTP430ADJW\r\nDec 02 20:31:21 xxx prometheus[207705]: level=info ts=2021-12-02T12:31:21.778Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNXA3WW1CZP6W09Y4PNDN9G8\r\nDec 02 20:57:09 xxx prometheus[207705]: level=info ts=2021-12-02T12:57:09.286Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=3 mint=1638403200000 maxt=1638419400000 ulid=01\r\nDec 02 20:57:10 xxx prometheus[207705]: level=info ts=2021-12-02T12:57:10.417Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNX28XF47G1PM4TJW76WDSA2\r\nDec 02 20:57:11 xxx prometheus[207705]: level=info ts=2021-12-02T12:57:11.966Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNWHB7WVF4DC8AM4XXX2TXQ7\r\nDec 02 20:57:13 xxx prometheus[207705]: level=info ts=2021-12-02T12:57:13.770Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNWR6Z4VTD52CEBVYX8SKY3S\r\nDec 02 21:24:31 xxx prometheus[207705]: level=info ts=2021-12-02T13:24:31.685Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=3 mint=1638419400000 maxt=1638435600000 ulid=01\r\nDec 02 21:24:34 xxx prometheus[207705]: level=info ts=2021-12-02T13:24:34.936Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNXE06ZW6VHK9X553ER9PNT2\r\nDec 02 21:24:37 xxx prometheus[207705]: level=info ts=2021-12-02T13:24:37.288Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNXFB4HESV44J12CFKKCMYNZ\r\nDec 02 21:24:39 xxx prometheus[207705]: level=info ts=2021-12-02T13:24:39.200Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FNXGPJ4P439D1X8M1R5PWA94\r\nDec 03 01:04:15 xxx prometheus[207705]: level=info ts=2021-12-02T17:04:15.288Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=2 mint=1625961600000 maxt=1627128000000 ulid=01\r\nDec 03 01:04:36 xxx prometheus[207705]: level=info ts=2021-12-02T17:04:36.548Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FBCTHE486E6DQ7V4RKGJCRYB\r\nDec 03 01:04:41 xxx prometheus[207705]: level=info ts=2021-12-02T17:04:41.985Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FAVE1SW8X5WWKRVZQ8SDHA0K\r\nDec 03 05:02:24 xxx prometheus[207705]: level=info ts=2021-12-02T21:02:24.232Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=2 mint=1627128000000 maxt=1628294400000 ulid=01\r\nDec 03 05:02:40 xxx prometheus[207705]: level=info ts=2021-12-02T21:02:40.943Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FBY73A55XQWZ8XCK1A9S14R6\r\nDec 03 05:02:44 xxx prometheus[207705]: level=info ts=2021-12-02T21:02:44.895Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FCFKFJWJK33820X9JAQNCQCS\r\nDec 03 09:25:08 xxx prometheus[207705]: level=info ts=2021-12-03T01:25:08.303Z caller=compact.go:454 component=tsdb msg=\"compact blocks\" count=2 mint=1631210400000 maxt=1632376800000 ulid=01\r\nDec 03 09:25:33 xxx prometheus[207705]: level=info ts=2021-12-03T01:25:33.824Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FG99HSTGN3SYDY3NVAA940FM\r\nDec 03 09:25:37 xxx prometheus[207705]: level=info ts=2021-12-03T01:25:37.694Z caller=db.go:1239 component=tsdb msg=\"Deleting obsolete block\" block=01FFQXF3XG3RDRPMGN8JWVD74S\r\n```\r\n","comments":["```\r\nSaved profile in \/root\/pprof\/pprof.prometheus.alloc_objects.alloc_space.inuse_objects.inuse_space.005.pb.gz\r\nFile: prometheus\r\nType: inuse_space\r\nTime: Dec 3, 2021 at 1:37pm (CST)\r\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\r\n(pprof) top\r\nShowing nodes accounting for 72.23GB, 78.38% of 92.15GB total\r\nDropped 468 nodes (cum <= 0.46GB)\r\nShowing top 10 nodes out of 71\r\n      flat  flat%   sum%        cum   cum%\r\n   26.65GB 28.92% 28.92%    26.66GB 28.93%  github.com\/prometheus\/prometheus\/tsdb.(*memSeries).mmapCurrentHeadChunk\r\n    7.39GB  8.02% 36.94%    14.52GB 15.75%  github.com\/prometheus\/prometheus\/tsdb\/record.(*Decoder).Series\r\n    7.27GB  7.89% 44.83%     7.27GB  7.89%  github.com\/prometheus\/prometheus\/tsdb\/encoding.(*Decbuf).UvarintStr (inline)\r\n    7.07GB  7.68% 52.51%     7.07GB  7.68%  github.com\/prometheus\/prometheus\/pkg\/labels.(*Builder).Labels\r\n    5.61GB  6.09% 58.60%     5.64GB  6.12%  github.com\/prometheus\/prometheus\/pkg\/textparse.(*PromParser).Metric\r\n    4.75GB  5.15% 63.75%     4.75GB  5.15%  github.com\/prometheus\/prometheus\/scrape.newScrapePool.func1\r\n    4.08GB  4.42% 68.17%     5.08GB  5.51%  github.com\/prometheus\/prometheus\/tsdb.newMemSeries\r\n    3.61GB  3.91% 72.09%     3.61GB  3.91%  github.com\/prometheus\/prometheus\/tsdb.(*txRing).add\r\n    2.92GB  3.16% 75.25%     2.92GB  3.16%  github.com\/prometheus\/prometheus\/tsdb\/index.(*MemPostings).addFor\r\n    2.88GB  3.12% 78.38%     2.88GB  3.12%  github.com\/prometheus\/prometheus\/tsdb\/chunkenc.NewXORChunk\r\n\r\n```","![profile004](https:\/\/user-images.githubusercontent.com\/12728608\/144551570-22a3b7d9-9de1-4a35-b475-7e9b03d83dc3.png)\r\n","@wanrui At first sight this shouldn't happen, but not sure of the cause. Maybe you could try to upgrade prometheus version, but that most likely wouldn't change anything since there isn't any relevant changes in the change logs.  Or you could try again but pass in the `skip_head` flag ([docs](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#snapshot)). Also the `fs_type=XFS_SUPER_MAGIC` is posix compliant and supported so that shouldn't be an issue.  \r\n\r\n@codesome Do you have any insight into what could be the cause of this?\r\n\r\n","\r\n@JessicaGreben After two snapshot operations, even if I end the snapshot process, it seems that Prometheus is still executing the snapshot action after restarting, because I see that the snapshot content is still generating. \r\n\r\nOn December 2, 2021, I took two snapshots. Due to inexperience, I did not use the nohup method for the first time, and the execution was still not completed after 5 hours. Therefore, I ended the request, and then used the nohup method to execute the request. The execution was still not completed after 12 hours the next day. I found that the memory occupation of Prometheus was getting higher and higher, So I killed the requesting process, then restarted the Prometheus process, and found that the memory was still rising and the snapshot was still being generated.\r\n\r\nI also think it has little to do with the version of the software.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/12728608\/144693929-0fb8b088-fc09-4b0a-b75c-2f371a7bea28.png)\r\n\r\n\r\n","That looks weird. And I am guessing you have the head block included in your snapshot looking at the profile. I wonder if Prometheus just does not shut itself down when snapshot is in process hence it is on-going in the background.\r\n\r\nUnfortunately I don't have enough bandwidth at the moment to investigate this at the moment, can get to it later (read few weeks)."],"labels":["kind\/bug","priority\/P3","component\/tsdb"]},{"title":"Record rules: Part of the data is lost after changed the `expr` of the record","body":"I modified the'expr' field of the record rule and found that part of the data was missing, as if it had not been calculated.\r\ntried to restart Promethues, but it didn't solve\r\n\r\n1. record rules\r\n```\r\ngroups:\r\n  - name: e2e_bandwidth\r\n    interval: 1m\r\n    rules:\r\n    - record: e2e_tx_bandwidth\r\n      expr: rate(collectd_destination_transmit_bytes_total{destination=~\"0|1|2|3|4|5\"}[2m]) * 8\r\n    - record: e2e_rx_bandwidth\r\n      expr: rate(collectd_destination_receive_bytes_total{destination=~\"0|1|2|3|4|5\"}[2m]) * 8\r\n\r\n```\r\n2. After changed expr,  the data of deviceId=\"GHB31001\" is gone\r\n![\u622a\u5c4f2021-11-29 16 57 25](https:\/\/user-images.githubusercontent.com\/89964135\/143838275-01bef037-68a0-4cfb-b08d-41db1a49f570.jpg)\r\n\r\n\r\n3. I re-added a record with the same `expr`, they only  different `record` name, the new rule is completely normal\r\nexpr config\r\n![image](https:\/\/user-images.githubusercontent.com\/89964135\/143844490-8fd277f7-94ea-40de-99f4-5e4278701b53.png)\r\n\r\nexpr graph\r\n![image](https:\/\/user-images.githubusercontent.com\/89964135\/143844694-35335020-636b-47af-b0e6-617f02efe57b.png)\r\n\r\n\r\n* System information:\r\n```\r\nLinux 4.15.0-48-generic x86_64\r\n```\r\n\r\n* Prometheus version:\r\n```\r\nprometheus, version 2.28.1 (branch: HEAD, revision: b0944590a1c9a6b35dc5a696869f75f422b107a1)\r\nbuild user:       root@2915dd495090\r\nbuild date:       20210701-15:20:10\r\ngo version:       go1.16.5\r\nplatform:         linux\/amd64\r\n```\r\n\r\n**May I ask why such a situation occurs and how to solve it\uff1f**  \r\n\r\n","comments":["We have not tried to reproduce this, but it sounds like a bug.","> We have not tried to reproduce this, but it sounds like a bug.\r\n\r\n@JessicaGreben  Is there any way to temporarily fix this problem?","Does collectd expose metrics with timestamps?\r\n\r\nCould you try:\r\n\r\n```\r\ngroups:\r\n  - name: e2e_bandwidth\r\n    interval: 1m\r\n    rules:\r\n    - record: e2e_tx_bandwidth\r\n      expr: rate(collectd_destination_transmit_bytes_total{destination=~\"0|1|2|3|4|5\"}[2m] offset 1m) * 8\r\n    - record: e2e_rx_bandwidth\r\n      expr: rate(collectd_destination_receive_bytes_total{destination=~\"0|1|2|3|4|5\"}[2m] offset 1m) * 8\r\n```\r\n\r\nof change 2m by 5m?"],"labels":["kind\/bug","component\/rules","priority\/P3"]},{"title":"Potential Race condition when compacting head and receiving samples at the same time","body":"We run cortex and we are seeing some blocks with samples out of order or duplicated samples. Looking into the issue seems that tsdb is accepting those samples due some race condition.\r\n\r\nhttps:\/\/github.com\/cortexproject\/cortex\/issues\/4573 has a detailed explanation what we see on our side but basically seems we have a race condition when receiving samples for a series that is being GC'ed after head compaction.\r\n\r\nEx Timeline:\r\n* SeriesA had samples from 22:00 to 23:50\r\n* At 1:30 u  compact a new block (with data up to 00:00)\r\n* We receive a new sample (t1, v1) for SeriesA and the push go routineA is on [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head_append.go#L243) line\r\n* At the same point the SeriesA is gced [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/db.go#L969) and [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head.go#L1377)\r\n*  routineA resume and (t1, v1)  is pushed to the store (even though the series was gc'ed already)\r\n* We receive another sample  (t1, v2), we create the SeriesA in memory and allow this push. \r\n\r\n**What did you do?**\r\n\r\nRun Cortex and send very sparse data with duplicated samples. It will eventually happen.\r\n\r\nRepro case (Unit test): https:\/\/github.com\/alanprot\/prometheus\/commit\/e30d8710b1e7ad2fbc6b2d3e0d59f76a58ba6807\r\n\r\nMaybe a potential fix could be something like: https:\/\/github.com\/alanprot\/prometheus\/commit\/fbc420637e52c83cd074bf5d77c4b643f681bf83\r\n\r\n**What did you expect to see?**\r\n\r\nDuplicates samples should not be ingested and shipped to the block.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nDuplicated samples are ingested.\r\n\r\n**Environment**\r\nCortex but it seems that the problem can happen in prometheus as well.","comments":["cc @codesome ","@alanprot was there any forced compaction called on TSDB? (like \/shutdown or \/flush calls on the Cortex ingesters?) This race sample should have resulted in \"out of bounds\" error ideally (i.e. overlaps the compaction).\r\n\r\nBut I think some other concurrent append can move the out of bound time while allowing this race sample to be appended. The right fix for this would be to re-check the out of bound property (i.e. not overlapping with compaction) for the GCed series for the head block's latest maxTime in Commit() instead of discarding the sample altogether.","Hi, thanks for looking into this.\r\n\r\nWe may have restarted the ingesters but we never called the force compaction ( \/shutdown or \/flush) but we may have trigged due idle timeout - even though on the cases I saw was just a normal compaction.\r\n","Hi @codesome I think i see what you mean but this still can happen right?\r\n\r\nFrom the timeline:\r\n\r\n1. SeriesA had samples from 22:00 to 23:50\r\n1. At 1:30 u  compact a new block (with data up to 00:00)\r\n1. We receive a new sample (t1, v1) for SeriesA and the push go routineA is on [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head_append.go#L243) line \r\n1. At the same point the SeriesA is gced [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/db.go#L969) and [here](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head.go#L1377)\r\n1.  routineA resume and (t1, v1)  is pushed to the store (even though the series was gc'ed already)\r\n1. We receive another sample  (t1, v2), we create the SeriesA in memory and allow this push. \r\n\r\nSo i think what you are saying is that step 3 should throw an \"out of bounds\" error, but this condition is checked at line [L237](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head_append.go#L237), so if this go routine has already passed that line, and we trigger a compactHead (truncateMemory + gc), the truncateMemory function will indeed update the `minValidTime ` field but is already too late.","> The right fix for this would be to re-check the out of bound property (i.e. not overlapping with compaction) for the GCed series for the head block's latest maxTime in Commit() instead of discarding the sample altogether.\r\n\r\nSo any samples that are appended but not committed should be discarded if `truncateMemory` is called- as `minValidTime ` will be updated? This seems a more impactful change as on the proposed solution we are only discarding samples if `truncateMemory` is called AND the series got gc'ed.","Let's replace 1:30 with 1:00, which is the actual thing that will happen.\r\n\r\nSo I am assuming `t1` is before 00:00, hence the issue happens.\r\n\r\nIn that case yes, we need to consider such samples as out of bound immediately after `truncateMemory`, because at that point a block has been cut, and if we allow this sample to go in, it can overlap with a block on disk.\r\n\r\nWe already do something like that for out of order samples. So it might be as simple as checking for out of bound in [this loop](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head_append.go#L448-L464).\r\n\r\nWould you like to open a PR? It should be a simple one, checking for out of bound only requires [this](https:\/\/github.com\/prometheus\/prometheus\/blob\/e63b85be4fd7a6425dc697adc8244065028dfae2\/tsdb\/head_append.go#L237).","> Let's replace 1:30 with 1:00, which is the actual thing that will happen.\r\nYes... correct. \r\n\r\nI would love to create a PR but i still a little confused. Seems that when we call `truncateMemory` we don't update the `headAppender#minValidTime` right? So this would not have any effect.\r\n\r\nSeems that in this case we would need to check the `headAppender#head.minValidTime`?\r\n\r\nDoing this change I still get the error:\r\n\r\n```\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \t\r\n        \t            \tmetric output does not match expectation; want:\r\n        \t            \t\r\n        \t            \t# HELP prometheus_tsdb_head_samples_appended_total Total number of appended samples.\r\n        \t            \t# TYPE prometheus_tsdb_head_samples_appended_total counter\r\n        \t            \tprometheus_tsdb_head_samples_appended_total 2508\r\n        \t            \t\r\n        \t            \tgot:\r\n        \t            \t\r\n        \t            \t# HELP prometheus_tsdb_head_samples_appended_total Total number of appended samples.\r\n        \t            \t# TYPE prometheus_tsdb_head_samples_appended_total counter\r\n        \t            \tprometheus_tsdb_head_samples_appended_total 2509\r\n```","> I would love to create a PR but i still a little confused. Seems that when we call truncateMemory we don't update the headAppender#minValidTime right? So this would not have any effect.\r\n\r\nOh right, we need to check from the Head as well.\r\n\r\n> Doing this change I still get the error:\r\n\r\nNot sure I understand what change is it","> Not sure I understand what change is it\r\n\r\nI did the change just looking at the `headAppender#minValidTime` and i still got the error in the test. Sorry for not being clear! :D\r\n\r\nSo you are recommending that we should check the head as well there?\r\n\r\nI just have one concern about that. the head `minValidTime` is an `atomic.Int64` so checking this on the commit maybe can have a perf impact?\r\n\r\nWhy you don't like the Abort flag? \r\n\r\n```\r\nIf you create an headAppender and append samples but the series is destroyed before the commit (gc'ed), your headAppender should not be valid anymore.\r\n```\r\n\r\nThis is exactly what this commit is doing https:\/\/github.com\/alanprot\/prometheus\/commit\/fbc420637e52c83cd074bf5d77c4b643f681bf83 ","> Why you don't like the Abort flag?\r\n\r\nIt will discard the sample even if it was a valid sample and did not overlap with the compaction. We should not be discarding any sample without a reason. In this case, we should discard the sample only if we know if it went out of bounds, else we are falsely counting any sample that comes under these conditions as an out of order sample.","> I just have one concern about that. the head minValidTime is an atomic.Int64 so checking this on the commit maybe can have a perf impact?\r\n\r\nThat is true. Maybe we want a version of your abort flag idea - have the flag, but only discard the sample if it goes out of bound. Hence we call the minValidTime() function only if the flag was set to true.","Seems that this fix also does not fix the race condition: :(\r\n\r\n```\r\n\t\tabort := series.abortCommit && s.T < a.head.minValidTime.Load()\r\n\t\tok, chunkCreated := false, false\r\n\t\tif !abort {\r\n\t\t\tok, chunkCreated = series.append(s.T, s.V, a.appendID, a.head.chunkDiskMapper)\r\n\t\t}\r\n```\r\n\r\nAnd the reason for that is that the series are removed from the map on the GC method https:\/\/github.com\/prometheus\/prometheus\/blob\/cd739214ddfe5ce5efd57e7a4c65d727d0b1227c\/tsdb\/head.go#L1409 and the minTime is only updated after in the `truncateMemory` method with no locking https:\/\/github.com\/prometheus\/prometheus\/blob\/cd739214ddfe5ce5efd57e7a4c65d727d0b1227c\/tsdb\/head.go#L842\r\n\r\nWhen i have some time i will run the fix with just abort for a long time in our test env and see if fiz the issue and everything works fine.","Hi @codesome \r\n\r\nhttps:\/\/github.com\/alanprot\/prometheus\/commit\/4b8e99d1e89f3f51f4669bf8e3c3ba4f8a213bed\r\n\r\n<strike>This change also fix the problem (and also another datapoint to prove we have a race condition on that code). What do you think?  <\/strike>\r\n\r\n\r\n\r\n\r\nnevermind .. this change also does not work\r\n","Hi @codesome \r\n\r\nI created a PR to try to address this.... I changed the repro case to use a real TSDB to make sure that this problem really exists (and its way easier to read now)\r\n\r\nThanks"],"labels":["kind\/bug","priority\/P3","component\/tsdb"]},{"title":"Allow label_join to be used in aggregation operators","body":"I have looked quite a bit and haven't found a way to do this but it seems simple so please let me know if I've missed something.\r\n\r\n\r\nI have a need to do capacity planning for failover events which rarely happen. So I can't rely on the data to cover these scenarios. However; assuming a label that designates a datacenter for example I can aggregate while maintaining these label values for further manipulation in outer queries which would eliminate the need to do a bunch of manipulation in other places like excel, grafana etc. This is necessary in my situation because my failover scenarios are extremely complex and just calculating totals isn't helpful unless I can do further manipulation with the labels.\r\n\r\n\r\n\r\n\t{instance=\"datacenter0\"}\r\n\t1000\r\n\t{instance=\"datacenter1\"}\r\n\t2000\r\n\t\r\n\tsum by(label_join(\"instance\", \"datacenters\", \",\")) (\r\n\t\tirate(\r\n\t\t\ttIPFilterParamsEgrHitByteCount{\r\n\t\t\t\tinstance=~\"datacenter.*\", \r\n\t\t\t}[10m]\r\n\t\t) \r\n\t)\r\n\t\r\n\t{datacenters=\"datacenter0,datacenter1\"}\r\n\t3000\r\n","comments":["You want a new label which contains all the values of a label that disapeared under aggregation?\r\n\r\nInteresting idea, though I don't think `label_join` is the right name for that.","Correct, similar to GROUP_CONCAT() in MySQL. It would be helpful to maintain those for presentation layer as well as further manipulation in a regex or things like that.\r\n\r\nSure, I was just using that as an example, perhaps group_concat might work as a function name?","It wouldn't be a function, in the sense that functions work currently.\r\nIt could be added as an optional part of `sum`, maybe `sum by(instance) concat_as(datacenters) (...)`.\r\nMaybe some cryptic code like `%` as was done for evaluation timestamp.\r\n\r\nAnyway, my intention was not to nitpick the name but to check if I understood the suggestion.","@SuperQ \/ @juliusv  We looked at this issue during the bug scrub meeting and we don't think this should be part of the language, but we want to get more input from you. ","It's an interesting feature request which I never saw requested before, but which doesn't seem too unreasonable either. Such a modifier to aggregators would easily fit into the language model (you'd probably want to be able to specify the concatenation separator as an optional parameter, similar to relabeling rules), but personally I'd want to see more interest from other people before considering adding it to the language.\r\n\r\nAnother question would be how to syntactically indicate which aggregated label names get concatenated into which output labels, since a simple `sum(foo)` could aggregate over arbitrarily many label dimensions, and you may want to preserve some of them as concatenated lists and others not.","I have a similar use case and would also like to have a feature like this. In my case, I have several instances of an application, and I monitor based on the up{application=\"foo\"} metric. Now I would like to create an alert query that indicates if at least two instances are down (up == 0), and ideally the resulting time series should contain the list of instances which are down.\r\n\r\nThe best I can currently do is the following, but that results in two separate time series:\r\n\r\n```\r\n(sum(clamp_min(up{application=\"foo\"} == 0, 1)) >= 2) + ignoring(instance) group_right() (sum by (instance) (up{application=\"foo\"} == 0))\r\n\r\n-> {instance=\"instance1\"}, {instance=\"instance2\"}\r\n```\r\n\r\nBut with the ability to concatenate labels, I would be able to get one metric which contains both instance names, e.g. like this:\r\n\r\n```\r\n(sum(clamp_min(up{application=\"foo\"} == 0, 1)) >= 2) + ignoring(instances) group_right() (sum by (instance) concat_as(instances) (up{application=\"foo\"} == 0))\r\n\r\n-> {instances=\"instance1,instance2\"}\r\n```","I'm also looking forward to this feature. I think it applies to all scenarios where one wants an alert on an aggregation but wants the alert to carry the list of offenders (through the label).","This feature would be very helpful.","Interesting, ok! By now a few more people have voiced interest in this feature here, and I think it's at least worth considering if we get a design proposal that takes into account the following points:\r\n\r\n* How to specify which labels you want to preserve as a concatentation.\r\n* How to set the separator character (defaulting to ',' or something when omitted).\r\n* How to specify whether you want the concatenated values to be distinct rather than repeated (some underlying label values in an aggregation can occur multiple times). The analogue to this in MySQL would be `GROUP_CONCAT(DISTINCT ...)`.\r\n* Optional(?): How to specify the sorting of the concatenated labels. Maybe just always keeping this lexicographic is fine though.\r\n\r\nAm I missing anything?\r\n\r\nIf we want to add these extra modifiers for separators and value distinctness at least, then coming up with a syntax that isn't too complex and still PromQL-ish is going to require a bit of creativity. Ideas welcome!"],"labels":["priority\/Pmaybe","component\/promql","kind\/feature"]},{"title":"Prometheus Agent: consider never moving samples to checkpoint","body":"Prometheus Agent will currently move samples to checkpoints if, during GC, the timestamp of those samples are behind the lowest sent timestamp across all remote_write endpoints. The intent is that samples are kept best-effort: i.e., try to not delete them before they're sent.\r\n\r\nHowever, remote_write will _never_ read samples from checkpoints. We should consider always dropping all samples when creating a new checkpoint for the Prometheus Agent WAL. Series records for live time series should still be kept in the checkpoint.\r\n\r\nAs an aside: @gouthamve pointed out that remote_write doesn't currently any existing samples at all from an existing WAL, which makes having a WAL seem a little pointless. I know that I've heard talk from @csmarchbanks and @cstyan of using some kind of marker so remote_write can re-start from a marker, but I don't know what the status of that is. Until we have that, I do agree that the Prometheus Agent's (and grafana\/agent's) WAL isn't really providing any benefits to the user. ","comments":["I wouldn't say its not providing any benefits. It is, when the remote is down. \r\n\r\nHaving said that, it doesn't help when the agent restarts. We basically restart onto a clean slate, and can save a lot of time during restarts by just deleting the WAL.","> I wouldn't say its not providing any benefits. It is, when the remote is down.\r\n\r\nIt seems like the most you'd get would be using less memory when remote_write is backed up by using the disk as a buffer. Am I missing anything else?","Yes because the agent should ideally work with a multiple hours outage.","I think even with the remote write checkpoint\/marker we don't need samples in the WAL checkpoint for the agent.","I'm pretty confident that's true. We've also noticed that the WAL replay for loading in series can take _forever,_ partially because of how many samples might exist in checkpoints. I think there's a few changes we can do here:\r\n\r\n1. Don't store samples in checkpoints, since they're never read by remote_write \r\n2. During replay, initialize in-memory series with a lastTs of `math.MinInt64` to indicate that it's new. This is opposed to the current initialization of 0 with lastTs being eventually set to the most recent TS in the WAL for that series. \r\n3. Given 2, don't decode samples while replaying, which will massively improve WAL replay speed \r\n\r\n(Credit to @cstyan for most of the above)"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/agent"]},{"title":"Alert PrometheusNotConnectedToAlertmanagers in alerts.libsonnet does not work","body":"File documentation\/prometheus-mixin\/alerts.libsonnet contains alert PrometheusNotConnectedToAlertmanagers, which uses prometheus_notifications_alertmanagers_discovered.\r\n\r\nThat alert is not working for me. More information is here:\r\n\r\nHow to detect lost connection to Alertmanager\r\nSaturday the 20th of November 2021\r\nhttps:\/\/groups.google.com\/g\/prometheus-users\/c\/vo5PRmu-AA8\r\n\r\nFrom that discussion:\r\nAlternative 1) Scrape the Alertmanager and alert on based on up{job=\"alertmanager\"}.\r\nAlternative 2) Use an alert based on prometheus_notifications_errors_total.","comments":["There is already [`PrometheusErrorSendingAlertsToSomeAlertmanagers`](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/documentation\/prometheus-mixin\/alerts.libsonnet#L43-L62) alert which is based on `prometheus_notifications_errors_total` and `prometheus_notifications_sent_total`. This one should kick in when prometheus cannot send alerts to alertmanagers.\r\n\r\nHaving [`TargetDown`](https:\/\/github.com\/prometheus-operator\/kube-prometheus\/blob\/main\/jsonnet\/kube-prometheus\/components\/mixin\/alerts\/general.libsonnet#L7-L18) alert based on `up` metric is a good idea for a catch-all alert if anything else fails. But I don't think this should be shipped with prometheus mixin though.\r\n\r\n---\r\nAs for `PrometheusNotConnectedToAlertmanagers` it is best used in conjunction with `PrometheusErrorSendingAlertsToSomeAlertmanagers` as they treat about different things and together give you the whole picture. The former treats about discovery of alertmanagers, whereas the latter is about actual alert sending.","Regarding to PrometheusNotConnectedToAlertmanagers, what do you mean by discovery?\r\n\r\nWhen I stop the prometheus-alertmanager service before starting the prometheus service, the Alertmanager is still \"discovered\" by Prometheus. If the Alertmanager was running, and I stop it, the Alertmanager remains \"discovered\" by Prometheus. What does that metric actually measure?","According to metric description, it measures \"The number of alertmanagers discovered and active.\" - https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/notifier\/notifier.go#L191-L194. However, looking closer it seems to be only looking at Service Disovery aspect and it always discovers alertmanagers that are statically defined. This could be considered a bug by some, but at the same time if we assume this metric is only about service discovery, then a statically defined endpoint is considered a discovered endpoint regardless of the state of the application behind that endpoint. This in turn can lead to the results which you described.\r\n\r\nRegardless, I think that either metric description should be changed (metric does not reflect if discovered alertmanager is active) or the logic handling that metric value should be changed to reflect if the discovered alertmanager is active.\r\n\r\n"],"labels":["kind\/enhancement","priority\/P3","component\/mixin"]},{"title":"promql function to sum distinct for gauge value","body":"## Background\r\n\r\nI have some issue with trying to build promql query from metrics that are being scraped from telegraf. For example\r\n```\r\nquery: my_metrics{app=\"telegraf\"}[5m]\r\n\r\nresult: {app=\"telegraf\"} 336 @1637319417.799\r\n                         336 @1637319427.803\r\n                         336 @1637319437.799\r\n                         510 @1637319497.799\r\n                         317 @1637319507.799\r\n                         317 @1637319517.799\r\n                         317 @1637319527.799\r\n                         317 @1637319537.799\r\n                         317 @1637319547.799\r\n                         317 @1637319557.799\r\n```\r\n\r\nSo I'm trying to get the sum_over_time from that gauge, but since Prometheus keeps pulling the same value until telegraf flushes the gauge from memory.\r\nIt kinda messed up the query result, from that 5m interval, I'm expecting to get 336+510+317 instead of all.\r\n\r\n## Proposal\r\n\r\nWould be nice to have a function to distinct gauge value over time, so we can get the right amount of sum, or maybe I'm missing a way to get the right value.\r\n","comments":["Hi! From what I understand, you are trying to track values from individual events, to then sum those up. Indeed with the scraping \/ metrics model of Prometheus, that is not possible, as the scrape is decoupled from the event and Prometheus fundamentally tracks metrics and not individual events. So the way you'd usually approach this is to expose a counter metric instead, which would already be the cumulative total over all tracked events.","Another big issue with this approach is that it would rely on every measurement to contain a different value. This would be a confusing function for most PromQL users, and I fear it could even not fulfil your usecase because of this requirement.","Yes, forgot to mention that explicitly!","@juliusv yes you got it right, in my case, I don't own the service, I'm just trying to translate influxql to promql, in a hope that I don't need on influxdb anymore \ud83e\udd1e\r\n\r\n@roidelapluie I kinda share the same feeling, initially, I was trying to create a different function like `distinct`, which should return distinct value in a vector, I think that one shares a more common understanding among users. But this is my first time touching promql folder, and  I was a bit stuck trying to do that func earlier, and ended up having this proposal. Do you think distinct is more acceptable?\r\n","Which prometheus integration are you using with teleraph? Maybe switching to remote write could help: https:\/\/github.com\/influxdata\/telegraf\/blob\/master\/plugins\/serializers\/prometheusremotewrite\/README.md (you need --enable-feature=remote-write-receiver and push to https:\/\/prometheus:9090\/api\/v1\/write)","our setup is like bunch of telegraf daemonset that expose http output with prometheus format and the metrics flushes every 2mins or something like that\n\n\n\nso the idea of using the write one, so then prometheus don't need to scrape telegraf endpoint? I think that something worth to try  ","ok I changes the architecture to use telegraf push to prometheus api\/v1\/write, rather than prometheus scraping telegraf and it looks promising, thank you @roidelapluie \ud83d\ude4f \r\n\r\nin this case I don't think I need distinct function since metrics stored exactly one time. So I guess unless there's other needs for this function, this issue can be closed I guess?\r\n\r\nI'm not an expert on promql but having such a function might be beneficial for some cases","if you folks think a distinct function is worthy to add, I can try to find some time wrapping up the function @juliusv @roidelapluie otherwise I guess we can pause or drop it :) ","> in this case I don't think I need distinct function since metrics stored exactly one time.\r\n\r\nNote that even with push, you are still treating time series samples as individual *events* with a special identity, whereas the whole Prometheus computational assumes a relatively arbitrary sampling of *state* (meaning the current value of something that is ongoing and that could be sampled as frequently as you want). It can always happen that either some pushes \/ scrapes fail (in which case some events would be lost), or that time series are downsampled at some later point in time, and the query language is also not really geared around an event-based use case. So I still don't think it fits well into the Prometheus model...","@alileza \r\n\r\n> I'm expecting to get 336+510+317 instead of all\r\n\r\nJust to +1 and slightly rephrase Julius, above: Prometheus may scrape its targets on discrete intervals, but metrics are actually continuous, so the concept of unique values of a gauge isn't really sensical. When you \"push\" a value of 336, you're not sending a single event, you're setting the gauge to a value, and it will retain that value indefinitely."],"labels":["priority\/Pmaybe","component\/promql","kind\/feature"]},{"title":"Support reload config file automatically","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\nSupport reload config file automatically without calling the reload API, when the file is changed.\r\n","comments":["Thanks for your proposal!\r\n\r\nWe decided that only allowing SIGHUP (and by extension the reload API) would be the best when reloading was first implemented. See the comment thread: https:\/\/github.com\/prometheus\/prometheus\/issues\/108#issuecomment-104318203.","Hello,\r\n\r\nActually while the consensus 6 years ago was not to have this, since then many people implemented their own reloaders.\r\n\r\nI think we should revisit this decision.\r\n\r\nThe new reloader would:\r\n\r\n- be enabled behind a feature flag to start with\r\n- use inotify where possible, manual reload otherwise (and look at checksum to see if there are differences)\r\n- throttle config reloads (e.g. not more than one\/minute)\r\n\r\nLet's see if we can get consensus on this: @beorn7 @brancz @discordianfish @superq","Rather than a feature flag, a simple normal boolean flag would be sufficient. Not every new feature we add needs a feature flag.\r\n\r\n```\r\n--config.auto-reload\r\n```","> Rather than a feature flag, a simple normal boolean flag would be sufficient. Not every new feature we add needs a feature flag.\r\n> \r\n> ```\r\n> --config.auto-reload\r\n> ```\r\n\r\nack, should it take the minimum interval ?","Agree that normal flag should be sufficient, but I think we'll need two flags\/options: Poll interval if not using inotify and max reload per minute. So I'd keep it a boolean flag and have a separate one for the other options.","I don't remember the arguments against auto-reloading from back then (and confusingly, I said myself I would be able to make a point against it in that [mysterious comment](https:\/\/github.com\/prometheus\/prometheus\/issues\/108#issuecomment-104318203), but I really cannot remember).\r\n\r\nIn general, as an optional feature, I don't have objections right now, but knowing how the discussion back then went would be better.","https:\/\/github.com\/prometheus\/prometheus\/issues\/108#issuecomment-92288938 and https:\/\/github.com\/prometheus\/prometheus\/issues\/108#issuecomment-104318819 would be the arguments, so it ended up done for everything except the config+rules files.\r\n\r\nSomething might update the rules\/configs in any order, or briefly have partial files on disk.","So the main argument would be that the config to reload is inherently multiple files (because rules are in a separate file, and then we even plan to support .d style config diretories).\r\n\r\nAvoiding partial files is fairly easy with a single file (via renaming).\r\n\r\nThe question is if the increased difficulty to update the set of files is prohibitive for providing auto-reload as an optional feature. (I guess we all agree that it should not become the default behavior, not even in 3.x.)","https:\/\/github.com\/gmiroshnykov\/prometheus-reloader\r\nhttps:\/\/github.com\/GoogleCloudPlatform\/prometheus-engine\/tree\/main\/cmd\/config-reloader\r\nhttps:\/\/github.com\/prometheus-operator\/prometheus-operator\/tree\/main\/cmd\/prometheus-config-reloader\r\nhttps:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go\r\n\r\n\r\nThat problem has been solved at least 4 times by the community. I suggest we look at those implementations, maybe write down how they work in a small design doc, and adopt the best behaviour.","> https:\/\/github.com\/gmiroshnykov\/prometheus-reloader https:\/\/github.com\/GoogleCloudPlatform\/prometheus-engine\/tree\/main\/cmd\/config-reloader https:\/\/github.com\/prometheus-operator\/prometheus-operator\/tree\/main\/cmd\/prometheus-config-reloader https:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go\r\n> \r\n> That problem has been solved at least 4 times by the community. I suggest we look at those implementations, maybe write down how they work in a small design doc, and adopt the best behaviour.\r\n\r\n+1","@roidelapluie just FYI the prometheus-operator is wrapping https:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go under the hood","..and I'm using https:\/\/github.com\/jimmidyson\/configmap-reload :)","At GL, we are using https:\/\/github.com\/weaveworks\/Watch , which is a fork of https:\/\/github.com\/eaburns\/Watch, and which isn't even doing anything fancy. It still seems to work just fine (but perhaps that's because configmap updates are (almost) atomic?).","\ud83d\udc4b\ud83c\udffd  It would be indeed epic to revisit this. I would love to propose (list done with the help from @roidelapluie):\r\n\r\n- [ ] `--config.watch-interval` (default 0) duration flag which, when non zero, watches changes of Prometheus config file, previously provided rule files, as well as previously provided scrape_config_files. When change is noticed, Prometheus will self-reload.\r\n- [ ] Generally we propose to add core watching code similar to already mentioned [thanos reloader](https:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go#L540), but without compression or environment substitution features. It could be slightly generic e.g. in `util\/fswatch` and used later for [file SD](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/file\/file.go#L242) if we want. It would:\r\n  * Watch given files using inotify (inotify will be on dirs), with regular interval to catch inotify flaky cases\r\n  * Check if specified file really changed using checksums\r\n  * It has be able to remove files from watching, not only adding (new feature of reloader)\r\n  * It will notify the callers through Go channel when any of the given file changed.\r\n  * It will have throttling behaviour. Perhaps it deserves additional `--config.watch-delay-duration` duration flag. We need that to only provide one notification after bigger update, instead of multiple updates within seconds for multiple files changed at once. One simple solution is [delay functionality](https:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go#L123) reload offers.\r\n- [ ] Such watcher will be instantiated in the main Prometheus loop (after reload is ready). Change notifications will trigger reload like SIGHUP\/reload API would (aka \"self-reload\")\r\n\r\nSince the feature will be optional, we don't envision polluting feature flags for this, but we would still mark it as experimental. If we prefer feature flag, let me know.\r\n\r\nWe (Prometheus Managed Team at Google) are happy to contribute\/lead this.\r\n\r\n## Motivation\r\n\r\nNo extra sidecar will be required for common file watching logic,  especially popular with Kubernetes and ConfigMaps. This is especially important for managed setups (like ours) where we have strict alerting and SLIs on each component. The problem with config reloader sidecar is that it's has strong dependency on Prometheus being up, thus the alerts and SLIs has to be even move complex (to avoid noise), which is overkill for such a simple (yet important) functionality\r\n\r\n## Background\r\n\r\nDid some research, but most of it was mentioned here already:\r\n\r\nBack in 2015 https:\/\/github.com\/prometheus\/prometheus\/issues\/108, we agreed\r\nthat SIGHUP only reloading is solid enough. The main argument against file\r\nwatching was that it's not implemented by all file systems. Further more, not\r\nall filesystem file changes are atomic. Finally, it was noticed that inotify can\r\nbe flaky, even on modern Linux OSes.\r\n\r\nHowever, despite the above decision Prometheus started to use file watchers for [file SD](https:\/\/github.com\/prometheus\/prometheus\/blob\/02277bbe0dd2d89fd3fe5f508399d54da9c61827\/discovery\/file\/file.go#L242) (with some safety intervals)\r\n\r\nFurthermore, with those flaws acknowledged, some advanced setups of Prometheus uses watch mechanisms\r\nwith some safety re-read interval (and extra features beyond this PR):\r\n\r\n* Thanos Sidecar uses [reload](https:\/\/github.com\/thanos-io\/thanos\/blob\/main\/pkg\/reloader\/reloader.go) package.\r\n* Prometheus Operator adds a [config-reloader sidecar](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/pkg\/operator\/config_reloader.go#L153) build from [here](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/cmd\/prometheus-config-reloader\/main.go) based on Thanos reloader.\r\n* Similarly, our Google operator adds [a config-reloader sidecar](https:\/\/github.com\/GoogleCloudPlatform\/prometheus-engine\/blob\/main\/manifests\/operator.yaml#L562) build from [here](https:\/\/github.com\/GoogleCloudPlatform\/prometheus-engine\/blob\/main\/cmd\/config-reloader\/main.go) and also based on Thanos reloader.\r\n\r\n","Yeah I'm (still) very much in favor of supporting this.","> watches changes of Prometheus config file, previously provided rule files, as well as previously provided scrape_config_files\r\n\r\nNote: This is not enough to replace side-cars. If that is a goal, then you would need to watch all file configurations \"_files\" within all scrape configs. For example, credentials files such as those used for mTLS `cert_file` where a certificate or key would expire or be rotated need to be watched for changes. The filepaths may also change dynamically so Prometheus would have to watch and unwatch files, as needed.","_files like TLS certificates do not require reloads. They are read on every request.","Thanks for the clarification!"],"labels":["component\/config","not-as-easy-as-it-looks","priority\/P3","kind\/feature"]},{"title":"Add timewrap like functionality to offset","body":"I frequently find myself using offsets to compare previous days\/weeks\/months to current to get a better idea of how seasonal data is behaving. \r\n\r\nI have run into a roadblock though in going deeper in that I can't figure out how to work with multiple offsets using aggregating functions to get something more 'intelligent' to use as a comparative baseline or alert threshold etc. For example if I wanted to find out what the average Wednesday looks like. \r\n\r\nSome other parts of PromQL inspired this syntax which is certainly not the important piece of this feature request and no doubt may clash with something but seeing it might help explain what I'm thinking.\r\n\r\n```\r\navg(\r\n    (http_requests_total offset [1d:1d:28d])\r\n)\r\n```\r\nThis would be similar to in splunk querying 28 days of data with a `timewrap span=1d` applied. So excuse my poor choice of words here, I'm sure there are better but the values in the brackets mean this: [ window length : window separation : how far back to look ] \r\n\r\nThis would be seriously helpful in working with seasonal data.\r\n\r\nBecause this would obviously return multiple series, I assume each one could have a label added to indicate which window it belongs to.\r\n\r\nPerhaps there is already some other way to accomplish this in Prometheus, if there is please tell me, I'd love to take advantage. \r\n\r\n","comments":["Yes, this would be useful. I am using very complex queries to get this. As you said, I am also unsure about the syntax.","Your usecase will be possible with `avg_over_time(rate(metric[5m])[28d::1d])`, but requires #9114","Indeed, I think #9114 will enable quite a few things. @LeviHarrison feel encouraged to pick it up again! (o:","I have marked #9114 as fixing this issue.","@roidelapluie just revisiting this out of curiosity, I noticed you mentioned complex queries to solve the same need, I've just been using something like this\r\n\r\n```\r\n#foo average day\r\n(\r\n\t(\r\n\t\tfoo{} offset 1d \r\n\t) + on (bar)\r\n\t(\r\n\t\tfoo{} offset 2d\r\n\t) + on (bar)\r\n\t(\r\n\t\tfoo{} offset 3d\r\n\t)\r\n)\/3\r\n```\r\n\r\nYou wouldn't happen to have anything more flexible even if it's complex? I'm looking to expand this to get 30+ days of data and with a 20 line subquery it's getting ugly not to mention causing problems with the grafana plugin.\r\n","I had no idea what \"timewrap\" meant.  From context I guess it is a Splunk feature.\r\nBased on the PR I think \"Optionally align subqueries to end-time of query_range\" is better, but maybe this issue is more general?"],"labels":["component\/promql","priority\/P3","kind\/feature"]},{"title":"unwrap paren expr with stepinvariant expr","body":"ref - https:\/\/github.com\/prometheus\/prometheus\/pull\/9591#discussion_r746650891\r\n\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["Closing as it's been in draft state for a long time.\r\nIf you come back to it please re-open."],"labels":["stale"]},{"title":"Make the size of all the `len` field in block's index 8 bytes","body":"Persistent block's [index has multiple sections](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/tsdb\/docs\/format\/index.md) (symbol table, labels offsets, label offset table, postings lists, postings offset table). All of these sections have first 4 bytes describing the size (the `len`) of the section, which limits the size of the section to 4GiB.\r\n\r\nWe have already seen issues being reported where users hit the symbol table size limit. Similarly we have seen postings offset table hitting the 4GiB limit silently without error during compaction while giving error when reading the block.\r\n\r\nI propose we make the length field 8 bytes. This will require index version bump.\r\n\r\n@Harkishen-Singh has shown interest to work on making postings 64 bits, which will also require index version bump. So I plan to club this with that to result in a single version bump (but if required, 2 version bumps, if the 64 bit postings work is going to take some time).","comments":[],"labels":["kind\/enhancement","priority\/P3","component\/tsdb"]},{"title":"refactor promql optimizer","body":"Currently, we do basic changes to the input promQL query which can be thought of as optimisations. Consider the [`PreprocessExpr`](https:\/\/github.com\/prometheus\/prometheus\/blob\/cda025b5b50328ccd129c141e09dd77b3b1b034e\/promql\/engine.go#L2462) function which wraps the nodes with `StepInvariantExpr` if applicable. This change allows the engine to get the range query result as an instant query in a single step. \r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/cda025b5b50328ccd129c141e09dd77b3b1b034e\/promql\/engine.go#L1549-L1551\r\n\r\nThis PR refactors this code into a new `OptimizeQuery` function and adds constant folding (for number literals for now). In the future, we can add other optimisations, eg: propagating label selectors across binary ops [#8053](https:\/\/github.com\/prometheus\/prometheus\/issues\/8053)\r\n\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["> This PR refactors this code into a new `OptimizeQuery` function and adds constant folding (for number literals for now). In the future, we can add other optimisations, eg: propagating label selectors across binary ops [#8053](https:\/\/github.com\/prometheus\/prometheus\/issues\/8053)\r\n\r\nIf you read #8053, you will see that it is not a safe optimization in Prometheus 2.x (or should be done behind a feature flag).","Closing as it's been in draft state for a long time.\r\nIf you come back to it please re-open."],"labels":["stale"]},{"title":"Rules API: support label-based matcher. [Feature Request]","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\nIn thanos-sidecar mode, fanout prometheus rules api may response lots of rules. Could we support label based matcher in prometheus rules api, like series api `match[]` parameters . https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/docs\/querying\/api.md#finding-series-by-label-matchers . \ud83d\ude03 \r\nrelevant issue in thanos https:\/\/github.com\/thanos-io\/thanos\/issues\/4812 \r\n","comments":["I think we could get the group name and the file as parameters. It is very difficult to have the match[] because the labels in the rules are templates and not values.","> I think we could get the group name and the file as parameters. It is very difficult to have the match[] because the labels in the rules are templates and not values.\r\n\r\nTemplates will be rendered on active alert,  so how about match rules on origin labels and match active alerts on rendered labels?","Hi, If this proposal reasonabl, I could start implement it \ud83d\ude04  (currently i write a query proxy which based prom-label-proxy to do it)","Hello \ud83d\udc4b, I wanted to move forward with this issue, regarding matcher support in Prometheus Rules API. A major use case for this is multi-tenant setups, where the only way to achieve tenancy seems to be modifying the API response, which is what [prom-label-proxy does](https:\/\/github.com\/prometheus-community\/prom-label-proxy\/blob\/1eac0933d51263ca7856fb8e4528f03039afa44f\/injectproxy\/rules.go#L166). Maybe I can raise a PR and get some feedback on the approach and implementation details? \ud83d\ude42  ","it is fine to filter on active alerts, but filtering on definitions would require an alternative approach (filter by name, group ...)","Why not a simple approach for Rules API to filter on non rendered labels? (Let's keep the Alerts API from the discussion for now).\r\n\r\nCommented on the [PR](https:\/\/github.com\/prometheus\/prometheus\/pull\/10194#pullrequestreview-863455107)"],"labels":["component\/rules","component\/api"]},{"title":"fail early with ErrTooManySamples in rangeEval","body":"account for the samples loaded in memory before `eval` of next expression\r\n\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["Picking this up as part of our bug scrub\u2026 @darshanime are you still up to working on this? What do you think about @codesome's comment? Test coverage is also an issue that needs to be addressed before merging this.","Closing as no update after 6 months.  We noted at the bug-scrub that a lot of code in this area has changed due to native histograms, so it probably has to be re-coded from scratch.","@codesome maybe you can help us out here?","BTW: The sub-queries are much harder to reason with. I cannot really explain the precise values in the tests. (Perhaps @codesome can, but he has still very limited availability.)","In any case, I'll create a follow-up PR where I had histograms to the tests, because reviewing this PR showed me that they are still missing.","Thanks for your comments. I'll look into them once I find time. (But I have to say that I feel heavily underqualified for this review. I guess we need help of somebody who is more familiar with this sample counting, maybe @jesusvazquez or @codesome . I'll take it as a challenge to learn more about it, but it will take me a lot of time, and I'm heavily distracted with other tasks.)"],"labels":["stale"]},{"title":"New operator: fallback","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n\r\nIn a perfect world we know exactly what recording rules we should add to our metrics systems from day 1. The reality is that recording rules get introduced over time as we realize that certain queries are useful and computationally expensive enough to justify a rule. \r\n\r\nAfter introducing a new rule, to maintain continuity in dashboards, it is tempting to reach for the `or` operator.\r\n\r\n```\r\njob:http_inprogress_requests:sum or sum by (job) (http_inprogress_requests)\r\n```\r\n\r\nUnfortunately when you are querying a time range when the recorded metric exists, it is much less efficient than just querying only for the recorded metric. In fact it is less efficient than just doing one or the other query individually for any time range, regardless of whether the recorded metric exists. The reason is that the promQL engine still has to perform the right hand side query and match up label sets to evaluate the 'or'. \r\n\r\nWhat I really want in this case is a `fallback` operator, which only evaluates the right hand side if exactly NO time series exist on the left hand side. Because the presence of any data on the left hand side indicates that the recording rule was present at that evaluation time, there is no need to continue to process the right hand side query, because I'm not interested in matching up the timeseries label sets. The recording rule is all or nothing. It's either present, in which case there is no need for the \"or\" behavior, or its missing, in which case, yes please go ahead and compute the right hand side query, as a _fallback_.","comments":["I'll add that backfilling, while theoretically possible is not an ideal work around for this especially if you are maintaining metrics for a lot of multi-tenant customers where actually performing the backfill would require more considerations such as computation and storage resources. Despite improved support for backfilling rule data, it also requires some engineering to achieve, especially if using other long term storage solutions such as Thanos. The goal of recording rules is typically to make queries more efficient going forward, but backfilling is expensive and not worth it in many cases. But still, continuity of metrics and dashboards is a very nice thing to have. When a recorded metric is not available, if you're taking the time to look, its probably worth computing it for that timeseries. Thus having this query would make it possible to maintain dashboard continuity. ","Thank you for your request. I understand it but it seems really difficult to implement: PromQL indeed is computing all the select at the beginning of the query and that's probably the expensive part. I also think that it is quite confusing for users to understand what it would mean.\r\n\r\nlet's gather @juliusv and @beorn7 opinion on this as well.","Thanks for the response. I could be totally off base, but I'd imagine it implemented something like this:\r\n\r\n- run the left hand side query\r\n- If the first timestamp of the returned data is greater than the START requested timestamp, then run the right hand side query for the remaining time range\r\n\r\n(small edit to fix my logic)","My first gut reaction is also:\r\n\r\n* While I get the use case, it does seem like a fairly niche feature request (at least I can't remember anyone requesting something similar before)\r\n* The required changes to PromQL under the hood would likely be pretty complex in relation to what it gives us, but @codesome or others who have written more engine code recently would understand that part better.\r\n* It's adding to the user-visible surface of PromQL, which is another cost in terms of learning and confusion for users if most people will not use it (correctly).","Various thoughts here:\r\n\r\nI also believe that what @AdamSLevy describes needs a lot of changes under the hood of the PromQL engine. (Running parts of an expression first to then decide how to run the rest of the expression is a very different flow from what the engine is doing right now.)\r\n\r\n_However_ I do think we should consider changes of that kind eventually. This is what I more or less mean when I say \"PromQL still needs a query planner\". I'm not sure if that's the correct terminology, but we often run into cases where the engine is doing \"obviously\" needless work. Some of them are easy to address, some are pretty tricky. Often the engine would need to evaluate a branch to then eliminate other branches rather than fully evaluate them. And that's what the engine cannot do at all right now. It's just evaluating the full tree always. Sometimes, you can write queries in certain ways to minimize the damage, but that's not always possible (and not always easy).\r\n\r\nTherefore, it would be really nice to eventually get a proper query planner (again with the disclaimer that I might use the wrong technical term here). _However_ I do not think we should use that feature to introduce a fairly obscure `fallback` operator. What we should do is to make it all work automatically. Look at this query:\r\n```PromQL\r\njob:http_inprogress_requests:sum{job=\"example\"} or sum by (job) (http_inprogress_requests{job=\"example\"})\r\n```\r\nIn this case, the (future version of) the engine can run the left side, and whenever there is a result at all, it can stop. But even with the more general query as originally quoted by @AdamSLevy, i.e.\r\n```\r\njob:http_inprogress_requests:sum or sum by (job) (http_inprogress_requests)\r\n```\r\nthe engine could run the left side and then check out if there are any other `job` labels on any `http_inprogress_requests` series at all, and then only start the `sum` for those. I would expect that index lookup to be relatively inexpensive, while the `sum` aggregation is the actually expensive part (which the engine is currently fully doing, only to throw away most or even all of the results later).\r\n\r\nIn short: I'd love to see a query planner for PromQL, and once we have one, we can get most of the savings implicitly, without introducing a new operator.\r\n\r\n","Thanks everyone for the comments. I agree that if this could be addressed by improving performance of promQL with a query planner that would completely eliminate the need for this feature. In other words, if using `or` was as efficient (or close to) as just making the individual queries for the appropriate time ranges, then this language feature would not be necessary. \r\n\r\nFor my application to work around this, I'm expanding our application config to include an `availableAfter` field which lets us know after what timestamp a recording rule is available to use. Any queries before the timestamp are made with the actual expression. Any queries after that timestamp can use the recorded metric. The tricky bit is just the logic to split up a query that spans both before and after into two queries and stitching the results back together. More logic than I would have like to write but it actually ends up being more performant than any 'or' query.\r\n\r\nHope we get a query planner soon! Is that already on the roadmap\/being tracked anywhere that I can watch?\r\n\r\nI'll let someone else close this as I don't exactly know the policies for issues in this repo but feel free to.","In my experience, the best thing to get such a more involved feature going is to discuss it at the monthly dev summit. I have added a proposal to the agenda already.\r\n\r\nIn most cases these days, it's not as hard to reach general consensus for the feature. However, someone has to create a design doc then, and eventually the work has to be invested to implement it. The many TODOs in our [list of design docs](https:\/\/prometheus.io\/docs\/introduction\/design-doc\/) speak volumes about all the heroic plans we have. But at least this page gives you an idea where domain experts would be very welcome if the came to the rescue.\r\n\r\nThere is also a nominal [roadmap](https:\/\/prometheus.io\/docs\/introduction\/roadmap\/) on the project site but I think it has stopped to represent the more ambitious plans of the project many years ago.","I would love to see some kind of planner of this kind. Similarly when you do `count(min_over_time(xxx[2w])` we could immediately skip all samples and start calculating a number of selected series with samples within a 2w period.\r\n\r\nI think that is the natural evolution of PromQL engine, especially needed for efforts like https:\/\/github.com\/thanos-io\/thanos\/issues\/305"],"labels":["component\/promql"]},{"title":"Prometheus can start off an agent WAL and vice versa","body":"Currently, the agent can start off an agent WAL. I think we should somehow disallow that because the codepath to generate the WAL is not the same.\r\n\r\nAn alternative is to make sure that both WAL will remain compatible in the future.","comments":["Could you explain this a little more? Is the concern that you might generate an agent WAL and then switch Prometheus to non-agent mode with the same directory?","Yes but I actually think that the WAL has to be compatible for remot write, so we can probably ignore this?\r\n\r\nHowever, what would happen if I run prometheus, then run a couple of hours as an agent, then run prometheus back? Could there be issues?","Ah, interesting. The agent WAL should be mostly byte compatible with TSDB's WAL with the exception of tombstones. The agent storage doesn't create them, and I don't know if that would cause any problems for TSDB. The deletion semantics are also going to be different: agent's WAL will more aggressively delete samples and series in a way that the TSDB might not expect. I agree this is another case of locking it down now and considering loosening the restriction later if there's a need. \r\n\r\nAs for how: maybe a touchfile that indicates agent\/TSDB? That way, loading a WAL made from the other mode can return a failure. The user would be free to delete the file, but then whatever behavioral issues that might cause would be unsupported. ","I would have new magic codes for the agent Wal, and support both in the watcher","Both WALs would be compatible, and I think it is generally safer to allow switching with the same WAL. The deletion semantics does not change the format and replay characteristics of the WAL+Checkpoint. Trying to disallow switching feels like a wasted effort with no upside.","> However, what would happen if I run prometheus, then run a couple of hours as an agent, then run prometheus back? Could there be issues?\r\n\r\nThe major (and only?) difference is that I see, as Robert pointed out, will be how much data is left in WAL. Aka the deletion semantics.","Should we maybe see if someone with a large existing TSDB is willing to toggle agent mode on\/off a few times to see what happens? :) "],"labels":["component\/agent"]},{"title":"Agent documentation","body":"I'd like to have a reference documentation page about the Prometheus agent, which we can link to from the flag's `--help`.","comments":["I would like to work on this. Please assign me.","@roidelapluie I suppose I should write 'Prometheus Agent' documentation Page first?","As I understand it, you want 'Prometheus Agent' documentation link listed in `prometheus --help` command. \r\nNow, to solve this issue, the documentation has to be written and listed in the `--help` command. Am I right? \r\n"],"labels":["component\/documentation","component\/agent"]},{"title":"feature request: Add agent support for beyond WAL persistent storage and metric export","body":"Just an idea. \r\n\r\nCurrently, we are creating an implementation that has a scrape and WAL only https:\/\/github.com\/prometheus\/prometheus\/pull\/8785\/files\r\n\r\nI believe we can go further and offer persistent storage beyond WAL, so if agent is disconnected for longer than 2h it would simply compact block and upload to object storage. This is would an amazing solution for \"Too old data\" ingestion problem pointed in https:\/\/github.com\/cortexproject\/cortex\/issues\/2366 \r\n\r\nWould work well for both Cortex and Thanos, might be more tricky for other vendors since this escapes just Remote Write. \r\n\r\nSomething to consider \ud83e\udd17 \r\n\r\ncc @tomwilkie @rfratto @gouthamve ","comments":["Does this require direct access to the object stores used by Cortex\/Thanos? (ie no support for a hosted Prometheus backed by either?)","Either that or Thanos\/Cortex could provide API for upload - whatever works \ud83e\udd37\ud83c\udffd ","Creating blocks, why not, but uploading to bucket storage changes the scope of the project and should be discussed on the dev mailing list or during dev summits.","> I believe we can go further and offer persistent storage beyond WAL, so if agent is disconnected for longer than 2h it would simply compact block and upload to object storage.\r\n\r\nThis idea seems applicable to current Prometheus too where the compaction is already done, just short of shipping the block. It will be easiest (and fastest) to try it out there first if we ever decide to allow it.","Totally to be discussed on next Dev Summit \ud83d\udc4d\ud83c\udffd Happy to shepherd this discussion. ","Hi, \r\n\r\nThat's a great idea.\r\nSo, it scrape targets and forward to a tsdb backend like victoriametrics...  very simple and very usefull mode.\r\n\r\nHow, in agent mode, will work alerting with altermanager ?\r\n\r\nI've see the log \"field alerting is not allowed in agent mode\"...  i made a mistake or i don't understand something.\r\n\r\n\r\n","In Agent mode, the alerting has *currently* to be done by the remote write target (in your case, victoriametrics).","@dginhoux , if you forward data from Prometheus agent to VictoriaMetrics, then alerting rules can be evaluated at VictoriaMetrics side with [vmalert](https:\/\/docs.victoriametrics.com\/vmalert.html).","@bwplotka did we end up discussing this?","any news about this feature?\r\n","is this under development?"],"labels":["component\/agent"]},{"title":"Remote Write: non-recoverable errors influence desired shards","body":"https:\/\/github.com\/prometheus\/prometheus\/blob\/f29caccc42557f6a8ec30ea9b3c8c089391bd5df\/storage\/remote\/queue_manager.go#L819\r\n\r\nThe number of desired shards accounts for how much read WAL data is still attempting to be delivered. This is done by looking at the difference from the newest timestamp sent to remote_write (`prometheus_remote_storage_queue_highest_sent_timestamp_seconds`) and the newest timestamp read from the WAL. \r\n\r\n`prometheus_remote_storage_queue_highest_sent_timestamp_seconds` is not updated when a batch is dropped from a non-recoverable error. This makes sense, and is useful for tracking the delay of data appearing into remote_write. However, it does not make sense for use in the desired shards caclulation. If a client is _only_ receiving 400s, the metric will never increase, and will cause the desired shards to unnecessarily balloon up. \r\n\r\nInstead of `prometheus_remote_storage_queue_highest_sent_timestamp_seconds`, the desired shards calculation should use the highest timestamp of a sample that left the queue, either from a successful delivery or a non-recoverable error. ","comments":[],"labels":["component\/remote storage"]},{"title":"Use testing.T.TempDir() instead of ioutil.TempDir() in unit tests","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nInstead of writing the code below in many many unit tests:\r\n```go\r\n      dir, err := ioutil.TempDir(\"\", \"wal_fuzz_live\")\r\n      require.NoError(t, err)\r\n      defer func() {\r\n        require.NoError(t, os.RemoveAll(dir))\r\n      }()\r\n```\r\n\r\nWrite this instead:\r\n```go\r\n    dir := t.TempDir()\r\n```\r\n\r\nAdvantages:\r\n- Error handling is implicit.\r\n- Cleanup is implicit.\r\n- Naming of directory derives from name of test, again, implicit.\r\n\r\nCurrent occurences:\r\n```console\r\n0 \u2713 (40.6ms) 11:11:22 invidian@dellxps15mateusz ~\/repos\/prometheus (\u2387  main)$ git show --no-patch\r\ncommit a6e6011d55ed913cb8e53968cead9a6a6fd84911 (HEAD -> main, upstream\/main, upstream\/HEAD)\r\nAuthor: Furkan T\u00fcrkal <furkan.turkal@trendyol.com>\r\nDate:   Mon Oct 25 00:45:31 2021 +0300\r\n\r\n    Add scrape_body_size_bytes metric (#9569)\r\n\r\n    Fixes #9520\r\n\r\n    Signed-off-by: Furkan <furkan.turkal@trendyol.com>\r\n0 \u2713 (37.3ms) 11:11:29 invidian@dellxps15mateusz ~\/repos\/prometheus (\u2387  main)$ git grep ioutil.TempDir *_test.go | wc -l\r\n129\r\n```","comments":[":+1:","Hi @invidian i would like to work on this ","Sure. Just be aware that `t.TempDir()` use `t.Cleanup()` for triggering, which might execute at different time than currently used `defer`. It also always checks for errors, so you may find some tests on Windows to fail. My commits like https:\/\/github.com\/prometheus\/prometheus\/pull\/9583\/commits\/81aec2f2d9c8a9dbfa7dcbce05a0cffd488aa152 and https:\/\/github.com\/prometheus\/prometheus\/pull\/9583\/commits\/a2009e0fe54f46b4276180e5172a31647f6e20de tries to address it in #9583, so feel free to cherry-pick them if needed, so your PR passes the CI!","Please do this in multiple pull requests as well, not a big pull request. There are a lot of occurences.","i was looking through this wanting to contribute but I didn't find any `ioutil.TempDir` calls in any of the test files, i think this can be closed ","I guess this is because `ioutil.TempDir` got deprecated and is now replaced by `os.MkdirTemp`:\r\n```\r\n$ git describe --always\r\nv0.39.1-270-g6dd4e907a\r\n$ git grep os.MkdirTemp\r\ncmd\/promtool\/tsdb.go:           dir, err := os.MkdirTemp(\"\", \"tsdb_bench\")\r\npromql\/engine_test.go:  dir, err := os.MkdirTemp(\"\", \"test_concurrency\")\r\ntsdb\/blockwriter.go:    chunkDir, err := os.MkdirTemp(os.TempDir(), \"head\")\r\ntsdb\/example_test.go:   dir, err := os.MkdirTemp(\"\", \"tsdb-test\")\r\ntsdb\/tsdbutil\/dir_locker_testutil.go:                   tmpdir, err := os.MkdirTemp(\"\", \"test\")\r\nutil\/teststorage\/storage.go:    dir, err := os.MkdirTemp(\"\", \"test_storage\")\r\nutil\/testutil\/directory.go:     directory, err = os.MkdirTemp(defaultDirectory, name)\r\nweb\/api\/v1\/api_test.go: dbDir, err := os.MkdirTemp(\"\", \"tsdb-api-ready\")\r\n```","thanks i changed one occurrence of `os.MkdirTemp` in `engine_test.go` the other two test files that had `os.MkdirTemp` could not be changed because both of the calls were outside test functions","this is close?","Doesn't seem like it. Example: https:\/\/github.com\/prometheus\/prometheus\/blob\/6e2905a4d4ff9b47b1f6d201333f5bd53633f921\/web\/api\/v1\/api_test.go#L2567","There is #9641 , which appears to be abandoned. If anyone wants to pick up that work, that would be great.","Heey @invidian  and @beorn7 , I would love to be assigned on this task if possible, thank you guys in advance","@milencium Please, go ahead. PRs welcome.","@beorn7, thanks for this amazing opportunity, will do ! ","@beorn7 , I will proceed as you suggested"],"labels":["help wanted","kind\/cleanup","component\/tests"]},{"title":"Warn when targets relabelled to same labels","body":"closes https:\/\/github.com\/prometheus\/prometheus\/issues\/5136\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["This indeed does not belong to the discovery manager but to the scrape manager.","thanks @LeviHarrison, addressed your comments. \r\n\r\n> This indeed does not belong to the discovery manager but to the scrape manager.\r\n\r\n@roidelapluie the logic is currently in scrape manager's `reload()` method, after the `Sync()` has been called for each group. Do you think it should be someplace else? ","> @roidelapluie the logic is currently in scrape manager's reload() method, after the Sync() has been called for each group. Do you think it should be someplace else?\r\n\r\nI think @roidelapluie is affirming the location in this PR is correct.","Could you please also add a quick test for this?","Picking this up during our bug scrub. @darshanime are you still up to adding a test?","Discussed again at the bug scrub; seems like a useful change.  @LeviHarrison since you looked through it could you add a test please?","~How about doing this here https:\/\/github.com\/prometheus\/prometheus\/blob\/eea6ab1cdd24ec69c94ba4b0d165030c89860c8b\/scrape\/scrape.go#L485-L494~~instead of re-looping again?~\r\n\r\nEDIT: that probably not going to work as you're looking for dups across sLoops.\r\n\r\n~We may need to do the same for `notifier\/notifier.go`, I don't know if it's possible to get dups in there.~","ran the benchmark:\r\n```\r\n$ go test -bench=BenchmarkScrapeLoop -run=- -count 6 | tee main\r\n$ go test -bench=BenchmarkScrapeLoop -run=- -count 6 | tee duplicate_targets\r\n$ benchstat main duplicate_targets\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com\/prometheus\/prometheus\/scrape\r\n                      \u2502    main     \u2502         duplicate_targets         \u2502\r\n                      \u2502   sec\/op    \u2502   sec\/op     vs base              \u2502\r\nScrapeLoopAppend-10     37.58\u00b5 \u00b1 2%   37.31\u00b5 \u00b1 1%       ~ (p=0.065 n=6)\r\nScrapeLoopAppendOM-10   36.76\u00b5 \u00b1 1%   36.64\u00b5 \u00b1 1%       ~ (p=0.485 n=6)\r\ngeomean                 37.16\u00b5        36.97\u00b5       -0.51%\r\n```"],"labels":["stale"]},{"title":"Optimise VectorAnd for step invariant expression","body":"closes https:\/\/github.com\/prometheus\/prometheus\/issues\/9368\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["there are similar opportunities to optimize `VectorOr`, `VectorUnless` and maybe elsewhere too...\r\nwill look into that if this looks good ","benchmarks look great for the somewhat contrived queries: \r\n\r\n```\r\nrate(a_X[1m] @ start()) or rate(a_X[1m])\r\nrate(a_X[1m]) unless rate(a_X[1m] @ start())\r\n```\r\n\r\nbut good overall too...\r\n\r\n<details>\r\n  <summary>see benchmark<\/summary>\r\n\r\n```\r\nbenchmark                                                                                                              old ns\/op       new ns\/op       delta\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=10-12                                                                1331415         521372          -60.84%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1-12                                                                 1130721         456700          -59.61%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=100-12                                                255612          104772          -59.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1-12                                                  323835          133848          -58.67%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=10-12                                                 97785           40514           -58.57%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=100-12                                                               3563958         1479771         -58.48%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1-12                                          2789163         1175406         -57.86%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1-12                                                  81503           34572           -57.58%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1000-12                                                              20807437        8878912         -57.33%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1000-12                                               1594136         683827          -57.10%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=100-12                                                2104084         913280          -56.59%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=10-12                                                                    64319           28382           -55.87%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=10-12                                                 461107          204123          -55.73%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1000-12                                                                  3597283         1594170         -55.68%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1-12                                                                     48018           21350           -55.54%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=10-12                                         4237556         1896584         -55.24%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1000-12                                               16528192        7411697         -55.16%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=10-12                                                  74637           33590           -55.00%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=10-12                                                                    163954          74059           -54.83%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=100-12                                                                   218469          101818          -53.39%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=100-12                                                                   488831          232907          -52.35%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1-12                                                                     126252          60326           -52.22%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1000-12                                                                  1689980         814135          -51.83%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-12                                       169512531       82276697        -51.46%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-12                                        19777856        9711314         -50.90%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=100-12                                                 209459          105420          -49.67%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1-12                                                   51078           26715           -47.70%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1000-12                                                1401361         782545          -44.16%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1-12                                                   125267          71963           -42.55%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=10-12                                                  157815          92847           -41.17%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=100-12                                                 555043          335455          -39.56%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-12                        33423683        20669163        -38.16%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1000-12                                                3897947         2423183         -37.83%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1-12                                               776183          516819          -33.42%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=10-12                                              981640          670158          -31.73%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-12                                3064840         2096917         -31.58%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-12                         4550706         3194092         -29.81%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-12                            54877943        38812508        -29.27%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-12                                             3573897         2534818         -29.07%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-12                             6919438         5002720         -27.70%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1000-12                                                14321695705     10440053725     -27.10%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-12                                 477834          351528          -26.43%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-12                          586308          442876          -24.46%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-12                                            24637059        18831991        -23.56%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-12                                    4924171         3776317         -23.31%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-12                                     678841          523893          -22.83%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-12                             172382          133898          -22.32%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-12                          2655246         2070273         -22.03%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=10-12                    1442609         1131510         -21.57%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-12                           448698          353171          -21.29%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-12                           110537          87158           -21.15%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-12                     1267412         1000285         -21.08%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=10-12                                  177513          140256          -20.99%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1-12                                                       107666          85341           -20.74%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=10-12                            186203          148357          -20.33%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-12                   3650015         2911199         -20.24%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=100-12                                                     2260458         1808996         -19.97%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1-12                61126           48934           -19.95%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-12                  22105815        17876267        -19.13%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=10-12                              1714027         1387673         -19.04%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-12                                     98885           80644           -18.45%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=10-12                            59165           48710           -17.67%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-12                                    526168          433234          -17.66%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-12                                       45794           37772           -17.52%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-12                                       158767          131462          -17.20%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1-12                                           913077          756964          -17.10%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=10-12                                      197591          164697          -16.65%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=10-12                          1415003         1180918         -16.54%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=100-12                                                 348574820       291463346       -16.38%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=10-12                                      48924           41027           -16.14%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-12                                393785          330602          -16.05%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=100-12                                                  269668          226680          -15.94%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=10-12                                                   40801           34330           -15.86%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=10-12                                                   100515          84702           -15.73%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=10-12                                                      2795878         2363813         -15.45%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=100-12              292734          248396          -15.15%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=10-12               80674           68455           -15.15%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=10-12                                                      281399          238858          -15.12%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-12                                              2482370         2113584         -14.86%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-12                                   147804          126210          -14.61%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1000-12                                                    347123292       296833134       -14.49%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=100-12                                                     24089761        20659101        -14.24%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=10-12                                               784511          673914          -14.10%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1000-12             2306692         1982635         -14.05%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1-12                161599          139320          -13.79%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-12                             52776           45595           -13.61%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1-12                                                   11018454        9533294         -13.48%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1-12                                                    83755           72613           -13.30%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1000-12                                                 1660891         1443622         -13.08%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-12                                             17063053        14866127        -12.88%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1000-12                                                 807432          703876          -12.83%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-12                                 79999           69815           -12.73%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-12                           1148224         1006050         -12.38%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1-12                                                619630          544433          -12.14%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-12                                   41702           36834           -11.67%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-12                               1237247         1095350         -11.47%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1-12                                                                         537426          478011          -11.06%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-12                                                             1145154908      1019986412      -10.93%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1000-12                                                    21239042        18940619        -10.82%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=10-12               207772          185631          -10.66%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=10-12                                  44315           39660           -10.50%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1-12                                              514834          460935          -10.47%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1000-12                                                   867558          778763          -10.24%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=10-12                                                  32321294        29038817        -10.16%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=100-12                                         611490          550018          -10.05%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1-12                                                                         19151           17260           -9.87%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=100-12                                                                       53955           48644           -9.84%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1-12                                                       822848          743691          -9.62%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1-12                                                      66817           60444           -9.54%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1000-12                                        58011444        52518539        -9.47%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-12                                            1385592         1259566         -9.10%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1-12        1138571         1037987         -8.83%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-12                                           7654401         6981733         -8.79%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=10-12                                                                            65891           60128           -8.75%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=100-12                                                              163932377       149661533       -8.71%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=100-12                                                  111478          101784          -8.70%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1-12                                                    30805           28140           -8.65%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=100-12                                                    162967          149074          -8.53%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=10-12                                                     73136           66969           -8.43%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=100-12                                                                       1263814         1157501         -8.41%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=100-12                                                    96207           88337           -8.18%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=100-12                                                                           134894          124003          -8.07%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=10-12                                                                        21918           20153           -8.05%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1000-12                                                                          606663          557997          -8.02%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=100-12              776220          714147          -8.00%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1-12                                                                      27420           25288           -7.78%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=10-12                                                                        573464          528922          -7.77%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1000-12                                                   384270          355006          -7.62%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=10-12                                         1134346         1048478         -7.57%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=100-12                                                                    86996           80437           -7.54%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1-12                                                                         70798           65474           -7.52%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1-12                                                                             60107           55741           -7.26%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1000-12                                                                 109812098       101925430       -7.18%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=10-12                                                   58835270        54639439        -7.13%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-12                                                     168838076       156872204       -7.09%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1000-12                                                                      5954804         5533434         -7.08%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1000-12                                                                      332915          309362          -7.07%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=10-12                                                     64131           59603           -7.06%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=10-12                                                    62262           57869           -7.06%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=10-12                                                                        99921           92906           -7.02%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=10-12                                                     22721           21150           -6.91%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1-12                                                             76370           81978           +7.34%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=100-12                                                                           23059           21488           -6.81%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1-12                                                        656629          611987          -6.80%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=100-12                                                      7963602         7429027         -6.71%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                               36664181        39286614        +7.15%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1-12                                                                             15706           14666           -6.62%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=10-12                                                     171396          160193          -6.54%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-12                                                  78946405        73802987        -6.52%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-12                                            5184141         4850644         -6.43%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=10-12                                                       5915840         5536868         -6.41%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=100-12                                                      872824          817066          -6.39%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                   3755519         3516986         -6.35%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1000-12                                                                 10956671        10266075        -6.30%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=10-12                                             1240859         1162762         -6.29%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1-12                                                      20927           19626           -6.22%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=100-12                                         6468910         6067290         -6.21%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1-12                                                    57198285        53653285        -6.20%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=10-12                                             563780          529334          -6.11%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1-12                                                      139191          130704          -6.10%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=10-12                                                 151520          142516          -5.94%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1000-12                                                     2871683         2701456         -5.93%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1-12                                                      60052           56528           -5.87%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=10-12                                                    153223          144352          -5.79%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-12                                                            7968892         7515837         -5.69%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-12                                                 273450657       258010526       -5.65%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=100-12                                                    41031           38723           -5.63%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=10-12                                                       675957          638186          -5.59%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1000-12                                                                   576385          544591          -5.52%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1-12                                                                          440894          466628          +5.84%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1-12                                              888545          839883          -5.48%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=100-12                                                    526731          498101          -5.44%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                       29261784        27676226        -5.42%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=100-12                                                                  1677868         1587433         -5.39%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1-12                                          833252          788528          -5.37%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=100-12                                                380734          360334          -5.36%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=100-12                                                                       80204           84736           +5.65%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=100-12                                                                       406632          384899          -5.34%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1000-12             5583537         5286508         -5.32%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1000-12                                                                          69393           65726           -5.28%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1000-12                                                                  8786494         9272112         +5.53%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=10-12                                                            132571          139860          +5.50%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1000-12                                                   198130          187860          -5.18%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1-12                                                                     584183          554491          -5.08%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-12                                       4612974         4856622         +5.28%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1000-12                                                                      3156869         2998970         -5.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1-12                                                        5706733         5421546         -5.00%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=10-12                                                                    877301          833571          -4.98%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                 940176          989449          +5.24%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=100-12                                                94029           89383           -4.94%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-12                                                          115414904       109720614       -4.93%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1-12                                                  59963           57026           -4.90%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=10-12                                                                        73190           76919           +5.09%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-12                                     64608443        61533467        -4.76%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-12                                    536398507       511031523       -4.73%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=10-12                                                                            16155           15402           -4.66%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1-12                                             830383          792301          -4.59%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1000-12                                                  240115          229105          -4.59%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1-12                                                     132115          126152          -4.51%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10000-12                                                                  62837808        60007157        -4.50%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-12                                                           65590665        62655909        -4.47%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1000-12                                               370492          354188          -4.40%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=100-12                                                                   1471197         1537662         +4.52%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=100-12                                                               2086024         1996364         -4.30%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1-12                                                     58969           56446           -4.28%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1-12                                                                      124192          118906          -4.26%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10-12                                                                     32142           30785           -4.22%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                  631755          659523          +4.40%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10000-12                                                                1507177         1572723         +4.35%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10-12                                                               562872          587317          +4.34%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                           41294808        39577021        -4.16%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1000-12                                                                   5965677         5717947         -4.15%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-12                                      39881716        41598280        +4.30%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=100-12                                                                  15631874        14986826        -4.13%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1-12                                                                     481565          502226          +4.29%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     116816          121821          +4.28%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=10-12                                                                 615308          591015          -3.95%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1000-12                                                     27564662        26479021        -3.94%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1000-12                                                          15510812        16146716        +4.10%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=10-12                                                 61672           59283           -3.87%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      31670           30444           -3.87%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1000-12                                                          6193568         6442952         +4.03%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=10-12                                        970634          1008644         +3.92%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1-12                                                  130116          125249          -3.74%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10000-12                                                                  5714969         5504012         -3.69%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=10-12                                                                    549536          570524          +3.82%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=10-12                                                               65721294        63306486        -3.67%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=10-12                                            1066136         1027259         -3.65%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=100-12                                                           2158929         2239629         +3.74%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=100-12                                                                        1088252         1128812         +3.73%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1000-12                                                                       5301169         5497316         +3.70%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=100-12                                                   374742          361401          -3.56%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1-12                                         645353          669137          +3.69%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1000-12                                                              14635653        14128018        -3.47%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1-12                                                                452797          468751          +3.52%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-12                                                          1865748927      1931480722      +3.52%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1-12                                                                    56657           58642           +3.50%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1000-12                                                                 1353317         1400198         +3.46%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-12                                                             13489831        13952406        +3.43%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    445721          460977          +3.42%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10000-12                                                          706119732       683035841       -3.27%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=10-12                                                                        24299           25115           +3.36%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=10-12                                                                 75027           72597           -3.24%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=10-12                                                           73852           76300           +3.31%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=100-12                                                                    745256          721496          -3.19%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1-12                                                                  495278          479625          -3.16%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=10-12                                                           614393          634112          +3.21%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-12                                                       18706355        19306123        +3.21%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1-12                                             25304           26114           +3.20%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10000-12                                                                14913911        15371416        +3.07%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10-12                                                                     180720          175348          -2.97%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1-12                                           99015           96114           -2.93%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1000-12                                                                 149069          153539          +3.00%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   3554753         3660572         +2.98%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10-12                                                             1693000         1644350         -2.87%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=100-12                                                   79087           76824           -2.86%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=100-12                                                                  209252          215400          +2.94%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=100-12                                                                1979838         1923438         -2.85%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=10-12                                                            75887936        73759049        -2.81%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1000-12                                               2309554         2245162         -2.79%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-12                                                           158475549       154145043       -2.73%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1-12                                                         6151198         5985056         -2.70%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1000-12                                          3870293         3976826         +2.75%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1-12                                                             56312992        57816657        +2.67%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1-12                                                             510632          524239          +2.66%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                4444048         4560490         +2.62%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1000-12                                                               13893723        13540716        -2.54%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=100-12                                                                       212330          217840          +2.60%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1-12                                                                         61522           63116           +2.59%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1-12                                                  807213          827922          +2.57%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=100-12                                                                  33804           34651           +2.51%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10-12                                                                   67686           69319           +2.41%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                          28454536        27785582        -2.35%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10-12                                                                   19345           19810           +2.40%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=100-12                                                          219071          224312          +2.39%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=10-12                                                                         478314          489753          +2.39%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=10-12                                                                615127          601109          -2.28%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=10-12                                            30941           31649           +2.29%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=100-12                                                              1983308         2028675         +2.29%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=100-12                                                                            114591          117170          +2.25%\r\nBenchmarkRangeQuery\/expr=a_one,steps=10-12                                                                             13076           13369           +2.24%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-12                                                            23094908        23608443        +2.22%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      86069           87968           +2.21%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1-12                                                                    18321           18725           +2.21%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1-12                                                                 497730          487048          -2.15%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=100-12                                                           707929          723393          +2.18%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1000-12                                                              18851879        19260276        +2.17%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=10-12                                                       7438882         7599827         +2.16%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1-12                                                  6457573         6596406         +2.15%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1-12                                                            61264           62560           +2.12%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1000-12                                                               1433929         1404989         -2.02%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                  2307191         2262952         -1.92%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1-12                                                                 5625217         5734192         +1.94%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1000-12                                                              187829080       191465504       +1.94%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1000-12                                                                           536931          547287          +1.93%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1-12                                       8785040         8622124         -1.85%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1000-12                                                                      615907          627205          +1.83%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1-12                                                                56072007        55080117        -1.77%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1000-12                                                         1458458         1483544         +1.72%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=100-12                                                                216598          212960          -1.68%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1000-12                                                              5317293         5406652         +1.68%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1000-12                                                                           61264           62291           +1.68%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=10-12                                                                   6336828         6442803         +1.67%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-12                                            616926036       627218936       +1.67%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1000-12                                                                      1404343         1427343         +1.64%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=100-12                                                               24439567        24086718        -1.44%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=10-12                                                                7495111         7386950         -1.44%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1-12                                              63760876        64625251        +1.36%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-12                                                      116359371       114839794       -1.31%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1-12                                                              6206721         6126452         -1.29%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1000-12                                        4423246         4480131         +1.29%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-12                                           3759993         3712458         -1.26%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1000-12                                                                  32665393        32267421        -1.22%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1-12                                                                    634130          627107          -1.11%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=10-12                                                                819936          810861          -1.11%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-12                                                      24015049        23758940        -1.07%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=10-12                                                             7347757         7425601         +1.06%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=100-12      5196157         5142001         -1.04%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10000-12                                                            151692952       153228015       +1.01%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1-12                                                                 639760          646183          +1.00%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   493456          488652          -0.97%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=100-12                                                               617925          623851          +0.96%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=100-12                                           475413          470911          -0.95%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1-12                                                        6095994         6153441         +0.94%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=100-12                                                           236450935       238671403       +0.94%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1000-12     37830462        37480703        -0.92%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=10-12                                                                             55168           55664           +0.90%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     35478           35168           -0.87%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=10-12       1427906         1415543         -0.87%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1-12                                                                              12974           12863           -0.86%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1-12                                                            489447          493601          +0.85%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=10-12                                      13297446        13408087        +0.83%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=10-12                                            115673          114726          -0.82%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=10-12                                                                   717918          723698          +0.81%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=10-12                                                                120154          119249          -0.75%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1-12                                                                 72165           71667           -0.69%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1-12                                                                    5574550         5536247         -0.69%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1-12                                                                              50216           50523           +0.61%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=100-12                                                               2478624         2463675         -0.60%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-12                                        3861827         3838984         -0.59%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=10-12                                                            651189          654902          +0.57%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=10-12                                          1221738         1214895         -0.56%\r\nBenchmarkRangeQuery\/expr=a_one,steps=100-12                                                                            19762           19656           -0.54%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1-12                                                                         19113           19206           +0.49%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=100-12                                                          2021592         2031001         +0.47%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=10-12                                                 1301995         1307311         +0.41%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1000-12                                                         14673260        14614751        -0.40%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1-12                                             81913           82237           +0.40%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-12                                                           18686405        18756932        +0.38%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=10-12                                                 11492026        11533407        +0.36%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=100-12                                                6362006         6339769         -0.35%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1000-12                                               56802478        57001530        +0.35%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    78649           78403           -0.31%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=10-12                                                        6909485         6890390         -0.28%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=100-12                                                                   3860351         3849851         -0.27%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=10-12                                             113883548       114161133       +0.24%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1-12                                                              1125156         1127807         +0.24%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1000-12                                               565215946       563943029       -0.23%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-12                                           5626168585      5637630801      +0.20%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=100-12                                           89588           89648           +0.07%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=10-12                                          135502          135578          +0.06%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=100-12                                                61781930        61816292        +0.06%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1-12                                                             5924430         5926604         +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=10-12                                                            7017715         7016133         -0.02%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1000-12                                          634417          634364          -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1-12                                                                  62401           62403           +0.00%\r\n\r\nbenchmark                                                                                                              old allocs     new allocs     delta\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-12                        18636          10192          -45.31%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-12                            26028          17611          -32.34%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-12                                4071           3053           -25.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-12                                    5168           4151           -19.68%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-12                         6424           5584           -13.08%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-12                             7269           6425           -11.61%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-12                                 1065           967            -9.20%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-12                                     1186           1087           -8.35%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-12                             339            345            +1.77%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=10-12                            360            366            +1.67%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=10-12                          4982           4902           -1.61%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=10-12                              5166           5085           -1.57%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1-12                377            383            +1.59%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-12                                   283            287            +1.41%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-12                                       285            289            +1.40%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=10-12                                  301            305            +1.33%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=10-12                                      303            307            +1.32%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1-12                                                                         153            155            +1.31%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1-12                                                                         161            163            +1.24%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=10-12                                                                        162            164            +1.23%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=10-12               521            527            +1.15%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-12                           550            556            +1.09%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1-12                                                      184            186            +1.09%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1-12                                                                     184            186            +1.09%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1-12                                                   198            200            +1.01%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=10-12                                                     202            204            +0.99%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1-12                                                                      216            218            +0.93%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1-12                                                    222            224            +0.90%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=10-12                                                                        224            226            +0.89%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1-12                                             230            232            +0.87%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10-12                                                                     234            236            +0.85%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-12                                 488            492            +0.82%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-12                                     490            494            +0.82%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=10-12                                  741            735            -0.81%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-12                             752            758            +0.80%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=10-12                                      764            758            -0.79%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=10-12                                                                    256            258            +0.78%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=100-12                                                                       257            259            +0.78%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1-12                771            777            +0.78%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=10-12                            773            779            +0.78%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=10-12                                                   258            260            +0.78%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=10-12                                                  261            263            +0.77%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      270            272            +0.74%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1-12                                                  283            285            +0.71%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=10-12                                            293            295            +0.68%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1-12                                                     298            300            +0.67%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1-12                                                      300            302            +0.67%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1-12                                                  300            302            +0.67%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=10-12                                                 301            303            +0.66%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=10-12               933            939            +0.64%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     315            317            +0.63%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=10-12                                                    316            318            +0.63%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=10-12                                                     318            320            +0.63%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=10-12                                                 318            320            +0.63%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1-12                                                                         319            321            +0.63%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-12                           1008           1014           +0.60%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1-12                                                                  338            340            +0.59%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1-12                                                            340            342            +0.59%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1-12                                                                     342            344            +0.58%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1-12                                                                         350            352            +0.57%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1-12                                                      361            363            +0.55%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=10-12                                                                        369            371            +0.54%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=10-12                                                     379            381            +0.53%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=10-12                                                                        382            384            +0.52%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1-12                                                    384            386            +0.52%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=100-12                                                    387            389            +0.52%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1-12                                                   395            397            +0.51%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1-12                                                                 400            402            +0.50%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=10-12                                                                 401            403            +0.50%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=10-12                                                   402            404            +0.50%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=10-12                                                           412            414            +0.49%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=10-12                                                                    414            416            +0.48%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1-12                                                             422            424            +0.47%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=100-12                                                                    424            426            +0.47%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1-12                                             445            447            +0.45%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=10-12                                                  467            469            +0.43%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-12                                   714            717            +0.42%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-12                                       727            730            +0.41%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1-12                                           485            487            +0.41%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=100-12                                                491            493            +0.41%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      496            498            +0.40%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=100-12                                                   501            503            +0.40%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=100-12                                                    503            505            +0.40%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=100-12                                                503            505            +0.40%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=10-12                                            518            520            +0.39%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1-12                                                       523            525            +0.38%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     551            553            +0.36%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1-12                                                     586            588            +0.34%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1-12                                                  586            588            +0.34%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1-12                                                      595            597            +0.34%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=100-12                                                                       597            599            +0.34%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=100-12                                                    600            602            +0.33%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=10-12                                                    604            606            +0.33%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=10-12                                                 604            606            +0.33%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1-12                                                                      609            611            +0.33%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=100-12                                                  628            630            +0.32%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=100-12                                                  630            632            +0.32%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=10-12                                                     632            634            +0.32%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10-12                                                                     637            639            +0.31%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=100-12              1971           1977           +0.30%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=10-12                                                                700            702            +0.29%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=10-12                                          728            730            +0.27%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1-12                                                  753            755            +0.27%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    770            772            +0.26%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=10-12                                                 781            783            +0.26%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-12                          2387           2393           +0.25%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=10-12                                                            821            823            +0.24%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=100-12                                                   846            848            +0.24%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=100-12                                                846            848            +0.24%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=100-12                                                                       859            861            +0.23%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=100-12              2635           2641           +0.23%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=100-12                                                 896            898            +0.22%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=100-12                                           928            930            +0.22%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=100-12                                                                   981            983            +0.20%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=100-12                                                                    1001           1003           +0.20%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-12                          3097           3103           +0.19%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=100-12                                                                       1053           1055           +0.19%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=100-12                                                                1076           1078           +0.19%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-12                                                           24210          24167          -0.18%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    1139           1141           +0.18%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=100-12                                                1145           1147           +0.17%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-12                                2322           2326           +0.17%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-12                                    2324           2328           +0.17%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=100-12                                                                   1175           1177           +0.17%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=100-12                                                          1177           1179           +0.17%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1000-12                                                                      1198           1200           +0.17%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=100-12                                                 1228           1230           +0.16%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=100-12                                           1286           1288           +0.16%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1-12        4571           4578           +0.15%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=10-12                                                      1463           1465           +0.14%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-12                     4732           4738           +0.13%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=10-12       4733           4739           +0.13%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=10-12                    4754           4760           +0.13%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1000-12                                               4528           4533           +0.11%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                  2772           2775           +0.11%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1-12                                                                     1853           1855           +0.11%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1000-12                                                              934            935            +0.11%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1-12                                                                 1876           1878           +0.11%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=10-12                                                                    1916           1918           +0.10%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-12                                              2889           2892           +0.10%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=10-12                                                                1948           1950           +0.10%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1000-12                                               1014           1013           -0.10%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1-12                                              2075           2077           +0.10%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1-12                                                                  2082           2084           +0.10%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1-12                                                                 2087           2089           +0.10%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=10-12                                             2093           2095           +0.10%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1-12                                                            2102           2104           +0.10%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1-12                                                2109           2111           +0.09%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1-12                                                             2109           2111           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=100-12                                                    1064           1065           +0.09%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=10-12                                               2143           2145           +0.09%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-12                   5439           5444           +0.09%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1000-12                                                   2228           2230           +0.09%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                               19265          19248          -0.09%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1-12                                                                     2266           2268           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1000-12                                                                   4581           4585           +0.09%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1-12                                               2295           2297           +0.09%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1000-12                                                                   2306           2308           +0.09%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=10-12                                                                    2341           2343           +0.09%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1000-12                                                  2342           2344           +0.09%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1000-12                                                   2344           2346           +0.09%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1000-12                                               2344           2346           +0.09%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1000-12                                               2353           2355           +0.08%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=10-12                                                                 2362           2364           +0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=10-12                                              2367           2369           +0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=100-12      7156           7162           +0.08%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=10-12                                                                2387           2389           +0.08%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-12                           4888           4884           -0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=10-12                                                           2472           2474           +0.08%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-12                                      21013          21030          +0.08%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-12                               5006           5002           -0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=10-12                                                            2508           2510           +0.08%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1-12                                         2541           2543           +0.08%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=10-12                                        2670           2672           +0.07%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-12                                            2674           2676           +0.07%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1000-12                                                   2783           2785           +0.07%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1000-12                                                 2873           2875           +0.07%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                 2883           2885           +0.07%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=100-12                                                                   2947           2949           +0.07%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=100-12                                                               3069           3071           +0.07%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=100-12                                         3203           3205           +0.06%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1-12                                           3207           3209           +0.06%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                  3222           3224           +0.06%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1000-12                                               3222           3224           +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1-12                                             3224           3226           +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1-12                                          3224           3226           +0.06%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=100-12                                                           4855           4858           +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1-12                                              3286           3288           +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=10-12                                            3339           3341           +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=10-12                                         3339           3341           +0.06%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-12                  10048          10054          +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=10-12                                             3446           3448           +0.06%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-12                                             3488           3490           +0.06%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   6998           7002           +0.06%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                   5340           5337           -0.06%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1-12                                                       3721           3723           +0.05%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=100-12                                                                5602           5605           +0.05%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=100-12                                                               3745           3747           +0.05%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-12                                                            6359           6356           -0.05%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1000-12                                                 4310           4312           +0.05%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=100-12                                                          6612           6615           +0.05%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1-12                                                              4429           4431           +0.05%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                       22236          22246          +0.04%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10-12                                                             4532           4534           +0.04%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10000-12                                                                  48497          48476          -0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=100-12                                                           6938           6941           +0.04%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-12                                        7236           7239           +0.04%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1-12                                          5307           5309           +0.04%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   5311           5313           +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1000-12             16433          16439          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=100-12                                                               5828           5830           +0.03%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=10-12                                          6312           6314           +0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1000-12             19401          19407          +0.03%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=100-12                                                                   3490           3491           +0.03%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1000-12                                                                      7200           7202           +0.03%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=100-12                                                     10835          10838          +0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1000-12                                                7227           7229           +0.03%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1000-12                                          7269           7271           +0.03%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1000-12                                                                  14742          14746          +0.03%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1000-12                                                                      7736           7738           +0.03%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1000-12                                                               7797           7799           +0.03%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=10-12                                                      11924          11927          +0.03%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1000-12                                                                  8222           8224           +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-12                                           8265           8267           +0.02%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-12                                                             78666          78647          -0.02%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1000-12                                                8711           8713           +0.02%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-12                                       4358           4359           +0.02%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1000-12                                                                  8758           8760           +0.02%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                4393           4392           -0.02%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1000-12                                                         8798           8800           +0.02%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1000-12                                          8948           8950           +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                           27328          27322          -0.02%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1000-12     28964          28970          +0.02%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1000-12                                                    104574         104553         -0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-12                                             10127          10129          +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-12                                           5087           5088           +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-12                                        5088           5089           +0.02%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=10-12                                         5409           5410           +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-12                                            5650           5649           -0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-12                                                          98256          98239          -0.02%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1000-12                                                                  13038          13040          +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10000-12                                                          283631         283591         -0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=10-12                                                                   7178           7177           -0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=100-12                                                                  7277           7278           +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1000-12                                                          51012          51019          +0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-12                                       23084          23087          +0.01%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1000-12                                                     7977           7976           -0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1000-12                                                                 7981           7980           -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1000-12                                                              34156          34160          +0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10000-12                                                                8644           8643           -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-12                                                           26624          26627          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-12                                                       27733          27736          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-12                                                            46350          46355          +0.01%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=100-12                                                     94429          94419          -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1-12                                                             18922          18924          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1-12                                                         18944          18946          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1000-12                                                         47790          47795          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=10-12                                                            19222          19224          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=10-12                                                        19343          19345          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1-12                                                        19600          19602          +0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-12                                           78850          78842          -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=10-12                                                             21450          21452          +0.01%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10000-12                                                                  21942          21944          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=10-12                                                       22550          22552          +0.01%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=100-12                                                 921860         921784         -0.01%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1000-12                                                    920495         920565         +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-12                                            13493          13494          +0.01%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1000-12                                        27925          27927          +0.01%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1000-12                                                              14061          14062          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-12                                                     393085         393058         -0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-12                                                      109258         109252         -0.01%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=100-12                                         37812          37814          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1-12                                                              19401          19402          +0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-12                                            71446          71449          +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-12                                                 78355          78352          -0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1-12                                       30150          30151          +0.00%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1000-12                                                9966037        9966339        +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1-12                                                    70351          70349          -0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=10-12                                                               70361          70359          -0.00%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1-12                                                   35473          35474          +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-12                                                  71350          71352          +0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1000-12                                                               37784          37783          -0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1000-12                                                              40005          40006          +0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-12                                                           292938         292931         -0.00%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1000-12                                                          45172          45173          +0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-12                                     372279         372287         +0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1000-12                                        352588         352595         +0.00%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-12                                                      56455          56454          -0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=10-12                                      60850          60851          +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=10-12                                                   70350          70349          -0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1-12                                                                70360          70359          -0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=100-12                                                           71362          71361          -0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-12                                                          78668          78667          -0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-12                                    3484059        3484094        +0.00%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=10-12                                                  115531         115530         -0.00%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1-12                                                                         2128           2128           +0.00%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=10-12                                                                        2128           2128           +0.00%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=100-12                                                                       2529           2529           +0.00%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1000-12                                                                      6320           6320           +0.00%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1-12                                                                             134            134            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=10-12                                                                            134            134            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=100-12                                                                           139            139            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1000-12                                                                          180            180            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1-12                                                                             319            319            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=10-12                                                                            319            319            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=100-12                                                                           360            360            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1000-12                                                                          743            743            +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1-12                                                                          1707           1707           +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=10-12                                                                         1707           1707           +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=100-12                                                                        2108           2108           +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1000-12                                                                       5899           5899           +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                          22245          22245          +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1-12                                                                              116            116            +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=10-12                                                                             116            116            +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=100-12                                                                            121            121            +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1000-12                                                                           162            162            +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1-12                                                                              264            264            +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=10-12                                                                             264            264            +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=100-12                                                                            305            305            +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1000-12                                                                           688            688            +0.00%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1000-12                                                                      2858           2858           +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1-12                                                        854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=10-12                                                       854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=100-12                                                      864            864            +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1000-12                                                     934            934            +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1-12                                                        7176           7176           +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=10-12                                                       7176           7176           +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=100-12                                                      7276           7276           +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1-12                                                             70359          70359          +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=10-12                                                            70360          70360          +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1-12                                                                 854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=10-12                                                                854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=100-12                                                               864            864            +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1-12                                                                 7177           7177           +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=10-12                                                                7177           7177           +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=100-12                                                               7278           7278           +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1000-12                                                              7987           7987           +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1-12                                              70440          70440          +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=10-12                                             70441          70441          +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1-12                                                  932            932            +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=10-12                                                 932            932            +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=100-12                                                942            942            +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1-12                                                  7258           7258           +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=10-12                                                 7256           7256           +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=100-12                                                7353           7353           +0.00%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1000-12                                               8067           8067           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=100-12                                                              71360          71360          +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1-12                                                                2149           2149           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10-12                                                               2149           2149           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=100-12                                                              2550           2550           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-12                                                             5354           5354           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10000-12                                                            84874          84874          +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1-12                                                                    854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=10-12                                                                   854            854            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=100-12                                                                  864            864            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1000-12                                                                 934            934            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1-12                                                                    155            155            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10-12                                                                   155            155            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=100-12                                                                  160            160            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1000-12                                                                 191            191            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10000-12                                                                1023           1023           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1-12                                                                    7177           7177           +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1-12                                                                    340            340            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10-12                                                                   340            340            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=100-12                                                                  381            381            +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1000-12                                                                 664            664            +0.00%\r\n\r\nbenchmark                                                                                                              old bytes       new bytes       delta\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-12                        5828254         591328          -89.85%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-12                                406532          115429          -71.61%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-12                            7486069         2248670         -69.96%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-12                         870285          348396          -59.97%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-12                                    570619          279409          -51.03%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-12                             1047192         523496          -50.01%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-12                                 78884           50068           -36.53%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-12                                     96162           67243           -30.07%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=10-12                          379616          327922          -13.62%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=10-12                              406311          354315          -12.80%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1000-12                                                             9169448         8567712         -6.56%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=10-12                                  46526           43944           -5.55%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=10-12                                      49071           46507           -5.23%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1000-12                                                                 1079797         1136860         +5.28%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-12                             16911           17380           +2.77%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=10-12                            17781           18259           +2.69%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-12                                   14391           14743           +2.45%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-12                                       14465           14815           +2.42%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=10-12                                      14895           15255           +2.42%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=10-12                                  14814           15170           +2.40%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1000-12                                                 7644960         7487732         -2.06%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-12                           26039           26513           +1.82%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-12                                 19475           19827           +1.81%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-12                                     19571           19919           +1.78%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=100-12                                                              5367416         5456412         +1.66%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-12                               343965          338877          -1.48%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1-12                20319           20616           +1.46%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-12                           332116          327432          -1.41%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1-12                                                                     7972            8083            +1.39%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1000-12                                                     1251232         1235204         -1.28%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=100-12                                                               1024462         1037714         +1.29%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1-12                                                   9053            9157            +1.15%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1-12                                                  15404           15573           +1.10%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1-12                                             9780            9884            +1.06%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=10-12               28270           28543           +0.97%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-12                             46458           46898           +0.95%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=10-12                                                  12789           12908           +0.93%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=10-12                                                                    12220           12333           +0.92%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=10-12                                                 15863           16009           +0.92%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1-12                                                                         7011            7075            +0.91%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=10-12                                            11437           11541           +0.91%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=10-12                            47351           47780           +0.91%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1-12                                                                         6578            6635            +0.87%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=10-12                                                                        6795            6851            +0.82%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=100-12                                                                  1024076         1032018         +0.78%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1000-12                                       3389489         3363594         -0.76%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      12534           12628           +0.75%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1-12                                                      8827            8891            +0.73%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=100-12                                                20768           20916           +0.71%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1-12                                                                    7107            7157            +0.70%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     13614           13709           +0.70%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-12                           57046           57442           +0.69%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=10-12                                                     9260            9323            +0.68%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=1-12                                                    5336926         5301322         -0.67%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10-12                                                                   7112            7158            +0.65%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=10-12                                                   5349218         5315449         -0.63%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=100-12                                                                       9244            9301            +0.62%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=100-12                                                                  588359          591881          +0.60%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=10-12                                                                        10828           10892           +0.59%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1-12                                                                     19455           19568           +0.58%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1-12                49626           49913           +0.58%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1-12                                                                      11524           11588           +0.56%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=100-12                                                                  7400            7441            +0.55%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1-12                                                    11820           11884           +0.54%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10-12                                                                     11956           12020           +0.54%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=1000-12                                                                 10175           10228           +0.52%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1-12                                           28756           28904           +0.51%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=10-12                                                   13404           13469           +0.48%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-12                                    67070           67384           +0.47%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=10-12                                                                    23993           24104           +0.46%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=10-12               58144           58409           +0.46%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=100-12                                                    13871           13934           +0.45%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=100-12                                                      1045254         1040536         -0.45%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-12                          150794          151474          +0.45%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=1-12                                                                5336516         5360112         +0.44%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m]),steps=10000-12                                                                87893           88260           +0.42%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1-12                                             26351           26458           +0.41%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=100-12                                                 124453477       124955540       +0.40%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=10-12                                          39716           39876           +0.40%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-12                          102325          102737          +0.40%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-12                                67075           67341           +0.40%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=100-12                                                                    16853           16919           +0.39%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    24708           24802           +0.38%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1-12                                                     20185           20261           +0.38%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=10-12                                                                   1024202         1020366         -0.37%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=100-12                                           28293           28396           +0.36%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=10-12                                            29475           29581           +0.36%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=10-12                                                  27773           27872           +0.36%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=100-12                                            6007456         6028596         +0.35%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1-12                                                                         18495           18559           +0.35%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1-12                                                   23723           23803           +0.34%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1-12                                                            19805           19871           +0.33%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1-12                                                                  19758           19823           +0.33%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                      29506           29601           +0.32%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1-12                                                      20894           20959           +0.31%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=100-12                                                                  21021           21086           +0.31%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=10-12                                                    20625           20688           +0.31%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                     32054           32151           +0.30%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=1-12                                                        1019762         1016750         -0.30%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=100-12              108156          108476          +0.30%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=10-12                                                     21327           21390           +0.30%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=10-12                                                                        22598           22663           +0.29%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=10-12                                                 20688           20746           +0.28%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=10-12                                                   24400           24468           +0.28%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1000-12                                                    124808498       124463966       -0.28%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=10-12                                                 49562           49696           +0.27%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1-12                                                    23876           23940           +0.27%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1-12                                                                 1019577         1016854         -0.27%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=10-12                                                     20676           20731           +0.27%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=10-12                                                           24129           24193           +0.27%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=100-12                                                    25310           25377           +0.26%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1-12                                                      20255           20308           +0.26%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=10-12                                                                 23865           23927           +0.26%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1-12                                                                 25172           25237           +0.26%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1-12                                                             25700           25765           +0.25%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1-12                                                                         21903           21958           +0.25%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=10-12                                                                        23584           23643           +0.25%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1000-12                                            804184          802182          -0.25%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1-12                                                                    18856           18902           +0.24%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=100-12                                                   25233           25293           +0.24%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=1000-12                                                              1195280         1192620         -0.22%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=100-12                                                    27811           27873           +0.22%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1000-12                                               1248442         1245670         -0.22%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=100-12                                                               588188          589491          +0.22%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1000-12                                                    9215131         9194920         -0.22%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1-12                                                  47530           47634           +0.22%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1000-12                                           9400432         9380472         -0.21%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=100-12                                                  29827           29890           +0.21%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1-12                                                  38562           38643           +0.21%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=1-12                                                       31982           32049           +0.21%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10-12                                                                   18858           18897           +0.21%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=100-12                                        678373          676978          -0.21%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=100-12                                                25300           25352           +0.21%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=100-12                                                                   55003           55114           +0.20%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1000-12                                                              604341          605461          +0.19%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=100-12                                                  32115           32174           +0.18%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1-12                                                        589414          588340          -0.18%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=100-12                                                 50604           50696           +0.18%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1-12                                                      39844           39915           +0.18%\r\nBenchmarkRangeQuery\/expr=abs(a_one),steps=1000-12                                                                      34285           34345           +0.18%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=100-12                                           62867           62976           +0.17%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=100-12                                                           1999471         2002909         +0.17%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                    59695           59797           +0.17%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=1000-12                                               126755          126966          +0.17%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1-12                                                                      38496           38560           +0.17%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=10000-12                                                                625925          624921          -0.16%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-12                     332015          332548          +0.16%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=10-12                                                    39033           39095           +0.16%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=100-12                                                                   71527           71639           +0.16%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                  126726          126919          +0.15%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1-12                                                  20271           20301           +0.15%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1-12                                           229407          229745          +0.15%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10-12                                                                     40532           40590           +0.14%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=100-12                                                                       49289           49356           +0.14%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=100-12              147608          147808          +0.14%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=10-12                                      3565737         3561029         -0.13%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=10-12                                                                   590936          591712          +0.13%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=10-12                                                                50039           50104           +0.13%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=100-12                                                     11392367        11377648        -0.13%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=100-12                                                1639150         1637062         -0.13%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-12                                   43476           43531           +0.13%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=1000-12                                                492123          491502          -0.13%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=100-12                                              271984          272327          +0.13%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=1000-12                                               314029          313636          -0.13%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=10-12                                              169192          169401          +0.12%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=10-12                                                            52943           53008           +0.12%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-12                  627865          628627          +0.12%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=100-12                                                            616561          615814          -0.12%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1-12                                                                          98126           98008           -0.12%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=100-12                                        567780          567112          -0.12%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=100-12                                                                       42582           42632           +0.12%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=100-12                                                      590768          591459          +0.12%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=10-12                                                                589106          588421          -0.12%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1000-12                                                              886391          885386          -0.11%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=1-12                                                     38611           38654           +0.11%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=10-12                    333305          333675          +0.11%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l='notfound'},steps=1000-12                                                   60523           60590           +0.11%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=1-12                                               165196          165376          +0.11%\r\nBenchmarkRangeQuery\/expr=a_one_and_b_one{l=~'.*[0-4]$'},steps=1000-12                                                  71966           72043           +0.11%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m])),steps=100-12                                             230670          230916          +0.11%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1000-12                                                          9175056         9165504         -0.10%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1000-12     1652947         1654659         +0.10%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]),steps=1000-12                                                                 38750           38790           +0.10%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=100-12                                                                    65172           65239           +0.10%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=100-12                                                                       65812           65877           +0.10%\r\nBenchmarkRangeQuery\/expr=a_one_unless_b_one{l=~'.*[0-4]$'},steps=1000-12                                               72028           72098           +0.10%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10000-12                                                          38106460        38069524        -0.10%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=100-12                                                                67290           67354           +0.10%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=10-12                                                     44381           44422           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=10-12                                                 39017           39053           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l=~'.*[0-4]$'},steps=100-12                                                   46629           46672           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_unless_b_ten{l=~'.*[0-4]$'},steps=100-12                                                46646           46689           +0.09%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=1000-12                                                                   316569          316860          +0.09%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=100-12                                                          69714           69778           +0.09%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=10-12                                                       589001          589512          +0.09%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=1-12                                                                 129781          129891          +0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m])),steps=100-12                                                 70394           70451           +0.08%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=10-12                                             5942373         5947106         +0.08%\r\nBenchmarkRangeQuery\/expr=a_one_or_b_one{l=~'.*[0-4]$'},steps=1000-12                                                   71991           72048           +0.08%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=1000-12                                                                   66938           66990           +0.08%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=10-12                                                      1362657         1363711         +0.08%\r\nBenchmarkRangeQuery\/expr=a_ten_+_on(l)_group_right_a_one,steps=1000-12                                                 112190          112276          +0.08%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=1-12        332675          332930          +0.08%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-12                                       44567           44601           +0.08%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_hundred[1d],_0.3,_0.3),steps=1-12                                              5935450         5931250         -0.07%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1-12                                                        1522603         1523680         +0.07%\r\nBenchmarkRangeQuery\/expr=a_ten_and_b_ten{l='notfound'},steps=1000-12                                                   95210           95277           +0.07%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=100-12                                                    92927           92992           +0.07%\r\nBenchmarkRangeQuery\/expr=label_replace(a_one,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   136198          136292          +0.07%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                           7529391         7524252         -0.07%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1m])_+_rate(b_one[1m]),steps=1000-12                                               69816           69769           -0.07%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=100-12                                                           5490793         5487102         -0.07%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=1-12                                                  1219658         1220458         +0.07%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=10-12       341196          341415          +0.06%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1000-12                                                                          9372            9366            -0.06%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=100-12                                                              155338          155240          -0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1-12                                          218391          218528          +0.06%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1-12                                         191124          191240          +0.06%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=1000-12                                      2269670         2271031         +0.06%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1000-12                                                                  2092111         2093345         +0.06%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-12                   357193          357403          +0.06%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=10-12                                         399462          399695          +0.06%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=10-12                                 214016          214140          +0.06%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=100-12                                            163096          163190          +0.06%\r\nBenchmarkRangeQuery\/expr=a_ten_or_b_ten{l=~'.*[0-4]$'},steps=1000-12                                                   581585          581918          +0.06%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=10-12                                                                    132926          133000          +0.06%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1-12                                  198122          198231          +0.06%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=10-12                                                                134359          134431          +0.05%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1-12                                                                     128824          128889          +0.05%\r\nBenchmarkRangeQuery\/expr=label_replace(a_ten,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                                   338563          338733          +0.05%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1-12                                              137462          137531          +0.05%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=10-12                                                      127251          127313          +0.05%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_hundred[1m]))_\/_sum_without_(l)(rate(b_hundred[1m])),steps=100-12      468174          468401          +0.05%\r\nBenchmarkRangeQuery\/expr=label_join(a_one,_'l2',_'-',_'l',_'l'),steps=1000-12                                          197403          197497          +0.05%\r\nBenchmarkRangeQuery\/expr=a_ten_-_b_ten,steps=10000-12                                                                  3508918         3507267         -0.05%\r\nBenchmarkRangeQuery\/expr=changes(a_ten[1d]),steps=10-12                                                                1016977         1016509         -0.05%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=10-12                                             137894          137957          +0.05%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=100-12                                         151777          151846          +0.05%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_hundred),steps=100-12                                                               200610          200701          +0.05%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1-12                                                                    589822          590089          +0.05%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_ten),steps=1-12                                                       320100          319956          -0.04%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=10-12                                        207588          207680          +0.04%\r\nBenchmarkRangeQuery\/expr=label_join(a_ten,_'l2',_'-',_'l',_'l'),steps=1000-12                                          399365          399538          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=100-12                                                       2026897         2026025         -0.04%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=100-12                                         1745527         1746275         +0.04%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1d]),steps=1-12                                                                    1019831         1020266         +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1-12                                                                  157412          157479          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1-12                                                                 157943          158010          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=10-12                                                            1489616         1488987         -0.04%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1d]),steps=10-12                                                               5340733         5338498         -0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1-12                                                              1518882         1518249         -0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=10-12                                                            188884          188962          +0.04%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10-12                                                               134384          134439          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=10-12                                                           186255          186331          +0.04%\r\nBenchmarkRangeQuery\/expr=a_one,steps=10-12                                                                             4971            4973            +0.04%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=100-12                                           567082          567303          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=10-12                                                        1492417         1491838         -0.04%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=1000-12                               2213793         2212955         -0.04%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1-12                                                              316006          316125          +0.04%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1000-12                                             1320340         1320837         +0.04%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1-12                                                   3101803         3100658         -0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1-12                                                            157896          157954          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=10-12                                                                 183608          183675          +0.04%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=10-12                                            247262          247350          +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_ten[1m]))_\/_sum_without_(l)(rate(b_ten[1m])),steps=1000-12             1033304         1033666         +0.04%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m]))_\/_sum_without_(l)(rate(b_one[1m])),steps=1000-12             906596          906911          +0.03%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=1000-12                                                           3431113         3429925         -0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=100-12                                                           327732          327844          +0.03%\r\nBenchmarkRangeQuery\/expr=a_one_+_on(l)_group_right_a_one,steps=1000-12                                                 195140          195206          +0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1-12                                                             158478          158528          +0.03%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=10-12                                               169544          169595          +0.03%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1000-12                                                                          43738           43725           -0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1-12                                                         1461793         1461381         -0.03%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_one),steps=1000-12                                                                  483346          483477          +0.03%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=10-12                                          365429          365331          -0.03%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                          3801932         3800921         -0.03%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_one),steps=100-12                                                     998725          998461          -0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(rate(a_one[1m])),steps=1000-12                                                428387          428276          -0.03%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_one[1d]),steps=1000-12                                                     611201          611044          -0.03%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=100-12                                                      4809091         4807878         -0.03%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=100-12                                     17401209        17397018        -0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=10-12                                                                185996          186039          +0.02%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1000-12                                                                           8687            8689            +0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_one),steps=1000-12                                                               504302          504416          +0.02%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=1-12                                                                     174676          174715          +0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=100-12                                                               301097          301161          +0.02%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1000-12                                                             322807          322875          +0.02%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=10-12                                                  12855186        12857867        +0.02%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=100-12                                                                   194888          194926          +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_-_b_hundred,steps=10-12                                                             339506          339571          +0.02%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=100-12                                                          492798          492889          +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l=~'.*[0-4]$'},steps=1-12                                             218497          218537          +0.02%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=10-12                                                                            5651            5650            -0.02%\r\nBenchmarkRangeQuery\/expr=rate(a_ten[1m])_+_rate(b_ten[1m]),steps=100-12                                                74445           74458           +0.02%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1-12                                       2203308         2202931         -0.02%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=100-12                                                                           5939            5938            -0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1000-12                                                          7335762         7336979         +0.02%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=10-12                                                 1651188         1650918         -0.02%\r\nBenchmarkRangeQuery\/expr=sum(a_ten),steps=1000-12                                                                      500535          500612          +0.02%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=100-12                                            946889          947034          +0.02%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=10-12                                                             1774235         1774488         +0.01%\r\nBenchmarkRangeQuery\/expr=a_hundred_+_on(l)_group_right_a_one,steps=1-12                                                161441          161464          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_one),steps=1000-12                                                         528353          528427          +0.01%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=10-12                                                            5348986         5349707         +0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=100-12                                                               489355          489420          +0.01%\r\nBenchmarkRangeQuery\/expr=sum(a_hundred),steps=1000-12                                                                  836306          836195          -0.01%\r\nBenchmarkRangeQuery\/expr=topk(1,_a_ten),steps=1000-12                                                                  549512          549584          +0.01%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=100-12                                                                   360686          360639          -0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=1000-12                                                          3811995         3812487         +0.01%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=100-12                                                                            15710           15712           +0.01%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1000-12                                                                           39891           39896           +0.01%\r\nBenchmarkRangeQuery\/expr=changes(a_one[1d]),steps=1-12                                                                 588590          588662          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_hundred),steps=1000-12                                                      7601010         7600106         -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=100-12                                                                468607          468662          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_ten),steps=100-12                                                           516003          516056          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=10-12                                                       1800275         1800455         +0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1-12                                                  1648801         1648963         +0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_one[1d]),steps=1000-12                                                                 600116          600168          +0.01%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1000-12                                                                      384440          384473          +0.01%\r\nBenchmarkRangeQuery\/expr=sum(a_one),steps=1000-12                                                                      434453          434490          +0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=100-12                                                            4563898         4563511         -0.01%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=10-12                                                                        132710          132699          -0.01%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=1-12                                                                         132701          132712          +0.01%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=10-12                                                                         97877           97869           -0.01%\r\nBenchmarkRangeQuery\/expr=-a_hundred,steps=100-12                                                                       153597          153585          -0.01%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_hundred),steps=1000-12                                                     35077115        35074388        -0.01%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_hundred[1d]),steps=100-12                                                  5583342         5582928         -0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_ten[1d],_0.3,_0.3),steps=1000-12                                               1726300         1726428         +0.01%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=1-12                                              230151          230135          -0.01%\r\nBenchmarkRangeQuery\/expr=label_join(a_hundred,_'l2',_'-',_'l',_'l'),steps=100-12                                       393061          393035          -0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=10-12                                                 1220908         1220988         +0.01%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_one[5m])),steps=1000-12                                        1274282         1274202         -0.01%\r\nBenchmarkRangeQuery\/expr=holt_winters(a_one[1d],_0.3,_0.3),steps=100-12                                                1222632         1222558         -0.01%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_ten),steps=1000-12                                                               3343384         3343184         -0.01%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=10-12                                                                            17377           17376           -0.01%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=1-12                                                                             17378           17377           -0.01%\r\nBenchmarkRangeQuery\/expr=abs(a_ten),steps=1000-12                                                                      235119          235131          +0.01%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=100-12                                                                        118769          118763          -0.01%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m])_+_rate(b_hundred[1m]),steps=1-12                                          376079          376097          +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=1000-12                                       3799872         3799706         -0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred_unless_b_hundred{l=~'.*[0-4]$'},steps=10-12                                         247255          247245          -0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_hundred),steps=1-12                                                             1461770         1461711         -0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_hundred[5m])),steps=1000-12                                    155588532       155582328       -0.00%\r\nBenchmarkRangeQuery\/expr=sum_without_(le)(h_ten),steps=1000-12                                                         3583637         3583775         +0.00%\r\nBenchmarkRangeQuery\/expr=label_replace(a_hundred,_'l2',_'$1',_'l',_'(.*)'),steps=100-12                                393660          393645          -0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_one),steps=1000-12                                                              2814090         2814176         +0.00%\r\nBenchmarkRangeQuery\/expr=a_one_-_b_one,steps=10000-12                                                                  654053          654071          +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred_or_b_hundred{l=~'.*[0-4]$'},steps=10-12                                             292389          292381          -0.00%\r\nBenchmarkRangeQuery\/expr=histogram_quantile(0.9,_rate(h_ten[5m])),steps=1000-12                                        15569001        15568584        -0.00%\r\nBenchmarkRangeQuery\/expr=sum_without_(l)(h_one),steps=1000-12                                                          3078696         3078614         -0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(le)(h_ten),steps=1000-12                                                              3548019         3548111         +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred,steps=1000-12                                                                       349552          349559          +0.00%\r\nBenchmarkRangeQuery\/expr=absent_over_time(a_ten[1d]),steps=10-12                                                       1021308         1021291         -0.00%\r\nBenchmarkRangeQuery\/expr=abs(a_hundred),steps=10-12                                                                    189702          189705          +0.00%\r\nBenchmarkRangeQuery\/expr=changes(a_hundred[1d]),steps=1-12                                                             5332761         5332843         +0.00%\r\nBenchmarkRangeQuery\/expr=a_hundred_and_b_hundred{l='notfound'},steps=1000-12                                           437197          437202          +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=1-12                                                                134381          134382          +0.00%\r\nBenchmarkRangeQuery\/expr=sum_by_(l)(h_hundred),steps=1000-12                                                           32670068        32670281        +0.00%\r\nBenchmarkRangeQuery\/expr=count_values('value',_h_hundred),steps=1000-12                                                17449497048     17449553472     +0.00%\r\nBenchmarkRangeQuery\/expr=rate(a_hundred[1m]),steps=10000-12                                                            5972985         5973003         +0.00%\r\nBenchmarkRangeQuery\/expr=-a_one,steps=1-12                                                                             5650            5650            +0.00%\r\nBenchmarkRangeQuery\/expr=-a_ten,steps=100-12                                                                           19539           19539           +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=1-12                                                                              4975            4975            +0.00%\r\nBenchmarkRangeQuery\/expr=a_one,steps=100-12                                                                            5260            5260            +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=1-12                                                                              13549           13549           +0.00%\r\nBenchmarkRangeQuery\/expr=a_ten,steps=10-12                                                                             13549           13549           +0.00%\r\n```\r\n<\/details>","@codesome (and maybe @bboreham ), we have run into this during the bug scrub, and it seems to us this is kind-of good to go. Would you agree? Of course, by now, it needs a rebase. @darshanime would you be able to do that?","If I understand it correctly, this kind of optimization would be very helpful for Home Assistant users, since the current method for filtering out unavailable values is very expensive and needs a recorded rule to be efficient.\r\n\r\nhttps:\/\/www.home-assistant.io\/integrations\/prometheus\/#metrics-in-unavailable-or-unknown-states","Discussed at the bug scrub.  I will take a closer look.","rebased. this patch adds some optimizations for the `Vector{And, Or, Unless}` functions.\r\n\r\nrelevant benchmarks\r\n\r\n```\r\ngo test -bench=BenchmarkRangeQuery -run=- -count 5 | tee main\r\ngo test -bench=BenchmarkRangeQuery -run=- -count 5 | tee optimize_joins\r\nbenchstat main optimize_joins\r\n```\r\n\r\n<details>\r\n  <summary>see benchmark<\/summary>\r\n\r\n```\r\n$ benchstat main optimize_joins\r\ngoos: darwin\r\ngoarch: arm64\r\npkg: github.com\/prometheus\/prometheus\/promql\r\n                                                                                             \u2502     main      \u2502           optimize_joins            \u2502\r\n                                                                                             \u2502    sec\/op     \u2502    sec\/op     vs base               \u2502\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-10               21.16\u00b5 \u00b1 \u221e \u00b9   21.87\u00b5 \u00b1 \u221e \u00b9   +3.35% (p=0.016 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-10             44.69\u00b5 \u00b1 \u221e \u00b9   45.12\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.222 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-10            226.3\u00b5 \u00b1 \u221e \u00b9   219.6\u00b5 \u00b1 \u221e \u00b9   -2.98% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-10               60.72\u00b5 \u00b1 \u221e \u00b9   62.41\u00b5 \u00b1 \u221e \u00b9   +2.77% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-10             169.1\u00b5 \u00b1 \u221e \u00b9   172.6\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.056 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-10            1.007m \u00b1 \u221e \u00b9   1.004m \u00b1 \u221e \u00b9        ~ (p=0.690 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-10       442.7\u00b5 \u00b1 \u221e \u00b9   448.0\u00b5 \u00b1 \u221e \u00b9   +1.21% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-10     1.376m \u00b1 \u221e \u00b9   1.405m \u00b1 \u221e \u00b9   +2.08% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-10    8.398m \u00b1 \u221e \u00b9   8.735m \u00b1 \u221e \u00b9   +4.02% (p=0.032 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-10                         17.83\u00b5 \u00b1 \u221e \u00b9   17.93\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.095 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-10                       40.92\u00b5 \u00b1 \u221e \u00b9   40.57\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.397 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-10                      215.4\u00b5 \u00b1 \u221e \u00b9   213.8\u00b5 \u00b1 \u221e \u00b9   -0.74% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-10                         58.80\u00b5 \u00b1 \u221e \u00b9   60.49\u00b5 \u00b1 \u221e \u00b9   +2.88% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-10                       254.5\u00b5 \u00b1 \u221e \u00b9   225.8\u00b5 \u00b1 \u221e \u00b9  -11.28% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-10                      1.888m \u00b1 \u221e \u00b9   1.616m \u00b1 \u221e \u00b9  -14.39% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-10                 477.3\u00b5 \u00b1 \u221e \u00b9   497.7\u00b5 \u00b1 \u221e \u00b9   +4.27% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-10               2.893m \u00b1 \u221e \u00b9   2.113m \u00b1 \u221e \u00b9  -26.94% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-10              23.75m \u00b1 \u221e \u00b9   15.66m \u00b1 \u221e \u00b9  -34.07% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-10                     17.80\u00b5 \u00b1 \u221e \u00b9   17.72\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.841 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-10                   36.31\u00b5 \u00b1 \u221e \u00b9   36.38\u00b5 \u00b1 \u221e \u00b9        ~ (p=1.000 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-10                  174.1\u00b5 \u00b1 \u221e \u00b9   176.0\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.151 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-10                     57.02\u00b5 \u00b1 \u221e \u00b9   57.70\u00b5 \u00b1 \u221e \u00b9        ~ (p=0.056 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-10                   199.2\u00b5 \u00b1 \u221e \u00b9   169.3\u00b5 \u00b1 \u221e \u00b9  -15.05% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-10                  1.346m \u00b1 \u221e \u00b9   1.030m \u00b1 \u221e \u00b9  -23.44% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-10             453.3\u00b5 \u00b1 \u221e \u00b9   453.3\u00b5 \u00b1 \u221e \u00b9        ~ (p=1.000 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-10           2.261m \u00b1 \u221e \u00b9   1.509m \u00b1 \u221e \u00b9  -33.27% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-10         17.333m \u00b1 \u221e \u00b9   9.947m \u00b1 \u221e \u00b9  -42.61% (p=0.008 n=5)\r\ngeomean                                                                                         321.7\u00b5         296.7\u00b5         -7.78%\r\n\u00b9 need >= 6 samples for confidence interval at level 0.95\r\n\r\n                                                                                             \u2502      main       \u2502            optimize_joins            \u2502\r\n                                                                                             \u2502      B\/op       \u2502     B\/op       vs base               \u2502\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-10                19.48Ki \u00b1 \u221e \u00b9   19.93Ki \u00b1 \u221e \u00b9   +2.33% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-10              28.86Ki \u00b1 \u221e \u00b9   29.32Ki \u00b1 \u221e \u00b9   +1.57% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-10             122.4Ki \u00b1 \u221e \u00b9   122.9Ki \u00b1 \u221e \u00b9   +0.38% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-10                39.41Ki \u00b1 \u221e \u00b9   39.86Ki \u00b1 \u221e \u00b9   +1.15% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-10              53.71Ki \u00b1 \u221e \u00b9   54.17Ki \u00b1 \u221e \u00b9   +0.85% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-10             169.8Ki \u00b1 \u221e \u00b9   170.2Ki \u00b1 \u221e \u00b9   +0.26% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-10        256.3Ki \u00b1 \u221e \u00b9   256.8Ki \u00b1 \u221e \u00b9   +0.18% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-10      318.2Ki \u00b1 \u221e \u00b9   318.7Ki \u00b1 \u221e \u00b9   +0.16% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-10     654.1Ki \u00b1 \u221e \u00b9   654.5Ki \u00b1 \u221e \u00b9   +0.06% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-10                          16.16Ki \u00b1 \u221e \u00b9   16.51Ki \u00b1 \u221e \u00b9   +2.13% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-10                        21.61Ki \u00b1 \u221e \u00b9   21.95Ki \u00b1 \u221e \u00b9   +1.60% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-10                       69.85Ki \u00b1 \u221e \u00b9   70.20Ki \u00b1 \u221e \u00b9   +0.51% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-10                          35.32Ki \u00b1 \u221e \u00b9   35.38Ki \u00b1 \u221e \u00b9   +0.16% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-10                        73.86Ki \u00b1 \u221e \u00b9   45.75Ki \u00b1 \u221e \u00b9  -38.06% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-10                       400.7Ki \u00b1 \u221e \u00b9   116.6Ki \u00b1 \u221e \u00b9  -70.91% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-10                  251.9Ki \u00b1 \u221e \u00b9   242.1Ki \u00b1 \u221e \u00b9   -3.93% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-10               1321.0Ki \u00b1 \u221e \u00b9   299.9Ki \u00b1 \u221e \u00b9  -77.30% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-10             10797.5Ki \u00b1 \u221e \u00b9   590.9Ki \u00b1 \u221e \u00b9  -94.53% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-10                      16.10Ki \u00b1 \u221e \u00b9   16.44Ki \u00b1 \u221e \u00b9   +2.14% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-10                    21.55Ki \u00b1 \u221e \u00b9   21.89Ki \u00b1 \u221e \u00b9   +1.60% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-10                   69.99Ki \u00b1 \u221e \u00b9   70.34Ki \u00b1 \u221e \u00b9   +0.50% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-10                      34.57Ki \u00b1 \u221e \u00b9   34.63Ki \u00b1 \u221e \u00b9   +0.17% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-10                    73.13Ki \u00b1 \u221e \u00b9   45.01Ki \u00b1 \u221e \u00b9  -38.46% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-10                   400.3Ki \u00b1 \u221e \u00b9   115.9Ki \u00b1 \u221e \u00b9  -71.05% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-10              243.7Ki \u00b1 \u221e \u00b9   233.8Ki \u00b1 \u221e \u00b9   -4.05% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-10           1312.0Ki \u00b1 \u221e \u00b9   291.7Ki \u00b1 \u221e \u00b9  -77.77% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-10         10791.8Ki \u00b1 \u221e \u00b9   582.4Ki \u00b1 \u221e \u00b9  -94.60% (p=0.008 n=5)\r\ngeomean                                                                                          140.8Ki         89.68Ki        -36.30%\r\n\u00b9 need >= 6 samples for confidence interval at level 0.95\r\n\r\n                                                                                             \u2502     main      \u2502           optimize_joins            \u2502\r\n                                                                                             \u2502   allocs\/op   \u2502  allocs\/op    vs base               \u2502\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1-10                355.0 \u00b1 \u221e \u00b9    361.0 \u00b1 \u221e \u00b9   +1.69% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=100-10              570.0 \u00b1 \u221e \u00b9    576.0 \u00b1 \u221e \u00b9   +1.05% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_and_topk(1,_rate(a_one[1m]_@_start())),steps=1000-10            2.416k \u00b1 \u221e \u00b9   2.422k \u00b1 \u221e \u00b9   +0.25% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1-10                585.0 \u00b1 \u221e \u00b9    591.0 \u00b1 \u221e \u00b9   +1.03% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=100-10              881.0 \u00b1 \u221e \u00b9    887.0 \u00b1 \u221e \u00b9   +0.68% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_and_topk(1,_rate(a_ten[1m]_@_start())),steps=1000-10            3.051k \u00b1 \u221e \u00b9   3.057k \u00b1 \u221e \u00b9   +0.20% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1-10       2.756k \u00b1 \u221e \u00b9   2.762k \u00b1 \u221e \u00b9   +0.22% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=100-10     3.838k \u00b1 \u221e \u00b9   3.844k \u00b1 \u221e \u00b9   +0.16% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_and_topk(1,_rate(a_hundred[1m]_@_start())),steps=1000-10    9.164k \u00b1 \u221e \u00b9   9.170k \u00b1 \u221e \u00b9   +0.07% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1-10                          294.0 \u00b1 \u221e \u00b9    298.0 \u00b1 \u221e \u00b9   +1.36% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=100-10                        503.0 \u00b1 \u221e \u00b9    507.0 \u00b1 \u221e \u00b9   +0.80% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m]_@_start())_or_rate(a_one[1m]),steps=1000-10                      2.345k \u00b1 \u221e \u00b9   2.349k \u00b1 \u221e \u00b9   +0.17% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1-10                          525.0 \u00b1 \u221e \u00b9    528.0 \u00b1 \u221e \u00b9   +0.57% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=100-10                        916.0 \u00b1 \u221e \u00b9    818.0 \u00b1 \u221e \u00b9  -10.70% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m]_@_start())_or_rate(a_ten[1m]),steps=1000-10                      4.002k \u00b1 \u221e \u00b9   2.984k \u00b1 \u221e \u00b9  -25.44% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1-10                 2.710k \u00b1 \u221e \u00b9   2.705k \u00b1 \u221e \u00b9   -0.18% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=100-10               4.610k \u00b1 \u221e \u00b9   3.781k \u00b1 \u221e \u00b9  -17.98% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m]_@_start())_or_rate(a_hundred[1m]),steps=1000-10             17.421k \u00b1 \u221e \u00b9   9.104k \u00b1 \u221e \u00b9  -47.74% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1-10                      293.0 \u00b1 \u221e \u00b9    297.0 \u00b1 \u221e \u00b9   +1.37% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=100-10                    502.0 \u00b1 \u221e \u00b9    506.0 \u00b1 \u221e \u00b9   +0.80% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_one[1m])_unless_rate(a_one[1m]_@_start()),steps=1000-10                  2.344k \u00b1 \u221e \u00b9   2.348k \u00b1 \u221e \u00b9   +0.17% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1-10                      524.0 \u00b1 \u221e \u00b9    527.0 \u00b1 \u221e \u00b9   +0.57% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=100-10                    915.0 \u00b1 \u221e \u00b9    817.0 \u00b1 \u221e \u00b9  -10.71% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_ten[1m])_unless_rate(a_ten[1m]_@_start()),steps=1000-10                  4.001k \u00b1 \u221e \u00b9   2.983k \u00b1 \u221e \u00b9  -25.44% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1-10             2.708k \u00b1 \u221e \u00b9   2.704k \u00b1 \u221e \u00b9   -0.15% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=100-10           4.610k \u00b1 \u221e \u00b9   3.780k \u00b1 \u221e \u00b9  -18.00% (p=0.008 n=5)\r\nRangeQuery\/expr=rate(a_hundred[1m])_unless_rate(a_hundred[1m]_@_start()),steps=1000-10         17.426k \u00b1 \u221e \u00b9   9.103k \u00b1 \u221e \u00b9  -47.76% (p=0.008 n=5)\r\ngeomean                                                                                         1.696k         1.551k         -8.51%\r\n\u00b9 need >= 6 samples for confidence interval at level 0.95\r\n```\r\n<\/details>"],"labels":["stale"]},{"title":"Flaky windows test","body":"A flaky test was introduced recently. Here is the output on windows.\r\n\r\n- #9459 \r\n\r\n```\r\n=== RUN   TestEvaluations\/testdata\\aggregators.test                                                                                        \r\nvector result 0 count(topk(1,max(up) without()) == topk(1,max(up) without()) == topk(1,max(up) without()) == topk(1,max(up) without()) == t\r\nopk(1,max(up) without()))                                            \r\n    promql_test.go:31:                                               \r\n                Error Trace:    promql_test.go:31\r\n                Error:          Received unexpected error:\r\n                                expected metric {} with 1: [1.000000] not found                                                            \r\n                                github.com\/prometheus\/prometheus\/promql.(*evalCmd).compareResult\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/test.go:409\r\n                                github.com\/prometheus\/prometheus\/promql.(*Test).exec\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/test.go:584\r\n                                github.com\/prometheus\/prometheus\/promql.(*Test).Run\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/test.go:437\r\n                                github.com\/prometheus\/prometheus\/promql.TestEvaluations.func1\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/promql_test.go:31\r\n                                testing.tRunner\r\n                                        C:\/Program Files\/Go\/src\/testing\/testing.go:1259\r\n                                runtime.goexit\r\n                                        C:\/Program Files\/Go\/src\/runtime\/asm_amd64.s:1581\r\n                                error in eval count(topk(1,max(up) without()) == topk(1,max(up) without()) == topk(1,max(up) without()) == \r\ntopk(1,max(up) without()) == topk(1,max(up) without())) (line 508) range mode\r\n                                github.com\/prometheus\/prometheus\/promql.(*Test).exec\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/test.go:587\r\n                                github.com\/prometheus\/prometheus\/promql.(*Test).Run\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/test.go:437\r\n                                github.com\/prometheus\/prometheus\/promql.TestEvaluations.func1\r\n                                        C:\/go\/src\/github.com\/prometheus\/prometheus\/promql\/promql_test.go:31\r\n                                testing.tRunner\r\n                                        C:\/Program Files\/Go\/src\/testing\/testing.go:1259\r\n                                runtime.goexit\r\n                                        C:\/Program Files\/Go\/src\/runtime\/asm_amd64.s:1581\r\n                Test:           TestEvaluations\/testdata\\aggregators.test\r\n```","comments":["It is flaky on Linux too..","Also, strange that it seems only \"range mode\" is failing.","It seems that aggregations are only deterministic in instant mode, ideally it should also be deterministic in range mode. \r\n\r\nSome investigation would be needed to see what would be the implications in the PromQL engine to make this happen."],"labels":["kind\/bug","priority\/Pmaybe","component\/promql"]},{"title":"dynamic  labels support","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nwe want to use the same metric, but use different tag in some case, for user or developer, because we have a lot of metric in our application, we cannot list them , after all,  the metric is still growing in the feature and now.\r\n\r\n\r\n*\u201cNice to have\u201d is not a good use case. :)*\r\n","comments":["I'm a bit confused by this request. What do you mean by \"tags\"? Labels?","> I'm a bit confused by this request. What do you mean by \"tags\"? Labels?\r\n\r\n@LeviHarrison  yes\uff0cit is Labels","@LeviHarrison \uff0cI found that all labels must be declared and then used it in prometheus sdk API\uff0c that's very very terrible\uff0ccan you change it?","Could you please clarify your need, your programming language, and whether you are looking for a solution in the Prometheus server (that would be relabeling) or in client libraries (that would be declaring variables with labeled gauges\/counters I think). In this case, can you also share some code that you find \"very very terrible\"? Thank you!"],"labels":["kind\/more-info-needed"]},{"title":"Kubernetes SD: Optionally attach namespace and node metadata to targets","body":"## Proposal\r\nKubernetes SD should be extended so the service, pod, endpoints, and ingress roles can optionally attach extra meta labels based on the namespace and node objects that the targets are discovered in (where appropriate; ingress wouldn't support node meta labels, etc.).\r\n\r\nThis could be used within Prometheus, but also within Grafana Agent (for traces) and Promtail (for logs) to use this new meta information in similar ways.\r\n\r\n**Use case. Why is this important?**\r\nThere are a few use cases for where this would be useful. One such use case has been documented in grafana\/agent#980. \r\n\r\nAnother use case is for filtering targets to scrape to those that exist within specific namespaces. In this scenario, managed namespaces are dynamically created on behalf of users where they have full control over the resources deployed in the environment. The namespace, which users do not have permissions to modify, would have labels or annotations that determine whether scraping is available. This makes checking labels\/annotations on the pods inappropriate as users have full control over their contents. \r\n","comments":["The obvious workaround in my second use case is to design some kind of admission controller which will force a specific set of labels\/annotations on pods based on the namespace. I think it's a reasonable workaround, but I believe that more meta labels based on the location of an object can be used in enough interesting ways to make this feature justifiable. ","An example of the labels exposed: `__meta_kubernetes_namespace_<labelname>`, `__meta_kubernetes_pod_node_<labelname>`, etc.","> An example of the labels exposed: __meta_kubernetes_namespace_<labelname>, __meta_kubernetes_pod_node_<labelname>, etc.\r\n\r\nThanks, good idea to include examples. There's probably a lot of ways this could be configured but I imagine we would add something similar to the following for kubernetes_sd_config:\r\n\r\n```yaml\r\nattach_metadata:\r\n  # Attaches node metadata to discovered targets. Only valid for role: pod, endpoints. \r\n  # When set to true, Prometheus must have permissions to get Nodes. \r\n  [ node: <boolean> | default = false ]\r\n  # Attaches namespace metadata to discovered targets. Invalid for role: node.\r\n  # When set to true, Prometheus must have permissions to get Namespaces.\r\n  [ namespace: <boolean> | default = false ]\r\n```","I think this makes sense. The only thing that might scare me that we should think through is: can we potentially expose information to users to exploit? It has happened in the past in kube-state-metrics for example that we accidentally exposed all annotations by default of secrets, which leaked secret content of the kubectl last-applied into an annotation. Just want to make sure we think this through, otherwise I can think of lots of useful use cases for this.","Security considerations are a great point. Off the top of my head, I don't know if there's any sensitive information that lives inside the metadata for nodes and namespaces, but I agree with being cautious before accepting the proposal. \r\n\r\nGenerally speaking, what is Prometheus' approach to security concerns when adding new features? Does the potential to exploit something (even if that something is disabled by default) generally lead to not adding the new functionality? ","We already have node labels because we have a node role\r\n\r\nAvailable meta labels:\r\n\r\n`__meta_kubernetes_node_name`: The name of the node object.\r\n`__meta_kubernetes_node_label_<labelname>`: Each label from the node object.\r\n`__meta_kubernetes_node_labelpresent_<labelname>`: true for each label from the node object.\r\n`__meta_kubernetes_node_annotation_<annotationname>`: Each annotation from the node object.\r\n`__meta_kubernetes_node_annotationpresent_<annotationname>`: true for each annotation from the node object.\r\n`__meta_kubernetes_node_address_<address_type>`: The first address for each node address type, if it exists.\r\n","> Generally speaking, what is Prometheus' approach to security concerns when adding new features? Does the potential to exploit something (even if that something is disabled by default) generally lead to not adding the new functionality?\r\n\r\nI have made a PuppetDB service discovery and I have added:\r\n```\r\n# Whether to include the parameters as meta labels.\r\n# Due to the differences between parameter types and Prometheus labels,\r\n# some parameters might not be rendered. The format of the parameters might\r\n# also change in future releases.\r\n#\r\n# Note: Enabling this exposes parameters in the Prometheus UI and API. Make sure\r\n# that you don't have secrets exposed as parameters if you enable this.\r\n[ include_parameters: <boolean> | default = false ]\r\n```",">Security considerations are a great point. Off the top of my head, I don't know if there's any sensitive information that lives inside the metadata for nodes and namespaces, but I agree with being cautious before accepting the proposal.\r\n\r\nJust looking at my nodes (checked one in each of GKE, EKS, and AKS), and the labels and annotations are all non-sensitive. The closest thing I could construe as sensitive is network metadata (CIDR ranges), internal names (resource names can include things like company and customer references), but all things that live within the standard realm of metadata you would associate with environment telemetry for the use in queries. I'm interested in what happened with kube-state-metrics here, it seems like labels and annotations are strongly considered by kubernetes to be non-sensitive identifying information. \r\n\r\nFor a bit of context on my use case (from the grafana-agent issue), we have clusters split up into pools, and we're interested in running queries on resource utilization metrics grouped by pool (currently not possible for any metrics gathered by something like kubernetes_sd pod, svc, or endpoint role). I think the proposal [here](https:\/\/github.com\/prometheus\/prometheus\/issues\/9510#issuecomment-943410228) makes a lot of sense as a generic approach so users can quickly enrich their targets and handle the relabeling to get what they actually want as normal. It would definitely fulfill the needs of my use case.","> We already have node labels because we have a node role\r\n>\r\n> Available meta labels:\r\n>\r\n> `__meta_kubernetes_node_name`: The name of the node object.\r\n> `__meta_kubernetes_node_label_<labelname>`: Each label from the node object.\r\n> `__meta_kubernetes_node_labelpresent_<labelname>`: true for each label from the node object.\r\n> `__meta_kubernetes_node_annotation_<annotationname>`: Each annotation from the node object.\r\n> `__meta_kubernetes_node_annotationpresent_<annotationname>`: true for each annotation from the node object.\r\n> `__meta_kubernetes_node_address_<address_type>`: The first address for each node address type, if it exists.\r\n\r\nI imagine we'd want something similar for namespaces, though there's a small wart here:\r\n\r\n`__meta_kubernetes_namespace_name`: The name of the Namespace object.\r\n`__meta_kubernetes_namespace_label_<labelname>`: Each label from the node object.\r\n`__meta_kubernetes_namespace_labelpresent_<labelname>`: true for each label from the node object. \r\n`__meta_kubernetes_namespace_annotation_<annotationname>`: Each annotation from the node object. \r\n`__meta_kubernetes_namespace_annotationpresent_<annotationname>`: true for each annotation from the node object.\r\n\r\n`__meta_kubernetes_namespace_name` might feel a little awkward since `__meta_kubernetes_namespace` already exists for many targets.","> We already have node labels because we have a node role\r\n> \r\n> Available meta labels:\r\n> \r\n> `__meta_kubernetes_node_name`: The name of the node object. `__meta_kubernetes_node_label_<labelname>`: Each label from the node object. `__meta_kubernetes_node_labelpresent_<labelname>`: true for each label from the node object. `__meta_kubernetes_node_annotation_<annotationname>`: Each annotation from the node object. `__meta_kubernetes_node_annotationpresent_<annotationname>`: true for each annotation from the node object. `__meta_kubernetes_node_address_<address_type>`: The first address for each node address type, if it exists.\r\n\r\nI honestly think those are nice for a cluster admin, but don't facilitate k8s-hosted service developer as his pods and services are usually discovered with endpoint discovery. Another use-case we have:\r\nSay, you have a cross-az k8s cluster with some pods of SUPER_APP running across some of the nodes. When SUPER_APP suddenly gets a surge of erroneous responses, person responsible for it should have a simple way to find out if it's a global issue, or it's local to some AZ \/ network pod \/ particular rack \/ particular node \/ etc. Since k8s the app in k8s pod doesn't have a sane way of knowing where it runs (and why should it know?) it's unable to report any of those things. That info is contained in node labels, and there is no way to propagate them to pod: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/40610\r\nTherefore, I think prometheus should be able to add those labels to discovered endpoint, enabling the user to simply query for `rate(errors_total{zone=\"us-east-1\", rack=\"123\"}[1m])`.\r\nThat is not limited to topology, of course, lots of useful things find its way to node labels: hw configurations, some sysctl settings, soft maintenance flags, etc","> > We already have node labels because we have a node role\r\n> > Available meta labels:\r\n> > `__meta_kubernetes_node_name`: The name of the node object.\r\n> > `__meta_kubernetes_node_label_<labelname>`: Each label from the node object.\r\n> > `__meta_kubernetes_node_labelpresent_<labelname>`: true for each label from the node object.\r\n> > `__meta_kubernetes_node_annotation_<annotationname>`: Each annotation from the node object.\r\n> > `__meta_kubernetes_node_annotationpresent_<annotationname>`: true for each annotation from the node object.\r\n> > `__meta_kubernetes_node_address_<address_type>`: The first address for each node address type, if it exists.\r\n> \r\n> I imagine we'd want something similar for namespaces, though there's a small wart here:\r\n> \r\n> `__meta_kubernetes_namespace_name`: The name of the Namespace object. `__meta_kubernetes_namespace_label_<labelname>`: Each label from the node object. `__meta_kubernetes_namespace_labelpresent_<labelname>`: true for each label from the node object. `__meta_kubernetes_namespace_annotation_<annotationname>`: Each annotation from the node object. `__meta_kubernetes_namespace_annotationpresent_<annotationname>`: true for each annotation from the node object.\r\n> \r\n> `__meta_kubernetes_namespace_name` might feel a little awkward since `__meta_kubernetes_namespace` already exists for many targets.\r\n\r\nFor me this would be a great help to have this capability for namespaces.","It looks like PR #10080 only applies to targets discovered through the `pod` role. Is there any plan to expand this feature to the `endpoints` role? For example, we'd like to have the node metadata attached to the node level metrics such as `node_memory_MemAvailable_bytes`."],"labels":["kind\/enhancement","component\/service discovery","priority\/P3"]},{"title":"Data Retention not working as was expected","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nRun prometheus server in docker,  and add command line --storage.tsdb.retention.time = 180d \r\n\r\ncmd=\"sudo docker run  --restart=always  -u 0 -d -p ${svrport}:9090 --name ${podname} \\\r\n           -v $(pwd)\/${DEPLOY}-prometheus-fed.yml:\/etc\/prometheus\/prometheus.yml \\\r\n           -v $(pwd)\/${DEPLOY}-rules.yml:\/etc\/prometheus\/rules.yml \\\r\n           -v $(pwd)\/web-config.yml:\/etc\/prometheus\/web-config.yml \\\r\n           -v \/data\/prometheus-fed:\/prometheus\/data \\\r\n           -v $(pwd)\/pswdfile:\/etc\/prometheus\/.pswdfile \\\r\n           \"${image}\" \\\r\n           --config.file=\/etc\/prometheus\/prometheus.yml \\\r\n           --web.config.file=\/etc\/prometheus\/web-config.yml \\\r\n           --web.enable-lifecycle \\\r\n           --web.external-url=http:\/\/${hostip}:${svrport}\/prometheus \\\r\n           --web.route-prefix=\"\/\" \\\r\n           --storage.tsdb.retention.time=180d \\\r\n           \"\r\n\r\n**What did you expect to see?**\r\n\r\n180d long of data reserved on disk\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nonly about 60d of data existed, either searching from web or listing in file system.\r\n\r\n**Environment**\r\ncentos 7.6\r\ndocker 2.\r\nprometheus 2.29.1, 2.30.0\r\n\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\nLinux 3.10.0-957.el7.x86_64 x86_64\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n2.29.1\r\n2.30.0\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\nv0.22.2\r\n\r\n* Prometheus configuration file:\r\n```\r\n\r\nglobal:\r\n  scrape_interval: 20s \r\n  scrape_timeout: 15s\r\n  evaluation_interval: 20s \r\n\r\nalerting:\r\n  alertmanagers:\r\n  - static_configs:\r\n    - targets: \r\n      - 'alert.xxxx.com'\r\n\r\nrule_files:\r\n  - 'rules.yml'\r\n\r\nscrape_configs:\r\n  - job_name: 'prometheus'\r\n    static_configs:\r\n    - targets: \r\n      - 'localhost:9090'\r\n\r\n  - job_name: 'federal_k8s_nodeexpo'\r\n    honor_labels: true\r\n    metrics_path: '\/federate'\r\n    params:\r\n      'match[]':\r\n        - '{job=\"node-exporter\"}'\r\n    static_configs:\r\n      - targets:\r\n        - 'k8s.xxxx.com'\r\n    \r\n  - job_name: 'federal_vir'\r\n    honor_labels: true\r\n    metrics_path: '\/federate'\r\n    params:\r\n      'match[]':\r\n        - '{job=~\".+\"}'\r\n    static_configs:\r\n    - targets: \r\n      - 'xxxxx:xxxx'\r\n \r\nremote_write:\r\n  - url: \"http:\/\/xxx:xxx\/write\"\r\nremote_read:\r\n  - url: \"http:\/\/xxx:xxx\/read\"\r\n\r\n\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\nn.a\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\n\r\nts=Z caller=db.go:1293 component=tsdb msg=\"Deleting obsolete block\" block=01FHXGD3YATAKJY6RM6R3ETKHG\r\nts=Z caller=db.go:1293 component=tsdb msg=\"Deleting obsolete block\" block=01FHX9HCPCXFJ76C50QDD7ABQ9\r\nts=Z caller=db.go:1293 component=tsdb msg=\"Deleting obsolete block\" block=01FHX2NNEBV7PFPHZPYBHNXKB0\r\nts=Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1634162400006 maxt=1634169600000 ulid=01FHY509PDNCVMZDH5AQZHAQ1J duration=10.482782426s\r\n```\r\n","comments":["Could you please show us the contents of `meta.json` of all the blocks? `cat <data-dir>\/*\/meta.json` should do the trick.","Can we please have the graph for prometheus_tsdb_lowest_timestamp_seconds over time?\r\n\r\nAlso, could you verify the values in the **status > runtime and build: Storage retention** ? That will show if Prometheus is configured differently than you think.","> Could you please show us the contents of `meta.json` of all the blocks? `cat <data-dir>\/*\/meta.json` should do the trick.\r\n\r\nthis is the meta.json\r\n[data-meta.json.LOG](https:\/\/github.com\/prometheus\/prometheus\/files\/7385771\/data-meta.json.LOG)\r\n\r\nalso the server log\r\n[fed-server.log](https:\/\/github.com\/prometheus\/prometheus\/files\/7385772\/fed-server.log)\r\n\r\n","> Can we please have the graph for prometheus_tsdb_lowest_timestamp_seconds over time?\r\n> \r\n![image](https:\/\/user-images.githubusercontent.com\/10938721\/138379776-d5b59958-b8eb-445e-b5c6-a251dc09dfdc.png)\r\n\r\n> Also, could you verify the values in the **status > runtime and build: Storage retention** ? That will show if Prometheus is configured differently than you think.\r\n![screencapture-ops-srdcloud-cn-prometheus-flags-2021-10-21-11_29_56](https:\/\/user-images.githubusercontent.com\/10938721\/138206944-44dac5b0-a519-4df5-b29e-230b10788410.png)\r\nthe configuration looks \r\n\r\n","The oldest block's compaction level is 4 while the blocks that are newer to that have level 5, which ideally should not be the case. And the block range of the oldest block is not what Prometheus would likely produce (it is 36h, while it should match the form `2*(3^i)`, so can be either 18h or 54h, but looks like only 2 18h blocks were merged instead of 3 18h blocks).\r\n\r\nDo you have anything running on the side that is likely interfering with the blocks? (Next step is to write a unit test to check this otherwise)","> The oldest block's compaction level is 4 while the blocks that are newer to that have level 5, which ideally should not be the case. And the block range of the oldest block is not what Prometheus would likely produce (it is 36h, while it should match the form `2*(3^i)`, so can be either 18h or 54h, but looks like only 2 18h blocks were merged instead of 3 18h blocks).\r\n> \r\n> Do you have anything running on the side that is likely interfering with the blocks? (Next step is to write a unit test to check this otherwise)\r\n\r\nNo,This server is currently running a federate server only. I can recall some change on it:\r\nt1: running 2 server with separated data directory, one for federate.\r\nt2: remove one server, leave the federate only\r\nt3: change retention time from default to 60d\r\nt4: there was a mis-operation which remove the retention time args in shell command, but when this is discover in a week , the data still has 60d long.\r\nt5: fix the retention time args problem and extend it to 180d , considering the data size and space available.\r\nt6: still data is 60d long.\r\nt7: upgrade server from 2.28 to 2.29.1\r\nt8: upgrade server to 2.30.0\r\n\r\n#### t9: 2021-10-26 , the oldest level 4 block files were manually removed. \r\n![image](https:\/\/user-images.githubusercontent.com\/10938721\/138850956-02720e1f-d835-4a4a-995a-dcabcbe47cf8.png)\r\n\r\nBut is there any duplications or other problems? The total size keeps growing . sum 180GB now.\r\n![image](https:\/\/user-images.githubusercontent.com\/10938721\/138852873-808a0d70-0cee-4040-a402-f7ceb5195ebf.png)\r\n\r\n\r\nIs there a way that could backup all the data and replay it in a refresh setup? \r\nwill the snapshot way work? It only copy all the old data.\r\n   ","> t5: fix the retention time args problem and extend it to 180d , considering the data size and space available.\r\nt6: still data is 60d long.\r\n\r\nWhat was the time gap between t5 and t6?\r\n\r\n> will the snapshot way work? It only copy all the old data.\r\n\r\nYou can use this https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#snapshot with `skip_head=false` and you can run a Prometheus on that.","> > t5: fix the retention time args problem and extend it to 180d , considering the data size and space available.\r\n> > t6: still data is 60d long.\r\n> \r\n> What was the time gap between t5 and t6?\r\n> its about a week or two \r\n\r\n> > will the snapshot way work? It only copy all the old data.\r\n> \r\n> You can use this https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/api\/#snapshot with `skip_head=false` and you can run a Prometheus on that.\r\n\r\nwill in this way fix the problem?\r\n\r\n\r\nt9: 2021-10-26 , the oldest level 4 block files were manually removed.\r\n![image](https:\/\/user-images.githubusercontent.com\/10938721\/140022250-a7e70035-27c9-4bfa-b1c2-270ccf849435.png)\r\n\r\nthe data is still being keep 60d long. Is there anything to do with the hash function or time or something?\r\n\r\nt10: After two weeks of observation.  Data retained! So removing those conflicting old unexpected level 4 blocks works. But the cause of unexpected blocks still unclear.\r\n![image](https:\/\/user-images.githubusercontent.com\/10938721\/141710181-ca3e442b-102a-41d1-b01b-107006cdd5d0.png)\r\n"],"labels":["kind\/more-info-needed","component\/tsdb"]},{"title":"remote.DecodeWriteRequest: returns \"snappy: corrupt input\"","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\n\r\nI was playing with the [remote write adapter demo server](https:\/\/github.com\/prometheus\/prometheus\/blob\/release-2.30\/documentation\/examples\/remote_storage\/example_write_adapter\/server.go) and I'm getting lots of following errors that spams my stdout:\r\n\r\n```\r\nsnappy: corrupt input\r\n```\r\n\r\nIt seems `remote.DecodeWriteRequest(r.Body)` could not decode the body properly. \r\n\r\nMy `go.mod` file:\r\n```\r\ngo 1.17\r\n\r\nrequire (\r\n ...\r\n github.com\/prometheus\/prometheus v1.8.2-0.20211006110246-b878527151e6\r\n)\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nSnappy should decode the write request?\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\n```\r\nsnappy: corrupt input\r\n```\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n```\r\nLinux 5.11.0-34-generic x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```\r\nNAME               \tNAMESPACE \tREVISION\tUPDATED                                \tSTATUS  \tCHART                       \tAPP VERSION\r\nprometheus-operator\tmonitoring\t16      \t2021-10-07 15:30:12.761238426 +0000 UTC\tdeployed\tkube-prometheus-stack-18.0.5\t0.50.0\r\n```\r\n\r\n`v2.28.1`\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n\r\n<details>\r\n  <summary>Click to expand!<\/summary>\r\n  \r\n  ```\r\nenabled: true\r\n  prometheusSpec:\r\n    additionalAlertManagerConfigs:\r\n    - consul_sd_configs:\r\n      - datacenter: mars\r\n        server: consul-api.external\r\n        services:\r\n        - alertmanager\r\n    additionalScrapeConfigs:\r\n    - job_name: kubernetes-pods\r\n      kubernetes_sd_configs:\r\n      - role: pod\r\n      relabel_configs:\r\n      - action: keep\r\n        regex: true\r\n        source_labels:\r\n        - __meta_kubernetes_pod_annotation_prometheus_io_scrape\r\n      - action: replace\r\n        regex: (.+)\r\n        source_labels:\r\n        - __meta_kubernetes_pod_annotation_prometheus_io_path\r\n        target_label: __metrics_path__\r\n      - action: replace\r\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n        replacement: $1:$2\r\n        source_labels:\r\n        - __address__\r\n        - __meta_kubernetes_pod_annotation_prometheus_io_port\r\n        target_label: __address__\r\n      - action: labelmap\r\n        regex: __meta_kubernetes_pod_label_(.+)\r\n      - action: replace\r\n        source_labels:\r\n        - __meta_kubernetes_namespace\r\n        target_label: kubernetes_namespace\r\n      - action: replace\r\n        source_labels:\r\n        - __meta_kubernetes_pod_name\r\n        target_label: kubernetes_pod_name\r\n    disableCompaction: true\r\n    enableAdminAPI: true\r\n    externalLabels:\r\n      _cluster: foo\r\n      _environment: bar\r\n      _federation: baz\r\n      _provider: qux\r\n      _region: quux\r\n      _team: platform\r\n    image:\r\n      repository: $(PROMETHEUS_REPOSITORY)\r\n    logFormat: json\r\n    nodeSelector:\r\n      floor: monitoring\r\n    podMonitorSelectorNilUsesHelmValues: false\r\n    query: {}\r\n    remoteRead: []\r\n    remoteWrite: []\r\n    resources:\r\n      requests:\r\n        memory: 400Mi\r\n    retention: 6h\r\n    ruleSelectorNilUsesHelmValues: false\r\n    secrets:\r\n    - etcd-certs\r\n    serviceMonitorSelectorNilUsesHelmValues: false\r\n    storageSpec:\r\n      volumeClaimTemplate:\r\n        spec:\r\n          accessModes:\r\n          - ReadWriteOnce\r\n          resources:\r\n            requests:\r\n              storage: 10Gi\r\n          storageClassName: manual\r\n          volumeName: prometheus\r\n    thanos:\r\n      baseImage:  $(THANOS_REPOSITORY)\r\n      objectStorageConfig:\r\n        key: thanos.yaml\r\n        name: thanos-objstore-config\r\n      tracingConfig:\r\n        key: thanos.yaml\r\n        name: thanos-tracing-config\r\n      version: v0.22.0\r\n    tolerations:\r\n    - key: floor\r\n      operator: Equal\r\n      value: monitoring\r\n    walCompression: false\r\n  service:\r\n    additionalPorts:\r\n    - name: thanos\r\n      nodePort: 30091\r\n      port: 10901\r\n      protocol: TCP\r\n      targetPort: 10901\r\n    annotations:\r\n      consul.hashicorp.com\/service-name: thanos-store-api\r\n      consul.hashicorp.com\/service-port: thanos\r\n      consul.hashicorp.com\/service-sync: \"true\"\r\n    nodePort: 30090\r\n    type: NodePort\r\n  serviceAccount:\r\n    create: true\r\n  serviceMonitor:\r\n    selfMonitor: true\r\n  ```\r\n<\/details>\r\n\r\n","comments":["Remote write requests are compressed with snappy. Are you directing a Prometheus instance to write to the adapter, or are you sending the samples by some other means?","> Remote write requests are compressed with snappy. Are you directing a Prometheus instance to write to the adapter, or are you sending the samples by some other means?\r\n\r\nHey! Directing a Prometheus instance to write to the adapter. Creating a deployment and a service from the [remote write adapter demo server](https:\/\/github.com\/prometheus\/prometheus\/blob\/release-2.30\/documentation\/examples\/remote_storage\/example_write_adapter\/server.go), and adding a new `remoteWrite` endpoint as follows:\r\n\r\n```bash\r\n$ helm upgrade prometheus-operator prometheus-community\/kube-prometheus-stack --reuse-values --version 18.0.5 --set \"prometheus.prometheusSpec.remoteWrite[0].url=http:\/\/10.233.2.104:8080\/receive\"\r\n``` ","Weird, I'm not sure what the issue could be here, but I doubt it's with the adapter as that code is used in many places without issue.\r\n\r\ncc @cstyan @csmarchbanks "],"labels":["kind\/more-info-needed"]},{"title":"Consider implement deepgo linter in pipeline","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nI think @guodongli-google can clarify much better what is this `deepgo` tool exactly and how can we use and contribute it. I could not find much about the tool on GitHub. It is something like a PoC project that actively testing on big projects, developed by Google? \ud83e\udd14 \r\n\r\nThis tool found some useful _race condition_ cases like the following:\r\n\r\n* #9432\r\n* #9433\r\n* #9443\r\n\r\nWdyt? @codesome\r\n","comments":["Based on the recent discussions, the **DeepGo** tool will be formally released in 1.19 (Aug 2022) as part of the \"golang.org\/x\/tools\". However it is very likely that an experimental version will be available in 1.18 (Feb 2022). When that happens I will update this issue.\r\nThanks for the interest and proposal.","I cannot find this tool via web search; is there an update?","AFAIR it was never published, the only thing that I remember is a mention in Google3 repo (see 1st comment) https:\/\/go-review.googlesource.com\/c\/tools\/+\/287173 (sorry for my 2c)\r\n"],"labels":["kind\/enhancement","priority\/P3"]},{"title":"Add tsdb relabel command to relabel a block","body":"Signed-off-by: Ben Ye <ben.ye@bytedance.com>\r\n\r\nFixes https:\/\/github.com\/prometheus\/prometheus\/issues\/8579.\r\n\r\nThis pr upstreams `thanos tools bucket rewrite relabel` feature to promtool.\r\n\r\nExample usage:\r\n\r\n```\r\npromtool tsdb relabel <db path> <block ID> relabel.yaml\r\n```\r\n\r\nrelabel.yaml\r\n\r\n```yaml\r\n- action: drop\r\n  regex: k8s_app_metric37\r\n  source_labels: [__name__]\r\n- action: replace\r\n  source_labels: [__name__]\r\n  regex: k8s_app_metric38\r\n  target_label: __name__\r\n  replacement: old_metric\r\n\r\n```\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["uuu sweet, let us know when it's rdy for review","Should be ready for review now. Not sure how to add the changelogger to the compactor itself. @bwplotka Any suggestions?","cc @codesome @dgl ","I think it is time to come back to this. Would you like to rebase your changes ?","Looks like the Windows test is not related to this change. Other than that, rebase should be done.\r\nPlease let me know if you have any suggestions on how to improve this feature to get it merged.","Thanks! I will review in the coming days ","Yeah I also think there is ways to apply `modifiers` earlier... \r\nNow modifiers are applied after merging the series so basically we iterate the series set again. Maybe an earlier position could be better.","You can not use a default value for the first arg of a command if the\nsecond arg is mandatory. The other promtool commands only have one arg, the\nfollowing are flags, so you can omit it\n\nLe lun. 20 f\u00e9vr. 2023, 03:26, Ben Ye ***@***.***> a \u00e9crit :\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In cmd\/promtool\/main.go\n> <https:\/\/github.com\/prometheus\/prometheus\/pull\/9413#discussion_r1111397307>\n> :\n>\n> > +\ttsdbRelabelCmd := tsdbCmd.Command(\"relabel\", \"Relabel TSDB block.\")\n> +\trelabelPath := tsdbRelabelCmd.Arg(\"db path\", \"Database path (default is \"+defaultDBPath+\").\").Default(defaultDBPath).String()\n> +\trelabelBlockID := tsdbRelabelCmd.Arg(\"block id\", \"BlockID to relabel.\").String()\n> +\trelabelConfig := tsdbRelabelCmd.Arg(\"file\", \"Relabel config file to apply.\").String()\n>\n> Hi @roidelapluie <https:\/\/github.com\/roidelapluie>, I have updated the\n> code follwing your sugggestion to take default relabel.yml as default\n> config and allow multiple block IDs.\n>\n> As such, having a default for the \"db path\" argument doesn't make sense\n> because there are multiple mandatory arguments following.\n>\n> I am trying to understand the statement. I see all Promtool TSDB command\n> takes this param, shall I remove the default value?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/pull\/9413#discussion_r1111397307>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AACHHJTIRRFBRV62ADJ4CXTWYLI6ZANCNFSM5E5ACGRQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n","I see `create-blocks-from openmetrics` also contains 2 args but its first arg is the input file. So I changed the tool to use the similar pattern.\r\nThe first argument is the required file path, no default value. The second one is the TSDB path.","> Does that mean at some point it will have all the series info and their chunk info (hence large part of the index of block involved) loaded into the memory?\r\n\r\nThat's true. Because of the sorting requirement I have to buffer the series and cannot make it streaming.","Given the index cannot go beyond 64G at the moment, and that not all index is loaded into memory at once, I guess it should be fine. The user should run this command offline and not on a production machine, and that needs to be made clear in docs. I will review the PR soon.","I would love to see this added.","@codesome any chance you'll still get to this? @jesusvazquez maybe you can pick things up? This seems to require deep TSDB knowledge\u2026","Discussed at the bug-scrub again; @jesusvazquez would you be able to take a look?\r\n\r\nI think if it works in Thanos we don't need to worry too much about it.\r\n"],"labels":["stale"]},{"title":"align retention.size format with IEC units","body":"It is now not possible to specify `5G` or `5Gi`, but we have to provide `5GB`. This seems like an arbitrary decision to not adopt the k8s standard, but I am probably unaware of limitations in this matter. (Like, is it not possible to use `i` (as in `Gi`) to round to thousands?)\r\n\r\n## Proposal\r\n**Use case. Why is this important?**\r\nTo not have to learn multiple formats.","comments":["it is unfortunate that the two projects use different notations. `5G` or `5Gi` misses the unit. To answer your secondary question, if you need to round up to thousands, you would need to use `GiB` (notice the extra `B`).","Wouldn't it be beneficial to put this on the roadmap for alignment?","Prometheus is independant for kubernetes. In this case, I think we do the right thing by enforcing a unit. \r\n\r\nWe also support time-based retention, which can be expressed in many units as well.\r\n\r\nHowever, it could be a reasonable thing to propose to the [Prometheus Operator](https:\/\/github.com\/prometheus-operator\/prometheus-operator), they could bridge the gap there.","Prometheus uses Kingpin, and it is simply using the normal standard Kingpin `ByteVar` type here. Arguably, deriving from it would cause even more confusion (Kingpin conventions for all flags, except for bytes, where it uses K8s conventions\u2026).\r\n\r\nPlease also note that the prefixes are always base-2. Both `10KB` and `10KiB` will result in 10240 bytes. (That's again how Kingpin decided to handle it.)\r\n\r\n","> Please also note that the prefixes are always base-2. Both `10KB` and `10KiB` will result in 10240 bytes. (That's again how Kingpin decided to handle it.)\r\n\r\nTo clarify, I think this only applies to \"KB\" and \"KiB\", not GB and GiB (https:\/\/github.com\/alecthomas\/units\/blob\/master\/si.go).","@roidelapluie I think this code is multiplying 1024 for all prefixes. The weird special-case handling at the bottom is just making sure that both `kB` and `KB` works for metric mode, while in binary mode, `KiB` and `KB` works but not `kb`.","> @roidelapluie I think this code is multiplying 1024 for all prefixes. The weird special-case handling at the bottom is just making sure that both `kB` and `KB` works for metric mode, while in binary mode, `KiB` and `KB` works but not `kb`.\r\n\r\nooh you are right.","Apart from the configuration not yet allowing the IEC (https:\/\/en.wikipedia.org\/wiki\/Binary_prefix#IEC_prefixes) prefixed to indicate binary units, ( as in `GiB`) there already seems to be a display inconsistency when looking at the flags at runtime:\r\n\r\nThe documentation at https:\/\/prometheus.io\/docs\/prometheus\/latest\/storage\/#operational-aspects clearly states that --storage.tsdb.retention.size is \"Based on powers-of-2, so 1KB is 1024B. \".\r\n\r\nI therefore set `--storage.tsdb.retention.size=750GB`, the only supported format for the Gigabyte unit. When looking at `prometheus_tsdb_retention_limit_bytes` being `805306368000` which is 750*1024*1024*1024 this also seems accurate.\r\n\r\nBut when then looking at \"http:\/\/localhost:9090\/flags\" is see that this is \"converted\" to `--storage.tsdb.retention.size 750GiB` using `GiB`. In any case, while potentially painful once when asking folks to change their config, it would be great to align any uses of units to avoid further confusion.\r\n\r\nAdding \/ allowing scientific units might also be nice, sometimes that's what you got from your configuration management or whatever data source you use to set the `tsdb.retention.size` value ...\r\n\r\n\r\n\r\n@Morriz  maybe the title of this issue should better ask for the alignment with the standardized \"IEC units\" (https:\/\/en.wikipedia.org\/wiki\/Binary_prefix#IEC_prefixes) than pointing to \"kubernetes sizes\" which is just another piece of software using them.\r\n\r\n\r\n"],"labels":["priority\/Pmaybe"]},{"title":"Scrape fail when target cert contains multiple 'Subject Alternative Name'","body":"**What did you do?**\r\n\r\nI tried to scrape a TLS protected target which has multiple 'Subject Alternative Name' in its cert\r\n\r\n**What did you expect to see?**\r\nScrape works without any error\r\n\r\n**What did you see instead? Under which circumstances?**\r\nAn error on the Prometheus target page:\r\n2021-09-27T09:23:44,312+00:00 WARN  [SparkUI-43] org.sparkproject.jetty.server.SecureRequestCustomizer: Host 172.17.0.8 does not match SNI X509@47daa1b2(my-cert,h=[domain1, domain2],w=[])\r\n\r\n**Environment**\r\n\r\n* Prometheus version: 2.15.2\r\n\r\n* Prometheus configuration file:\r\n\r\n      - job_name: tls-target\r\n        scheme: https\r\n        tls_config:\r\n          ca_file: \/run\/secrets\/cacert\/ca.pem\r\n          cert_file: \/run\/secrets\/clicert\/cert.pem\r\n          key_file: \/run\/secrets\/clicert\/key.pem\r\n          server_name: domain1\r\n        kubernetes_sd_configs:\r\n          - role: pods\r\n\r\n* Server cert relevant part\r\n```\r\n            X509v3 Subject Alternative Name:\r\n                DNS:domain1, DNS:domain2\r\n```\r\n\r\n* Logs:\r\n       - No logs on Prometheus side\r\n       - Log message on target side :`2021-09-27T09:23:44,312+00:00 WARN [SparkUI-43] org.sparkproject.jetty.server.SecureRequestCustomizer: Host 172.17.0.8 does not match SNI X509@47daa1b2(my-cert,h=[domain1, domain2],w=[])`\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------------\r\n\r\n**What works**\r\n\r\n1. Either curling the target by hand https:\/\/domain1:4440\/metrics\/prometheus.   It gives back the metrics successfully\r\n2. Or removing one SAN entry from the server cert(In this case Prometheus can scrape the metrics successfully):\r\n```\r\n            X509v3 Subject Alternative Name:\r\n                DNS:domain1\r\n```","comments":["I would need to double check the code, but it seems that we would be setting SNI without changing the Host: header. I will investigate.","@roidelapluie this seems to be related to the fact that net\/http uses the request's `Host` field to set the Host header[1]. \r\n\r\nI am trying to get familiar with the Prometheus codebase and I have time this week to look into this one if that's okay.\r\n\r\n[1] https:\/\/github.com\/golang\/go\/issues\/29865"],"labels":["kind\/bug","not-as-easy-as-it-looks","priority\/P3","component\/scraping"]},{"title":"Old exemplars get re-added continuously","body":"Imagine we have an application exporting a histogram with typical long-tailed distribution.\r\nThe exemplar for popular buckets will change on nearly every observation, while those in rare buckets might stay the same for hours.\r\nThe Prometheus client library treats them all equally, so the hours-old exemplars are scraped every time same as the brand-new ones.\r\n\r\nStorage for exemplars is a circular buffer: once full, the exemplar added to the buffer longest ago is removed to add a new one.\r\nBut we are continually re-adding those hours-old exemplars: they displace ones with newer timestamps.\r\n\r\nTo illustrate: the buffer here allows about 30 minutes of exemplars, but we have a few going back 2 hours:\r\n![image](https:\/\/user-images.githubusercontent.com\/8125524\/134318537-759101cf-8886-4067-a755-ca917e7d6355.png)\r\n\r\n\r\nI am reporting this because the observed effect was surprising to me, and I suspect it is unwanted.\r\nIt means this description is incorrect: https:\/\/github.com\/prometheus\/prometheus\/blob\/93886d84172503c30a58c60a90d0f3be3a9903c0\/tsdb\/exemplar.go#L79-L82\r\n\r\nThe timestamp is not \"useful to check for what time range the current exemplar buffer limit allows\", since it is greatly extended back in time for rare buckets.\r\n\r\nAs a fix I might suggest: when the buffer is full, don't add a timestamp older than the one you would displace.\r\nHowever it may be simpler and cheaper to just discard incoming exemplars older than a fixed limit, e.g. the stale limit (5 mins).","comments":["I think this makes sense, but we would need to also decide what to do with examplars too far in the future.\r\n\r\ncc @cstyan WDYT?","> decide what to do with examplars too far in the future\r\n\r\nExemplars should never be newer than the sample; the point is to add detail to data that has already been collected.\r\n\r\nWhen the scrapee does not supply a timestamp for the sample, Prometheus will add one, so now we have the issue that clocks can drift, so a small amount of slop should be allowed.  I would probably arrive at the same figure of 5 minutes, although I could no longer argue to re-use the stale limit.","Sorry, just saw this issue. I think doing something to reduce the churn and adding of really old exemplars is a good idea but I don't know about \"not older than the one you would replace\". Sounds relatively sane at the moment though, not going to block this if you want to proceed @bboreham "],"labels":["help wanted","kind\/enhancement","priority\/P3","component\/tsdb"]},{"title":"PromQL: optimise joins where a subexpression is step-invariant","body":"## Proposal\r\nIn the blog post [Introducing the '@' Modifier](https:\/\/prometheus.io\/blog\/2021\/02\/18\/introducing-the-@-modifier\/) is this example:\r\n\r\n```\r\nrate(http_requests_total[1m]) # This acts like the actual selector.\r\n  and\r\ntopk(5, rate(http_requests_total[1h] @ end())) # This acts like a ranking function which filters the selector.\r\n```\r\n\r\nAs I read the current PromQL code, the engine will evaluate `topk(5, rate(http_requests_total[1h] @ end()))` just once, replicate those results out to the length of the query range, then go step by step calling `VectorAnd` on those results.\r\nBut, since the rhs data doesn't change, that wastes a lot of work building a map on every step.\r\n\r\nPerhaps we can modify the binary-op implementations to allow for one side being constant.\r\n(Both sides constant is already optimised).","comments":[],"labels":["kind\/enhancement","component\/promql","priority\/P3"]},{"title":"Disable container discovery for kubernetes_sd_config endpoints","body":"## Proposal\r\n\r\n> The endpoints role discovers targets from listed endpoints of a service. For each endpoint address one target is discovered per port. If the endpoint is backed by a pod, all additional container ports of the pod, not bound to an endpoint port, are discovered as targets as well.\r\n\r\nSometimes you don't want to have such discovery, in addition, to already existent endpoints and it would be nice to have the ability to disable such unused but **heavy** behaviour.\r\nSuch behaviour may break Kubernetes API by tons of requests - when Prometheus gets endpoint discovery and understand it is the pod behind - it sends a LIST pods request to the API. And this happens for each `endpoints` object, which leads to apiserver heavy load in big clusters.\r\n","comments":["Do you have a reproducible setup for this (maybe with audit logs)? The way I read the code is that we list\/watch endpoints and pods only once and do all the following operations on the informer cache, so this should not be causing any more load on the Kubernetes API than the initial list\/watch (once for endpoints once for pods). Prometheus also re-uses list\/watches as best as possible. For example two kubernetes service discoveries that are identical except in their relabel configs also share a list-watch. The only pathological case that I am aware of is when there is a kubernetes service discovery per namespace, as then a list-watch per namespace is needed, if this is the case you are running into, then I would suggest instead of using the `namespaces` field in the kubernetes service discovery config, use relabeling to perform namespace selection instead (as that way the list-watch can be shared between configs).","Yes, we have per-namepace service monitors  and I am really worried about using a shared list-watch and if this request will be too heavy for apiserver","I don't think it would be, but this appears to be a feature request for the prometheus-operator instead (since it doesn't allow re-using the list\/watch today I think).","Actually, I played with it and got almost empty targets lists, because the requests were too heavy, so timed out or killed by apiserver or Prometheus from time to time. And Prometheus Operator brought its own case too.\r\n"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/service discovery"]},{"title":"Follow-up on limiting the number of alerts \/ rules","body":"This is a followup on the PR #9260.\r\n\r\nI think we would benefit from a small dedicated section in the docs, that explains the semantics behind this. In particular:\r\n\r\n- How to monitor this (evaluation failed metric)\r\n- What happens when you hit the limit\r\n  - alert is not seen as active or pending, they just disappear)\r\n  - this is an error in evaluation, no stale markers are written for rules\r\n- What happens when you recover (alerts go back to pending state)\r\n- some caveats (**I think** the recently closed alerts count in the limit <state=inactive>)","comments":["(from https:\/\/github.com\/prometheus\/prometheus\/pull\/9260#issuecomment-919782079)\r\n> Mmh this pull request takes into consideration the \"inactive alerts\", which are not pending and not firing. I feel like they should not count, WDYT?\r\n\r\nIn the original issue (#9225), the problem was `\/api\/v1\/rules` becoming unusable. I'm pretty sure that endpoint returns inactive rules, so if we didn't include them in the count, there could still be overload.","We should definitely be explicit about this though.","@roidelapluie @LeviHarrison \r\nAfter observing this thread, I tried to write a rule file:\r\n```\r\ngroups:\r\n- name: my-project\r\n  limit: 5\r\n  rules:\r\n  # Alert for any instance that is unreachable for >1 s.\r\n  - alert: InstanceDown\r\n    expr: up == 0\r\n    for: 1s\r\n    labels:\r\n      severity: page\r\n    annotations:\r\n      summary: \"Instance {{ $labels.instance }} down\"\r\n      description: \"{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.\"\r\n```\r\nbut there is some issue with the syntax here for the limit, as the prometheus server keeps exiting on start after adding the limit property.\r\n\r\nWould appreciate your guidance, really keen on contributing more!\r\n\r\n","Could you please provide any logs from the server?","> Could you please provide any logs from the server?\r\n\r\nI was running the server inside a docker container, attached is the error logs, I am using the latest version(hopefully)\r\n[error.logs.txt](https:\/\/github.com\/prometheus\/prometheus\/files\/7357915\/error.logs.txt)\r\n","You're running `v2.30.3` of Prometheus. This feature hasn't made it in to a release yet.","If you want to try it out, you'll have to build from the `main` branch, or use one of the `main` Docker images.","> If you want to try it out, you'll have to build from the `main` branch, or use one of the `main` Docker images.\r\n\r\nThanks @LeviHarrison Running the main image worked!, but in the rules UI shouldnt we see the limits property too? \r\nAlso ideally the firing alerts should disappear after 1s if the limit is 1(for 1s interval), please correct me if i am wrong\r\n\r\n![rules](https:\/\/user-images.githubusercontent.com\/29231015\/137597692-f8bc60fc-7850-4d26-8645-9fc3e7424d32.PNG)\r\n\r\n PS : I am a little new too this paradigm, so its highly likely that i might not make sense at times","The `limit` belongs to the [rule group](https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/recording_rules\/#rule_group), and the other numerical field, `interval`, isn't displayed on the rules UI either.\r\n\r\nWhat your comment did make me realize though is that I forgot to add `limit` to the rules API (https:\/\/prometheus.io\/docs\/prometheus\/latest\/configuration\/recording_rules\/#rule_group). If you're interested in doing that, you would need to add the field to the `RuleGroup` struct:\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/4414351576ac27754d9eec71c271171d5c020677\/web\/api\/v1\/api.go#L1141-L1152\r\n\r\nassign it in the API response here:\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/4414351576ac27754d9eec71c271171d5c020677\/web\/api\/v1\/api.go#L1198-L1205\r\n\r\nand add the field to the API docs (with a note the `0` means there is no limit): https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/docs\/querying\/api.md#rules","Was able to do the above, used gitpod for testing(finally read contributing.md), limit is showing in the responses of \/rules\r\nAlso are there any unit test case fixes which should be done?\r\n\r\nplanning to raise separate PRs for changes in documentation and the code, please let me know if it is okay, and thanks for the help!\r\nupdate: PR-https:\/\/github.com\/prometheus\/prometheus\/pull\/9528","That sounds great. Thanks so much! There might be a test to update in web\/api\/v1\/api_test.go, but I'm not at my computer right now so I can't link it.\n\nWhen you get around to opening a PR please make sure to sign off on the DCO in your commits.","done, will resolve if there are any issues https:\/\/github.com\/prometheus\/prometheus\/pull\/9531","@LeviHarrison request you to have a look into my commit before i open a PR again https:\/\/github.com\/Rudy167\/prometheus\/commit\/2207b36fae55b64c41e99347fc7e9489466c1264\r\n\r\nhad trouble while running the linter and test cases on git pod(for linting i got this issue : WARN [runner] The linter 'golint' is deprecated (since v1.41.0) due to: The repository of the linter has been archived by the owner.  Replaced by revive. )\r\n\r\n\r\nlet me know if everything looks ok so i can raise a PR again","@Rudy167 friendly ping :)\r\n\r\nIt would be great to get this in before the upcoming release.","> @Rudy167 friendly ping :)\r\n> \r\n> It would be great to get this in before the upcoming release.\r\n@LeviHarrison \r\nohh..i thought it was already taken up as the thread got closed, i am sorry, give me a while","Sorry, I have re-opened it."],"labels":["kind\/enhancement","priority\/P3","component\/documentation"]},{"title":"promtool: too many files open when backfilling a long time range","body":"**What did you do?**\r\n\r\nI wanted to backfill a larger dataset using `promtool tsdb create-blocks-from openmetrics`\r\n\r\n**What did you expect to see?**\r\n\r\nAll blocks created without issue.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\n```\r\n01FFJ65SW4MQV0Z9TB7QDYW91E  1631455224000  1631462364001  1h59m0.001s  120          1            1            783\r\nblock creation: process blocks: get blocks: 2 errors: corrupted block 01FFJ65SW4MQV0Z9TB7QDYW91E: fcntl data\/01FFJ65SW4MQV0Z9TB7QDYW91E\/chunks: too many open files; corrupted block 01FFJ65TX4J9MD71X0AWJE2C2F: fcntl data\/01FFJ65TX4J9MD71X0AWJE2C2F\/chunks: too many open files\r\n```\r\n\r\n**Environment**\r\n\r\nmacOS, Alpine Linux\r\n\r\n* System information:\r\n\r\nDarwin 19.6.0 x86_64\r\n\r\n* Prometheus version:\r\n\r\n```\r\n$ promtool --version\r\npromtool, version 2.29.2 (branch: non-git, revision: non-git)\r\n  build user:       brew@iMac-Pro\r\n  build date:       20210827-15:14:42\r\n  go version:       go1.17\r\n  platform:         darwin\/amd64\r\n ```\r\n\r\n(2.30.0 was released at the time of writing - tried it with that as well later on and encountered the same issue)\r\n\r\n* Logs:\r\n\r\nSee above.\r\n\r\n---\r\n\r\n**Reproducer**\r\n\r\nGenerate some sample data that span a large number of 2-hour blocks:\r\n\r\n```python\r\nimport random\r\nimport datetime as dt\r\n\r\nFILENAME = \"metrics.prom\"\r\nMETRIC = \"some_random_metric_total\"\r\nN = 10000\r\n\r\nif __name__ == \"__main__\":\r\n    ts = dt.datetime.utcnow() - dt.timedelta(minutes=N)\r\n    with open(FILENAME, \"wt\", encoding=\"utf-8\") as fw:\r\n        for _ in range(N):\r\n            value = random.randint(1000, 2000)\r\n            fw.write(f\"{METRIC} {value} {int(ts.timestamp())}\\n\")\r\n            ts += dt.timedelta(minutes=1)\r\n        fw.write(\"# EOF\")\r\n```\r\n\r\nThen with a low ulimit (which happens to be the default on my Mac)\r\n\r\n```\r\n$ ulimit -n\r\n256\r\n```\r\n\r\npromtool will err having too many files open, can be seen live via `lsof -p $(pgrep promtool)`. Works just fine on a system with a higher ulimit (e.g. stock Alpine).\r\n\r\n---\r\n\r\nI tried introspecting the code to understand where the file handlers get opened and if\/when they get closed, but I got nowhere :( If the handlers need to be open (e.g. if incoming data are not time-ordered?), I'd expect promtool to be either ulimit-aware or have a fixed size pool of handlers (or a combination of both). But I don't really understand the storage mechanics to give an appropriate piece of advice.","comments":["Yes, I guess we could evaluate how many FD's can be needed and issue a warning if we hit this.","We have looked at this again in the bug triage meeting.\r\n\r\nPart of the solution would be #8518. Another workaround would be to use `.\/promtool tsdb create-blocks-from  --max-block-duration 18h` so you have less blocks, therefore less fd's needed.\r\n","Since Prometheus uses block ranges `2h, 6h, 18h, ...` (3x every turn), maybe `18h` would be a better choice to align well."],"labels":["kind\/enhancement","priority\/Pmaybe","component\/promtool"]},{"title":"Support for transactional remote write","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\nAtomic writes for Prometheus scraped data to remote storage.\r\n\r\n[Here](https:\/\/tsdb.co\/txn-rw-design) is the design doc, that contains the plan and discussions around the topic.\r\n\r\n**Status: Review** phase ","comments":["Perhaps link the doc under https:\/\/prometheus.io\/docs\/introduction\/design-doc\/ ?","That will be a good place. How do we add to that list?","Just update https:\/\/github.com\/prometheus\/docs\/blob\/master\/content\/docs\/introduction\/design-doc.md and open a PR for that repo."],"labels":["priority\/Pmaybe","component\/remote storage","kind\/feature"]},{"title":"Discovery for ECS","body":"In an earlier evaluation, ECS discovery was rejected due to API rate limiting issues described at the [discovery](https:\/\/github.com\/prometheus\/prometheus\/blob\/60918b8415d928363ea4bc766d450e707035abe0\/discovery\/README.md) section.  As of today, there are ECS users that are publishing Prometheus metrics and using CloudWatch Agent's Prometheus scraping capabilities. They configure the agent with task selection mechanism to shard the load among multiple clusters. Influenced by what the users already do, we think we can tackle the problem in a couple of ways:\r\n\r\n* Asking users to configure the discovery to discover a set of matching tasks from a cluster, cache metadata in memory where possible.\r\n* Querying the initial data with the ECS API and then relying on ECS events to identify new and terminated tasks.\r\n* Asking users to run Prometheus as a sidecar in their ECS tasks as a last resort.\r\n\r\nGiven we have this functionality in the CW Agent, not having a similar capability in Prometheus is confusing the ECS users. We would like to fill this gap by contributing an ECS discovery agent to Prometheus and want to switch to the discovery mechanism provided here in all our other collection agents (CW Agent, OpenTelemetry Prometheus Receiver, etc)\r\n\r\n## Goals\r\n* Discovery will only discover metric endpoints from a single cluster.\r\n* We will allow users to filter the tasks by the Cluster Query language and ECS tags.\r\n* Users should be able to specify ports and metrics path where the Prometheus metrics are published from the task. (See the config for more.)\r\n* ECS discovery will support both ECS on EC2 and ECS on Fargate.\r\n\r\n## Config\r\nOnce implemented, ECS discovery will be supported in the Prometheus config. The example below will query the cluster to discover ECS tasks\/containers matching the given task selectors.\r\n\r\n```\r\nscrape_configs:\r\n  - job_name: ecs-job\r\n    [ metrics_path: <string> ]\r\n    ecs_sd_configs:\r\n      - [ refresh_interval: <string> | default = 720s ]\r\n        [ region: <string> ]\r\n        cluster: <string>\r\n        [ access_key: <string> ] \r\n        [ secret_key: <secret> ]\r\n        [ profile: <string> ]\r\n        [ role_arn: <string> ]\r\n        ports:\r\n            - <int>\r\n        task_selectors:\r\n          - [ service: <string> ]\r\n            [ family: <string> ]\r\n            [ revisions: <int> ]\r\n            [ launch_type: <string> ]\r\n            [ query: <string> ]\r\n            [ tags: \r\n               - <string>:  <string> ]\r\n```\r\n\r\n## Discovery\r\nDiscovery is done by periodically pulling the ListTasks API. Discovery will only return the ACTIVE tasks.\r\n\r\nAs an improvement, we will switch to a model where we will listen to ECS events to be notified about the task start and terminations in the future. This will allow us to call the ListTasks for once and rely on the events for the changes as an optimization.\r\n\r\n## Labels\r\nPrometheus discovery can automatically add ECS task\/container labels to the scraped metrics. The discovery will add the following labels:\r\n\r\n\r\nLabel | Source | Type | Description\r\n-- | -- | -- | --\r\n__meta_ecs_cluster | ECS Cluster | string | ECS cluster name.\r\n__meta_ecs_task_launch_type | ECS Task | string | \"ec2\" or \"fargate\".\r\n__meta_ecs_task_family | ECS Task | string | ECS task family.\r\n__meta_ecs_task_family_revision | ECS Task | string | ECS task family revision.\r\n__meta_ecs_task_az | ECS Task | string | Availability zone\r\n__meta_ecs_ec2_instance_id | EC2 | string | EC2 instance id for EC2 launch type. Otherwise \"fargate\".\r\n\r\n## Authentication & IAM\r\nWe will use the default credential provider chain, the following permissions are required:\r\n* ec2:DescribeInstances\r\n* ecs:ListTasks\r\n* ecs:DescribeContainerInstances\r\n* ecs:DescribeTasks\r\n\r\n","comments":["Thank you for this proposal.\r\n\r\nOverall the proposal is interesting and I recognize the need to have this additional AWS integration. I have a few comments\/questions, just by reading your proposal. I am currently not familiar with ECS, so I have a few questions. \r\n\r\nIs there any additional metadata? The fact that you use tags to filter the targets means that we can probably expose the tags as additional metadata.\r\n\r\n\r\n> We will use the default credential provider chain.\r\n\r\nIs is the same thing we use for EC2\/Lightsail\/Sigv4 ? Or should we all align them to this new technique as an intermediate step? (we'll have to be careful and be backwards compatible).\r\n\r\n\r\n> port_path\r\n\r\nport_path as explained here might be confusing. Prometheus is generally explicit in its configuration. Could we have:\r\n```\r\n- port: <int> | default 80\r\n  metrics_path: <string> | default \/metrics\r\n```\r\n\r\nIt's unsure why we would use port 9090 by default, should we simply ask the user to set at least one port? Is there a way to also filter the ports by a portName?\r\n\r\nI'd note that metrics_path is probably not really useful here since it can be set at the scrape_config level and via relabeling.\r\n\r\nIt is also unclear to me why port_path is a list. Do you plan to verify it against the exposed ports of the containers or add it anyway for every target?\r\n\r\nYou also plan to filter on ACTIVE tasks. Does this state also cover the containers that are starting and terminating?","The proposal didn't have too much detail but additional metadata could be the EC2 instance metadata that runs the containers. It requires an additional request to get details of an EC2 instance for every ECS task container and instance metadata can be useful to identify the internal and external IP addresses of the task. These IPs don't change until the task is killed, so we can cache them in memory rather having to query them again and again.\r\n\r\n> Is is the same thing we use for EC2\/Lightsail\/Sigv4 ? Or should we all align them to this new technique as an intermediate step? (we'll have to be careful and be backwards compatible).\r\n\r\nThis is what they use, nothing new here. It's the standard best practice and mechanism to do auth.\r\n\r\n> port_path as explained here might be confusing. Prometheus is generally explicit in its configuration. Could we have:\r\n\r\nI wanted this to allow containers to publish at paths they would prefer but I have no objections to your suggested and my initial version included a port and a metrics_path just like yours.\r\n\r\n> It's unsure why we would use port 9090 by default, should we simply ask the user to set at least one port? Is there a way to also filter the ports by a portName?\r\n\r\nGood question, 9090 came from the sidecar I'm writing that will publish ECS infra metrics in the Prometheus format but it's not a good port to default to. Ports don't have names so it's not possible to query ports by name. I think I overoptimized this for the sidecar and expecting users to set at least one port sounds reasonable.\r\n\r\n> I'd note that metrics_path is probably not really useful here since it can be set at the scrape_config level and via relabeling.\r\n\r\nI agree, let me move them to the ecs_sd_configs level.\r\n\r\n> You also plan to filter on ACTIVE tasks. Does this state also cover the containers that are starting and terminating?\r\n\r\nThis means starting and terminating tasks won't be discovered. Starting tasks will only be discovered at the next discovery if they started by the time we are querying the tasks again.\r\n\r\n\r\n\r\n","I updated the configuration in the original proposal. I also turned the port into an int array because a task contains multiple containers that may publish multiple metric handlers.","We know some users use one prometheus with multiple AWS accounts. Is it planned to have the current EC2's auth parameters?\r\n\r\n```\r\n# The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`\r\n# and `AWS_SECRET_ACCESS_KEY` are used.\r\n[ access_key: <string> ]\r\n[ secret_key: <secret> ]\r\n# Named AWS profile used to connect to the API.\r\n[ profile: <string> ]\r\n\r\n# AWS Role ARN, an alternative to using AWS API keys.\r\n[ role_arn: <string> ]\r\n```\r\n\r\nGiven than metrics_path is no longer per port, it would be easier to just keep them at the scrape_configs level and not repeat it at the service discovery level.","Updated the config to add the auth and removed the metrics_path. Not sure if I got the notation correctly, I'm not very familiar with it.","`prometheus_scrape: \"true\"` should probably be `string: string`, otherwise LGTM.","Updated the labels to be aligned with what we are doing for the ECS exporter, https:\/\/github.com\/prometheus-community\/ecs_exporter\/pull\/2.","My experience has been with a fork of https:\/\/github.com\/teralytics\/prometheus-ecs-discovery which is working well. We added ratelimiting of AWS API calls and a cache of task definitions between discovery runs which has pretty much solved problems with hitting API ratelimits. We run one prometheus instance per AWS region per account and it manages to discover and scrape from a large number of clusters, tasks and containers.\r\n\r\nThe proposal here wouldn't replace it for our use case, sadly, as we are using it to dynamically discover all ECS clusters (we run large multitenant accounts where clusters can come and go at any time) so the requirement to specify a cluster in the SD config won't cut it for us. I still think it's a useful addition to prometheus, though, and I expect we will in future be looking to deploy a prometheus within each cluster, at which point hardcoding the cluster name would be fine.\r\n\r\nI'm working on getting the changes we made cleared for contribution back to the original project which may be helpful to inform this design?\r\n\r\n`prometheus-ecs-sd` works by expecting containers within tasks to specify their scrape config via their `dockerLabels` map. Exposing a container's `dockerLabels` as e.g. `__meta_ecs_container_dockerlabel_<name>` labels would allow the same behaviour via a relabel config. e.g. allowing a per-container scrape path, port or scheme as well as selection of targets via their docker labels. This would match up with what Prometheus's Kubernetes service discovery does too.\r\n\r\nAs well as the AZ name, a label for the AZ ID would be useful (as a parallel to EC2 service discovery's `__meta_ec2_availability_zone_id` label). This is not available in the ECS `ListTasks` API response though - that only has `availabilityZone`: https:\/\/docs.aws.amazon.com\/AmazonECS\/latest\/APIReference\/API_Task.html#ECS-Type-Task-availabilityZone. An opportunity for an API enhancement request perhaps?","Hello, We have the same need, I'm thinking of going to https:\/\/github.com\/teralytics\/prometheus-ecs-discovery for the moment, but I would have liked native support, do you have more information on the progress of this feature? \r\n\r\nThank You","We've been considering a few options how we can support this on ECS more natively so the autodiscovery problem becomes a non-problem. If existing solutions are sufficient for now, I'd highly recommend using them. I'll update the proposal once we have something more concrete.","is anyone using any updated forks for the prometheus-ecs-discovery project? or any idea if we are to see an `ecs_sd_configs` anytine soon? asking this as I've seen recently in the operator that we have the equivalent for ec2"],"labels":["component\/service discovery","priority\/P3","kind\/feature"]},{"title":"Public IPs of Azure virtual machines in scale sets are not exposed by the `__meta_azure_machine_public_ip` label in `azure_sd_config`","body":"**What did you do?**\r\n\r\nWe have Azure virtual machines that are part of a virtual machine scale set.  We attempted to scrape metrics from them using the following `azure_sd_configs` block:\r\n```\r\nazure_sd_configs:\r\n  - environment: AzurePublicCloud\r\n    port: 9182\r\n    subscription_id: ***\r\n    tenant_id: ***\r\n    client_id: ***\r\n    client_secret: ***\r\n    refresh_interval: 5m\r\n    authentication_method: OAuth\r\n  relabel_configs:\r\n  - source_labels: [__meta_azure_machine_tag_prometheus]\r\n    separator: ;\r\n    regex: true.*\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_azure_machine_tag_node_exporter]\r\n    separator: ;\r\n    regex: true.*\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_azure_machine_name]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: instance\r\n    replacement: $1\r\n    action: replace\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nWe expected that the public IP addresses of the virtual machines would be visible in Prometheus under the `__meta_azure_machine_public_ip` label.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nThe private IP addresses of the virtual machines are visible under the `__meta_azure_machine_private_ip` tag, but the public IP addresses are not visible:\r\n\r\n<img width=\"1003\" alt=\"Screenshot of Azure console showing partially consored virtual machine public and private IP addresses\" src=\"https:\/\/user-images.githubusercontent.com\/28942094\/131953807-4578adfe-9fe2-4b95-bcde-21adf8a5d9a7.png\">\r\n<img width=\"625\" alt=\"Screenshot of Prometheus labels for the same image, showing the private IP address label but not the public IP address label\" src=\"https:\/\/user-images.githubusercontent.com\/28942094\/131953812-a56f5665-1c5b-4be4-a0b4-21e95e08f67b.png\">\r\n\r\n* Prometheus version:\r\n\r\n\tWe have tested with both `prometheus:v2.29.2` and `prometheus:v2.20.1`.\r\n\r\n* Prometheus configuration file:\r\n```\r\nazure_sd_configs:\r\n  - environment: AzurePublicCloud\r\n    port: 9182\r\n    subscription_id: ***\r\n    tenant_id: ***\r\n    client_id: ***\r\n    client_secret: ***\r\n    refresh_interval: 5m\r\n    authentication_method: OAuth\r\n  relabel_configs:\r\n  - source_labels: [__meta_azure_machine_tag_prometheus]\r\n    separator: ;\r\n    regex: true.*\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_azure_machine_tag_node_exporter]\r\n    separator: ;\r\n    regex: true.*\r\n    replacement: $1\r\n    action: keep\r\n  - source_labels: [__meta_azure_machine_name]\r\n    separator: ;\r\n    regex: (.*)\r\n    target_label: instance\r\n    replacement: $1\r\n    action: replace\r\n```","comments":["This sounds like a bug, I will try to poke around the code. In the mean time, help wanted.","This issue seems to be a duplicate of https:\/\/github.com\/prometheus\/prometheus\/issues\/5588, which is related to the underlying https:\/\/github.com\/Azure\/azure-sdk-for-go\/issues\/4829 Go SDK","Hi, I worked on a fix and created a PR #13241. Please help to review the changes."],"labels":["help wanted","kind\/bug","component\/service discovery","priority\/P3"]},{"title":"Reduce CPU consumption at low cardinality","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nWhen we have a large number of low cardinality queries, such as idc and machine survival, uncompressed postinglist will consume a large amount of computing resources in crc32. Should we consider compressing postinglist like lucene or caching frequently used term?\r\n*\u201cNice to have\u201d is not a good use case. :)*\r\n","comments":["cc @codesome @bboreham do you have any opinion here?","We eventually do want to compress the postings list while also increasing the postings to 64bit from the current 32bit. There will likely be reduction in the size with 64bit+compression compared to 32bit+no compression from our earlier experiments, but it wont be a big reduction depending on trade-off with reading performance.\r\n\r\nA cache looks like a good idea to try out and benchmark. The cache that I have mind is for `single matcher -> postings list` per block, and if feasible with not so complex code, we could store the postings list compressed in memory to fit more stuff in the cache.","Can you show in a profile where the CPU is being used?","> Can you show in a profile where the CPU is being used?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8488141\/135408836-68c342b8-e18f-4b4d-a479-77a1c672572a.png)\r\n","Are you saying by caching the lookup from matchers->series we can save CRC calculations?\r\nHow would you invalidate the cache?\r\n","> Are you saying by caching the lookup from matchers->series we can save CRC calculations? How would you invalidate the cache?\r\n\r\nI think it can use the LRU to cache high-frequency calls matchers","Ok but after series are added the cached value is now wrong. ","The CRC calculations are only done when reading from persisted blocks which are immutable.\r\n\r\nI can confirm that I have also observed cpu time being dominated by CRC calculations on setups where you have a lot of alert or recording rules running that touch data beyond the head block.","> Ok but after series are added the cached value is now wrong.\r\n\r\nwe don't have that problem, historical blocks are immutable","So, the summary could be: \"by caching the lookup from [block id+matchers] -> series we can save CRC calculations\".\r\n","> So, the summary could be: \"by caching the lookup from [block id+matchers] -> series we can save CRC calculations\".\r\n\r\nI think it will be better to use [block id+name+value] -> Postings  that Will have a higher hit rate.\r\n\r\n"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/tsdb"]},{"title":"integrate_since() function","body":"I would like a query function that can integrate a rate over time, in order to give a value that counts the total amount of \"things\" that rate has been counting, since a given time.\r\n\r\nRaw counters are subject to resets, and do not start at zero for a given query period, but if they did then graphing them would be sufficient. Since they don't, the solution is to record the rate of a counter, and then graph the integral of that counter, since the start time.\r\n\r\nI would suggest some syntax like\r\n\r\n```\r\nintegrate_since(vector, starttime)\r\n```\r\n\r\nwhere `starttime` is given in the same form as things like the `@` operator. This function would yield zero when queried at the start time, and increasingly positive values according to the underlying rate vector, for later times. The behaviour at earlier times doesn't need to be described - I'd suggest either an error, or NaN or somesuch would be appropriate. In actual use it would then be easy enough to graph this in e.g. Grafana.\r\n\r\n---\r\n\r\nBackground: I've been using Prometheus to store weather data, trying to draw a graph of total rainfall in mm, when given a rainfall rate that counts mm\/hour.","comments":["Here's a screenshot showing how a graph would look when using this function:\r\n![rain-4days](https:\/\/user-images.githubusercontent.com\/350941\/130360254-42eca2c9-1924-49f3-9fc0-e758215449b8.png)\r\n\r\nThis graph was generated by just plotting the raw counter value on the righthand axis to generate the light blue line. This particular time range only works because there happened not to be any counter resets within that time, and because it started at zero at the beginning of the time range. If either of these facts does not hold, then the graph is no longer that neat. An `integrate_since()` function would allow creating that sort of graph from arbitrary rates, without needing that.\r\n\r\nIn a lot of places whenever you plot a rate it's handy to be able to see the accumulated integral of that rate across the same time period. It's a useful ability to plot a graph of a rate besides the integral of that rate - this goes far beyond rainfall data. The rate allows you to answer certain questions, such as \"when is the most busy time in terms of hit count?\", whereas a graph of the integral allows you to answer certain other questions, such as \"by what point in the week have we received 50% of our total hits?\". E.g. given the rainfall graph above I can easily see that I got a bit more rainfall 1 day ago, than the amount I got 6 days ago. Answering such a question from only looking at the green rate line would have been much more difficult; eyeballing it from the blue integral line is much easier.","can you use something like this?\r\n```\r\nsum_over_time(increase(my_counter{tag=\"value\"}[1m])[10h:1m])\r\n```","@darshanime \r\n> can you use something like this?\r\n\r\nI don't believe so. That would report, at every time in the query range, the total increase over the preceding (in your case) 10hours before that moment. That would yield a graph that doesn't begin at zero on the lefthand edge.\r\n\r\nThe correct query should yield zero for the first moment, the increase over the preceding 1 minute at the next minute, the increase over the preceding 2 minutes at the next minute, and so on.","I'm aware of the history in https:\/\/github.com\/prometheus\/prometheus\/issues\/1335 which covered much of the same ground, except the key difference here is the explicit \"since timestamp\" parameter, which solves the outstanding design problems with the earlier discussion.","I understand the need, but it is way more general and complex.\r\n\r\nBasically we would need to think about a general way to calculate e g. Network bandwidth since beginning of the month, error budget burned since the beginning of the quarter etc..\r\n\r\nI will add more links to previous discussions in the future.","> That would yield a graph that doesn't begin at zero on the lefthand edge.\r\n\r\nCould you subtract the value at the start using [the `@` modifier](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/basics\/#modifier)?","@bboreham I think that it would require subtracting a different value for different points. \r\n\r\nA solution for this problem can be masking out the values that are outside of the evaluation range, i.e.:\r\n```\r\nsum_over_time(\r\n    increase((up{} unless timestamp(up{}) < 123456)[1m:])[10h:]\r\n)\r\n```\r\n\r\nWhere `123456` is the `start()` of the query range. The drawback is that querier needs to calculate that as we can't use `start()` in a binary expression like `timestamp(...) < start()`.","Could we also not do something like `rate(my_counter[<the-duration>]) * <duration-in-seconds>`? And with work ongoing for the duration literal, we might soon be able to do `end() - anystarttime` for the duration.","> `rate(my_counter[<the-duration>]) * <duration-in-seconds>`\r\n\r\nThat's the same as `increase(my_counter[<the-duration>])`, isn't it?\r\n\r\nAnd generally, yes, I think this is pretty much what we discussed earlier, which would also allow burn-down graphs for your error budget during the billing period etc. I think we should support that in a straight-forward way, and `@` and arithmetic on durations is a step into that direction. Ultimately, we might want to consider an optional syntax for range selectors that allows absolute points in time, something like `increase(my_counter[@<timestamp>])` for increase since `<timestamp>` (or even something trickier for \"since the beginning of this month\" where \"this month\" is dynamic, but that requires knowledge of the calendar \u2013 it's a rabbit hole, but one that we might need to go down at some point).","I have tested multiple approaches in the past to solve this with plain PromQL. One of big issues is that we do extrapolate the values if we do not cover the full range (with @colega  proposal).","@colega \r\n\r\n> sum_over_time(\r\n>     increase((up{} unless timestamp(up{}) < 123456)[1m:])[10h:]\r\n> )\r\n\r\nI'm having some success with this. For short time ranges (e.g. up to a couple of days), I can graph:\r\n\r\n```\r\nsum_over_time(increase((weather:rainfall:mm unless timestamp(weather:rainfall:mm) < ${__from:date:seconds})[30s:])[${__range_s}s:])\r\n```\r\n\r\n(The `${__from:...}` is Grafana syntax for the epoch timestamp of the start of the graphing range, and `${__range_s}` is the total graph width in seconds)\r\n\r\nHowever, two problems:\r\n\r\n* It's very CPU-intensive; this graph renders noticably slower than any of my other ones. Beyond about 2d of range it becomes basically unusable. Rendering a week took over a minute. I haven't tried showing a month of historic data yet. Since the entire point of this sort of query is to smooth over counter resets over longer periods of time, this makes it unhelpful. Short-term views are unlikely to contain a reset so they tend to draw OK as raw counter values (if you just ignore the non-zero offset)\r\n\r\n* It gets easily broken by NaNs or holes in the data. The moment there is a gap anywhere in the data, nothing to the right of it will show, however small the gap was.\r\n\r\nGiven as `increase()` already has logic within it to cope with counter resets, I'd love to be able to render simply\r\n\r\n`increase_since(vector, ${__from:date:seconds})`, or maybe the other suggested syntax of `increase(vector[@${__from:date:seconds}])`.","Or maybe, can we allow a `[since TIMESTAMP]` notation instead of a fixed duration? Then we could simply plot e.g.:\r\n\r\n`increase(weather:rainfall:mm[since 1234567000])`\r\n\r\nthe number being filled in by grafana.","Would `[since TIMESTAMP]` be fundamentally different from the already suggested `[@TIMESTAMP]`?","@beorn7 Oh likely it's the same idea.. I just think the spelling is a bit better, it more clearly suggests what it's doing. It's not grabbing an instant \"at\" that time, it's grabbing the entire (variable width) range of time since then. The `@` symbol would suggest an instantaneous point in time","> The @ symbol would suggest an instantaneous point in time\r\n\r\nWhich we already support, i.e. `my_metric @ TIMESTAMP`.\r\n\r\nSince `[...]` is well known already as a range selector, `[@...]` would read as \"select a range from a fixed point in time\".\r\n\r\nI generally dislike modifiers that are words. `offset` in hindsight rubs me the wrong way (and with negative offsets, we kind of have created a double negation, i.e. `-2h` is actually 2 hours into the future\u2026).\r\n\r\nIn this case, I also think it's nicer to consistently prefix absolute timestamps with `@`.\r\n\r\nBut all of this is just details. First we need consensus that we want to have something like this (which I currently believe we should do).","I :+1: [@ timestamp], as well as [@start()].","I just feel that `since` conveys more obvious meaning to the reader. Plus it makes it obvious that the range is \"since\" that timestamp until, er.. whenever we're querying I suppose.. The idea being it's some point in the past. Leaves scope open for possible other ideas like \"until\" or whatever.\r\n\r\nIt perhaps seems odd that a Perl language designer would be advocating in favour of wordy names and against nonobvious symbols, but that's just my experience from doing this sort of thing a lot :)\r\n\r\nBut whatever - feel free to pick whatever spelling you feel works best within your existing culture. As I said before, it's only the spelling so I'm sure I'd be happy with the feature however it was spelled.","I came across this issue while looking for something similar to do a monthly billing report.\r\nIn the mean time I solved it with a recording rule looking like this:\r\n```\r\nmy_counter - my_counter @ <start_timestamp>\r\n```\r\n\r\nAnd an external cron job that updates the `<start_timestamp>` in the recording rule monthly in my case.\r\nMaybe this can help others too.","This issue hasn't been touched by devs in a while, so in particular I note nobody's closed it. Can I presume from that this isn't a rejection so far, and you're simply awaiting someone to actually implement it? I.e. if someone turned up with a working solution you would consider accepting it?\r\n\r\nI am keen to have my graphs of rainfall-over-time you see...  :)","I would say the conclusions we came up here are still too vague to inform an actual implementation. Thus, I would expect if someone just came up with a PR, it would trigger a lot of discussions. So perhaps it would be better to write a design doc first, which is a better form to discuss the fundamentals, rather than mixing them up with implementation details, as it would inevitable happen in a PR review.\r\n\r\nYou could also put it on the agenda of the [monthly dev summit](https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit?pli=1), but I would bet the outcome there is \"yes, interesting, please write a design doc and come back\". So whoever is interested could just start with the design doc, which will then help to guide further discussions (on the doc and also during the dev summit).\r\n\r\nAlso note this [brainstorming document](https:\/\/docs.google.com\/document\/d\/1jMeDsLvDfO92Qnry_JLAXalvMRzMSB1sBr9V7LolpYM\/edit) (shameless plug), which also touches what's discussed here. It's not a design doc yet, but could serve as inspiration for one.\r\n\r\nAs you can see, we might solve quite a few problems if we get the syntax extension right.\r\n\r\n","> Also note this [brainstorming document](https:\/\/docs.google.com\/document\/d\/1jMeDsLvDfO92Qnry_JLAXalvMRzMSB1sBr9V7LolpYM\/edit) (shameless plug), which also touches what's discussed here. It's not a design doc yet, but could serve as inspiration for one.\r\n\r\nI've just had a read of that now. Nothing jumps out at me as being at all similar to this integration idea.\r\n\r\nI thought the design of this idea seemed fairly concrete: Given a rate, integrate it since some time to show the accumulated \"thing\" that it is the rate of. E.g. if it's a rate in bytes\/sec, the integral of it would give the total bytes. In effect, if the rate is in fact calculated by applying `rate()` to some counter, then in the absence of counter-resets the integral would just give the difference between the counter at the query time and at the start time. In fact the only real reason I even *want* the integral function is to smooth over the fact that my counter *does* have resets. If my counter never reset, I could just graph `counter - counter@starttime` and we'd be done already.\r\n\r\nDo you have a template for a design doc? If there's some framework to write some words into I'd be happy to write those words.","> Nothing jumps out at me as being at all similar to this integration idea.\r\n\r\nMaybe I have misunderstood something, but isn't the section about [Richer syntax for range selectors and subqueries](https:\/\/docs.google.com\/document\/d\/1jMeDsLvDfO92Qnry_JLAXalvMRzMSB1sBr9V7LolpYM\/edit#heading=h.ormgh6nlwfxe) pretty much identical with the suggestion here?\r\n\r\n> Given a rate, integrate it since some time to show the accumulated \"thing\" that it is the rate of. E.g. if it's a rate in bytes\/sec, the integral of it would give the total bytes. In effect, if the rate is in fact calculated by applying rate() to some counter, then in the absence of counter-resets the integral would just give the difference between the counter at the query time and at the start time. In fact the only real reason I even want the integral function is to smooth over the fact that my counter does have resets. If my counter never reset, I could just graph counter - counter@starttime and we'd be done already.\r\n\r\nAgain, maybe I'm mistunderstanding some things. But it seems to me you are confusing `rate` vs. `increase` on the one hand and the problem that range selectors only ever select a _duration_ going back from the evaluation time rather than doing something like \"select from this fixed point in time to now\".\r\n\r\nAbout the former: `increase` is essentially `rate` integrated, or in other words: `rate` is the result of `increase` divided by the duration of the range selector in seconds. So in a way, that part of the problem is already solved.\r\n\r\nAbout the latter: IIUC that's where we run into the problem. If you want \"increase since the beginning of the month\" (perfect to express how a monthly error budget gets burned down etc.), you cannot express this as a single PromQL expression to graph. The idea floated here as well as in the brainstorming doc is to extend the known `@` syntax to range selectors. For example, 2022-09-01 00:00:00 UTC is 1661990400 in Unix time. So I could graph \"errors since beginning of the months\" with the expression `increase(api_errors_total[@1661990400])`.\r\n\r\nSo this looks like a good idea. Putting it into a design doc is a chance to get more eyes on it and discuss the implications and possible alternatives. (What comes to mind, for example, is how to improve the handling \u2013 typing in Unix time is clunky, and you have to update the number for each new months. Something like `@beginning_of_this_month()` would be cool, but that has a millions of implications because the beginning of the months depends on your timezone, so we bleed into the bigger discussion of timezone support, for which a design doc is in the works etc. \u2013 which shouldn't imply we have to solve everything at the same time, but it might make sense to think about it for a few moments and perhaps have some ideas how it could be supported later so that we don't design things now in a way that makes any future improvement impossible etc.)\r\n\r\nWRT a design doc template: We don't have one (yet), although some really like the template our sister project Thanos uses (I cannot find a good link to the template right now, perhaps a Thanos expert like @bwplotka could provide it). You could also have a look at [existing docs](https:\/\/prometheus.io\/docs\/introduction\/design-doc\/) to get an idea.\r\n"],"labels":["priority\/Pmaybe","component\/promql","kind\/feature"]},{"title":"Historic n-points over time query","body":"(I admit that is a terrible title for this issue; I'm open to suggestions for a better one)\r\n\r\nThe prometheus `max_over_time()` query gives one number for the maximum of a vector over its entire history range. E.g.\r\n\r\nThe hottest temperature seen over the past 7 days was:\r\n\r\n```\r\nmax_over_time(weather:temperature:degrees[7d])\r\n```\r\n\r\nYou can optionally provide an \"interval\" in the notation, asking it to only calculate that value at the given interval:\r\n\r\n```\r\nmax_over_time(weather:temperature:degrees[7d:1d])\r\n```\r\n\r\nThis is veryalmost but not what I want. What I would like from the query is a timeseries of many values, showing the maximum value at the given time of the day, for each of the past 7 days. (Obviously the actual \"1 day\" and \"7 day\" parameters should be tuneable, they're just the easiest ones to explain). So, for this data, it would allow me to graph the maximum temperature for each moment of the day, across the past 7 days.\r\n\r\n---\r\n\r\nBackground: I've been using Prometheus to store weather data, trying to generate useful graphs of measurements that follow a strong daily pattern, such as temperature or light level.","comments":["By way of perhaps better explanation here is a screenshot of what I can get out currently:\r\n![temperature](https:\/\/user-images.githubusercontent.com\/350941\/130359994-19647435-5016-4425-b719-91471c7f9ad5.png)\r\n\r\nIt has evaluated the value just once per 24h (1d) period of time, and drawn a flat yellow line between them all. Instead, I would like to see something like the following badly-drawn mockup:\r\n![temperature-7daymax](https:\/\/user-images.githubusercontent.com\/350941\/130360094-a95c98b9-7c8f-47c4-ae11-416487e1ddcf.png)\r\n\r\nWith this kind of query, I'd then be able to draw a graph of the current temperature, displayed within a context of the min\/max over the past 7 days.","I think this should be possible with subquery syntax.","An initial attempt at subquerying doesn't appear to work. E.g. I've added\r\n\r\n```yaml\r\n      - record: 'weather:temperature:max7d'\r\n        expr: 'max_over_time(weather:temperature:avg5m[7d:1d])'\r\n      - record: 'weather:temperature:min7d'\r\n        expr: 'min_over_time(weather:temperature:avg5m[7d:1d])'\r\n```\r\n\r\nand the numbers I get out don't vary at all across a single 24h period. I believe this comes from this part of https:\/\/prometheus.io\/blog\/2019\/01\/28\/subquery-support\/:\r\n\r\n> rate(http_requests_total[5m]) is executed from start=<now>-30m to end=<now>, at a resolution of 1m. Note that start time is aligned independently with step of 1m (aligned steps are 0m 1m 2m 3m ...).\r\n\r\nwhere my \"start\" times are being quantized to a per-day basis.","#9114 adds support for aligning subqueries to the evaluation time, ignoring the performance optimization. Maybe that helps?","Indeed, I think this is another use case that #9114 addresses.","I agree with @beorn7 , this will be addressed by #9114"],"labels":["kind\/enhancement","component\/promql"]},{"title":"Local storage limits are not working when using remote_write as primary persistent store","body":"**What did you do?**\r\nRunning Prometheus 2.29.1 (same happens in 2.25.0) with InfluxDB as primary persistent store. CLI arguments (see below) limit local storage to 250 MB or 30 minutes.\r\n\r\n**What did you expect to see?**\r\nLocal storage grows to **maximum** of 250 MB.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nPrometheus used all disk space in designated partition (say \/data), then it used all disk space in `\/var` partition by spamming logs reporting it ran out of disk space (multiple entries every second).\r\nLocal storage keeps growing indefinitely. Mostly space consumed by WAL segments, 2nd in disk usage is chunks_head.\r\n\r\nBelow example is **after the cleanup post the above described outage**:\r\n```\r\n# ps faux | grep -v grep | grep \"tsdb.retention\" && du -sh \/data\/prometheus\/*\r\nuser  23155 47.6 27.2 8046840 6678468 ?     Ssl  10:49  21:56 \/opt\/prometheus\/prometheus --config.file=\/etc\/prometheus\/prometheus.yml --storage.tsdb.path=\/data\/prometheus\/ --storage.tsdb.retention.time=30m --storage.tsdb.retention.size=250MB --web.console.templates=\/opt\/prometheus\/consoles --web.console.libraries=\/opt\/prometheus\/console_libraries\r\n248M\t\/data\/prometheus\/chunks_head\r\n0\t\/data\/prometheus\/lock\r\n20K\t\/data\/prometheus\/queries.active\r\n1.4G\t\/data\/prometheus\/wal\r\n```\r\n\r\nFrom metrics it looks like after a few hours of running the storage cleanup\/compaction (whatever else it is called) did not run even once:\r\n\r\n```\r\n# curl -s localhost:9090\/metrics | egrep 'wal|retention' | grep -v \"#\"\r\nprometheus_tsdb_retention_limit_bytes 2.62144e+08\r\nprometheus_tsdb_size_retentions_total 0\r\nprometheus_tsdb_time_retentions_total 0\r\nprometheus_tsdb_wal_completed_pages_total 15504\r\nprometheus_tsdb_wal_corruptions_total 0\r\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} NaN\r\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} NaN\r\nprometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} NaN\r\nprometheus_tsdb_wal_fsync_duration_seconds_sum 48.380230839999996\r\nprometheus_tsdb_wal_fsync_duration_seconds_count 3\r\nprometheus_tsdb_wal_page_flushes_total 25864\r\nprometheus_tsdb_wal_segment_current 14\r\nprometheus_tsdb_wal_truncate_duration_seconds_sum 0\r\nprometheus_tsdb_wal_truncate_duration_seconds_count 0\r\nprometheus_tsdb_wal_truncations_failed_total 0\r\nprometheus_tsdb_wal_truncations_total 0\r\nprometheus_tsdb_wal_writes_failed_total 0\r\nprometheus_wal_watcher_current_segment{consumer=\"influxdb\"} 14\r\nprometheus_wal_watcher_record_decode_failures_total{consumer=\"influxdb\"} 0\r\nprometheus_wal_watcher_records_read_total{consumer=\"influxdb\",type=\"samples\"} 28299\r\nprometheus_wal_watcher_records_read_total{consumer=\"influxdb\",type=\"series\"} 589\r\nprometheus_wal_watcher_samples_sent_pre_tailing_total{consumer=\"influxdb\"} 0\r\n```\r\n\r\n**Environment**\r\n\r\n* System information:\r\n```\r\n# uname -srm\r\nLinux 3.10.0-862.el7.x86_64 x86_64\r\n```\r\n\r\n* Prometheus version:\r\n```\r\n# \/opt\/prometheus\/prometheus --version\r\nprometheus, version 2.29.1 (branch: HEAD, revision: dcb07e8eac34b5ea37cd229545000b857f1c1637)\r\n  build user:       root@364730518a4e\r\n  build date:       20210811-14:48:27\r\n  go version:       go1.16.7\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Prometheus CLI options:\r\n```\r\n# ps faux | grep prometheus\r\nuser  23155 49.3 27.2 8046840 6689004 ?     Ssl  10:49  19:04 \/opt\/prometheus\/prometheus --config.file=\/etc\/prometheus\/prometheus.yml --storage.tsdb.path=\/data\/prometheus\/ --storage.tsdb.retention.time=30m --storage.tsdb.retention.size=250MB --web.console.templates=\/opt\/prometheus\/consoles --web.console.libraries=\/opt\/prometheus\/console_libraries\r\n```\r\n\r\n* Prometheus configuration file:\r\n```yaml\r\n# my global config\r\nglobal:\r\n  scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n  scrape_timeout:      30s  # scrape_timeout is set to the global default (10s).\r\n  evaluation_interval: 30s # Evaluate rules every 10 seconds. The default is every 1 minute.\r\n\r\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\nrule_files:\r\n    - \"...\"\r\n\r\nremote_write:\r\n  - name: \"influxdb\"\r\n    url: \"http:\/\/redacted:8086\/api\/v1\/prom\/write?db=prometheus_staging\"\r\n    remote_timeout: \"300s\"\r\n\r\nremote_read:\r\n  - name: \"influxdb\"\r\n    url: \"http:\/\/redacted:8086\/api\/v1\/prom\/read?db=prometheus_staging\"\r\n    remote_timeout: \"300s\"\r\n\r\n# A scrape configuration containing exactly one endpoint to scrape:\r\n# Here it's Prometheus itself.\r\nscrape_configs:\r\n...\r\n```\r\n\r\n* Logs:\r\nNot applicable","comments":["https:\/\/github.com\/prometheus\/prometheus\/issues\/8884#issuecomment-902508495\r\n\r\nThe the retention duration cannot be less than the min block duration, which is 2h, because the in-memory part (the Head block) works in batches to flush the data to disk (and hence skip the block because of being beyond the retention)","Then what is the point of even having size limiting options if they cannot limit the size. Systems where remote storage is used may have very limited disk capacity. Systems with very high number of metrics (ours is the case of this) will consume disk space very quickly, 2 hours can be gigabytes of data. \n\nI would even go one step further and question why does it need to write locally at all if remote storage is intended as permanent store. But that is entirely separate discussion then.","> I would even go one step further and question why does it need to write locally at all if remote storage is intended as permanent store. But that is entirely separate discussion then.\r\n\r\nThe short answer is that because this isn't an intended use case of Prometheus. \r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/pull\/8785 implements a WAL only (no persistent blocks storage) agent intended for remote write only deployments, but in the mean time you could also try the [Grafana Agent](https:\/\/grafana.com\/docs\/grafana-cloud\/agent\/) which the Prometheus agent work is based on.","I would close this issue as we have documented this in #9245. WDYT @cstyan","Agreed, though I think there's probably still enough confusion on this topic that maybe a docs page with examples or a prometheus blog post would be worth our time.","If I may suggest, would also be helpful to add some minimal info about this case to '--help' output in binaries to at least direct people to finding more details on this. Also here as it mentions nothing about minimum requirements -- https:\/\/prometheus.io\/docs\/prometheus\/latest\/storage\/#operational-aspects\n\nAlso below that section in remote storage it mentions nothing about needing to write locally. ","None of the docs site pages mention either. There are a few blog posts and lightning talks explaining how remote write works under the hood, such as: https:\/\/grafana.com\/blog\/2019\/03\/25\/whats-new-in-prometheus-2.8-wal-based-remote-write\/\r\n\r\nLet me know if the content there is useful, and if so we can include some of it in the docs site in the future. As usual, contributions are welcome :)"],"labels":["component\/remote storage","component\/tsdb"]},{"title":"Remote write duplicate\/out of order drops also valid timeseries and does not report error details","body":"**Current behavior:**\r\n\r\nMultiple Prometheus instances are collecting metrics on DMZ  and sending them trough remote write to single Prometheus acting as remote write receiver in private network. Remote write is used to metrics collection to avoid scraping from private network.\r\n\r\nSometimes there might be incorrect scraping configuration, for example missing unique labels from the collector Prometheus metrics scrape resulting single identical metrics within scrape interval exists in each collector Prometheus in DMZ.\r\nWhen WAL is sent to remote write receiver large number of valid unique timeseries are lost when single timeseries is interpreted as duplicate\/out of order.\r\n\r\nExpected behavior:\r\nWhen duplicate timeseries is sent only duplicate should be dropped.\r\nError log should contain first time series interpreted as duplicate with necessary details (time series name and labels key\/values.\r\n\/metrics should also report if there are errors in remote write sender or receiver.\r\n\r\n**What did you see instead? Under which circumstances?**\r\nNumber of valid timeseries dropped, potentially all time series from whole packet resulting strange partial metrics.\r\n\r\nError happen when WAL write order is different than scrape order, normally 50% of the instance has write fails failure in chunks having duplicate metrics resulting all valid unique metrics to be dropped from packets.\r\n\r\nin addition to dropping\/discarding also valid timeseries the error log on both does not indicate what was duplicate. At least one\/first duplicate timeseries which if out of order should be logged. Otherwise very hard to find duplicate timeseries.\r\n\r\n**Environment**\r\n* System information:\r\nLinux\r\n* Prometheus version:\r\n2.27.1\r\n\tinsert output of `prometheus --version` here\r\n\r\n* Prometheus configuration file:\r\ndefault remote write config values used, up to \"max_samples_per_send=500\" metrics missing due single duplicate metrics.\r\n\r\n* Logs:\r\n```\r\nSender:\r\nAug 17 13:44:03 xxx.yyy.com prometheus[1801]: ts=2021-08-17T10:39:44.471Z caller=dedupe.go:112 component=remote level=error remote_name=66b3a7 url=https:\/\/zzz.com\/api\/v1\/write msg=\"non-recoverable error\" count=454 exemplarCount=0 err=\"server returned HTTP status 400 Bad Request: out of order sample\"\r\n\r\nreceiver:\r\nAug 17 13:44:03 zzz.com prometheus[1628]: level=error ts=2021-08-17T11:44:03.690Z caller=write_handler.go:53 component=web msg=\"Out of order sample from remote write\" err=\"out of order sample\"\r\n```\r\n","comments":["Hi @jtorkkel \r\n\r\n>Sometimes there might be incorrect scraping configuration, for example missing unique labels from the collector Prometheus metrics scrape resulting single identical metrics within scrape interval exists in each collector Prometheus in DMZ.\r\n\r\nYou seem to have answered your own question. For Prometheus, only the label-values on the series matter to identify a series. So if multiple sources are emitting the same labels, you need to add unique labels for each source Prometheus as you mentioned. There is nothing the Prometheus accepting remote-write can do here.\r\n\r\n> At least one\/first duplicate timeseries which if out of order should be logged. Otherwise very hard to find duplicate timeseries.\r\n\r\nThis is something that can be done as enhancement though","This is the expected behaviour, since Prometheus' remote write receiver treats the whole remote write batch as if it was scrape data.","Thanks, \r\n\r\nFully understand that dropping full packet is by design.\r\n\r\nHowever detecting errors is very painful at the moment and would be great if improved.\r\n\r\nNow errors can be seen only from logs which cannot be easily checked without admin access, metrics would allow creation of alert and UI would allow quick check if sending\/receiving works as expected.\r\n\r\nThese could be improved\r\n- error logging first out-of-odder timeseries sample sample like in recording rule logging\r\n- success\/error metrics counter for remote write sender \/ receiver\r\n- remote write sender\/receiver counters and last error in UI (like recording rules show errors, and scrape\/TSDB shows scrape and timeseries related status).\r\n\r\nI am very excited Prometheus having also remote write receiver, great solution to aggregate metrics in medium size setup using push. No need to use Telegraf or pull federate) which have issue for large number of timeseries (> 100k).\r\n\r\nRemote write sender is already de-facto for Cortex, Thanos and Influx.\r\n","There are metrics for the appending of the samples to the tsdb head: https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/tsdb\/head_append.go#L257-L263\r\n\r\nI think improved logging would require changing the error definition + tsdb code. Not sure how we'd want to expose or track this, our convention is usually to return the last seen error not the first one.\r\n\r\n> Remote write sender is already de-facto for Cortex, Thanos and Influx.\r\n\r\nThese systems handle receiving of remote write batches differently.\r\n","I would recommend setting unique external_labels to the prometheus server writers.","I'm seeing the same errors in logs, except we are on 2.32.1 on FreeBSD, we have five Prometheis sending data to one writer, and I don't know where to start looking. A hint of what is being dropped, if anything, would be really helpful. I even used the correct plural, see? :)\r\n\r\nPrometheus version:\r\n```\r\nprometheus, version 2.32.1 (branch: release-2.32, revision: 0)\r\n  build user:       root\r\n  build date:       20211227-15:14:28\r\n  go version:       go1.17.6\r\n  platform:         freebsd\/amd64\r\n```\r\n\r\nThe writers are NOT running in agent mode.\r\n\r\nI tried deleting everything in the data directory on agents (`\/var\/db\/prometheus` on FreeBSD), as suggested in another issue (now marked as fixed), it didn't make any difference. So I assume it's not the same issue as that one.\r\n\r\nI didn't find the duplicates yet, and I don't know anything about Prometheus internals to make the debugging go faster. I'm pretty sure it's a configuration error on our side, but it's hard to identify it. At this point anything would help. Metric name and\/or instance printed with the error message on either side, would go a long way towards finding out where the issue is.","`At this point anything would help. Metric name and\/or instance printed with the error message on either side, would go a long way towards finding out where the issue is.` Unfortunately the error contents are set by the receiving side. Without code modifications to either Prometheus or the thing your sending to I don't have any debugging suggestions off the top of my head. You could try comparing the list of targets of your Prometheus instances to eachother. It's hard to say anything else without seeing more of your config. Try adding external_labels to each prometheus' config, this issue is usually the result of multiple prometheus instances scraping the same targets and writing the data to the same remote write endpoint.","I found the issue yesterday. We did have external_labels already, and we were scraping different pieces of the network (one sender in each VPC), which is why it was so confusing. I posted to the mailing list because I'm not sure GitHub is the right place and it seems like a different issue than this one, but I don't see my message there. I'll try again.\r\n\r\nAnyway, the issue was that we had the same recording-rules.yaml on all servers, including the remote write receiver. The receiver is also a scraper, so it makes sense that it also has the rules. But I assume the rules get parsed twice in that case?\r\n\r\nHowever, another thing that is confusing is that this recording-rules.yaml config:\r\n```\r\ngroups:\r\n  - name: node-exporter\r\n    rules:\r\n      # CPU cores per node\r\n      - record: instance:node_cpus:count\r\n        expr: count(node_cpu_seconds_total{mode=\"idle\"}) without (cpu,mode)\r\n\r\n      # CPU in use by CPU\r\n      - record: instance_cpu:node_cpu_seconds_not_idle:rate5m\r\n        expr: sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m])) without (mode)\r\n```\r\ncauses complains about duplicates, while this one:\r\n```\r\ngroups:\r\n  - name: node-exporter\r\n    rules:\r\n      # CPU cores per node\r\n      - record: instance:node_cpus:count\r\n        expr: count(node_cpu_seconds_total{mode=\"idle\"}) without (cpu,mode)\r\n```\r\nwhich has the second rule deleted, does not. And I don't understand why. So I posted to ask, but it didn't go through for some reason. I'm not sure if it's a bug and it belongs on GitHub or not."],"labels":["priority\/Pmaybe","kind\/more-info-needed"]},{"title":"(prometheus web)when delete monitoring object ,Status-Targets always exist","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nI  delete all monitoring object and execut hot reload.but prometheus web Status-Targets always exist\r\n**What did you expect to see?**\r\nprometheus web Targets are deleted.\r\n**What did you see instead? Under which circumstances?**\r\n\r\n**Environment**\r\nprometheus V2.19.0\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["The configuration was probably not reloaded or not reloaded properly. Could you provide us more details about your setup alongside with the logs that prove that prometheus did the reload?","The configuration already reloaded ,we used 'curl -X POST http:\/\/IP\/-\/reload'';The problem I described only exists when the last monitored object is deleted.\r\n","When I delete the last monitored object, the log of prometheus is as follows.\r\n> level=debug ts=2021-08-25T01:59:54.840Z caller=file.go:340 component=\"discovery manager scrape\" discovery=file msg=\"file_sd refresh found file that should be removed\" file=\/etc\/prometheus\/host.yml\r\nlevel=info ts=2021-08-25T01:59:54.855Z caller=main.go:799 msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/redfish.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/etcd.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/vm.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=string\/0 subs=[prometheus]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/oracle.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/1 subs=\"[OracleStorage OracleLog Oracle OracleAvailable]\"\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/snmp.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/2 subs=[etcdcluster]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/3 subs=[BulkPing]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/hardware.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/4 subs=\"[tcp ping]\"\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/5 subs=[Redfish]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/cloud.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/6 subs=[HostHardware]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/7 subs=[VM]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/blackbox.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/8 subs=\"[IO Host]\"\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/9 subs=\"[Cloud_PS Cloud_FS]\"\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/host.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager scrape\" msg=\"Starting provider\" provider=*file.SDConfig\/10 subs=[Snmp]\r\nlevel=debug ts=2021-08-25T01:59:54.861Z caller=manager.go:224 component=\"discovery manager notify\" msg=\"Starting provider\" provider=string\/0 subs=[config-0]\r\nlevel=debug ts=2021-08-25T01:59:54.862Z caller=file.go:307 component=\"discovery manager scrape\" discovery=file msg=\"File discovery stopped\"\r\nlevel=debug ts=2021-08-25T01:59:54.862Z caller=file.go:307 component=\"discovery manager scrape\" discovery=file msg=\"File discovery stopped\"\r\nlevel=debug ts=2021-08-25T01:59:54.862Z caller=file.go:284 component=\"discovery manager scrape\" discovery=file msg=\"Stopping file discovery...\" paths=[\/etc\/prometheus\/bulkping.yml]\r\nlevel=debug ts=2021-08-25T01:59:54.862Z caller=manager.go:242 component=\"discovery manager scrape\" msg=\"Discoverer channel closed\" provider=string\/0\r\nlevel=info ts=2021-08-25T01:59:54.863Z caller=main.go:827 msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\n","Can you please tell me if you change your configuration between reloads? Thanks!","When reloaded,\uff0cPromrtheus.yml did not change","Thanks , I can reproduce this. It is a race condition. In your case, you do not need to reload Prometheus."],"labels":["kind\/bug","component\/service discovery","priority\/P3"]},{"title":"Stop remote-write flapping between two shard counts","body":"I took this screen grab to illustrate: shards is flapping from 2 to 3 and back again, as the lag between queued data and sent data grows and falls.  Round-trip latency is broadly flat over the period.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8125524\/129541393-fd6f0899-d937-43d4-a722-a68334bd2445.png)\r\n\r\nI see that when desired-shards moves from 2.01 to 1.99 this triggers an instant shard-down from 3 to 2, only to see lag increase and shard back up to 3 again.\r\n\r\nThe algorithm avoids shifts of less than 30%, however 3->2 is 33% (and 2->1 is 50%).\r\nOne simple modification would be to also prohibit a downshard of 1.\r\n\r\nI considered adding more dampening to the algorithm, but in my picture it's happening over 20-40 minutes; it's hard to imagine dampening that works a bit better at that scale but doesn't still oscillate over hours.","comments":["The only problem could be when users have set queue configs that will result in them only ever needing a small amount of shards, and where sharding up or down by one shard changes throughput a lot.","Going _up_ by one would be allowed. "],"labels":["kind\/enhancement","priority\/Pmaybe","component\/remote storage"]},{"title":"React UI: insufficient significant digits in Y axis labels over small range","body":"**What did you do?**\r\n\r\nIn the PromQL React UI, gave a PromQL expression where the Y values are over a small range, then view as a graph.\r\n\r\nTo replicate: e.g. `foo < 0.10 > 0.096` or `foo < 0.11 > 0.096`\r\n\r\n**What did you expect to see?**\r\n\r\nEach Y-axis label should be distinct, with sufficient significant digits to distinguish them.  e.g.\r\n\r\n* 0.096\r\n* 0.097\r\n* 0.098\r\n* 0.099\r\n* 0.100\r\n* 0.101\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nIn the following graph, the data points range from 0.0965 to 0.0999:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/44789\/129471355-2fc6c307-5737-4e55-bfc2-0d3fbad67d97.png)\r\n\r\nI believe the Y axis label positions actually correspond to 0.0960, 0.0965, 0.0970 ... 0.1000, 0.1005, but all are showing as 0.10.\r\n\r\nWith values from 0.0965 to 0.11:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/44789\/129471416-1ba72c16-cbe4-48ac-8b78-e181caf73e37.png)\r\n\r\nIn this case I believe the Y axis positions correspond to 0.096, 0.098, 0.100, 0.102, ... 0.110, 0.112; but half of them show as 0.10 and half as 0.11\r\n\r\n(Real-world use case which triggered this: alerts set at disks having less than 10% free space, then viewing the graph of the alerting expression when the values are only just dipping below and above 10%)\r\n\r\n**Workaround**\r\n\r\nManually add `* 100` (or other scale as required) to the expression you are graphing.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/44789\/129471876-ea428237-cb44-412e-9fa8-e9ce7f97919e.png)\r\n\r\n**Reference**\r\n\r\nPossibly relates to old issue #3084\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n    ```\r\n    Linux 4.15.0-153-generic x86_64\r\n    ```\r\n\r\n* Prometheus version:\r\n\r\n    ```\r\n    prometheus, version 2.28.1 (branch: HEAD, revision: b0944590a1c9a6b35dc5a696869f75f422b107a1)\r\n      build user:       root@2915dd495090\r\n      build date:       20210701-15:20:10\r\n      go version:       go1.16.5\r\n      platform:         linux\/amd64\r\n    ```\r\n\r\n* Alertmanager version:\r\n\r\n    n\/a\r\n\r\n* Prometheus configuration file:\r\n\r\n    n\/a\r\n\r\n* Alertmanager configuration file:\r\n\r\n    n\/a\r\n\r\n* Logs:\r\n\r\n    n\/a\r\n\r\n\r\n","comments":["cc @juliusv "],"labels":["kind\/enhancement","priority\/Pmaybe","component\/ui"]},{"title":"Prometheus reports an error EOF when sending alert to alertmanager","body":"**Related background\uff1a**\r\nprometheus version :  2.28.1\r\nalertmanager version:  0.22.2   (They are the latest version)\r\n\r\nI used docker to build an alertmanager cluster (started the alertmanager cluster consisting of two instances), and started two prometheus services at the same time. At the beginning, the environment was normal and the alert emails were also received. It didn't take long for me to start prompting EOF.\r\n\r\nTried: Replace other versions of prometheus and alertmanager, restart docker.service, etc., all of which are not resolved\r\n\r\n**error info:**\r\nprometheus report:\r\ncaller=notifier.go:527 component=notifier alertmanager=http:\/\/10.10.20.11:19194\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/10.10.20.11:19194\/api\/v2\/alerts\\\": EOF                                                                         \r\ncaller=notifier.go:527 component=notifier alertmanager=http:\/\/10.10.20.11:19193\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/10.10.20.11:19193\/api\/v2\/alerts\\\": EOF\r\n\r\nalertmanager report:\r\ncomponent=cluster memberlist=\"2021\/08\/06 06:18:27 [ERR] memberlist: Received invalid msgType (80) from=172.17.0.1:51102\\n\"\r\n\r\nThe startup command is as follows:\r\n\r\n**alertmanager-1\uff1a**\r\ndocker run -d -p 19093:9093 -p 19093:9093\/udp -p 19193:19193 -p 19193:19193\/udp --name alert1 --restart always -v \/opt\/prom_mappings\/alert1:\/etc\/alertmanager  prom\/alertmanager:0.22.2  --config.file=\/etc\/alertmanager\/alertmanager.yml --cluster.listen-address=0.0.0.0:19193 --cluster.advertise-address=10.10.20.11:19193 --log.level=debug --cluster.peer-timeout=40s\r\n\r\n**alertmanager-2\uff1a**\r\ndocker run -d -p 19094:9093 -p 19094:9093\/udp  -p 19194:19194 -p 19194:19194\/udp --name alert2 --restart always -v \/opt\/prom_mappings\/alert2:\/etc\/alertmanager prom\/alertmanager:0.22.2  --config.file=\/etc\/alertmanager\/alertmanager.yml --cluster.listen-address=0.0.0.0:19194 --cluster.advertise-address=10.10.20.11:19194 --cluster.peer=10.10.20.11:19193 --log.level=debug --cluster.peer-timeout=40s\r\n\r\n**prometheus config:**\r\n<img width=\"636\" alt=\"\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_1628499820569\" src=\"https:\/\/user-images.githubusercontent.com\/76472970\/128682854-df1c726f-69fb-4fda-bdf9-982f5d841c57.png\">\r\n\r\n\r\nIt's urgent!!!    I hope to get help and solutions from friends, If there is any possible solution, please reply. thanks\r\n","comments":["Would you be able to teel us how many file descriptors are used by both processes?","> Would you be able to teel us how many file descriptors are used by both processes?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/76472970\/128690556-37d90927-ec2d-4f5a-803e-488d1a77bf3e.png)\r\nnode_exporter, prometheus, alertmanager occupy a total of 63 file descriptors","You don't have proxy between prometheus and alertmanager, right?\r\nHave you tried to make the `evaluation_interval` to be larger value, for example `60s`?","<img width=\"580\" alt=\"\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_16285044752474\" src=\"https:\/\/user-images.githubusercontent.com\/76472970\/128692600-2a261bb6-b2a0-4f3c-af5e-563d0b58815c.png\">\r\nThe upper limit of the file descriptor is much larger than the current 63","> You don't have proxy between prometheus and alertmanager, right?\r\n> Have you tried to make the `evaluation_interval` to be larger value, for example `60s`?\r\n\r\nI don't set proxy between prometheus and alertmanager\uff0call operations are in https:\/\/github.com\/prometheus\/prometheus\/issues\/9176\uff1b The same error remains after changing evaluation_interval to 60s\r\n\r\n","10596 seems the user id, not the process ID, so 63 is not the number of files of the processes. Can you provide the accurate number? I think your issue is more related to docker-proxy or docker, because you do not seem to have the same scale as #9057.","Hi @roidelapluie ,\r\n\r\nWe are running cortex and getting the same error mentioned in this issue.\r\n\r\nRuler logs: \r\n```\r\nlevel=error ts=2022-09-13T16:37:20.663333971Z caller=notifier.go:527 user=something-else-for-testing alertmanager=http:\/\/cortex-aggregator-v2-alertmanager-0.cortex-aggregator-v2-alertmanager-headless.cortex-2.svc.cluster.local:9094\/api\/prom\/alertmanager\/api\/v1\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"http:\/\/cortex-aggregator-v2-alertmanager-0.cortex-aggregator-v2-alertmanager-headless.cortex-2.svc.cluster.local:9094\/api\/prom\/alertmanager\/api\/v1\/alerts\\\": EOF\"\r\n```\r\n\r\nAlertmanager logs:\r\n```\r\nlevel=debug ts=2022-09-13T16:33:23.849663134Z caller=cluster.go:329 component=cluster memberlist=\"2022\/09\/13 16:33:23 [ERR] memberlist: Received invalid msgType (80) from=10.115.16.92:58804\\n\"\r\n```\r\n\r\n**Ruler config:**\r\n```\r\nruler:\r\n  external_url: \"\"\r\n  ruler_client:\r\n    max_recv_msg_size: 104857600\r\n    max_send_msg_size: 16777216\r\n    grpc_compression: \"\"\r\n    rate_limit: 0\r\n    rate_limit_burst: 0\r\n    backoff_on_ratelimits: false\r\n    backoff_config:\r\n      min_period: 100ms\r\n      max_period: 10s\r\n      max_retries: 10\r\n    tls_enabled: false\r\n    tls_cert_path: \"\"\r\n    tls_key_path: \"\"\r\n    tls_ca_path: \"\"\r\n    tls_server_name: \"\"\r\n    tls_insecure_skip_verify: false\r\n  evaluation_interval: 30s\r\n  poll_interval: 1m0s\r\n  storage:\r\n    type: configdb\r\n    configdb:\r\n      configs_api_url: http:\/\/cortex-aggregator-v2-configs.cortex-2.svc.cluster.local:80\r\n      client_timeout: 5s\r\n      tls_cert_path: \"\"\r\n      tls_key_path: \"\"\r\n      tls_ca_path: \"\"\r\n      tls_server_name: \"\"\r\n      tls_insecure_skip_verify: false\r\n  rule_path: \/data\/rules\r\n  alertmanager_url: http:\/\/_cluster._tcp.cortex-aggregator-v2-alertmanager-headless\/api\/prom\/alertmanager\r\n  enable_alertmanager_discovery: true\r\n  alertmanager_refresh_interval: 1m0s\r\n  enable_alertmanager_v2: false\r\n  notification_queue_capacity: 10000\r\n  notification_timeout: 10s\r\n  alertmanager_client:\r\n    tls_cert_path: \"\"\r\n    tls_key_path: \"\"\r\n    tls_ca_path: \"\"\r\n    tls_server_name: \"\"\r\n    tls_insecure_skip_verify: false\r\n    basic_auth_username: \"\"\r\n    basic_auth_password: \"\"\r\n  for_outage_tolerance: 1h0m0s\r\n  for_grace_period: 10m0s\r\n  resend_delay: 1m0s\r\n  enable_sharding: true\r\n  sharding_strategy: default\r\n  search_pending_for: 5m0s\r\n  ring:\r\n    kvstore:\r\n      store: consul\r\n      prefix: rulers\/\r\n      consul:\r\n        host: consul:8500\r\n        acl_token: \"\"\r\n        http_client_timeout: 20s\r\n        consistent_reads: false\r\n        watch_rate_limit: 1\r\n        watch_burst_size: 1\r\n      etcd:\r\n        endpoints: []\r\n        dial_timeout: 10s\r\n        max_retries: 10\r\n        tls_enabled: false\r\n        tls_cert_path: \"\"\r\n        tls_key_path: \"\"\r\n        tls_ca_path: \"\"\r\n        tls_server_name: \"\"\r\n        tls_insecure_skip_verify: false\r\n        username: \"\"\r\n        password: \"\"\r\n      multi:\r\n        primary: \"\"\r\n        secondary: \"\"\r\n        mirror_enabled: false\r\n        mirror_timeout: 2s\r\n    heartbeat_period: 5s\r\n    heartbeat_timeout: 1m0s\r\n    instance_id: cortex-aggregator-v2-ruler-6c8b4b988f-4t96b\r\n    instance_interface_names:\r\n    - eth0\r\n    - en0\r\n    instance_port: 0\r\n    instance_addr: \"\"\r\n    num_tokens: 128\r\n  flush_period: 1m0s\r\n  enable_api: true\r\n  enabled_tenants: \"\"\r\n  disabled_tenants: \"\"\r\n```\r\n\r\n**Alertmanager cluster config:**\r\n```\r\nalertmanager:\r\n  data_dir: data\/\r\n  retention: 120h0m0s\r\n  external_url: \/api\/prom\/alertmanager\r\n  poll_interval: 15s\r\n  max_recv_msg_size: 16777216\r\n  sharding_enabled: false\r\n  sharding_ring:\r\n    kvstore:\r\n      store: consul\r\n      prefix: alertmanagers\/\r\n      consul:\r\n        host: localhost:8500\r\n        acl_token: \"\"\r\n        http_client_timeout: 20s\r\n        consistent_reads: false\r\n        watch_rate_limit: 1\r\n        watch_burst_size: 1\r\n      etcd:\r\n        endpoints: []\r\n        dial_timeout: 10s\r\n        max_retries: 10\r\n        tls_enabled: false\r\n        tls_cert_path: \"\"\r\n        tls_key_path: \"\"\r\n        tls_ca_path: \"\"\r\n        tls_server_name: \"\"\r\n        tls_insecure_skip_verify: false\r\n        username: \"\"\r\n        password: \"\"\r\n      multi:\r\n        primary: \"\"\r\n        secondary: \"\"\r\n        mirror_enabled: false\r\n        mirror_timeout: 2s\r\n    heartbeat_period: 15s\r\n    heartbeat_timeout: 1m0s\r\n    replication_factor: 3\r\n    zone_awareness_enabled: false\r\n    instance_id: cortex-aggregator-v2-alertmanager-0\r\n    instance_interface_names:\r\n    - eth0\r\n    - en0\r\n    instance_port: 0\r\n    instance_addr: \"\"\r\n    instance_availability_zone: \"\"\r\n  fallback_config_file: \"\"\r\n  auto_webhook_root: \"\"\r\n  storage:\r\n    type: configdb\r\n    configdb:\r\n      configs_api_url: http:\/\/cortex-aggregator-v2-configs.cortex-2.svc.cluster.local:80\r\n      client_timeout: 5s\r\n      tls_cert_path: \"\"\r\n      tls_key_path: \"\"\r\n      tls_ca_path: \"\"\r\n      tls_server_name: \"\"\r\n      tls_insecure_skip_verify: false\r\n  cluster:\r\n    listen_address: 0.0.0.0:9094\r\n    advertise_address: \"\"\r\n    peers: cortex-aggregator-v2-alertmanager-headless.cortex-2.svc.cluster.local:9094\r\n    peer_timeout: 15s\r\n    gossip_interval: 200ms\r\n    push_pull_interval: 1m0s\r\n  enable_api: false\r\n  alertmanager_client:\r\n    remote_timeout: 2s\r\n    tls_enabled: false\r\n    tls_cert_path: \"\"\r\n    tls_key_path: \"\"\r\n    tls_ca_path: \"\"\r\n    tls_server_name: \"\"\r\n    tls_insecure_skip_verify: false\r\n  persist_interval: 15m0s\r\n```\r\n\r\nCan you provide some pointers on how I can fix this?","+1 for running Cortex and being affected"],"labels":["kind\/more-info-needed"]},{"title":"Option to disable security on Prometheus health endpoints, \/-\/healthy and \/-\/ready","body":"## Proposal\r\nI am using Prometheus in kubernates, and recently enabled basic Auth with version 2.28. But it turn out Prometheus also enables security on health endpoints. Their is no way to safely provide credentials for liveness and readiness probe. Following works but not really the safest option. I think we should have a option to disable security on these endpoints.\r\n\r\n```\r\nlivenessProbe:\r\n      httpGet:\r\n        path: \/-\/healthy\r\n        port: 9090\r\n        httpHeaders:\r\n        - name: Authorization\r\n          value: Basic dXNlcjpwYXNz\r\n```","comments":["This could be a generic functionality to disable authentication on certain endpoints. Will need an additional option in  [web-config.yml](https:\/\/github.com\/prometheus\/exporter-toolkit\/blob\/master\/web\/web-config.yml) and corresponding [check in users.go](https:\/\/github.com\/prometheus\/exporter-toolkit\/blob\/7cd0e9032b5574fa5a5a4d324f7a64a69f290667\/web\/users.go#L61).\r\n\r\nShould this issue be moved to [exporter toolkit repo](https:\/\/github.com\/prometheus\/exporter-toolkit)?\r\n","Thank you, I think we could maybe have an excludeEndpoint configuration option in the web.yml, but I feel like we should discuss this more broadly on the [developers mailing list](https:\/\/groups.google.com\/forum\/#!forum\/prometheus-developers), because e.g. the pushgateway might want to go one step further and even have per-path authentication. ","Would be very helpful if I can disable security for certain endpoints like \"\/-\/ready\" or \"\/-\/healthy\" - specially when running under K8S. Is there already an ongoing discussion on the dev mail list? I was not able to find it ... ","Not yet. ","yes, I think it's necessary","Is there an ETA yet?","Sent https:\/\/github.com\/prometheus\/exporter-toolkit\/pull\/151."],"labels":["priority\/Pmaybe"]},{"title":"Have a configuration to limit the query response JSON size to prevent OOMKilled","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nThe default `max_samples` is 50 million which, is about 800MB for the \"domain model\" where each sample point is about 16 bytes. But, if those 50 million sample points are translated into JSON; then, by my calculation, it would require 46GB to hold the serialized JSON in memory. So typically the host will go OOM while performing serialization. This problem is more of a headache when running in a multi-tenant environment (Cortex, which we are using), because of noisy neighbour problem. \r\n\r\nI am aware that streaming large query result JSON is a no-go based on discussion in https:\/\/github.com\/prometheus\/prometheus\/issues\/3601 and https:\/\/github.com\/prometheus\/prometheus\/issues\/3690.\r\n\r\nHowever, I feel that there should be a knob to limit how much memory can be used per query during serialization such that when a query is attempting to serialize a very large response, Prometheus should reject the request with HTTP 4xx instead of going OOM. \r\n\r\nControlling JSON serialization memory usage using `max_samples` is not idea because there are queries that loads a lot of samples, but return small JSON. \r\n\r\nFor example,  given that I have a lot of time series: \r\n\r\n* I would like `{__name__=\"*\"}[100d:1s]` to be rejected, not because it hits `max_samples` limit, but because it hits a non-existing \"json_response_size\" limit.\r\n\r\n* I would like `count({__name__=~\".*\"})` to work because it loads a lot of samples, but the resulting JSON is small, so if I lower `max_samples` in this case to avoid OOM for the above case,  `count({__name__=~\".*\"})`  may start hitting the `max_samples` limit, which is not ideal. \r\n\r\n","comments":["Thanks, we are streaming json in the API: https:\/\/github.com\/prometheus\/prometheus\/pull\/3536","> Thanks, we are streaming json in the API: #3536\r\n\r\nHmm interesting, but I was looking at https:\/\/github.com\/prometheus\/prometheus\/blob\/03bee3b5df0107117cf2acb228edc3ef05eba4fb\/web\/api\/v1\/api.go#L1469  it looks like everything gets saved into a byte slice before it is written to the response?","Hello,\r\n\r\nWe could try to use a jsoniter EncoderStream or Encoder in a pull request to take a better decision, and maybe then try to have some benchmarks. Are you willing to open such a pull request?\r\n\r\nThanks!\r\n\r\nAlso pinging @pracucci for the cortex side.","I am welling to work on a pull request, but probably can't get to it until after end of Sept. So if someone else want to pick it up before that please do so :-)\r\n\r\n> We could try to use a jsoniter EncoderStream or Encoder in a pull request to take a better decision\r\n\r\nCan you elaborate more on this? What do you have in mind that one should do in the pull request? What \"decision\" and \"benchmarks\" are you thinking? \r\n","Well, I think we could continue to use jsoniter up to when we write the result.\r\n\r\nThe decision and benchmark refers to our Pull request template: Performance improvements would need a benchmark test to prove it.\r\n\r\nI think we might be in a situation a bit more error-prone, so we would need to see if there is a real benefit to this approach. Benchmarks would help.\r\n\r\n",">  it looks like everything gets saved into a byte slice before it is written to the response?\r\n\r\nIn your example query `count({__name__=~\".*\"})`, the byte slice is already very small since the response size is small. \r\n\r\n>  would like count({__name__=~\".*\"}) to work because it loads a lot of samples, but the resulting JSON is small\r\n\r\nI think this is an potential optimization in promQL evaluation (i believe https:\/\/github.com\/prometheus\/prometheus\/pull\/9071 should help). \r\n\r\n","> In your example query count({__name__=~\".*\"}), the byte slice is already very small since the response size is small.\r\n\r\n@darshanime Yea this is true, but that's not the problem in this issue. The problem here is not `count({__name__=~\".*\"})`; the problem is `{__name__=\"*\"}[100d:1s]`.  If I tune `max_samples` to prevent OOM for `{__name__=\"*\"}[100d:1s]`, we may inadvertently affect `count({__name__=~\".*\"})`, which is not desirable.  ","we should probably forbid this at all.\r\n\r\nPrometheus limits to 11k the number of steps, but we should probably enforce that in the subqueries as well.","Agreed, I was surprised when I found out query_rage enforces number of steps but one can workaround it by using subqueries :)\r\n\r\nAnother limit to consider is number of metrics in the result that is returned to the user, to defend select way too many metrics.","> Agreed, I was surprised when I found out query_rage enforces number of steps but one can workaround it by using subqueries :)\r\n> \r\n> Another limit to consider is number of metrics in the result that is returned to the user, to defend select way too many metrics.\r\n\r\nThat should be covered by max_samples I think."],"labels":["priority\/Pmaybe","component\/api"]},{"title":"scrape_duration_seconds and scrape_samples_scraped are empty for some nodes","body":"Hello,\r\n\r\n\r\nWe have prometheus setup to scrape data from various wildfly instances,\r\nFor some of them, the up command give 0 ( node down ) information.\r\n\r\nBut some metrics are being extracted from those nodes !\r\n\r\nWhen we look at the scrape_duration for those we also have values, and the scrape_sample_scraped is also giving 0\r\n\r\nIs this something known ?\r\n\r\nYou can find attached, screenshots of the different commands\r\n![scrape_duration_seconds](https:\/\/user-images.githubusercontent.com\/88373321\/127986550-f09091aa-a12f-444b-9cfa-127c383096a3.JPG)\r\n![scrape_sample_scraped](https:\/\/user-images.githubusercontent.com\/88373321\/127986553-344bae4a-7764-45d6-8baa-825d333082a1.JPG)\r\n![up](https:\/\/user-images.githubusercontent.com\/88373321\/127986556-c68377c0-3147-40e9-a080-bc1ed4d44580.JPG)\r\n","comments":["The scrape duration is always reported, whether the target is up or not. I believe the reason why you're seeing some downed targets have a non-zero `scrape_samples_scraped` is that the target might have been up at some point, and is now down, while others may have always been down, and therefore have never had any samples scraped.","Thanks for your reply,\r\n\r\nWhen looking at the dashboard we have built using grafana, we do have metrics being fetched from those host wich have a 0 in the scrape_sample_scraped ","Can you please provide us relevant parts of your configuration, and a screenshot of the nodes in the targets page? Your Prometheus version would be useful too."],"labels":["kind\/question","kind\/more-info-needed"]},{"title":"Adds support to handle checkpoint corruption errors.","body":"Signed-off-by: Harkishen Singh <harkishensingh@hotmail.com>\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n\r\nFixes: #7530 \r\n\r\nWe check for `wal.CorruptionErr` while backfilling checkpoint and if found, remove the entire `Head` data (WAL + head_chunks) and start a new `Head`.","comments":["Sorry for being inactive on this. I had got stuck in another task. I just added the unit tests for checking checkpoint corruption from Head. Will be happy to add another one if required, for opening DB as an E2E test, but I think the current one should catch it.","Finding this stalled PR during our bug scrub\u2026 This looks it was almost ready for merging. @jesusvazquez could you have a look?","Hitting this again during the bug scrub.\r\n\r\nThis probably should be behind a feature flag or some other way of opting in, because it would delete the WAL, which might be dangerous. But it should be easy to add and get this into mergeable state after a rebase.\r\n\r\n@jesusvazquez you are probably still the best to shepherd this. Are you up to it?","FYI there are 2 other attempts for similar part of code: https:\/\/github.com\/prometheus\/prometheus\/pull\/6129, https:\/\/github.com\/prometheus\/prometheus\/pull\/8205"],"labels":["stale"]},{"title":"Proposal: add generic (EDS) based XDS SD mechanism","body":"## Proposal\r\n**Use case. Why is this important?** [Envoy](https:\/\/envoyproxy.io\/) is CNCF graduated project, and has more or less become a de-facto standard proxy implementation for service meshes and cloud-native Ingresses.\r\n\r\nAside from acting as a proxy implementation, Envoy also contains a vendor-neutral [XDS API](https:\/\/www.envoyproxy.io\/docs\/envoy\/latest\/api-docs\/xds_protocol#xds-rest-and-grpc-protocol). This API is implemented not only by Envoy itself, but also [gRPC](https:\/\/grpc.github.io\/grpc\/cpp\/md_doc_grpc_xds_features.html) and a few other proxies ([MOSN](https:\/\/github.com\/mosn\/mosn), [Quilin](https:\/\/github.com\/googleforgames\/quilkin\/blob\/main\/docs\/xds.md) and a lot of proprietary proxies).\r\n\r\nSupport was recently added for Kuma SD (https:\/\/github.com\/prometheus\/prometheus\/issues\/7919). However, there are a few issues with this approach:\r\n* The SD only has support for a Kuma-specific API, `MonitoringAssignment`. This locks out the vast ecosystem mentioned above.\r\n* The implementation uses an uncommon variant of the API (`REST-JSON polling`), rather than the much more common gRPC streaming. This is far less supported and much less efficient.\r\n\r\nIt would be great to add support for a more vendor-neutral API, [`ClusterLoadAssignment`](https:\/\/github.com\/envoyproxy\/envoy\/blob\/0d2418e9f19d50197ed237dfe5497c715d3c99f0\/api\/envoy\/config\/endpoint\/v3\/endpoint.proto#L32), which is part of the core XDS APIs and implemented by all(?) users of XDS. Additionally, support for gRPC streaming would allow existing control planes to be used in place, and avoid high performance costs.\r\n\r\nSimilar request in the past: https:\/\/github.com\/prometheus\/prometheus\/issues\/6484\r\n\r\ncc @austince @mandarjog @roidelapluie \r\n","comments":["Thanks for the ping @howardjohn + the interest! \r\n\r\nI think adding support for `ClusterLoadAssignment` sounds like a good idea, though it does not seem designed for metric scraping \u2013 do you have a specific proposal for mapping those to Prometheus targets?\r\n\r\nThe current xDS SD implementation is only vendor-specific in terms of the API. It is built to be extended to new APIs easily. Since xDS is just the protocol, any API can be supported on top of it (Kuma, envoy-specific `ClusterLoadAssignment`, etc.), no locking here :) You could even implement the Kuma `MonitoringAssignment` API outside of Kuma, if you'd like.\r\n\r\nFor gRPC, we had a similar conversation (perhaps see [this ML thread](https:\/\/groups.google.com\/g\/prometheus-developers\/c\/w2F6gxYW8U8\/m\/hRrKWRu4BAAJ) for context), but ultimately went with HTTP since it simplified both the mechanics and the dependencies of the SD. @roidelapluie would have the golden insights there.\r\n\r\n ","We looked quite a bit for a standard API that fully expressed something like a Prometheus Target, but did not find anything w\/in Envoy or elsewhere. Would be happy to have more support to define a standard there. Here are some threads in OpenTelemetry around that:\r\n* https:\/\/github.com\/open-telemetry\/opentelemetry-proto\/issues\/263\r\n* https:\/\/github.com\/open-telemetry\/opentelemetry-specification\/issues\/1078#issuecomment-776334709","I would envision the mapping to be like this:\r\n```json\r\n{\r\n    \"@type\": \"type.googleapis.com\/envoy.config.endpoint.v3.ClusterLoadAssignment\",\r\n    \"cluster_name\": \"some-backend\",\r\n    \"endpoints\": [\r\n        {\r\n            \"lb_endpoints\": [\r\n                {\r\n                    \"endpoint\": {\r\n                        \"address\": {\r\n                            \"socket_address\": {\r\n                                \"address\": \"10.36.3.86\",\r\n                                \"port_value\": 8080\r\n                            }\r\n                        }\r\n                    },\r\n                    \"metadata\": {\r\n                        \"__meta_prometheus_job\": \"something\",\r\n                        \"__meta_prometheus_foobar\": \"else\"\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nThe thread suggest `metadata` is not intended to be used for this purpose, but I am not sure I agree - it seems well suited? I think the docs are just a bit misleading (`The endpoint metadata specifies values that may be used by the load balancer to select endpoints in a cluster for a given request. The filter name should be specified as envoy.lb.`), because it is documenting how Envoy the proxy uses them. There is a common detail\/terminology confusion between Envoy-the-proxy, XDS transport protocol (ie sending Discovery{Request,Response}), and the XDS apis (ie `ClusterLoadAssignment`) - I think in this case they are discussing Envoy-the-proxy's usage of `metadata` - it seems perfectly valid to use `metadata` for other usages as well (we do this in Istio at least, and have never had issues).\r\n\r\n> ultimately went with HTTP since it simplified both the mechanics and the dependencies of the SD\r\n\r\nMy reading of the thread is there is this primary concern:\r\n\r\n> hould we have a generic SD, I would not like that much GRPC as would\r\n> bring many dependencies, that we already had issues with. Additionally,\r\n> we have more chances to use a more-established standard like HTTP (with\r\n> long poll). Especially since the communications only need to be one-way.\r\n\r\ngRPC is currently a transitive dependency (from the XDS import for one, among others like k8s). I don't know how to exactly quantify the impact of moving it from an indirect -> direct dependency, but from importing it explicitly it seems there are no real changes to `go.sum` (and go.mod just has the new grpc line). So its plausible its not a huge new dependency in terms of go imports - it is of course a new runtime dependency. I think a lot of the complexity is already a sunk cost with the go-control-plane import for kuma_sd.\r\n\r\nRegarding the one-way part - one of the biggest benefits of the gRPC streaming is that the control plane can push updates. So instead of polling every Xs, and getting the full state-of-the-world (which may be very large!! like O(megabytes)), we can connect once and then just get incremental updates as endpoints change.\r\n\r\ngRPC is less standard than HTTP, but its not horribly bespoke - it is mostly just \"protobuf over HTTP\/2\" if you squint your eyes :slightly_smiling_face: .","@howardjohn Yeah :\/ the xDS API <> envoy-proxy line is not the most straightforward. It seems like using metadata for this would work, though would be tougher to implement\/validate as it relies on arbitrary keys in metadata being present.\r\nDo you envision this being pulled from the main xDS API? How does one distinguish a request for  `ClusterLoadAssignment` the load balancer spec vs. `ClusterLoadAssignment` the metric scrape target spec? \r\n\r\nFor gRPC, I agree with you completely that incremental updates would be great! Here are a couple more threads on the gRPC\/Prometheus relationship that you might be interested in:\r\n* https:\/\/github.com\/prometheus\/prometheus\/issues\/7836\r\n* https:\/\/github.com\/prometheus\/prometheus\/issues\/8414\r\n\r\nIn the context of the HTTP polling in the current kuma_sd, it is important to note that it uses HTTP long-polling (thanks @roidelapluie  :)) to reduce the polling requests, and ofc uses the version of the current resources to only send back the state of the world when that changes. This can be O(megabytes) like you mentioned but, with versioning, it is much more efficient than the same over the http_sd. \r\n\r\n\r\n","> How does one distinguish a request for ClusterLoadAssignment the load balancer spec vs. ClusterLoadAssignment the metric scrape target spec?\r\n\r\nIts _possible_ a control plane would not care, and just always set all the labels. For scalability reasons, that seems like it would always work - certainly for us that would cause scalability concerns to add all the prom labels for every client.\r\n\r\nThe XDS requests contain metadata about the client in the `node` - there is arbitrary `struct` config as well as user agent, etc. So somewhere in there could specify prometheus and a control plane could choose to respond differently in that case","Hello,\r\n\r\nThis is indeed kuma-sd, within a package that can do XDS REST-JSON polling. Adding different types should be possible within the same go package -- if it supports the REST-JSON polling.\r\n\r\nWe went for HTTP polling for now because we are thinking about what we will do with our protobuf stacks, and I did not want a service discovery to interfere with this until we have a proper solution for the protobuf we use in the rest of Prometheus. I also did not want to take the kuma PR in hostage pending this proto work.","I think we may also need to tweak the api for simplicity.\r\nEDS does not allow wildcard resource request. If not, the controlplane can choose to still implement a wildcard eds and specifically deltaEds."],"labels":["priority\/Pmaybe","component\/service discovery","kind\/feature"]},{"title":"Support string typed metrics","body":"## Proposal\r\n**Use case. Why is this important?**\r\n\r\nThere are cases where the interpretation of other metric values depends on mode-like properties. For example, on the Z system machines, logical partitions can dedicate or share the processors. That is represented as a string enum typed resource property \"processor-mode\" with values \"shared\" or \"dedicated\". That mode is important to know if you want to calculate derived metrics or define alerts based on other integer or float-typed metrics around the processors (I don't go into the details here). In our case, the processor mode of a logical partition can be changed over time, so it is appropriate to transfer it along with the other metrics whose interpretation depends on it.\r\n\r\nAnother use case is status values that someone wants to put into the metric store for correlating other metrics with it, or simply for long term storage of the status. For example, if a logical partition is in the stopped state, it still has processors assigned in its definition, but they are now used (time-shared) for other partitions. That needs to be taken into account when calculating derived metrics, and therefore it is important to have status information available along with the metrics.\r\n\r\nThe use cases above are not unique for Z systems. I assume that similar use cases exist in almost any complex environment, e.g. in container based environments, or in cloud services.\r\n\r\nI did read issue #2227, but this is now 5 years old and there has been development since then. Other monitoring solutions meanwhile support string-typed metrics. Here are just some that I stumbled across:\r\n* SysDig: https:\/\/docs.sysdig.com\/en\/metrics-dictionary.html.\r\n* Nagios supports enumerated state values (https:\/\/assets.nagios.com\/downloads\/nagioscore\/docs\/nagioscore\/3\/en\/statetypes.html).\r\n* Zabbix supports items that can have \"short character data\" as a value (https:\/\/www.zabbix.com\/documentation\/current\/manual\/config\/items\/item).\r\n\r\n**Alternatives**\r\n\r\nI do understand that we can map string enum typed metrics into integer values, and that is also how we will start out supporting this in our exporter (https:\/\/github.com\/zhmcclient\/zhmc-prometheus-exporter), but it is just way more natural to be able to use the string enum values directly, and avoids having to document long mapping lists for all kinds of status or mode properties.\r\n\r\nAnother dimension of alternatives would be to transport string typed values from the monitored system using another monitoring solution that supports this, but that puts the burden on the user to now maintain two metric gatherer environments on each system, and it is also a slippery slope since it opens the door to future consolidation in the \"wrong\" direction.","comments":["Thanks for this. I have been speaking with other people and it sounds like it would open the door for event based monitoring, if you expose strings with timestamps, which prometheus is not. If it is an enum, you could also map the state to particular integers.\r\n\r\nFor a more in-deep decision, I will link this issue in our next developers summit talks (where anyone can participate) to have broader discussions.","I am aware that the below are not direct replies, but they still might be useful:\r\n* OpenMetrics talks a bit about how to do ENUMs in Prometheus: https:\/\/github.com\/OpenObservability\/OpenMetrics\/blob\/main\/specification\/OpenMetrics.md#stateset\r\n* Grafana Loki can store arbitrary strings and blobs, which can be queried with a language based on PromQL; and turn those queries into metrics","That would be great as the current no-\"implementation\" is suboptimal for Dashboarding and storage count\r\nhttps:\/\/github.com\/prometheus-community\/windows_exporter\/issues\/938 "],"labels":["priority\/Pmaybe"]},{"title":"Making durations and number literals the same","body":"Design doc: https:\/\/docs.google.com\/document\/d\/1LaZfknXuuRWGtQSbULoMtclQhuLUMrdwg15wMvoBvCQ\/edit#\r\n\r\n\r\nSigned-off-by: darshanime <deathbullet@gmail.com>\r\n\r\n\r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n","comments":["Thank you. I expect that Beorn will want to look at this one, but you can expect some delays due to the summer period.","According to your design doc, the following should work: `rate(prometheus_http_requests_total[0.25])`, but instead I get this error: `Error executing query: 1:37: parse error: strconv.Atoi: parsing \"0.25\": invalid syntax`. Otherwise seems good!","I tried `predict_linear(foo[5h], 1h)`, but I get:\r\n\r\n    parse error: trailing commas not allowed in function call args\r\n\r\nI also left some comments on the design doc on questions I had.","One thing I'm interested in is how this will affect the printing of PromQL expressions once they have been parsed.\r\n\r\nIt would be sad if the input was:\r\n\r\n    predict_linear(foo[1h], 4h)\r\n\r\n...and we could only serialize it out again as:\r\n\r\n    predict_linear(foo[1h], 14400)\r\n\r\nNot sure if this is the plan already, but would it make sense to store the original tokens in the parsed AST somehow so that we can render them out again in a way that makes most sense to the user?","> One thing I'm interested in is how this will affect the printing of PromQL expressions once they have been parsed.\r\n> [\u2026]\r\n\r\nThat's a good point, but perhaps we can solve that later, once the main functionality is implemented.\r\n\r\nI think the comments above( https:\/\/github.com\/prometheus\/prometheus\/pull\/9138#issuecomment-897756494 , https:\/\/github.com\/prometheus\/prometheus\/pull\/9138#issuecomment-897615611 ) are real issues that need to be fixed.\r\n","thank you, will address \ud83d\udc4d \r\nmarking as draft to avoid notifications in the meantime... ","FYI, [VictoriaMetrics](https:\/\/github.com\/VictoriaMetrics\/VictoriaMetrics\/) supports using durations instead of numeric values and vice versa in [MetricsQL](https:\/\/docs.victoriametrics.com\/MetricsQL.html). See, for example, how does `predict_linear(sum(rate(vm_http_requests_total[300])) offset 3600, 3h)` work in [VictoriaMetrics playground](https:\/\/play.victoriametrics.com\/select\/accounting\/1\/6a716b0f-38bc-4856-90ce-448fd713e3fe\/prometheus\/graph\/?g0.range_input=30m&g0.step_input=3.886&g0.relative_time=last_30_minutes&g0.tab=chart&g0.expr=predict_linear(sum(rate(vm_http_requests_total%5B300%5D))%20offset%203600%2C%203h)). It would be great if Prometheus could support this too.","We discussed this during our bug scrub. I think my point above is still valid and hasn't been addressed. @darshanime are you still up to working on this?\r\n\r\nThis general topic comes up quite often, so we should finish this up one way or another.","Apologies for the _delay_, I've updated the design doc and the PR to limit the functionality to just allowing number literals to be used for indicating duration. ","Thank you very much. I'll have a look ASAP.","Actually, I'll first look at the design doc to check if we are on the same page there.","As you can probably already guess from my comments on the design doc: This isn't addressing yet the reverse case, using a duration literal as a number.\r\n\r\nExamples:\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/5609886\/c834b112-2560-4c7e-98b6-c7848e7eebf2)\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/5609886\/15e9040e-2a3f-4c67-ba06-c2c6ee8f3d74)\r\n\r\nAnother point is that the frontend code hasn't been updated. So while `rate(process_cpu_seconds_total[300])` works, the UI shows squiggly lines and red numbers because it sees it as a syntax error:\r\n![image](https:\/\/github.com\/prometheus\/prometheus\/assets\/5609886\/6c4a564c-9ed4-480d-a942-1896f8c60ae0)\r\n\r\nDo you think you can address these two issues?\r\n","pipeline is red due to an unrelated issue, raised https:\/\/github.com\/prometheus\/prometheus\/pull\/13704 for that","@beorn7, the updated grammar accepts number literals (4.018) whereever durations were earlier accepted (4s18ms), and not necessarily the other way around. For example, `foo @ 3.3335s` doesn't parse, as expected.","Thanks for your updates. I'll review ASAP.","> Feedback would be welcome, especially from old hands like @juliusv and @roidelapluie\r\n\r\nI agree that properly putting a feature flag around everything would probably be annoying to implement, especially on the frontend. It'd be nice if we could at least have a feature flag that raises an error if it's not set when someone tries to use the new features, but since we basically lose all the information about whether something was a number or duration literal at the lexer level already, even that may be annoying to add. So I'd be ok with 2 as well. At least we should still call it out as experimental for a little while, in case anyone finds serious issues with it.","> The timestamp is the duration after 1970-01-01 0:00 UTC. So you could say foo @ 30y if you want to point to 2000-01-01 or something\u2026\r\n\r\nI have to correct myself as `30y` is just 365*30 days in PromQL, so it doesn't take leap years into account. But my fundamental line of argument still stands: Duration syntax doesn't always make sense for each and every number literal, but we shouldn't even start judging about it. The idea here is to allow it everywhere, so we shouldn't exclude the `@` modifier as the  only exception.","> The idea here is to allow it everywhere\r\n\r\nI skipped it originally because `foo @ 3s` did not \"make sense\" and I thought we need not make holes in the type system if we can help it. Made the changes now, agree with your point about being consistent everywhere. \r\n\r\n> Do you know of any other cases where a number cannot be replaced by a duration?\r\n\r\nThere should be no such place now, and we should be able to use duration and number interchangeably...\r\n\r\n> the documentation needs a corresponding update.\r\n\r\nDoes this mean making a change in `CHANGELOG.md` or someplace else too? \r\n\r\n> we need to update the frontend part as well so that the frontend doesn't put squiggly lines underneath correct syntax...\r\n\r\nCan we do this as part of a separate PR?\r\n\r\n","> > Do you know of any other cases where a number cannot be replaced by a duration?\r\n>\r\n> There should be no such place now, and we should be able to use duration and number interchangeably...\r\n\r\nExcellent. Thank you very much. I'll have a detailed look in the coming week, but it seems we are now where we want to be.\r\n\r\n> > the documentation needs a corresponding update.\r\n>\r\n> Does this mean making a change in `CHANGELOG.md` or someplace else too? \r\n\r\nThe CHANGELOG.md update is only created during the release, but there is \"real\" documentation for PromQL. Once this change is declared stable, we have to go through the whole body of PromQL documentation and make sure that numbers and durations are the same everywhere. For now, this will be an experimental change (see Julius's comment), so we only have to extend the definition of [float literals](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/basics\/#float-literals) and [durations](https:\/\/prometheus.io\/docs\/prometheus\/latest\/querying\/basics\/#time-durations) by something like \"As of version 2.52, durations can also be represented by float values, implying the number of seconds. This is an experimental feature and might still change.\" or \"As of version 2.52, float values can also be represented using the syntax of durations, where the duration literal is converted into a float value corresponding to the number of seconds the duration literal represents.\" (I guess more wordsmithing should apply here, just trying to point in the right direction. Also, an example might be good.) The file to change is https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/docs\/querying\/basics.md .\r\n\r\n> > we need to update the frontend part as well so that the frontend doesn't put squiggly lines underneath correct syntax...\r\n>\r\n> Can we do this as part of a separate PR?\r\n\r\nTechnically yes, but we should definitely minimize the duration of having inconsistent version of the UI code and the PromQL implementation in the backend in the main branch.\r\n\r\nIf you plan to work on the UI code yourself, it would indeed be best to add commits to this PR (but having UI code and backend code in different commits would be desired).\r\n\r\nIf you cannot work on the UI code, we should ask our React experts for help. If one of them works on this, they could create a separate PR, but we should merge both PRs at about the same time.\r\n\r\nSo the question is: Do you plan to work on the UI code yourself?\r\n"],"labels":["stale"]},{"title":"React UI: Millisecond durations cause x-axis to be set to ~26 years","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\n\r\nI set a range of 1ms.\r\n\r\n**What did you expect to see?**\r\n\r\nMetrics over a range of 1ms.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nMetrics over a range of ~26 years:\r\n\r\nhttps:\/\/prometheus.demo.do.prometheus.io\/graph?g0.expr=prometheus_http_requests_total&g0.tab=0&g0.stacked=0&g0.range_input=1ms\r\n\r\n**Environment**\r\n\r\n* Prometheus version:\r\n\r\n\tIn this example, `2.27.0`, but it's present in every React UI release as far as I can tell.\r\n\r\nI discovered this while working on #8977, and now that it's going to be released I expect a lot more users will run into this bug. I'm not sure if we can (reasonably) or should do anything about it, it's more of just a minor annoyance, I don't think we even allow that level of precision in the graph. But, I thought I'd make a note of it here.","comments":["In IRC we discussed a few solutions to this newfound ability to \"hack\" time...\r\n\r\ncc @juliusv"],"labels":["kind\/bug","component\/ui","priority\/P3"]},{"title":"Change the definition of ready or create a new endpoint with a different definition","body":"At the moment we use a readiness probe on `http-get http:\/\/:web\/-\/ready` which makes sense. This is actually from the Prometheus Operator but I assume this is the de facto standard to use it. \r\n\r\nNow regarding 0 downtime deployments, we face an issue. Even though using a stateful set with 2 replica's, we experience hiccups\/downtime. This is caused by two factors:\r\n- The LB\/svc but also other components that do discovery (such as Thanos) might just not be as quick to spot the new pod.\r\n- The newly made replica has not scraped all the targets yet, causing missing data.\r\n\r\nI thought to be clever and create an issue at the Prometheus Operator here: https:\/\/github.com\/prometheus-operator\/prometheus-operator\/issues\/3711 to implement .spec.minReadySeconds. However this is actually not a k8s thing yet. See: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/65098 - that should come in k8s 1.22 (?)\r\n\r\nSo to solve this I'm thinking about a combination:\r\n1: We need to allow Prometheus to be 'not ready' and start scraping. I.e. make sure to have done 3 rounds of scrapes (or whatever)\r\n2: We need to allow an extra period of 'ready state' to allow other components (such as Thanos) to discover it. This IMO only can be done at k8s 1.22 when we actually have such a feature.\r\n\r\nThis would (I think) solve any issues with deploying and allows us to only run 2 replica's to have 0 downtime deployments.\r\n\r\nFor point number 1, it would mean a new 'ready endpoint' by either changing the definition of `ready` or create a new endpoint with a different definition of `ready` \r\n\r\nOne definition can be: It's healthy and started\r\nThe other one can be:  It's healthy, started and has done a few scrape rounds so there is actually data present\r\n\r\n","comments":["Hello, thanks for your pull request.\r\n\r\nI think this complexity would belong to kubernetes and not in Prometheus, as in the issue linked. I do not think we need additional endpoints in Prometheus for this. An alternative would be to have a custom readinessprobe that does a curl \/-\/ready && sleep 30s","> I think this complexity would belong to kubernetes and not in Prometheus, as in the issue linked. \r\n\r\nFor the best outcome, it would consist of effort in Kubernetes and in Prometheus. I see features in Prometheus complementary to other techniques. Like the health\/ready endpoints. \r\n\r\n> An alternative would be to have a custom readinessprobe that does a curl \/-\/ready && sleep 30s\r\n\r\nI mean, that could work. However I strongly feel that this is not a robust solution. More a workaround. \r\n\r\nThe point is that there is only one place which knows the exact state. That is Prometheus. It does however also need the feature from k8s. Yet I think it's so important to know if Prometheus has actual \"data ready\", rather than \"being ready\" itself. Personally, I would favour 'data ready' over anything. Perhaps I'm the odd one here though :D \r\n\r\n","The data is queryable when \/-\/ready is OK.\r\n\r\nData ready is a big word and it is out of scope here: what if Prometheus can not scrape targets? What if you have a scrape interval of 5m? This would add a 20 min delay on startup.\r\n\r\nI think that `minReadySeconds` of 5m should fix your specific needs.","Wow sorry for the super late response here. Totally missed this one :)\r\n\r\n> The data is queryable when \/-\/ready is OK.\r\n\r\nPrometheus is ready and technically you can run queries but there won't be any data for the period Prometheus was not online.\r\nWhen running in HA, that's exactly the moment the only Prometheus that _has_ that data, will go offline due to the rolling update (since there is now a Prom online). \r\n\r\n> Data ready is a big word and it is out of scope here: what if Prometheus can not scrape targets? What if you have a scrape interval of 5m? This would add a 20 min delay on startup.\r\n\r\nI agree on all parts, just with a different outcome. A part of the HA setup makes it absolutely fine to have a rollover which adds a delay of 20 minutes if for some reason needs that to be actually useful for the use-case. IMO 20 min delay seems quite unrealistic though but regardless; yes I would actually prefer this.\r\n\r\n> I think that `minReadySeconds` of 5m should fix your specific needs.\r\n\r\nIt certainly helps and it should be an easy implementation that gets us a lot further. However, it's just an estimate and a lucky guess on how long Prometheus should take to be ready and consists of data. \r\n\r\nTo answer your question regarding the \r\n> what if Prometheus can not scrape targets? \r\nAlright, I think it should be more about 'have I done scrapes' - I.e. if Prometheus has iterated over all the scrape targets (maybe for n-amount of times) -> than we consider it \"data ready\". If Prometheus cannot get to your targets, than that's your data.\r\n\r\nI guess the tl;dr part here is: Prometheus is the source of truth on it's state. Using any other method is just hoping for the best. "],"labels":["priority\/Pmaybe"]},{"title":"Move some of storage config from command line to config file","body":"## Proposal\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/pull\/8974 introduced a `storage` section in the config file and also added mechanism to reload the config in TSDB without needing a restart. I think we can (and should) move some of the CLI flags to the config file so that it can be reloaded without having to restart Prometheus (hence the WAL replay).\r\n\r\nThe config file would take precedence over the CLI flags, and deprecate the CLI flags in favour of config file.\r\n\r\nSome of the flags that I am proposing to move to file (not including any hidden flags here):\r\n\r\n* `--storage.tsdb.retention.time=STORAGE.TSDB.RETENTION.TIME`\r\n* `--storage.tsdb.retention.size=STORAGE.TSDB.RETENTION.SIZE`\r\n* `--storage.tsdb.allow-overlapping-blocks` - this only to allow overlapping blocks if it is disabled. If it's already allowed, config reload should not disable it again. Have to restart Prometheus to disable so that overlapping blocks fail to startup if disabled.\r\n\r\n~Maybe also~\r\n~* `--storage.tsdb.wal-compression`~\r\n\r\n@cstyan do you think we can move some of these below flags to config file too?\r\n```\r\n--storage.remote.flush-deadline=<duration>  \r\n--storage.remote.read-sample-limit=5e7  \r\n--storage.remote.read-concurrent-limit=10  \r\n--storage.remote.read-max-bytes-in-frame=1048576\r\n```\r\n\r\ncc @bwplotka ","comments":["Hello,\r\n\r\n:+1: for TSDB options\r\n\r\n--storage.tsdb.wal-compression is pretty much an internal stuff and users should generally not be aware of that, I would not put it in the config file.","> --storage.tsdb.wal-compression is pretty much an internal stuff and users should generally not be aware of that, I would not put it in the config file.\r\n\r\nThen I guess we should hide it from `-h` output","> > --storage.tsdb.wal-compression is pretty much an internal stuff and users should generally not be aware of that, I would not put it in the config file.\r\n> \r\n> Then I guess we should hide it from `-h` output\r\n\r\nYou still need it if you want to keep you WAL backwards compatible with pretty old Prometheus releases, but that's it I think. There was a bug that we fixed recently about very large records: #8790, but that's the only case we have ever needed to disable it, and it is no longer needed since the fix.\r\n\r\nOverall yes I think in next releases we could hide the flag.","Opened https:\/\/github.com\/prometheus\/prometheus\/pull\/9117 which should take care of hiding it","@roidelapluie The TSDB will read both compressed and un-compressed WAL files. The flag is only useful if you wanted to disable compression and roll back to <= 2.19.0.","> @roidelapluie The TSDB will read both compressed and un-compressed WAL files. The flag is only useful if you wanted to disable compression and roll back to <= 2.19.0.\r\n\r\nYes that is what I tried to explain, apparently badly :)","Honestly I'd forgotten that `--storage.remote.flush-deadline=<duration> ` was even a CLI flag and not a config file option. TBH it feels to me like that should be a per queue configuration option with the same default as the current CLI flag. @csmarchbanks any thoughts?\r\n\r\nFor the rest of the `storage.remote` options @bwplotka or someone else with more remote read context should chime in.","Hi peeps!\r\n@codesome I would try to implement this one, can I?","@nicolastakashi yes, please go ahead! Thanks.\r\n\r\nSince remote flags are not decided yet, can you only do 3 `--storage.tsdb.xx` flags for now for now?","@codesome sure right!\r\n\r\nJust to clarify, these flags should be placed on the global config file or should I create a new one?","@nicolastakashi Global config file. You will already find a place for `storage`, these flags should be as `storage -> tsdb -> xx` in the yaml.","Nice!\r\nThank you for the explanation, I'll take a look on that and as soon as possible open a MR","I agree with Callum about flush-deadline, that should be in the config file for sure. The read ones seem reasonable to put in the config file to me, but I would also appreciate someone who uses remote read more than me to chime in.","We just hit this issue as well.\r\nhttps:\/\/forum.cloudron.io\/topic\/5633\/add-option-for-users-to-add-starting-parameters?_=1631174530839\r\n\r\nSeeing this merged would be awesome.\r\n\r\nBlocking this PR as well should be changes to the doc https:\/\/github.com\/prometheus\/docs right?\r\n\r\nIf I can be of assistance let me know.","How about allowing these variables to be set by environment variables? That way we could set the retention time for a docker based deployment without having to copy out the default command line arguments.","@codesome what's the update on this? Is someone still working on this if not I'll like to pick this one.","@ivange94 There is #9158 open by @nicolastakashi which seems to have gone stale. I have pinged the author on that PR and will wait for any response for a week. If there is no response for a week (or if the author says they are not working on it anymore), you can consider it to be open for you to pick.","@codesome Is someone still working on this? If not, can I take this up?","Hey @tashif-hoda my priorities shift a bit and I wont able to continue this for now.\r\nFeel free to take over it if you wanna.","@codesome @nicolastakashi One clarification regarding \"--storage.tsdb.allow-overlapping-blocks\". This configuration is now set to True by default and the flag is marked as deprecated and hidden. Do we still need to port this to the config file?"],"labels":["help wanted","low hanging fruit","component\/tsdb","kind\/feature"]},{"title":"Evaluation aligned subqueries","body":"Fixes #9767 \r\n\r\n<!--\r\n    Don't forget!\r\n\r\n    - Please sign CNCF's Developer Certificate of Origin and sign-off your commits by adding the -s \/ --sign-off flag to `git commit`. See https:\/\/github.com\/apps\/dco for more information.\r\n\r\n    - If the PR adds or changes a behaviour or fixes a bug of an exported API it would need a unit\/e2e test.\r\n\r\n    - Where possible use only exported APIs for tests to simplify the review and make it as close as possible to an actual library usage.\r\n\r\n    - No tests are needed for internal implementation changes.\r\n\r\n    - Performance improvements would need a benchmark test to prove it.\r\n\r\n    - All exposed objects should have a comment.\r\n\r\n    - All comments should start with a capital letter and end with a full stop.\r\n -->\r\n\r\nI'm pretty sure this works judging by my testing with `offset` compared to the `::` subquery.","comments":["I might have missed some discussion. Could you please link here any discussion around this? Thanks!","This was discussed extensively in last week dev summit. https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit","Another TODO: This has to be locked behind a feature flag.","Yeah, I haven't got the stuff yet","I've basically duplicated the existing subquery tests with the new syntax. While most of the results are the same, the differences you'll see lie in the new syntax returning one less sample. @codesome pointed this line of code out earlier, which I added when I saw an extra sample in the UI, for example, 5 samples from a `[1m::15s]` query. I'm pretty sure using the quotient of the range and the interval is the correct behavior, but for some reason internally (existing) subqueries return 1 extra, so I'm not sure.","I went with `promql-eval-aligned-subqueries` for the feature flag, as opposed to the singular `promql-eval-aligned-subquery`. I think it sounds better, but that's a matter of opinion.","I removed `+ newEv.interval` and updated any conflicting test cases, so now things should be more normal.","The build failure seems unrelated. Perhaps merging in main would help?\r\n\r\nI'll review this ASAP, but I would appreciate @codesome's opinion, too.","Sorry for the waiting time. Both @codesome and myself are working against a deadline right now, but we'll get to this ASAP.","Absolutely no problem, please take your time. I'm in no particular rush to get this in.","There is an interesting failure that you might face when you add test cases in `promql\/testdata` because of range queries, which will require a little involved solution. Let me explain with an example:\r\n\r\nTake range query with `start=100s,end=300s,step=100s`, and the query `rate(mymetric[10s::3s])`.\r\n\r\nThese are the timestamps that `mymetric[10s::3s]` should be resulting in at each step:\r\n```\r\n@100s -> [  90s,  93s,  96s,  99s ]\r\n@200s -> [ 190s, 193s, 196s, 199s ]\r\n@300s -> [ 290s, 293s, 296s, 299s ]\r\n```\r\n\r\nBut when `mymetric[10s::3s]` is evaluated only once for the entire range in the current PR, it will produce\r\n```\r\n[ 90s, 93s, 96s, 99s, 102s, ... , 189s, 192s, 195s, 198s, 201s, ... , 288s, 291s, 294s, 297s, 300s, ...  ]\r\n```\r\nand use them, which will be wrong.\r\n","@LeviHarrison WDYT about https:\/\/github.com\/prometheus\/prometheus\/pull\/9114#issuecomment-920784607?","Sorry, I haven't had time to fully think it over as of late. I'll get to it when I can. Thanks for the reminder.","I've started to add the tests, although only a few so far. I haven't seen any unnormal behavior that would indicate the issue in https:\/\/github.com\/prometheus\/prometheus\/pull\/9114#issuecomment-920784607, but we'll see.","Something weird has been happening when working on these tests: the received result of a query will be one number, and upon changing the expected result to be that number, the received result will become a different number, and so on.\r\n\r\n```\r\nload 10s\r\n  metric1 0+1x1000\r\n  metric2 0+2x1000\r\n  metric3 0+3x1000\r\n\r\neval instant at 1010s sum_over_time(metric1[30s::11s] offset 11s)\r\n  {} 294\r\n```\r\n\r\n```shell\r\nexpected 294 for {} but got 293\r\n```\r\n\r\n```\r\nload 10s\r\n  metric1 0+1x1000\r\n  metric2 0+2x1000\r\n  metric3 0+3x1000\r\n\r\neval instant at 1010s sum_over_time(metric1[30s::11s] offset 11s)\r\n  {} 293\r\n```\r\n\r\n```shell\r\nexpected 293 for {} but got 294\r\n```","Also if you can, it would be nice to add new tests around the autocomplete feature here : \r\n* https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/web\/ui\/module\/codemirror-promql\/src\/complete\/hybrid.test.ts#L516-L520\r\n* https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/web\/ui\/module\/codemirror-promql\/src\/complete\/hybrid.test.ts#L1170-L1180","> Something weird has been happening when working on these tests\r\n\r\nThat is weird. Were you able to figure out what is happening?\r\n\r\nAnd let me know if you need help in forming test cases for the issue I described above (you can consider taking the example from https:\/\/github.com\/prometheus\/prometheus\/pull\/9114#issuecomment-920784607, and to make it easy to detect this you can add test cases in the encing_test.go files that are explicitly doing _range_ queries)","> > Something weird has been happening when working on these tests\r\n> \r\n> That is weird. Were you able to figure out what is happening?\r\n\r\nNote that those PromQL tests for `*.test` files internally also do range queries in addition to instant queries to test some constraints.","> That is weird. Were you able to figure out what is happening?\r\n\r\nNope, although I haven't taken a closer look. I'll probably just proceed adding non-flakey test cases for now.\r\n\r\n> And let me know if you need help in forming test cases for the issue I described above\r\n\r\nThat would be great, thanks. I added one to `engine_test.go` but I'm not sure if it was formed correctly or if the result is what we're expecting.","@Nexucis one of the autocomplete tests is failing with an unexpected result:\r\n\r\n```\r\n1) analyzeCompletion test\r\n       autocomplete duration for an evaluation-aligned subQuery 2:\r\n\r\n      AssertionError: expected [ { kind: 9 } ] to deeply equal [ { kind: 6 }, { kind: 10 } ]\r\n      + expected - actual\r\n\r\n       [\r\n         {\r\n      -    \"kind\": 9\r\n      +    \"kind\": 6\r\n         }\r\n      +  {\r\n      +    \"kind\": 10\r\n      +  }\r\n       ]\r\n\r\n      at Context.<anonymous> (src\/complete\/hybrid.test.ts:557:50)\r\n      at processImmediate (node:internal\/timers:464:21)\r\n```","Just a note from the bug scrub: This is still super relevant, but \"harder than it looks\". It's definitely high on my list, but any help will be very welcome.","Hitting this again in the bug scrub. It's still on my list, and it's still relevant. Contributors welcome."],"labels":["not-as-easy-as-it-looks"]},{"title":"Potentially wasted space when storing chunk files on Btrfs","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\n\r\nRan Prometheus in a container using Podman and a Btrfs volume for storage.\r\n\r\n`$ podman run --name prometheus_test --net host -v prometheus_test:\/prometheus:Z -v \/home\/$USER\/config.yml:\/prometheus\/prometheus.yml prometheus`\r\n\r\n**What did you expect to see?**\r\n\r\nA lower disk usage than 256M for every chunk file.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nA disk usage of precisely 256M for every chunk file (as reported by `compsize`).\r\n\r\n```\r\n$ ls -l \/home\/$USER\/.local\/share\/containers\/storage\/volumes\/prometheus_test\/_data\/\r\ntotal 4\r\ndrwxr-xr-x. 1 165533 165533    60 Jul 21 03:43 01FB3BPMXJ9HH7KS5AKV5HCNYY\r\ndrwxr-xr-x. 1 165533 165533    60 Jul 21 05:00 01FB3G3MMMY0TB7WWMBJWR1KZY\r\ndrwxr-xr-x. 1 165533 165533    60 Jul 21 07:00 01FB3PZBWMNTTVEG5NA41D83BJ\r\ndrwxr-xr-x. 1 165533 165533    60 Jul 21 09:00 01FB3XV34K0WNACCYBYCD6Q4AR\r\ndrwxr-xr-x. 1 165533 165533    60 Jul 21 11:00 01FB44PTCGS3RKG2K8K2PQRSJX\r\ndrwxr-xr-x. 1 165533 165533    24 Jul 21 11:00 chunks_head\r\n-rw-r--r--. 1 165533 165533     0 Jul 21 00:42 lock\r\n-rwx------. 1 curry  curry      0 Jul 21 00:42 prometheus.yml\r\n-rw-r--r--. 1 165533 165533 20001 Jul 21 00:42 queries.active\r\ndrwxr-xr-x. 1 165533 165533   102 Jul 21 11:00 wal\r\n\r\n$ ls -l \/home\/$USER\/.local\/share\/containers\/storage\/volumes\/prometheus_test\/_data\/01FB3XV34K0WNACCYBYCD6Q4AR\/chunks\r\ntotal 200\r\n-rw-r--r--. 1 165533 165533 204698 Jul 21 09:00 000001\r\n\r\n$ sudo compsize \/home\/$USER\/.local\/share\/containers\/storage\/volumes\/prometheus_test\/_data\/01FB3XV34K0WNACCYBYCD6Q4AR\r\n\/chunks\/000001 \r\nProcessed 1 file, 1 regular extents (1 refs), 0 inline.\r\nType       Perc     Disk Usage   Uncompressed Referenced  \r\nTOTAL      100%      256M         256M         200K       \r\nnone       100%      256M         256M         200K\r\n\r\n$ ls -l \/home\/$USER\/.local\/share\/containers\/storage\/volumes\/prometheus_test\/_data\/01FB44PTCGS3RKG2K8K2PQRSJX\/chunks\r\ntotal 196\r\n-rw-r--r--. 1 165533 165533 198213 Jul 21 11:00 000001\r\n\r\n$ sudo compsize \/home\/$USER\/.local\/share\/containers\/storage\/volumes\/prometheus_test\/_data\/01FB44PTCGS3RKG2K8K2PQRSJX\/chunks\/000001 \r\nProcessed 1 file, 1 regular extents (1 refs), 0 inline.\r\nType       Perc     Disk Usage   Uncompressed Referenced  \r\nTOTAL      100%      256M         256M         196K       \r\nnone       100%      256M         256M         196K \r\n```\r\n\r\n**Environment**\r\n\r\nRunning \r\n\r\n* System information:\r\n\r\n```\r\n$ uname -srm\r\nLinux 5.12.15-300.fc34.x86_64 x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```\r\n$ podman run --rm prometheus --version\r\nprometheus, version 2.28.1 (branch: HEAD, revision: b0944590a1c9a6b35dc5a696869f75f422b107a1)\r\n  build user:       root@2915dd495090\r\n  build date:       20210701-15:20:10\r\n  go version:       go1.16.5\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Prometheus configuration file:\r\n```\r\nglobal:\r\n  scrape_interval: 15s\r\n\r\nscrape_configs:\r\n  - job_name: 'localhost'\r\n    static_configs:\r\n      - targets: ['localhost:9100']\r\n\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\npodman run --name prometheus_test --net host -v prometheus_test:\/prometheus:Z -v \/home\/curry\/config.yml:\/prometheus\/prometheus.yml:Z prometheus\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:389 msg=\"No time or size retention was set so using the default time retention\" duration=15d\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:443 msg=\"Starting Prometheus\" version=\"(version=2.28.1, branch=HEAD, revision=b0944590a1c9a6b35dc5a696869f75f422b107a1)\"\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:448 build_context=\"(go=go1.16.5, user=root@2915dd495090, date=20210701-15:20:10)\"\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:449 host_details=\"(Linux 5.12.15-300.fc34.x86_64 #1 SMP Wed Jul 7 19:46:50 UTC 2021 x86_64 passionator (none))\"\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:450 fd_limits=\"(soft=524288, hard=524288)\"\r\nlevel=info ts=2021-07-20T22:42:51.543Z caller=main.go:451 vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nlevel=info ts=2021-07-20T22:42:51.545Z caller=web.go:541 component=web msg=\"Start listening for connections\" address=0.0.0.0:9090\r\nlevel=info ts=2021-07-20T22:42:51.545Z caller=main.go:824 msg=\"Starting TSDB ...\"\r\nlevel=info ts=2021-07-20T22:42:51.547Z caller=tls_config.go:191 component=web msg=\"TLS is disabled.\" http2=false\r\nlevel=info ts=2021-07-20T22:42:51.548Z caller=head.go:780 component=tsdb msg=\"Replaying on-disk memory mappable chunks if any\"\r\nlevel=info ts=2021-07-20T22:42:51.548Z caller=head.go:794 component=tsdb msg=\"On-disk memory mappable chunks replay completed\" duration=1.869\u00b5s\r\nlevel=info ts=2021-07-20T22:42:51.548Z caller=head.go:800 component=tsdb msg=\"Replaying WAL, this may take a while\"\r\nlevel=info ts=2021-07-20T22:42:51.549Z caller=head.go:854 component=tsdb msg=\"WAL segment loaded\" segment=0 maxSegment=0\r\nlevel=info ts=2021-07-20T22:42:51.549Z caller=head.go:860 component=tsdb msg=\"WAL replay completed\" checkpoint_replay_duration=22.633\u00b5s wal_replay_duration=182.827\u00b5s total_replay_duration=219.49\u00b5s\r\nlevel=info ts=2021-07-20T22:42:51.550Z caller=main.go:851 fs_type=9123683e\r\nlevel=info ts=2021-07-20T22:42:51.550Z caller=main.go:854 msg=\"TSDB started\"\r\nlevel=info ts=2021-07-20T22:42:51.550Z caller=main.go:981 msg=\"Loading configuration file\" filename=\/etc\/prometheus\/prometheus.yml\r\nlevel=info ts=2021-07-20T22:42:51.550Z caller=main.go:1012 msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=680.073\u00b5s remote_storage=1.222\u00b5s web_handler=243ns query_engine=653ns scrape=371.624\u00b5s scrape_sd=28.4\u00b5s notify=17.267\u00b5s notify_sd=8.621\u00b5s rules=930ns\r\nlevel=info ts=2021-07-20T22:42:51.550Z caller=main.go:796 msg=\"Server is ready to receive web requests.\"\r\nlevel=info ts=2021-07-21T01:43:23.335Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1626820988306 maxt=1626825600000 ulid=01FB3BPMXJ9HH7KS5AKV5HCNYY duration=21.06348ms\r\nlevel=info ts=2021-07-21T01:43:23.336Z caller=head.go:967 component=tsdb msg=\"Head GC completed\" duration=950.267\u00b5s\r\nlevel=info ts=2021-07-21T03:00:23.368Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1626825608306 maxt=1626832800000 ulid=01FB3G3MMMY0TB7WWMBJWR1KZY duration=52.575503ms\r\nlevel=info ts=2021-07-21T03:00:23.370Z caller=head.go:967 component=tsdb msg=\"Head GC completed\" duration=949.16\u00b5s\r\nlevel=info ts=2021-07-21T05:00:23.357Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1626832808306 maxt=1626840000000 ulid=01FB3PZBWMNTTVEG5NA41D83BJ duration=40.943109ms\r\nlevel=info ts=2021-07-21T05:00:23.358Z caller=head.go:967 component=tsdb msg=\"Head GC completed\" duration=959.689\u00b5s\r\nlevel=info ts=2021-07-21T07:00:23.414Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1626840008306 maxt=1626847200000 ulid=01FB3XV34K0WNACCYBYCD6Q4AR duration=98.595624ms\r\nlevel=info ts=2021-07-21T07:00:23.415Z caller=head.go:967 component=tsdb msg=\"Head GC completed\" duration=921.802\u00b5s\r\nlevel=info ts=2021-07-21T07:00:23.418Z caller=checkpoint.go:97 component=tsdb msg=\"Creating checkpoint\" from_segment=0 to_segment=1 mint=1626847200000\r\nlevel=info ts=2021-07-21T07:00:23.436Z caller=head.go:1064 component=tsdb msg=\"WAL checkpoint complete\" first=0 last=1 duration=17.954747ms\r\nlevel=info ts=2021-07-21T09:00:23.391Z caller=compact.go:518 component=tsdb msg=\"write block\" mint=1626847208306 maxt=1626854400000 ulid=01FB44PTCGS3RKG2K8K2PQRSJX duration=78.676554ms\r\nlevel=info ts=2021-07-21T09:00:23.392Z caller=head.go:967 component=tsdb msg=\"Head GC completed\" duration=915.667\u00b5s\r\n```\r\n\r\n**Misc comments\/thoughts\/suspicions**\r\n\r\nCould this be an issue with too aggressive `fallocate` calls? However, I think if that is the case, `compsize` should report some of the 256M as pre-allocated, which it does not. I would do a bit more digging in the code but I'm not too well versed in either Go or this project.","comments":["I am unfamiliar with how BTRFS deals with preallocations, but you could set `--storage.tsdb.max-block-chunk-segment-size` to a lower value to waste less disk space, if that is an issue for you. I will let this issue open and kidly ask @carroarmato0 (who I think also uses BTRFS) if he has a clue.","Was not aware of that option, it completely solves my issue. Thank you!","I think we can close this.","Given that `--storage.tsdb.max-block-chunk-segment-size` is a hidden flag, we should perhaps document this trick somewhere.","Please correct if I misunderstand, but preallocation itself works the same on btrfs as on any other file system. The space for files is allocated, but not yet written to. This improves performance as writer can write large chunks without doing regular system calls.\r\n\r\nOn file systems other then btrfs the same setup would produce the same \"wasted space\". The difference in btrfs in its default copy-on-write configuration (and other CoW file systems) is that it doesn't improve performance when space is preallocated. In CoW file systems, every write allocates new space, so preallocation does not help. The common recommendation is to turn off CoW for individual files, directories or volumes, though this won't change the space that is allocated to a file.\r\n\r\nWith this in mind, would it make sense to simply no longer hide this option and document it properly? We could certainly add a note, that this option could be used in lieu of turning off CoW if one is willing to take the performance hit. But the effect on chunk file size, that are smaller then the default `--storage.tsdb.max-block-chunk-segment-size` should be the same on all file systems.","Hey there.\r\n\r\nI've just stumbled over the same issue. Running Prometheus on btrfs and with --storage.tsdb.retention.size=36GB on a 40GB volume,... my btrfs ran out of space, while the actual files counted only some 29 GB or so.\r\n\r\nFirst I thought it must be a Prometheus bug (well it still may be ;-) )... but when I noticed that the sum of the files \"nominal\" bytes was much less than what btrfs thought that space was free (and I had no snapshots or so at all), I first brought up the issue on the btrfs mailing list, see:\r\nhttps:\/\/lore.kernel.org\/linux-btrfs\/2d5838efc179a557b41c84e9ca9a608be6a159e8.camel@scientia.org\/T\/#t\r\nfor the thread.\r\n\r\nI'm not fully sure which conclusion to draw... btrfs developers suggested the use of `autodefrag`, but I think that might also re-introduce new issues.\r\n\r\n@roidelapluie I'm unfamiliar with `--storage.tsdb.max-block-chunk-segment-size`. What exactly does it do? And will it still work, when I run Thanos (with `sidecar` or `receive` - I have 2 Prometheuses and use each with another Thanos method) on top of Prometheus?<br>\r\nEspecially, because of Thanos, I had to set `--storage.tsdb.min-block-duration=2h --storage.tsdb.max-block-duration=2h` (the require this for some reason).\r\n\r\nAs I've said in the btrfs mailing list thread... I would have been less surprised if the space was wasted in the WAL (many small writes?)... but actually it seems all wasted in the chunks (see https:\/\/lore.kernel.org\/linux-btrfs\/2d5838efc179a557b41c84e9ca9a608be6a159e8.camel@scientia.org\/T\/#m8ddf5c28f7352c7cd049c3228e5532b527707bbe).\r\n\r\nThe best solution would perhaps be if Prometheus would use an IO pattern which doesn't kill btrfs ;-)\r\n\r\nCould some Prometheus developers perhaps have a look at the thread on the btrfs mailing list? Btrfs developer Qu Wenru describes there which IO patterns he thinks may lead to the problem.\r\n\r\n\r\nThanks,\r\nChris","> I am unfamiliar with how BTRFS deals with preallocations, but you could set `--storage.tsdb.max-block-chunk-segment-size` to a lower value to waste less disk space, if that is an issue for you. I will let this issue open and kidly ask @carroarmato0 (who I think also uses BTRFS) if he has a clue.\r\n\r\nSorry I missed you tagging me here about 2 years ago!\r\n\r\nBTRFS does COW (Copy on Right), meaning writes do not overwrite data in place, instead, a modified copy of the block is written to a new location, and metadata is updated to point at the new location.\r\n\r\nFor certain applications it is recommended, just as @jan--f mentioned, to disable the COW functionality on the directory or mountpoint level.\r\n\r\nThis is recommended for example when running Virtual Machines with the disk in RAW format, but this is mostly to avoid system slow response due to heavy I\/O, since this disables checksums among things.\r\n\r\n```\r\nchattr +C \/path\/to\/dir\r\n```\r\n\r\nDo note that after applying this after data was already there, the change will only apply to new files.\r\nYou can find more information here: https:\/\/wiki.archlinux.org\/title\/btrfs#:~:text=To%20disable%20copy%2Don%2Dwrite,nodatacow%20option%20also%20disables%20compression.","Well, but with disabling CoW, one also looses features provided by it. In the base of btrfs that's mainly checksumming (which I'd rather not want to loose).\r\n\r\nAlso, keep in mind that except for ext, all major general purpose filesystems on Linux (ZFS, btrfs) already CoW, or are to get CoW (XFS).\r\nSo more uses will get hit by that problem sooner or later.\r\n\r\nWhat I don't understand is, why does Prometheus even overwrite in case of the chunks? Don't you write them at once, when enough data has accumulated in the WAL... and only periodically re-write them (the chunks) when compacting?","@carroarmato0 coming back to this issue.\r\n\r\nIt [seems](https:\/\/lore.kernel.org\/linux-btrfs\/aa69e84f-d466-457d-9b29-47043c2aef53@suse.com\/) that Prometheus may to the following:\r\n\r\nFirst pre-allocate a larger amount of storage (e.g. 256M), then eventually truncate it to something much smaller.\r\n\r\nIn btrfs, this won't cause the unused space to be released (until being defragmented, which is however plagued by some bug right now), but all the extra space being wasted.<br>\r\nIn my case I've seen quite often files where only ~15M are actually used and the remainder is \"lost\".\r\n\r\nI would guess that other CoW filesystems are affected by this, too, cause they would likely, too, need to rewrite the whole extent to get the preallocated but unused parts back.<br>\r\nSo presumably, ZFS and sooner or later XFS might hit by that as well. In the btrfs case - though I haven't tested it - it might even apply in the `NODATACOW` case.\r\n\r\nWhile the defrag bug will be fixed on the btrfs side, I still think the whole thing should be necessary in the first place.\r\n\r\nSo I wonder whether Prometheus could perhaps change it's IO pattern? Especially not pre-allocate so much, or better only what's actually guaranteed to be needed."],"labels":["component\/documentation"]},{"title":"Add support for multiple listening addresses","body":"## Proposal\r\n\r\n**Use case. Why is this important?**\r\n\r\nOn systems with multiple interfaces, one might only want to listen on only a subset of addresses, for example for loopback and intranet facing addresses.\r\n\r\nListening on everything and then restricting based on firewall rules is not satisfactory as that is prone to accidentally opening the services (possibly unprotected) to external and hostile networks, due to firewall misconfiguration or in case it needs to be brought down or similar. Having security in depth, were the firewall is an additional layer on top of the restricted listening seems always superior.\r\n\r\nAdding support for specifying multiple -web.listen-address options would solve this. This would also make it possible to later on implement something like systemd sd_listen_fds(3) protocol.","comments":["thank you for your pull request. This seems very niche, managing the lifecycle of multiple listeners is out of scope for Prometheus, and I think that\r\n\r\n> Listening on everything and then restricting based on firewall rules is not satisfactory as that is prone to accidentally opening the services (possibly unprotected) to external and hostile networks\r\n\r\n\r\ncould be also said about multiple web.listen-address because you could set it and open a new network socket without noticing that it was already set.\r\n\r\nWe could **maybe** revisit this if it is easily supported by the go std libraries directly (without looping on sockets ourselves etc), which does not seem to be the case as per a small research.","I think that although could help anyone and still be in the Prometheus perimeter on --web.listen commands is to at least get a --web.listen.interface command in order to isolate your prometheus server listening while not having to know in advance about nic CIDR.","Hello,\r\n\r\nThat solution would raise a lot of other questions and practical issues. Should we listen to all the addresses of that NIC? What if the address of the NIC change? Should we switch listen addresses on reload? How to deal with in progress connections, or \r\n ongoing connection pool, like remote read? Should we listen to the link local IPv6 addresses of that NIC?","This is [already supported by `exporter-toolkit`](https:\/\/github.com\/prometheus\/exporter-toolkit\/issues\/91). Are there any issues with supporting it here that aren't issues in exporters?"],"labels":["priority\/Pmaybe","kind\/feature"]},{"title":"Problems with BenchmarkLoadWAL","body":"I looked very closely at what it was doing, and made a list of questionable behaviours.\r\nI expect a benchmark to be somewhat similar to what the real Prometheus application will do.\r\n\r\nI am preparing a PR to improve the benchmark, but the list was getting so long I thought I should write them down before I forget.\r\n\r\n### 1. `BenchmarkLoadWAL` does not simulate existing mmapped chunks.\r\nThis was noted at https:\/\/github.com\/prometheus\/prometheus\/pull\/8645#issuecomment-807717816; it means the benchmark cannot be used to compare optimisations to the code which deals with mmapped chunks.\r\n\r\n### 2. `BenchmarkLoadWAL` creates new chunks while reading the WAL.\r\nReal Prometheus will not do this, because those chunks would be on disk already.\r\nBecause: the code to populate dummy data adds 7200 samples without creating any chunks: https:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head_test.go#L139\r\nwhile the WAL-reading code has 120 samples per chunk hard-coded:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head.go#L2467\r\nand this gets translated into a limit on the end timestamp for the chunk:\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head.go#L2489-L2493\r\n(there is also a hard-coded chunk range of 1000 (1 second) which comes into that calculation, but after you fix that the 120 samples still dominates)\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head_test.go#L231\r\n\r\n### 3. `BenchmarkLoadWAL` has very uneven sharding.\r\nBecause: series references are created as multiples of 100: https:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head_test.go#L200\r\nThen we take the modulus of this number with gomaxprocs: \r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head.go#L509\r\nand assign to that shard.\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/a1c1313b3c86b1d50a2f099f4f2697ccb01388e4\/tsdb\/head.go#L707-L708\r\nWith 4 cores, we only use 1 shard; with 8 cores only 2, and so on.\r\nReal Prometheus assigns series IDs sequentially, so would tend to spread evenly across the shards.\r\n\r\n### 4. `BenchmarkLoadWAL` leaks goroutines\r\nBecause the WAL is never closed.\r\n```\r\n...\r\nBenchmarkLoadWAL\/batches=10,seriesPerBatch=1000,samplesPerSeries=480,exemplarsPerSeries=24-4                   1         736243863 ns\/op\r\nPASS\r\ngoleak: Errors on successful test run: found unexpected goroutines:\r\n[Goroutine 8 in state select, with github.com\/prometheus\/prometheus\/tsdb\/wal.(*WAL).run on top of the stack:\r\ngoroutine 8 [select]:\r\ngithub.com\/prometheus\/prometheus\/tsdb\/wal.(*WAL).run(0xc00025a000)\r\n        \/home\/vagrant\/src\/github.com\/prometheus\/prometheus\/tsdb\/wal\/wal.go:332 +0xbc\r\ncreated by github.com\/prometheus\/prometheus\/tsdb\/wal.NewSize\r\n        \/home\/vagrant\/src\/github.com\/prometheus\/prometheus\/tsdb\/wal\/wal.go:301 +0x325\r\n...\r\n```\r\n","comments":["Also the code runs for a very short time - under 1 second on my not-very-fast machine.\r\nI think a longer running time, even 10 seconds, would help improve the comparison against real-world WAL loads which can run to minutes.\r\n","1, 3 and 4 will be fixed by #8645; 2 is less of a problem when running tests with 1 fixed."],"labels":["kind\/cleanup","priority\/P3","component\/tsdb"]},{"title":"Fail to send alerts to alertmanager due to EOF","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\n\r\nWe configured multiple(1000+) prometheus instance to send alerts to one alertmanager, at the beginning, everything is fine, after a few minutes, we found some prometheus failed to send alerts due to the error:\r\n```\r\nlevel=error ts=2021-07-01T12:33:16.623Z caller=notifier.go:527 component=notifier alertmanager=https:\/\/alertmanager\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"https:\/\/alertmanager\/api\/v2\/alerts\\\": EOF\"\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nAll prometheus instances send alerts to Alertmanager successfully.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nWe found some prometheus failed to send alerts due to the error:\r\n```\r\nlevel=error ts=2021-07-01T12:33:16.623Z caller=notifier.go:527 component=notifier alertmanager=https:\/\/alertmanager\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"https:\/\/alertmanager\/api\/v2\/alerts\\\": EOF\"\r\n```\r\n\r\n**Environment**\r\n\r\nPrometheus v2.4.2\r\nAlertmanager 0.21.0\r\n\r\n\r\nChecked the source code for the client http transport settings, we found that each prometheus keeps large max idle connection(20000 in total and 1000 per host) to send alerts to Alertmanager.\r\n\r\nSee: https:\/\/github.com\/prometheus\/common\/blob\/a1b6ede20323252d2b99a0f57178a4b7d364d0ca\/config\/http_config.go#L370-L380\r\n\r\nI wonder why we hard code these settings, they should be customized by end users. For my case, we have 1000+ Prometheus to send alerts to one Alertmanager, the client connection numbers will be *1000 for the upstream Alertmanager.\r\n\r\nIf there are some rate limit in upstream, the connection will be closed. In any case, we need to make sure the connection from each Prometheus instance can be customized. \r\n","comments":["Prometheus should not use that many connections. I suggest you to update\nprometheus and alertmanager to the latest release to benefit for all the\nbugfixes, including the go bugfixes. Please also check the number of file\ndescriptors (limit) of each process.\n\nLe mar. 6 juil. 2021 \u00e0 04:39, Morven Cao ***@***.***> a\n\u00e9crit :\n\n> *What did you do?*\n>\n> We configured multiple(1000+) prometheus instance to send alerts to one\n> alertmanager, at the beginning, everything is fine, after a few minutes, we\n> found some prometheus failed to send alerts due to the error:\n>\n> level=error ts=2021-07-01T12:33:16.623Z caller=notifier.go:527 component=notifier alertmanager=https:\/\/alertmanager\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"https:\/\/alertmanager\/api\/v2\/alerts\\\": EOF\"\n>\n> *What did you expect to see?*\n>\n> All prometheus instances send alerts to Alertmanager successfully.\n>\n> *What did you see instead? Under which circumstances?*\n>\n> We found some prometheus failed to send alerts due to the error:\n>\n> level=error ts=2021-07-01T12:33:16.623Z caller=notifier.go:527 component=notifier alertmanager=https:\/\/alertmanager\/api\/v2\/alerts count=1 msg=\"Error sending alert\" err=\"Post \\\"https:\/\/alertmanager\/api\/v2\/alerts\\\": EOF\"\n>\n> *Environment*\n>\n> Prometheus v2.4.2\n> Alertmanager 0.21.0\n>\n> Checked the source code for the client http transport settings, we found\n> that each prometheus keeps large max idle connection(20000 in total and\n> 1000 per host) to send alerts to Alertmanager.\n>\n> See:\n> https:\/\/github.com\/prometheus\/common\/blob\/a1b6ede20323252d2b99a0f57178a4b7d364d0ca\/config\/http_config.go#L370-L380\n>\n> I wonder why we hard code these settings, they should be customized by end\n> users. For my case, we have 1000+ Prometheus to send alerts to one\n> Alertmanager, the client connection numbers will be *1000 for the upstream\n> Alertmanager.\n>\n> If there are some rate limit in upstream, the connection will be closed.\n> In any case, we need to make sure the connection from each Prometheus\n> instance can be customized.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/9057>, or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AACHHJR4QJ6B4QQNVTPHMRLTWJUEZANCNFSM473U2KDA>\n> .\n>\n","thanks @roidelapluie for the quick reply.\r\nI tried to upgrade to latest version, it didn't fix the issue.\r\nI checked the source code, and the root cause should that the http connection is not closed proactively after each alert send request is done.\r\n\r\nsee: https:\/\/github.com\/prometheus\/prometheus\/blob\/62598878dd080d0488457e2238fb6e5a847a8247\/notifier\/notifier.go#L580-L590\r\n\r\nactually `resp.Body.Close()` is not enough in my case, because the connection is still there in the pool until maxIdleConnectionTimeout(by default 5 minutes), I add the following code:\r\n```\r\nreq.Close = true\r\n```\r\nto make sure http connection is closed after the request is done, and then I rebuild it and didn't see `EOF` any more.\r\n\r\nSo IMO, **prometheus should probably provide an option in settings to close the http connection, or customize the http connection settings, there are many scenarios similar to mine, there is rate limit in the upstream, or there is client-connection timeout in upstream less than the `maxIdleConnectionTimeout` of the prometheus, the the connection will be closed by the upstream unexpectedly, which will the http connections in the prometheus side are actually broken.**","I did more experiments, if I change the http connection settings as follows, the issue is also gone.\r\n```\r\ndiff --git a\/config\/http_config.go b\/config\/http_config.go\r\nindex 07de306..0fde8f9 100644\r\n--- a\/config\/http_config.go\r\n+++ b\/config\/http_config.go\r\n@@ -223,14 +223,14 @@ func NewRoundTripperFromConfig(cfg HTTPClientConfig, name string, disableKeepAli\r\n                \/\/ It is applied on request. So we leave out any timings here.\r\n                var rt http.RoundTripper = &http.Transport{\r\n                        Proxy:               http.ProxyURL(cfg.ProxyURL.URL),\r\n-                       MaxIdleConns:        20000,\r\n-                       MaxIdleConnsPerHost: 1000, \/\/ see https:\/\/github.com\/golang\/go\/issues\/13801\r\n+                       MaxIdleConns:        200,\r\n+                       MaxIdleConnsPerHost: 100, \/\/ see https:\/\/github.com\/golang\/go\/issues\/13801\r\n                        DisableKeepAlives:   disableKeepAlives,\r\n                        TLSClientConfig:     tlsConfig,\r\n                        DisableCompression:  true,\r\n                        \/\/ 5 minutes is typically above the maximum sane scrape interval. So we can\r\n                        \/\/ use keepalive for all configurations.\r\n-                       IdleConnTimeout:       5 * time.Minute,\r\n+                       IdleConnTimeout:       5 * time.Second,\r\n                        TLSHandshakeTimeout:   10 * time.Second,\r\n                        ExpectContinueTimeout: 1 * time.Second,\r\n                        DialContext: conntrack.NewDialContextFunc(\r\n```","This would effectively disable keepalive, which is not desired.\r\n\r\nDid you look at the actual bottleneck? It could be a bottleneck on the network or on the alertmanager, how many file descriptors are open and how many are allowed?","@roidelapluie \r\nWe have a gateway in front of the Alertmanager, and the gateway is implemented HAProxy, which has client-connection timeout less than the `IdleConnTimeout` of the prometheus. I checked the prometheus code, the `IdleConnTimeout` of the prometheus is 5 minutes and it can be customized for sending alerts.\r\nIf I understanding it correctly, this will results in broken http connections that the prometheus(client) doesn't even know.","The `maxConnection` for the gateway is 20000, equal to the value of `MaxIdleConns` for single prometheus instance. But we have 1k+ prometheus instances sending alert via the same gateway, the connections will not be sufficient in gateway.\r\n\r\nBut if we can customize the maxIdleConnection at the prometheus side, we can make it to a more reasonable value to make sure each prometheus instance can have some active connections.","Do you have evidence that Prometheus is actually using those? This should be a connection pool and Prometheus should use one or two connections. However, it could be a misconfiguration of HAProxy. HAProxy client-connection timeout probably has nothing to do with this situation.\r\n\r\nnote: you could also disable the keepalive in haproxy: `option httpclose`","@roidelapluie thanks, we currently don't have `option httpclose`, in HAProxy settings, but we have `option http-pretend-keepalive`.\r\nAnyway, let me give it a try. ","@roidelapluie \r\nWe tried to add `option httpclose` in HAProxy, but adding that configuration will prevent connection reuse, it will results poor performance when we have much more traffic going through the HAProxy. Besides, the HAProxy are shared by many other services, so we can't simply change the configuration that may impact other services.\r\n\r\nAt the same time, I found in the prometheus code, if there are multiple alertmanagers configured, the alert will be dropped if it is sent successfully to at least one Alertmanager. I'm not sure what reasons are for this design, but in our case, the critical Alertmanager can't receive the alerts, but there are no retry logic out there, because some other Alertmanager received the alert. So I wonder if this can be enhanced by retry if the alert send request failed.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/1bcd13d6b5d0e6724dbcf662fdcccebaba947e65\/notifier\/notifier.go#L449-L451\r\n\r\n","option httpclose can be used with an ACL to only match prometheus traffic. Also, this is doing the same as you have done with resp.Close. I think you might have a  bottleneck somewhere in your network, unrelated to this setting that has not changed or caused issues in years.\r\n\r\nI also recommend you to upgrade prometheus to benefit from 3 years of improvements and bugfixes in the golang http libraries.","thanks @roidelapluie for your suggestion.\r\nWe figured out the issue is actually caused by the connection closed by the proxy end, but the fix is not that easy, because the change to the proxy may impact may services. We wonder if it possible to add retry logic for alert sending.","> option httpclose can be used with an ACL to only match prometheus traffic. Also, this is doing the same as you have done with resp.Close. I think you might have a bottleneck somewhere in your network, unrelated to this setting that has not changed or caused issues in years.\r\n> \r\n> I also recommend you to upgrade prometheus to benefit from 3 years of improvements and bugfixes in the golang http libraries.\r\n\r\nhi:\r\nI have a similar error, please move: https:\/\/github.com\/prometheus\/prometheus\/issues\/9176\r\nIt's urgent, do you have a solution to the error, what should I do, thank you very much","I have the same problem. Is the configuration file or startup command incorrect? Please go to https:\/\/github.com\/prometheus\/prometheus\/issues\/9176 to view the detailed information. I have built a set of prometheus and alertmanager environments without involving the introduction of any code layer. At the beginning, both prometheus and alertmanager are running normally, and the alarm is also normal. The error is reported after a short while, please give some solutions or suggestions","In my case, I have proxy between prometheus and alertmanager. I made some changes to the proxy settings to timeout of closing idle connection, it mitigated the issue but not resolve this, still looking for a better solution.   ","@morvencao - have you found a better solution since your last comment ?","no better solution found @AlexandreRoux ","If we make `maxIdleConnectionTimeout` configurable, it would solve the problem right? I see a lot of discussion around closing connection per request, but looks to me like allow configurable `maxIdleConnectionTimeout` in [HTTPClientConfig](https:\/\/github.com\/prometheus\/common\/blob\/8c9cb3fa6d01832ea16937b20ea561eed81abd2f\/config\/http_config.go#L252) is the way to go here? \r\n\r\n@roidelapluie  WDYT?","Also, looks like this issue and https:\/\/github.com\/prometheus\/prometheus\/issues\/9176 are dups :) ","Retries for alert sends implicitly happen on the next rule evaluation interval. We should be careful not to pile up too much in-flight or pending work, so for that reason I am wary of adding another layer of retries. They have the potential to make a slightly overloaded Alertmanager _very_ overloaded very quickly. We could retry specific errors that definitely mean the request did not go out, but I am not sure EOF guarantees that.\r\n\r\nHowever, for the same reason, I do agree that up to 20000 open connections _per Prometheus_, with no way to reduce that, is excessive. Real world installations with hundreds of servers exist, and I don't think it helps anyone if Alertmanager has to juggle millions of TCP connections. \"Check file handle limits\" is easier said than done esp. for containers, and you always find out that you missed a spot at the worst of times."],"labels":["kind\/more-info-needed"]},{"title":"Kubernetes node discovery does not use kubeconfig auth for endpoints discovered","body":"**What did you do?**\r\nI used the \"new\" kubernetes discovery with a kubeconfig file to access cadvisor endpoints\r\n\r\n**What did you expect to see?**\r\nPrometheus discover all the nodes endpoints correctly and access to them.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nPrometheus indeed discover the nodes correctly. But can't access the cadvisor endpoint receiving a `403 Forbidden` because prometheus tries to connect with an anonymous user.\r\n\r\n* Prometheus version: 2.28.0\r\n\r\n* Alertmanager version: N\/A\r\n\r\n* Prometheus configuration file:\r\n```yml\r\n  - job_name: cadvisor\r\n    kubernetes_sd_configs:\r\n    - role: node\r\n      kubeconfig_file: \/var\/lib\/prometheus\/.kube\/config\r\n    relabel_configs:\r\n    - action: labelmap\r\n      regex: __meta_kubernetes_node_label_(.+)\r\n    - replacement: $EKS_API_ENDPOINT:443\r\n      target_label: __address__\r\n    - regex: (.+)\r\n      replacement: \/api\/v1\/nodes\/$1\/proxy\/metrics\/cadvisor\r\n      source_labels:\r\n      - __meta_kubernetes_node_name\r\n      target_label: __metrics_path__\r\n    scheme: https\r\n    tls_config:\r\n      insecure_skip_verify: true\r\n```\r\n\r\n* Logs:\r\n\r\nPrometheus log :\r\n```\r\nJul 05 16:58:42 ip-1-2-3-4 prometheus[31195]: level=debug ts=2021-07-05T14:58:42.877Z caller=scrape.go:1236 component=\"scrape manager\" scrape_pool=cadvisor target=https:\/\/xxxxxx.eks.amazonaws.com:443\/api\/v1\/nodes\/ip-1-2-3-4.eu-west-X.compute.internal\/proxy\/metrics\/cadvisor msg=\"Scrape failed\" err=\"server returned HTTP status 403 Forbidden\"\r\n```\r\n\r\nKubernetes log :\r\n```json\r\n{\r\n    \"kind\": \"Event\",\r\n    \"apiVersion\": \"audit.k8s.io\/v1\",\r\n    \"level\": \"Request\",\r\n    \"auditID\": \"223b12f1-aaba-408f-b2fc-7a292606c946\",\r\n    \"stage\": \"ResponseStarted\",\r\n    \"requestURI\": \"\/api\/v1\/nodes\/ip-1-2-3-4.eu-west-X.compute.internal\/proxy\/metrics\/cadvisor\",\r\n    \"verb\": \"get\",\r\n    \"user\": {\r\n        \"username\": \"system:anonymous\",\r\n        \"groups\": [\r\n            \"system:unauthenticated\"\r\n        ]\r\n    },\r\n    \"sourceIPs\": [\r\n        \"4.3.2.1\"\r\n    ],\r\n    \"userAgent\": \"Prometheus\/2.28.0\",\r\n    \"objectRef\": {\r\n        \"resource\": \"nodes\",\r\n        \"name\": \"ip-1-2-3-4.eu-west-X.compute.internal\",\r\n        \"apiVersion\": \"v1\",\r\n        \"subresource\": \"proxy\"\r\n    },\r\n    \"responseStatus\": {\r\n        \"metadata\": {},\r\n        \"status\": \"Failure\",\r\n        \"reason\": \"Forbidden\",\r\n        \"code\": 403\r\n    },\r\n    \"requestReceivedTimestamp\": \"2021-07-05T14:08:25.734129Z\",\r\n    \"stageTimestamp\": \"2021-07-05T14:08:25.734334Z\",\r\n    \"annotations\": {\r\n        \"authorization.k8s.io\/decision\": \"forbid\",\r\n        \"authorization.k8s.io\/reason\": \"\"\r\n    }\r\n}\r\n```\r\n\r\nAs we can see here, the user used is system:anonymous for the metrics retrieval. Except that the user from kubeconfig is not system:anonymous and for service discovery, the user seems to be the one I have set up in the kubeconfig.","comments":["Indeed. We can not use kubeconfig file for scraping as that does not give us full control over the http client. We need full control in the scraping client to properly work with timeouts, http2, tls etc. A job can also mix multiple service discoveries (multiple k8s or even k8s+other) so we can not use any configuration from the service discovery.\r\n\r\nYou would need to use another authentication mechanism to do the scraping.","So, we can't use the token retrieval process from the kubeconfig to establish the http auth ? Because we actually use an AWS IAM role on the instance to avoid having token walking around and kubeconfig supports this feature.","I fear you would have to use a kind of sidecar proxy at the moment. I will gather more feedback around this issue to our kube maintainers.","If there is no token, what exactly is it that the kubeconfig does to authenticate (maybe it's something we can do with the myriad of other functionalities we already support)?","In my case, the kube config file states an AWS cli command that retrieves a temporary token. [Documentation link](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/create-kubeconfig.html#create-kubeconfig-manually).\r\nActual command : `aws --region eu-west-1 eks get-token --cluster-name example-cluster-eu-west-1`\r\nResponse \r\n```json\r\n{\r\n  \"status\":{\r\n    \"expirationTimestamp\":\"2021-07-09T07:37:24Z\",\r\n    \"token\":\"k8s-aws-v1.Aed...(restofthetoken)\"\r\n  },\r\n  \"kind\":\"ExecCredential\",\r\n  \"spec\":{},\r\n  \"apiVersion\":\"client.authentication.k8s.io\/v1alpha1\"\r\n}\r\n```","oh wow I was entirely unaware of this functionality of kubeconfig ... it doesn't make the mechanism less bespoke though, my suggestion would be to have a sidecar that just writes the token to an empty-dir volume that is shared with Prometheus and Prometheus will detect the file change","I also spent a lot of time today, trying to understand why kubeconfig_file discovery does not.\r\nIt would be really great if prometheus can automatically retrieve tls & auth information from provided kubeconfig"],"labels":["priority\/Pmaybe","component\/scraping","kind\/feature"]},{"title":"delete persisted head block after failed db xxxx remove","body":"## Thank you for your help\r\n###  I don't know why this problem occurs. After an interval of 4 hours, 2 hours of data will be lost.\r\n### I am using the default configuration.\r\nprometheus  Version 2.18.1 \r\nWindows Server 2016\r\n### image:\r\n![image](https:\/\/user-images.githubusercontent.com\/11703570\/124239904-c25ffc80-db4c-11eb-804c-26a0cef3ca74.png)\r\n\r\n### log: It's all that kind of information\r\nlevel=error ts=2021-07-02T05:00:01.788Z caller=db.go:667 component=tsdb msg=\"compaction failed\" err=\"delete persisted head block after failed db reload:01F9JSD0DPACPW09RGEWXPDC7Z: remove E:\\\\monitor\\\\server\\\\data\\\\prometheus\\\\01F9JSD0DPACPW09RGEWXPDC7Z\\\\chunks\\\\000001: The process cannot access the file because it is being used by another process.\"\r\nlevel=error ts=2021-07-02T05:01:01.790Z caller=db.go:667 component=tsdb msg=\"compaction failed\" err=\"plan compaction: open E:\\\\monitor\\\\server\\\\data\\\\prometheus\\\\01F8KWMCDQ1EEQGEVDGQ6KBSVH\\\\meta.json: The system cannot find the file specified.\"\r\nlevel=error ts=2021-07-02T05:04:01.795Z caller=db.go:667 component=tsdb msg=\"compaction failed\" err=\"plan compaction: open E:\\\\monitor\\\\server\\\\data\\\\prometheus\\\\01F8KWMCDQ1EEQGEVDGQ6KBSVH\\\\meta.json: The system cannot find the file specified.\"\r\nlevel=error ts=2021-07-02T05:07:01.806Z caller=db.go:667 component=tsdb msg=\"compaction failed\" err=\"plan compaction: open E:\\\\monitor\\\\server\\\\data\\\\prometheus\\\\01F8KWMCDQ1EEQGEVDGQ6KBSVH\\\\meta.json: The system cannot find the file specified.\"\r\nlevel=info ts=2021-07-02T07:00:01.022Z caller=compact.go:495 component=tsdb msg=\"write block\" mint=1625198400000 maxt=1625205600000 ulid=01F9K08QNXSAZFFRWEKS6QHZ87 duration=705.0403ms\r\nlevel=error ts=2021-07-02T07:00:01.117Z caller=db.go:946 component=tsdb msg=\"failed to read meta.json for a block\" dir=E:\\monitor\\server\\data\\prometheus\\01F8KWMCDQ1EEQGEVDGQ6KBSVH err=\"open E:\\\\monitor\\\\server\\\\data\\\\prometheus\\\\01F8KWMCDQ1EEQGEVDGQ6KBSVH\\\\meta.json: The system cannot find the file specified.\"","comments":["Hello,\r\n\r\nCould you please update your prometheus to the latest release and try to reproduce this issue? It is likely that the root cause is The process cannot access the file because it is being used by another process., but you should not lose data, compaction should just fail.","Any update on this? Please also confirm that the filesystem is NTFS."],"labels":["kind\/more-info-needed","component\/tsdb"]},{"title":"Promtool: support rule evaluation command","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nAs a Prometheus user\/SRE, I want to add some new alerting rules. But right now, there is not a good way to test these new rules against a running Prometheus. Compare to `promtool test rules` command, the rule evaluation command can specify the server address and evaluates the rules against that server. This is more like an E2E test rather than a unit test.\r\n\r\n## Example usage\r\n\r\n`promtool  rules <server> <rule-files-to-evaluate>`\r\n\r\n## Possible implementation\r\n\r\nSimilar to `promtool tsdb create-blocks-from rules` command, this command can iterate through all rules in the rule files and perform instant queries against the server based on the given query expression. This goes through the `api\/v1\/query` API so `ALERTS` and `ALERTS_FOR_STATE` won't be written to TSDB.","comments":["It's an interesting idea and something I've thought about too, but really this direction leads to re-implementing Prometheus itself in promtool (rules that depend on each other need to be written to an underlying TSDB... how do you view the results? do you want a graph UI? etc...)\r\n\r\nA way you can do this right now is write a minimal Prometheus config that points at another Prometheus through remote read, literally just this:\r\n```yaml\r\nremote_read:\r\n  - url: http:\/\/prometheus.demo.do.prometheus.io\/api\/v1\/read\r\n    read_recent: true\r\nrule_files:\r\n  - the-extra-rules-you-want-test.yaml\r\n```\r\n\r\nAnother idea that I quite like is making it easier to write rule tests, maybe a way to point at some data and generate the `input_series` based on that data (this gets into needing to understand relationships of rules and so on, e.g. https:\/\/github.com\/vatine\/promgraph type thing); that could be a tool that sits outside of prometheus itself and help with this kind of thing though.","> It's an interesting idea and something I've thought about too, but really this direction leads to re-implementing Prometheus itself in promtool (rules that depend on each other need to be written to an underlying TSDB... how do you view the results? do you want a graph UI? etc...)\r\n\r\nWe can do it in a simple way and I think it is okay to not handle recording rules and these dependencies. Instant query for rules expr would be good enough.","... which we already support in promtool","and in the prometheus UI","This seems like\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/pull\/4277\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/1220\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/1154\r\n\r\nWDYT?","@roidelapluie Thanks, I see. Yeah, there is some overlapping here","With the strong limitation on not having dependencies, it is like running instant queries, because you can only rely on one rule at the same time. I also agree with @dg's comment about how to view the results etc..."],"labels":["priority\/Pmaybe","component\/promtool","kind\/feature"]},{"title":"PromQL makes it unnecessarily inefficient to work with time series timestamps even in straightforward cases","body":"## Proposal\r\nRight now, the fact that PromQL does not expose first class access to time series timestamps means that anything you do with timestamps requires using functions and thus subqueries, which makes them quite inefficient despite the fact that the data is directly present in the TSDB, directly loaded by Prometheus, and so on.\r\n\r\nSuppose that you have a long retention time and a metric that tracks per-something usage; per-user VPN usage, for example. When a user is using the VPN, they have a time series with an appropriate label for their usage; when they're not connected, there is no time series. Such intermittent time series is a very common metric pattern. If you want to query the maximum number of connections that a user has had over a time range (and also determine if they've used the VPN at all), this is very efficient: ``max_over_time(yourmetric{user=\"...\"}[730d])``. Prometheus will evaluate this (for us) in nothing flat. However, if you want to know the earliest or the latest that a time series for a user was present, you must use ``timestamp()``, which forces a subquery, which is far less efficient. When you run ``max_over_time( timestamp(yourmetric{user=\"...\"})[730d:1m] )``, Prometheus will sit there grinding away for a significant amount of time.\r\n\r\nThis inefficiency is unnecessary. The timestamps exist beside the values in the TSBD, and the core time series scan that Prometheus is doing for ``max_over_time()`` (and other ``*_over_time`` functions) is loading both the values and their timestamps from the TSDB. If there was a way to access the timestamp at that point our query for maximum or minimum timestamp would be just as efficient as the query for the maximum or minimum value. Only PromQL's limitations are making this inefficient.\r\n\r\nI don't have any particular ideas on how this should be solved. One brute force solution would be some form of ``timestamp_over_time`` function that transformed range vectors from being range vectors of values to range vectors of timestamps. Another solution would be a way to mark metrics as yielding their timestamp instead of their value. ``@`` is already taken as a syntax character, otherwise you might write ``@yourmetric{user=\"...\"}`` to mean that you wanted the timestamp of the time series instead of its value (which could be used in both instant vectors and range vectors).\r\n","comments":["Thanks!\r\n\r\nI think this is something we should think about.\r\n\r\nI note that this seems related to https:\/\/github.com\/prometheus\/prometheus\/issues\/5003, even if this issue is wider and more generic.","Another use case is to get the last successful time for a certain metric (regardless of the presence). e.g. last probe_success == 1; which would be max_over_time_last_timestamp(probe_success[1h])","VictoriaMetrics provides the following functions via [MetricsQL](https:\/\/docs.victoriametrics.com\/MetricsQL.html):\r\n\r\n* `tmax_over_time(m[d])` - returns the timestamp for the maximum value of `m` over the duration `d`.\r\n* `tmin_over_time(m[d])` - returns the timestamp for the minimum value of `m` over the duration `d`.\r\n* `tfirst_over_time(m[d])` - returns the timestamp for the first sample of `m` over the duration `d`.\r\n* `tlast_over_time(m[d])` - returns the timestamp for the last sample of `m` over the duration `d`.\r\n\r\nIt would be great if these or similar functions will be available in Prometheus as well.","Thanks for bringing this up. I am not a fan of the single letter. We see a lot of confusion already with \"i\" rate (i for instant). There could be multiple values for the timestamp, which is why my proposal was _last_timestamp.\r\n\r\nNote that the PromQL execution model does not play nicely with first_over_time, so we do not provide it. It is better to use offset.","I'm not a fan of single-letter shortcuts in general case. But in this particular case the `tmax_over_time()` looks easier to write and read than `max_over_time_last_timestamp()`.","'The timestamp of the first time series point over a time range' is useful information in general that can't be obtained through use of `offset` unless the time series is always present. The naive implementation appears to me to be loading the time series range vector as for all of the `*_over_time()` functions and then returning the first point's timestamp (or value, if you wanted to add that function too). The existing implementations already loop over the range vector for each time series, so they can pick whatever they like out of it.","Ping @juliusv @beorn7  , I am interested to know your opinions here.","I don't have strong opinions on this, I guess I'd be fine with adding these functions but would also prefer clearer names for them. When looking at the `tmax_over_time()` vs. `tlast_over_time()` I had to read their descriptions to understand the difference between them.\r\n\r\nIt would be even nicer to have composable \/ orthogonal query constructs for telling the function what aspect of the sample to look at for selecting the output sample vs. which part of it to return, rather than having a bunch of special-case functions. But that may require too different of a query language.","One general idea would be to expand what can be used to generate a range vector from a simple metric to some degree of filters and transformations on this. You can already do this with subqueries, but they're too general. The obvious syntax would be something like:\r\n```\r\ncount_over_time( (yourmetric > 10)[1w] )\r\n```\r\nI think this works out okay in the execution model (provided that you restrict it to a single metric); rather than load the range vector and iterate over it doing whatever, the `_over_time` functions would load it, optionally run a transformation and\/or filter on each sample point, and then do whatever. However I don't know if this does terrible violence to the language model.","Yeah, I think this goes quite deep. In the depth of its soul, PromQL would like to forget that the timeseries consists of samples with timestamps. But we already break that barrier, most prominently by `timestamp`, but also `count_over_time` is acknowledging that there is a finite number of samples in the range. And something like `avg_over_time` is even, IMHO, broken because if the scrape interval happens to double in the middle of the range, you get a result that is biased in a way that most users wouldn't expect (arguably).\r\n\r\nSo `timestamp(prometheus_build_info)` gives you the timestamp of all samples matching that selector. Adding even the tiniest amount of evaluation gives you the timestamp of the evaluation, e.g. `timestamp(-prometheus_build_info)` or even `timestamp(+prometheus_build_info)`. This behavior makes sense intuitively if the evaluation actually involves different samples with (potentially) different timestamps. But our intuition would like to be able to access the original timestamp if only ever one sample contributed to the result.\r\nI find it problematic to go through the language and identify all functions and aggregators that yield a result based on a single sample, and then add a new function or aggregator to calculate the timestamp of that sample. In addition to what was already mentioned, we needed `tmax` and `tmin` aggregators and binary operators like `t>` and `t<` for filtering etc. That's probably the same itch that made @juliusv state:\r\n\r\n> It would be even nicer to have composable \/ orthogonal query constructs for telling the function what aspect of the sample to look at for selecting the output sample vs. which part of it to return, rather than having a bunch of special-case functions. \r\n\r\nOne might argue that `timestamp` should always return the timestamp of evaluation and never the sample timestamp, because even just stating `prometheus_build_info` is an valid PromQL expression, and thus an evaluation, and as the query API call returns the timestamp of the request rather than the timestamp of the sample, `timestamp` should be equally consistent. It would be consistent, but also useless. So one could argue in the other direction, the `timestamp(prometheus_build_info)` is a special case, and that we could extend that special case to `timestamp(+prometheus_build_info)` and so on, wherever the evaluation doesn't need to touch more than one sample from the TSDB (of course, that would be a breaking change, just speculating here). That would be what many asked for before, and would remove the need for a zoo of additional `t-something` functions and aggregators. However, it would not solve the \"filter and count\" case @siebenmann mentioned in the comment above. That goes even deeper, generally seeing \"selecting\" or \"filtering\" as different from \"aggregating\" or \"joining\" different samples from the TSDB. `yourmetric > 10` would return a \"filtered view\" of the original samples, while `yourmetric > mymetric` would return the result of a \"real\" evaluation. Now I feel very dizzy\u2026\r\n","I realize the filtering case (like `t>` and `t<`) is not as convincing as I hoped. \"Obviously\", for those cases, you can just label-match a `timestamp(foo)` call with the result of the filtering expression, e.g. `timestamp(foo) and (foo > 42)`. But that's in a way just a lucky coincidence. `foo > 42` comes back with enough labels to do the label-match. The same doesn't work over time (cf. `count_over_time((foo > 42)[1w])` as above, or should we even say `count_over_time(foo[1w]>42)`?), and it doesn't work for `max` because aggregation is not filtering, even if some aggregations seemingly \"filter out\" one particular sample. (Which is not even strictly true for `max` (and `max_over_time`) because multiple samples could tie for the same maximum value.)\r\n\r\nTo prove myself wrong: You could do `timestamp(foo) and (foo == scalar(max(foo)))` \u2013 gives you the timestamps of all `foo` elements whose value is the maximum value. But it's obscure enough to demonstrate that we are somehow trying to do something PromQL doesn't really like\u2026","Sorry for the noise. Misclicked\u2026","Are the any plans to implement these proposed functions?   I've had many occasions where I've had to return the timestamp of the earliest sample in a given series.   For example, I might use something that resembles this to find the earliest occurrence of `my_custom_metric` over the last 90 days, using a 6h step to avoid evaluating each sample :\r\n```\r\nmin_over_time(\r\n  (timestamp(my_custom_metric{hostname=\"web01\"})[45d:6h])\r\n)\r\n```\r\nalthough having a simplified shortcut function to perform this would be convenient.   If there is currently a more appropriate or efficient way of doing this, I would appreciate the clarification.   Thank you.","Tagging this for v3, mostly to raise attention to the idea of not changing the timestamp if an evaluation doesn't touch multiple samples or something\u2026"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/promql"]},{"title":"POD Exessive RAM (Dirty) usage","body":"**What did you do?**\r\n\r\nNotice huge difference between K8S POD Ram usage and Go Proffiling tool .\r\nWe have many Prometheus. We've notice two of them have huge and stabilized RAM consumation after some days of running.\r\nAfter restart POD consume ~4GB. \r\nAfter some days POD consume ~8 to 11GB. (never less to 8GB if POD not restarted)\r\n\r\nGO proff tool give us a less than 3GB usage. This usage is typically the same on all our anothers Prom.\r\n`Showing nodes accounting for 1838.65MB, 70.49% of 2608.42MB total`\r\n\r\nTOP inside POD give us interessting informations:\r\n```\r\nMem total:32891980 anon:6882652 map:1559988 free:20287524\r\n slab:350440 buf:75452 cache:5176960 dirty:2724 write:0\r\nSwap total:0 free:0\r\n  PID   VSZ^VSZRW^  RSS (SHR) DIRTY (SHR) STACK COMMAND\r\n    1 24.5g 7945m 7607m     4 6246m     0   140 \/bin\/prometheus --web.console.templates=\/etc\/prometheus\/consoles --web.console.libraries=\/etc\/prometheus\/console_libraries --config.file=\/etc\r\n   43  1348   204   596   456    76     0   140 sh\r\n   49  1332   188   604   456    64     0   140 top\r\n```\r\nOn these high memory Prom usage, we've notice important \"DIRTY\" Ram usage.\r\n\r\n**What did you expect to see?**\r\n\r\nRAM usage = GO Pproff RAM usage + slicy overhead (30% max, not 300%)\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nHigh RAM usage \/ overhead after days\r\n\r\n**Environment**\r\n\r\nGKE Kubernetes\r\n\r\n**Prometheus version**\r\n\r\n Prometheus 2.27.1\r\n\r\n**TSDB page:**\r\nWe've done lot of Cardinality optimization into TSDB in order to reduce POD RAM usage => Not effect since RAM usage is regarding kernel stack\r\n\r\n```\r\nTSDB Status\r\nHead Stats\r\nNumber of Series\tNumber of Chunks\tNumber of Label Pairs\tCurrent Min Time\tCurrent Max Time\r\n1092426\t1552205\t36480\t2021-06-17T12:00:00.000Z (1623931200000)\t2021-06-17T13:50:44.966Z (1623937844966)\r\nHead Cardinality Stats\r\nTop 10 label names with value count\r\nName\tCount\r\nname\t7279\r\ndevice\t3708\r\npod\t3216\r\naddress\t3074\r\nuid\t3050\r\npod_ip\t2863\r\n__name__\t2078\r\nsecret\t1018\r\nreplicaset\t999\r\nlabel_pod_template_hash\t999\r\nTop 10 series count by metric names\r\nName\tCount\r\ncontainer_memory_failures_total\t39988\r\nkube_pod_container_status_waiting_reason\t26530\r\nkube_pod_container_status_last_terminated_reason\t22740\r\nkube_pod_container_status_terminated_reason\t22740\r\napiserver_request_duration_seconds_bucket\t19836\r\nkube_pod_status_phase\t15740\r\ncontainer_fs_reads_total\t13654\r\ncontainer_fs_writes_total\t13654\r\nnginx_ingress_controller_request_duration_seconds_bucket\t12264\r\nnginx_ingress_controller_response_size_bucket\t12264\r\nTop 10 label names with high memory usage\r\nName\tBytes\r\nname\t712589\r\npod\t147293\r\nuid\t109796\r\n__name__\t73361\r\naddress\t52258\r\ndevice\t43577\r\nreplicaset\t38052\r\npod_ip\t32041\r\nsecret\t30073\r\nmountpoint\t25731\r\nTop 10 series count by label value pairs\r\nName\tCount\r\nendpoint=http-metrics\t748337\r\nnamespace=sites\t605393\r\nservice=kubelet\t542554\r\njob=kubelet\t542550\r\nmetrics_path=\/metrics\/cadvisor\t495985\r\njob=kube-state-metrics\t206743\r\ninstance=XXXX:YYYY\t206647\r\nservice=kube-state-metrics\t206075\r\ncontainer=enqueue\t200183\r\ncontainer=POD\t175660\r\n```\r\n","comments":["Can you please send us screenshots of the following grafana dashboard, when you see this: https:\/\/grafana.com\/grafana\/dashboards\/12054\r\n\r\nThanks!","![Screenshot 2021-06-21 at 15-07-01 Prometheus Benchmark - 2 17 x - Grafana](https:\/\/user-images.githubusercontent.com\/59478661\/122767105-961dc380-d2a2-11eb-8d45-ef36f56ebf05.png)\r\n\r\n![Screenshot 2021-06-21 at 15-06-42 Prometheus Benchmark - 2 17 x - Grafana](https:\/\/user-images.githubusercontent.com\/59478661\/122767296-cbc2ac80-d2a2-11eb-869f-4d6ea4e6d284.png)\r\n\r\n\r\n\r\nHello @roidelapluie. If you need another(s) missing metric(s)s, tell me !\r\nIs \"issue\" about Golang Garbage Collector ? ","@codesome what is your opinion here?\r\n\r\nI see that you probably have a long retention, so I am wondering if the reduction in carnality is spread alongside all the blocks already? Overall the graphes do not show anything special.\r\n\r\nCan you tell us if you use the released Prometheus or build your own prometheus ? Thanks!","Hello,\r\n\r\nRetention is pretty little (6 days). Carnality seem had effect if I look RSS mem usage \r\n![image](https:\/\/user-images.githubusercontent.com\/59478661\/123981517-a6305400-d9c2-11eb-86b3-43013f8e9584.png)\r\n\r\nUnfortunately, we cannot MAP RSS usage and container memory usage\r\n![image](https:\/\/user-images.githubusercontent.com\/59478661\/123981753-c7914000-d9c2-11eb-87a2-1398a19f0a48.png)\r\n\r\n\r\nBut We keep notice slightly different between container mem usage and RSS (~20 - 30%)\r\n\r\ntop usage in container give\r\n ```\r\nMem total:32891980 anon:4968936 map:1462764 free:20533648\r\n slab:367232 buf:267868 cache:6251124 dirty:828 write:0\r\nSwap total:0 free:0\r\n  PID   VSZ^VSZRW^  RSS (SHR) DIRTY (SHR) STACK COMMAND\r\n    1 19.5g 4636m 5100m     4 4117m     0   140 \/bin\/prometheus --web.console.templates=\/etc\/prometheus\/consoles --web.console.libraries=\/etc\/prometheus\/console_libraries --config.file=\/etc\r\n   21  1348   204   656   516    76     0   140 sh\r\n   27  1332   188   644   516    64     0   140 top\r\n ```\r\n\r\nDo you have idea regarding usage comprehension ?","It is strange that you have 80 blocks for 6 days. I would expect way less.","Could you run promtool tsdb list?","Hi @roidelapluie \r\nThanks for support !\r\nBelow output of requested command\r\n```\r\n\/prometheus $ promtool tsdb list \/prometheus\/\r\nBLOCK ULID                  MIN TIME       MAX TIME       DURATION      NUM SAMPLES  NUM CHUNKS   NUM SERIES   SIZE\r\n01F8Z8HWK60F4A7VXFQQKH3SV6  1624536000004  1624543200000  1h59m59.996s  101627801    906359       534030       220502153\r\n01F8ZFDKV7RDA2PETATCNJA1ZN  1624543200027  1624550400000  1h59m59.973s  102749545    920150       500317       219801818\r\n01F8ZP9B37R7NC34MT2485NGHG  1624550400037  1624557600000  1h59m59.963s  105712324    938338       512477       224669845\r\n01F8ZX52B7HSFTPQG97YAYBYDM  1624557600031  1624564800000  1h59m59.969s  104648519    898306       453458       210850437\r\n01F9040SK6M1YVCFGBN83BZXZW  1624564800002  1624572000000  1h59m59.998s  104324624    874612       427211       203671048\r\n01F90AWGV6GN9DH8D6MZT015H7  1624572000000  1624579200000  2h0m0s        104344740    875819       428361       204261419\r\n01F90HR836TC196AXY0DZSPZT3  1624579200015  1624586400000  1h59m59.985s  108924616    1198885      751030       275639245\r\n01F90RKZA9BFHVY3CVM6NZ3EJA  1624586400008  1624593600000  1h59m59.992s  90637218     1006958      669668       244416625\r\n01F90ZFPJJA6HYG9M0GBP33HZ1  1624593600037  1624600800000  1h59m59.963s  90617665     918300       578744       227407055\r\n01F916BDQGTPRFR0F60GNFX3P6  1624600800000  1624608000000  2h0m0s        95803935     1016943      653669       252235303\r\n01F91D74ZPSJ58S2985J42XYFQ  1624608000013  1624615200000  1h59m59.987s  94835173     1099395      758725       272149392\r\n01F91M2W8KXKX7N730NZJT3DR3  1624615200018  1624622400000  1h59m59.982s  95365312     1092461      729747       263673639\r\n01F91TYKG0065XZMYAKPHS63W9  1624622400017  1624629600000  1h59m59.983s  96748371     1058053      690090       258736043\r\n01F921TAVT0W59KVPYFX7QDQR6  1624629600015  1624636800000  1h59m59.985s  99687882     1331806      957385       304975115\r\n01F928P2005XH23DGP3NBHE4A9  1624636800006  1624644000000  1h59m59.994s  101246104    1043868      662360       260570637\r\n01F92FHS88W9RQ6YH7S9GWZ0X1  1624644000013  1624651200000  1h59m59.987s  96342391     970849       575171       231007681\r\n01F92PDGFRVADHP578Y13BKJB5  1624651200007  1624658400000  1h59m59.993s  95147664     871706       475173       214809953\r\n01F92X97QTR29VA4ZHDCJVV99Q  1624658400019  1624665600000  1h59m59.981s  93841721     843045       445585       206748429\r\n01F9344Z0DGT193BSP23SPTS1E  1624665600040  1624672800000  1h59m59.96s   95822609     944089       547531       225409325\r\n01F93B0P7NEYVVVJDC717EC3X7  1624672800005  1624680000000  1h59m59.995s  95717571     974518       609977       236114221\r\n01F93HWDD16G6746DBJ93KKADH  1624680000011  1624687200000  1h59m59.989s  90752650     853936       499300       209094392\r\n01F93RR4N0QTW6KM5BXGTM6844  1624687200012  1624694400000  1h59m59.988s  95936711     888450       509299       220938098\r\n01F93ZKVZKZRHGCZ7C6YV7BX53  1624694400012  1624701600000  1h59m59.988s  89736805     878966       526710       212040591\r\n01F946FKB6X24VH2EE2JKFKD48  1624701600024  1624708800000  1h59m59.976s  89757949     870371       507087       208288740\r\n01F94DBAD0DARFW8PDZ50H0T7N  1624708800022  1624716000000  1h59m59.978s  87714382     805271       446697       193823231\r\n01F94M71N20C3SC9FF5Z1VYHA0  1624716000024  1624723200000  1h59m59.976s  88971989     841001       487685       202849428\r\n01F94V2RZNXPFTNPQ78SB5HVXR  1624723200009  1624730400000  1h59m59.991s  84379100     782276       444002       183390020\r\n01F951YG50W4Z3BG5KXSCZ9S4X  1624730400023  1624737600000  1h59m59.977s  86046621     831602       482661       193078069\r\n01F958T7D1634MHK6VHG8S6XK8  1624737600010  1624744800000  1h59m59.99s   86686526     822207       469216       193167131\r\n01F95FNYMZNP47HJ02V33RP7KK  1624744800011  1624752000000  1h59m59.989s  83917665     764424       413172       179054225\r\n01F95PHP3778HB6DA8TTW2BYD6  1624752000000  1624759200000  2h0m0s        83987940     837382       486017       193676735\r\n01F95XDD85FAJ8JFPCVEGDK4MY  1624759200020  1624766400000  1h59m59.98s   86890671     958651       640514       223213795\r\n01F96494G8KR6W8MWWRE7565H8  1624766400004  1624773600000  1h59m59.996s  83142694     840745       505765       194984885\r\n01F96B4VQEHE8VF8VWDM3VRBYW  1624773600022  1624780800000  1h59m59.978s  84135047     781018       467307       191943949\r\n01F96J0JZFHPG0RRT6YMYQEHGJ  1624780800026  1624788000000  1h59m59.974s  83168014     861388       537658       201684517\r\n01F96RWA7ZFM03XAXE03PVN1FX  1624788000006  1624795200000  1h59m59.994s  83440063     867400       534362       199467701\r\n01F96ZR1G02XN7S2DAHKV7JSDA  1624795200011  1624802400000  1h59m59.989s  81752815     775715       441819       185243735\r\n01F976KRR79QGG29K7HDKC92F8  1624802400012  1624809600000  1h59m59.988s  79676039     746875       412052       177343748\r\n01F97DFFZGCCK9MGEG608P523H  1624809600000  1624816800000  2h0m0s        79188176     763500       437253       172676953\r\n01F97MB780CT6ETCR108D489K6  1624816800000  1624824000000  2h0m0s        79842259     779232       449450       178730779\r\n01F97V6YG6ZMF196RVEEKFT90D  1624824000043  1624831200000  1h59m59.957s  78996653     770497       440050       171190435\r\n01F9822NQY8AP57CGWJJHSP1C2  1624831200009  1624838400000  1h59m59.991s  79177398     756027       426211       172386205\r\n01F988YCZZ70Z8WA6SG6AJF3AK  1624838400004  1624845600000  1h59m59.996s  78721004     829297       515652       185528787\r\n01F98FT47EB7EXC9NMXZEV6P9R  1624845600001  1624852800000  1h59m59.999s  80448886     916974       632934       206635053\r\n01F98PNVFDJWWAJ1JC6MQ518R2  1624852800029  1624860000000  1h59m59.971s  74887217     712827       408950       162065429\r\n01F98XHJNH911TGJQJZWH2F1D7  1624860000022  1624867200000  1h59m59.978s  75330552     906671       626753       205190396\r\n01F994D9XJ46K81PNFGZTWK52G  1624867200000  1624874400000  2h0m0s        85721263     1113089      828753       257830172\r\n01F99B915MZ5KWQCA183F76Q2C  1624874400025  1624881600000  1h59m59.975s  83771919     830568       500352       191315291\r\n01F99J4RDMD1N7A964PRWKZVB8  1624881600008  1624888800000  1h59m59.992s  81543623     777386       445138       177075823\r\n01F99S0FQ2NH2KZGV2ZESQQP88  1624888800041  1624896000000  1h59m59.959s  84338926     967341       635594       213375198\r\n01F99ZW6XH0R5X8EVK68WC4BJJ  1624896000002  1624903200000  1h59m59.998s  83322450     823082       480385       191010584\r\n01F9A6QY5JGVSRPTMEVNXMQ7FJ  1624903200000  1624910400000  2h0m0s        82982986     792766       444175       181739137\r\n01F9ADKNDJPPEFYVBZGZRE40NR  1624910400005  1624917600000  1h59m59.995s  83724415     786142       440464       181088724\r\n01F9AMFCNHPGZGKQWPMKDA44EP  1624917600010  1624924800000  1h59m59.99s   82482765     769139       428679       175744056\r\n01F9AVB3XHJXMQBZXFCS1W0X4F  1624924800000  1624932000000  2h0m0s        81808211     764275       420691       171933566\r\n01F9B26V5H6YRFZ3NCPZSA6CRS  1624932000028  1624939200000  1h59m59.972s  84669023     909510       594530       206388234\r\n01F9B92JDJYF3GP1KKRSPGK9WN  1624939200009  1624946400000  1h59m59.991s  83858383     850130       509768       193295879\r\n01F9BFY9SEK4NFMYPFBXZ2MZD0  1624946400000  1624953600000  2h0m0s        81584812     947849       632082       212615248\r\n01F9BPT11JFV95J6GFM9F5NPY4  1624953600002  1624960800000  1h59m59.998s  86318788     955497       633227       221830511\r\n01F9BXNR9J6Q1ASVKVX8648B97  1624960800010  1624968000000  1h59m59.99s   79727221     819786       514900       188031015\r\n01F9C4HFJQTNGAKXGMPGSDAZ3E  1624968000001  1624975200000  1h59m59.999s  81097636     840618       516514       191046516\r\n01F9CBD6SY6R4Z9ZRS578SM4C1  1624975200000  1624982400000  2h0m0s        83201259     991638       707650       225744693\r\n01F9CJ8Y37B5V8M17NJK0Y4C1M  1624982400007  1624989600000  1h59m59.993s  85393451     780170       429364       185593731\r\n01F9CS4NA7V3DDHH69GTQDMXX0  1624989600029  1624996800000  1h59m59.971s  82786814     818045       477798       182297231\r\n01F9D00CJH1CA2N97CPWVHJ42K  1624996800004  1625004000000  1h59m59.996s  85033824     791452       447745       181257232\r\n01F9D6W3STPRAPCT9RM125QBG9  1625004000001  1625011200000  1h59m59.999s  83941409     797628       448034       181989011\r\n01F9DDQV361RAKPHPAWGPYZQBT  1625011200008  1625018400000  1h59m59.992s  83660641     770951       421535       177923311\r\n01F9DMKJB64G17MB4CXHHFF0GD  1625018400006  1625025600000  1h59m59.994s  83049582     811583       482061       184734612\r\n01F9DVF9CJ5H0EAD5HCZARJANF  1625025600000  1625032800000  2h0m0s        87298571     821931       479753       196109856\r\n01F9E2B0PXDM790EKWFAT918TC  1625032800038  1625040000000  1h59m59.962s  84001619     897584       564317       209007276\r\n01F9E96QYW410FF6J0BCCWQHJ9  1625040000004  1625047200000  1h59m59.996s  90520936     1159622      869488       269781610\r\n01F9EG2F7357QW3DCY6DCNB7S2  1625047200004  1625054400000  1h59m59.996s  92011038     923832       569127       218655018\r\n01F9EPY6EZ6C52BZC8C0BEHD3B  1625054400001  1625061600000  1h59m59.999s  90128901     1023183      671498       235699700\r\n\/prometheus $ \r\n```\r\n\r\n73 blocks","Your Prometheus is not compacting the blocks. Are you using thanos?","Indeed We use Thanos as sidecar to keep metrics > 6 days","Do you have some ideas ?"],"labels":["kind\/more-info-needed"]},{"title":"Promtool should support openmetrics","body":"Promtool should support openmetrics, next to the text format.\r\n\r\nMaybe via --format=openmetrics","comments":["Hi, does this mean check openmetrics format like check prom text format, could you tell me?","This issue is to add support for `promtool check metrics --format=openmetrics`","Thanks, i would like to try  if not anybody is working on this.","Hi, i'm a little confused about the check rule. Do we parse the text wire openmetrics format into  prometheus MetricFamily, and then encoded the metrics into bytes stream, at last use promlint package to do the check?  need your help, thanks :) @roidelapluie ","This issue would require a openmetrics text parser in go, which does not exist at the moment. We only have a textformat parser in go.","Here is the textparser in go: https:\/\/github.com\/prometheus\/common\/blob\/main\/expfmt\/text_parse.go\r\nWe should have one openmetrics parser in go for this issue. It should be developed next to the text parser, in the prometheus\/common repository.","Thanks, i'm understanding the issue better!","Is this Issue still valid and can I try it? :)","I think so, especially that prometheus supports back filling from openmetrics https:\/\/github.com\/prometheus\/prometheus\/pull\/8084"],"labels":["help wanted","component\/promtool","priority\/P3","kind\/feature"]},{"title":"Import a package that does not exist","body":"The client package imported by the code on [line 25](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/documentation\/examples\/remote_storage\/remote_storage_adapter\/influxdb\/client.go#L25) no longer exists in influxdb of version [v2.0.7](https:\/\/github.com\/influxdata\/influxdb\/releases\/tag\/v2.0.7) (the latest version). It still exists in its last [v1.9.1](https:\/\/github.com\/influxdata\/influxdb\/releases\/tag\/v1.9.1) .So should we update the code of this module synchronously?\r\n","comments":["Yes, if the example keeps working. Are you willing to contribute this? Thanks!","Can I work on this? Any starting point would be helpful :)","I see that this issue is still open. I am an active contributor to the open-source community. Please acknowledge me so that I can start working on this."],"labels":["help wanted","kind\/cleanup","component\/remote storage","priority\/P3"]},{"title":"Remote write on slower CPUs not catching up when high ingest rate","body":"**What did you do?**\r\n\r\nConfigured remote write on a server with ~211k samples\/s, 3.2M series and 12GB free RAM.\r\n\r\n**What did you expect to see?**\r\n\r\nRemote write working OK\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nRemote write is incrementally lagging behind TSDB and requires doubling our current Prometheus memory footprint.\r\n\r\n- Difference between `prometheus_remote_storage_highest_timestamp_in_seconds `and `prometheus_remote_storage_queue_highest_sent_timestamp_seconds `starts at ~400 seconds and tends to infinity, increasing ~15 seconds per minute\r\n- `prometheus_remote_storage_samples_pending `never goes below 50k\r\n- `prometheus_remote_storage_shards_desired `is above prometheus_remote_storage_shards by 10-20\r\n\r\nSo I assume remote write is requiring much more memory to work properly in this scenario. Pending samples decrease if configuring more shards, but then OOM hits. In our server with 32GB RAM and 20GB already used by Prometheus, remote write turns out prohibitive.\r\n\r\n**Environment**\r\n\r\n* Prometheus version: 2.26\r\nNumber of Series   3215988\r\nNumber of Chunks   10094950\r\nNumber of Label Pairs  98493\r\n   Prometheus without remote write consuming 20GB\r\n\r\n* System information:\r\n   Linux 4.15.0-124-generic x86_64\r\n   32GB total RAM\r\n\r\n* Prometheus configuration file:\r\n\r\nWe tried different configurations:\r\n\r\n- Fewer and bigger shards, with up to 10GB increase in RAM\r\n- More and smaller shards; only noticed more CPU used\r\n- Different batch send sizes (samples per send)\r\n- (read https:\/\/github.com\/prometheus\/prometheus\/issues\/5166)\r\n\r\nLast config used:\r\n```\r\nremote_write:\r\n  - url: http:\/\/<remote_endpoint>\r\n    queue_config:\r\n      min_shards: 20\r\n      max_shards: 20\r\n      capacity: 500000\r\n      max_samples_per_send: 200000\r\n      batch_send_deadline: 3s\r\n```\r\n\r\n* Logs:\r\n```\r\n\r\nts=2021-06-03T16:44:27.802Z caller=main.go:975 msg=\"Completed loading of configuration file\" filename=\/etc\/prometheus\/prometheus.yml totalDuration=19.869602402s remote_storage=19.57459134s web_handler=18.825\u00b5s query_engine=36.299\u00b5s scrape=97.226061ms scrape_sd=102.553221ms notify=215.192\u00b5s notify_sd=1.79649ms rules=85.807098ms\r\nts=2021-06-03T16:45:20.049Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"Read series references from checkpoint\" checkpoint=\/prometheus\/wal\/checkpoint.00057444\r\nts=2021-06-03T16:45:20.084Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"Tailing WAL\" lastCheckpoint=\/prometheus\/wal\/checkpoint.00057444 checkpointIndex=57444 currentSegment=57445 lastSegment=57550\r\nts=2021-06-03T16:45:20.084Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"Processing segment\" currentSegment=57445\r\n...\r\nts=2021-06-03T16:51:21.727Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"Processing segment\" currentSegment=57550\r\nts=2021-06-03T16:51:39.634Z caller=dedupe.go:112 component=remote level=info remote_name=51b711 url=http:\/\/remote msg=\"Done replaying WAL\" duration=7m12.017743087s\r\nts=2021-06-03T16:51:40.618Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"runShard timer ticked, sending samples\" samples=11154 shard=3\r\nts=2021-06-03T16:51:40.618Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"runShard timer ticked, sending samples\" samples=11232 shard=4\r\n...\r\nts=2021-06-03T16:51:47.554Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.calculateDesiredShards samplesInRate=211258.9384042259 samplesOutRate=76165.5 samplesKeptRatio=1 samplesPendingRate=135093.4384042259 samplesPending=9.21088971442425e+07 samplesOutDuration=1.2216670717 timePerSample=1.6039638309995995e-05 desiredShards=18.162450915020234 highestSent=1.622738671e+09 highestRecv=1.622739107e+09\r\nts=2021-06-03T16:51:47.555Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.updateShardsLoop lowerBound=11.2 desiredShards=18.162450915020234 upperBound=20.8\r\nts=2021-06-03T16:51:47.571Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"runShard timer ticked, sending samples\" samples=38440 shard=2\r\nts=2021-06-03T16:51:47.578Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"runShard timer ticked, sending samples\" samples=38540 shard=14\r\n...\r\nts=2021-06-03T16:51:57.548Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.calculateDesiredShards samplesInRate=207850.6907233807 samplesOutRate=90107.52 samplesKeptRatio=1 samplesPendingRate=117743.17072338071 samplesPending=9.124645322756413e+07 samplesOutDuration=1.4797343033400001 timePerSample=1.6421873594345955e-05 desiredShards=18.397674977911443 highestSent=1.622738678e+09 highestRecv=1.622739117e+09\r\nts=2021-06-03T16:51:57.548Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.updateShardsLoop lowerBound=11.2 desiredShards=18.397674977911443 upperBound=20.8\r\nts=2021-06-03T16:51:57.889Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"runShard timer ticked, sending samples\" samples=31817 shard=5\r\n...\r\nts=2021-06-03T16:53:57.542Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.calculateDesiredShards samplesInRate=211063.34899871438 samplesOutRate=138086.28516344595 samplesKeptRatio=1 samplesPendingRate=72977.06383526843 samplesPending=1.0088828082138547e+08 samplesOutDuration=1.2585937397887086 timePerSample=9.114545577781118e-06 desiredShards=11.11925485235289 highestSent=1.622738759e+09 highestRecv=1.622739237e+09\r\nts=2021-06-03T16:53:57.542Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=QueueManager.updateShardsLoop lowerBound=11.2 desiredShards=11.11925485235289 upperBound=20.8\r\nts=2021-06-03T16:53:57.542Z caller=dedupe.go:112 component=remote level=debug remote_name=51b711 url=http:\/\/remote msg=\"Not downsharding due to being too far behind\"\r\n...\r\nts=2021-06-03T17:41:46.587Z caller=dedupe.go:112 component=remote level=debug remote_name=c00095 url=http:\/\/remote msg=QueueManager.calculateDesiredShards samplesInRate=204886.2989921398 samplesOutRate=95566.91107518307 samplesKeptRatio=1 samplesPendingRate=109319.38791695674 samplesPending=2.0877913867299047e+08 samplesOutDuration=1.6238373785915945 timePerSample=1.699162775402578e-05 desiredShards=38.95632579575029 highestSent=1.622741087e+09 highestRecv=1.622742106e+09\r\n\r\n```\r\n","comments":["Tried same configuration with speedier CPU  (but same RAM size), and it works. The issue should be rephrased to something like:\r\n- CPU speed is a considerable factor when tuning remote write, in such a way that less memory is needed and higher network  throughput achieved for the same workload.\r\n","Thanks for the detailed information.\r\n\r\nIt is documented that remote write will take ~25% more memory than a Prometheus with remote write, but I do not see anything about CPU guidance on https:\/\/prometheus.io\/docs\/practices\/remote_write\/. I would say we should update that page to also give a guideline on additional CPU usage required, does that seem like a reasonable solution?","To clarify: we were using rather old on-prem hardware, but upfront we didn't suspect it was related to \"processing speed\", because CPU was not saturating (peaked ~50%) and network interfaces averaged 40 Mbps throughput, so we perceived it as a remote write inefficiency: not fully using resources, although requiring ~50% more memory. Then, when we upgraded to newer hardware (beefier cores), --but same core quantity\/ram size--, it catched up with no further shards tuning.\r\n","I don't think there is anything that needs to be done here, other than the docs that Chris mentioned?","I wonder if this was related to the large amount of context switches that could be happening before https:\/\/github.com\/prometheus\/prometheus\/pull\/9830. That would explain the inefficiencies seen where we were not using all available resources. If so this will have been fixed in 2.33.","I couldn't retrieve context switch metrics of the problematic setup, but I can document here the leap of CPU hardware that solved remote write not catching up.\r\n\r\nDidn't catch up:\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              16\r\nOn-line CPU(s) list: 0-15\r\nThread(s) per core:  1\r\nCore(s) per socket:  1\r\nSocket(s):           16\r\nNUMA node(s):        4\r\nVendor ID:           AuthenticAMD\r\nCPU family:          21\r\nModel:               2\r\nModel name:          AMD Opteron 63xx class CPU\r\nStepping:            0\r\nCPU MHz:             2300.016\r\nBogoMIPS:            4600.03\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           64K\r\nL1i cache:           64K\r\nL2 cache:            512K\r\nNUMA node0 CPU(s):   0-3\r\nNUMA node1 CPU(s):   4-7\r\nNUMA node2 CPU(s):   8-11\r\nNUMA node3 CPU(s):   12-15\r\n```\r\n\r\nWorked OK:\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              16\r\nOn-line CPU(s) list: 0-15\r\nThread(s) per core:  1\r\nCore(s) per socket:  1\r\nSocket(s):           16\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel Xeon Processor (Skylake, IBRS)\r\nStepping:            4\r\nCPU MHz:             2099.990\r\nBogoMIPS:            4199.98\r\nVirtualization:      VT-x\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            4096K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-15\r\n```"],"labels":["component\/remote storage","priority\/P3","component\/documentation"]},{"title":"storage.tsdb.retention.size not working on 2.27","body":"**What did you do?**\r\n\r\nWe were running out of disk by using the retention.time setting. We had initially set this to 30 days but kept filling up our data directory. We stopped using retention.time and instead adjusted to retention.size. After reporting this bug to not be working on 2.21 we were advised to upgrade. We then upgraded to the latest version of 2.27 and again set the retention size setting instead of retention time. Still it does not appear to be working.\r\n\r\n**What did you expect to see?**\r\n\r\nWe have an 8.8TB partition for our Prometheus data. After reading that the retention.size setting does not include WAL files or potentially compaction blocks we set the retention.size to 7500GB assuming 1.3TB would be plenty of space for WAL and anything else that is not being accounted for. We'd expect that not all 8.8TB would be used.\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\nDisk filled up, all 8.8TB were used.\r\n\r\n**Environment**\r\n\r\nCentOS Linux release 7.8.2003 (Core)\r\n\r\n* System information:\r\n\r\n\tLinux 3.10.0-1127.18.2.el7.x86_64 x86_64\r\n\r\n* Prometheus version:\r\n\r\n\tprometheus, version 2.27.1 (branch: HEAD, revision: db7f0bcec27bd8aeebad6b08ac849516efa9ae02)\r\n  build user:       root@fd804fbd4f25\r\n  build date:       20210518-14:17:54\r\n  go version:       go1.16.4\r\n  platform:         linux\/amd64\r\n\r\nQUESTION: Does the retention size setting only apply to one block of data? or the entire set? I'm seeing several blocks of data.\r\n\r\n\r\n\r\n\r\n\r\n","comments":["The WAL should even be included in the size now. Can you please provide us the following metrics\r\n\r\nprometheus_tsdb_retention_limit_bytes\r\nprometheus_tsdb_size_retentions_total\r\nprometheus_tsdb_storage_blocks_bytes\r\n\r\nand tell us what is the size of the wal?","@roidelapluie thanks for your response. Excuse my naivety but how\/where can this be queried?\r\n\r\nAt the time disk last filled up the WAL directory size was a little under 100GB.\r\n\r\nThanks!\r\n\r\n","In the \/metrics endpoint of Prometheus, you can query it on the console of Prometheus if you scrape it.","Great thanks. Below are the requested metrics:\r\n\r\nprometheus_tsdb_retention_limit_bytes 0\r\nprometheus_tsdb_size_retentions_total 0\r\nprometheus_tsdb_storage_blocks_bytes 7.487427882967e+12\r\n\r\nThanks for your help.\r\n\r\n","It looks like the retention is not set? What value are you passing to\nPrometheus?\n\nLe mer. 2 juin 2021 \u00e0 18:18, brosev ***@***.***> a \u00e9crit :\n\n> Great thanks. Below are the requested metrics:\n>\n> prometheus_tsdb_retention_limit_bytes 0\n> prometheus_tsdb_size_retentions_total 0\n> prometheus_tsdb_storage_blocks_bytes 7.487427882967e+12\n>\n> Thanks for your help.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/8884#issuecomment-853164342>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AACHHJUH53XMSPXAUDCBN4TTQZKT5ANCNFSM455LUERQ>\n> .\n>\n","Right now this is the only flag we're passing:\r\n\r\n--storage.tsdb.retention.time=25d \\\r\n\r\nIt is set in the systemd unit file. \r\n\r\nProcess looks like: \r\n\r\nprometh+  4124     1 99 Jun01 ?        6-22:22:52 \/usr\/bin\/prometheus --config.file=\/etc\/prometheus\/prometheus.yml --web.console.templates=\/usr\/local\/share\/prometheus\/consoles --web.console.libraries=\/usr\/local\/share\/prometheus\/console_libraries --storage.tsdb.path=\/export\/prometheus --storage.tsdb.retention.time=25d --web.enable-admin-api\r\n","So, you are note setting a retention size. What is this bug about?","Apologies, I should have been more clear. When we set retention size it did not work and it filled up our disk within a matter of hours. We could not leave it and had to revert back to using retention time.","Can you please provide those metrics from the time where it was configured with the retention?\r\n\r\nI also note that Prometheus only cares about its TSDB, if you had other files on the filesystem, Prometheus would not count them in its limit.","Jumping into this one - I'm seeing this behavior as well\r\n\r\n```\r\n\/bin\/prometheus --web.console.templates=\/etc\/prometheus\/consoles --web.console.libraries=\/etc\/prometheus\/console_libraries --storage.tsdb.retention.size=25GB --config.file=\/etc\/prometheus\/config_out\/prometheus.env.yaml --storage.tsdb.path=\/prometheus --storage.tsdb.retention.time=30d --web.enable-lifecycle --storage.tsdb.no-lockfile --web.external-url=http:\/\/localhost --web.route-prefix=\/\r\n```\r\n\r\n```\r\n\/tmp $ \/bin\/prometheus --version\r\nprometheus, version 2.19.1 (branch: HEAD, revision: eba3fdcbf0d378b66600281903e3aab515732b39)\r\n  build user:       root@62700b3d0ef9\r\n  build date:       20200618-16:35:26\r\n  go version:       go1.14.4\r\n```\r\n\r\n```\r\n[james@shadow|hny9:~]$ curl -s localhost:9090\/metrics | egrep 'retention|storage_blocks_bytes' | grep -v \"#\"\r\nHandling connection for 9090\r\nprometheus_tsdb_retention_limit_bytes 2.68435456e+10\r\nprometheus_tsdb_size_retentions_total 0\r\nprometheus_tsdb_storage_blocks_bytes 3.1265358383e+10\r\nprometheus_tsdb_time_retentions_total 2\r\n```\r\n\r\n```\r\n\/prometheus $ du -hs \/prometheus\/*\r\n1.7G    \/prometheus\/01F7RMM2DPXB3B9P1TQZS68HPB\r\n1.8G    \/prometheus\/01F7YE0PSKWF3EMXZRCVDA5WTH\r\n1.8G    \/prometheus\/01F847DABT0DTH7W6F7JKG94DX\r\n1.9G    \/prometheus\/01F8A0SYQ3HXKFKYJ78NG5SPAG\r\n2.0G    \/prometheus\/01F8FT6JW3KTRYGKK2NMNHFR6W\r\n2.0G    \/prometheus\/01F8NKK6E0FF72THD2SHMT2BBN\r\n2.1G    \/prometheus\/01F8VCZWJPC7YQ8QJF3NAW8CA9\r\n2.1G    \/prometheus\/01F916CHTSNZW2Q870AVG3FC9J\r\n2.1G    \/prometheus\/01F96ZS9QS1V0S7S577RZRWR4R\r\n2.2G    \/prometheus\/01F9CS5T5C3XY9G8F4XGEN2RN5\r\n2.2G    \/prometheus\/01F9JJJEEQ5HAFQMVY7RZX8T1G\r\n2.3G    \/prometheus\/01F9RBZ6HWW41MXSJ1K4BR6SRY\r\n799.5M  \/prometheus\/01F9T9RA342D7SR8BFXXN2T602\r\n795.6M  \/prometheus\/01F9W7HV59BFG68K1DKC6HE0D2\r\n796.4M  \/prometheus\/01F9Y5BCT6040Z385WS6D2T0YX\r\n799.5M  \/prometheus\/01FA034Y6GA6BEWJS98QHGA3A9\r\n796.3M  \/prometheus\/01FA20YFN54TN4DCXZJG4D57C3\r\n106.8M  \/prometheus\/01FA3YQ44MM1J33M54A8E4EYDV\r\n797.6M  \/prometheus\/01FA3YR0MSB27RPZDX9MZWAAGZ\r\n173.2M  \/prometheus\/chunks_head\r\n0       \/prometheus\/file\r\n20.0K   \/prometheus\/queries.active\r\n1.8G    \/prometheus\/wal\r\n\/prometheus $ df -h \/prometheus\r\nFilesystem                Size      Used Available Use% Mounted on\r\n\/dev\/vdb                 31.2G     31.2G         0 100% \/prometheus\r\n```\r\n","This feature is bugged in 2.19.1. please upgrade.","It looks like it is not working even in latest version.\r\n\r\nWe are using `remote_write`, but Prometheus just HAS to write locally. We ran out of disk space first on `\/data` partition that got filled with WAL in 2.29.1, then on `\/var` that got filled by prometheus spamming logs.\r\n\r\n**fresh after cleanup of all the disk space here is what I see**\r\n\r\n```\r\n# curl -s localhost:9090\/metrics | egrep 'retention|storage_blocks_bytes' | grep -v \"#\"\r\nprometheus_tsdb_retention_limit_bytes 2.62144e+08\r\nprometheus_tsdb_size_retentions_total 0\r\nprometheus_tsdb_storage_blocks_bytes 0\r\nprometheus_tsdb_time_retentions_total 0\r\n```\r\n\r\n```\r\n# ps faux | grep prometheus\r\nuser  18348 54.4 26.8 7359380 6578712 ?     Ssl  09:46   6:02 \/opt\/prometheus\/prometheus --config.file=\/etc\/prometheus\/prometheus.yml --storage.tsdb.path=\/data\/prometheus\/ --storage.tsdb.retention.size=250MB --web.console.templates=\/opt\/prometheus\/consoles --web.console.libraries=\/opt\/prometheus\/console_libraries\r\n```\r\n\r\n```\r\n# du -sh \/data\/prometheus\/wal\r\n432M\t\/data\/prometheus\/wal\r\n```\r\n\r\n```\r\n# \/opt\/prometheus\/prometheus --version\r\nprometheus, version 2.29.1 (branch: HEAD, revision: dcb07e8eac34b5ea37cd229545000b857f1c1637)\r\n  build user:       root@364730518a4e\r\n  build date:       20210811-14:48:27\r\n  go version:       go1.16.7\r\n  platform:         linux\/amd64\r\n```\r\n\r\n**and it keeps growing**","@akamensky the size retention only deletes persistent blocks since it is not feasible performance wise to do it with WAL\/Checkpoint. The minimum space require is the space taken by `\/wal` + `\/chunks_head` (this will peak once every 2 hrs and the peak should be within a small range if the series remain same).","@codesome Does the size of retention considers the folder ending with `.tmp-for-creation` which are created periodically and deleted after some time ?\r\nI have noticed that the total size occupied in the pod goes beyond the size set by the flag `storage.tsdb.retention.size` + `size of \/wal` + `size of \/chunk_heads` ; whenever this folder is created. ","@PritomAhmed it is not counted since it will soon become a non-temporary and size retention will kick off at that time.\r\n\r\nI can see how all of this is not intuitive, we need to improve docs (which I will get to in near future). Size retention is not a strict limit in Prometheus, rather a soft hint on when to start deleting data. Honouring the size retention when writing a new block with `.tmp-for-creation` suffix will open new failure cases and I don't think it is worth it.","@codesome Is there a parameter that controls \r\n1. How often a temp file gets flushed  \r\n2. How to limit the size of the temp file grows to before it gets flushed ","@codesome I noticed if a pod gets killed, a tmp file (as large as 148GB) disappears. What is the impact of losing this temp file ? Because in Kubernetes, I can't control when a pod gets restarted.","> What is the impact of losing this temp file ?\r\n\r\nNothing. The operation that created this temp file will run itself again when Prometheus is up.\r\n\r\nThis temp file is just a block that is not fully created yet. So you can think of it as a block which is not ready for reading. Existing blocks are also renamed to tmp blocks before deleting it."],"labels":["kind\/more-info-needed"]},{"title":"Timezone support in Prometheus","body":"I have put a design document to introduce simple timezone handling in Prometheus:\r\n\r\nhttps:\/\/docs.google.com\/document\/d\/1xfw1Lb1GIRZB_-4iFVGkgwnpwuBemWfxYqFdBm7APsE\/edit#","comments":["cc @juliusv @beorn7 feel free to react."],"labels":["priority\/Pmaybe","not-as-easy-as-it-looks"]},{"title":"promtool: junit output","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\n\r\nWe are using promtool for checking rule syntax as well as evaluating rule unit-tests. For this to integrate nicely in our tooling, we have created a wrapper to produce junit compatible output: https:\/\/github.com\/sorend\/promunit\r\n\r\nJunit compatible output is supported by many tools, in our case we use Jenkins to collect it during CI phase.\r\n\r\nOur proposal is to make promtool checks output junit (or other standard) machine-parseable test\/check results.\r\n","comments":["cc @dgl WDYT?","Yes, I think provided it isn't too heavy having xUnit XML output would make the most sense as many tools support that. I have been thinking about refactoring the output generation as the text output right now is a bit messy (and not tested), so could make sense to combine with a bit of a refactor."],"labels":["kind\/enhancement","component\/promtool","priority\/P3"]},{"title":"Allow for query execution within range vector","body":"## Proposal\r\n**Use case. Why is this important?**\r\nWhen trying to query the amount of bandwidth that has gone over an interface over the past 30 days, one can simply run this query (quick draft, only calculating In, skipping Out):\r\n`increase(ifHCInOctets{instance=\"<ip>\",ifDescr=\"<ifdescr>}[30d])`\r\n\r\nThis works fine, however, when we want to alert when the amount of bandwidth has gone over X TB in the current month (1st day of the month 00:00:00 until now), one cannot use 30d but needs to enter the amount seconds since the start of the month.\r\n\r\nFor some reason, I cannot find any documentation nor do I succeed in replacing the range vector [30d] with a query that calculates the amount of seconds since this point in time. It seems that there is no room for executing a query within this range vector and only static values are allowed (Xs,Xm,Xh,Xd,...)\r\n\r\nUsing a recording rule that calculates the seconds_this_month, one would be in the possibility of using the vector [seconds_this_month].\r\nWhen we implement this in the demo query, it would look like this:\r\n`increase(ifHCInOctets{instance=\"<ip>\",ifDescr=\"<ifdescr>}[seconds_this_month])`\r\n\r\nIs there any chance this might be added in a future version or, if it really does exist, that this can be added to the documentation?\r\nIf there is no need for this method because there is another way of achieving this, please do let me know.\r\n\r\nThanks in advance!","comments":["Yes there are chances that this is added in the future. We are looking for someone willing to put that in a design document, with all the options to achieve that and other things.","@roidelapluie what if we update the `[ ]` in PromQL to something like `[ start_time => end_time ]`. The `.*_time` here can be `unix seconds` or `RFC3339` and we fetch the data for the corresponding vector selector for that time range only."],"labels":["component\/promql","not-as-easy-as-it-looks"]},{"title":"docker_sd: Deduplicate containers attached to multiple networks","body":"## Proposal\r\n\r\nI have several containers attached to multiple Docker networks (e.g. `frontend`, `backend`). At the moment `docker_sd` creates a scraping target for each container on **all** networks. As some services are only attached to one network, it's not feasible to use relabelling to drop all containers in a certain network (or only keep those).\r\n\r\nIt would be nice if `docker_sd` could be configured to only use the first network, for example or find some other way to avoid duplicate targets.","comments":["This is interesting. Can you workaround the issue by taking multiple labels in input_labels and use regexes ? e.g. can you find which container is attached to which network based on the container name?\r\n\r\nAre you using docker swarm?","> Can you workaround the issue by taking multiple labels in input_labels and use regexes ? e.g. can you find which container is attached to which network based on the container name?\r\n\r\nThe container network name is stored in `__meta_docker_network_name`, so I tried setting a label `prometheus.network` on the container, but AFAIK there's no way to compare `__meta_docker_network_name` and the resulting `__meta_docker_container_label_prometheus_network` during relabeling, as the RE2 engine doesn't support back references.\r\n\r\n> Are you using docker swarm?\r\n\r\nNo I'm using `docker_sd`, but I guess this is also applies to `docker_swarm_sd`.","Indeed, you would need to hardcode them:\r\n\r\n```\r\n- source_labels: [__meta_docker_network_name,__meta_docker_container_label_prometheus_network]\r\n  regex: frontend;frontend\r\n  target_label: __tmp_docker_keep\r\n  replacement: keep\r\n- source_labels: [__meta_docker_network_name,__meta_docker_container_label_prometheus_network]\r\n  regex: backend_backend\r\n  target_label: __tmp_docker_keep\r\n  replacement: keep\r\n- source_labels: [__tmp_docker_keep]\r\n  regex: keep\r\n  action: keep\r\n```\r\n\r\nor, shorter\r\n```\r\n- source_labels: [__meta_docker_network_name,__meta_docker_container_label_prometheus_network]\r\n  regex: frontend;frontend|backend;backend\r\n  action: keep\r\n```\r\n\r\n\r\nThe issue with the approach you are proposing is that I do not know what are the guarantees from Docker that the networks are ordered in fixed order between API calls, and how it behaves when you attach\/detach to networks, then restarts the daemon, etc.","I'm not sure about the behavior of the Docker API either, it would just be nice to have **some** built-in way to avoid duplicate targets.","okay. I think we could have a networks array where we take a list of networks. For each container, we add its target on that network. If not network is matched, we dismiss the target. Does it sound like a solution?","Ok, I'm not sure I understand you correctly. Given the following container setup (and assuming all containers have Prometheus metric endpoints), how can we make sure the \"api\" container doesn't end up twice in the target list?\r\n\r\n- container: frontend\r\n  networks: `frontend`\r\n\r\n- container: api\r\n  networks: `frontend`, `backend`\r\n\r\n- container: database\r\n  networks: `backend`","We could implement\/explore with an alternative mode, like we do for kubernetes, where one target has the addresses over all the networks.\r\n\r\nI have tagged this issue as P3.","Currently, for each container we retrieve we loop through each of its networks and furthermore each of its ports, adding a target for each network+port combination.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/9e88c3bb4dd7b01456639df06df07053d5626445\/discovery\/moby\/docker.go#L186-L205\r\n\r\nI think we could just provide a configuration setting to `break` after the first network iteration, allowing target(s) to be added for only one network per container (there could be multiple ports).","If we break, we should order them. We should also have one label for each  network address (without port).","> If we break, we should order them.\r\n\r\nUsing what method?\r\n\r\n> We should also have one label for each network address (without port).\r\n\r\nSo when we break and only add a target for the first network, we should add extra labels to that target with the other network addresses (like `__meta_docker_network_other_frontend=127.0.0.1`)","I think Docker, Docker Swarm and Kubernetes SD should ideally behave the same way, as it boils down to \"software running containers attached to (multiple) networks\". Not saying, that the current Kubernetes implementation is the way to go, but if we come to a decision, it should apply to all three SD methods.","Kubernetes implementation is the way to go, but should be behind a flag if possible.","> > If we break, we should order them.\r\n> \r\n> Using what method?\r\n> \r\n> > We should also have one label for each network address (without port).\r\n> \r\n> So when we break and only add a target for the first network, we should add extra labels to that target with the other network addresses (like `__meta_docker_network_other_frontend=127.0.0.1`)\r\n\r\nI think it's no need to order multiple networks by default, just get the first network will be ok.\r\nPeople can order the networks in docker-compose config file.\r\nKubernetes meet the same promblem when pod has multiple networks and it just shows the first network as default network.\r\nThe command look as below:\r\n`nsenter -n -t19032 -F -- ip -o -4 addr show dev eth0 scope global`\r\n","For anyone curious, I solved the network deduplication using the `keepequal` action in the relabel configs. I also use an extra couple relabel configs to deduplicate the port (if you have a container that publishes multiple ports). I am also able to override the port when the default 80 is not the port I want. With these relabel configs I am able to monitor 13 containers (all of which use a different setup, some with http, some https, some using custom port, some using discovered port, some using a hostname, some using an ip) using just a few simple labels on each docker swarm service and double checking the prometheus UI it is truly only picking up one target on the port and network I expect.\r\n\r\nrelabel configs with comments:\r\n```yml\r\n    relabel_configs:\r\n      # Only keep containers that should be running.\r\n      - source_labels: [__meta_dockerswarm_task_desired_state]\r\n        regex: running\r\n        action: keep\r\n      # Only keep containers that have a `prometheus-port` label.\r\n      - source_labels: [__meta_dockerswarm_service_label_prometheus_port]\r\n        regex: .+\r\n        action: keep\r\n      # get the port from this target\r\n      - source_labels: [__address__]\r\n        regex: '[^\\s]+:(\\d+)'\r\n        target_label: __tmp_found_port\r\n      # keep only if this is the port we're looking for\r\n      - source_labels: [__meta_dockerswarm_service_label_prometheus_port]\r\n        target_label: __tmp_found_port\r\n        action: keepequal\r\n      # limit to one network\r\n      - source_labels: [__meta_dockerswarm_service_label_prometheus_network]\r\n        target_label: __meta_dockerswarm_network_name\r\n        action: keepequal\r\n      # replace address with prometheus-replacement-port if available\r\n      - source_labels: [__address__, __meta_dockerswarm_service_label_prometheus_replace_port]\r\n        separator: ':'\r\n        regex: '([^:]+):[^:]+:(\\d+)'\r\n        replacement: \"${1}:${2}\"\r\n        target_label: __address__\r\n      # override address if an override is provided\r\n      - source_labels: [__meta_dockerswarm_service_label_prometheus_address]\r\n        regex: '([^\\s]+)'\r\n        target_label: __address__\r\n      # override scheme if an override is provided\r\n      - source_labels: [__meta_dockerswarm_service_label_prometheus_scheme]\r\n        regex: '([^\\s]+)'\r\n        target_label: __scheme__\r\n```"],"labels":["help wanted","kind\/enhancement","component\/service discovery","priority\/P3"]},{"title":"readCheckpoint wasn't able to read all data from the checkpoint  xxx","body":"**Environment**\r\n\r\n* System information:\r\n\r\n\tLinux slave07  5.4.61-050461-generic #202008260931 SMP Wed Aug 26 09:34:29 UTC 2020 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n* Prometheus version:\r\n\r\n\t2.22.0\r\n\r\n\r\n\r\n* Logs:\r\n```\r\nts=2021-05-14T00:30:44.168Z caller=dedupe.go:112 component=remote level=error remote_name=prometheus-center-adapter url=http:\/\/localhost:9201\/write msg=\"error tailing WAL\" err=\"readCheckpoint: readCheckpoint wasn't able to read all data from the checkpoint \/prometheus\/wal\/checkpoint.00000366\/00000000, size: 13697024, totalRead: 2335785\"\r\n```\r\n","comments":["remote_write:\r\n- url: http:\/\/localhost:9201\/write\r\n  remote_timeout: 30s\r\n  name: prometheus-center-adapter\r\n  queue_config:\r\n    capacity: 2500\r\n    max_shards: 200\r\n    min_shards: 1\r\n    max_samples_per_send: 500\r\n    batch_send_deadline: 5s\r\n    min_backoff: 30ms\r\n    max_backoff: 100ms","cc @cstyan @csmarchbanks @codesome ","@fcddk can you please send us more of the logs from that Prometheus instance? I can't really debug or make any suggestions based on that one log line alone.","@cstyan   I only get this exception log, other logs did not appear.   ","Can we get prometheus --version, and the filesystem you are using?","df -hT |grep monitor: \r\n\/dev\/monitor           ext4      930G  3.5G  879G   1% \/monitor","\r\n\r\nprometheus, version 2.22.0 (branch: non-git, revision: non-git)\r\n  build user:       root@5jje19a\r\n  build date:       20210503-03:12:53\r\n  go version:       go1.15.1\r\n  platform:         linux\/amd64\r\n\r\n\r\n","cc  @cstyan do you have an idea about what is going on here?","Not with the information given, no.\r\n\r\nIIRC even if TSDB creates a new checkpoint while remote write is reading the current checkpoint, which results in deletion of the current checkpoint, remote write having the file pointer open means the file isn't actually deleted\/cleaned up until remote write closes that pointer. So without more information, off the top of my head I don't have any ideas."],"labels":["kind\/bug","component\/remote storage","kind\/more-info-needed"]},{"title":"Direct support for configuration in Kubernetes","body":"## Proposal\r\n\r\nThe current way to pass configuration such as extra alerts or rules in Prometheus on Kubernetes is to use a ConfigMap, have it mounted inside the Prometheus\/AlertManager Pods and to use a sidecar container to perform the reloading.\r\n\r\nThis works through inotify, however, the sync between the ConfigMap and the Volume depends on the configuration of Kubelet which is in charge of the sync.\r\n\r\nBy default, this can take a minute.  While this can be changed to something lower, depending on the infrastructure you might not have control over this.\r\n\r\nIdeally, Prometheus and Alertmanager would be able to directly talk to the Kubernetes API and watch for changes, allowing modifications of rules to be near instant.","comments":["I am not sure we want to interact directly with config maps but I recall @brancz to express the will to not have \"reloaders\" sidecar and therefore Prometheus would gain the ability to reload the configuration from disk as it changes by itself.","@roidelapluie that would still have the issue of you being at the mercy of the kubelet configuration which you may or may not have control over depending on the cloud vendor, and have inconsistencies to take care of.","I get the appeal of this, though I do wonder whether this is a step too far in regards to integrating. There are various projects out there allowing this, and the difficulty I have is that if we add Kubernetes integration then people will want consul\/etcd\/...\r\n\r\nI'm not saying this is not a good idea, but I'm not sure the maintenance burden given the potential scope is justified.","@brancz would perhaps some kind of HTTP endpoint be possible from which to fetch dynamic configuration from via polling be an option? (not sure if it's already possible today).\r\nIn kube everything is an API object, so theoretically speaking, if Prometheus could be given a URL to an HTTP endpoint containing the configuration, and having a configurable polling, might work?  That would it should be more compatible I guess with consolue and etcd? Although you might still have to deal with proper parsing options depending on how the other things implement or display the data....","Would auto-reloading configuration on change partially fix this? At least you would not have to 'guess' when the configmap is synced.","The ideal scenario would be that a change in configuration would be near instant. That will not always be possible, but the biggest pain right now is that you are at the mercy of however the kubelet has been configured to sync the contents of the configmap with the mounted volume in the pods.\r\n\r\nThe specific use-case here which I'm busy with,  users can configure alert rules in a GUI, the GUI app translates the configuration in Alert Rules for Prometheus in a ConfigMap, the ConfigMap contents are mounted inside the Prometheus Pod(s), and there's a config-reload sidecar which takes it from there. From a user's perspective, having to wait minutes for something to visibly happen can make them doubt if the rules have been applied or not. Unless they are informed that it can take some time.","In order to keep Prometheus less specific to Kubernetes, this can be implemented through a dedicated config reloader which is going to have a watch on a single Secret or ConfigMap and update the Prometheus config file whenever the configuration changes. Then this reloader can be run as a sidecar only for the Kubernetes use case. "],"labels":["priority\/Pmaybe","component\/config"]},{"title":"Remote write should best-effort start from a checkpoint instead of time.Now","body":"Right now remote write starts sending samples with a timestamp after `time.Now()` when Prometheus started (more or less). On restart or config reload if remote write wasn't able to flush it's pending samples cleanly or was behind for some reason there will be samples missing in the remote storage. If remote write always starts from a checkpoint in some kind of checkpoint file for it's queue name\/config hash we can avoid that situation.\r\n\r\nsee: https:\/\/github.com\/prometheus\/prometheus\/pull\/7710","comments":["I think the \"best-effort\" here would be to see if the last send data was not compressed i.e., it still lies in (tsdb) checkpoint and segments.\r\n\r\nIf the last send was from a segment and that segment still exists, resume from the next record using the offset (segment number + offset in that segment) in that segment (this needs to be stored in some file, as last successful sent).\r\notherwise, start from the starting of the checkpoint, as we do not have any means (AFAIK) to know the at send. But this can lead to too much duplicate data being sent to remote storage, so not sure if we are happy with this.\r\n\r\nWDYT?","I'm actually working on this at the moment, based on Nicole's original branch.\r\n\r\nThe main thing is that if the checkpoint file is not present, or if the segment mentioned in the checkpoint file no longer exists, we start from `time.Now()` as we do currently.","relevant \"feature\" requirements: https:\/\/github.com\/prometheus\/prometheus\/pull\/9862#issuecomment-979471947","Here is an idea how we can do it:\r\nFor alerting rules, we make use of TSDB to store the \"for\" state of alerting rules, and after a restart we use these metrics to restore the state. Note that this metric is not exposed, rather directly put into TSDB.\r\nSimilarly, how we expose the highest timestamp remote-written, we could ingest something similar to TSDB regularly (like the highest timestamp remote-written, or the WAL file number and the offset up to which it was written), and use this during startup to start remote-write from where we left. With the highest timestamp remote-written, we might still lose some samples (for example smaller timestamp present later in the WAL).\r\n\r\nPS: it might not be a good idea to remove the highest timestamp metric from the `\/metrics` output. So if we want to do it this way, we need to ingest the same metric under a different name into TSDB.","@codesome We've been calling this feature \"remote write checkpointing\" so I'll refer to it as that here. Regardless of the mechanism we use for deciding what to write (I still think either just the segment # or a bytes\/record offset into a segment is the most accurate) the main source of complexity is going to be indicating from remote write to the checkpointing code that up to that offset has been successfully written to the remote endpoint. IMO Right now we don't have boundaries we can easily delineate on except segment files, and because data coming into remote write is split across all shards we would need to accept a signal from the Watcher saying \"I've sent you all the data for segment X\" and then ensure that all the buffered samples from that point in time have been batched within a shard and successfully sent."],"labels":["help wanted","component\/remote storage","not-as-easy-as-it-looks"]},{"title":"reevaluate remote write queue config defaults","body":"We'll want to be able to maintain the same max samples\/s output with the new defaults, but bumping the max samples per send would lower the shard requirements, and upsharding can lead to DDOS like issues since we have no back pressure mechanism (related issues: https:\/\/github.com\/prometheus\/prometheus\/issues\/5166)","comments":["Should we offer multiple \"profiles\" (pre-set defaults)?\r\n\r\nprofile: high_throughtput \r\nprofile: medium_throughput\r\nprofile: adaptive\r\n\r\n?","missed that this had already been addressed in #5166\/#5267","`Should we offer multiple \"profiles\" (pre-set defaults)?` We could, but I think I'd prefer them as config examples rather than baked into Prometheus itself.","The main thing I would change is to lower max-shards from 1,000 - maybe 100.  Even that is too big for most installations.\r\n","The defaults have been changed: https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/config\/config.go#L155-L172","And it was me that changed them \ud83e\udd26\u200d\u2642\ufe0f \r\nCurrent `MaxShards: 200` is still too high, but not quite as bad."],"labels":["help wanted","component\/remote storage"]},{"title":"Test failed: TestIngressDiscoveryAddMixed ","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\n\r\nChecked out v2.26.0, and ran:\r\n```\r\nmake test\r\n```\r\n\r\n**What did you expect to see?**\r\n\r\nThe test passed\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\n```\r\n--- FAIL: TestIngressDiscoveryAddMixed (1.04s)\r\n    kubernetes_test.go:135: timed out, got 0 (max: 1) items, some events are skipped\r\n    ingress_test.go:174: \r\n        \tError Trace:\tkubernetes_test.go:154\r\n        \t            \t\t\t\tkubernetes_test.go:109\r\n        \t            \t\t\t\tingress_test.go:174\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: \"{\\\"ingress\/default\/testingress\\\":{\\\"Targets\\\":[{\\\"__address__\\\":\\\"example.com\\\",\\\"__meta_kubernetes_ingress_host\\\":\\\"example.com\\\",\\\"__meta_kubernetes_ingress_path\\\":\\\"\/\\\",\\\"__meta_kubernetes_ingress_scheme\\\":\\\"https\\\"},{\\\"__address__\\\":\\\"example.com\\\",\\\"__meta_kubernetes_ingress_host\\\":\\\"example.com\\\",\\\"__meta_kubernetes_ingress_path\\\":\\\"\/foo\\\",\\\"__meta_kubernetes_ingress_scheme\\\":\\\"https\\\"},{\\\"__address__\\\":\\\"test.example.com\\\",\\\"__meta_kubernetes_ingress_host\\\":\\\"test.example.com\\\",\\\"__meta_kubernetes_ingress_path\\\":\\\"\/\\\",\\\"__meta_kubernetes_ingress_scheme\\\":\\\"http\\\"}],\\\"Labels\\\":{\\\"__meta_kubernetes_ingress_annotation_test_annotation\\\":\\\"testannotationvalue\\\",\\\"__meta_kubernetes_ingress_annotationpresent_test_annotation\\\":\\\"true\\\",\\\"__meta_kubernetes_ingress_label_test_label\\\":\\\"testvalue\\\",\\\"__meta_kubernetes_ingress_labelpresent_test_label\\\":\\\"true\\\",\\\"__meta_kubernetes_ingress_name\\\":\\\"testingress\\\",\\\"__meta_kubernetes_namespace\\\":\\\"default\\\"},\\\"Source\\\":\\\"ingress\/default\/testingress\\\"}}\"\r\n        \t            \tactual  : \"{}\"\r\n        \t            \t\r\n        \t            \tDiff:\r\n        \t            \t--- Expected\r\n        \t            \t+++ Actual\r\n        \t            \t@@ -1 +1 @@\r\n        \t            \t-{\"ingress\/default\/testingress\":{\"Targets\":[{\"__address__\":\"example.com\",\"__meta_kubernetes_ingress_host\":\"example.com\",\"__meta_kubernetes_ingress_path\":\"\/\",\"__meta_kubernetes_ingress_scheme\":\"https\"},{\"__address__\":\"example.com\",\"__meta_kubernetes_ingress_host\":\"example.com\",\"__meta_kubernetes_ingress_path\":\"\/foo\",\"__meta_kubernetes_ingress_scheme\":\"https\"},{\"__address__\":\"test.example.com\",\"__meta_kubernetes_ingress_host\":\"test.example.com\",\"__meta_kubernetes_ingress_path\":\"\/\",\"__meta_kubernetes_ingress_scheme\":\"http\"}],\"Labels\":{\"__meta_kubernetes_ingress_annotation_test_annotation\":\"testannotationvalue\",\"__meta_kubernetes_ingress_annotationpresent_test_annotation\":\"true\",\"__meta_kubernetes_ingress_label_test_label\":\"testvalue\",\"__meta_kubernetes_ingress_labelpresent_test_label\":\"true\",\"__meta_kubernetes_ingress_name\":\"testingress\",\"__meta_kubernetes_namespace\":\"default\"},\"Source\":\"ingress\/default\/testingress\"}}\r\n        \t            \t+{}\r\n        \tTest:       \tTestIngressDiscoveryAddMixed\r\nFAIL\r\nFAIL\tgithub.com\/prometheus\/prometheus\/discovery\/kubernetes\t19.929s\r\n```\r\n\r\n**Environment**\r\n\r\nubuntu 20.04, go1.16.3\r\n\r\n* System information:\r\n```\r\n$ uname -srm\r\nLinux 5.4.0-65-generic x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```\r\nv2.26.0\r\n```\r\n\r\n","comments":["Thanks.\r\n\r\nIs that flapping or consistent?","It's flapping, I can't reproduce it anymore. Seems to be a flapper only when the machine under large pressure.","The only way I see to fix this is by changing the timeout. It's just a case of a test that has to be written in a way it might possibly fail under latency, yet only under extremely high load. I'm not sure it's even worth addressing if it will only happen in a small percentage of scenarios.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/60918b8415d928363ea4bc766d450e707035abe0\/discovery\/kubernetes\/kubernetes_test.go#L91\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/60918b8415d928363ea4bc766d450e707035abe0\/discovery\/kubernetes\/kubernetes_test.go#L131-L136"],"labels":["kind\/bug","component\/service discovery","priority\/P3"]},{"title":"Data corruption on OpenBSD","body":"Hello guys\r\n\r\nI wanted to run my prometheus on OpenBSD but encountered an issue. So I let the tests run and many of them were failing.\r\n\r\nFor simplicity I will focus on the `TestCreateBlock` test.\r\n\r\nThe error I encountered is:\r\n```\r\n$ cd tsdb\/\r\n$ GO111MODULE=on go test -run TestCreateBlock\r\n--- FAIL: TestCreateBlock (0.07s)\r\n    block_test.go:424: \r\n        \tError Trace:\tblock_test.go:424\r\n        \t            \t\t\t\tblock_test.go:86\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \t2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\r\n        \t            \tcompactor write\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.(*BlockWriter).Flush\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/blockwriter.go:109\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.CreateBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/tsdbblockutil.go:70\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.createBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/block_test.go:423\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.TestCreateBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/block_test.go:86\r\n        \t            \ttesting.tRunner\r\n        \t            \t\t\/usr\/local\/go\/src\/testing\/testing.go:1194\r\n        \t            \truntime.goexit\r\n        \t            \t\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1371\r\n        \tTest:       \tTestCreateBlock\r\n```\r\n\r\nI tracked this issue down to the function [finishSymbols()](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L525).\r\n\r\nOn line [L537](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L537) the value `\"hash\"` is written to the index file.\r\nOn line [L544](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L544) the file is opened via mmap.\r\nOn line [L552](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L552) the actual CRC32 checksum is written to the file, overwriting the previously written `\"hash\"`.\r\nOn line [L557](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L557) the mmap contents are fed to `NewSymbols()`.\r\n\r\nNow since OpenBSD lacks a unified buffer cache for mmap the checksum gets written to the underlying file descriptor but is not synced into the mmaped region. The mmap still contains the value `\"hash\"`.\r\n\r\nAs a quick fix I added this patch, reopening the mmap after the CRC32 checksum write:\r\n\r\n```\r\ndiff --git a\/tsdb\/index\/index.go b\/tsdb\/index\/index.go\r\nindex a6ade9455..f6698a0cc 100644\r\n--- a\/tsdb\/index\/index.go\r\n+++ b\/tsdb\/index\/index.go\r\n@@ -553,6 +553,16 @@ func (w *Writer) finishSymbols() error {\r\n                return err\r\n        }\r\n \r\n+       \/\/ close and reopen mmaped file\r\n+       if err := sf.Close(); err != nil {\r\n+               return err\r\n+       }\r\n+       sf, err = fileutil.OpenMmapFile(w.f.name)\r\n+       if err != nil {\r\n+               return err\r\n+       }\r\n+       w.symbolFile = sf\r\n+\r\n        \/\/ Load in the symbol table efficiently for the rest of the index writing.\r\n        w.symbols, err = NewSymbols(realByteSlice(w.symbolFile.Bytes()), FormatV2, int(w.toc.Symbols))\r\n        if err != nil {\r\n```\r\n\r\nNow letting the tests run again I encountered another issue:\r\n\r\n```\r\nGO111MODULE=on go test -run TestCreateBlock \r\n--- FAIL: TestCreateBlock (0.07s)\r\n    block_test.go:424: \r\n        \tError Trace:\tblock_test.go:424\r\n        \t            \t\t\t\tblock_test.go:86\r\n        \tError:      \tReceived unexpected error:\r\n        \t            \tseries not 16-byte aligned at 43\r\n        \t            \tcompactor write\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.(*BlockWriter).Flush\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/blockwriter.go:109\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.CreateBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/tsdbblockutil.go:70\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.createBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/block_test.go:423\r\n        \t            \tgithub.com\/prometheus\/prometheus\/tsdb.TestCreateBlock\r\n        \t            \t\t\/tmp\/prometheus\/tsdb\/block_test.go:86\r\n        \t            \ttesting.tRunner\r\n        \t            \t\t\/usr\/local\/go\/src\/testing\/testing.go:1194\r\n        \t            \truntime.goexit\r\n        \t            \t\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1371\r\n        \tTest:       \tTestCreateBlock\r\n```\r\n\r\nI tracked this error down to the function [writePostingsToTmpFiles()](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L799).\r\n\r\nOn line [L806](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L806) the file is flushed to the disk.\r\nOn line [L809](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L809) a new mmap is opened for the index file.\r\n\r\nThe issue now is because of an already open mmap to the index file in another place of the program.\r\nThe mmap of line [L809](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L809) reuses the already open mmap and does not create a new one.\r\nThis old mmap does not contain the changes flushed to the file from line [L806](https:\/\/github.com\/prometheus\/prometheus\/blob\/0a28f1ae9d33b71ed2240d8e000ca5d73f6874fd\/tsdb\/index\/index.go#L806).\r\n\r\nFor better understanding I created a simple PoC which needs to be run on OpenBSD.\r\nI also tried playing with [msync(2)](https:\/\/man.openbsd.org\/msync) which does not seem to have any effect on syncing FD writes back to the mmaped regoin.\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"github.com\/edsrzf\/mmap-go\"\r\n\t\"io\/ioutil\"\r\n\t\"os\"\r\n\t\"syscall\"\r\n\t\"unsafe\"\r\n)\r\n\r\nfunc getMMapedFile(filename string, filesize int) ([]byte, mmap.MMap, error) {\r\n\tfile, err := os.Open(filename)\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\r\n\tfileAsBytes, err := mmap.Map(file, mmap.RDONLY, 0)\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\r\n\treturn fileAsBytes, fileAsBytes, err\r\n}\r\n\r\nconst (\r\n\tmsAsync = 1 << iota\r\n\tmsSync\r\n\tmsInvalidate\r\n)\r\n\r\nfunc msync(buf []byte) error {\r\n\t_, _, errno := syscall.Syscall(syscall.SYS_MSYNC, uintptr(unsafe.Pointer(&buf[0])), uintptr(len(buf)), msInvalidate)\r\n\tif errno != 0 {\r\n\t\treturn errno\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc main() {\r\n\tfile, err := ioutil.TempFile(\"\", \"mmapedFile\")\r\n\tif err != nil {\r\n\t\tfmt.Println(err)\r\n\t}\r\n\r\n\tfilename := file.Name()\r\n\tdefer os.Remove(filename)\r\n\t\/\/ first write\r\n\tfile.Write([]byte(\"1234567890\"))\r\n\r\n\tbuf, mm, err := getMMapedFile(filename, 10)\r\n\tif err != nil {\r\n\t\tfmt.Println(err)\r\n\t}\r\n\t\/\/ mmap has file contents\r\n\tfmt.Println(string(buf))\r\n\r\n\t\/\/ second write\r\n\tfile.WriteAt([]byte(\"AAAA\"), 5)\r\n\tmsync(buf) \/\/ no effect\r\n\t\/\/ mmap has old content\r\n\tfmt.Println(string(buf))\r\n\r\n\t\/\/ closing and reopening mmap\r\n\tmm.Unmap()\r\n\tbuf, mm, err = getMMapedFile(filename, 10)\r\n\t\/\/ mmap has new content\r\n\tfmt.Println(string(buf))\r\n\r\n\t\/\/ third write\r\n\tfile.WriteAt([]byte(\"CCCC\"), 0)\r\n\r\n\t\/\/ mm.Unmap() \/\/ keeping mmap open\r\n\r\n\t\/\/ opening another mmap\r\n\tbuf2, _, _ := getMMapedFile(filename, 10)\r\n\tmsync(buf2) \/\/ no effect\r\n\t\/\/ mmap has old contents as long as the same file is still opened in another mmap\r\n\tfmt.Println(string(buf2))\r\n}\r\n```\r\n\r\nNow I am a bit stuck on how to continue from here, since I don't know much about the tsdb internals and how\/where the mmaps are kept open all over the place.\r\nI hope the explanation is well understandable.\r\n\r\nGreetings, ston1th","comments":["I'm having an issue with the latest 6.9 version from ports, which may be related. I upgraded from OpenBSD 6.8 and Prometheus 2.13.1 to  OpenBSD 6.9 and Prometheus 2.24.1.\r\n\r\nI recently noticed the following error starting to appear:\r\n\r\n```\r\nlevel=error ts=2021-05-28T00:33:10.139Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\r\n```\r\nAfter a period of time, Prometheus will panic and die:\r\n\r\n```\r\npanic: corruption in head chunk file \/var\/prometheus\/chunks_head\/000001: checksum mismatch expected:00000000, actual:d096d996\r\n```\r\n\r\nI first encountered this early yesterday after my upgrade to Prometheus 2.24.1 and suspected a bit flip or something. I completely wiped the \/var\/prometheus directory and restarted yesterday morning to hopefully fix the issue. However, I noticed it just started happening again. I'm sure it's not filesystem corruption now, since I don't see any other indicators of corruption.\r\n\r\nI'm going to fallback to the older version to see if it continues. \r\n","I've confirmed on a second OpenBSD 6.9 host running Prometheus 2.24.1 that it also has the same checksum issue:\r\n\r\n```\r\nprometheus --version                                                                                                                                      \r\nprometheus, version 2.24.1 (branch: non-git, revision: non-git)\r\n  build user:       OpenBSD\r\n  build date:       20210419-21:27:27\r\n  go version:       go1.16.2\r\n  platform:         openbsd\/amd64\r\n```","Hello,\r\n\r\nLet's gather feedback from @codesome @bwplotka  here. Thank you for your detailed investigation! I am not a TSDB expert but I was able to understand it.","It seems that we should call msync() on openbsd after writing to mmaped data.","Yes thats what I thought too. But the thing is the writes do not happen to the mmaped byte slices but to file descriptors.\r\n\r\nAnd as demonstrated in the PoC, calling msync() does not sync FD writes to the already open mmaped byte slices. Once they are open, they do not change.\r\n\r\nI think the purpose of msync() is to sync writes that happen directly to the mmaped regions to the underlying FD \/ file on disk. At least that's what I think is how boltdb works.\r\n\r\nSo the code probably would need to do the reads and writes via mmap (+msync) and not writes via FD and reads via mmap.","https:\/\/groups.google.com\/u\/1\/g\/prometheus-developers\/c\/fFEnTout9Qw","Thank you so much for deep diving and trying to fix this. I use Prometheus on OpenBSD extensively :)","I also rely on Prometheus on OpenBSD and I'm running into a similar issue. I found this issue through googling the error message. I'm running `prometheus-2.24.1p1` on `OpenBSD 7.0 GENERIC.MP#5 amd64`\r\n```\r\nlevel=error ts=2022-03-29T02:18:05.850Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\nlevel=error ts=2022-03-29T02:19:05.933Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\nlevel=error ts=2022-03-29T02:20:06.047Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\nlevel=error ts=2022-03-29T02:21:06.181Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\nlevel=error ts=2022-03-29T02:22:06.256Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\nlevel=error ts=2022-03-29T02:23:06.328Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n```\r\n\r\nI've tried deleting `\/var\/prometheus\/`, restarting the service, rebooting, and even downloading the latest release from github but the same thing keeps happening. It's possible it's my disk but nothing else is breaking.","Same problem here\r\n\r\nprometheus-2.24.1p1\r\nOpenBSD 7.1 GENERIC#443 amd64\r\n\r\ncan do test\r\n\r\n@ston1th is there a tag to apply or a patch (https:\/\/github.com\/ston1th\/prometheus\/commit\/bf930161eb7e3202564f2bbfac75466d70279087) must be applied ?","> I also rely on Prometheus on OpenBSD and I'm running into a similar issue. I found this issue through googling the error message. I'm running `prometheus-2.24.1p1` on `OpenBSD 7.0 GENERIC.MP#5 amd64`\r\n> \r\n> ```\r\n> level=error ts=2022-03-29T02:18:05.850Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> level=error ts=2022-03-29T02:19:05.933Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> level=error ts=2022-03-29T02:20:06.047Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> level=error ts=2022-03-29T02:21:06.181Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> level=error ts=2022-03-29T02:22:06.256Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> level=error ts=2022-03-29T02:23:06.328Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"compact head: persist head block: 2 errors: populate block: add series: read symbols: invalid checksum; read symbols: invalid checksum\"\r\n> ```\r\n> \r\n> I've tried deleting `\/var\/prometheus\/`, restarting the service, rebooting, and even downloading the latest release from github but the same thing keeps happening. It's possible it's my disk but nothing else is breaking.\r\n\r\nit could be useful to log the disk driver you are using, dmesg | grep diskname (sd0?)\r\n\r\ni have\r\n`sd0 at scsibus1 targ 0 lun 0: <QEMU, QEMU HARDDISK, 2.5+>`\r\n\r\n\r\nAlso the constant breaking of the react part is making the packaging a pain :-(","This is unrelated to the disk driver; OpenBSD lacks a unified buffer cache for mmap and read, which nearly all other OSes have. Here's an old thread from 2015 citing that SQLite takes a 30% performance hit because of this: https:\/\/marc.info\/?l=openbsd-misc&m=143238034103484&w=2\r\n\r\nPrometheus deeply relies on mmap for performance, as the [mailing list thread](https:\/\/groups.google.com\/u\/1\/g\/prometheus-developers\/c\/fFEnTout9Qw) mentioned above says we'd happy take contributions to fix this, but we don't want to impact performance for other OSes.\r\n\r\nCurrently if you care about your data I suggest you use something other than OpenBSD for your Prometheus host.","> This is unrelated to the disk driver; OpenBSD lacks a unified buffer cache for mmap and read, which nearly all other OSes have. Here's an old thread from 2015 citing that SQLite takes a 30% performance hit because of this: https:\/\/marc.info\/?l=openbsd-misc&m=143238034103484&w=2\r\n\r\nThank you for that reference\r\n\r\n> \r\n> Prometheus deeply relies on mmap for performance, as the [mailing list thread](https:\/\/groups.google.com\/u\/1\/g\/prometheus-developers\/c\/fFEnTout9Qw) mentioned above says we'd happy take contributions to fix this, but we don't want to impact performance for other OSes.\r\n\r\nMy guess is, it is very hard to implement that in a secure manner.\r\n\r\n> \r\n> Currently if you care about your data I suggest you use something other than OpenBSD for your Prometheus host.\r\n\r\nI use Prometheus as a API\/front the data are written to another backend that compress better, I do not see the point of moving from OpenBSD because it lacks a Unified Buffer Cache (UBC) especially because i kinda run multiple instance on one host instead or running a bazillion of host\/ client\r\n\r\nMaybe the 'care' you imagine is 'harm' to other\r\n\r\nI m gonna dig some UBC knowledge : https:\/\/flylib.com\/books\/en\/3.126.1.93\/1\/\r\n","I dug up the internet a bit.\r\n\r\nOn internet there's a test about the way 'everyone' use a mix of mmap and read\/write and expect that the kernel treat it as the same.\r\n\r\n[ censored rant ]\r\n\r\nhttps:\/\/lists.samba.org\/archive\/samba-technical\/2001-May\/013552.html\r\n\r\nthank you Tridge\r\n\r\n```c\r\n\/*\r\n  trivial test program to see if file IO and mmap are coherent. \r\n  [tridge at samba.org](http:\/\/lists.samba.org\/mailman\/listinfo\/samba-technical), May 2001\r\n*\/\r\n\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\n#include <fcntl.h>\r\n#include <unistd.h>\r\n#include <string.h>\r\n#include <fcntl.h>\r\n#include <errno.h>\r\n#include <sys\/mman.h>\r\n#include <sys\/stat.h>\r\n\r\n#define SIZE 20000\r\n\r\nstatic void test(char *map, int fd, int line)\r\n{\r\n\tunsigned char buf1[SIZE], buf2[SIZE];\r\n\r\n\tmemcpy(buf1, map, SIZE);\r\n\tlseek(fd, 0, SEEK_SET);\r\n\tread(fd, buf2, SIZE);\r\n\tif (memcmp(buf1, buf2, SIZE) != 0) {\r\n\t\tint i;\r\n\t\tfor (i=0;i<SIZE;i++) {\r\n\t\t\tif (buf1[i] != buf2[i]) {\r\n\t\t\t\tprintf(\"mismatch at %d (%d %d)\\n\",\r\n\t\t\t\t       i, buf1[i], buf2[i]);\r\n\t\t\t}\r\n\t\t}\r\n\t\tprintf(\"not equal on line %d!\\n\", line);\r\n\t\texit(1);\r\n\t}\r\n}\r\n\r\n#ifndef MAP_FILE\r\n#define MAP_FILE 0\r\n#endif\r\n\r\nint main(void)\r\n{\r\n\tint fd;\r\n\tchar *map;\r\n\tchar b = 0;\r\n\r\n\tfd = open(\"test.dat\", O_RDWR|O_CREAT|O_TRUNC, 0600);\r\n\r\n\tlseek(fd, SIZE-1, SEEK_SET);\r\n\twrite(fd, &b, 1);\r\n\r\n\tmap = mmap(NULL,SIZE,PROT_READ|PROT_WRITE,MAP_SHARED|MAP_FILE, fd,\r\n0);\r\n\r\n\tmap[3000] = 17;\r\n\tfsync(fd);\r\n\ttest(map, fd, __LINE__);\r\n\r\n\tlseek(fd, 76, SEEK_SET);\r\n\twrite(fd, &fd, sizeof(fd));\r\n\t\r\n\ttest(map, fd, __LINE__);\r\n\treturn 0;\r\n}\r\n\r\n```\r\n\r\nReading https:\/\/man.openbsd.org\/msync.2 this , imho cannot work - for legit good reason -.\r\n\r\n\r\nThe above program mmap the file then use read\/write on the open fd to check in modifying map\r\nactually modified the map , against the documentation ( write to map are not guaranteed until msync )\r\n\r\nand in  https:\/\/misc.openbsd.narkive.com\/Hp3vXMls\/will-mmap-and-the-read-buffer-cache-be-unified-anyone-working-with-it OpenBSD mailing list is pointing to a solution clearly :\r\n```\r\nStuart Henderson\r\n6 years ago\r\n[Permalink](https:\/\/narkive.com\/Hp3vXMls.9)\r\n[Post by Tinker](https:\/\/misc.openbsd.narkive.com\/Hp3vXMls\/will-mmap-and-the-read-buffer-cache-be-unified-anyone-working-with-it#post8)\r\nA separate question about combining mmap access and file\r\nIf I have a readonly mmap and do fwrite to it, could I use\r\nfsync (or msync or any other call) right after the fwrite, as a tool to\r\nguarantee that the memory mapping interface is up to date?\r\nIt needs more than fsync.\r\n\r\nPlaying around with the simple test program from\r\nhttps:\/\/lists.samba.org\/archive\/samba-technical\/2001-May\/013552.html\r\n(at least on amd64) it appears that msync (with either MS_SYNC,\r\nMS_ASYNC or MS_INVALIDATE) is needed after changes are made on the\r\nmmap side of things, and msync with MS_INVALIDATE is needed after\r\nchanges done on the file io side of things.\r\n```\r\n\r\nThank you Stuart Henderson.\r\n\r\nSo yeah telling the OS I did stuff in mmap pls put it in the actual file FD\r\nand telling the OS, I wrote the FD please update mmap\\\r\nworks perfectly\r\n\r\n```c\r\nint main(void)\r\n{\r\n        int fd;\r\n        char *map;\r\n        char b = 0;\r\n\r\n        fd = open(\"test.dat\", O_RDWR|O_CREAT|O_TRUNC, 0600);\r\n\r\n        lseek(fd, SIZE-1, SEEK_SET);\r\n        write(fd, &b, 1);\r\n\r\n        map = mmap(NULL,SIZE,PROT_READ|PROT_WRITE,MAP_SHARED|MAP_FILE, fd,\r\n0);\r\n\r\n        map[3000] = 17;\r\n        \/\/ nop: fsync(fd);\r\n        msync(&map[3000], 1, MS_SYNC);\r\n\r\n        printf(\"test 1\\n\");\r\n        test(map, fd, __LINE__);\r\n\r\n        lseek(fd, 76, SEEK_SET);\r\n        write(fd, &fd, sizeof(fd));\r\n\r\n        msync(&map[3000], 1, MS_INVALIDATE);\r\n\r\n        printf(\"test 2\\n\");\r\n        test(map, fd, __LINE__);\r\n        return 0;\r\n}\r\n```\r\n\r\n$ .\/fun\r\ntest 1\r\ntest 2\r\n(amd64 arch)\r\n\r\nI am not sure why it is expected that accessing a similar resource multiple different way\r\nis expected to work nor why you would mix up API and I am not fluent in GO.\r\n\r\n\r\nI put this here so the root problem is somewhat documented. I will now look at the go thingy, unsure why would go mix up read\/write that way though.\r\n\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7523314\/173613371-81ad36f6-cb7d-4b63-9dd6-88bd1871f0a4.png)\r\n\r\nhttps:\/\/mobile.twitter.com\/annoyedobrien","> This is unrelated to the disk driver; OpenBSD lacks a unified buffer cache for mmap and read, which nearly all other OSes have. Here's an old thread from 2015 citing that SQLite takes a 30% performance hit because of this: https:\/\/marc.info\/?l=openbsd-misc&m=143238034103484&w=2\r\n> \r\n\r\nNB: SQLlite user have workaround for that apparently.\r\n\r\n","Sorry, I actually missed there's an open PR (it was hidden in some noise): https:\/\/github.com\/prometheus\/prometheus\/pull\/9085\r\n\r\n","> Sorry, I actually missed there's an open PR (it was hidden in some noise): #9085\r\n\r\nI think this PR highlight a few interesting point for Prometheus as a whole \r\n\r\nI will apply and test ","[prometheus.ports.tar.gz](https:\/\/github.com\/prometheus\/prometheus\/files\/8904179\/prometheus.ports.tar.gz)\r\n\r\nThis modified ports of OpenBSD apply the PR patch that remove mmap reference, so anyone can test ( so far the compacting occurs , more test will follow )"],"labels":["help wanted","kind\/bug","priority\/P2","component\/tsdb"]},{"title":"Add exemplar support to rules (recording, alerts)","body":"## Problem\r\n\r\nWe are adding exemplar to various parts of the Prometheus Ecosystem. We have in-mem storage, we have scrape API (OpenMetrics), we have Query API we have even dashboard supports thanks to Grafana. Recently we also [added remote write support](https:\/\/github.com\/prometheus\/prometheus\/pull\/8296).\r\n\r\nYet something we are still missing is the ability to preserve exemplars on recording rules and alerts that are evaluated by Prometheus itself. \r\n\r\n## Motivation\r\n\r\nJust imagine. Exemplars are instrumented for critical request-scoped metrics, we define alert for e.g high number of errors or high latency. When triggered you can get the example Trace ID for example affected request to debug *immediately*! Similarly, you can aggregate metrics that have exemplars attached already and use in recording rule, it would be ideal to have also some exemplar attached to resulted recording rule too!\r\n\r\n## Acceptance Criteria \/ Goals\r\n\r\n* Recording rules append relevant exemplars if the underlying metrics have them.\r\n* Alerts are exposing exemplar when triggered via some standard mechanism (e.g function in the annotation).\r\n* Alert metrics (ALERT) are appending relevant exemplar.\r\n\r\n## Proposal\r\n\r\nWe discussed this briefly on [Prometheus Storage Working Group](https:\/\/docs.google.com\/document\/d\/1HWL-NIfog3_pFxUny0kAHeoxd0grnqhCBcHVPZN4y3Y\/edit#). Some raw notes:\r\n\r\n```\r\nBartek: Exemplars in Recording Rules\r\nBartek: Use Case: Aggregation recording rules being written to remote storage.\r\nBjorn: Query Layer for Exemplars?\r\nJulien: Current API is not great. I went from dashboard to exploring grafana - examples was 1 thousands lower. Recording rule might be confusion. Maybe more selective exemplars.\r\nRichi: I like the idea. Let\u2019s make it as stupid and quick as we can (experimental) for now. Tail based sampling will be more complicated. Logic on selecting. It might be more useful to have something simple but it will give us more opinions. Even if we randomly select one. We can gain experience, and our users, too.\r\nJulien: Storing label from exemplar? \r\nPriority - enable to ecosystem. \r\nJulien: Federation etc\r\nRichi: Alerting? Random exemplar? Add it as an annotation if experimental feature enabled. Query exemplar function - Jaeger trace.\r\nBartek: Sure, same as recording rule. Instead of dashboard configured with exemplar query, then alert annotation too\r\nJulien: Another function in template. \r\n```\r\n\r\nOne solution is to start with a simple approach and add the following changes:\r\n\r\n1. Enabled by feature flag: Recording Rules are adding a random exemplar from exemplars returned by passing the recorded eval query to Exemplars API. Also if enabled alerts will do the same and append exemplar to it's `Alert` metric sample.\r\n2. Add function to Alert templated which returns exemplars and allow specifying the method of choosing which one? e.g random?\r\n\r\nAlternative to 1: Allowing the choice of this, as a new field to recording rules configuration (if exemplar is added or not). I think if users want this, they want all of them at once? \ud83e\udd14 \r\n\r\nThoughts? (: Help wanted if we have consensus on this.","comments":["It would go against our philosophy to add them with the current state of the API. Let me explain.\r\n\r\nWe promote the fact that recording rules are generalized. e.g.:\r\n\r\n```\r\nalert: PrometheusErrorSendingAlertsToSomeAlertmanagers\r\nexpr: (rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m]) \/ rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m])) * 100 > 1\r\n```\r\n\r\nCurrently we can add as many prometheis instances there and it would render correct alerts.\r\n\r\nImagine now prometheus_notifications_errors_total has an examplar.\r\n\r\nHow do we send the \"correct\" examplar, the one from the failing prometheus server? To avoid this, users would be tempted to write an alert rule per instance, and that would be a real anti-pattern.\r\n\r\nThe current API could send an examplar for just *any* prometheus server, totally irrelevant to the alert.\r\n\r\nAlso, do we send the same examplar for each metric in the output? Do we send different ones?","Thanks for pointing this out. In this case, a certain prometheus will trigger it, so the result of such query will have certain series relevant to the Prometheus. \r\n\r\nSo, isn't this as easy as passing such result series to Exemplar API?","Ok, as we spoke on community hours, it makes sense to get it from the result, but not always as there are sometimes not any metrics names in the result.\r\n\r\nAn idea we settle in so far is to use exemplars query in the annotation. However, does it mean we should add annotations to recording rules? \r\n\r\nPros:\r\n* It's explicit, user decide what exemplar we should use\r\n\r\nCons:\r\n* It's quite verbose to use and pretty manual to work with\r\n* The labels and other variables are hard to use really","I have tried to give a first stab at this on https:\/\/github.com\/prometheus\/prometheus\/pull\/12056.\r\n\r\nThe approach basically adds support specifically for recording rules by matching raw exemplars for the same query being evaluated against exemplar storage against the newly recorded series. If the labels match then the series labels from the recorded metric are used to store the exemplar back into exemplar storage. \r\n\r\n"],"labels":["help wanted","component\/rules","kind\/feature"]},{"title":"Render Exemplars in Prometheus UI","body":"Just sharing idea and feedback we have. We talk about better multi-signal correlation techniques, so we should probably start with our own UI too! (: \r\n\r\n## Acceptance Criteria:\r\n\r\n* Graph page can render exemplars on top of time series line, like Grafana does.\r\n* Add Instant exemplars query view?","comments":["This looks great! I would however make them optional via a checkbox.","I'm working on this. The entire PromQL query can be passed to `\/query_exemplars` to get the exemplars, correct?","Yes","> Add Instant exemplars query view?\r\n\r\n@bwplotka I'd be interested to know more about your idea for this. #8832 will add support to the graph, but won't touch anywhere else.","So looks like the first AC `Graph page can render exemplars on top of time series line, like Grafana does.` was done and I don't remember what I meant by instant query view exemplars, so let's close it \ud83d\ude43 \r\n\r\n\r\n\r\n","\"instant query view exemplars\" => examplars in the console tab ?","Yeah that's what I was thinking, but how would that work? I don't believe we even have an instant exemplar query API. Also there's the question of how to display them."],"labels":["help wanted","component\/ui","kind\/feature"]},{"title":"Compress very large records in the WAL","body":"Right now, if a record is larger than the maximum size snappy can compress (~3.7 GB) we do not compress it at all, see https:\/\/github.com\/prometheus\/prometheus\/pull\/8790. A user would probably like these large datasets compressed as chances are a scrape of that size has a fair amount of repeated label names\/values that could be compressed away.","comments":["Can the record be arbitrarily split into smaller chunks (which are then compressed by themselves) or are there only certain places in the data where it can happen?","I don't think the record could be arbitrarily split into smaller chunks, as then a single entry inside the record might be split across two records and we would not be able to decode it later.\r\n\r\nI think the easiest thing to do might be to use the framed version of snappy via a snappy writer, but that would need a new flag in the record header, and there might be better approaches before we commit to that.","What I meant was splitting a record into multiple chunks, compressing those, and then putting them back together as one record, although there are probably issues with that.","Yep, that is kind of what the framed version of snappy does. Upon decoding time we would have to know where the multiple chunks start\/end to snappy decompress them appropriately. I think that information would have to be stored at write time which would involve a new record type, but if there is a clever way to do it that would be awesome.","When we work on this, we need to take into account our will to have transactionnal remote write and not do something that would go against that goal.","I've managed to combine chunks of encoded data into one, without any additional records needed, and get it to decode. But the decoder seems to cap out at 4.39 GB, which is likely the highest we can push it with one input chunk. Better, but not by much.\r\n\r\nAll snappy encoded data chunks include the length at the start and then the encoded data begins. By stripping that length, combining it with other length-stripped chunks, and then adding the length of all the chunks combined to the start, you can create one \"super chunk\". Snappy will happily decode this like any regular piece of encoded data, making it compatible with existing WAL records.","Actually, I believe I found the reason for the cap. 4.39 GB -> bytes is curiously close to the limit of `uint32`, Golang's default `uint` type. Encoding `4294967295` bytes (the exact maximum) split into three chunks and then decoding it yields a success. But, `4294967298` bytes is a failure.\r\n\r\nThis seems to check out when taking a peek into the decoding function in snappy (although the amd and arm versions are in assembly, I assume it's the same).\r\n\r\nhttps:\/\/github.com\/golang\/snappy\/blob\/33fc3d5d8d990c6bccaf4da1d7bb661e0cc53cb6\/decode_other.go#L25-L27\r\n","I am not sure if this is worth a try. 7 million series scrape is extreme, and I wonder if we should focus on teaching others and ensuring:\r\n\r\n* Scrape targets never are so large (especially by [default](https:\/\/github.com\/kubernetes\/kube-state-metrics\/issues\/1475)), or users select what they need.\r\n* It's easy to shard scrape by target within single process as well as multiple Prometheus-es or Agent modes. "],"labels":["kind\/enhancement","priority\/P3","component\/tsdb"]},{"title":"support traversing an AST concurrently","body":"<!--\r\n\r\n    Please do *NOT* ask support questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use our\r\n    community support.\r\n\r\n    https:\/\/prometheus.io\/community\/\r\n\r\n    There is also commercial support available.\r\n\r\n    https:\/\/prometheus.io\/support-training\/\r\n\r\n-->\r\n## Proposal\r\npql: A +B\r\nCurrent implementation in [promql\/parser\/ast.go](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/promql\/parser\/ast.go#L302) implement is not concurrent.\r\nWhen A & B costs a lot of time, traversing an AST concurrently can be greatly improved.\r\nA concurrent version has been implemented in our company, and works great. If interested, I can contribute to the community.\r\n\r\n","comments":["Currently the promql engine is designed to run in one thread. This is a feature, we have limit on concurrent queries etc to protect the server. In general this works fine.\r\n\r\nAre you using the parser outside of the promql code?","We fork the prometheus, extend and modifiy a few modules of prometheus, and push down some operations to the underlying storage.\r\nThe concurrent version has achieved a great performance improvement in our company.\r\nCan we reduce the granularity of concurrency to a [Selector](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/promql\/parser\/ast.go#L185)?\r\n[query.max-concurrency](https:\/\/github.com\/prometheus\/prometheus\/blob\/v2.25.2\/cmd\/prometheus\/main.go#L290) may need to be modified to ensure that the concurrency limit is effective.\r\n\r\n```\r\n\ta.Flag(\"query.max-concurrency\", \"Maximum number of queries executed concurrently.\").\r\n\t\tDefault(\"20\").IntVar(&cfg.queryConcurrency)\r\n```","Not really since it would mean that a complex query would 'block' other prometheus queries. Currently the complex queries are a bit \"penalized\" by the current system. If you have really complex queries you run often and they take a lot of time it is preferable to use recording rules."],"labels":["priority\/Pmaybe","component\/promql"]},{"title":"group together samples and exemplars in a single TimeSeries proto","body":"Remote Write is currently doing the easiest but least efficient thing, which is allocating a new `prompb.TimeSeries` per sample or exemplar that is buffered, even if some samples\/exemplars share the same series labels, in which case they could be put in the same `prompb.TimeSeries` for the write request. \r\n\r\nNothing in the protocol or proto definitions currently stop us from making this improvement.","comments":["Also similar we could add for buffering samples together across series. As far as I can see: https:\/\/github.com\/prometheus\/prometheus\/blob\/8fd73b1d281d3227c337b291e8c797a85251d3d5\/storage\/remote\/queue_manager.go#L525 we only add one sample to each write request.","> we only add one sample to each write request.\r\n\r\nFrom what I understand, we are only adding the sample to a local queue. The requests only triggers when we either reach the configured max number of samples in a request or we exceed the batch send deadline.\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/blob\/8fd73b1d281d3227c337b291e8c797a85251d3d5\/storage\/remote\/queue_manager.go#L1055-L1127","@bwplotka `Also similar we could add for buffering samples together across series.` There's limited scenarios in which this would ever actually come up. \r\n\r\nThe first is if the user's Prometheus has less active series than `max_samples_per_send*max_shards`, which seems unlikely. \r\n\r\nThe second is the replay\/catch up scenario you brought up somewhere else. This one would be hard to get right. We'd have to balance buffering additional samples so we could slot them into the same `TimeSeries` with the right labels, with no using too much memory and OOM'ing the users Prometheus.","Yup, just for noting","I think the reason we do not group exemplar and samples in a single Timeseries is that WAL for exemplar is different than the WAL for samples. If we store exemplar in the same sample's WAL, this will be easier to handle (otherwise we would have to group and look back the samples queued and check of ref which will be expensive), given that both the WAL (exemplar and sample) have `ref` field.\r\n\r\nIs there any reason why we didn't do that?\r\n\r\nMy guess is to keep WAL size reduced, but I don't think the increase will be significant. ","@Harkishen-Singh IIRC one of the main reasons was that modification of an existing WAL record type, such as Samples records, would have broken forwards\/backwards compatibility between versions without some kind of converter tool.\r\n\r\nAssume any improvements made as part of resolving this issue can't make changes to the WAL format."],"labels":["help wanted","component\/remote storage"]},{"title":"remote storage metrics audit","body":"Remote storage, particularly remote write, has a lot of metrics as it has grown organically. Some metrics have names or help text that aren't necessarily still accurate, and we have many metrics that are repeated per data type (sample, exemplar, metadata). This came up most recently here: https:\/\/github.com\/prometheus\/prometheus\/pull\/8296#discussion_r612872067\r\n\r\nWe should do an audit of the metrics in remote storage and decide which, if any, should change before declaring remote storage stable.","comments":["@cstyan what do you think about this?\r\n\r\n```\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| old metric                                        | new metric                                                                      |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_enqueue_retries_total   | prometheus_remote_storage_enqueue_retries_total -- add label for entity?        |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_retried_total |                                                                                 |\r\n| prometheus_remote_storage_metadata_retried_total  | prometheus_remote_storage_retries_total{\"entity\": \"metadata\/exemplars\/samples\"} |\r\n| prometheus_remote_storage_samples_retried_total   |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_dropped_total | prometheus_remote_storage_dropped_total{\"entity\": \"exemplars\/samples\"}          |\r\n| prometheus_remote_storage_samples_dropped_total   |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_failed_total  |                                                                                 |\r\n| prometheus_remote_storage_metadata_failed_total   | prometheus_remote_storage_failed_total{\"entity\": \"exemplars\/metadata\/samples\"}  |\r\n| prometheus_remote_storage_samples_failed_total    |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_in_total      | prometheus_remote_storage_in_total{\"entity\": \"exemplars\/samples\"}               |\r\n| prometheus_remote_storage_samples_in_total        |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_pending       | prometheus_remote_storage_pending{\"entity\": \"exemplars\/samples\"}                |\r\n| prometheus_remote_storage_samples_pending         |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_exemplars_total         |                                                                                 |\r\n| prometheus_remote_storage_metadata_total          | prometheus_remote_storage_total{\"entity\": \"exemplars\/metadata\/samples\"}         |\r\n| prometheus_remote_storage_samples_total           |                                                                                 |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n| prometheus_remote_storage_bytes_total             | to be split into prometheus_remote_bytes_total{\"entity\": \"{samples\/exemplars}\"} |\r\n| prometheus_remote_storage_metadata_bytes_total    | prometheus_remote_bytes_total{\"entity\": \"metadata\"}                             |\r\n|---------------------------------------------------+---------------------------------------------------------------------------------|\r\n```\r\n\r\nUnchanged:\r\n```\r\n- prometheus_remote_storage_highest_timestamp_in_seconds\r\n- prometheus_remote_storage_max_samples_per_send\r\n- prometheus_remote_storage_queue_highest_sent_timestamp_seconds\r\n- prometheus_remote_storage_sent_batch_duration_seconds\r\n- prometheus_remote_storage_shard_capacity\r\n- prometheus_remote_storage_shards\r\n- prometheus_remote_storage_shards_desired\r\n- prometheus_remote_storage_shards_max\r\n- prometheus_remote_storage_shards_min\r\n- prometheus_remote_storage_string_interner_zero_reference_releases_total\r\n```\r\n","A few notes:\r\n- `entity` should be something like `type` to keep in line with other metric labels for things like WAL record types\r\n- `prometheus_remote_storage_bytes_total ` can't be split into two metrics, exemplar and samples data are grouped together in the same write requests before being snappy encoded, after which we calculate the bytes for that metric\r\n- `prometheus_remote_storage_enqueue_retries_total ` I don't know if we need an extra label here, the metric just indicates that we had to retry because resharding was happening, knowing what kind of data we were trying to enqueue doesn't add much value\r\n- @gotjosh can you confirm we still don't need metadata_in\/pending metadata as metrics since we're not getting metadata from the WAL and we're not using sharding for metadata?"],"labels":["component\/remote storage","priority\/P3"]},{"title":"count_values string formatting","body":"## Proposal\r\n\r\nIt would be useful\/consistent if count_values used %g formatting, consistent with the le bucket label in histograms. It is useful to be able to assume the two are consistent. This is of practical use when calculating a true apdex as it permits the application to expose a numeric apdex target value, which we can then translate into a bucket name for matching purposes.\r\n\r\nThe numeric value is useful for establishing which apdex target value to use in the case where a deployment has change the target apdex value.\r\n\r\nThe technique is documented here:\r\n\r\nhttps:\/\/medium.com\/@tristan_96324\/prometheus-apdex-alerting-d17a065e39d0\r\n\r\nbut is unreliable, and additional regex stages are likely needed to append a .0 to integer values in the case where %g has been used for the bucket labels, but is not used by count_values.","comments":["hello,\r\n\r\nI think this is a breaking change in Prometheus, so not doable as-is in Prometheus 2.x. We would need to think about alternatives, like adding an extra \"formatting\" parameter for the count_values.\r\n\r\nI would also need to check what the openmetrics spec for formatting is and what the actual implementation is to see the better solution.\r\n\r\nWhy do you speak about %g but your proposed code does not use %g ?","Your application could maybe also expose http_apdex_target_seconds with the correct label.","> hello,\r\n> \r\n> I think this is a breaking change in Prometheus, so not doable as-is in Prometheus 2.x. We would need to think about alternatives, like adding an extra \"formatting\" parameter for the count_values.\r\n> \r\n> I would also need to check what the openmetrics spec for formatting is and what the actual implementation is to see the better solution.\r\n> \r\n> Why do you speak about %g but your proposed code does not use %g ?\r\n\r\nThe \"%g\" terminology came from some previous bug reports around different bucket le labels in different clients. Before I found the actual code. It's also referenced in the OpenMetrics spec.\r\n```The target rendering is equivalent to the default Go rendering of float64 values (i.e. %g), with a .0 appended in case there is no decimal point or exponent to make clear that they are floats.```","Okay, would not %g render +inf, Nan etc out of the box as we would expect?","> Your application could maybe also expose http_apdex_target_seconds with the correct label.\r\n\r\nThe advantage with using a numeric , instead of a hard coded label, is that I can use max\/min to pick if two different versions of the same app happen to provide different values. (I could expose the metrics with both the value and the bucket, I'd actually have to recreate the openmetrics string code in my own app to know how the thing would be rendered).\r\n\r\nHopefully it makes sense that my example case here is just an example and that formalising the actually format count_values uses (even if it needs an extra arg or different function), has some value in itself.\r\n\r\n","> Okay, would not %g render +inf, Nan etc out of the box as we would expect?\r\n\r\nThe code in the PR is taken from the prom\/common repo so does handle rendering +\/-Inf and NaN, and enforces the .0 suffix.","> The advantage with using a numeric , instead of a hard coded label, is that I can use max\/min to pick if two different versions of the same app happen to provide different values. \r\n\r\nYou could use `topk(1, http_apdex_target_seconds) without(le)`","> You could use `topk(1, http_apdex_target_seconds) without(le)\r\n\r\nThe exact choice of how to pick the highest isn't super relevant (we just use min at present I think). When generating the metric I'd have to generate...\r\n```\r\nhttp_apdex_target_seconds{le=\"5.0\"} = 5.0\r\nhttp_apdex_tollerable_seconds{le=\"20.0\"} = 20.0\r\n```\r\nAnd have to be confident that I'm rendering the le label in the same way that the client library does.\r\nThat's doable, but not as convenient as being able to just do the maths on `http_apdex_target_seconds` and convert the value into a label in a stable\/reliable way that can match a known well formatted bucket.\r\n\r\nPhrased differently...\r\n\r\ncurrently the format for the resulting label from `count_values` is unspecified. It might be better if it were well specified, and matched other places where the rendering of float is labels is already well specified.\r\n","I will ping @juliusv and @beorn7 for their opinion on this.","The original sin here is that we (ab-)use label values (which are strings) as numbers (in this case bucket boundaries).\r\n\r\nThe most striking dissonance here comes from the different formatting conventions in different languages, mostly that Go doesn't append a `.0`, while Java or Python do. While Prometheus itself is just Go, instrumentation libraries are in different languages. So a Python or Java target would expose a bucket boundary of 5 as a label `{le=\"5.0\"}`, while a Go target uses `{le=\"5\"}`. That's only a problem in the text format, as the protobuf format uses actual floats. Prometheus 1.x circumvented the problem by sanitizing `le` and `quantile` labels (the latter of summaries) so that only Go formatting of those \"numbers in labels\" would hit the TSDB. Prometheus 2.x tossed both the protobuf format and the sanitizing. Which led to different formatting conventions ending up in the TSDB.\r\n\r\nOpenMetrics tried to address that problem, sadly [not by reintroducing sanitation of the format](https:\/\/github.com\/OpenObservability\/OpenMetrics\/issues\/129) (and, in continuation, keeping open the possibility of a TSDB that can handle those numbers as actual numbers, as Prometheus will do with the new [Sparse Histograms](https:\/\/docs.google.com\/document\/d\/1cLNv3aufPZb3fNfaJgdaRBZsInZKKIHo9E6HinJVbpM\/edit)), but by requiring the instrumented target to do the \"correct\" formatting. The latter wouldn't be too bad if the \"correct\" formatting were actually well defined. But in fact it is not. The OM spec works for integer numbers and certainly covers most cases in practice, but there are numbers that can be correctly represented in up to nine different ways, and the OM spec does not specify which one to pick in those cases. (The usual formatting algorithms and language specs are silent about this. They are all only concerned that the text representation will parse back into exactly the same float, but that isn't enough for our case.)\r\n\r\nThe conclusion is that OM might work in most practical cases, but it is _not_ a proper solution to the problem. (With something like exponential buckets I could see the above described problem to also occur in practice. Needs to be investigated, but my assumption is that this is not purely academical.)\r\n\r\nAnother aspect of the problem is that we have maneuvered Prometheus 2.x into a position where it is hard to get out without significant friction of breaking changes. If you switch Go targets to using OM, all your bucket and quantile timeseries change. If we re-introduced sanitizing, all the Java\/Python\/... time series would change. And in neither case would the price we pay get us to a proper solution (as formatting of numbers with many decimal places will still not be well defined).\r\n\r\nWe mitigated the situation for `histogram_quantile` by letting that function internally parse the `le` label into a float and then only use the resulting value. But if you want to pick a specific bucket, you still have to find work arounds (like using a regexp match?).\r\n\r\nWith the label value created by `count_values`, we have a similar problem. With the same implications. I.e. we could apply the OM formatting, which would break current usage but still doesn't get us all the way to a proper solution. At least, because `count_values` is just happening inside Prometheus, it always uses the Go formatting. It is as well (or not-well) defined as OM formatting. The formatting matches what the Go instrumentation library uses (if not configured to use OM), and it matches what  all `le` and `quantile` labels used in Prometheus 1.x. I would say we should not change the current behavior until we have found a solution that is significantly better (i.e. not just replacing one ill-defined string formatting with another ill-defined string formatting).","FWIW, I decided to \"bite the bullet\" and switch to OpenMetrics for our Go apps, and that is what brought me to this. The decision  was based entirely on it being the only way to get exemplar support, and I can imagine *a lot* of people will likely do the same. It is worth a few days of broken heat maps. I've not seen any discussion of introducing exemplars in the non-OM expfmt (though I get if proto comes back, that's an option).\r\n","I'll crack on with the regexp.","It feels like this is easily done (openmetrics -> text format):\r\n\r\n```\r\nmetric_relabel_configs:\r\n- source_labels: [le]  \r\n  target_label: le        \r\n  regex: '([0-9]+)\\.0' \r\n```\r\n\r\nThe other way around (text format -> openmetrics):\r\n\r\n```\r\nmetric_relabel_configs:\r\n- source_labels: [le]  \r\n  target_label: le        \r\n  regex: '([0-9]+)'\r\n  replacement: '${1}.0' \r\n```","Cool. Just that it is the other way around, isn't it? The first relabels OM into the old (client_golang) behavior, while the second relabels old (client_golang) behavior into OM. (Note that the old text format didn't specify any particular float format. Prometheus 1.x was sanitizing the floats-as-string anyway.)","Thanks, I have updated my comment.","Thanks for the tips. I didn't raise this to fix an unresolvable problem though, if count_values is going to convert numbers to strings, it seems like it should do so in a \"known way\". I appreciate @beorn7 points, though I would say that there is, so far as I can see, a 1 <-> 1 mapping of the OpenMetrics \"%g with trailing .0\", and the behaviour should be reproducable by clients, so it seemed like a sensible option. If changing the behaviour is undesirable, it might be worth at least documenting the existing behaviour (as golang %f at least), and possibly mentioning that it is not string matchable against OpenMetrics histogram le labels without regexp.\r\n\r\nOn a side note, purely out of curiosity, would that le relabeling impact exemplar lookup?","Saying that OM uses %g is incorrect. We are using %g in the text format too: https:\/\/github.com\/prometheus\/common\/blob\/main\/expfmt\/text_create.go#L449\r\nSomehow %g does not add the .0 and we artificially add it in openmetrics.","> On a side note, purely out of curiosity, would that le relabeling impact exemplar lookup?\r\n\r\nIt should yes.","> Saying that OM uses %g is incorrect. We are using %g in the text format too: https:\/\/github.com\/prometheus\/common\/blob\/main\/expfmt\/text_create.go#L449\r\n> Somehow %g does not add the .0 and we artificially add it in openmetrics.\r\n\r\nI appreciate it does not use %g directly (the PR I've closed included the copied code from client_golang so that the behaviour would match, as mentioned above), the terminology was stuck in my head from reading previous PRs that referred to it as such. It's not terribly relevant to anything if this isn't going ahead.\r\n\r\n","What `%g` precisely does depends on the language\/implementation. In Go, it does not add `.0` to floats with an integer value.\r\n\r\n","It would be worth OM documenting whatever it expects languages to do, probably without reference to \"Go %g\".\r\nIt would be worth Prom documenting what count\\_values does, without direct reference to %f (or at least linking to docs).\r\nIt might be worth mentioning that they aren't compatible.\r\nPeople probably aren't tripping over this all that often, I was doing tricksy promql things, and someone that read an article about them happens to suffer through applications with apdex targets over 0.25s, which triggered this problem (I am more fortunate).\r\n@beorn7 I've been following the histogram work closely (have even attempted to use the protobuf implemented stuff on your client\\_golang for a side project I'm playing with), it's extremely exciting, though does raise interesting question about integration with exemplars (which have some of my dev focused colleagues doing backflips of joy).\r\n","> It would be worth OM documenting whatever it expects languages to do, probably without reference to \"Go %g\".\r\n> It would be worth Prom documenting what count_values does, without direct reference to %f (or at least linking to docs).\r\n> It might be worth mentioning that they aren't compatible.\r\n\r\nOM's formatting behavior is documented in their [draft spec](https:\/\/docs.google.com\/document\/d\/1sFuesBxDl4XNWQfvg0IHaL9Lp0NCYU8CDL-Lu7wVsHs\/edit#heading=h.slbs6ftdzbfp), and it's even discussing a number of more subtle issues with number formatting.\r\n\r\nDefinitely agree that the `count_values` documentation should mention how it formats the numerical value when it ends up as a string in the label. However, I think we really have to just say \"what Go does with `%g` directive\" because it would be quite verbose to explain it completely, and my suspicion is that there are no guarantees of the details to be changing (between versions of Go or even between platforms). As I understand it, the only guarantee is that the string-formatted float will parse back into exactly the same binary representation of a float (although this cannot be true for `NaN`, but that's yet another story).\r\n\r\n> it's extremely exciting, though does raise interesting question about integration with exemplars (which have some of my dev focused colleagues doing backflips of joy).\r\n\r\nDefinitely we'll have exemplars in the new histogram world, too. It just won't be \"one per bucket\" anymore, so probably a separate section in the protocol. But those are details to flesh out later.","> Definitely agree that the `count_values` documentation should mention how it formats the numerical value when it ends up as a string in the label. However, I think we really have to just say \"what Go does with `%g` directive\" because it would be quite verbose to explain it completely, and my suspicion is that there are no guarantees of the details to be changing (between versions of Go or even between platforms). As I understand it, the only guarantee is that the string-formatted float will parse back into exactly the same binary representation of a float (although this cannot be true for `NaN`, but that's yet another story).\r\n\r\nFrom what I remember of the code, `count_values` uses %f. (it uses strconv.FormatFloat  with f IIRC)","I've just checked it out: It uses `strconv.FormatFloat(s.V, 'f', -1, 64))`, which boils down to `%f` with as many significant digits to create a unique value. And that's rather unfortunate. It means that it won't use exponents for very large or very small values, which is different from both OpenMetrics as well as the usual (but not specified) behavior of instrumentation libraries when exposing the classic Prometheus text format. It's using a third format! :angry: \r\n\r\nNow I'm thinking we might want to add an optional parameter to `count_values` so that the user can pick a format string (in Go `fmt` style).","That sounds like a good idea, though I'm not sure if there's literally a sprintf form that would be OM compatible. A reimplementation of a %.. formatter seems ideal but feels like a large job.\r\n(I'm not OM obsessed, honest, but it would be a shame if any solution couldn't produce something that matches the one well specified bucket label format)\r\n","We could use the Go formatter, but add another verb that would format in OpenMetrics style.","Well today I learned of fmt.Formatter, though there is no obvious way to fall back, so this kind of thing doesn't look super performant. Fun though:\r\nhttps:\/\/play.golang.org\/p\/Loj_KVLky1U","It's not obvious that the full power of a Printf strings makes that much sense (e.g., there's not much point in multiple values. so maybe it'd be better to just allow something like \r\n`count_values(\"le,f.3\",....)` to allow precision to be set? The we can use strconv.FormatFloat, with extra stuff done for some other verb? So drop support for padding and the extra flags (not sure they do anything for floats anyway), just support precision, and the regular float formatting verbs.","Sounds good to me. We could use `o` for OpenMetrics.","`o` is already used for octal though, some poor soul might need that.\r\n","But if we use https:\/\/golang.org\/pkg\/strconv\/#FormatFloat , we can stick to the letters allowed there, and there is no `o`, so we could use it. ","@beorn7 I can have a go at implementing this, but would it be worth discussing on prometheus-dev? The use of \"thing,soemthing\" seem like the kind of thing that could prove contentious, and I'd rather not start if it's going to get shot down.","Yeah, this sounds like a good idea.","Is prometheus-developers@googlegroups.com the right list?","yes.","Just for the record, conversation continued here:\r\nhttps:\/\/groups.google.com\/g\/prometheus-developers\/c\/1OCdPPqGBuQ","It seems that the consensus and direction in the mailing list is to introduce %.\r\n\r\nIf someone wants to implement Tristan's suggestion, I would welcome it.","I didn't get much of a consensus vibe, but such that anything has support beyond myself and @beorn7 , I think a `%2g` style suffix on the label argument was the final decision. With the use of `o` discussed here (not really on list though) for OpenMetrics,\r\n This would probably make a good first issue for someone (if there's a label for such things?), but if no one steps up, I'll have a go (I'm not going to be rushing though, bit busy at the moment).\r\n","I clarified in the mailing list that I agree, indeed.","A workaround in recording rules (where you can't [use metric_relabel_configs](#issuecomment-827964668)) is to use `label_replace(..., \"le\", \"$1.0\", \"le\", \"([0-9]+)\")`."],"labels":["help wanted","kind\/enhancement","component\/promql","priority\/P3"]},{"title":"support aliyun ecs scrape","body":"we have many aliyun_ecs instance and hope they are monitored.\r\n\r\nhttps:\/\/www.alibabacloud.com\/product\/ecs?spm=a3c0i.267727.6791778070.dnavproductelastic1.376a5258DZMQe1","comments":["@roidelapluie \r\nHi~ We are alibaba aliyun team, we have a aliyun ecs discovery implement.\r\nhttps:\/\/github.com\/AliyunContainerService\/prometheus\/tree\/master\/discovery\/ecs\r\nCan we take a merge-request of our commit to master branch?\r\n\r\n- pressure test: this implement support pressure test with 1K+ ECS, and every ECS have 20 tags, support frequency ECS scale scenario. \r\n- support filter with ECS label.\r\n- use scrolling API to discovery ecs to support frequency ECS scale change scenario.\r\n \r\nrefer to #11931"],"labels":["priority\/Pmaybe","component\/service discovery","kind\/feature"]},{"title":"VMWare Discovery","body":"\r\n## Proposal\r\n**Use case. Why is this important?**\r\n\r\nWe would like to monitor all ESXI hosts and also all virtual machines in a VMWare vCenter environment; however since VMs and ESXI hosts are so dynamic (they come and go super often) it seems silly to define them statically. \r\n\r\nWe could use `dns_sd_configs` however that doesn't seem right as there would be no distinction between ESXI hosts for example and VMs.\r\n\r\nIf prometheus had VMWare discovery built in that scraped a vCenter for targets then that woiuld be awesome\r\n\r\n","comments":["There are a vmware file_sd config generator that could be used as a basis for implementing this.\r\n\r\nhttps:\/\/github.com\/AppliedTrust\/vmware-filesd","https:\/\/github.com\/vmware\/govmomi\/blob\/master\/examples\/virtualmachines\/main.go is the Go equivalent of this, and adding in https:\/\/pkg.go.dev\/github.com\/vmware\/govmomi@v0.25.0\/vapi\/tags#Manager.ListAttachedTags would allow you to get the tags."],"labels":["help wanted","component\/service discovery"]},{"title":"Metric \"up\" may miss when error in scrape","body":"Prometheus version: 2.19.2\r\nWe found that if the scrape returns an error \"upstream connect error or disconnect\/reset before headers. reset reason: connection termination\", the metric \"up\" will miss the data point (instead of a value of 0 or 1). However, scrape_duration_seconds and scrape_samples_scraped keep normal.","comments":["Thank you for your issue. The logic for this has changed in Prometheus 2.20. Can you please try to reproduce with a higher version and provide us more data to better understand what is going on?","@wangzhao765 Can you please try with a higher version and report back? Thank you!"],"labels":["kind\/more-info-needed"]},{"title":"As a third party, I can't validate the Rule using the package rulefmt","body":"Hello,\r\n\r\nThrough the PR #6533, the package `rulefmt` doesn't expose anymore a way to validate a single rule when using the struct `Rule`.\r\n\r\nBefore it was possible and useful. Because when you are wrapping the `Rule` in a third component that would like to expose this struct before deploying it into Prometheus, then it was possible to ensure that the `Rule` is valid in the Prometheus terme.\r\n\r\nI know it's been a while the API changed for this package, but well PromQL didn't change so much before v2.25 and v2.26, so I didn't have to update the Prometheus dependencies :D.\r\n\r\nI understand why the changes were performed, and I don't have so good idea so far, to provide the method `Validate` for the struct `Rule` that follows more or less the same code than the method Validate for the struct `RuleNode` (Same applies for the struct `RuleGroup` of course)\r\n\r\nAnyway, is it something possible on your side to provide back the same API ? ","comments":["cc @cyriltovena I think loki uses this code. Is @Nexucis missing something, or do you need this too?","on my side I ended up by created the yaml struct expected in a `[]byte` and then use `rulefmt.Parse`\r\n\r\nLike that for example: https:\/\/github.com\/perses\/perses\/blob\/20b812ac88d7436d415a728be746f8efe8908d31\/internal\/api\/impl\/v1\/prometheusrule\/service.go#L30-L41","so if there is a more elegant solution, that would be so cool !","@roidelapluie Loki uses its own group rule loader. We have also validation done here https:\/\/github.com\/grafana\/cortex-tools\/blob\/main\/pkg\/rules\/parser.go#L35 but I don't think we uses `rulefmt`.","````\r\ngo.mod \r\ngithub.com\/prometheus\/prometheus v1.8.2-0.20200724121523-657ba532e42f\r\n\r\nrule.go\r\nimport \"github.com\/prometheus\/prometheus\/pkg\/rulefmt\"\r\n\r\nsome for\r\n                                        var a rulefmt.RuleNode\r\n\r\n\t\t\t\t\terr = yaml.Unmarshal(ajson, &a)\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\tlog.Errorln(rule.Rule, \":\", err)\r\n\t\t\t\t\t}\r\n\r\n\t\t\t\t\terrs := a.Validate()\r\n\t\t\t\t\tif errs != nil || len(errs) > 0 {\r\n\t\t\t\t\t\tfor _, err := range errs {\r\n\t\t\t\t\t\t\tlog.Errorln(\"Error in rule\", group, rule.Rule, \":\", err)\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t}\r\n`````"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/rules","kind\/more-info-needed"]},{"title":"rate() function giving spurious peak","body":"**What did you do?**\r\nPlotting graph of a metric (say M) and the rate of that metric.\r\n\r\n**What did you expect to see?**\r\nNo deviation in normal graph of rate(M)\r\n\r\n**What did you see instead? Under which circumstances?**\r\nUnexpected spike in the graph\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4148936\/113478185-e015bb00-94a4-11eb-9b2a-43321e1e0b2e.png)\r\n\r\nData points for that duration\r\n![image](https:\/\/user-images.githubusercontent.com\/4148936\/113478250-53b7c800-94a5-11eb-8ed0-4a95fa804c5e.png)\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\nDon't have access to the system. Can get it if needed.\r\n\r\n* Prometheus version:\r\n\r\n\t2.18.1\r\n\t\r\n![image](https:\/\/user-images.githubusercontent.com\/4148936\/113478265-8235a300-94a5-11eb-827d-0002d94f8c4c.png)\r\n\r\n\r\n* Alertmanager version:\r\n\r\nNA\r\n\r\n* Prometheus configuration file:\r\nDon't have access to the system. Can get it if needed.\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["You probably have resets.\r\n\r\nCan you please provide the raw metrics?\r\n\r\nto do that, run `mymetric{label=\"a\"}[3h]` in the \"console\" tabs.","Thanks Julien for responding.\r\n\r\nPFA raw metrics. \r\nAlso, adding graphs from the exact same duration for easy correlation.\r\n![image](https:\/\/user-images.githubusercontent.com\/4148936\/113503133-7bfb0180-954d-11eb-8f31-8ea8e58dd900.png)\r\n[raw_metrics.txt](https:\/\/github.com\/prometheus\/prometheus\/files\/6254176\/raw_metrics.txt)\r\n","You have a counter reset:\r\n\r\n439 @1617442554.304\r\n439 @1617442554.315\r\n440 @1617442564.304\r\n439 @1617442564.315\r\n440 @1617442574.33\r\n440 @1617442584.297","It might be a bug in prometheus. Why is that scraped twice at the same time?\r\n\r\nCan you are more about your configuration?","cc: @sanjeevpandey19: Sanjeev, would you be able to help with above questions?","Reminds me a bit of this issue: https:\/\/github.com\/prometheus\/prometheus\/issues\/6894","I totally agree and I digged multiple times in the codebase without finding the bug yet...","I see. Can close this in favor of earlier one then.\r\nThanks Julien and Vasily","Can you still provide the requested info please?","> Can you still provide the requested info please?\r\n\r\n@roidelapluie what sort of specific information you need here.\r\n\r\nDo you want to know if we have setup using operator or vanila Prometheus or you would need Prometheus.yaml itself  ?\r\n","I'd like the prometheus.yml to see the relabeling rules and the jobs, if possible.","@roidelapluie any update on this issue we are also facing the same issue in Prometheus 2.25.0 where in the rate() function has unexpected spikes in the data","Team, Is there any update on this issue.. We're also facing similar issue. But with increase() function"],"labels":["kind\/bug","priority\/P3","kind\/more-info-needed","component\/scraping"]},{"title":"Customize through the flags the configuration of the new PromQL editor","body":"the lib `codemirror-promql` has different parameters that defines:\r\n*  the amount of time to consider when retrieving the metric. By default the interval is 12h. So currently when the new PromQL editor is autocompleting the metric, it is retrieving the metric existing since 12h.\r\n* the maximum number of metrics in Prometheus for which metadata is fetched. If the number of metrics exceeds this limit, no metric metadata is fetched at all.\r\nBy default the limite is 10k.\r\n\r\n## Proposal\r\nIt would be interesting to customize these default values using some flags like `--web.editor.lookbackInterval=12h` and `--web.editor.maxMetricsMetadata=10000`\r\n\r\nLike that we can decrease or increase these limits depending of the size of the Prometheus we are running\r\n","comments":["and I forgot also there is a LRU cache embedded into the lib where there is a maxAge which is the maximum amount of time that a cached completion item is valid before it needs to be refreshed. By default it's 5min\r\n\r\nMaybe for this one, it's not really important, but I guess it won't hurt if it's possible to change as well the default value.","aaand if Thanos is going to integrate the new PromQL editor, maybe it would be interesting to have these flags as well in Thanos.","I don't have an opinion here.\r\n\r\ncc @juliusv ","@juliusv ping, do you think it is worth the overhead?","No opinion either, really. Happy to let someone add configurability when they actually require it, I would say. Another option could be to add those options into the web UI, but then hide the increasingly large number of UI settings behind a settings menu, instead of having them all at the top.","It will be interesting behind this issue to create a new package in `web\/ui\/module` that contains the code of [CMExpressionInput](https:\/\/github.com\/prometheus\/prometheus\/blob\/e5b1b15dc3ad11c18de97e600fd5fb84ca69076b\/web\/ui\/react-app\/src\/pages\/graph\/CMExpressionInput.tsx).\r\n\r\nLike that it can be used by PromLens, Perses, Thanos ...etc. That will help to provide a common way to enter the PromQL query + the configuration of codemirror-promql","WDYT @juliusv ?","@Nexucis The input in PromLens is already totally different than in Prometheus in the way it executes things (since it doesn't actually evaluate the query but tries to parse it into a tree via the backend), so at least for that use case it wouldn't help me much. Maybe it's helpful for others though. IMO it should be motivated by a concrete use case. For the case of Thanos\/Cortex, I guess the reusability would be more interesting on a higher level (showing the entire expression browser, maybe more).","I was proposing to do that because on Thanos side they would like to add more configuration around codemirror-promql. See https:\/\/github.com\/thanos-io\/thanos\/issues\/4685\r\n\r\nSo to avoid to do the work twice, I was thinking to export the CMExpression. But perhaps yeah it would be more interesting to export it on a higher level.\r\n\r\nI was also thinking about Perses with a potential metric explorer. I think having a react component around codemirror-promql would help to commonize the way to use it across Prometheus\/Thanos\/Perses. Cortex I don't know if they are using it anyway."],"labels":["kind\/enhancement","priority\/Pmaybe","component\/ui"]},{"title":"prometheus runtime.sigpanic()  signal SIGBUS: bus error","body":"**Environment**\r\n\r\n* System information:\r\n\r\n```\r\nkubernetes version 1.11.9\r\n\r\n```\r\n* Prometheus version:\r\n\r\n```\r\nprometheus version 2.19.3\r\n\r\n```\r\n\r\n* Prometheus configuration file:\r\n```\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: prometheus1-config\r\n  namespace: manage\r\ndata:\r\n  prometheus.yml: |\r\n    global:\r\n      scrape_interval:     60s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\r\n      evaluation_interval: 60s # Evaluate rules every 15 seconds. The default is every 1 minute.\r\n      scrape_timeout: 30s\r\n      # scrape_timeout is set to the global default (10s).\r\n      external_labels:\r\n        prometheus: prometheusN1\r\n        area: nanjing\r\n        cid: c100\r\n    # Alertmanager configuration\r\n    alerting:\r\n      alertmanagers:\r\n      - static_configs:\r\n        - targets:\r\n          - alertmanager1:9093\r\n          - alertmanager2:9093\r\n          - alertmanager3:9093\r\n        timeout: 20s\r\n\r\n    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\r\n    rule_files:\r\n      - \"\/storage\/prometheus\/rules\/*.yml\"\r\n      - \"\/alert-rule\/*.yml\"\r\n\r\n    # A scrape configuration containing exactly one endpoint to scrape:\r\n    # Here it's Prometheus itself.\r\n    scrape_configs:\r\n      #The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n      - job_name: 'consul-bm'\r\n        honor_labels: true\r\n        params:\r\n          'match[]':\r\n            - '{mtype=\"sys\"}'\r\n            - '{__name__=\"up\"}'\r\n        metrics_path: '\/federate'\r\n        consul_sd_configs:\r\n        - server: 'http:\/\/consul:8500'\r\n          services:\r\n            - prometheus1BareMetalCluster\r\n        relabel_configs:\r\n        - regex: __meta_consul_service_metadata_(.+)\r\n          action: labelmap\r\n\r\n      - job_name: 'consul-k8s'\r\n        honor_labels: true\r\n        params:\r\n          'match[]':\r\n            - '{mtype=\"sys\"}'\r\n            - '{__name__=\"up\"}'\r\n        scheme: https\r\n        tls_config:\r\n          ca_file: \/conf\/pem\/ca.pem\r\n          cert_file: \/conf\/pem\/client.pem\r\n          key_file: \/conf\/pem\/client-key.pem\r\n          insecure_skip_verify: true\r\n        metrics_path: '\/federate'\r\n        consul_sd_configs:\r\n        - server: 'http:\/\/consul:8500'\r\n          services:\r\n            - prometheus1K8sCluster\r\n        relabel_configs:\r\n        - regex: __meta_consul_service_metadata_(.+)\r\n          action: labelmap\r\n\r\n      - job_name: 'paasmgr'\r\n        kubernetes_sd_configs:\r\n        - role: pod\r\n        relabel_configs:\r\n        - source_labels: [__meta_kubernetes_pod_annotation_prom_scrape]\r\n          action: keep\r\n          regex: true\r\n        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\r\n          action: replace\r\n          target_label: __metrics_path__\r\n          regex: (.+)\r\n        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\r\n          action: replace\r\n          regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n          replacement: $1:$2\r\n          target_label: __address__\r\n        - action: labelmap\r\n          regex: __meta_kubernetes_pod_label_(.+)\r\n        - source_labels: [__meta_kubernetes_namespace]\r\n          action: replace\r\n          target_label: kubernetes_namespace\r\n        - source_labels: [__meta_kubernetes_pod_name]\r\n          action: replace\r\n          target_label: kubernetes_pod_name\r\n\r\n```\r\n\r\n* Prometheus deployment file:\r\n```\r\napiVersion: apps\/v1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    use: prometheus\r\n    app: prometheus1\r\n    thanos-store-api: \"true\"\r\n  name: prometheus1\r\n  namespace: manage\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: prometheus1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        use: prometheus\r\n        app: prometheus1\r\n        thanos-store-api: \"true\"\r\n    spec:\r\n      containers:\r\n      - image: paas_public\/prometheus:v2.19.3.202012071826\r\n        imagePullPolicy: Always\r\n        name: prometheus\r\n        command:\r\n        - \"\/bin\/prometheus\"\r\n        args:\r\n        - \"--config.file=\/etc\/prometheus\/prometheus.yml\"\r\n        - \"--web.enable-lifecycle\"\r\n        - \"--storage.tsdb.path=\/storage\/prometheus1\/tsdb\"\r\n        - \"--query.lookback-delta=4m\"\r\n        - \"--storage.tsdb.retention=6h\"\r\n        - \"--conf.enable-configmap-rules\"\r\n        - \"--storage.tsdb.no-lockfile\"\r\n        - \"--web.enable-admin-api\"\r\n        - \"--storage.tsdb.min-block-duration=2h\"\r\n        - \"--storage.tsdb.max-block-duration=2h\"\r\n        env:\r\n          - name: NAMESPACE\r\n            valueFrom:\r\n              fieldRef:\r\n                fieldPath: metadata.namespace\r\n          - name: PROMETHEUS_SD_CM\r\n            value: prometheus-rules-config\r\n        ports:\r\n        - containerPort: 9090\r\n          protocol: TCP\r\n        volumeMounts:\r\n        - mountPath: \"\/storage\"\r\n          name: data\r\n        - mountPath: \"\/etc\/prometheus\"\r\n          name: config-volume\r\n        - mountPath: \"\/alert-rule\/\"\r\n          name: alert-rule\r\n        - mountPath: \"\/conf\/pem\/\"\r\n          name: pem-volume\r\n        resources:\r\n          requests:\r\n            cpu: 100m\r\n            memory: 100Mi\r\n          limits:\r\n            cpu: 15000m\r\n            memory: 50Gi\r\n      - name: thanos-sidecar\r\n        image: prometheus\/thanos:v0.18.1\r\n        args:\r\n          - \"sidecar\"\r\n          - \"--log.level=debug\"\r\n          - \"--tsdb.path=\/storage\/prometheus1\/tsdb\"\r\n          - \"--prometheus.url=http:\/\/127.0.0.1:9090\"\r\n          - \"--reloader.config-file=\/etc\/prometheus\/prometheus.yml\"\r\n          - \"--reloader.rule-dir=\/storage\/prometheus\/rules\/\"\r\n          - --objstore.config=$(OBJSTORE_CONFIG)\r\n        ports:\r\n          - name: http-sidecar\r\n            containerPort: 10902\r\n          - name: grpc\r\n            containerPort: 10901\r\n        env:\r\n          - name: OBJSTORE_CONFIG\r\n            valueFrom:\r\n              secretKeyRef:\r\n                key: thanos.yaml\r\n                name: thanos-objstore-config\r\n                optional: true\r\n        resources:\r\n          requests:\r\n            memory: \"2Gi\"\r\n            cpu: \"1\"\r\n          limits:\r\n            memory: \"2Gi\"\r\n            cpu: \"1\"\r\n        volumeMounts:\r\n          - mountPath: \"\/storage\"\r\n            name: data\r\n          - mountPath: \"\/etc\/prometheus\"\r\n            name: config-volume\r\n          - mountPath: \"\/alert-rule\/\"\r\n            name: alert-rule\r\n          - mountPath: \"\/conf\/pem\/\"\r\n            name: pem-volume\r\n      initContainers:\r\n      - name: install\r\n        args:\r\n        - mkdir -pv \/storage\/prometheus1;chmod -R 777 \/storage\/prometheus1\r\n        command:\r\n        - \/bin\/sh\r\n        - -c\r\n        image: paas_public\/busybox:nl\r\n        volumeMounts:\r\n        - name: data\r\n          mountPath: \"\/storage\"\r\n      imagePullSecrets:\r\n        - name: dockercfg\r\n      serviceAccountName: prometheus\r\n      volumes:\r\n      - name: data\r\n        persistentVolumeClaim:\r\n          claimName: glusterfs-volprome-claim\r\n      - name: config-volume\r\n        configMap:\r\n          name: prometheus1-config\r\n      - name: alert-rule\r\n        configMap:\r\n          name: prometheus-alert-cm\r\n      - name: pem-volume\r\n        configMap:\r\n          name: prometheus-https-config\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\n[signal SIGBUS: bus error code=0x2 addr=0x7f2deed75405 pc=0x71ce54]\r\n\r\nMarch 28th 2021,   10:00:43.598 | {\"log\":\"github.com\/prometheus\/prometheus\/tsdb.(*memSeries).chunk(0xc093cdf5e0,   0x2, 0xc000448300, 0xc1daa23b48, 0xc1dab2ca00, 0xc1dab2ca20,   0x25c1ec0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722621155Z\"}\r\n-- | --\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/runtime\/signal_unix.go:739   +0x485 fp=0xc1c4e64260 sp=0xc1c4e64230   pc=0x44fe25\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722563166Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"runtime.throw(0x2905b2d,   0x5)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722550129Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"runtime.sigpanic()\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72255941Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"goroutine 2913   [running]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722540902Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"github.com\/prometheus\/prometheus\/tsdb\/chunks.(*ChunkDiskMapper).Chunk(0xc000448300,   0x100053fe3ed, 0x0, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722584113Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.719324386Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"github.com\/prometheus\/prometheus\/tsdb.(*headChunkReader).Chunk(0xc1dab1f9b0,   0x151b22000002, 0x0, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722658306Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"fatal error: faultlevel=warn   ts=2021-03-28T02:00:41.719Z caller=engine.go:188 component=\\\"query   engine\\\"   query=\\\"kube_pod_labels{pod=\\\\\\\"node-fluentd-app-hvfwf\\\\\\\"}\\\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.719319917Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"[signal SIGBUS: bus error code=0x2   addr=0x7f2deed75405   pc=0x71ce54]\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72252708Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/tsdb\/head.go:1474   +0xe5 fp=0xc1c4e64530 sp=0xc1c4e644a0   pc=0x9f6625\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722667267Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/tsdb\/head.go:2041   +0x15d fp=0xc1c4e644a0 sp=0xc1c4e64440   pc=0x9fa4fd\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722636406Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722537009Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"unexpected fault address   0x7f2deed75405\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.719312841Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/tsdb\/chunks\/head_chunks.go:479   +0x6b4 fp=0xc1c4e64440 sp=0xc1c4e64260   pc=0x71ce54\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72260324Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/runtime\/panic.go:1116   +0x72 fp=0xc1c4e64230 sp=0xc1c4e64200   pc=0x4395f2\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722554018Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"level=warn ts=2021-03-28T02:00:41.719Z   caller=engine.go:188 component=\\\"query engine\\\"   query=\\\"kube_pod_labels{pod=\\\\\\\"a19xtf7dy-5ys-containeri-unit-5dc6779649-2prtw\\\\\\\"}\\\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.719254602Z\"}\r\nMarch 28th 2021, 10:00:43.598 | {\"log\":\"github.com\/prometheus\/prometheus\/tsdb.(*populatedChunkSeries).Next(0xc1daa23b90,   0x2)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722684699Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\\u003cautogenerated\\u003e:1 +0x3c   fp=0xc1c4e646d0 sp=0xc1c4e646b0   pc=0x72e31c\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722748929Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.(*Engine).execEvalStmt(0xc00068ea80,   0x2cc8740, 0xc1dab1f890, 0xc1dab095c0, 0xc1dab2e460, 0x0, 0x0, 0x0, 0x0, 0x0,   ...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722925207Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.(*evaluator).eval(0xc1c4e65170,   0x2cc8e00, 0xc1da8fd960, 0x29180cf,   0xc1dab1ce00)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722841371Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:542   +0x5a5 fp=0xc1c4e651e8 sp=0xc1c4e64f20   pc=0xa2ca85\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722955368Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:1283   +0x151 fp=0xc1c4e64ec8 sp=0xc1c4e647c0   pc=0xa30931\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722860141Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.expandSeriesSet(0x2cc8740,   0xc1dab1faa0, 0x2cb3c00, 0xc1dab30000, 0x0, 0x0, 0xc1c4e64838, 0x40f6b0,   0xc1dab1cdc0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72278467Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:763   +0x58 fp=0xc1c4e64768 sp=0xc1c4e646f0   pc=0xa2e2f8\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722794754Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:752   +0x86 fp=0xc1c4e647c0 sp=0xc1c4e64768   pc=0xa2e1e6\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722836363Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/tsdb\/querier.go:833   +0x48 fp=0xc1c4e646b0 sp=0xc1c4e64608   pc=0xa03d48\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72272342Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/storage.(*genericSeriesSetAdapter).Next(0xc1daa77ff0,   0xc1dab2c980)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72272871Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/tsdb.(*blockSeriesSet).Next(0xc1dab1cdc0,   0x8)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.7227061Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/tsdb\/querier.go:795   +0x131 fp=0xc1c4e64608 sp=0xc1c4e64530   pc=0xa038d1\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722696535Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/storage.(*seriesSetAdapter).Next(0xc1dab30000,   0xc1c4e64720)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722757752Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:821   +0x88 fp=0xc1c4e64f20 sp=0xc1c4e64ec8   pc=0xa2e948\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722895184Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.(*evaluator).Eval(0xc1c4e65170,   0x2cc8e00, 0xc1da8fd960, 0x0, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722871607Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.checkForSeriesSetExpansion(0x2cc8740,   0xc1dab1faa0, 0x2cc8e00,   0xc1da8fd960)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722829145Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.(*Engine).exec(0xc00068ea80,   0x2cc8740, 0xc1dab1f890, 0xc1dab095c0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,   ...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722964839Z\"}\r\nMarch 28th 2021, 10:00:43.599 | {\"log\":\"\\u0009\\u003cautogenerated\\u003e:1 +0x3c   fp=0xc1c4e646f0 sp=0xc1c4e646d0   pc=0x72e41c\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722768251Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/promql.(*query).Exec(0xc1dab095c0,   0x2cc8740, 0xc1c4df8bd0,   0xc1da8f6f60)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722997539Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:493   +0x53e fp=0xc1c4e65538 sp=0xc1c4e651e8   pc=0xa2c01e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.722985597Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Manager).Update.func1.1(0xc00068ef60,   0xc14b458700)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723274472Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:638   +0x106 fp=0xc1c4e65d10 sp=0xc1c4e65c90   pc=0x2024246\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723221964Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/promql\/engine.go:190   +0x1ec fp=0xc1c4e65620 sp=0xc1c4e65538   pc=0xa29b6c\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723002664Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Group).Eval.func1(0x2cc8740,   0xc177f39ce0, 0xc14b458700, 0x181511da, 0xed7f1da48, 0x0, 0x4, 0x2cf8f60,   0xc039567c20)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723169505Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:324   +0xc5 fp=0xc1c4e65d90 sp=0xc1c4e65d10   pc=0x202bfa5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723242118Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*RecordingRule).Eval(0xc039567c20,   0x2cc8740, 0xc1c4df8bd0, 0x181511da, 0xed7f1da48, 0x0, 0xc000801900,   0xc0008a9cb0, 0x10, 0x2cf9260,   ...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723143151Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:573   +0x342 fp=0xc1c4e65c90 sp=0xc1c4e65908   pc=0x202cca2\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723194391Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Group).Eval(0xc14b458700,   0x2cc8740, 0xc177f39ce0, 0x181511da, 0xed7f1da48,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723212748Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/recording.go:123   +0x1d6 fp=0xc1c4e65908 sp=0xc1c4e657c8   pc=0x2029cd6\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723156007Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.EngineQueryFunc.func1(0x2cc8740,   0xc1c4df8bd0, 0xc1da8f6f60, 0x2d, 0x181511da, 0xed7f1da48, 0x0, 0x9,   0xc1c4e65788, 0x40f6b0,   ...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723040765Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:399   +0x4de fp=0xc1c4e65fa8 sp=0xc1c4e65d90   pc=0x202245e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723268613Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Group).run(0xc14b458700,   0x2cc86c0,   0xc000138000)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723252413Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Group).run.func1()\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723232332Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"github.com\/prometheus\/prometheus\/rules.(*Apprelabel).RuleFilter(0x3df6778,   0xc1dab2e410, 0x2cc8740, 0xc1c4df8bd0, 0x181511da, 0xed7f1da48, 0x0,   0xc000801900,   0x1401)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723093137Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:176   +0xcd fp=0xc1c4e656a8 sp=0xc1c4e65620   pc=0x202bcad\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723055807Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/apprelabel.go:26   +0x1ac fp=0xc1c4e657c8 sp=0xc1c4e656a8   pc=0x201ec6c\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72310692Z\"}\r\nMarch 28th 2021, 10:00:43.600 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:980   +0x65 fp=0xc1c4e65fd0 sp=0xc1c4e65fa8   pc=0x202f5c5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723292354Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:821   +0x77de\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723432386Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"goroutine 1 [chan receive, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723345208Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:43   +0xed\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723378387Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"runtime.goexit()\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723299778Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/rules.(*Manager).Update.func1\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723332815Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"goroutine 137 [select, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723459735Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"github.com\/oklog\/run.(*Group).Run(0xc001a87b48,   0xc0008e6d40,   0x8)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723371434Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"main.main()\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723425127Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/rules\/manager.go:975   +0x56\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723336883Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72345307Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/runtime\/asm_amd64.s:1374   +0x1 fp=0xc1c4e65fd8 sp=0xc1c4e65fd0   pc=0x471e61\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723324791Z\"}\r\nMarch 28th 2021, 10:00:43.601 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723340952Z\"}\r\nMarch 28th 2021, 10:00:43.701 | {\"log\":\"google.golang.org\/grpc.(*ccBalancerWrapper).watcher(0xc00029c880)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723464593Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"goroutine 89   [select]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723551086Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723542185Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"created by   google.golang.org\/grpc.newCCBalancerWrapper\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723477081Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"goroutine 136 [syscall, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723489342Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"created by   os\/signal.Notify.func1.1\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723534229Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"go.opencensus.io\/stats\/view.(*worker).start(0xc00047c140)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723555515Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/go.opencensus.io@v0.22.3\/stats\/view\/worker.go:154   +0x105\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723564559Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"os\/signal.loop()\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723520128Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/runtime\/sigqueue.go:147   +0x9d\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723508501Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"os\/signal.signal_recv(0x471e66)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723501691Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/os\/signal\/signal.go:150   +0x45\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723538249Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"created by   go.opencensus.io\/stats\/view.init.0\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72357106Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/os\/signal\/signal_unix.go:23   +0x25\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723524278Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723485741Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/balancer_conn_wrappers.go:60   +0x172\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723481028Z\"}\r\nMarch 28th 2021, 10:00:43.702 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/balancer_conn_wrappers.go:69   +0xc8\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723468497Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"main.main.func8(0x443196,   0x29ff508)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723663802Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000145000,   0xc000145020)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72368165Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/go.opencensus.io@v0.22.3\/stats\/view\/worker.go:32   +0x57\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723579895Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:601   +0x4e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723667609Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723703279Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/k8s.io\/klog\/v2@v2.0.0\/klog.go:1107   +0x8b\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723607211Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/k8s.io\/klog\/v2@v2.0.0\/klog.go:416   +0xd8\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723625532Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"k8s.io\/klog\/v2.(*loggingT).flushDaemon(0x3dc4d40)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723597177Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"github.com\/prometheus\/prometheus\/discovery.(*Manager).Run(0xc0001af360,   0xc0001d0360,   0xc00019f780)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72365239Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"goroutine 194 [chan receive, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723636195Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"created by   k8s.io\/klog\/v2.init.0\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723611429Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723687107Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:171   +0x74\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723659107Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723632181Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"goroutine 90 [chan   receive]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723587735Z\"}\r\nMarch 28th 2021, 10:00:43.703 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723584083Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723803278Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000980000,   0xc0008e6ce0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723737256Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/pkg\/logging.Dedupe\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723795024Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"goroutine 150   [select]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72377626Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723771484Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723766882Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723741993Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"goroutine 113 [select, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723724602Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:580   +0xff\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723733293Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723759124Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"main.main.func6(0x443196,   0x29ff508)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723728286Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/pkg\/logging\/dedupe.go:61   +0xcf\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723799234Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723710095Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/pkg\/logging\/dedupe.go:75   +0x1e5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723787895Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"github.com\/prometheus\/prometheus\/pkg\/logging.(*Deduper).run(0xc000612d80)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723781215Z\"}\r\nMarch 28th 2021, 10:00:43.704 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723720133Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/storage\/remote\/write.go:84   +0xb6\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723816476Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"goroutine 195 [chan receive, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723844485Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/storage\/remote\/write.go:77   +0x19f\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723826306Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:615   +0x4e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723877186Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:171   +0x74\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723862674Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723837598Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"github.com\/prometheus\/prometheus\/storage\/remote.(*WriteStorage).run(0xc0004c8e80)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723811546Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"goroutine 156 [chan   receive]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723807785Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723910517Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000145080,   0xc0001450c0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723887751Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/storage\/remote.NewWriteStorage\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723821925Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"main.main.func10(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723873226Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"github.com\/prometheus\/prometheus\/discovery.(*Manager).Run(0xc0001af4a0,   0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723858286Z\"}\r\nMarch 28th 2021, 10:00:43.705 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723904896Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"goroutine 196 [select, 238   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723926142Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"github.com\/prometheus\/prometheus\/scrape.(*Manager).Run(0xc00068e600,   0xc0001d05a0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72394218Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723992317Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/scrape\/manager.go:142   +0xe7\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723947211Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"goroutine 197 [select, 238   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724000346Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000809f80,   0xc0001450e0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723979813Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.7239183Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723984502Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723988542Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723996771Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:635   +0x8b\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72397565Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:660   +0x17f\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724021634Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"main.main.func12(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723968326Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.723922494Z\"}\r\nMarch 28th 2021, 10:00:43.706 | {\"log\":\"main.main.func14(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724016626Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"main.main.func16(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724079711Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc0003010e0,   0xc0008e6d20)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724094588Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"goroutine 199 [chan receive, 238   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724131743Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:706   +0x39e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724086817Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724123032Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724039357Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724053268Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:722   +0x72\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72414295Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000301080,   0xc0008e6d10)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72403463Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724111525Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"main.main.func18(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724135627Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724048683Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724127187Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724044371Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"goroutine 198 [chan receive, 238   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724057447Z\"}\r\nMarch 28th 2021, 10:00:43.707 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724119003Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72424747Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:784   +0x45\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724302148Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/web\/web.go:616   +0x10c5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724273186Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724179752Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724243659Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000145120,   0xc000145140)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724155969Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000980080,   0xc000809fb0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72421614Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72423778Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:769   +0x969\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724211174Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"goroutine 201 [select, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.7242556Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724187756Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"main.main.func22(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724285165Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724183726Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"goroutine 200 [chan receive, 238   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724193174Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724171544Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"github.com\/prometheus\/prometheus\/web.(*Handler).Run(0xc00095a000,   0x2cc8680, 0xc000612e40, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724268171Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"main.main.func20(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724197101Z\"}\r\nMarch 28th 2021, 10:00:43.708 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72425137Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724387853Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"github.com\/prometheus\/prometheus\/notifier.(*Manager).Run(0xc0004c8f80,   0xc0001d0660)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724334272Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"main.main.func24(0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724356943Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72440207Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"goroutine 202   [select]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72433049Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000145180,   0xc0008e6d30)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724309562Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72439813Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/notifier\/notifier.go:308   +0x105\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724339798Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:38   +0x27\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724313595Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"github.com\/oklog\/run.(*Group).Run.func1(0xc000301140,   0xc000984000,   0xc0008e6d40)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724371601Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724318448Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/cmd\/prometheus\/main.go:807   +0x8b\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724363702Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724326073Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/oklog\/run@v1.1.0\/group.go:37   +0xbb\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724322102Z\"}\r\nMarch 28th 2021, 10:00:43.709 | {\"log\":\"created by   github.com\/oklog\/run.(*Group).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724394221Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"goroutine 193   [select]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724447842Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/clientconn.go:1156   +0x709\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724541209Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"github.com\/prometheus\/prometheus\/discovery.(*Manager).sender(0xc0001af360)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72441619Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:170   +0x45\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724466207Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:263   +0x125\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724456317Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"goroutine 138 [chan receive, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724474267Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724470679Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"goroutine 203   [select]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724412112Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724436744Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/discovery.(*Manager).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72446225Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:263   +0x125\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724420192Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/discovery\/manager.go:170   +0x45\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724432778Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"github.com\/prometheus\/prometheus\/discovery.(*Manager).sender(0xc0001af4a0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72445232Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/discovery.(*Manager).Run\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724428636Z\"}\r\nMarch 28th 2021, 10:00:43.710 | {\"log\":\"google.golang.org\/grpc.(*addrConn).resetTransport(0xc000836000)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724535279Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"created by   github.com\/prometheus\/prometheus\/prompb.RegisterAdminHandlerFromEndpoint.func1\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724593283Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/internal\/poll\/fd_poll_runtime.go:87   +0x45\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724653006Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/runtime\/netpoll.go:222   +0x55\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724632292Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724553664Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/clientconn.go:799   +0x12a\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724549559Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724605468Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"goroutine 139 [chan receive, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724557363Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"github.com\/prometheus\/prometheus\/prompb.RegisterAdminHandlerFromEndpoint.func1.1(0x2cc8680,   0xc000612e40, 0xc00082c000, 0x29109de,   0xc)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724571162Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/prompb\/rpc.pb.gw.go:204   +0x1a5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724600564Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"goroutine 211 [IO wait, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724612125Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"internal\/poll.(*pollDesc).wait(0xc000959018, 0x72,   0x2c7d600, 0x3d1da50,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724643037Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"internal\/poll.runtime_pollWait(0x7f2e6316ace0,   0x72,   0x2c7d6c0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.72462406Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"created by   google.golang.org\/grpc.(*addrConn).connect\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724545282Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/internal\/poll\/fd_poll_runtime.go:92\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724668262Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"\\u0009\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/src\/prometheus\/prompb\/rpc.pb.gw.go:205   +0x4c\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724576544Z\"}\r\nMarch 28th 2021, 10:00:43.711 | {\"log\":\"internal\/poll.(*pollDesc).waitRead(...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724664099Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/golang.org\/x\/net@v0.0.0-20200602114024-627f9648deb9\/http2\/frame.go:492   +0xa5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724891347Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/net\/fd_posix.go:55   +0x4f\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724727073Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"net.(*netFD).Read(0xc000959000, 0xc000ecc000,   0x8000, 0x8000, 0xc000ebfd08, 0x407c1c,   0xc000500d80)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724721655Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/net\/net.go:182   +0x8e\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724774639Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/bufio\/bufio.go:227   +0x222\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724786987Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/io\/io.go:333\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724832366Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"bufio.(*Reader).Read(0xc000151260, 0xc000708118,   0x9, 0x9, 0xc000ebfdc0, 0x408ac5,   0xc00011f920)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724781795Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"golang.org\/x\/net\/http2.(*Framer).ReadFrame(0xc0007080e0,   0xc000bf57d0, 0x1, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724871051Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/io\/io.go:314   +0x87\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724824068Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"net.(*conn).Read(0xc00062fdf0, 0xc000ecc000,   0x8000, 0x8000, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724747909Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/golang.org\/x\/net@v0.0.0-20200602114024-627f9648deb9\/http2\/frame.go:237   +0x89\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724856367Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"golang.org\/x\/net\/http2.readFrameHeader(0xc000708118,   0x9, 0x9, 0x2c775a0, 0xc000151260, 0x0, 0xc000000000, 0xc000b2ca80,   0xc00062fdf8)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724850911Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"google.golang.org\/grpc\/internal\/transport.(*http2Client).reader(0xc00096a000)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724900257Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"io.ReadFull(...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724828047Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"internal\/poll.(*FD).Read(0xc000959000,   0xc000ecc000, 0x8000, 0x8000, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724690816Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"\\u0009\/root\/go1_15_6\/src\/internal\/poll\/fd_unix.go:159   +0x1a5\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724709567Z\"}\r\nMarch 28th 2021, 10:00:43.712 | {\"log\":\"io.ReadAtLeast(0x2c775a0, 0xc000151260,   0xc000708118, 0x9, 0x9, 0x9, 0xc00096c5c0, 0x249ae20,   0xc000b2ca80)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724816427Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"goroutine 140 [chan   receive]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.725028186Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/http2_client.go:300   +0xd31\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724917698Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"created by   google.golang.org\/grpc\/internal\/transport.newHTTP2Client\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724913781Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/http2_client.go:1273   +0x179\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724908908Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"github.com\/soheilhy\/cmux.muxListener.Accept(...)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.725031997Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/controlbuf.go:395   +0x125\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724960388Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.725024448Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724922283Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/github.com\/soheilhy\/cmux@v0.1.4\/cmux.go:229\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.725036582Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"google.golang.org\/grpc\/internal\/transport.newHTTP2Client.func3(0xc00096a000)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724987683Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/http2_client.go:346   +0x7b\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724991894Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/http2_client.go:344   +0xefc\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.725016845Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"goroutine 212 [select, 239   minutes]:\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724932448Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"google.golang.org\/grpc\/internal\/transport.(*controlBuffer).get(0xc000158a50,   0x1, 0x0, 0x0, 0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724946906Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"google.golang.org\/grpc\/internal\/transport.(*loopyWriter).run(0xc0001514a0,   0x0,   0x0)\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724972702Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"created by   google.golang.org\/grpc\/internal\/transport.newHTTP2Client\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724996131Z\"}\r\nMarch 28th 2021, 10:00:43.713 | {\"log\":\"\\u0009\/root\/jenkins\/data\/workspace\/paas23-prometheus_2.19.3\/pkg\/mod\/google.golang.org\/grpc@v1.29.1\/internal\/transport\/controlbuf.go:513   +0x1d3\\n\",\"stream\":\"stderr\",\"time\":\"2021-03-28T02:00:41.724977054Z\"}\r\n\r\n\r\n```\r\n\r\nhad similar problems before\r\n\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/3375\r\nhttps:\/\/github.com\/prometheus\/prometheus\/issues\/3781","comments":["Thanks for this. It might be https:\/\/github.com\/prometheus\/prometheus\/issues\/8318, because it seems it happened around 10:00, so when we change the head. ","cc @codesome WDYT?","> Thanks for this. It might be #8318, because it seems it happened around 10:00, so when we change the head.\r\n\r\nIs this bug solved?\r\n","That but is not yet solved.","> That but is not yet solved.\r\n\r\nIs there any way to circumvent this problem temporarily? Our prometheus always restarts","If you know the rule group that causes this you can change the rule group name slightly.","> If you know the rule group that causes this you can change the rule group name slightly.\r\n\r\nMeans caused by the alert rule or recording rule \uff1f","Can you please try now that  #8318 is fixed ? (Prometheus v2.27.1)","```\r\nunexpected fault address 0x7f27a662a0b8\r\nfatal error: fault\r\n[signal SIGBUS: bus error code=0x2 addr=0x7f27a662a0b8 pc=0x7149f4]\r\n\r\ngoroutine 4738720 [running]:\r\nruntime.throw(0x2e766cc, 0x5)\r\n\t\/home\/go\/src\/runtime\/panic.go:1117 +0x72 fp=0xc05a7c6d80 sp=0xc05a7c6d50 pc=0x439f12\r\nruntime.sigpanic()\r\n\t\/home\/go\/src\/runtime\/signal_unix.go:731 +0x2c8 fp=0xc05a7c6db8 sp=0xc05a7c6d80 pc=0x4516c8\r\ngithub.com\/prometheus\/prometheus\/tsdb\/chunks.(*ChunkDiskMapper).Chunk(0xc00021a240, 0x155017ff0a0, 0x0, 0x0, 0x0, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/tsdb\/chunks\/head_chunks.go:514 +0x6b4 fp=0xc05a7c6f80 sp=0xc05a7c6db8 pc=0x7149f4\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*memSeries).chunk(0xc026674fc0, 0x2f, 0xc00021a240, 0x7f27637e8e60, 0x18, 0x28, 0x7f286289c1d8)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/tsdb\/head.go:2372 +0x15d fp=0xc05a7c6fe0 sp=0xc05a7c6f80 pc=0x9fea3d\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*headChunkReader).Chunk(0xc02511a700, 0x2842c300002f, 0x0, 0x0, 0x0, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/tsdb\/head.go:1780 +0xe5 fp=0xc05a7c7070 sp=0xc05a7c6fe0 pc=0x9fad05\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*populateWithDelGenericSeriesIterator).next(0xc02529c280, 0xc05a7c7120)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/tsdb\/querier.go:555 +0xca fp=0xc05a7c70f8 sp=0xc05a7c7070 pc=0xa0504a\r\ngithub.com\/prometheus\/prometheus\/tsdb.(*populateWithDelSeriesIterator).Next(0xc0251831b8, 0x32dd0d8)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/tsdb\/querier.go:608 +0x4a fp=0xc05a7c7130 sp=0xc05a7c70f8 pc=0xa0542a\r\ngithub.com\/prometheus\/prometheus\/storage.(*MemoizedSeriesIterator).Reset(...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/storage\/memoized_iterator.go:58\r\ngithub.com\/prometheus\/prometheus\/promql.(*evaluator).eval(0xc0278c7b00, 0x32dc9a0, 0xc02138c980, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:1474 +0xbb2 fp=0xc05a7c7da8 sp=0xc05a7c7130 pc=0xa32552\r\ngithub.com\/prometheus\/prometheus\/promql.(*evaluator).rangeEval(0xc0278c7b00, 0x0, 0xc05a7c8e88, 0xc02138ca00, 0x5, 0x8, 0x0, 0x0, 0x0, 0x330bae0, ...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:987 +0x1add fp=0xc05a7c8438 sp=0xc05a7c7da8 pc=0xa3105d\r\ngithub.com\/prometheus\/prometheus\/promql.(*evaluator).eval(0xc0278c7b00, 0x32dc7e0, 0xc02510dd70, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:1256 +0x32dd fp=0xc05a7c90b0 sp=0xc05a7c8438 pc=0xa34c7d\r\ngithub.com\/prometheus\/prometheus\/promql.(*evaluator).Eval(0xc0278c7b00, 0x32dc7e0, 0xc02510dd70, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:901 +0xb2 fp=0xc05a7c9128 sp=0xc05a7c90b0 pc=0xa2f172\r\ngithub.com\/prometheus\/prometheus\/promql.(*Engine).execEvalStmt(0xc0005215e0, 0x32dc3b8, 0xc02510dec0, 0xc0278c79e0, 0xc024f90a00, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:584 +0x4bd fp=0xc05a7c9338 sp=0xc05a7c9128 pc=0xa2cc1d\r\ngithub.com\/prometheus\/prometheus\/promql.(*Engine).exec(0xc0005215e0, 0x32dc3b8, 0xc02510dec0, 0xc0278c79e0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:536 +0x4fe fp=0xc05a7c9690 sp=0xc05a7c9338 pc=0xa2c27e\r\ngithub.com\/prometheus\/prometheus\/promql.(*query).Exec(0xc0278c79e0, 0x32dc3b8, 0xc02510dd10, 0xc0250e7200)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/promql\/engine.go:174 +0x9d fp=0xc05a7c9738 sp=0xc05a7c9690 pc=0xa2a05d\r\ngithub.com\/prometheus\/prometheus\/rules.EngineQueryFunc.func1(0x32dc3b8, 0xc02510dd10, 0xc0250e7200, 0x36, 0x3163e9bf, 0xed88ac84f, 0x0, 0x0, 0x0, 0x1, ...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:192 +0xca fp=0xc05a7c97c0 sp=0xc05a7c9738 pc=0x240c4ca\r\ngithub.com\/prometheus\/prometheus\/rules.(*RecordingRule).Eval(0xc050925540, 0x32dc3b8, 0xc02510dd10, 0x3163e9bf, 0xed88ac84f, 0x0, 0xc000175f40, 0xc000760120, 0x10, 0x330bae0, ...)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/recording.go:93 +0xad fp=0xc05a7c9908 sp=0xc05a7c97c0 pc=0x240a48d\r\ngithub.com\/prometheus\/prometheus\/rules.(*Group).Eval.func1(0x32dc3b8, 0xc019233380, 0xc050962540, 0x3163e9bf, 0xed88ac84f, 0x0, 0xc05a7c9c88, 0x1, 0x330c570, 0xc050925540)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:595 +0x351 fp=0xc05a7c9c30 sp=0xc05a7c9908 pc=0x240d791\r\ngithub.com\/prometheus\/prometheus\/rules.(*Group).Eval(0xc050962540, 0x32dc3b8, 0xc019233380, 0x3163e9bf, 0xed88ac84f, 0x0)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:672 +0x133 fp=0xc05a7c9cd0 sp=0xc05a7c9c30 pc=0x2405f53\r\ngithub.com\/prometheus\/prometheus\/rules.(*Group).run.func1()\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:345 +0x19e fp=0xc05a7c9d78 sp=0xc05a7c9cd0 pc=0x240c89e\r\ngithub.com\/prometheus\/prometheus\/rules.(*Group).run(0xc050962540, 0x32dc348, 0xc00005e038)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:420 +0x56e fp=0xc05a7c9f90 sp=0xc05a7c9d78 pc=0x2403dce\r\ngithub.com\/prometheus\/prometheus\/rules.(*Manager).Update.func1(0xc05fecbf00, 0x0, 0xc0559718b0, 0xc0003075c0, 0xc050962540)\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:1012 +0x8d fp=0xc05a7c9fb8 sp=0xc05a7c9f90 pc=0x241026d\r\nruntime.goexit()\r\n\t\/home\/go\/src\/runtime\/asm_amd64.s:1371 +0x1 fp=0xc05a7c9fc0 sp=0xc05a7c9fb8 pc=0x473dc1\r\ncreated by github.com\/prometheus\/prometheus\/rules.(*Manager).Update\r\n\t\/apphome\/jenkins\/data\/workspace\/paas23-prometheus_2.27.0.1\/src\/prometheus\/rules\/manager.go:1002 +0x5c5\r\n\r\n```\r\nprometheus:v2.27.0 has the same error","@roidelapluie [BUGFIX] TSDB: Avoid panic when mmaped memory is referenced after the file is closed. #8723","Unfortunately you do not say much about the release you are using, or the operating system. Can we get full logs and could you use an official build?\r\n\r\nThanks","we are facing the same issue\r\n\r\n`ts=2023-10-31T08:02:55.181Z caller=main.go:533 level=info vm_limits=\"(soft=unlimited, hard=unlimited)\"\r\nunexpected fault address 0x7f201c73d000\r\nfatal error: fault\r\n[signal SIGBUS: bus error code=0x2 addr=0x7f201c73d000 pc=0x46c222]\r\ngoroutine 1 [running]:\r\nruntime.throw({0x2e0ce11?, 0x0?})\r\n\t\/usr\/local\/go\/src\/runtime\/panic.go:992 +0x71 fp=0xc0006bf0d8 sp=0xc0006bf0a8 pc=0x4384b1\r\nruntime.sigpanic()\r\n\t\/usr\/local\/go\/src\/runtime\/signal_unix.go:815 +0x125 fp=0xc0006bf128 sp=0xc0006bf0d8 pc=0x44e525\r\nruntime.memmove()\r\n\t\/usr\/local\/go\/src\/runtime\/memmove_amd64.s:151 +0x102 fp=0xc0006bf130 sp=0xc0006bf128 pc=0x46c222\r\n[github.com\/prometheus\/prometheus\/promql.NewActiveQueryTracker({0x7ffdf33ddf61](http:\/\/github.com\/prometheus\/prometheus\/promql.NewActiveQueryTracker(%7B0x7ffdf33ddf61), 0xb}, 0x14, {0x3757500, 0xc000af4550})\r\n\t\/app\/promql\/query_logger.go:123 +0x230 fp=0xc0006bf270 sp=0xc0006bf130 pc=0x228f550\r\nmain.main()\r\n\t\/app\/cmd\/prometheus\/main.go:587 +0x61bf fp=0xc0006bff80 sp=0xc0006bf270 pc=0x23b705f\r\nruntime.main()\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:250 +0x212 fp=0xc0006bffe0 sp=0xc0006bff80 pc=0x43abd2\r\nruntime.goexit()\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1571 +0x1 fp=0xc0006bffe8 sp=0xc0006bffe0 pc=0x46b2a1\r\ngoroutine 14 [select]:\r\n[go.opencensus.io\/stats\/view.(*worker).start(0xc0000a7080)](http:\/\/go.opencensus.io\/stats\/view.(*worker).start(0xc0000a7080))\r\n\t\/go\/pkg\/mod\/go.opencensus.io@v0.23.0\/stats\/view\/worker.go:276 +0xad\r\ncreated by [go.opencensus.io\/stats\/view.init.0](http:\/\/go.opencensus.io\/stats\/view.init.0)\r\n\t\/go\/pkg\/mod\/go.opencensus.io@v0.23.0\/stats\/view\/worker.go:34 +0x8d\r\ngoroutine 114 [select]:\r\n[github.com\/prometheus\/prometheus\/util\/logging.(*Deduper).run(0xc000a9c780)](http:\/\/github.com\/prometheus\/prometheus\/util\/logging.(*Deduper).run(0xc000a9c780))\r\n\t\/app\/util\/logging\/dedupe.go:75 +0xe8\r\ncreated by [github.com\/prometheus\/prometheus\/util\/logging.Dedupe](http:\/\/github.com\/prometheus\/prometheus\/util\/logging.Dedupe)\r\n\t\/app\/util\/logging\/dedupe.go:61 +0x10a`"],"labels":["kind\/more-info-needed"]},{"title":"Maintaining free range markers on the the default port allocations page","body":":wave: I noticed the [default port allocations](https:\/\/github.com\/prometheus\/prometheus\/wiki\/Default-port-allocations) wiki page was missing some markers for free ranges, so I [wrote a little program](https:\/\/gist.github.com\/tsibley\/8a705ba15fef4bc03d833f424419237b) to keep them up-to-date (mostly so I could myself easily see what ranges were free :-). I don't know if there's any interest in making this an automated part of the maintenance of that page (e.g. with GH Actions). I can't submit a PR to the wiki repo, so I'm creating this issue FWIW. Thanks for Prometheus!","comments":["Thank you for this. The default port allocation page is mainly community maintained, and the format is quite fragile. I am unsure we should \"automate\" it in its current form.\r\n\r\nI have added this topic to the discussion topic we have at our monthly public developers summits: https:\/\/prometheus.io\/community to see if we need have a better solution than a wiki page.","Yes, we didn't imagine needing thousands of individual port allocations when we first created the page. :joy_cat: "],"labels":["priority\/Pmaybe","component\/documentation"]},{"title":"Remaining tasks for rule backfiller","body":"The main [PR](https:\/\/github.com\/prometheus\/prometheus\/pull\/7675) that implements retroactive rule backfiller (ref: Issue #11) is now merged, but there are a few final tasks that need to be completed. This issue tracks those issues:\r\n\r\n- [x] add documentation in the \"storage\" page  ([ref](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/docs\/storage.md#backfilling-from-openmetrics-format)) \r\n- [ ] add stale markers\r\n- [ ] use log.logger from the beginning, to avoid mixing fmt.Println and logger\r\n","comments":[],"labels":["kind\/enhancement","component\/promql","priority\/P3"]},{"title":"Add distroless containers","body":"## Proposal\r\n\r\nPer the discussion on various issues relating to busybox security, and the resolution from the 2020-03-25 Developer Summit, we want to add \"distroless\" containers as an option for Prometheus and related components.\r\n\r\n* Add additional `-busybox` tags to existing containers.\r\n* Create \"distroless\" containers tagged with `-distroless`.\r\n* Announce change to distroless as the default container tags for Prometheus 3.0.","comments":["for Prometheus 3.0.","I'm confused by the tags on this issue. Help wanted mean this is something that someone could work on? \r\nBut then there is a comment saying this is for 3.0.  \r\n\r\nThe original description isn't clear on timeframe, but it sounds like this could be implemented as a non-breaking change in 2.x, with the announcement that distroless would be be the default in 3.0. \r\n\r\n* Keep existing \"busybox\" containers, tag them with the existing tags and also `*-busybox`. \r\n* Create \"distroless\" containers tagged with `*-distroless`.\r\n* Announce that distroless will be the default container style for Prometheus 3.0.\r\n* Continue producing both styles in 2.x.\r\n\r\n@roidelapluie @SuperQ \r\nAre you open to someone implementing this for 2.x in a non-breaking fashion? \r\n\r\n","Another reason to provide distroless containers is BusyBox licensing.\r\nBusyBox is GPLv2 licensed. See BusyBox licensing docs and enforcement [here](https:\/\/www.busybox.net\/license.html). I believe the CNCF also requires additional approval for distribution of GPLv2 assets. See CNCF licensing docs [here](https:\/\/github.com\/cncf\/foundation\/blob\/main\/allowed-third-party-license-policy.md#approved-licenses-for-allowlist)."],"labels":["help wanted","priority\/P3","kind\/feature"]},{"title":"Cosmetic bug when scraping IPv6 addresses containing double colons ::","body":"In v2.23.0 when I have a scraping endpoint IPv6 address such as 2a03:f347:40:1::4f, I see an error on the web UI\r\n```\r\nError: Failed to construct 'URL': Invalid URL\r\n\r\n: address 2a03:f347:40:1::6d:9100: too many colons in address\r\n```\r\nThe scrape does happen and the \"State\" is showing as up. ","comments":["I think this was fixed in 2.25 (#8359).\r\n\r\nCan you please upgrade to 2.25.2 and verify the fix?\r\n\r\nThanks!","Just checked, 2.25.2 still has this issue. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/2947691\/111723062-3980b780-8820-11eb-94c2-11b4257ef7e2.png)\r\n","I think I have figured out the reason\r\n\r\nIn IPV6, the host:port syntax has been changed. Here's the detail in [rfc3986](https:\/\/www.ietf.org\/rfc\/rfc3986.txt)\r\n\r\n> A host identified by an Internet Protocol literal address, version 6 [RFC3513] or later, is distinguished by enclosing the IP literal within square brackets (\"[\" and \"]\").  This is the only place where square bracket characters are allowed in the URI syntax.  In anticipation of future, as-yet-undefined IP literal address formats, an implementation may use an optional version flag to indicate such a format explicitly rather than rely on heuristic determination.\r\n\r\nSo \"xxx:xx:xx:xx:port\" is not right, it will cause error here https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/web\/api\/v1\/api.go#L795\r\n\r\nI'll try to make a PR for this.\r\n","I am surprised that Prometheus can scrape `2a03:f347:40:1::6d:9100` in the first place. It should scrape `[2a03:f347:40:1::6d]:9100`.","@roidelapluie yes, I'm surprised there's not error when Go access the 2a03:f347:40:1::6d:9100 too.\r\n\r\nAnyway, I'm working on a PR for this issue","@aned Hello, Would you mind introducing what's kind of discovery method you use?\r\n\r\nI think it will help to find out the root cause?","@Zheaoli we query internal DB and generate yaml files for prometheus. ","It seems like the yaml generated by the DB is not correct. However, we should still fix this in the UI if Prometheus can actually scrape it.","Also facing problems with scraping most of IPv6 (pods on Kubernetes) addresses. ","With the right syntax scrapping is working for me on version 2.31.1, but the web interface still shows an error in the targets list:\r\n\r\n    Error: URL constructor: http:\/\/[fe80::1c33:5ff:fe06:92fc%25enx8c04ba675c4c]:9100\/metrics is not a valid URL."],"labels":["help wanted","kind\/bug","component\/ui","priority\/P3"]},{"title":"Per rule evaluation duration metric","body":"## Proposal\r\n\r\nCurrently we provide metrics (duration and success) at a rule-group level. But a group can have 10s of rules inside it. If a rule_group is skipping iterations, then its likely caused by a few rules inside the group. Today, we can easily find the culprit rules by looking at the UI.\r\n\r\nHaving said that, we are seeing some groups slowly creeping up in evaluation duration and knowing the historical evaluation duration for each rule would be quite helpful. This would basically be adding `prometheus_rule_group_rule_last_duration_seconds` which has an index of the rule being evaluated as a label.\r\n\r\nA workaround to this is to write an exporter which queries the API and exposes the metrics, but I'd like to avoid that if possible.\r\n","comments":["This seems like something for the query_log > loki.\r\n\r\nA label with the rule ID would just be too much cardinality here. As you say, you can have 10s of rules inside a group.\r\n\r\nNote that you would probably not need to write an exporter, you could simply use the json_exporter.","This seems really expansive in terms of metrics we are exposing. It sounds like events. This can be extracted from the query log or the traces we have in PromQL.","Interrested for keep track of rules duration evalution and alert on it"],"labels":["priority\/Pmaybe","component\/rules","kind\/feature"]},{"title":"Unit testing for relabeling rules","body":"It would be great to have unit testing for relabeling configs. Relabeling configs can shift over time, and having unit tests would give confidence that we don't break them.\r\n\r\nIt is also not easy to test them without a cloud provider or k8s cluster on. I think we should not have that dependency.\r\n\r\nWe already have unit testing for rules, but I'd like to have something to test relabeling rules inside promtool.\r\n\r\nIdeas welcome.\r\n","comments":["I think this could be pretty easily added to the existing test file by doing the following:\r\n\r\n- Adding a `config_files` field, where a user could specify the Prometheus config file their relabeling configs are in.\r\n- Adding an `input_labels` field, which holds further labels and their values, besides the ones that can already be gathered from `input_series`.\r\n- Then we could add a `relabel_config_test` field, which accepts the name of the job that the relabeling configs belong to, and tests the configs for that job using the labels from `input_series` and `input_labels`.\r\n\r\nExample:\r\n```yaml\r\n# rule.yaml\r\nconfig_files:\r\n - prometheus.yml\r\n\r\ntests:\r\n - interval: 1m\r\n\r\n   input_series:\r\n    - series: 'up{job=\"prometheus\", instance=\"localhost:9090\"}'\r\n      values: '0'\r\n\r\n    - input_labels:\r\n       - name: 'foo'\r\n         value: 'bar'\r\n\r\n    - relabel_config_test:\r\n     # Given the labels 'prometheus', 'instance' and 'foo'\r\n     - name: prometheus\r\n       exp_labels:\r\n         - name: 'foo'\r\n           value: 'car'\r\n```\r\n```yaml\r\n# prometheus.yml\r\n...\r\nscrape_configs:\r\n - job_name: 'prometheus'\r\n   ...\r\n   relabel_configs:\r\n      # This is probably wrong\r\n    - source_labels: ['foo']\r\n      replacement: 'car'\r\n      target_label: 'foo'\r\n```\r\n\r\nA possible thing to consider is if we should keep the changed label set and pass it to the `alert` and `promql` tests in the same test group.","cc @dgl you are better placed to comment on this","Something like this approach could work for _metric_ relabels. Maybe that would be an interesting thing to prototype first...\r\n\r\nHowever I think the real value is something that could test service discovery relabelling configs, as those are usually more complex. The tricky bit is it wouldn't necessarily be possible to infer the job as proposed, as that could be changed by relabelling.\r\n\r\nIn fact your example nicely shows how the unit tests currently get away without caring about the source of timeseries (internal vs user metrics), but per the docs on metric relabelling:\r\n\r\n```\r\nMetric relabeling does not apply to automatically generated timeseries such as `up`.\r\n```\r\n\r\nSo we'd need to know which series those are, so as not to apply metric relabelling to them. This leads to a potential design: mark series as either processed by service discovery, or from a particular job.\r\n\r\n```\r\n- input_series:\r\n    # Series has no labels, as those will be provided by service discovery (or\r\n    # user can override them?)\r\n    - series: 'up{}'\r\n      values: '0x200'\r\n      sd:\r\n        source_job: \"a-job-defined-in-config\"\r\n        labels:\r\n          __meta_consul_node: node1\r\n          instance: \"...\"\r\n```\r\n\r\nor for a metric_relabel_configs test:\r\n\r\n\r\n```\r\n- input_series:\r\n    - series: 'something_total{job=\"a-job\"}'\r\n      values: '0+1x200'\r\n      metric_relabel_source_job: \"a-job-defined-in-config\"\r\n```\r\n\r\n\r\n> A possible thing to consider is if we should keep the changed label set and\r\n> pass it to the alert and promql tests in the same test group.\r\n\r\nIt definitely should, in the same way it would in the real system. I suspect actually making that happen may be challenging due to how the promtool unit tests currently pass the series straight to the promql testing code.\r\n\r\nI think if that can be done then there's no need for `relabel_config_test`, just let the user match labels normally on their series, rather than making relabels special.","> Something like this approach could work for metric relabels. Maybe that would be an interesting thing to prototype first...\r\n>\r\n> However I think the real value is something that could test service discovery relabelling configs, as those are usually more complex.\r\n\r\nI have been naive about the difference between these two in the past; I wrote that proposal quite a while ago and I assume that I had combined them in my mind. I definitely think that we should cover both methods.\r\n\r\n> So we'd need to know which series those are, so as not to apply metric relabelling to them. This leads to a potential design: mark series as either processed by service discovery, or from a particular job.\r\n\r\nIt took me a little bit to wrap my head around this, but my understanding is this is the distinction between target relabeling and metric relabeling. To that end, I think we should consider changing the `sd` field to `job` (and subsequently `source_job` to just `source`) to make it clear that this won't actually utilize any of the service discovery mechanisms (I think) and therefore has nothing to do with service discovery except that you might want to simulate a target garnered from it (or from `static_configs`, which is different)\r\n\r\n> I think if that can be done then there's no need for relabel_config_test, just let the user match labels normally on their series, rather than making relabels special.\r\n\r\nThis might cause some confusion\/inconvenience. I do think it's fine for metric relabeling, as it's specifically for series, but target relabeling is somewhat separate. I could definitely see a user wanting to only test their target relabeling rules but have to specify series anyway _and_ write tests for PromQL expressions, all to get access to those target labels. Actually, now that I'm thinking about it, that might be an annoyance for testing metric relabeling as well. Both are independent of PromQL and shouldn't have to be tested under its functionality.\r\n\r\nA few more things we may want to think about:\r\n\r\n* Targets can be dropped during relabeling and we should support testing for that.\r\n\r\n* This is a whole other ordeal, but given that target relabeling is often used to filter out good targets from what is returned by service discoveries, the labels these mechanisms assign are necessary. Making users do the work themselves when testing is tedious but more importantly allows for inconstancies with the actual behavior of service discoveries. This means that relabeling rules can be written and tested mistakenly based on a different implementation than what Prometheus actually uses. But enabling service discoveries to be configured for offline testing would be tricky, and I don't think it's worth the effort.\r\n\r\n* Currently, we call this \"unit testing for rules\", and although I wasn't privy to its naming, testing PromQL expressions already falls out of its bounds in my opinion. Now adding relabeling will stray even further, so I think we should consider giving it a more general name.","Service discovery relabelling configs can be tested with #8970.\r\n\r\nThis would be to test metric relabeling.","Right, forgot that it had that. I think the service discovery capability in this still is needed given the nature of the `check sd` command, which outputs the result of relabeling but doesn't test it. Sure, you can go by sight, but this provides an automated and rigid system. In my opinion, both these features can live together."],"labels":["component\/promtool","priority\/P3","kind\/feature"]},{"title":"SigV4, EC2 SD: use AWS shared config file by default","body":"#8509 introduced support for SigV4, where most of the logic configuring the AWS SDK (v1) was copied from the EC2 SD code. A [comment](https:\/\/github.com\/prometheus\/prometheus\/pull\/8509#discussion_r581366746) from @rakyll noted that by default, the configuration in `~\/.aws\/config` is not loaded unless `AWS_SDK_LOAD_CONFIG` is set to a non-empty string in the environment variables. \r\n\r\nThe dependency of `AWS_SDK_LOAD_CONFIG` changes based on the specific AWS SDK being used. For example, the Go _v1_ and Javascript SDKs need it defined, but the Java and Go _v2_ SDKs don't. I'm not an expert on the SDKs, but I think it's becoming expected user behavior that the shared config files are loaded by default. \r\n\r\nSince SigV4 was written with the intent of sharing config settings with the EC2 SD config, I propose we should update both of them to change the default behavior. This can be done in one of two ways: \r\n\r\n1. When instantiating the AWS config, always enable the shared config by appropriately configuring the [SharedConfigState](https:\/\/pkg.go.dev\/github.com\/aws\/aws-sdk-go@v1.34.29\/aws\/session#SharedConfigState).\r\n2. Upgrade usage of the AWS SDK to v2, as recommended [by this comment in the same thread](https:\/\/github.com\/prometheus\/prometheus\/pull\/8509#discussion_r581598451).\r\n\r\nMoving to the newer version of the SDK makes the most sense to me, since I presume that's where most development effort will eventually end up. \r\n\r\nI'm planning on making this change myself next week, but I wanted to open this as an issue first for discussion in case there are problems with the v2 SDK (and to make sure I don't forget to do the work I promised :) ","comments":["This would be a big change in the way we deal with service discovery credentials\/configuration. I think we should wait enough time to let people express their interest or concerns.","Good point, I'll hold off for a while. Any concerns about getting this change into the upcoming release? (with the consideration that the workaround is to supply `AWS_SDK_LOAD_CONFIG`)","I see two challenges:\r\n-  be careful about the priority of existing env variables regarding this, to not break users\r\n- find a good source of documentation for this","My understanding of the behavior so far has been that, if no authentication information is given in the configuration file, we defer to the SDK. Since this is the default SDK behavior in the latest version, we should not get in the way of it. Upgrading the SDK, if feasible, deems like the cleanest option \u2013 Prometheus continues to not have an opinion on where the SDK gets its configuration, users get exactly the same behavior as from other tools.\r\n\r\nWe should call this out as a change (\"the `AWS_SDK_LOAD_CONFIG` workaround is no longer necessary\") but I don't think we need to reproduce documentation of the standard SDK behavior. We should add to the docs that the default SDK behavior applies if no explicit options are given; that is already true today.","I am wondering if the file would override env variables or not. I agree we don't need to duplicate the docs, but we need to find and link to accurate docs for ursers.","Is this the right one?\n\nhttps:\/\/docs.aws.amazon.com\/credref\/latest\/refdocs\/overview.html\n\nOn Sat, Mar 13, 2021, 00:46 Julien Pivotto ***@***.***> wrote:\n\n> I am wondering if the file would override env variables or not. I agree we\n> don't need to duplicate the docs, but we need to find and link to accurate\n> docs for ursers.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/prometheus\/prometheus\/issues\/8592#issuecomment-797816934>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AABAEBUD5OPG3QW6J7WBAKDTDKRTRANCNFSM4ZCRRCPA>\n> .\n>\n","Yesssss thanks","As someone who wanted to use the AWS config profile option with sigv4 please consider mentioning the `AWS_SDK_LOAD_CONFIG` env variable in the docs. I just spent a day figuring it out why it just wouldn't use the AWS config file. \r\n\r\nIf you're using the remote_write option with sigv4 in an AWS environment it'll always try to use the WebToken or the IAM role authentication (that would be the default credentials provider chain) but given that there is a real possibility that the remote_write URL is outside of your control (e.g. a third party cost allocation tool) means that these authentication options won't work and you need to use explicit credentials (e.g. the AWS config file). ","It also appears that, for non-aws environments,  there is no logic for the remote write client to reload the credentials from the config file periodically, It's a common usecase that the credentials are refreshed and re-written to the same file, to be reloaded up by the applications. The expectation seems to be that the credentials are permanent unless explicitly revoked. Did anyone else encounter this? ","If you mean when using non-SigV4 authentication methods, various other authentication methods have an option to specify a file for their respective credentials (`basic_auth.password_file`, `authorization.credentials_file`, etc.) And the credentials in those files are checked for changes on every request.","On another note, @roidelapluie I don't think any concerns have been raised that would that would make us reconsider adding this feature, and there certainly is some interest, so maybe we could upgrade it to a p3?"],"labels":["kind\/enhancement","priority\/Pmaybe","component\/service discovery"]},{"title":"Prometheus should have the ability to rename metrics","body":"<!--\r\n\r\n    Please do *NOT* ask usage questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use:\r\n    https:\/\/groups.google.com\/forum\/#!forum\/prometheus-users. If\r\n    you are unsure whether you hit a bug, search and ask in the\r\n    mailing list first.\r\n\r\n    You can find more information at: https:\/\/prometheus.io\/community\/\r\n\r\n-->\r\n## Proposal\r\n\r\n**Use case. Why is this important?**\r\n\r\nIn projects like Kubernetes, it is common for hundreds of metrics that end users rely on to be created inconsistently. It is very difficult and disruptive to standardize these metrics right now as there is no means of renaming a metric in Prometheus, so we follow a multi-release deprecation cycle for end users to update all their use cases. This is extremely painful for both the development cycle and end users.\r\n\r\nIt would be much improved if Prometheus offered a means of *renaming* metrics, so clients could update the metric names to something more reasonable while not breaking the world.\r\n\r\nIf an exported metric looks like\r\n\r\n```\r\n# HELP go_goroutines Number of goroutines that currently exist.\r\n# TYPE go_goroutines gauge\r\ngo_goroutines 42\r\n```\r\n\r\nA renamed metric could perhaps look like\r\n\r\n```\r\n# HELP not_go_goroutines Number of goroutines that currently exist.\r\n# TYPE not_go_goroutines gauge\r\n# RENAMED not_go_goroutines go_goroutines\r\nnot_go_goroutines 42\r\n```\r\n\r\nClients could continue to query for either `go_goroutines` or `not_go_goroutines` interchangeably so long as the rename flag was present at scrape. (I am not attached to this particular implementation, just throwing something out there for discussion.)\r\n\r\n**Why not use a recording rule?**\r\n\r\nRecording rules must be configured by clients. Clients must therefore know that the rename happened. This is very manual and labour-intensive. I would like a mechanism at the _scrape_ point or server side to be able to indicate a rename.","comments":["Thank you.\r\n\r\nI have read your issue with attention, and it is something that indeed can't be achieved by targets. There are multiple ways to do this on the server side, but not something that can be done at the exposer side.\r\n\r\nPrometheus currently uses OpenMetrics as standards for the exposition format, and I think this issue could be discussed better in the OpenMetrics project.\r\n\r\nWould you mind open an issue in https:\/\/github.com\/OpenObservability\/OpenMetrics and link it back here? Thanks!","As an additional note, you can also use metrics relabeling to change metrics name at scrape time: https:\/\/alexandrev.medium.com\/how-to-change-the-names-of-your-metrics-in-prometheus-b78497efb5de but it requires configuration in the prometheus server itself.","The overhead in the index seems manageable if we decided to support this.\r\n\r\nWith my OpenMetrics hat on, `# ALIAS` seems cleaner.\r\n\r\nThis does not solve potential changes and migrations in labelset, though.\r\n\r\nThe main risk is that the \"temporary migration\" will be a long-term cost for everyone in the ecosystem.\r\n\r\nOverall, I think something of this depth needs long thought and discussion in -team; personally, I am intrigued but not sold.\r\n\r\nLoosely related: Improving usability of relabelling could solve this from a different angle.","Done: https:\/\/github.com\/OpenObservability\/OpenMetrics\/issues\/189","We will discuss this in a future dev summit.","Loose cross-ref to https:\/\/github.com\/prometheus\/node_exporter\/pull\/2059 which addresses a related need.","The relabeling tool is already available via `thanos tools bucket rewrite --rewrite.to-relabel-config` to rewrite a TSDB block  for relabeling. Or you can try the same functionality with https:\/\/github.com\/yeya24\/promrelabel, but for Prometheus TSDB block only (no obj store support).","As per https:\/\/docs.google.com\/document\/d\/11LC3wJcVk00l8w5P3oLQ-m3Y37iom6INAMEu2ZAGIIE\/edit#heading=h.n3sg0f2t8804 on 2021-09-23 :\r\n\r\n> CONSENSUS: We would like to support the use case of migrating metric names and label sets. We do not intend to support it through modifying TSDB implementation or metric semantics, but through a relabelling tool.","@yeya24 would be possible to upstream this into `promtool`?","To upstream this, we need to also upstream the code of thanos compactv2 package. cc @bwplotka ","CC @codesome ","@yeya24 do you need to upstream the entire compactv2 package? I think we can extend the current compaction code in Prometheus to make few things pluggable (like some kind of processing step before adding the series to the new block) and upstream the relevant pluggable part."],"labels":["priority\/Pmaybe","component\/scraping","component\/tsdb","kind\/feature"]},{"title":"Prometheus 2.25 does not start up (upgrade from 2.19)","body":"**What did you do?**\r\n\r\n* Existing 2.19 prometheus system with a VERY large amount of metrics\r\n* Upgrade to 2.25.0\r\n\r\n**What did you expect to see?**\r\n\r\n* Prometheus loads ALL WAL and starts up\r\n\r\n**What did you see instead? Under which circumstances?**\r\n\r\n* Prometheus loads WAL files to a certain point and then hangs doing nothing for hours. Last log entry is `WAL segment loaded segment=14381 maxSegment=15632` and it does not progress from here\r\n\r\n**Environment**\r\n\r\nCentos 7.5 \/ systemd\r\n\r\n* System information:\r\n\r\n```\r\nLinux 3.10.0-862.el7.x86_64 x86_64\r\n```\r\n\r\n* Prometheus version:\r\n\r\n```\r\nprometheus, version 2.25.0 (branch: HEAD, revision: a6be548dbc17780d562a39c0e4bd0bd4c00ad6e2)\r\n  build user:       root@615f028225c9\r\n  build date:       20210217-14:17:24\r\n  go version:       go1.15.8\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Prometheus configuration file:\r\n```\r\nnot relevant\r\n```\r\n\r\n* Logs:\r\n```\r\nMar 08 18:42:44 <redacted> prometheus[435720]: level=info ts=2021-03-08T10:42:44.171Z caller=head.go:740 component=tsdb msg=\"WAL segment loaded\" segment=14381 maxSegment=15632\r\n```\r\n","comments":["Noticed that it loads from beginning very fast, the more segments are loaded the slower it becomes. Not sure if it is related.\r\n\r\nPS: same upgrade on another server with much smaller set of metrics -- it started fine.","Can you take a go routine dump when it is 'stuck' ? (  `http:\/\/<url>\/debug\/pprof\/goroutine?debug=2` ) ","Hi @akamensky , while planing an upgrade to 2.25 I found this issue. A couple of my prometheus servers have also large amount of metrics and historical data so wanted to check if you finally fixed it.\r\nThanks"],"labels":["kind\/more-info-needed"]},{"title":"Remote read: The remote endpoint is down, causing the overall query to use timeout blocking","body":"  **What did you do?**\r\n   Remote read is configured for rules evaluation and queries.\r\n\r\n  **What did you expect to see?**\r\n    Once the remote enpoint is unavailable once, the endpoint will be marked as unavailable and ignored within a certain period of time\r\n  **What did you see instead? Under which circumstances?**\r\n   When a remote endpoint is unavailable, it will increase the time consumption for subsequent queries and rules evaluation.\r\n  **Environment**\r\n\r\n  * Prometheus version:\r\n\r\n      2.22.0\r\n\r\n  * Prometheus configuration file:\r\n  ```\r\nremote_read:\r\n  - url: http:\/\/10.110.19.80:9094\/api\/v1\/read\r\n    remote_timeout: 15s\r\n    read_recent: true\r\n  - url: http:\/\/prometheus:9090\/api\/v1\/read\r\n    remote_timeout: 15s\r\n    read_recent: true\r\n  - url: http:\/\/10.110.64.58:9090\/api\/v1\/read\r\n    remote_timeout: 15s\r\n    read_recent: true\r\n  ```\r\n\r\n\r\n\r\n  * Logs:\r\n  ```\r\n  web ui log:\r\n    remote_read: error sending request: Post \"http:\/\/10.110.19.80:9094\/api\/v1\/read\": context deadline exceeded\r\n\r\n  ```\r\n","comments":["If `read_recent` is true, then ignoring even if storage is down might be wrong.","I am not sure that it makes sense to mark remote reads as 'offline' or 'defunct', because you would want your prometheus to catch up as fast as possible when it comes back up. I think a better approach would be to lower your timeout.","I think the mark is useful. It can be used to judge whether the endpoint is online and available when performing remote read to avoid an endpoint being unavailable and all queries will wait for timeout.","I tried to add a mark to the read client, the effect is still significant, at least it will only affect one query within the expiration time.","`\/\/ Client allows reading and writing from\/to a remote HTTP endpoint.\r\n\r\ntype Client struct {\r\n\tremoteName string \/\/ Used to differentiate clients in metrics.\r\n\turl        *config_util.URL\r\n\tClient     *http.Client\r\n\ttimeout    time.Duration\r\n\table       bool       \/\/ able mark\r\n\r\n\treadQueries         prometheus.Gauge\r\n\treadQueriesTotal    *prometheus.CounterVec\r\n\treadQueriesDuration prometheus.Observer\r\n\texpire              time.Time    \/\/ expire time\r\n}`","`\/\/ Read reads from a remote endpoint.\r\nfunc (c *Client) Read(ctx context.Context, query *prompb.Query) (*prompb.QueryResult, error) {\r\n\tif !c.able {\r\n\t\tif !time.Now().After(c.expire) {\r\n\t\t\treturn nil, fmt.Errorf(\"endpoint[%s] is unavailable\", c.url.String())\r\n\t\t}\r\n\t}\r\n\toldAbleState := c.able\r\n......\r\n\tstart := time.Now()\r\n\thttpResp, err := c.Client.Do(httpReq)\r\n\tif err != nil {\r\n\t\tc.expire = time.Now().Add(1 * time.Minute)\r\n\t\tc.able = false\r\n\t\tc.readQueryAble.Set(0)\r\n\t\treturn nil, errors.Wrap(err, \"error sending request\")\r\n\t}\r\n\tif !oldAbleState {\r\n\t\tc.able = true\r\n\t\tc.readQueryAble.Set(1)\r\n\t}\r\n.....\r\n}`","I don't think that timeouts are significant to know if a remote read is available. It is very well possible that you reach a timeout if you ask for a query which is too expansive. It could also be that for some queries, the data is not available at the remote read (which often would be distributed systems).\r\n\r\nTherefore, timeouts are not a good way to know if a remote read is available or not, and blocking all requests to remote endpoints would be unwise.","Ping @csmarchbanks @cstyan @bwplotka as remote storage maintainers.","Thank you for your request! I do not think a naive marker implementation such as the example where a single timeout\/5xx error would cause all requests to fail for a minute would work well in practice. Transient failures happen, and having a minute of degraded responses for one 5xx seems extreme and not the behavior I would expect. \r\n\r\nHaving a remote client health check endpoints in the background to mark them as unhealthy after several subsequent failures might work, I am not sure the complexity is worthwhile however. It might be easier to just run envoy or another proxy with built in health checking and circuit breaking features.","For this issue, if we have a background routine that does a periodic scan of all remote read endpoints, by evaluating with a light promql expression, like `1` and then wait for the actual remote-read timeout to occur. If the timeout occurs, mark that DOWN otherwise if it succeeds within the timeout, mark as UP. Meanwhile, the PromQL evaluation from secondary storages will happen only on UP marked remote-read endpoints, so that timeout is never affected, unless the expression is heavy enough.\n\nDoes that sound like a good approach?","Will be happy to work on this if the approach is good.","This would add a lot of complexity, and we also guarantee to always send the external labels as selectors for remote reads, so we can not do simple queries. Additionally I expect most queries not needing to go to remote storage because in happy paths read_recent should be false.\r\n\r\nI think the behavior of Prometheus is simple and correct, as I explained above, and that this issue is not actionable."],"labels":["priority\/Pmaybe","component\/remote storage"]},{"title":"Clarification on `(*tsdb.DB).ChunkQuerier` and populated chunks.","body":"Hello, I'm looking for clarification about `(*tsdb.DB).ChunkQuerier`. When iterating over series and chunks returned by `(*tsdb.DB).ChunkQuerier`, I'm using `chunks.Iterator`, which has following comment for its `At()` method:\r\n\r\n```\r\n\/\/ Iterator iterates over the chunk of a time series.\r\ntype Iterator interface {\r\n\t\/\/ At returns the current meta.\r\n\t\/\/ It depends on implementation if the chunk is populated or not.\r\n\tAt() Meta\r\n...\r\n```\r\n\r\n(https:\/\/github.com\/prometheus\/prometheus\/blob\/77e648534e2f716cc3cf090eadf009d4c37d0dbf\/tsdb\/chunks\/chunks.go#L75)\r\n\r\nMy question is if it is possible for returned chunks to be unpopulated, when using `(*tsdb.DB).ChunkQuerier`. If so, how can I get the relevant chunk from `*tsdb.DB` based on returned `chunks.Meta`? Given that `Meta` doesn't say which block (or Head) it is coming from, and only contains `Ref`, that doesn't seem to be possible.\r\n\r\nThanks for answer.","comments":["I have moved the issue to prometheus.\r\n\r\ncc @codesome @bwplotka ","> I have moved the issue to prometheus.\r\n\r\nOh, thanks :) I looked at AM PR before and opened this at wrong place. \ud83e\udd26 ","When the querier is on > 1 blocks, I don't see an easy way to only have the `Ref` since the definition of the `Ref` only contains the chunk files and not block info (and cant?). I think it should be the populated chunk always. @pstibrany by any chance have you come across any implementation that returns `Ref`? I am happy to change the comments on this if not.","> When the querier is on > 1 blocks, I don't see an easy way to only have the `Ref` since the definition of the `Ref` only contains the chunk files and not block info (and cant?). I think it should be the populated chunk always. @pstibrany by any chance have you come across any implementation that returns `Ref`? I am happy to change the comments on this if not.\r\n\r\nI haven't come across such implementation yet, but I suspect that I may eventually, given the comment :)\r\n","Considering the multiple block scenario, I would be happy to change the comment then :) WDYT @bwplotka?"],"labels":["kind\/enhancement","priority\/P3","component\/tsdb"]},{"title":"Generally enable reading secrets from files","body":"For a bunch of secrets in the config file, we already allow to alternatively read it from a file, e.g. if there is a `password` field, there is also a `password_file` field. Only one of those may be used.\r\n\r\nHowever, we do not offer this behavior consistently. Occasionally, we get feature requests to allow it for particular fields. This issue proposes to just consistently offer the feature for all secrets in general (exceptions might be possible or even required in particular cases, but those would then be exceptions and not just the normal case).\r\n\r\nSee also [this discussion](https:\/\/groups.google.com\/g\/prometheus-developers\/c\/10duXKRNOuc\/m\/PZnyLy6xBAAJ).\r\n\r\nNote that the semantics generally is to read the secret from the file for each new request (so that password rotation works seamlessly). Exceptions need to be documented clearly (but ideally, we would avoid those entirely). Cf. a previous issue about this: #6140\r\n\r\nAlso note that for consistency with the already quite frequent existing usage, we won't go for a new YAML construct like \r\n```Yaml\r\nsecret:\r\n  type: file\r\n  file: \"\/etc\/my-secrets\/...\"\r\n```\r\n\r\nThis could be considered for a future release, though.","comments":["@mem wants to work on this, at least on a few secrets, so assigning to him.","For Prometheus common\r\n----------------------------------\r\n\r\nThese are part of [TLSConfig](https:\/\/github.com\/prometheus\/common\/blob\/47ee79a16821e03c31ca741659d4f7919aed91c1\/config\/http_config.go#L436):\r\n\r\n- [ ] CAFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n- [ ] CertFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n- [ ] KeyFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n\r\nThis is part of [HTTPClientConfig](https:\/\/github.com\/prometheus\/common\/blob\/47ee79a16821e03c31ca741659d4f7919aed91c1\/config\/http_config.go#L104):\r\n\r\n- [x] BearerTokenFile, from github.com\/prometheus\/common\/config\/http_config.go, this is already handled in the desired way,  as there's another entry BearerToken\r\n\r\nThis is part of BasicAuth:\r\n\r\n- [x] PasswordFile, from github.com\/prometheus\/common\/config\/http_config.go, this is already handled in the desired way with the entry Password.\r\n\r\nFor Prometheus\r\n---------------------\r\n\r\nThis is part of [SDConfig](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/marathon\/marathon.go#L73)\r\n\r\n- [x] AuthTokenFile, already handled in the desired way with AuthToken.\r\n\r\nFor exporter-toolkit\r\n-------------------------\r\n\r\nThis is part of [TLSStruct](https:\/\/github.com\/prometheus\/exporter-toolkit\/blob\/master\/web\/tls_config.go#L42)\r\n\r\n- [ ] TLSCertPath\r\n- [ ] TLSKeyPath\r\n- [ ] ClientCAs","\r\n\ufffc CAFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n\ufffc CertFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n\ufffc KeyFile, from github.com\/prometheus\/common\/config\/http_config.go\r\n\r\nAre already files\r\n\r\n\r\n\ufffc TLSCertPath\r\n\ufffc TLSKeyPath\r\n\ufffc ClientCAs\r\n\r\n\r\nAre already files\r\n\r\n","I think the main goal here is to be able to read secrets from external files, not to load certificates from primary file.","The agreement in the linked discussion is to make this consistent, so those options which only offer the possibility of reading a file should have the inline form added.\r\n\r\nI was actually looking for the options where the user can only specify inline secrets, and I haven't found any, but I have only looked at the mentioned packages.","here are the secrets I think we can't read from files:\r\n\r\n- azure sd's client secret\r\n- consul token\r\n- aws secret key\r\n- openstack password\r\n- openstak application_credential_secret\r\n","ah, thanks!","Prometheus\r\n----------------\r\n\r\n- [ ] [Azure SD's client secret](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/azure\/azure.go#L79)\r\n- [x] [Consul SD's token](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/consul\/consul.go#L112)\r\n- [x] [Consul SD's password](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/consul\/consul.go#L117)\r\n- [ ] [EC2 SD's secret key](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/ec2\/ec2.go#L85)\r\n- [x] [Marathon SD's auth token](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/marathon\/marathon.go#L76)\r\n- [ ] [OpenStack SD's password](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/openstack\/openstack.go#L51)\r\n- [ ] [OpenStack SD's application credential secret](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/discovery\/openstack\/openstack.go#L58)","Pretty sure #8926 does both the token and password for Consul.","Yes","If @mem isn't working on this anymore I can pick it up.","Sorry about the radio silence, I'm picking this up again. Thanks for the reminder!","> I think the main goal here is to be able to read secrets from external files, not to load certificates from primary file.\r\n\r\nI stand by this. Certificates are files almost everywhere, it is not natural to load them inline.","Note: The discussion is now also happening in https:\/\/github.com\/prometheus\/common\/pull\/318 , which is @mem's implementation of what's suggested here for TLS.\r\n\r\nMy thoughts: I totally get @mem's argument about consistency both ways, but I have to admit, when I emphasized consistency in the discussion on the mailing list, I was only thinking about fields with inline secrets so far, i.e. that we should rather give all of them the option to provide the secret in a file than just selected few. I honestly had not thought about secrets that we used to _only_ provide as a file and never inline.\r\n\r\nSeeing the implementation in https:\/\/github.com\/prometheus\/common\/pull\/318 and @roidelapluie's argument that is very uncommon to provide TLS certificates inline and that users never requested the latter.\r\n\r\nSo perhaps we can formulate the consistency somewhat weaker: We consistently want to enable providing secrets in files. However, there is no requirement for an option to represent a secret inline in the config file.\r\nThe latter comes in handy for short secrets in text format (like passwords), and it's an encouraged feature in those cases, but it's rarely desired to put long and\/or binary-encoded secrets (like TLS certificates) inline into the config file.","OK, I can understand that the case for inline CA certificates is weak.\r\n\r\nFor client keys on the other hand the use case is a large blackbox_exporter configuration where the secrets are stored in a central location and the BBE configuration is generated, not written by hand, so it's actually simpler to manage a single configuration file instead of a configuration file plus a number of different certificates and their keys. It's neither more or less secure (if the configuration file with the embedded secrets is in the filesystem, that is just as secure or insecure has having the configuration file plus the multiple certificates in the filesystem), it's just simpler. I can understand the simplicity of having a file be reloaded over and over again in case it ever changes, every time a scrape job runs, \r\n\r\n(I mention blackbox_exporter because it's the example where it's most obvious to me how this can be used, but the same thing applies to Prometheus with scrape jobs that require different client authentication secrets, which might sound artificial, but it was actually the case at `$PAST_JOB` in a slightly different context)\r\n\r\nI'll make the other changes I mentioned before.","I guess if there is an actual need to inline keys (or even certs), then we can just offer it. My impression was that @roidelapluie was opposed to offering the feature merely for consistency reasons without anyone having ever asked for it.","Yes I am opposed to it. Secrets are multi line and just the standard is to keep them away in separated files. Multi line in YAML is something I really want to avoid.","Then let's just separate things and really just enable reading secrets from files as part of this issue.\r\n\r\nInlining secrets that can only be read from files right now is then a separate topic to discuss later.","Also, related proposal from @TheSpiritXIII https:\/\/groups.google.com\/g\/prometheus-developers\/c\/WKOej_pnhXg"],"labels":["priority\/P2","component\/config","kind\/feature"]},{"title":"Empty target groups are not shown in the react UI","body":"<!--\r\n\r\n    Please do *NOT* ask usage questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use:\r\n    https:\/\/groups.google.com\/forum\/#!forum\/prometheus-users. If\r\n    you are unsure whether you hit a bug, search and ask in the\r\n    mailing list first.\r\n\r\n    You can find more information at: https:\/\/prometheus.io\/community\/\r\n\r\n-->\r\n\r\n**What did you do?**\r\nCreated an empty target group using prometheus-operator's servicemonitor. Tested on v2.24.0 and v2.25.0.\r\n\r\n**What did you expect to see?**\r\nA target group of `default\/monitoring-kube-prometheus-blackbox-exporter\/0 (0\/0 up)` like in the classic UI.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/33333571\/108629098-6366da00-742c-11eb-9487-42f06bac5b0f.png)\r\n\r\n**What did you see instead? Under which circumstances?**\r\n`default\/monitoring-kube-prometheus-blackbox-exporter\/0 (0\/0 up)` is missing in the react UI.\r\n![image](https:\/\/user-images.githubusercontent.com\/33333571\/108629228-fbfd5a00-742c-11eb-8959-cba36cfcbcb3.png)\r\n\r\n**Environment**\r\n\r\n* System information:\r\n\r\n\tinsert output of `uname -srm` here\r\n\r\n* Prometheus version:\r\n\r\n\tinsert output of `prometheus --version` here\r\n\r\n```\r\n\/prometheus $ prometheus --version\r\nprometheus, version 2.25.0 (branch: HEAD, revision: a6be548dbc17780d562a39c0e4bd0bd4c00ad6e2)\r\n  build user:       root@615f028225c9\r\n  build date:       20210217-14:17:24\r\n  go version:       go1.15.8\r\n  platform:         linux\/amd64\r\n```\r\n\r\n* Alertmanager version:\r\n\r\n\tinsert output of `alertmanager --version` here (if relevant to the issue)\r\n\r\n* Prometheus configuration file:\r\n```\r\ninsert configuration here\r\n```\r\n\r\n* Alertmanager configuration file:\r\n```\r\ninsert configuration here (if relevant to the issue)\r\n```\r\n\r\n\r\n* Logs:\r\n```\r\ninsert Prometheus and Alertmanager logs relevant to the issue here\r\n```\r\n","comments":["Thanks! cc @juliusv for awareness","The new UI uses the HTTP API (`\/api\/v1\/targets`, e.g. https:\/\/demo.promlabs.com\/api\/v1\/targets) to get information about targets, but while that API endpoint tells you for each target in which scrape pool it is, it does not include any information about scrape pools that don't have any targets. So either we just leave it as it is in the new UI, or we would have to change the API to provide information about empty scrape pools as well.","We have looked at that during our bug scrub and it seems that indeed we don't return any information about scrape pools, but we would accept a pull request that includes the list of scrape pools and includes empty scrape pools in the UI.\r\nAdding a field in the API was not considered a breaking change in the past.","Hi @roidelapluie I've been looking at issues for a first contribution. This looks pretty reasonable to me - can you assign me in accordance with [\u00ab steps to contribute \u00bb](https:\/\/github.com\/prometheus\/prometheus\/blob\/main\/CONTRIBUTING.md#steps-to-contribute) ?\r\n\r\nI have just one question on functionality.\r\n\r\n> Adding a field in the API was not considered a breaking change in the past.\r\n\r\nThe default call to `\/targets` has the `active` query param set to true. Are you saying that this change should add a query param for the API route that would enable users to show empty pools (i.e. an empty pool is not considered active)? If so, any preference for a name - `empty`, `emptyPools`, `showEmptyPools`, ... ?\r\n\r\nThanks!","@conorevans So `active` \/ `dropped` refers to individual targets that were kept or dropped during relabeling. If all targets for a scrape pool were dropped *or* there were 0 targets to begin with in a pool, then the old UI still shows the empty pool, while the new one doesn't. The tricky bit is that pools are only contained in the API response (e.g. https:\/\/demo.promlabs.com\/api\/v1\/targets) as part of a target - if there's no target (potentially not even a dropped one), there's no info on the pool either.\r\n\r\nIt's not ideal API design in retrospect to only indirectly convey information about target pools like this vs. providing an explicit list of all target pools somehow. Given the situation we are in now we could:\r\n\r\na) Add a completely new top level field (parallel to `activeTargets` and `droppedTargets`) that is just an array of scrape pools, independent of whether they have any targets or not.\r\nb) Add a completely new API endpoint for target pools, but then we'd have to make two API calls just to display the page.\r\nc) Or add a new API endpoint that gives all info in one go, but groups by scrape pools on the top level, with target info nested underneath each scrape pool. (That is the way the info is passed into the server-side HTML template in the classic UI).\r\n\r\na) or c) seem like the best options to me, what do others think?\r\n\r\nIn any case, it'll require some deeper Go-level changes to the way info is passed from the scraping layer to the web API, as currently the Go-level interface only allows for retrieving active \/ dropped targets, not (potentially empty) scrape pools: https:\/\/github.com\/prometheus\/prometheus\/blob\/431ea75a11ca165dad9dd5d629b3cf975f4c186b\/web\/api\/v1\/api.go#L777-L781","Sorry, for the Go interface I meant to link https:\/\/github.com\/prometheus\/prometheus\/blob\/431ea75a11ca165dad9dd5d629b3cf975f4c186b\/web\/api\/v1\/api.go#L91-L95, which actually does group by scrape pool on the first level, so that part is good."],"labels":["kind\/bug","component\/ui","priority\/P3"]},{"title":"Relabeling default regex does not match newlines","body":"The default regex for relabeling `(.*)` does not support newlines `(?s)(.*)`.\r\n\r\nWe can consider this a bug and fix it in the next minor release, or fix this in prometheus 3.","comments":["it would be interesting that why would need new lines in a label value...","The intention is that the default regex catches the new lines, but because of the widespread use of relabeling, we would prefer to delay this until prometheus 3.x.","And once this is fixed, we can allow the `regex` field on relabeling in mode `hashmod`."],"labels":["kind\/bug","component\/config","priority\/P3"]},{"title":"Flaky TestIntern_MultiRef_Concurrent","body":"I am documented a test that failed in a circleci Pipeline: https:\/\/app.circleci.com\/pipelines\/github\/prometheus\/prometheus\/10954\/workflows\/d296c094-e783-4177-9406-b2ba804fb491\/jobs\/48029\r\n\r\nThe error is:\r\n\r\n```\r\n=== RUN   TestIntern_MultiRef_Concurrent\r\n    intern_test.go:89: \r\n        \tError Trace:\tintern_test.go:89\r\n        \tError:      \tNot equal: \r\n        \t            \texpected: true\r\n        \t            \tactual  : false\r\n        \tTest:       \tTestIntern_MultiRef_Concurrent\r\n--- FAIL: TestIntern_MultiRef_Concurrent (0.00s)\r\n```\r\n\r\nI don't know what happened and if it is something that can bite us in a production setup.","comments":["hmm i'm getting a 404 when i click on the link"],"labels":["kind\/bug","component\/remote storage","priority\/P3"]},{"title":"not cleaning up checkpoint tmp files ","body":"**What did you do?**\r\nWhen restarting prometheus, there are many useless tmp files that have been ignored and always exist.\r\n\r\n**What did you expect to see?**\r\nPrometheus automatically cleans up junk files \r\n\r\n**What did you see instead? Under which circumstances?**\r\n     many tmp files\r\n     `rw-r--r-- 1 nobody nogroup 121M Feb 18 14:46 00106268\r\n-rw-r--r-- 1 nobody nogroup 123M Feb 18 14:49 00106269\r\n-rw-r--r-- 1 nobody nogroup 121M Feb 18 14:51 00106270\r\n-rw-r--r-- 1 nobody nogroup 110M Feb 18 14:53 00106271\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 14  2020 checkpoint.024541.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 15  2020 checkpoint.025061.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 15  2020 checkpoint.025450.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 16  2020 checkpoint.025567.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 17  2020 checkpoint.026366.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 18  2020 checkpoint.026978.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 18  2020 checkpoint.027046.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 19  2020 checkpoint.027500.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 20 13:03 checkpoint.027936.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 21 01:03 checkpoint.028196.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 21 07:03 checkpoint.028316.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 22 01:02 checkpoint.028723.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 22 03:10 checkpoint.028738.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 24 01:03 checkpoint.029770.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 24 07:03 checkpoint.029887.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 25 01:03 checkpoint.030288.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 26 13:03 checkpoint.031073.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 27 01:02 checkpoint.031331.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 03:03 checkpoint.031901.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 05:03 checkpoint.031916.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 11:09 checkpoint.032047.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 13:03 checkpoint.032060.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 15:03 checkpoint.032073.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 17:11 checkpoint.032087.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 19:11 checkpoint.032097.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 21:03 checkpoint.032109.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 23:04 checkpoint.032119.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 01:06 checkpoint.032131.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 03:06 checkpoint.032139.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 05:03 checkpoint.032151.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 07:03 checkpoint.032164.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 09:21 checkpoint.032177.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Feb 18 13:04 checkpoint.106132\r\nroot@mgt07:\/monitor\/prometheus\/wal# date\r\nThu Feb 18 14:55:10 CST 2021`\r\n\r\n**Environment**\r\n\r\n\r\n* System information:\r\n\r\n\tLinux 5.0.0-29-generic x86_64\r\n\r\n* Prometheus version:\r\n\r\n  prometheus, version 2.15.2 (branch: master, revision: e7e1a937a55f009e6fe9224c2ee5064abe85e769)\r\n  build user:       root@master01\r\n  build date:       20200415-01:14:21\r\n  go version:       go1.13.8\r\n\r\n* Logs:\r\n```\r\n  nothing\r\n```\r\n","comments":["tmp file:\r\n\r\n-rw-r--r-- 1 nobody nogroup 121M Feb 18 14:46 00106268\r\n-rw-r--r-- 1 nobody nogroup 123M Feb 18 14:49 00106269\r\n-rw-r--r-- 1 nobody nogroup 121M Feb 18 14:51 00106270\r\n-rw-r--r-- 1 nobody nogroup 110M Feb 18 14:53 00106271\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 14  2020 checkpoint.024541.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 15  2020 checkpoint.025061.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 15  2020 checkpoint.025450.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 16  2020 checkpoint.025567.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 17  2020 checkpoint.026366.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 18  2020 checkpoint.026978.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 18  2020 checkpoint.027046.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 19  2020 checkpoint.027500.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 20 13:03 checkpoint.027936.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 21 01:03 checkpoint.028196.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 21 07:03 checkpoint.028316.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 22 01:02 checkpoint.028723.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 22 03:10 checkpoint.028738.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 24 01:03 checkpoint.029770.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 24 07:03 checkpoint.029887.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 25 01:03 checkpoint.030288.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 26 13:03 checkpoint.031073.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 27 01:02 checkpoint.031331.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 03:03 checkpoint.031901.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 05:03 checkpoint.031916.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 11:09 checkpoint.032047.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 13:03 checkpoint.032060.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 15:03 checkpoint.032073.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 17:11 checkpoint.032087.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 19:11 checkpoint.032097.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 21:03 checkpoint.032109.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 28 23:04 checkpoint.032119.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 01:06 checkpoint.032131.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 03:06 checkpoint.032139.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 05:03 checkpoint.032151.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 07:03 checkpoint.032164.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Aug 29 09:21 checkpoint.032177.tmp\r\ndrwxr-xr-x 2 nobody nogroup 4.0K Feb 18 13:04 checkpoint.106132\r\nroot@mgt07:\/monitor\/prometheus\/wal# date\r\nThu Feb 18 14:55:10 CST 2021","Thank you for this. a lot of bugs around this have been fixed lately. Could you try Prometheus 2.25.0 and see if the error appears again?","We have looked at this during the bug scrub. Did you have any chance to upgrade?\r\n\r\nAlso, which filesystem are you using?\r\nYou can see that by the log line `fs_type=`, e.g. `level=info ts=2021-03-01T15:44:47.192Z caller=main.go:799 fs_type=EXT4_SUPER_MAGIC`.","level=info ts=2021-03-05T08:31:38.866Z caller=main.go:684 fs_type=EXT4_SUPER_MAGIC","Upgrading should get rid of those files, which were created in August."],"labels":["kind\/more-info-needed"]},{"title":"Consider shipping multiple go modules.","body":"<!--\r\n\r\n    Please do *NOT* ask usage questions in Github issues.\r\n\r\n    If your issue is not a feature request or bug report use:\r\n    https:\/\/groups.google.com\/forum\/#!forum\/prometheus-users. If\r\n    you are unsure whether you hit a bug, search and ask in the\r\n    mailing list first.\r\n\r\n    You can find more information at: https:\/\/prometheus.io\/community\/\r\n\r\n-->\r\n## Proposal\r\n\r\nShip multiple go modules in this repository. For example, `github.com\/prometheus\/prometheus\/promql` (or simply github.com\/prometheus\/promql) could be a module, as could `github.com\/prometheus\/discovery`. Many sub-directories in the project don't have direct dependencies on each other, but depend on `github.com\/prometheus\/prometheus\/common` and `github.com\/prometheus\/prometheus\/util`.\r\n\r\n**Use case. Why is this important?**\r\n\r\nMy primary issue is that the whole prometheus module lists a requirement for `k8s.io\/client-go v0.20.0`, but this is only needed by the `discovery\/` path. I needn't use the kubernetes discovery in my existing project, but I *would* like to use a recent version of the promql package. I do use the kubernetes API in other parts of my project, and I need it pinned to an earlier version of the code than prometheus requires. MVS always picks the version required by the prometheus module. I therefore can't build my project in a way that's compatible with my runtime while including the prometheus\/prometheus module.\r\n\r\nI can work around this by copying the part of the source tree that I need into its own directory, or shadowing the repo, deleting all the code I don't want and updating the go.mod file, but it's not an ideal long-term solution.\r\n","comments":["Thank you for your interest. While having multiple modules would slow down prometheus development and raise the barrier to entry, this is something that will be discussed in our developer summit, in the coming months.\r\n\r\nFor your specific dependency conflict, it should be fixed in the coming release of go (go 1.17): https:\/\/github.com\/golang\/go\/issues\/36460\r\nCan you hold a few month more with your current workflow? Thanks.\r\n\r\n\r\n","(note: we also experimented with https:\/\/github.com\/modularise but it seems not to work anymore)","Thanks - I think a discussion at the developer summit sounds like a great way to consider the issue. It's certainly not a slam-dunk do-it-now proposal. I think our workaround will be sufficient for my team for now.\r\n\r\nI'm finding this design document a little hard to follow: https:\/\/go.googlesource.com\/proposal\/+\/master\/design\/36460-lazy-module-loading.md\r\nBut I do believe that as long as we don't import the discovery package, we won't have the prometheus transitive dependency on k8s and it won't contribute to the k8s version selected by us.","Yes that is my understanding.","Last week's dev summit didn't get far enough through the agenda to cover this, but I expect it to be handled soon at one of the upcoming summits (happen monthly at the moment).\r\n\r\nRelevant previous discussions:\r\nhttps:\/\/groups.google.com\/g\/prometheus-developers\/c\/F1Vp0rLk3TQ\/m\/TyF2WxlkBgAJ\r\nhttps:\/\/groups.google.com\/g\/prometheus-developers\/c\/DY88o6yOg28\/m\/Bzc7E0x9BgAJ"],"labels":["priority\/Pmaybe"]},{"title":"Provide JSON Schemas for Config YAML","body":"## Proposal\r\nProvide a [json schema](https:\/\/json-schema.org\/) for the config yaml (ideally as a released\/documentation artifact). The rules yaml would be a nice to have, but its format is not as complex so is not a priority.\r\n\r\n### Use Case\r\nThe usecase is that our prometheus config has gotten complex enough that hand editing yaml is no longer sustainable (think 8000+ lines of yaml). There are a few yaml generation tools we have been using for Kubernetes resources that offer strongly typed interfaces. I.e. https:\/\/github.com\/awslabs\/cdk8s and https:\/\/github.com\/dhall-lang\/dhall-haskell (among many others). The idea here is that if one has a json schema, the correct types can be imported into these languages from json schema as interfaces to be written against. This allows all yaml management to be done in a centralized\/deduplicated\/abstract way for K8S, prometheus, etc\r\n","comments":["I think this could run on PR merge using `go run`, and compare its output to the published file to determine if a PR should be opened, as `repo_sync` does."],"labels":["component\/config","not-as-easy-as-it-looks","priority\/P3","component\/documentation","kind\/feature"]},{"title":"Need selector functions for data science, such as being able to select data from multiple sources","body":"## Proposal\r\nWhen building data science tools, an analyst needs to have the ability to merge multiple datasets together based on an arbitrary selector.  Currently, Prometheus does not have the ability to combine data sources in any way.  Enter my pull request, #8457. These functions create the ability to merge datasets together, both in a selector value and also in the temporal domain, over time.  I have a working pull request ready for your review and acceptance.","comments":["thank you, could you share practical exemples and use cases for this, so we can better evaluate the need?","Here is an example for each function:\r\nStep\/rect: if you want to merge two or more data sources together, for example, analog sensors a-to-d converters are only valid within a specific range.  Other sensors can accommodate a larger window with a loss of resolution.  Being able to window the metric and add them together while keeping the original data as it came from the sensor is both valuable for historical purposes and to validate results.\r\n\r\nTimestamp_range: is useful for example, if one has an upcoming maintenance window, planned ahead of time between time \"a\" and time \"b\" and you would like to graph this maintenance window with linear regression.  This window can happen any time in the future, so a label is valuable on the metric \"maintenance_window{start=a,stop=b}\" to set the start and stop time automatically.  This also allows the ability to estimate values such as back log in a factory when turning off a component at a specific time.  ","Thank you. For the `step` and `rect`, you should be able to do this simply with promql.\r\n\r\nFor `timestamp_range`, it is possible with promql, but I would recommend to simply change the start and end parameter of the query API.\r\n\r\n`step(v)` can be written as `v > bool 0`\r\n\r\n`rect(v, 5, 10)` can be written as `v > bool 5 * v <= bool 10`\r\n\r\n`timestamp_range` can be written as `v and on () vector(time()) >  1612811853 < 1612817853`","I appreciate the bool syntax.  I was using the > syntax without the bool and was unable to add them as the non-match was missing data.  I'll try this, thank you.\r\n\r\nIn terms of the time() vector, I considered doing this, but the missing key information here is the start and stop time.  There are many in a week, sometimes multiple in a day.  This comes from external data source and hard coding the values is not preferred.","I think that I know what you mean by maintenance window. You could simply emit the metric 'maintenance_window' during the maintenance windows? and the use v and on () maintenance_window","The challenge here is the maintenance window is in the future.  Say today is Monday and the window is scheduled for Thursday. \r\n\r\nI am thinking something like this:\r\n\r\n`\r\nvector(time()+86400 * 5) > bool last_over_time(maintenance_window_start[5d]) * vector(time()+86400 * 5) < bool last_over_time(maintenance_window_stop[5d])\r\n`\r\n\r\nand graph this over 5 days...\r\n\r\nwhen one has values like\r\nmaintenance_window_start{id=\"maint1234\"} 1612811853 \r\nmaintenance_window_stop{id=\"maint1234\"} 1612817853\r\n\r\nThe maintenance window can shift, so this way the graph can account for any changes.","Have you looked into region events in grafana instead? https:\/\/grafana.com\/docs\/grafana\/latest\/dashboards\/annotations\/","Thank you for the reply again, I really appreciate your time and attention into this.\r\n\r\nI use them and really like them, especially with the grafana alerts.  The challenge still remains in that I cannot do an estimation of backlog without a boolean.\r\n\r\nI'm thinking that I could trick grafana into giving me a \"graph\" of the next 5 days if I were to build a last_over_time function and then do an aggrOverTime selection of 5 days to ensure the latest value of the maintenance window is selected.  What do you think of this?\r\n\r\npromql\/functions.go:\r\n```\r\n\/\/ === nanskip_last_over_time(Matrix ValueTypeMatrix) Vector ===\r\nfunc funcNaNSkipLastOverTime(ev *evaluator, args Expressions) Value {\r\n  return aggrOverTime(ev, args, func(values []Point) float64 {\r\n    for i := len(values) - 1; i >= 0; i-- {\r\n      if !math.IsNaN(values[i]) {\r\n        return values[i]\r\n      }\r\n    }\r\n    return math.NaN()\r\n  })\r\n}\r\n```\r\n","Orthogonal from the specific use case, we are considering to allow more syntactic sugar, i.e. to make some use cases easier even if there's equivalent, but more complex to write, functionality in PromQL today.","> Thank you for the reply again, I really appreciate your time and attention into this.\r\n\r\nThere are 2 functions that I would like, it is last_over_time and `sgn` (https:\/\/en.wikipedia.org\/wiki\/Sign_function).\r\n\r\nIf we do a last_over_time, it should not ignore NaNs.","Agreed, last_over_time is a great thing.  I have it in a stack here that could be pulled and agree that nans should not be skipped when there is nan data.  I can start putting together a pull request.\r\n\r\nYes!  I missed that, SIGN.  It can be implemented in the same as my step function. maybe one could do a  (v > bool 0) - (v < bool 0), but going over the vector twice here is twice as expensive in looping over the vector twice.  I'll throw put one together to see what you think.\r\n\r\nTo the consideration that sometimes one needs to ignore NaN values:  I've compiled the entire suite of NaNSkip_ functions for all of the _over_time operators, such as nanskip_max_over_time and nanskip_min_over_time etc... The reason being is that I have to have a numerical value, even when I know there are values that cannot be included.  In Matlab\/Comsol this method is max(A,[],'omitnan') and python np.nanmax().  So having this ability really helps.\r\n\r\nAll the best!","@RichiH @roidelapluie : Could you take a look at the set of functions I pushed?  I am curious to hear what you think.  \r\n\r\nNote:  I added a vector function omit_nan which should help deal with the NaN issue including the one here:\r\nhttps:\/\/stackoverflow.com\/questions\/53430836\/prometheus-sum-one-nan-value-result-into-nan-how-to-avoid-it\r\n\r\n---\r\nAside:   I am getting a build error now from CircleCI\r\n\r\n```\r\nAn error occurred in .circleci\/config.yml\r\nTo see the YAML syntax errors, click the button below:\r\n\r\n#!\/bin\/sh -eo pipefail\r\n# ERROR IN CONFIG FILE:\r\n# [#] required key [jobs] not found\r\n# \r\n# -------\r\n# Warning: This configuration was auto-generated to show you the message above.\r\n# Don't rerun this job. Rerunning will have no effect.\r\nfalse\r\n\r\nExited with code exit status 1\r\nCircleCI received exit code 1\r\n```","Tanks again for this. The error you mention will be taken care of when https:\/\/github.com\/prometheus\/prometheus\/pull\/8483 will be merged.\r\n\r\n\r\nWhere is `ord` coming from?\r\n\r\nRect seems like it can be easily dealt with with bool\r\n\r\nSign should be called `sgn`.\r\n\r\nStep is simply > bool 0 and should not be a function then if I am not mistaken.\r\n\r\ntimestamp_range should not be part of promql at all in my opinion but handled at the display layer somehow. It does not appear fine to me to parse labels for dates.\r\n\r\nI would need to gather more feedback about the NaN functions, to see if other see that fit in Prometheus.","@roidelapluie ,\r\n\r\nI really appreciate your fast replies.  This helps with momentum.  Thank you for your patience, and please know your words are not being ignored. I have removed the disputed functions and I have written a little of my thought process below.  I would value knowing what your decision process is for knowing what may or may not be a suggestion.\r\n\r\n> Where is ord coming from?\r\n\r\nOrd was my original way to test for infinity conditions.  I have removed it as I proved (to myself) that \"inf == bool inf\" works!\r\n\r\n> Rect seems like it can be easily dealt with with bool\r\n\r\nThis has been removed.  My line of thinking here was one could do two booleans in one function and save looping over the vector twice, only really matters if your vector is really long or many calls, but I can be convinced otherwise. \r\n\r\n> Sign should be called sgn.\r\n\r\nFixed.\r\n\r\n> Step is simply > bool 0 and should not be a function then if I am not mistaken.\r\n\r\nIndeed so, you are right as stated before, so removed.\r\n\r\n> timestamp_range should not be part of promql at all in my opinion but handled at the display layer somehow. It does not appear fine to me to parse labels for dates.\r\n\r\nAgreed, and removed, \r\n\r\nNote, as an alternative I altered the offset function to allow negative values.  This combined with last_over_time-- I can get what I was originally looking for in the timestamp_range function by using two metrics and bool operations.  Here is an example to help illustrate what I mean:\r\n\r\nTime() = 1110\r\n\r\nData:\r\nstart{id=\"a\"} 1112\r\nstop{id=\"a\"} 1113\r\nstart{id=\"b\"} 1115\r\nstop{id=\"b\"} 1116\r\n\r\nquery:\r\n(time() bool > last_over_time(start[5d] offset -5d))\r\n*\r\n(time() bool < last_over_time(stop[5d] offset -5d))\r\n\r\nResult\r\n1112 - 1113 id a is high\r\n1115 - 1116 id b is high\r\n\r\n> I would need to gather more feedback about the NaN functions, to see if other see that fit in Prometheus.\r\n\r\nI look forward to your feedback.  \r\n\r\nPlease consider omit_nan() and is_nan(), as they are very important in dealing with NaN values.  One cannot do NaN == NaN so this is the only chance of testing for these values.","@roidelapluie Here is a write up for background, although I know you already know and understand this.  The inspiration behind the nan_* functions is that a special case is needed to do all of these basic operations.  If one were to first omit_nan() on all the functions, then the set may disappear like it didn't exist, however, one needs to see that the dataset exists.  As no operations can be done against a NaN, it really needs to have the equivalent set of functions so users can choose to include the NaN in operands, or \"fall back\" to a NaN and operate on everything that is a valid value first.","My point is that NaN is a bug of the exporter, not a bug in Prometheus. Can you tell me how you get NaN in the first place?","The exporter is working as expected.  I set a value to NaN when I have a measurement which is not a successful measurement, it serves a placeholder to show an operation took place but the result is invalid.\r\n\r\nTake for example: a ping time for a host which is down.  The \"icmp not reachable\" or \"no arp reply\" is mapped to a NaN value.  This way I know there was an attempt to collect a metric but it was unsuccesful, with these nan functions I can track this validitity along with the valid data in a single metric and do analytics over the dataset.","Thanks for the details.\r\n\r\nPrometheus has a way to deal with this: we use `success` 0 and 1 gauges. That is a pattern that is there accross the board. Then, for the failed measurements values themselves, I recommend to simply not expose them. If you use Prometheus 2.x, you will be able to do so without surprise, considering the staleness handling.\r\n\r\nBased on this, my recommendation is to fix the exported data to match the prometheus data model.\r\n\r\nI am happy to explain this more into details if you have questions :)","Here are more thought on this.\r\n\r\nI understand why you want to expose `NaN` when a measurement can't be done. However, exposing `NaN` from exporters should not happen, as it is not easy to reason about. In the same way, once you have a `NaN`, you can't do anything with that. Comparing to `NaN`, looking for max, etc, does not make sense once you have `NaN` in the data set.\r\n\r\nIn the same way, `NaN == NaN` is not true, as we can't reason about that.\r\n\r\nYou have linked yourself into a stack overflow question: https:\/\/stackoverflow.com\/questions\/53430836\/prometheus-sum-one-nan-value-result-into-nan-how-to-avoid-it\r\nThat question itself turned into a bug report: https:\/\/github.com\/prometheus\/memcached_exporter\/issues\/38 showing that `NaN`s should not be exposed.\r\n\r\nHow to get the same result as you have now then?\r\n\r\nThe pattern we see in other exporters, is to have \"booleans\"-kind of gauges. It is gauges that report `1` or `0`. That would mean basically to have 2 metrics:\r\n\r\n```\r\nmy_measurement_success 1\r\nmy_measurement 432\r\n```\r\nIn that case, you should not expose `my_measurement` when it is not successful, and `my_mesurement_success` should be `0`.\r\n\r\nThat enables you to do: `sum_over_time(my_mesurement_success[1d])\/count_over_time(my_mesurement_success[1d])` to know your \"success\" ratio over the last 24h.\r\n\r\nThat pattern is used in the `haproxy_exporter`, which has `haproxy_up` metric. It is used in the `apache_exporter`, which has `apache_up metric`. It is used in the blackblox_exporter, which has `probe_success` metrics.\r\n\r\n\r\nLooking back at your pull request now:\r\n\r\n- Please remove negative offset (offset -1w) from that specific pull request. That should be managed within its own pull request, and with a feature flag as it changes PromQL semantics. I am happy to have that, and I know other people have implemented it.\r\n- Given the current discussion, I think that we do not need the `NaN` functions at this stage.\r\n- `sgn` is something I would like to have.\r\n- `rect` can be useful too, but it is achievable with `clamp_max(clamp_min(a, 10),20)`. If you feel like we need a shortcut, let's call that `clamp`.\r\n\r\n\r\nOnce we agree with the scope of the pull request, I will start reviewing it :)","I was in the process of figuring out how to phrase my thoughts. so your email proves to be helpful here.  Thank you.\r\n\r\n* I split this up into multiple pull requests as suggested #8487 \r\n* I will remove the NaN implementations.  I fully understand your points and appreciate the thought process that has been put into this.  I admit that I am trying to keep things simple (aka: be lazy) and overload one function to do both the response time and ping success.  Breaking it into two metrics as you suggest is possible, so I will do so.  I like to collapse the two functions into one graph, so I may do something like:\r\n\r\nquery A: my_measurement{server=\"a\"}\r\nquery B: my_measurement_success{server=\"a\"}  mapped to the second Y axis\r\n\r\n* sgn, sounds good, agreed.\r\n* last_over_time is included\r\n* clamp - I can put this together but this was not the goal of what rect() provided.\r\n* rect() or pulse() would look more like this:   https:\/\/en.wikipedia.org\/wiki\/Rectangular_function\r\n\r\n","query A: my_measurement{server=\"a\"}\r\nquery B: my_measurement_success{server=\"a\"}\r\n\r\nOnce the exporter is fixed, you could also do: my_measurement{server=\"a\"} or Nan * my_measurement_success{server=\"a\"} if you want to keep your current graph.\r\n\r\n\r\nNote that for PromQL tests, test tests in testdata should work for most cases, and are preferred compared to tests in golang.","What is you usecase for rect ?That sounds like very specific. If you need need for a hand or queries, that can be achieved easily in plain promql:\r\n\r\nabs(a) == 0.5 or abs(a) < bool 0.5","Note that let's keep clamp away for now then -- clamp_min and max cover this, I don't foresee a need for now to have a simplified form of that.","This is true you could produce the same result with multiple bool operators.  The goal was to keep the bool operators to a minimum by building a combined > and < bool operator in one rect\/pulse function. \r\n\r\n> Step\/rect: if you want to merge two or more data sources together, for example, analog sensors a-to-d converters are only valid within a specific range. Other sensors can accommodate a larger window with a loss of resolution. Being able to window the metric and add them together while keeping the original data as it came from the sensor is both valuable for historical purposes and to validate results.","Since the rect seems symetric around 0, you can leverage abs() to have only one < bool","If you want to retain original data, or use another value than 0.5, that should not be called rect, as I see rect() has well defined definition.","The idea of rect was to allow a defined lower and upper bound... so it would be more general than an abs(v) < bool a","Then we closer to a `clamp(a, 0, 1)` function. Rect seems to really be with fixed values only.","Oooooh, that's perfect!","Note: We host public contributor hours on Mondays, if you want to discuss with us, get help, you can register :)\r\nYou can find the info here: https:\/\/docs.google.com\/document\/d\/1bPIsOssTswA244z5SDlws5Qwzc27hQQ2IukNmkIjmPk\/edit#heading=h.oy8b2jtupl8m"],"labels":["kind\/question","priority\/Pmaybe","component\/promql"]},{"title":"Apply for \"Official\" status for our Docker images","body":"## Proposal\r\n\r\nWe can apply to make our images marked as \"Official\" on Docker hub.\r\n\r\nSee: https:\/\/docs.docker.com\/docker-hub\/official_images\/","comments":["@roidelapluie and I thought we should do this when we looked at this during bug scrub. @SuperQ do you want to own this?","Sure, I'll see if I have time this week to look into it.","@SuperQ any progress on this? I might be able to pick it up if that's helpful :)"],"labels":["priority\/P3"]}]
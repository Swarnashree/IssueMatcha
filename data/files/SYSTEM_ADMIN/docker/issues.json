[{"title":"Disabling the default bridge changes Docker's behavior regarding DOCKER-USER chain","body":"### Description\r\n\r\nWhat I was trying to do is use \"netfilter-persistent\" together with Docker. The logic was that the rules governed by \"netfilter-persistent\" only touched INPUT and OUTPUT chains, and added the DOCKER-USER chain, placed it at the top of the FORWARD chain, and manipulated the rules in the DOCKER-USER chain. Docker would then continue from here, adding its own rules _after_ the jump to DOCKER-USER.\r\n\r\nThe overall intent was to be able to control the forwarded traffic from Docker out to the Internet at large.\r\n\r\nWhat I discovered was that adding `\"bridge\": \"none\"` to `\/etc\/docker\/daemon.json` changes Docker's behavior in a rather unpredictable fashion.\r\n\r\nBy default, and when the setting is not there then Docker creates `docker0` adapter, creates DOCKER-USER, adds jump to it to the top of the FORWARD chain before adding the other chains, isolation stages and all the other stuff. So, all is good. I can manually manipulate DOCKER-USER to enforce more stricter control.\r\n\r\nWhen I add the setting, Docker's behavior changes. It no longer creates `docker0` (which is expected) but it also does not create DOCKER-USER, and doesn't place a jump to it to the top of the FORWARD chain. Instead, it only creates the other chains and the isolation stages. This is rather perplexing, at least to me, and looks like a bug.\r\n\r\nIf I use `iptables` commands to add DOCKER-USER manually, add it to the top of FORWARD chain, and add rules to this chain, and put it to start before Docker (by manipulating \"docker.service\"'s systemd file) then when Docker starts it takes its own rules and inserts them _before_ the jump to DOCKER-USER. This is also rather peculiar.\r\n\r\nNote that I have a workaround for this, so all I want is for someone who knows Docker's internal behavior better than me take a look and explain why Docker omits the DOCKER-USER chain when the default bridge is set to \"none\".\r\n\r\n### Reproduce\r\n\r\n1. Install \"moby-engine\"\r\n2. Use `docker network create --driver=bridge my-bridge` to create a user-defined bridge network\r\n3. Use `systemctl stop docker` to shutdown Docker\r\n4. Use `iptables -F`and `iptables -X` on \"filter\" and \"nat\" tables to ensure everything is clean\r\n5. Use `systemctl start docker` to restart Docker\r\n6. With `iptables -S` ensure that jump to DOCKER-USER chain is at the top of FORWARD chain\r\n7. Use `systemctl stop docker` to shutdown Docker\r\n8. Add `\"bridge\": \"none\"` to \/etc\/docker\/daemon.json\r\n9. Use `iptables -F`and `iptables -X` on \"filter\" and \"nat\" tables to clean all rules again\r\n10. Use `systemctl start docker` to restart Docker\r\n11. With `iptables -S` observe the jump to DOCKER-USER chain has not been added\r\n\r\n### Expected behavior\r\n\r\nExpected behavior is that `\"bridge\": \"none\"` does not affect DOCKER-USER chain's addition or placement in the rules. Docker should behave consistently in this regard, and it should always ensure that jump to DOCKER-USER occurs before any other rules.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.9-1\r\n API version:       1.43\r\n Go version:        go1.20.13\r\n Git commit:        293681613032e6d1a39cc88115847d3984195c24\r\n Built:             Wed Jan 31 20:53:14 UTC 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.9-1\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.13\r\n  Git commit:       fca702de7f71362c8d103073c7e4a1d0a467fadd\r\n  Built:            Thu Feb  1 00:12:23 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.30-1\r\n  GitCommit:        d68034c2fe20ce0fc29692f87afced1edf7d77da\r\n runc:\r\n  Version:          1.1.12-1\r\n  GitCommit:        51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.9-1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.13.1-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 3\r\n Server Version: 24.0.9-1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: journald\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d68034c2fe20ce0fc29692f87afced1edf7d77da\r\n runc version: 51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.146.1-microsoft-standard-WSL2\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 15.58GiB\r\n Name: DESKTOP-5M6TJ8E\r\n ID: 9f4523df-2d8d-46b7-a5a1-069d47c921f4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nThis test was executed in Windows 11's WSL, using Ubuntu 22.04 distro.","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Restore the SetKey prestart hook.","body":"**- What I did**\r\n\r\n- Fix https:\/\/github.com\/moby\/moby\/issues\/47619\r\n- Partially reverted https:\/\/github.com\/moby\/moby\/commit\/0046b16d87105d334b72f5e98efd28bbd94d9659 - daemon: set libnetwork sandbox key w\/o OCI hook\r\n- Partially reverted https:\/\/github.com\/moby\/moby\/pull\/47521\/commits\/ef5295cda40d3d2babb5ce85ffcf67371c3ecb47 - Don't configure IPv6 addr\/gw when IPv6 disabled.\r\n\r\nRunning SetKey to store the OCI Sandbox key after task creation, rather than from the OCI prestart hook, meant it happened after sysctl settings were applied by the runtime - which was the intention, we wanted to complete Sandbox configuration after IPv6 had been disabled by a sysctl if that was going to happen.\r\n\r\nBut, it meant '--sysctl' options for a specfic network interface caused container task creation to fail, because the interface is only moved into the network namespace during SetKey.\r\n\r\n**- How I did it**\r\n\r\nRestored the SetKey prestart hook.\r\n\r\nRegenerate config files that depend on the container's support for IPv6 after the task has been created.\r\n\r\nThe changes in the second partially-reverted commit, to check IPv6 support before assigning an interface address\/gateway would no longer work, but are no longer necessary.  IPv6 addresses applied during the SetKey prestart hook will be removed when the sysctl disabling IPv6 in the container is applied.\r\n\r\n**- How to verify it**\r\n\r\nAdded a regression test, to make sure it's possible to set an interface-specfic sysctl.\r\n\r\nThe tests for IPv6 addresses in '\/etc\/hosts' when IPv6 is disabled still work.\r\n\r\n**- Description for the changelog**\r\n```markdown changelog\r\nFix a regression that meant network interface specific `--sysctl` options prevented container startup.\r\n```","comments":[],"labels":["area\/networking","kind\/bugfix"]},{"title":"failed to register layer: operation not permitted","body":"### Description\r\n\r\nSome Docker images throw the error \"failed to register layer: operation not permitted\" when the user tries to pull them in Docker running inside a systemd-nspawn container.\r\n\r\nThe described behavior is tested on Arch nspawn (instructions in the \"Reproduce\" section) and on NixOS Containers (tested on 23.11 and unstable).\r\n\r\nThis issue probably affects all bitnami images, but it's only tested on the following ones:\r\n- bitnami\/minideb:bookworm\r\n- bitnami\/mariadb:11.2\r\n- bitnami\/joomla:5\r\n- bitnami\/drupal:10\r\n\r\nAnd it also happens on this image:\r\n- linuxserver\/nextcloud:27.1.1 (issue: https:\/\/github.com\/linuxserver\/docker-nextcloud\/issues\/371)\r\n\r\nDocker works fine with other images like:\r\n- hello-world\r\n- debian:bookworm\r\n- mariadb:11.2\r\n- serjs\/go-socks5-proxy\r\n- niklasf\/fishnet:2\r\n- portainer\/portainer-ce\r\n\r\n### Reproduce\r\n\r\n1. Create a new systemd-nspawn container. I tested it on Arch following their instructions: https:\/\/wiki.archlinux.org\/title\/Systemd-nspawn\r\n\t```bash\r\n\t# Prepare the environment\r\n\tpacman -S arch-install-scripts\r\n\tmkdir \/opt\/arch-nspawn\r\n\tpacstrap -K -c \/opt\/arch-nspawn base docker\r\n\t\r\n\t# Enter the environment\r\n\tsystemd-nspawn -D \/opt\/arch-nspawn\r\n\t\r\n\t# Reset root password and exit\r\n\tpasswd root\r\n\tlogout\r\n\t```\r\n2. Create a new bridge network or use an existing one on host. I reused my virbr0, from libvirt.\r\n3. Start the container with the following command to disable seccomp and enable private networking. Docker may not run with seccomp and\/or host networking.\r\n`SYSTEMD_SECCOMP=0 systemd-nspawn -b --network-bridge=your-bridge -D \/opt\/arch-nspawn`\r\n4. Login with root and the previous passwod\r\n5. Start the Docker services with `systemctl start docker`\r\n6. Set an IP for the host0 interface, a gateway and a nameserver in `\/etc\/resolv.conf`\r\n\t```bash\r\n\tip a add 192.168.122.2\/24 dev host0\r\n\tip l set dev host0 up\r\n\tip r add default via 192.168.122.1\r\n\techo \"nameserver 9.9.9.9\" >\/etc\/resolv.conf\r\n\t```\r\n7. Try to pull, or run any of the mentioned images\r\n\t```bash\r\n\tdocker run --rm -it hello-world # Should succeed\r\n\tdocker pull bitnami\/mariadb:11.2 # Should throw the described error\r\n\t```\r\n\r\n### Expected behavior\r\n\r\nDocker should be able to pull any image\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           25.0.4\r\n API version:       1.44\r\n Go version:        go1.22.1\r\n Git commit:        1a576c50a9\r\n Built:             Wed Mar 13 15:44:41 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          25.0.4\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.22.1\r\n  Git commit:       061aa95809\r\n  Built:            Wed Mar 13 15:44:41 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.14\r\n  GitCommit:        dcf2847247e18caba8dce86522029642f60fe96b.m\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    25.0.4\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: 25.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: btrfs\r\n  Supports d_type: true\r\n  Using metacopy: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: dcf2847247e18caba8dce86522029642f60fe96b.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.7.9-zen1-1-zen\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 31.25GiB\r\n Name: arch-nspawn\r\n ID: c782b9f8-d825-479c-b58e-ecc08d6758cc\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Found the solution. It looks like it required a user namespace inside systemd-nspawn. This is done in the CLI by adding the `-U` flag (resulting command: `SYSTEMD_SECCOMP=0 systemd-nspawn -b --network-bridge=your-bridge -D \/opt\/arch-nspawn -U`).\r\n\r\nIn the case of NixOS Containers, adding `containers.<name>.extraFlags = [ \"-U\" ];` to the config should do the trick.","Enabling user namespace triggers another error upon starting the Docker Containers, that seems common in unprivileged environments:\r\n\r\n```\r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"sysfs\" to rootfs at \"\/sys\": mount sysfs:\/sys (via \/proc\/self\/fd\/7), flags: 0xf: operation not permitted: unknown.\r\n```\r\n\r\nI am working on finding solution to that as well.\r\n\r\nEDIT: Probably related to https:\/\/github.com\/systemd\/systemd\/issues\/27994\r\n\r\nBased on [this comment](https:\/\/github.com\/systemd\/systemd\/issues\/27994#issuecomment-1704005670), a workaround is to run `SYSTEMD_SECCOMP=0 systemd-nspawn -b --network-bridge=your-bridge -D \/opt\/arch-nspawn -U --bind=\/proc:\/run\/proc --bind=\/sys:\/run\/sys`"],"labels":["status\/0-triage","kind\/bug"]},{"title":"simple run command with --sysctl for network interface fails after upgrade","body":"### Description\r\n\r\nafter upgrade my Debian from bullseye to bookworm today,  my container failed to work.\r\n\r\ni traced it down to this simple command:\r\n\r\n> docker run --rm --sysctl net.ipv4.conf.eth0.forwarding=1 alpine \r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: open \/proc\/sys\/net\/ipv4\/conf\/eth0\/forwarding: no such file or directory: unknown.\r\n\r\nbut when the container is started, the _eth0_ conf is right there. maybe network interface renaming timing changed ?\r\nand if i change the interface name to _lo_ or _all,_ the above command works fine.\r\n\r\nunfortunately, i have to rely on the interface name: if i use _all_ to set both _forwarding=1_ and _accept_ra=2_, the container seems not respected _accept_ra_ setting. in other words, if i use following commands, the containers won't get its IPv6 address form RA. (_XXX_ is my custom IPv6 enabled MacVLAN network)\r\n\r\n> docker run -it --rm --network XXX --sysctl net.ipv6.conf.all.forwarding=1 --sysctl net.ipv6.conf.all.accept_ra=2 ubuntu bash\r\ndocker run -it --rm --network XXX --sysctl net.ipv6.conf.all.forwarding=1 --sysctl net.ipv6.conf.all.accept_ra=2 alpine sh\r\n\r\n### Reproduce\r\n\r\nfor _--sysctl_ failure:\r\n1. _docker run --rm --sysctl net.ipv4.conf.eth0.forwarding=1 alpine sh_ will fail\r\n\r\nfor no IPv6 address from RA (this probably not related to docker, just i can't use _all_ for interface name)\r\n1. create a IPv6 enabled network XXX\r\n2. _docker run -it --rm --network XXX --sysctl net.ipv6.conf.all.forwarding=1 --sysctl net.ipv6.conf.all.accept_ra=2 alpine sh_ will not get IPv6 address from RA\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           26.0.0\r\n API version:       1.45\r\n Go version:        go1.21.8\r\n Git commit:        2ae903e\r\n Built:             Wed Mar 20 15:18:02 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          26.0.0\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       8b79278\r\n  Built:            Wed Mar 20 15:18:02 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    26.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.25.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 16\r\n Server Version: 26.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.21-v8+\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 3.705GiB\r\n Name: rpi4\r\n ID: 60af6eb1-813d-4d13-929e-23993c2a56dc\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["i pinned docker-ce to **25.0.5**, my container works as before.\r\n\r\nso i guess it's caused by the recent **26** release.","Hi @jwfang - thank you for narrowing down the issue and raising the clear report.\r\n\r\nIt's fallout from https:\/\/github.com\/moby\/moby\/pull\/47062 - in https:\/\/github.com\/moby\/moby\/pull\/47062\/commits\/0046b16d87105d334b72f5e98efd28bbd94d9659 we moved some of the network configuration from a pre-start hook in the runtime to after the container task has been created.\r\n\r\nAs you suggest, that means the network interface renaming (moving one end of a veth device into the container namespace in `Sandbox.populateNetworkResources`, `sb.osSbox.AddInterface`) happens after sysctls are applied by the runtime.\r\n\r\ncc @corhere - I think we'll need to go back to using the pre-start hook."],"labels":["kind\/bug","area\/networking","status\/confirmed","version\/26.0"]},{"title":"Windows: Process running within Docker Container throws OOM error but container doesn't report the error","body":"### Description\r\n\r\nI noticed an issue where a process running within a docker container on a Windows VM reported an Out of Memory error according to the logs emitted by docker, however the container itself reported as not exiting due to an OOM error (OOMKilled flag was false when running `docker inspect <containerid>`) and the exit code reported from the container never matched the expected exit code for an OOM error (Exit code 137). When trying to see if linux had the same behavior when it comes to the process running inside of the container reporting an OOM error, the docker container reported that it was killed due to OOM (OOMKilled flag was true when running `docker inspect <containerid>` and container reported exit code of 137). I always saw the same behavior in regards to the container reporting that itself wasn't killed due to OOM while the process was reporting OOM errors.\r\n\r\n### Reproduce\r\n\r\nWindows Reproduction Steps:\r\n1. Created simple DockerFile \r\n\r\n```\r\nFROM mcr.microsoft.com\/windows\/servercore:ltsc2019\r\n\r\nCOPY test.ps1 .\r\n\r\nENTRYPOINT [\"powershell.exe\"]\r\n\r\nCMD [\".\\\\test.ps1\"]\r\n```\r\n\r\ntest.ps1 contents\r\n```\r\nfunction Consume-Memory {\r\n    $mem = @()\r\n\r\n    while ($true) {\r\n        # Allocate 10 MB of memory\r\n        $data = New-Object byte[] (100*1024*1024)\r\n        if (-not $data) {\r\n            throw \"Failed to allocate memory. Exiting...\"\r\n        }\r\n        $mem += $data\r\n        Write-Host \"Allocated chunks: $($mem.Count)\"\r\n        Start-Sleep -Seconds 10  # Adjust sleep duration as necessary\r\n    }\r\n}\r\n\r\nfunction Main {\r\n    Consume-Memory\r\n}\r\n\r\nMain\r\n```\r\n\r\n\r\n2. Built an image using the above file `docker build -t <yourtaghere> .`\r\n3. Ran a docker container using `docker run -m 256 <tag here> `\r\n4. Monitored the container memory usage using `docker stats` \r\n5. When container would eventually exit, I inspected the container using `docker inspect <containeridhere>`\r\n6. Verified the container did not set the OOMKilled flag as True when the associated docker logs for the container through the path given in the above `docker inspect` command and process running within the container reported stopping due to an out of memory error.\r\n\r\n\r\nLinux Reproduction Steps:\r\n\r\nThese steps are relatively short and simpler than the windows steps. \r\n\r\n1. Created simple Dockerfile\r\n```\r\nFROM public.ecr.aws\/amazonlinux\/amazonlinux:latest\r\n\r\nCMD [\"python3\", \"-c\", \"foo=' '*1024*1024*512; import time; time.sleep(10)\"]\r\n```\r\n2. Built the image and initiated a container from the image with a limit of 512mb for the container. I used `docker build -t <yourtaghere>` and `docker run -m 512 <tag here>`\r\n4. The container would exit almost immediately, emitting exit code 137 which represents an OOM error and the process alongside reported an OutOfMemoryException.\r\n\r\n### Expected behavior\r\n\r\nDocker container running on Windows when encountered with an OutOfMemory error from the process running within the container should report itself as being killed due to OOM error which is the behavior on linux.\r\n\r\n### docker version\r\n\r\n```bash\r\nWindows Client:\r\nClient:\r\n Version:           22.06.0-beta.0-189-g4e05b45a90.m\r\n API version:       1.41 (downgraded from 1.42)\r\n Go version:        go1.19.2\r\n Git commit:        4e05b45a90\r\n Built:             Sat Oct 29 21:48:57 2022\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.24)\r\n  Go version:       go1.18.10\r\n  Git commit:       6051f14291\r\n  Built:            Wed Oct 25 18:58:05 2023\r\n  OS\/Arch:          windows\/amd64\r\n  Experimental:     false\r\n\r\nLinux Version:\r\nClient:\r\n Version:           20.10.25\r\n API version:       1.41\r\n Go version:        go1.19.9\r\n Git commit:        b82b9f3\r\n Built:             Wed Jul 12 19:37:13 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.25\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.19.9\r\n  Git commit:       5df983c\r\n  Built:            Wed Jul  5 00:00:00 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.11\r\n  GitCommit:        64b8a811b07ba6288238eefc14d898ee0b5b99ba\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        4bccb38cc9cf198d52bebf2b3a90cd14e7af8c06\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nLinux Info:\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., 0.0.0+unknown)\r\n\r\nServer:\r\n Containers: 9\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 8\r\n Images: 11\r\n Server Version: 20.10.25\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 64b8a811b07ba6288238eefc14d898ee0b5b99ba\r\n runc version: 4bccb38cc9cf198d52bebf2b3a90cd14e7af8c06\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 6.1.79-99.164.amzn2023.x86_64\r\n Operating System: Amazon Linux 2023.3.20240312\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.493GiB\r\n Name: ip-172-31-40-192.us-west-2.compute.internal\r\n ID: 4YDE:PK2F:GCNR:2KXX:4QIW:GJZP:AHPB:7P4V:5KKU:GPJO:IC4Y:UFT4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWindows Info:\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 23\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 23\r\n Images: 28\r\n Server Version: 20.10.23\r\n Storage Driver: windowsfilter\r\n  Windows:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Default Isolation: process\r\n Kernel Version: 10.0 17763 (17763.1.amd64fre.rs5_release.180914-1434)\r\n Operating System: Windows Server 2019 Datacenter Version 1809 (OS Build 17763.5458)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.707GiB\r\n Name: EC2AMAZ-8CRB3OK\r\n ID: JZVV:HI7E:KKII:ULNM:VNL7:KL22:QPAG:UKU5:Z4KF:IDJC:3IDC:HRU2\r\n Docker Root Dir: C:\\ProgramData\\docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nProcess logs from Windows:\r\n```\r\n{\"log\":\"Allocated chunks: 104857600\\n\",\"stream\":\"stdout\",\"time\":\"2024-03-22T15:51:37.3341515Z\"}\r\n{\"log\":\"Array dimensions exceeded supported range.\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7072964Z\"}\r\n{\"log\":\"At C:\\\\test.ps1:11 char:9\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7072964Z\"}\r\n{\"log\":\"+         $mem += $data\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7072964Z\"}\r\n{\"log\":\"+         ~~~~~~~~~~~~~\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7072964Z\"}\r\n{\"log\":\"    + CategoryInfo          : OperationStopped: (:) [], OutOfMemoryException\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7085132Z\"}\r\n{\"log\":\"    + FullyQualifiedErrorId : System.OutOfMemoryException\\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7085132Z\"}\r\n{\"log\":\" \\r\\n\",\"stream\":\"stderr\",\"time\":\"2024-03-22T15:52:04.7085132Z\"}\r\n```\r\n`docker inspect` logs:\r\ndocker inspect exciting_rubin\r\n[\r\n    {\r\n        \"Id\": \"6fe653aeccb41201d50629356f057df650195f63bb16d0070fc31eefeb93a29b\",\r\n        \"Created\": \"2024-03-22T15:50:54.4760729Z\",\r\n        \"Path\": \"powershell.exe\",\r\n        \"Args\": [\r\n            \".\\\\test.ps1\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"exited\",\r\n            \"Running\": false,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false, `should be true`\r\n            \"Dead\": false,\r\n            \"Pid\": 0,\r\n            \"ExitCode\": 3221225473,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2024-03-22T15:50:56.2195812Z\",\r\n            \"FinishedAt\": \"2024-03-22T16:03:36.7360532Z\"\r\n        },\r\n","comments":["If there is any other information you need regarding this issue, please let me know and I will be happy to provide any other context\/information needed.","This is by design; the Windows Containers code has no way to determine if a container died to an out of memory condition, because of the current underlying implementation:\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/83ae9927fb1dd45b9876479c1783eb970256c04e\/daemon\/monitor.go#L158-L162\r\n\r\nThe underlying implementation is in the shim, the actual \"runtime\" component that manages the lifecycle of containers. It uses `epoll` to find out from the kernel when an OOM condition occurs:\r\n\r\nhttps:\/\/github.com\/containerd\/containerd\/blob\/dcf2847247e18caba8dce86522029642f60fe96b\/runtime\/v2\/runc\/task\/service.go#L65-L69\r\n\r\nhttps:\/\/github.com\/containerd\/containerd\/blob\/dcf2847247e18caba8dce86522029642f60fe96b\/runtime\/v2\/runc\/task\/service.go#L318-L341\r\n\r\nIt's possible this could be supported in the future with the 'containerd' implementation of Windows containers (where we call to a equivalent 'shim' to run containers instead of driving the OS directly, as today Windows does not use a shim, unlike Linux containers); however the equivalent code does not implement any sort of OOM notification as required by containerd:\r\n\r\nhttps:\/\/github.com\/microsoft\/hcsshim\/blob\/a58b41457cca7c4f08f489e15ca1768ebfd84df5\/cmd\/containerd-shim-runhcs-v1\/task_hcs.go\r\n\r\n[Searching the repository](https:\/\/github.com\/microsoft\/hcsshim\/issues?q=is%3Aissue+OOM+), I don't see any issues related to an OOM event; I would suggest creating one (keeping in mind that this may just not be possible on Windows, I don't have the expertise there to say)."],"labels":["area\/runtime","platform\/windows","exp\/expert","kind\/feature"]},{"title":"Docker engine starts containers before making shure docker.sock is created on host-system","body":"### Description\n\nI've noticed in some cases docker stop\/start\/restart may fail while using docker services\/containers that need bind-mound to docker.sock. This is the case for example for:\r\n- portainer\r\n- portainer-agent\r\n- traefik\r\n\r\nIn this case for some reason trying to (re)start docker-engine if there is no docker.sock the container bind-mount will result into a empty directory on the host (in this case \/var\/run\/docker.sock will be a directory, not a socket)! Even deleting the directory does not resolve the issue as after stop\/restart the procedure repeats.\r\n\r\nThis is a real problem as the only current solution is to determine to container uuid and delete it in \/var\/lib\/docker\/containers or even delete them all. I don't think this is a gentle and solid solution as we need to re-deploy everything afterwards having an additional downtime then for the docker-server until everything works again as it should.\r\n\r\ndocker-engine should make shure docker.sock is created before even going to start any container that will do any mounts.\n\n### Reproduce\n\n1. install portainer: ```docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v \/var\/run\/docker.sock:\/var\/run\/docker.sock -v portainer_data:\/data portainer\/portainer```\r\n2.systemctl stop docker\r\n3. rm \/var\/run\/docker.sock\r\n4. systemctl start docker\n\n### Expected behavior\n\ndocker should make shure the engine & docker.sock itself is healthy before going to start any services\/containers that require docker is really ready.\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        24.0.5-0ubuntu1\r\n Built:             Wed Aug 16 21:32:36 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\npermission denied while trying to connect to the Docker daemon socket at unix:\/\/\/var\/run\/docker.sock: Get \"http:\/\/%2Fvar%2Frun%2Fdocker.sock\/v1.24\/version\": dial unix \/var\/run\/docker.sock: connect: permission denied\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.7\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\nERROR: permission denied while trying to connect to the Docker daemon socket at unix:\/\/\/var\/run\/docker.sock: Get \"http:\/\/%2Fvar%2Frun%2Fdocker.sock\/v1.24\/info\": dial unix \/var\/run\/docker.sock: connect: permission denied\r\nerrors pretty printing info\n```\n\n\n### Additional Info\n\n_No response_","comments":["> ```shell\r\n> Server:\r\n> ERROR: permission denied while trying to connect to the Docker daemon socket at unix:\/\/\/var\/run\/docker.sock: Get \"http:\/\/%2Fvar%2Frun%2Fdocker.sock\/v1.24\/info\": dial unix \/var\/run\/docker.sock: connect: permission denied\r\n> ```\r\n\r\nWe need the server info before we can triage this issue."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug"]},{"title":"Rootless container internet connectivity not working when iptables disabled","body":"### Description\r\n\r\nI have noticed that any sort of internet request (ping\/curl\/dig) doesn't work inside containers when I set `iptables` to `false` in `~\/.config\/docker\/daemon.json` (rootless daemon config).\r\nWhen I start the `docker` user service it used to log `\"skipping firewalld management for rootless mode\"`, so I thought that the iptables setting is skipped anyway for rootless docker and I disabled it. After all the rootless docker daemon doesn't have the required root access to modify iptables rules and there aren't any changes when I checked with `sudo iptables -L`.\r\n\r\nI think that somewhere in the code some critical internet connectivity thing depends on the state of the iptables setting even if it shouldn't.\r\n\r\n### Reproduce\r\n\r\n1. Setup rootless docker\r\n2. Set `iptables` to `false` in `~\/.config\/docker\/daemon.json`:\r\n   ```json\r\n   {\r\n     \"iptables\": false\r\n   }\r\n   ```\r\n3. Start a docker container and run something like `wget`:\r\n   ```sh\r\n   docker run --rm alpine wget https:\/\/google.com\r\n   ```\r\n4. Internet connectivity isn't working and wget outputs something like:\r\n   ```\r\n   wget: bad address 'google.com'\r\n   ```\r\n5. Remove the `iptables` setting from the `daemon.json` file and try again, wget should work now.\r\n\r\n### Expected behavior\r\n\r\nInternet connectivity should not be affected by the iptables setting in rootless mode.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.5\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        5dc9bcc\r\n Built:             Tue Mar 19 15:05:34 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           rootless\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.5\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       e63daec\r\n  Built:            Tue Mar 19 15:05:34 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          2.0.2\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    slirp4netns\r\n  PortDriver:       builtin\r\n  StateDir:         \/run\/user\/1000\/dockerd-rootless\r\n slirp4netns:\r\n  Version:          1.2.0\r\n  GitCommit:        656041d45cfca7a4176f6b7eed9e4fe6c11e8383\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.5\r\n Context:    rootless\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.25.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 25.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 6.1.0-18-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 15.62GiB\r\n Name: ordontest\r\n ID: b5969804-5e9a-42df-a2f7-20690bdbdfe0\r\n Docker Root Dir: \/home\/jonas\/.local\/share\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["cc @akerouanton @robmry @AkihiroSuda ","I think that's to be expected - there are normally iptables masquerade rules in the network namespace the rootless dockerd runs in, needed to route traffic to the outside world, and those aren't set up.\r\n\r\nIs there a reason to want to do this?"],"labels":["status\/0-triage","kind\/bug"]},{"title":"docker checkpoint create failed with a huge image.","body":"### Description\r\n\r\nI am trying to use the docker checkpoint feature https:\/\/docs.docker.com\/reference\/cli\/docker\/checkpoint\/.\r\n\r\nI have done the following commands.\r\n```\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:~# docker run --init --security-opt=seccomp:unconfined --name cr -d busybox \/bin\/sh -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done'\r\n7a5fe57e1352c60530ee82a9df977cb4a95d2712e9e769980dcfe2f984362e19\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:~# docker checkpoint create cr checkpoint1\r\ncheckpoint1\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:~# docker start --checkpoint checkpoint1 cr\r\n```\r\nAlso I've tried nginx and redis,and they seems to be working in a nice condition.\r\nBut when I tried to use my own image,I failed.Infact, as the size of this image if about 10GB,I am wondering if this condition is because of the size of current image is too huge or someting.\r\n```\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:~# docker run --init --security-opt=seccomp:unconfined --name trycheck -d luochengxi\/uosvnc:latest\r\n9ddc9685d8276719aa8a7336c8bbc360747736f4728f0419c341019bef8520f6\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:~# docker checkpoint create trycheck cp1\r\nError response from daemon: Cannot checkpoint container trycheck: runc did not terminate successfully: exit status 1: criu failed: type NOTIFY errno 0 path= \/run\/containerd\/io.containerd.runtime.v2.task\/moby\/9ddc9685d8276719aa8a7336c8bbc360747736f4728f0419c341019bef8520f6\/criu-dump.log: unknown\r\n```\r\n\r\n### Reproduce\r\n\r\n```\r\ndocker run --init --security-opt=seccomp:unconfined --name trycheck -d luochengxi\/uosvnc:latest\r\ndocker checkpoint create trycheck cp1\r\n```\r\n\r\n### Expected behavior\r\n\r\ndocker checkpoint create successfully.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.4\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        1a576c5\r\n Built:             Wed Mar  6 16:32:12 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.4\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       061aa95\r\n  Built:            Wed Mar  6 16:32:12 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.4\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.7\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-92-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.493GiB\r\n Name: iZ2zef7sqxs9idaeluuxe0Z\r\n ID: 12769291-2d96-4e49-b1a7-68ec8208c028\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: luochengxi\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nMy CRIU version\r\n```\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/# criu --version\r\nVersion: 3.19\r\n```\r\n\r\nThe criu-dump. log.\r\n[criu-dump.log](https:\/\/github.com\/moby\/moby\/files\/14692751\/criu-dump.log)","comments":["Hm... yeah, not immediately clear to me either what's causing CRIU to fail (error comes from CRIU itself).\r\n\r\nAs you mentioned it happened with a large image; things work as expected on smaller images?\r\n\r\nI did notice that the amount of memory on that machine is not very large;\r\n\r\n```\r\nTotal Memory: 3.493GiB\r\n```\r\n\r\nIf you have a test-environment to run on, I'm curious if the issue also reproduces on a machine with more memory available (really thinking out loud here!)\r\n\r\n","Thanks for your reply. \r\n\r\nI'm sorry I cannot get a test-environment which has more memory available right now as I am a poor sophomore, but I will try to borrow one and I promise I will try to show you whether the issue also reproduces on a machine with more memory available\ud83e\udee1\r\n\r\n\ud83d\udc2d\ud83d\udc2d\ud83d\udc2d\r\nHowever,I can show you that it is not because of image size. I use the same machine and I create a Dockerfile.\r\n```dockerfile\r\nFROM  ubuntu:latest\r\n\r\n# I create a 20G file in my image.\r\nRUN fallocate -l 20G rumenz.img\r\n\r\n# to keep the container running\r\nCMD \/bin\/sh -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done'\r\n\r\n```\r\nI build it , and get an image which is more than 20G.And the checkpoint works well.\r\n\r\n```\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker build .\r\n[+] Building 99.5s (6\/6) FINISHED                                             docker:default\r\n => [internal] load build definition from Dockerfile                                    0.0s\r\n => => transferring dockerfile: 171B                                                    0.0s\r\n => [internal] load metadata for docker.io\/library\/ubuntu:latest                        0.0s\r\n => [internal] load .dockerignore                                                       0.0s\r\n => => transferring context: 2B                                                         0.0s\r\n => CACHED [1\/2] FROM docker.io\/library\/ubuntu:latest                                   0.0s\r\n => [2\/2] RUN fallocate -l 20G rumenz.img                                               0.2s\r\n => exporting to image                                                                 99.3s\r\n => => exporting layers                                                                99.3s\r\n => => writing image sha256:66f26beb67c9bb501d1fa891d37e2b2430347c8a26980c92baa3a2be44  0.0s\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker run -d 667\r\nUnable to find image '667:latest' locally\r\n^C\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker run -d 66f\r\nf8f17cd4ca0c1c0d74d8b19e16b858ac66bde97e1b731043d74e179eff872412\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker logs f8f\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker checkpoint create f8f cp1\r\ncp1\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker logs f8f\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker start --checkpoint cp1 f8f\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# docker logs f8f\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\nroot@iZ2zef7sqxs9idaeluuxe0Z:\/dockertest# \r\n```\r\n\r\n","I tried this feature in a machine with 4 CPUs and about 8GB memory, and its checkpoint worked well with busybox,too.\r\n\r\n```\r\nroot@VM-4-11-ubuntu:\/# docker run --init --security-opt=seccomp:unconfined --name cr -d busybox \/bin\/sh -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done'\r\n63567bf94fe6420be69e5b72095719776219b050780c258a2823ddd2c14af90d\r\nroot@VM-4-11-ubuntu:\/# docker checkpoint create cr checkpoint1\r\ncheckpoint1\r\nroot@VM-4-11-ubuntu:\/# docker start --checkpoint checkpoint1 cr\r\n```\r\n\r\nI attempted to use the luochengxi\/uosvnc:latest image. However, it failed with an output identical to what I received on a machine with 2 CPUs and 4GB of memory. I am now wondering whether Docker's checkpoint feature have any limitations regarding the image itself?\ud83e\udd14\r\n\r\n```\r\nroot@VM-4-11-ubuntu:\/# docker run --init --security-opt=seccomp:unconfined --name trycheck -d luochengxi\/uosvnc:latest\r\n5abcabc7cef16152dbc96d853c8570cc0a2724ac252b3aeeb69ee65957324e24\r\nroot@VM-4-11-ubuntu:\/# docker checkpoint create trycheck cp1\r\nError response from daemon: Cannot checkpoint container trycheck: runc did not terminate succ\r\nessfully: exit status 1: criu failed: type NOTIFY errno 0 path= \/run\/containerd\/io.containerd\r\n.runtime.v2.task\/moby\/5abcabc7cef16152dbc96d853c8570cc0a2724ac252b3aeeb69ee65957324e24\/criu-d\r\nump.log: unknown\r\n```\r\n\r\n### docker version\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:           25.0.5\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        5dc9bcc\r\n Built:             Tue Mar 19 15:05:10 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.5\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       e63daec\r\n  Built:            Tue Mar 19 15:05:10 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:    25.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.25.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 10\r\n Images: 8\r\n Server Version: 25.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\nroot@VM-4-11-ubuntu:\/# docker info\r\nClient: Docker Engine - Community\r\n Version:    25.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.25.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 10\r\n Images: 8\r\n Server Version: 25.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-86-generic\r\n Operating System: Ubuntu 22.04 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.251GiB\r\n Name: VM-4-11-ubuntu\r\n ID: fb288d48-6587-4207-8dd9-ff7dbd650bb1\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Registry Mirrors:\r\n  https:\/\/mirror.ccs.tencentyun.com\/\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nMy CRIU version\r\n```\r\nVersion: 3.19\r\n```\r\n\r\nthe log file\r\n[criu-dump.log](https:\/\/github.com\/moby\/moby\/files\/14721221\/criu-dump.log)\r\n\r\n"],"labels":["status\/0-triage","kind\/bug","kind\/experimental","version\/25.0","area\/checkpoint"]},{"title":"Incorrect documentation docker volume create","body":"### Description\n\nTrying to create two volumes with same name should throw error as mention in the docs https:\/\/docs.docker.com\/reference\/cli\/docker\/volume\/create\/. However there is no error thrown and it just returns the volume name. I think this is not documented.  \r\n\n\n### Reproduce\n\n1. docker volume create <exisiting_volume_name>\n\n### Expected behavior\n\nThe docs need to be updated\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.10\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.21.3\r\n Git commit:        afdd53b4e3\r\n Built:             Sun Oct 29 15:42:02 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.21.3\r\n  Git commit:       311b9ff0aa\r\n  Built:            Sun Oct 29 15:42:02 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.11\r\n  GitCommit:        64b8a811b07ba6288238eefc14d898ee0b5b99ba.m\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1-desktop.4\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5-desktop.1\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-compose\r\n  debug: Get a shell into any image or container. (Docker Inc.)\r\n    Version:  0.0.24\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-debug\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  v1.0.4\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v1.0.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-sbom\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.4.1\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 8\r\n  Running: 8\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 22\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 64b8a811b07ba6288238eefc14d898ee0b5b99ba.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.6.5-arch1-1\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 19.45GiB\r\n Name: archlinux\r\n ID: c755af18-ccd5-4262-9be4-a17d19c101b2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: gkarthikraja\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Add resolver for default bridge, remove default nameservers","body":"**- What I did**\r\n\r\n**_Internal resolver for default bridge network_**\r\n\r\nUntil now, containers on the default bridge network have been configured to talk directly to external DNS servers - their resolv.conf files have either been populated with nameservers from the host's resolv.conf, or with servers from '--dns' (or with Google's nameservers as a fallback).\r\n\r\nThis change makes the internal bridge more like other networks by using the internal resolver.  But, the internal resolver is not populated with container names or aliases - it's only for external DNS lookups.\r\n\r\nContainers on the default network, on a host that has a loopback resolver (like systemd's on 127.0.0.53) will now use that resolver via the internal resolver. So, the logic used to find systemd's current set of resolvers is no longer needed by the daemon.\r\n\r\nLegacy links work just as they did before, using '\/etc\/hosts' and magic.\r\n\r\n**_No default nameservers for internal resolver_**\r\n\r\nDon't fall-back to Google's DNS servers in a network that has an internal resolver.\r\n\r\nNow the default bridge uses the internal resolver, the only reason a network started by the daemon should end up without any upstream servers is if the host's resolv.conf doesn't list any.  In this case, the '--dns' option can be used to explicitly configure nameservers for a container if necessary.\r\n\r\n(Note that buildkit's containers do not have an internal resolver, so they will still set up Google's nameservers if the host has no resolvers that can be used in the container's namespace.)\r\n\r\n**- How I did it**\r\n\r\nStart the internal resolver, unless the network is 'host', 'container' or 'none'.\r\n\r\nDeleted a bunch of code - but not the 'resolvconf' code that configures the resolver for a no-internal-resolver network, because that's still needed by buildkit.\r\n\r\n**- How to verify it**\r\n\r\nNow the default bridge has an internal resolver, there's a danger we could accidentally populate DNS names for its containers - added a regression test to catch that.\r\n\r\n**- Description for the changelog**\r\n```markdown changelog\r\nRun an internal resolver on the default bridge network to make forward DNS requests\r\nto external resolvers, even if they are on localhost addresses, or IPv6 addresses when\r\nthe default bridge does not have IPv6 connectivity. To preserve existing behavior, the\r\ninternal resolver on the default bridge will not resolve container names, unlike the\r\nresolver on user-defined networks.\r\n\r\nDo not use Google's DNS servers as a fallback when no external DNS servers are\r\nsupplied in configuration via `--dns` or available from the host's `resolv.conf`.\r\n```\r\n","comments":["@robmry this is probably changelog-worthy? (and may need \"impact\/docs\" as well?); if so, can you add those labels and a description for the change-log?","> @robmry this is probably changelog-worthy? (and may need \"impact\/docs\" as well?); if so, can you add those labels and a description for the change-log?\r\n\r\nWill do ... I haven't filled in any of the description yet, the PR's still draft - wanted the full test run before committing!"],"labels":["kind\/enhancement","area\/networking","impact\/changelog","impact\/documentation","area\/networking\/dns"]},{"title":"Flaky test: TestNewClientWithTimeout","body":"### Description\r\n\r\n```\r\n=== FAIL: github.com\/docker\/docker\/pkg\/plugins TestNewClientWithTimeout (0.03s)\r\n    client_test.go:167: started remote plugin server listening on: http:\/\/127.0.0.1:49311\/\r\n    client_test.go:180: assertion failed: error is nil, not os.IsTimeout\r\n```\r\n\r\n\r\n### docker version\r\n\r\n```bash\r\nmaster @ 23e1af45c67b6931d4942c73652bb9883fe499d8\r\n```\r\n","comments":[],"labels":["kind\/bug","area\/testing"]},{"title":"Windows DNS resolver forwarding","body":"**- What I did**\r\n\r\n* Fix https:\/\/github.com\/moby\/moby\/issues\/46792\r\n\r\nMake the internal DNS resolver for Windows containers forward requests to upsteam DNS servers when it cannot respond itself, rather than returning SERVFAIL.\r\n\r\nWindows containers are normally configured with the internal resolver first for service discovery (container name lookup), then external resolvers from '--dns' or the host's networking configuration.\r\n\r\nWhen a tool like ping gets a SERVFAIL from the internal resolver, it tries the other nameservers. But, nslookup does not, and with this change it does not need to.\r\n\r\n**- How I did it**\r\n\r\nThe internal resolver learns external server addresses from the container's HNSEndpoint configuration, so it will use the same DNS servers as processes in the container.\r\n\r\nThe internal resolver for Windows containers listens on the network's gateway address, and each container may have a different set of external DNS servers. So, the resolver uses the source address of the DNS request to select external resolvers.\r\n\r\nA [feature option](https:\/\/docs.docker.com\/reference\/cli\/dockerd\/#feature-options) can be used to prevent the internal resolver from forwarding requests (restoring the old behaviour). In `daemon.json` ...\r\n\r\n```\r\n  \"features\": {\r\n\t  \"windows-no-dns-proxy\": true\r\n  },\r\n```\r\n\r\n**- How to verify it**\r\n\r\nNew unit and integration tests.\r\n\r\nBefore the change, or with `\"features\": { \"windows-no-dns-proxy\": true }` in daemon.json ...\r\n```\r\n...\r\ndocker run -ti --rm --network mynat --name c1 mcr.microsoft.com\/windows\/servercore:ltsc2022 powershell.exe\r\n...\r\nPS C:\\> nslookup google.com\r\nServer:  UnKnown\r\nAddress:  172.31.144.1\r\n```\r\n\r\nNow ...\r\n```\r\n.\\dockerd.exe\r\n...\r\ndocker run -ti --rm --network mynat --name c1 mcr.microsoft.com\/windows\/servercore:ltsc2022 powershell.exe\r\n...\r\nPS C:\\> nslookup google.com\r\nServer:  UnKnown\r\nAddress:  172.31.144.1\r\n\r\nNon-authoritative answer:\r\nName:    google.com\r\nAddresses:  2a00:1450:4009:817::200e\r\n          172.217.169.14\r\n```\r\n\r\n**- Description for the changelog**\r\n```markdown changelog\r\nNative Windows containers are configured with an internal DNS server for container name\r\nresolution, and external DNS servers for other lookups. Not all resolvers, including\r\n'nslookup' fall back to the external resolvers when they get a SERVFAIL from the internal\r\nserver. So now, by default, the internal DNS server will now forward requests to the\r\nexternal resolvers. The previous behaviour can be restored using a feature option\r\nin the `daemon.json` file: `\"features\": { \"windows-no-dns-proxy\": true }`.\r\n```","comments":["@akerouanton - suggested replacing \"--dns-no-proxy\" with a feature-flag ... as it's only intended as an emergency off-switch for this change, and we'll want to get rid of it.\r\n\r\nI've made it Windows-only, as we don't really have a use-case for it on Linux. So, removed it from the regression test. But I left in `Sandbox.RefreshResolverPolicy()` as it seems a bit neater anyway.\r\n\r\nTo disable this change, it's now something like this in `daemon.json` ...\r\n\r\n```\r\n  \"features\": {\r\n\t  \"windows-no-dns-proxy\": true\r\n  },\r\n```\r\n"],"labels":["platform\/windows","area\/networking","kind\/bugfix","area\/networking\/dns"]},{"title":"Subreaper for execs","body":"### Description\n\nEach process started by `docker exec` and each health-check probe has PPID=0, same as the container's init process. When it exits, all of its child processes are reparented to the pid namespace's (i.e. the container's) PID 1. If the container's PID 1 does not reap zombie children, they will start to pile up. Exhausing the process table by health-checking a container which has a PID 1 that does not reap zombies using a probe command that forks is not an uncommon issue in the community. Using programs incapable of reaping zombies as a container PID 1 is tragically commonplace, so as tempting as it is to merely declare that `docker exec` and health checks on containers without an appropriate PID 1 be used at their own risk, we would not be doing our users any favours. We could potentially do more to reap zombies of execs.\r\n\r\nWe could perhaps spawn each exec and health probe as a supervised process tree implicitly under `docker-init -sg` so that the foreground process group is killed (and zombies reaped) when the exec process exits. Any daemonized (read: double-forked) processes not part of the foreground process group would become orphaned and reparented to container PID 1 when the `docker-init` subreaper exits. Alternatively, we could find or build a subreaper program (or extend tini, a.k.a. docker-init) which waits for all children to exit before exiting. Daemonized processes spawned from an exec would block such a subreaper from exiting, but the subreaper would be able to reap zombie children the daemonized process failed to reap when the daemonized process does exit.\r\n\r\nGiven that the child processes of an exec were not forked from a (child of) PID 1, it is arguably just a quirk of the existing implementation that orphaned children of an exec could be waited on by PID 1. It is possible, albeit unlikely, that someone would be broken by the behaviour change of adding implicit subreapers to execs. I expect the amount of breakage would be minimal so long as processes could daemonize themselves such that they are not killed when their exec'ed ancestor exits. We _might_ be able to get away with unconditionally running execs and health probes under a subreaper!","comments":[],"labels":["area\/runtime","status\/0-triage","kind\/enhancement"]},{"title":"docker ps very slow with containerd image store","body":"### Description\n\nWhen using containerd as the image store, `docker ps` has to query containerd for every container.\r\nNotably, it seems to do this for every single container in the system, not just running ones (which is what `docker ps` shows by default).\r\n\n\n### Reproduce\n\n1. Run a few containers (the more, the slower it gets)\r\n2. Run `docker ps`\n\n### Expected behavior\n\n`docker ps` should be a fast operation.\r\n\r\nI'm haven't looked at what dockerd is reading from containerd yet, though it is hitting `\r\ncontainerd.services.content.v1.Content\/Info`\r\n\r\nSome of the issue is these reads are happening sequentially, so it just takes time to build up the result.\r\nBut IMO, we shouldn't have to do this request to containerd at all, especially not for `docker ps`.\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.0-beta.1-450-gfa5a0e8139.m\r\n API version:       1.44\r\n Go version:        go1.21.1\r\n Git commit:        fa5a0e8139\r\n Built:             Sun Oct  1 19:58:37 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.22.0\r\n  Git commit:       be0a184c46ad56adc4ff367d2c4db5c530cd80d5\r\n  Built:            Fri Mar 15 14:44:45 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.14-1\r\n  GitCommit:        dcf2847247e18caba8dce86522029642f60fe96b\r\n runc:\r\n  Version:          1.1.12-1\r\n  GitCommit:        51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.0-beta.1-450-gfa5a0e8139.m\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.13.0-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.24.7-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 11\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 8\r\n Images: 68\r\n Server Version: dev\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc runc-patched\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: dcf2847247e18caba8dce86522029642f60fe96b\r\n runc version: 51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-1016-azure\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.33GiB\r\n Name: dev2\r\n ID: ccb953ef-cff6-4d73-94de-24e41bbff5c2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 50\r\n  Goroutines: 76\r\n  System Time: 2024-03-18T17:13:00.175001129Z\r\n  EventsListeners: 0\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nTracing data: [traces-1710782075653.json](https:\/\/github.com\/moby\/moby\/files\/14640104\/traces-1710782075653.json)\r\n\r\nIn this case I have 3 running containers and 11 total containers.\r\n<img width=\"703\" alt=\"image\" src=\"https:\/\/github.com\/moby\/moby\/assets\/799078\/de1c2507-656a-4bd0-8037-c2ec53fba622\">\r\n\r\nWe've made 89 requests to containerd to produce this result.\r\n\r\n\r\nHere is with `docker ps -a` with the same container set:\r\n\r\n<img width=\"701\" alt=\"image\" src=\"https:\/\/github.com\/moby\/moby\/assets\/799078\/53192b53-4c59-40f7-8f2f-58f35c691def\">\r\n\r\n\r\n","comments":["cc @vvoland @laurazard @rumpl ","Just to add a bit of info, `docker ps` calls `refreshImage` for each container https:\/\/github.com\/moby\/moby\/blob\/641e341eeda6b54f88b8e5b927db381d1f26164e\/daemon\/list.go#L135 which calls `GetImage` on the image service https:\/\/github.com\/moby\/moby\/blob\/641e341eeda6b54f88b8e5b927db381d1f26164e\/daemon\/list.go#L597"],"labels":["status\/0-triage","kind\/bug","area\/performance","containerd-integration"]},{"title":"HNS endpoint is not deleted after windows server patching","body":"### Description\r\n\r\nHi, \r\n\r\nwe have a small DEV infra where we run Windows Docker containers using BcContainerHelper. For some time, we are struggling with issue after Windows Server patching, that we are unable to start containers without deleting and recreating the transparent network. The error we are getting is:\r\n\r\n```\r\ndocker start my-container\r\nError response from daemon: failed to create endpoint my-container on network transparent-network: failed during hnsCallRawResponse: hnsCall failed in Win32: An address provided is invalid or reserved. (0x803b002f)\r\nError: failed to start containers: my-container\r\n```\r\n\r\nI troubleshooted it today and found that this is caused most probably by Windows patching procedure - before the server reboot, containers are not stopped correctly and HNS endpoint does not get deleted (which normally does after container stop). When the server is rebooted after patching, when I run:\r\n\r\n`Get-HNSEndpoint`\r\n\r\nI can see an endpoint of a container that is not running. When we want to start the container and due to endpoint being present, it throws error that it is reserved. This issue is solved by running:\r\n\r\n`Get-HNSEndpoint -ID EndpointID | Remove-HNSEndpoint`\r\n\r\nIs it possible to solve this somehow from docker perspective (e.g. through automatic deletion of endpoints of containers that are not running) or we must delete all endpoints after Windows Server patching via script? During the docker start, there is endpoint creation, is it possible to reuse already existing endpoint for example?\r\n\r\nOriginal issue created here https:\/\/github.com\/docker\/for-win\/issues\/13965\r\n\r\n\r\n\r\n\r\n\r\n### Reproduce\r\n\r\nGet-HNSEndpoint\r\nGet-HNSEndpoint -ID EndpointID | Remove-HNSEndpoint\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\ndocker version\r\nClient:\r\n Version:           23.0.6\r\n API version:       1.42\r\n Go version:        go1.19.9\r\n Git commit:        ef23cbc\r\n Built:             Fri May  5 21:18:35 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          23.0.6\r\n  API version:      1.42 (minimum version 1.24)\r\n  Go version:       go1.19.9\r\n  Git commit:       9dbdbd4\r\n  Built:            Fri May  5 21:17:32 2023\r\n  OS\/Arch:          windows\/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\ndocker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 8\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 7\r\n Images: 14\r\n Server Version: 23.0.6\r\n Storage Driver: windowsfilter\r\n  Windows:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Default Isolation: process\r\n Kernel Version: 10.0 20348 (20348.1.amd64fre.fe_release.210507-1500)\r\n Operating System: Microsoft Windows Server Version 21H2 (OS Build 20348.2113)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 32GiB\r\n Docker Root Dir: D:\\ProgramData\\Docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Reproduce\r\n\r\n.\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\n.\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n.\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["cc @robmry @neersighted ","As noted in the original, this seems like an odd inverse of the well-known #45462."],"labels":["platform\/windows","status\/0-triage","kind\/bug","area\/networking"]},{"title":"Flaky test: TestReadPluginNoRead","body":"### Description\n\nSeen this test fail at least twice recently;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47570#issuecomment-2001983871\r\n- https:\/\/github.com\/moby\/moby\/pull\/47569#issuecomment-2000141748\r\n\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.plugin.logging TestReadPluginNoRead\/explicitly_enabled_caching (0.88s)\r\n    read_test.go:90: assertion failed: strings.TrimSpace(buf.String()) is not \"hello world\": []\r\n    --- FAIL: TestReadPluginNoRead\/explicitly_enabled_caching (0.88s)\r\n\r\n=== FAIL: amd64.integration.plugin.logging TestReadPluginNoRead (4.83s)\r\n    read_test.go:93: [db86db6a18064] daemon is not started\r\n```\r\n","comments":["Another case;\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.plugin.logging TestReadPluginNoRead\/default (0.89s)\r\n    read_test.go:90: assertion failed: strings.TrimSpace(buf.String()) is not \"hello world\": []\r\n    --- FAIL: TestReadPluginNoRead\/default (0.89s)\r\n\r\n=== FAIL: amd64.integration.plugin.logging TestReadPluginNoRead (5.26s)\r\n    read_test.go:93: [d31e37ef4418e] daemon is not started\r\n```"],"labels":["kind\/bug","area\/testing"]},{"title":"Image list with containerd store is very slow","body":"### Description\r\n\r\n`docker images` is very slow when backed by the containerd store.\r\n\r\nUpon inspect it looks like every image is being processed sequentially, where \"process\" involves a number of API requests to containerd.\r\n\r\nListing 68 images takes roughly a full second.\r\nAs part of that there 2891 spans from containerd.\r\nSome of that even involves calculating disk usage from the snapshotters.\r\n\r\nThis is after I've pruned some images.\r\nBefore pruning I had about 325 images and that took approximately 5s.\r\n\r\n\r\n### Reproduce\r\n\r\ndocker pull a few images and read the trace data\r\n\r\n### Expected behavior\r\n\r\nImage listing should scale better\r\n\r\n### docker version\r\n\r\n```bash\r\ndocker version\r\nClient:\r\n Version:           24.0.0-beta.1-450-gfa5a0e8139.m\r\n API version:       1.44\r\n Go version:        go1.21.1\r\n Git commit:        fa5a0e8139\r\n Built:             Sun Oct  1 19:58:37 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.22.0\r\n  Git commit:       bddd892e9108a7944df8fca9fde821bb97616bab\r\n  Built:            Thu Mar 14 19:29:50 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.14-1\r\n  GitCommit:        dcf2847247e18caba8dce86522029642f60fe96b\r\n runc:\r\n  Version:          1.1.12-1\r\n  GitCommit:        51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.0-beta.1-450-gfa5a0e8139.m\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.13.0-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.24.7-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 8\r\n Images: 66\r\n Server Version: dev\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc runc-patched\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: dcf2847247e18caba8dce86522029642f60fe96b\r\n runc version: 51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-1016-azure\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.33GiB\r\n Name: dev2\r\n ID: ccb953ef-cff6-4d73-94de-24e41bbff5c2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 36\r\n  Goroutines: 62\r\n  System Time: 2024-03-14T19:44:24.766150687Z\r\n  EventsListeners: 0\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nI'm using the last build off master that actually successfully lists images for me (It seems there's another bug on master introduced by 52a80b40e25372f6f6c320bbeebe652199a81914\r\n\r\nUsing the current master (cdf70c0a51ab9833879d48861712d0426766b3c6) things are actually a bit slower and even more API requests are made to containerd (more than 1000 more spans!)\r\n\r\n\r\nAttached jaeger json, this is from a different capture than reported above, but similar results\r\n[traces-1710445565373.json.zip](https:\/\/github.com\/moby\/moby\/files\/14607247\/traces-1710445565373.json.zip)\r\n\r\nFrom the flamegraph you'll see there's nothing that's taking a lot of time in particular, just the act of processing things sequentially:\r\n\r\n<img width=\"1956\" alt=\"image\" src=\"https:\/\/github.com\/moby\/moby\/assets\/799078\/5f45f33f-68af-41ea-9d6f-cfb79b2d94a1\">\r\n\r\n","comments":["\/cc @vvoland ","For the slowness, my first thinking was this PR, but that was already in rc1;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/45967\r\n\r\nThis one could be related, but was already in rc2;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47449","Quick check on my machine running 26.0.0-rc2 with 38 images showed nearly 900ms on a first run which indeed doesn't seem very fast no.\r\n\r\n```\r\ndocker image ls | wc -l\r\n      38\r\n\r\ntime docker image ls -a\r\n\r\nreal\t0m0.855s\r\nuser\t0m0.042s\r\nsys\t0m0.022s\r\n```"],"labels":["status\/0-triage","kind\/bug","area\/images","containerd-integration"]},{"title":"update containerd binary to v1.7.14","body":"Update the containerd binary that's used in CI and for the static packages.\r\n\r\n- full diff: https:\/\/github.com\/containerd\/containerd\/compare\/v1.7.13...v1.7.14\r\n- release notes: https:\/\/github.com\/containerd\/containerd\/releases\/tag\/v1.7.14\r\n\r\nWelcome to the v1.7.14 release of containerd!\r\n\r\nThe fourteenth patch release for containerd 1.7 contains various fixes and updates.\r\n\r\nHighlights\r\n\r\n- Update builds to use go 1.21.8\r\n- Fix various timing issues with docker pusher\r\n- Register imagePullThroughput and count with MiB\r\n- Move high volume event logs to Trace level\r\n\r\nContainer Runtime Interface (CRI)\r\n\r\n- Handle pod transition states gracefully while listing pod stats\r\n\r\nRuntime\r\n\r\n- Update runc-shim to process exec exits before init\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\nUpgrade containerd to v1.7.14 (static binaries only). [moby\/moby#47557](https:\/\/github.com\/moby\/moby\/pull\/47557)\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Failure is a known flaky;\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.system TestDiskUsage\/empty (0.00s)\r\n    disk_usage_test.go:41: assertion failed: \r\n        --- du\r\n        +++ \u2192\r\n          types.DiskUsage{\r\n        - \tLayersSize: 4096,\r\n        + \tLayersSize: 0,\r\n          \tImages:     {},\r\n          \tContainers: {},\r\n          \t... \/\/ 3 identical fields\r\n          }\r\n        \r\n    --- FAIL: TestDiskUsage\/empty (0.00s)\r\n\r\n=== FAIL: amd64.integration.system TestDiskUsage (2.65s)\r\n```","It seems that the `TestExecCgroup` gets stuck consistently on this one \ud83e\udd14 "],"labels":["area\/runtime","status\/2-code-review","impact\/changelog","area\/testing","process\/cherry-pick","area\/packaging"]},{"title":"container is restarting status after docker daemon restart","body":"### Description\r\n\r\n1.When I start 30 containers like below\r\n\r\n```bash\r\nfor((index=0;index<>30;index++))\r\ndo\r\n        docker create --restart=always -ti --name=restartalways${index}  \"$ubuntu_image\" sh -c \"sleep $interval\"\r\n        docker start restartalways${index}\r\ndone\r\n```\r\n2.Restart docker.service multiple times after the container is started\r\n\r\n```bash\r\nfor((loopindex=0;loopindex<5;loopindex++))\r\ndo\r\n        systemctl restart docker.service\r\n        sleep 5\r\ndone\r\n```\r\n\r\n3.Finally, I deleted these 30 containers and found that there is a probability that the container is in the restarting state and cannot be deleted.\r\n\r\n```bash\r\ndocker stop restartalways28\r\nError response from daemon: cannot stop container: restartalways28: tried to kill container, but did not receive an exit event  docker rm restartalways28\r\nError response from daemon: cannot remove container \"\/restartalways28\": container is restarting: stop the container before removing or force remove\r\n```\r\n\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n```bash\r\ndocker create --restart=always -ti --name=restartalways${index}  \"$ubuntu_image\" sh -c \"sleep $interval\"\r\ndocker start restartalways${index}\r\n```\r\n```bash\r\nsystemctl restart docker.service\r\n```\r\n\r\n`docker stop restartalways28` or `docker rm restartalways28`\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\ndocker version\r\nClient:\r\n Version:           25.0.4\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        1a576c5\r\n Built:             Wed Mar  6 16:32:02 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.4\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       061aa95\r\n  Built:            Wed Mar  6 16:11:38 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.13\r\n  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\ndocker version\r\nClient:\r\n Version:           25.0.4\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        1a576c5\r\n Built:             Wed Mar  6 16:32:02 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.4\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       061aa95\r\n  Built:            Wed Mar  6 16:11:38 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.13\r\n  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n[root@localhost devicemapper]# docker info\r\nClient:\r\n Version:    25.0.4\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 1\r\n Server Version: 25.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.10.0-136.12.0.86.h1583.eulerosv2r12.aarch64\r\n Operating System: EulerOS 2.0 (SP12)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.232GiB\r\n Name: localhost.localdomain\r\n ID: 6de58aee-8774-4bbd-8271-5ae7194f6b91\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  10.175.125.206:80\r\n  k8s.gcr.io\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["like this\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/112141162\/9798cd34-74b0-480c-b45a-3b0fb5baec18)\r\n","![image](https:\/\/github.com\/moby\/moby\/assets\/112141162\/7b2f24d8-7b50-4f43-8688-bcbe12897584)\r\nrunc init process occupation","Does the container _remain_ in the restarting state, or is this only the race condition where you try to stop (or rm) the container but it's just in process of restarting it (after it exited)?\r\n\r\nIf your need is to remove the container, you may be able to use `--force` on `docker rm` (but haven't tried if that succesds during the \"restarting\" state.","> Does the container _remain_ in the restarting state, or is this only the race condition where you try to stop (or rm) the container but it's just in process of restarting it (after it exited)?\r\n> \r\n> If your need is to remove the container, you may be able to use `--force` on `docker rm` (but haven't tried if that succesds during the \"restarting\" state.\r\n\r\ndocker rm --force is not  effective like this \r\n```\r\ndocker rm -f e8885dec7c68\r\nError response from daemon: cannot remove container \"\/restartalways12\": could not kill: tried to kill container, but did not receive an exit event\r\n```\r\nand  tried force killing the runc init process using kill -9 and it worked\r\n","I also noticed this issue, it happens specially when using `docker stats` in a different terminal or creating multiple containers in a row although I also happens when creating 1 container. \r\n\r\nIf the container is using ipvlan\/mavclan, they also get in a weird state were the container is created without network interfaces.\r\n\r\nThis started happening since v25.x"],"labels":["status\/0-triage","kind\/bug"]},{"title":"hack\/unit: Rerun failed flaky libnetwork tests","body":"libnetwork tests tend to be flaky (namely `TestNetworkDBIslands` and `TestNetworkDBCRUDTableEntries`).\r\n\r\nMove execution of tests which name has `TestFlaky` prefix to a separate gotestsum pass which allows them to be reran 4 times.\r\n\r\nOn Windows, the libnetwork test execution is not split into a separate pass.\r\n\r\n\r\n**- What I did**\r\nRestored joy in creating PRs (hopefully).\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\nCI\r\n\r\n**- Description for the changelog**\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["~Meh, this breaks passing custom `-run` flags via `TESTFLAGS`~\r\nFixed"],"labels":["status\/2-code-review","kind\/enhancement","area\/networking","area\/testing"]},{"title":"CentOS Linux release 7.9.2009 (Core) docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown","body":"### Description\n\n# error\r\n```\r\ndocker run hello-world\r\n\r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown\r\n```\r\n# os\r\n```\r\nuname -a\r\n\r\nLinux VM-0-4-centos 3.10.0-1160.108.1.el7.x86_64 #1 SMP Thu Jan 25 16:17:31 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux \r\n```\r\n```\r\ncat \/etc\/centos-release\r\n\r\nCentOS Linux release 7.9.2009 (Core)\r\n```\r\n#  docker  daemon.json\r\n```\r\n{\r\n  \"userns-remap\":\"default\"\r\n}\r\n```\n\n### Reproduce\n\ndocker run hello-world\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:17:10 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:16:08 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.6\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 21\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 21\r\n Images: 2\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  userns\r\n Kernel Version: 3.10.0-1160.108.1.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.545GiB\r\n Name: VM-0-4-centos\r\n ID: a6ec06bf-07c3-4fc8-8ad8-f9e41309b248\r\n Docker Root Dir: \/var\/lib\/docker\/100000.100000\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n# log\r\n## docker.service\r\n```\r\njournalctl -fu docker.service\r\n\r\nMar 12 14:31:28 VM-0-4-centos dockerd[25629]: time=\"2024-03-12T14:31:28.864364787+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\"\r\nMar 12 14:31:28 VM-0-4-centos dockerd[25629]: time=\"2024-03-12T14:31:28.864416167+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\"\r\nMar 12 14:31:28 VM-0-4-centos dockerd[25629]: time=\"2024-03-12T14:31:28.905311955+08:00\" level=error msg=\"Handler for POST \/v1.44\/containers\/f84443817d1600bbb96dd4ad46056ebd0a04d6cf44cde5fb9ab7c6800480533d\/start returned error: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown\"\r\n```\r\n## containerd.service\r\n```\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.062332781+08:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.062453929+08:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.062479668+08:00\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.062657805+08:00\" level=info msg=\"starting signal loop\" namespace=moby path=\/run\/containerd\/io.containerd.runtime.v2.task\/moby\/22fd6cedcf87252ef2585db60da56d1692f7aa64d4d3f7d77df51f1342931833 pid=5807 runtime=io.containerd.runc.v2\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.082531736+08:00\" level=info msg=\"shim disconnected\" id=22fd6cedcf87252ef2585db60da56d1692f7aa64d4d3f7d77df51f1342931833\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.082584785+08:00\" level=warning msg=\"cleaning up after shim disconnected\" id=22fd6cedcf87252ef2585db60da56d1692f7aa64d4d3f7d77df51f1342931833 namespace=moby\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.082595325+08:00\" level=info msg=\"cleaning up dead shim\"\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.088379390+08:00\" level=warning msg=\"cleanup warnings time=\\\"2024-03-12T14:32:51+08:00\\\" level=info msg=\\\"starting signal loop\\\" namespace=moby pid=5830 runtime=io.containerd.runc.v2\\ntime=\\\"2024-03-12T14:32:51+08:00\\\" level=warning msg=\\\"failed to read init pid file\\\" error=\\\"open \/run\/containerd\/io.containerd.runtime.v2.task\/moby\/22fd6cedcf87252ef2585db60da56d1692f7aa64d4d3f7d77df51f1342931833\/init.pid: no such file or directory\\\" runtime=io.containerd.runc.v2\\n\"\r\nMar 12 14:32:51 VM-0-4-centos containerd[1900]: time=\"2024-03-12T14:32:51.088573206+08:00\" level=error msg=\"copy shim log\" error=\"read \/proc\/self\/fd\/12: file already closed\"\r\n```\r\n## os log\r\n```\r\ntail -f \/var\/log\/messages\r\n\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered blocking state\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered disabled state\r\nMar 12 14:34:39 VM-0-4-centos kernel: device vethd6bc28b entered promiscuous mode\r\nMar 12 14:34:39 VM-0-4-centos kernel: IPv6: ADDRCONF(NETDEV_UP): vethd6bc28b: link is not ready\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered blocking state\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered forwarding state\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered disabled state\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.211561646+08:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.211711242+08:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.211733432+08:00\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.211880469+08:00\" level=info msg=\"starting signal loop\" namespace=moby path=\/run\/containerd\/io.containerd.runtime.v2.task\/moby\/0acc1a759f03aeda353f36a467136c0fb0a0c660655ebca9bc7f8b2969b19e09 pid=5864 runtime=io.containerd.runc.v2\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.232562374+08:00\" level=info msg=\"shim disconnected\" id=0acc1a759f03aeda353f36a467136c0fb0a0c660655ebca9bc7f8b2969b19e09\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.232627552+08:00\" level=warning msg=\"cleaning up after shim disconnected\" id=0acc1a759f03aeda353f36a467136c0fb0a0c660655ebca9bc7f8b2969b19e09 namespace=moby\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.232638892+08:00\" level=info msg=\"cleaning up dead shim\"\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.238766721+08:00\" level=warning msg=\"cleanup warnings time=\\\"2024-03-12T14:34:39+08:00\\\" level=info msg=\\\"starting signal loop\\\" namespace=moby pid=5886 runtime=io.containerd.runc.v2\\ntime=\\\"2024-03-12T14:34:39+08:00\\\" level=warning msg=\\\"failed to read init pid file\\\" error=\\\"open \/run\/containerd\/io.containerd.runtime.v2.task\/moby\/0acc1a759f03aeda353f36a467136c0fb0a0c660655ebca9bc7f8b2969b19e09\/init.pid: no such file or directory\\\" runtime=io.containerd.runc.v2\\n\"\r\nMar 12 14:34:39 VM-0-4-centos containerd: time=\"2024-03-12T14:34:39.238954256+08:00\" level=error msg=\"copy shim log\" error=\"read \/proc\/self\/fd\/12: file already closed\"\r\nMar 12 14:34:39 VM-0-4-centos dockerd: time=\"2024-03-12T14:34:39.239456785+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\"\r\nMar 12 14:34:39 VM-0-4-centos dockerd: time=\"2024-03-12T14:34:39.239533924+08:00\" level=error msg=\"stream copy error: reading from a closed fifo\"\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered disabled state\r\nMar 12 14:34:39 VM-0-4-centos kernel: device vethd6bc28b left promiscuous mode\r\nMar 12 14:34:39 VM-0-4-centos kernel: docker0: port 1(vethd6bc28b) entered disabled state\r\nMar 12 14:34:39 VM-0-4-centos dockerd: time=\"2024-03-12T14:34:39.278416306+08:00\" level=error msg=\"Handler for POST \/v1.44\/containers\/0acc1a759f03aeda353f36a467136c0fb0a0c660655ebca9bc7f8b2969b19e09\/start returned error: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown\"\r\n```","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Docker restart failed on rare cases and then container never start again.","body":"### Description\r\n\r\nDocker restart failed on rare cases and then container never start again.\r\n\r\n### Reproduce\r\n\r\ndocker restart container.\r\nOn rare cases, SIGKILL take time.\r\n\r\n### Expected behavior\r\n\r\nThe container should be brought up.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.25\r\n API version:       1.41\r\n Go version:        go1.20.10\r\n Git commit:        b82b9f3a0e763304a250531cb9350aa6d93723c9\r\n Built:             Wed Oct 18 08:30:50 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.25\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       5df983c\r\n  Built:            Wed Oct 18 08:32:37 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        ccaecfcbc907d70a7aa870a6650887b901b25b82\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 7\r\n  Running: 7\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 16\r\n Server Version: 20.10.25\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: ccaecfcbc907d70a7aa870a6650887b901b25b82\r\n init version:\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.15.138.1-4.cm2\r\n Operating System: CBL-Mariner\/Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.815GiB\r\n Name: d02224a04ed0\r\n ID: ICZ5:M2ZI:PJYL:PVGE:722V:XWCT:QLFR:SKFZ:JF5K:VXFS:BWGT:PQPY\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n## Log\r\ntime=\"2024-03-02T15:58:19.549568691Z\" level=info msg=\"Container failed to exit within 1m30s of signal 15 - using the force\" container=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f\r\ntime=\"2024-03-02T15:58:29.579026954Z\" level=error msg=\"Container failed to exit within 10 seconds of kill - trying direct SIGKILL\" container=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f error=\"context deadline exceeded\"\r\ntime=\"2024-03-02T15:58:33.580895749Z\" level=error msg=\"Error killing the container\" container=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f error=\"tried to kill container, but did not receive an exit event\"\r\ntime=\"2024-03-02T15:58:33.588485246Z\" level=error msg=\"Handler for POST \/containers\/MySQL\/restart returned error: Cannot restart container MySQL: tried to kill container, but did not receive an exit event\"\r\ntime=\"2024-03-02T15:58:51.564412478Z\" level=info msg=\"ignoring event\" container=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\ntime=\"2024-03-02T15:58:51.564448059Z\" level=info msg=\"shim disconnected\" id=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f\r\ntime=\"2024-03-02T15:58:51.564496272Z\" level=warning msg=\"cleaning up after shim disconnected\" id=38d614925484c56efcba34aa3a3f25259e2f2acc4fbb00b6a550051942c7505f namespace=moby\r\ntime=\"2024-03-02T15:58:51.564504721Z\" level=info msg=\"cleaning up dead shim\"\r\ntime=\"2024-03-02T15:58:51.570746324Z\" level=warning msg=\"cleanup warnings time=\\\"2024-03-02T15:58:51Z\\\" level=info msg=\\\"starting signal loop\\\" namespace=moby pid=3996640 runtime=io.containerd.runc.v2\\n\"\r\n \r\n## Code path\r\n \r\nIn\r\n```go\r\n\/\/ daemon\/restart.go:35\r\nfunc (daemon *Daemon) containerRestart(ctx context.Context, daemonCfg *configStore, container *container.Container, options containertypes.StopOptions) error {\r\n```\r\nIt called\r\n \r\n```go\r\n\/\/ daemon\/restart.go:35\r\nerr := daemon.containerStop(ctx, container, options)\r\n```\r\nThen it called\r\n```go\r\n\/\/daemon\/stop.go:48\r\nfunc (daemon *Daemon) containerStop(ctx context.Context, ctr *container.Container, options containertypes.StopOptions) (retErr error) {\r\n```\r\n \r\nThen it called\r\n```go\r\n\/\/ daemon\/stop:113-124\r\n    \/\/ Stop either failed or container didn't exit, so fallback to kill.\r\n    if err := daemon.Kill(ctr); err != nil {\r\n        \/\/ got a kill error, but give container 2 more seconds to exit just in case\r\n        subCtx, cancel := context.WithTimeout(ctx, 2*time.Second)\r\n        defer cancel()\r\n        status := <-ctr.Wait(subCtx, container.WaitConditionNotRunning)\r\n        if status.Err() != nil {\r\n            log.G(ctx).WithError(err).WithField(\"container\", ctr.ID).Errorf(\"error killing container: %v\", status.Err())\r\n            return err\r\n        }\r\n        \/\/ container did exit, so ignore previous errors and continue\r\n    }\r\n```\r\n \r\nThen it called\r\n```go\r\n\/\/daemon\/kill.go:148\r\nfunc (daemon *Daemon) Kill(container *containerpkg.Container) error {\r\n```\r\n \r\nThen it returns error\r\n```go\r\n\/\/daemon\/kill.go:187-189\r\n    if status := <-container.Wait(ctx2, containerpkg.WaitConditionNotRunning); status.Err() != nil {\r\n        return errors.New(\"tried to kill container, but did not receive an exit event\")\r\n    }\r\n```\r\n \r\nIn the restart error handling.\r\n```go\r\n\/\/daemon\/restart.go:63-66\r\n        err := daemon.containerStop(ctx, container, options)\r\n        if err != nil {\r\n            return err\r\n        }\r\n```\r\n \r\n \r\n## Proposal\r\n \r\nThe restart can fail due to kill container, the worst thing is that it will nerver bring up the container again.\r\n \r\nSo one solution may be:\r\n \r\nWhen restart find the error is \"tried to kill container, but did not receive an exit event\", let it wait the container be killed then start the container","comments":["The SIGKILL may cannot completed in 2 seconds on some expected cases, such as the process is exiting, it was using a lot of swap space and when exiting the kernel needs to free the space. \r\nOn such cases, SIGKILL and wait 2 seconds is not enough.","You're using quite a dated Docker version now. Can you reproduce this issue on the latest v25.0.4?","We can reproduce the issue on the latest version."],"labels":["status\/0-triage","kind\/bug","version\/20.10"]},{"title":"How to set seccomp on docker services?","body":"### Description\n\nI'm trying to deploy a service that uses io_uring, which is [disallowed by the default seccomp profile](https:\/\/github.com\/moby\/moby\/issues\/47532).\r\n\r\nWhen trying to set the property in my service's yml file like [the documentation suggests](https:\/\/docs.docker.com\/compose\/compose-file\/05-services\/#security_opt):\r\n```\r\n  security_opt:\r\n    - seccomp:unconfined # io_uring init\r\n```\r\n\r\nDocker reports: `Ignoring unsupported options: security_opt`\r\n\r\nAnd that seems to be because that option [hasn't been wired up](https:\/\/github.com\/moby\/moby\/issues\/41371) yet for services.\r\n\r\nI could set docker's [daemon.json file](https:\/\/docs.docker.com\/reference\/cli\/dockerd\/#daemon-configuration-file) on all of the target boxes to run with `seccomp:unconfined` by default, but that seems a little heavy handed since it will affect every service on those boxes.\r\n\r\nIs there something better to do in the mean time until the `security_opt` flag gets wired in?\r\n\r\n\r\n\n\n### Reproduce\n\n1) Have a docker service that needs security options, as specified in a yml file\r\n2) docker stack deploy yml file\r\n3) see `Ignoring unsupported options: security_opt`\r\n4) Ask what to do on github!\n\n### Expected behavior\n\nIt should work like the `ulimits` set of options.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.4\r\n API version:       1.44\r\n Go version:        go1.21.8\r\n Git commit:        1a576c5\r\n Built:             Wed Mar  6 16:32:12 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.4\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.8\r\n  Git commit:       061aa95\r\n  Built:            Wed Mar  6 16:32:12 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.4\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.13.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.7\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-21-generic\r\n Operating System: Linux Mint 21.3\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 20\r\n Total Memory: 62.5GiB\r\n Name: spore\r\n ID: ZIHG:IUVG:WBMF:SWXK:7BIZ:WGNV:WDLY:B2FI:YHT7:LI2X:MV7G:D3HB\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: project5\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["cc @dperny; I believe this is just not plumbed yet","(also, please note that you are looking at the docs for the Compose spec; not Compose v3 which Swarm uses -- we know this is confusing and are working on it \ud83d\ude05)","Thanks for the heads up about the documentation, it looks like the proper place [would have informed me](https:\/\/docs.docker.com\/compose\/compose-file\/compose-file-v3\/#security_opt) of this option being ignored rather than going digging!\r\n\r\nIs it possible to plumb this in? At the moment, it looks like anything gated behind the default seccomp profile is just unavailable in swarm unless it's opened up machine wide in `\/etc\/docker\/daemon.json`."],"labels":["kind\/enhancement","area\/security\/seccomp","area\/swarm"]},{"title":"[EU GDPR] [Feature Request] Fallback-DNS (Google) should be removed as it is incompatible with GDPR, PCI-DSS, most company policies, doesn't follow secure coding practices etc.","body":"### Description\r\n\r\n### Feature-Request\r\nThe automatic fallback to Google's DNS servers 8.8.8.8, 8.8.4.4, 2001:4860:4860::8888, 2001:4860:4860::8844 should be:\r\n\r\n1. **Removed completely**\r\nOR\r\n2. **Disabled by default and possible to enable via a dedicated config-flag for those who actually need it**\r\nOR\r\n3. **Possible to disable in such a way that any other configuration-changes on the host will never risk re-enabling the behavior while still allowing for dynamic DNS-configuration of the host**\r\n(\"Disabling\" it by configuring a static dns-server in docker daemon's config is not a valid workaround as it's easy to forget and it goes against any reasonable design-practices since it will \"fail-open\".)\r\n\r\n#### Feature-request vs Security-issue\r\nThis is a feature request as and not a security issue as the current behavior is clearly the intended (although flawed) behavior of docker.\r\n\r\n### Background\r\nThe current [hard coded DNS fallback](https:\/\/github.com\/moby\/moby\/blob\/master\/libnetwork\/internal\/resolvconf\/resolvconf.go#L39-L48) implemented in Docker injects \"Google's\" DNS servers into containers seemingly \"at random\" (I tried figuring out exactly when this happens but after digging through the code for a couple hours i'm still not certain).\r\nIt is \"automatic\" - meaning it's also \"silent\" and most people won't notice anything wrong unless they start digging through Docker's logs to find a log-message like this:\r\n\r\n> Mar 09 08:37:05 \\<hostname\\> dockerd[169401]: time=\"2024-03-09T08:37:05.693129370Z\" level=info msg=\"No non-localhost DNS nameservers are left in resolv.conf. Using default external servers: [nameserver 8.8.8.8 nameserver 8.8.4.4]\"\r\n\r\nOr enter the containers and notice that something isn't right with the DNS setup:\r\n\r\n```\r\nroot@1f8e57d83ea1:\/# cat \/etc\/resolv.conf\r\nnameserver 8.8.8.8\r\nnameserver 8.8.4.4\r\nnameserver 2001:4860:4860::8888\r\nnameserver 2001:4860:4860::8844\r\n```\r\n\r\n### Discussion\r\n\r\nThis \"hacky solution\" bypasses any well-known dynamic-configuration methods of DNS servers like DHCP (something many \/ most servers use these days) and the only way to \"disable\" this _feature_ seems to be to configure static DNS-servers in daemon.json which overrides all dns-servers, but this replaces the behavior with another and isn't the right way to go, it's also very easy to forget to do this on every host you manage.\r\n\r\nIn practice the effect from this is that most systems out there might at some point redirect DNS traffic from containers to those hard coded IP's without notifying anyone about it and it could be triggered by an unrelated change outside of the host.  \r\nAny DNS-requests issued from such a container will be sent towards public internet with no regard for the potentially confidential information it might contain (end-user ip's, authentication system adresses) or who the receiver of that request is. The adresses might belong to Google administratively but [DNS hijacking is common](https:\/\/en.wikipedia.org\/wiki\/DNS_hijacking) and few people, if any, are aware when such hijacks takes place. Is it really Google on the other end or is it \"Google\" you're sending your users traffic to?\r\n\r\nThe fact that this fallback happens automatically and nothing explodes for users when their DNS configuration is wrong means that there are probably a lot of docker-users out there who aren't even aware that they're \"leaking\" private \/ sensitive \/ confidential information to whoever happens to be on the other end at the moment.\r\nFor all we know it might as well be some attacker who managed to hijack the prefix due to some lazy filtering at your local ISP and then Docker suddenly decides to ask the attackers DNS-server where some authentication-server is, not a very helpful or secure fallback-behavior from Docker.\r\n\r\n### Incompatible with GDPR\r\n\r\n* According to GDPR, any information that can be used to identify an end-user is \"Personally Identifiable Information\" (PII).\r\n* An IP address can be used to identify a user; therefore IP addresses are considered PII.\r\n* DNS resolves to IP addresses (A, AAAA) or from IP addresses (PTR). If a DNS request (like a PTR request) can be used to identify an end user's actions, then it's also PII.\r\n  * Anyone intercepting or receiving this request can (usually correctly) assume that the end-user communicated with the service around that time.\r\n* PII can only be shared with third parties with the user's approval - trying to \"fake it into being legal\" by pointing to some general terms of service of Google's DNS is a no-go.\r\n  * The end-user would need to approve\/accept that Google processes their PII, and most businesses aren't even aware that they're risking sending their PII to Google by simply running Docker.\r\n  * Each organization using Docker with its \"magic fallback\" would need a contract with Google regarding the processing of PII if they intend to send PII to them.\r\n  * There aren't even any guarantees that it's actually Google on the other end and why even trust Google?\r\n      Why is a company who's main source of income is from selling advertising considered trustworthy?\r\n* PII must be protected, the confidentiality of the end-user needs to be guaranteed, it's never okay to share PII over insecure channels and DNS is, by design, an insecure channel.\r\n  * DNS can only guarantee data integrity (dnssec), not confidentiality.\r\n\r\n<details>\r\n<summary>ChatGPT's view aligns with mine<\/summary>\r\n\r\n[ChatGPT link](https:\/\/chat.openai.com\/share\/1077a7d5-3eeb-4481-8dbb-c512260bf173)\r\n\r\n> Q: Based on GDPR make a detailed technical argument about the practice of using third-party software which has hard coded fallbacks to public DNS Servers with references to the relevant GDPR paragraphs.\r\n\r\n> ChatGPT\r\n> The General Data Protection Regulation (GDPR) aims to protect the personal data of European Union (EU) citizens by regulating its processing and ensuring the rights of individuals. When discussing the practice of using third-party software with hardcoded fallbacks to public DNS servers, several GDPR principles and provisions come into play.\r\n> \r\n> 1. Lawfulness, fairness, and transparency (Article 5(1)(a)): The GDPR requires that personal data be processed lawfully, fairly, and transparently. Hardcoding fallbacks to public DNS servers may lead to the transmission of users' personal data without their explicit consent or knowledge. This lack of transparency violates the principle of fairness and transparency, as users are not adequately informed about the potential processing of their data.\r\n> 2. Purpose limitation (Article 5(1)(b)): Personal data should be collected for specified, explicit, and legitimate purposes and not further processed in a manner that is incompatible with those purposes. Using third-party software with hardcoded fallbacks to public DNS servers may result in the unintentional collection and processing of personal data for purposes beyond what users consented to. This violates the principle of purpose limitation, as the data may be processed for unintended purposes without the users' knowledge or consent.\r\n> 3. Data minimization (Article 5(1)(c)): Personal data should be adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed. Hardcoding fallbacks to public DNS servers may lead to the unnecessary transmission of personal data, as the software may query these servers even when it's not essential for the service provided. This violates the principle of data minimization, as the software may collect and transmit more data than necessary for its intended purpose.\r\n> 4. Security of processing (Article 32): Data controllers and processors are required to implement appropriate technical and organizational measures to ensure a level of security appropriate to the risk. Hardcoding fallbacks to public DNS servers may expose personal data to security risks, as these servers are publicly accessible and may not provide adequate protection for the data transmitted to them. This could lead to unauthorized access, disclosure, or alteration of personal data, thereby violating the security of processing requirement.\r\n> 5. Accountability and governance (Article 5(2)): Data controllers are responsible for demonstrating compliance with the principles relating to processing of personal data. Using third-party software with hardcoded fallbacks to public DNS servers without adequate safeguards or mechanisms to protect users' personal data demonstrates a lack of accountability. Data controllers should implement measures such as data protection impact assessments (DPIAs) and privacy by design and by default to ensure compliance with the GDPR principles.\r\n>\r\n> In conclusion, the practice of using third-party software with hardcoded fallbacks to public DNS servers raises significant concerns regarding compliance with the GDPR. It violates several key principles and provisions, including lawfulness, fairness, and transparency; purpose limitation; data minimization; security of processing; and accountability and governance. Data controllers must ensure that any software they use complies with the GDPR and adequately protects users' personal data. Failure to do so may result in significant legal and financial consequences.\r\n<\/details>\r\n\r\n### Considered a security risk in PCI-DSS\r\n\r\nI let ChatGPT write an explanation for this.\r\n<details>\r\n<summary>The practice of using third-party software with hard-coded fallbacks to public DNS servers introduces significant security risks and may result in non-compliance with multiple PCI-DSS requirements.<\/summary>\r\n\r\n[ChatGPT link](https:\/\/chat.openai.com\/share\/c85195a9-d2e2-4dda-b976-d73f86974253)\r\n\r\n> Q: Based on PCI-DSS make a detailed technical argument about the practice of using third-party software which has hard coded fallbacks to public DNS Servers with references to the relevant PCI-DSS requirements.\r\n\r\n> ChatGPT\r\n> Using third-party software with hard-coded fallbacks to public DNS servers poses significant security risks and may lead to non-compliance with several PCI-DSS (Payment Card Industry Data Security Standard) requirements. PCI-DSS is a set of security standards designed to ensure that all companies that accept, process, store, or transmit credit card information maintain a secure environment. Let's delve into the technical argument:\r\n> \r\n> 1. Requirement 1: Install and maintain a firewall configuration to protect cardholder data: By using third-party software with hard-coded fallbacks to public DNS servers, you're potentially bypassing your organization's firewall rules. This can expose your network to unauthorized access and compromise cardholder data.\r\n> \r\n> 2. Requirement 2: Do not use vendor-supplied defaults for system passwords and other security parameters: Hard-coded fallbacks to public DNS servers often involve default configurations provided by the software vendor. These defaults may include insecure settings or credentials, making it easier for attackers to exploit vulnerabilities and gain unauthorized access to cardholder data.\r\n> \r\n> 3. Requirement 4: Encrypt transmission of cardholder data across open, public networks: Public DNS servers are part of the public network infrastructure and may not provide adequate encryption for transmitting sensitive cardholder data. By relying on these servers as fallbacks, you increase the risk of data interception and compromise during transmission.\r\n> \r\n> 4. Requirement 6: Develop and maintain secure systems and applications: Hard-coded fallbacks to public DNS servers represent poor software design and development practices. Secure applications should allow for configurable DNS settings and avoid relying on external services that are beyond your organization's control.\r\n> \r\n> 5. Requirement 8: Identify and authenticate access to system components: Third-party software with hard-coded fallbacks may introduce unauthorized access points into your network. These access points could be exploited by attackers to gain unauthorized access to cardholder data and compromise the integrity of your payment processing systems.\r\n> \r\n> 6. Requirement 10: Track and monitor all access to network resources and cardholder data: By using third-party software with hard-coded fallbacks to public DNS servers, you may overlook important network traffic that bypasses your organization's monitoring and logging mechanisms. This blind spot can hinder your ability to detect and respond to security incidents in a timely manner.\r\n> \r\n> 7. Requirement 11: Regularly test security systems and processes: Hard-coded fallbacks to public DNS servers should be identified and tested as part of your organization's vulnerability management program. Regular testing can help uncover potential security weaknesses and ensure that appropriate controls are in place to mitigate risks effectively.\r\n> \r\n> In summary, the practice of using third-party software with hard-coded fallbacks to public DNS servers introduces significant security risks and may result in non-compliance with multiple PCI-DSS requirements. Organizations should prioritize the use of secure software that allows for configurable DNS settings and adheres to best practices for secure application development and deployment.\r\n<\/details>\r\n\r\n### Leaking confidential information from organizations to Internet\r\n\r\nThe current design with the automagic \"Google-DNS-hack\" also conflicts with the confidentiality requirements most organizations\/businesses have. I'm fairly certain that most businesses have not made a decision to share sensitive \/ internal information like where the secrets are stored or what Amazon-buckets they use publicly, yet Docker might just decide to share it for them via its DNS-\"fallback\".\r\n\r\nIt's relatively simple to figure out a lot of information about a client, mapping an organization's internal infrastructure, usage patterns etc based on the DNS queries that a client sends. If the other end is a malicious attacker then it might even be a severe security-risk for the users.\r\n\r\n* System-\/Service-Names for most internal\/external services\r\n* Build-systems\r\n* Storage-systems\r\n* VM\/Container env\r\n* Databases\r\n* Authentication providers\/secrets management\r\n* Service-discovery scheme\/scheduler in use\r\n* etc.\r\n\r\n### Summary\r\n* The current design might \"randomly\" leak GDPR PII data unencrypted over the public Internet to an unauthorized third-party putting organizations at risk of Violating GDPR regulation.\r\n* The current design is incompatible with GDPR and places the risk of Violating GDPR on the user and most aren't even aware that they're leaking privacy\/confidential data publicly.\r\n* The current design introduces significant security risk according to PCI-DSS and may result in non-compliance with multiple PCI-DSS requirements.\r\n* The current design might \"randomly\" leak internal confidential data from organizations to the public internet\/competitors etc.\r\n* The design should be changed so that it's possible to disable the fallback DNS hack completely so it's never used.","comments":["If forwarding queries to Google DNS is such a large compliance risk to your company, why is your gateway firewall not configured to block it?\r\n\r\nThe fallback behaviour you are complaining about primarily affects containers attached to the legacy default bridge network on systems where the only `nameserver` in the host's \/etc\/resolv.conf is a loopback (127.0.0.0\/8) address, most commonly found on systems configured to use systemd-resolved. The fallback exists to work around a limitation of how DNS resolution works in containers attached to the legacy default bridge. Containers on the same hosts attached to user-defined networks will by default have their DNS queries forwarded to the local systemd-resolved resolver, respecting the host's resolv.conf, because user-defined networks are not subject to the same limitations. We have been actively discussing what to do about the default bridge and its special cases, so your request is quite timely.\r\n- https:\/\/github.com\/moby\/moby\/issues\/47464\r\n\r\nThat said, we could lift that limitation on the default bridge (and at the same time make the default bridge less of a special case) and get rid of the Google DNS fallback entirely, without having to drop the legacy networking mode. All we'd have to do is make containers on the default bridge resolve non-container DNS names through the resolver built into dockerd, exactly like how DNS resolution works today with user-defined networks.","It's not just a risk to a single company, GDPR's requirements and the risks that comes from not following the regulation applies to any organization that processes information regarding EU citizens. And it's not so simple to solve as \"drop it in the main firewall\" either, I work in quite varying environments for many different organizations and to block it over this past decade we (me + colleagues) would have had to reliably remember to block it in thousands of places. There's just too many opportunities to make mistakes when the default behavior is the wrong one so it isn't a security practice anyone can really rely on.\r\n\r\nUbuntu uses systemd-resolved (before that dnsmasq) so the fallback of sending all DNS-requests to google is enabled by default on the most used distro. Fallback is what you get if you don't make any custom configuration and just run \"docker run ...\" \"docker build ..\" etc and it has been like this for a long time.\r\n\r\nIt does seem like good timing. I'm glad to hear that you could consider dropping the behavior, it would be great. :)","> That said, we could lift that limitation on the default bridge (and at the same time make the default bridge less of a special case) and get rid of the Google DNS fallback entirely, without having to drop the legacy networking mode. All we'd have to do is make containers on the default bridge resolve non-container DNS names through the resolver built into dockerd, exactly like how DNS resolution works today with user-defined networks.\r\n\r\nThat's now https:\/\/github.com\/moby\/moby\/pull\/47602 (it doesn't affect the default DNS servers used in build containers).\r\n"],"labels":["status\/0-triage","kind\/feature","area\/networking","area\/networking\/dns"]},{"title":"Docker-proxy process uses 100% cpu and causes timeouts after 30k connections","body":"### Description\r\n\r\nWhen opening and closing 30k connections to redis sequentially using \"docker-proxy\" and \"port mapping\",\r\n- docker-proxy process takes 100% cpu\r\n- connection time increases significantly\r\n- network timeouts (max. 3s) are reached\r\n\r\nRunning the same test with \"network host\" does not cause any errors.\r\n\r\n### Reproduce\r\n\r\nexample repository: https:\/\/github.com\/thbley\/docker-proxy\r\n\r\nexample github pipeline which succeeds using \"network host\":\r\nhttps:\/\/github.com\/thbley\/docker-proxy\/actions\/runs\/8204298979\/job\/22438697761\r\n\r\nexample github pipeline which fails reproducable using \"docker proxy\" and \"port mapping\" after 30k connections:\r\nhttps:\/\/github.com\/thbley\/docker-proxy\/actions\/runs\/8204298979\/job\/22438697761\r\n\r\n### Expected behavior\r\n\r\nconstant connection times using \"docker-proxy\" and \"port mapping\", no timeouts\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.9\r\n API version:       1.43\r\n Go version:        go1.20.13\r\n Git commit:        2936816\r\n Built:             Thu Feb  1 00:48:39 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.9\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.13\r\n  Git commit:       fca702d\r\n  Built:            Thu Feb  1 00:48:39 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\ngithub.com\/docker\/buildx v0.12.1 30feaa1a915b869ebc2eea6328624b49facd4bfb\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.9\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.3\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 14\r\n Server Version: 24.0.9\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-1015-azure\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 15.61GiB\r\n Name: fv-az735-962\r\n ID: 66cd06a6-b278-495c-ae3e-04e926acdc46\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: githubactions\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n![Screenshot_20240308_150613](https:\/\/github.com\/moby\/moby\/assets\/941223\/938ae341-8af2-47f7-9e3f-82bca11e8d52)\r\n","comments":["As mentioned in https:\/\/github.com\/docker\/docs\/issues\/17312 , I disabled docker-proxy (= userland-proxy) locally on Ubuntu 22.04 (linux-image-6.2.0-36-generic) using:\r\n\r\n```\r\ncat \/etc\/docker\/daemon.json \r\n{\r\n        \"userland-proxy\": false\r\n}\r\n\r\nsudo \/etc\/init.d\/docker stop\r\nsudo \/etc\/init.d\/docker start\r\n```\r\n\r\nWith this change, the docker-proxy process is no longer started and the test finishes successfully with very good performance:\r\n(with docker-proxy enabled, the test fails)\r\n\r\n```\r\ndocker run --rm -it --name redis -d -p 6379:6379 redis:7.2-alpine\r\nsleep 5\r\ntime docker run --rm -it --network host -v$(pwd):\/var\/www redistest php test.php\r\n0.4921ms\r\n922.2383ms\r\n895.3366ms\r\n1017.895ms\r\n936.6858ms\r\n948.3333ms\r\n870.9774ms\r\n893.5945ms\r\n918.2863ms\r\n891.0484ms\r\n\r\nreal    0m9,789s\r\nuser    0m0,016s\r\nsys     0m0,017s\r\n```\r\n\r\nRegarding github actions, I used:\r\n\r\n```\r\n    - name: Disable userland proxy in docker daemon\r\n      run: |\r\n        sudo jq '.+{\"userland-proxy\":false}' \/etc\/docker\/daemon.json > \/tmp\/daemon.json\r\n        sudo mv \/tmp\/daemon.json \/etc\/docker\/daemon.json\r\n        sudo systemctl restart docker.service\r\n```"],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/performance","area\/networking\/proxy","version\/24.0"]},{"title":"feat: ctx to client API","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\nThis is a breaking change to the Docker Client APIs as consumers of the API will be forced to add the `ctx` parameter to their callback functions.\r\n\r\nThis PR is a small piece of https:\/\/github.com\/moby\/moby\/pull\/47518. \r\n\r\n**- What I did**\r\nAdd ctx to the client API functions used inside the Docker CLI.\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\nDocker Client API callback functions `client.RequestPrivilegeFunc`, `client.ImageSearchOptions.AcceptPermissionsFunc` and `image.ImportOptions.PrivilegeFunc` require a context parameter.\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["I really have a feeling something's broken in GitHub actions; I noticed this before: it shows \"approval is needed\" for actions to run, but it still runs most (all??) of them. It mentions \"2 workflows awaiting approval\", but can't see immediately... which ones didn't run?\r\n\r\n<img width=\"853\" alt=\"Screenshot 2024-03-14 at 16 11 06\" src=\"https:\/\/github.com\/moby\/moby\/assets\/1804568\/72640e51-8189-4124-9f66-8e03e20de7c3\">\r\n","I think the missing ones are the `validate-pr` checks that were recently added","Would it be possible to get this one merged since v26 has been released? "],"labels":["area\/api","area\/distribution","status\/2-code-review","impact\/changelog","kind\/refactor","area\/authentication"]},{"title":"[WIP] c8d: Multi-platform image list","body":"- needs: https:\/\/github.com\/moby\/moby\/pull\/47449\r\n- related to: https:\/\/github.com\/moby\/moby\/issues\/44582#issuecomment-1456938816\r\n\r\nAdd the `PlatformImages` field to `ImageSummary`.\r\nThe original proposal was to enhance `ImageSummary` with `Platform ocispec.Platform` and `Images []ImageSummary` fields, but IMO having a separate type gives us more flexibility.\r\n\r\nI made a CLI plugin with a prototype implementation of the mockup: https:\/\/github.com\/vvoland\/docker-plugin-imgx.\r\nIt only implements the full tree view, no filters are supported etc.\r\n\r\n**- How to verify it**\r\n```bash\r\n$ echo 'FROM docker.io\/pawelgronowski465\/docker-plugin-imgx' | docker buildx build - -o type=local,dest=$HOME\/.docker\/cli-plugins --pull\r\n$ docker pull busybox\r\n$ docker imgx list\r\n```\r\n<img width=\"466\" src=\"https:\/\/github.com\/moby\/moby\/assets\/5046555\/acf679ff-9c8b-464f-8ceb-828c53ea4ce4\">\r\n\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\n- containerd image store: Make `docker images` output multi-platform aware\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n![a7c73b012f](https:\/\/github.com\/moby\/moby\/assets\/5046555\/01556436-d375-4617-a2f7-f3a5741c82f6)\r\n","comments":[],"labels":["area\/api","kind\/enhancement","kind\/feature","impact\/changelog","area\/images","containerd-integration"]},{"title":"`docker build -o type=local` doesn't work with Windows images","body":"### Description\r\n\r\nLocal exporter when extracting Windows images on Linux with containerd integration enabled.\r\nWorks fine with graphdrivers.\r\n\r\n### Reproduce\r\n\r\n```bash\r\n\u2771 echo 'FROM dockereng\/cli-bin:master' | docker buildx build - -o type=local,dest=asdf --platform windows\/arm64\r\n[+] Building 1.5s (5\/5) FINISHED                                                                                 docker:desktop-linux\r\n => [internal] load build definition from Dockerfile                                                                             0.0s\r\n => => transferring dockerfile: 67B                                                                                              0.0s\r\n => [internal] load metadata for docker.io\/dockereng\/cli-bin:master                                                              1.4s\r\n => [internal] load .dockerignore                                                                                                0.0s\r\n => => transferring context: 2B                                                                                                  0.0s\r\n => CACHED [1\/1] FROM docker.io\/dockereng\/cli-bin:master@sha256:0013ce37d4702d7742fc5e36dffa32a95240e472869d7efd6a4559f6a422759  0.0s\r\n => => resolve docker.io\/dockereng\/cli-bin:master@sha256:0013ce37d4702d7742fc5e36dffa32a95240e472869d7efd6a4559f6a422759c        0.0s\r\n => exporting to client directory                                                                                                0.0s\r\n => => copying files                                                                                                             0.0s\r\n\r\nView build details: docker-desktop:\/\/dashboard\/build\/desktop-linux\/desktop-linux\/ssthmlvujk5p89u2icvala1fs\r\n\u2771 ls asdf\r\n\u2771 file asdf\/docker\r\nasdf\/docker: cannot open `asdf\/docker' (No such file or directory)\r\n\r\n\u2771 echo 'FROM dockereng\/cli-bin:master' | docker buildx build - -o type=local,dest=asdf --platform linux\/amd64\r\n[+] Building 10.5s (5\/5) FINISHED                                                                                docker:desktop-linux\r\n => [internal] load build definition from Dockerfile                                                                             0.0s\r\n => => transferring dockerfile: 67B                                                                                              0.0s\r\n => [internal] load metadata for docker.io\/dockereng\/cli-bin:master                                                              3.6s\r\n => [internal] load .dockerignore                                                                                                0.0s\r\n => => transferring context: 2B                                                                                                  0.0s\r\n => [1\/1] FROM docker.io\/dockereng\/cli-bin:master@sha256:0013ce37d4702d7742fc5e36dffa32a95240e472869d7efd6a4559f6a422759c        6.6s\r\n => => resolve docker.io\/dockereng\/cli-bin:master@sha256:0013ce37d4702d7742fc5e36dffa32a95240e472869d7efd6a4559f6a422759c        0.0s\r\n => => sha256:1ea87b8314781767f86078ecf38d53aa7b2a95758fd8140dfbe16933dbb8ef43 16.92MB \/ 16.92MB                                 6.5s\r\n => => extracting sha256:1ea87b8314781767f86078ecf38d53aa7b2a95758fd8140dfbe16933dbb8ef43                                        0.1s\r\n => exporting to client directory                                                                                                6.8s\r\n => => copying files 35.69MB                                                                                                     0.2s\r\n\r\nView build details: docker-desktop:\/\/dashboard\/build\/desktop-linux\/desktop-linux\/ljkh0c3fuj0opvaxxqhyeyc62\r\n\r\n\u2771 file asdf\/docker\r\nasdf\/docker: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=All6rb_OcZQ2nE7SLxqV\/-bpdsBaI9sb8Mm_yLoJ2\/YO3LzdIKq9J6twgR7AcS\/1FGP4NGXkCE_xuZ8VjI1, BuildID[xxHash]=40b5133172e66d53, with debug_info, not stripped\r\n\r\n\u2771 ls asdf\r\ndocker*\r\n```\r\n\r\n### Expected behavior\r\n\r\nWindows images should be unpacked\r\n\r\n### docker version\r\n\r\n```bash\r\n25.0.3 and master @ 6c10086976d07d4746e03dcfd188972a2f07e1c9\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nN\/A\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["kind\/bug","containerd-integration","version\/25.0"]},{"title":"feat: more context on docker client","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\nThis is a WIP and a breaking change to the docker client APIs. Most likely required by OTEL for better tracing.\r\n\r\nRelates to \r\n- https:\/\/github.com\/docker\/cli\/pull\/4698\r\n- https:\/\/github.com\/docker\/cli\/pull\/4774\r\n\r\n**- What I did**\r\n\r\nAdd context to more of the docker client function calls.\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\n\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/api","kind\/enhancement"]},{"title":"`docker load` fails with lsetxattr com.apple.provenance \/manifest.json: operation not supported on Ventura & later","body":"### Description\n\nStarting with macOS Ventura 13, macOS adds additional xattr metadata to downloaded files. The following article explains this in more detail: https:\/\/eclecticlight.co\/2023\/05\/10\/how-macos-now-tracks-the-provenance-of-apps\/\r\n\r\nWe've observed on some systems, when building OCI images (via Bazel), we would get the error in the title when attempting to `docker load --input <file>` the tarball. To workaround this issue, the affected users have had to untar the image, and retar using `tar --no-xattr`.\n\n### Reproduce\n\nIf your system doesn't set these attributes already, you can recreate it with any existing image tarball:\r\n```\r\n$ tar xf <image_tarball_file>\r\n$ xattr -w com.apple.provenance 'sample text' manifest.json\r\n$ tar cf output.tar *\r\n$ docker load --input output.tar\r\nlsetxattr com.apple.provenance \/manifest.json: operation not supported\r\n```\r\n\r\nLink to an affected image tarball for convenience (GH won't allow me to attach a tar file): https:\/\/cdn.discordapp.com\/attachments\/312648998328729603\/1214914140821258320\/tarball.tar. Built from `bazel build \/\/cmd\/batcheshelper:image_tarball` in https:\/\/github.com\/sourcegraph\/sourcegraph\n\n### Expected behavior\n\n`docker load` ignores unsupported xattrs, similarly to https:\/\/github.com\/moby\/moby\/pull\/47483\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.10\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:26 2024\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.27.2 (137060)\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:22 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    25.0.3\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1-desktop.4\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5-desktop.1\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-compose\r\n  debug: Get a shell into any image or container. (Docker Inc.)\r\n    Version:  0.0.24\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-debug\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  v1.0.4\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v1.0.0\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-sbom\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.4.1\r\n    Path:     \/Users\/noah\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 21\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.6.12-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 10\r\n Total Memory: 5.79GiB\r\n Name: docker-desktop\r\n ID: b63f794a-cd4c-467e-b52e-5e02df615db9\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Similar issue w latest Docker desktop on OSX M3.\r\n\r\n \/bin\/bash -c '\/Users\/xxx\/tmp\/bazel\/base\/execroot\/__main__\/bazel-out\/darwin_arm64-fastbuild\/bin\/test\/integration\/intenv\/intenv_\/intenv \/Users\/xxx\/tmp\/bazel\/base\/execroot\/__main__\/bazel-out\/darwin_arm64-fastbuild-ST-4a519fd6d3e4\/bin\/cdr\/cdr-events-in\/container_image.executable '\r\nlsetxattr com.apple.provenance \/f52af47487c7689b94bd0d30141a31953d1e9ff251a758202fc41a45a14a1f9f.tar: operation not supported\r\ntar: Write error\r\n\r\nTo fix the issue, rolling back to\r\n\r\n```\r\n\u2717 docker version\r\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:04:20 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.26.1 (131620)\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:15 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.25\r\n  GitCommit:        d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\nAnd it works again.","Thanks for reporting. I think this is related to a bug-fix where previously failures on applying extended-attributes were silently ignored, which in some cases could result to an image that was silently broken (due to those attributes getting lost in the process). The expectation for `docker load` from a tarball is to restore the filesystem from the tarball, including attributes set on the file. In this case, the generated tarbal contains extended-attributes that are specific to macOS, and that cannot be applied to Linux filesystems (causing an error to be produced). Admitted, handling of extended attributes is not well-defined in the OCI image specification (there's some discussion ongoing in this area to have a better definition).\r\n\r\nThere's also a difference between Linux and macOS (bsd) tar in handling extended attributes. On Linux, creating a tar by default omits all extended attributes, whereas on macOS the reverse is true, and extended-attributes are included by default. If the files you're adding to the tar are not expected to have extended attributes set that must be preserved (i.e., if they only contain extended attributes added by macOS), then I would recommend using the `--no-xattrs` options when creating the archive. Also see https:\/\/github.com\/moby\/moby\/issues\/47137#issuecomment-1905860170\r\n\r\nLooking at your report though, it looks like the failure is happening on the `manifest.json`, which is not part of the the image rootfs, so perhaps there's something to address here to exclude such files.\r\n\r\n\r\ncc @vvoland @neersighted @corhere ","In contrast to #47483, which had to do with authoring of images where the _source inputs_ were tarballs containing xattrs; this issue has to do with the importing of _already-authored_ images. Images are an interchange format authored by specialized tooling so it would not necessarily be unreasonable for `docker load` to be stricter than a Dockerfile `ADD` instruction about what is acceptable. Furthermore, `manifest.json` is the \"Docker-compatible manifest\" which is part of the Docker image repository archive format, not the OCI Image specification. Correct me if I'm wrong, but I suspect dockerd is the only consumer of `manifest.json` files in image repository archives. I could not find a formal specification for the Docker image repository archive format (only the contents of a distribution manifest) so assuming one does not exist, the specification is de facto \"what `docker save` produces and what `docker load` accepts.\" _If_ all my preceding speculation is true, we would be entirely within our rights to say that Moby is working as intended and the bug is entirely within rules_oci for authoring a Docker-compatible image repository archive which is incompatible with Docker. We would _also_ be entirely within our rights to unilaterally decide to be more liberal in what we accept and change `docker load` to ignore file xattrs on `manifest.json`. Barring new information, neither option is more technically correct than the other so we will have to decide on a course of action some other way.\r\n\r\nMy read on this issue is that it mostly only impacts direct users of rules_oci -- image authors, by definition. End-users pulling images from registries will not be affected due to how image manifest lists are returned directly in HTTP responses when pulling images rather than being packed into tarballs. Given the users impacted are limited to direct users of the image authoring tooling and the fact that [the root cause is being addressed in the authoring tooling](https:\/\/github.com\/bazel-contrib\/rules_oci\/pull\/385), I am inclined to consider this a bug in rules_oci which Moby should not work around.","> I could not find a formal specification for the Docker image repository archive format (only the contents of a distribution manifest) so assuming one does not exist, the specification is de facto \"what docker save produces and what docker load accepts.\"\r\n\r\nWe have \r\nhttps:\/\/github.com\/moby\/docker-image-spec\/blob\/main\/spec.md#combined-image-json--filesystem-changeset-format - but it doesn't cover this specific subject.\r\n\r\nAlso, this behavior is strictly implementation-specific due to how `archive` package used by graphdriver works. The `manifest.json` isn't expected to be persisted, but is still extracted into a temporary daemon directory using the same tar unpack code which also attempts to restore the xattrs.\r\n\r\nIt wouldn't happen with the containerd image store backend enabled, because it reads the `manifest.json` directly from the tar and only cares about its content."],"labels":["status\/0-triage","kind\/bug","version\/25.0"]},{"title":"Update GoDoc for ioutils (AtomicWriteFile) to document expected behavior","body":"### Description\n\n- Relates to https:\/\/github.com\/moby\/moby\/issues\/47498#issuecomment-1978920171\r\n\r\nThe `ioutils` utilities have different semantics than Go stdlib file utilities; unlike stdlib, these functions do not depend on the active umask. We should update their GoDoc to document the behavior, so that consumers are aware of their semantics.\r\n\r\nAlternatively, we can change their behavior to take `umassk` into account, but this would be a breaking change, so not sure if we want to go that direction.\r\n","comments":[],"labels":["kind\/enhancement","exp\/beginner","exp\/intermediate"]},{"title":"api\/image\/list: Add `container-count` parameter","body":"- related to: https:\/\/github.com\/moby\/moby\/pull\/42531\r\n\r\nThis parameter was already supported for some time in the backend (for purposes related to `docker system prune`).\r\nIt was also already present in the `imagetypes.ListOptions` but was never actually handled by the client.\r\n\r\nThis makes the client respect the `ContainerCount` and set the form field when sending the request.\r\n\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\n`GET \/images\/json` now supports the `container-count` parameter which makes the response fill the `Containers` field with a count of containers using the image.  \r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5046555\/8ba4e302-f31b-4cea-ab6b-a525c7ce0bfd)\r\n","comments":["This looks to be similar to https:\/\/github.com\/moby\/moby\/pull\/43350 - do we have a use-case for this, or should we consider going the other direction (make sure we properly define types for each purpose). ISTR some of the `usage` types were just reusing existing types because someone was lazy to define separate ones \ud83d\ude1e ","We don't have an usage yet, I was mostly just fiddling around the multi-platform `image list`, and wanted to use the containers count as an indicator whether the image is used or not, but noticed that setting this field client-side doesn't actually do anything \ud83d\ude48 "],"labels":["area\/api","status\/2-code-review","kind\/enhancement","impact\/api","impact\/changelog","area\/images"]},{"title":"seccomp: allow specifying a custom profile with `--privileged`","body":"\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n\r\nFix #47499\r\n\r\n**- How I did it**\r\n\r\nUpdated `daemon\/seccomp_linux.go`\r\n\r\n**- How to verify it**\r\n\r\n```console\r\n$ cat <<EOF >foo.json\r\n{\r\n \"defaultAction\": \"SCMP_ACT_ALLOW\",\r\n \"syscalls\": [ { \"names\": [ \"connect\" ], \"action\": \"SCMP_ACT_ERRNO\" } ]\r\n}\r\nEOF\r\n\r\n$ docker run --security-opt seccomp=foo.json --privileged busybox wget -O- http:\/\/1.1.1.1\/\r\nConnecting to 1.1.1.1 (1.1.1.1:80)\r\nwget: can't connect to remote host (1.1.1.1): Operation not permitted\r\n```\r\n\"Operation not permitted\" is the expected error here.\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog.\r\nIt must be placed inside the below triple backticks section:\r\n-->\r\n```markdown changelog\r\nseccomp: allow specifying a custom profile with `--privileged`\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\ud83d\udc27 \r\n","comments":["To clarify the background, I'm proposing this for capturing socket syscalls with `SCMP_ACT_NOTIFY` to accelerate rootless containers: https:\/\/github.com\/rootless-containers\/bypass4netns\r\n\r\nThis is not an attempt to harden privileged containers beyond as it is.","@neersighted do you have a strong objection against this one? Overall, I think it's fine to do this, and it seems like a more logical option than silently discarding the user's preference (we should either _error_ or _accept_, not \"discard\").\r\n\r\nWe should make sure that we're clear in the docs what the expectation is though; @dvdksn has a PR pending in the CLI;\r\n\r\n- https:\/\/github.com\/docker\/cli\/pull\/4929\r\n\r\nI think that one is \"good enough\" for now, but we should probably outline that _even if_ we accept options like this, a `--privileged` container won't be able to provide full guarantees, as the reduced security could allow a compromised container to \"undo\" some of the (security) options that were set. (e.g., escaping the container's filesystem, and modify the container's config).\r\n "],"labels":["status\/2-code-review","impact\/changelog","area\/security\/seccomp","impact\/documentation"]},{"title":"`--privileged --security-opt seccomp=<CUSTOM.json>` ignores `<CUSTOM.json>`","body":"### Description\r\n\r\n`docker run --privileged --security-opt seccomp=<CUSTOM.json>` ignores `<CUSTOM.json>`\r\n\r\n### Reproduce\r\n\r\n1. Create `foo.json`:\r\n ```json\r\n{\r\n  \"defaultAction\": \"SCMP_ACT_ALLOW\",\r\n  \"syscalls\": [ { \"names\": [ \"connect\" ], \"action\": \"SCMP_ACT_ERRNO\" } ]\r\n}\r\n```\r\n\r\n2. Make sure that `docker run --security-opt seccomp=foo.json busybox wget -O- http:\/\/1.1.1.1\/` fails:\r\n```console\r\n$ docker run --security-opt seccomp=foo.json busybox wget -O- http:\/\/1.1.1.1\/\r\nConnecting to 1.1.1.1 (1.1.1.1:80)\r\nwget: can't connect to remote host (1.1.1.1): Operation not permitted\r\n```\r\n\r\n3. Repeat the previous step, but with `--privileged`\r\n\r\n### Expected behavior\r\n\r\nExpected behavior: it should consume `foo.json` and let `wget` fail\r\n\r\nActual behavior: it does not consume `foo.json`, so `wget` does not fail\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Thu Feb  1 00:22:57 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.2\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       fce6e0c\r\n  Built:            Thu Feb  1 00:22:57 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: 25.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-92-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 3.818GiB\r\n Name: lima-vm0\r\n ID: 095cc593-bf50-4b63-9ebb-6168ad3a60e4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n- I assume this is *not* a vulnerability, as `--privileged` is never intended to be secure\r\n- `--security-opt apparmor=<CUSTOM>` doesn't seem ignored","comments":[],"labels":["status\/0-triage","kind\/enhancement","area\/security\/seccomp"]},{"title":"Swarm networking broken on Raspberry Pi ARMv6","body":"### Description\r\n\r\nSwarm services always time out with no response because the IPVS socket is not available.\r\n\r\n### Reproduce\r\n\r\n`docker swarm init`\r\n`docker service create --name nginx -p 80:80 nginx`\r\n\r\n### Expected behavior\r\n\r\nShould just work.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf411d1e6efbd9ce65e4250718e9c529a6525\r\n Built:             Sat Feb 10 05:14:21 2024\r\n OS\/Arch:           linux\/arm\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435e5f6216828dec57958c490c4f8bae4f98\r\n  Built:            Sat Feb 10 05:14:21 2024\r\n  OS\/Arch:          linux\/arm\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.10\r\n  GitCommit:        4e1fe7492b9df85914c389d1f15a3ceedbb280ac\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: active\r\n  NodeID: qj3t5xsw04ej9usriymv1ce5i\r\n  Is Manager: true\r\n  ClusterID: s6qmpae77llkj3hnjivvqithz\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.0.200\r\n  Manager Addresses:\r\n   10.0.0.200:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 4e1fe7492b9df85914c389d1f15a3ceedbb280ac\r\n runc version: 51d5e94601ceffbbd85688df1c928ecccbfa4685\r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.6.14-0-rpi\r\n Operating System: Alpine Linux v3.19\r\n OSType: linux\r\n Architecture: armv6l\r\n CPUs: 1\r\n Total Memory: 428.7MiB\r\n Name: rpi0\r\n ID: b2a7e3c9-f0f4-4753-987f-bbfab3ec6de1\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\nWARNING: No cpuset support\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nDocker log\r\n```\r\ntime=\"2024-01-11T15:38:20.349979587+01:00\" level=info msg=\"Starting up\"\r\ntime=\"2024-01-11T15:38:20.419691710+01:00\" level=info msg=\"containerd not running, starting managed containerd\"\r\ntime=\"2024-01-11T15:38:20.495338299+01:00\" level=info msg=\"started new containerd process\" address=\/var\/run\/docker\/containerd\/containerd.sock module=libcontainerd pid=4439\r\ntime=\"2024-01-11T15:38:20.834853126+01:00\" level=info msg=\"starting containerd\" revision=4e1fe7492b9df85914c389d1f15a3ceedbb280ac version=v1.7.10\r\ntime=\"2024-01-11T15:38:21.901701009+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.aufs\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.923674140+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.aufs\\\"...\" error=\"aufs is not supported (modprobe aufs failed: exit status 1 \\\"modprobe: FATAL: Module aufs not found in directory \/lib\/modules\/6.6.14-0-rpi\\\\n\\\"): skip plugin\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.925220048+01:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.exchange\\\"...\" type=io.containerd.event.v1\r\ntime=\"2024-01-11T15:38:21.925946961+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.opt\\\"...\" type=io.containerd.internal.v1\r\ntime=\"2024-01-11T15:38:21.927112621+01:00\" level=info msg=\"loading plugin \\\"io.containerd.warning.v1.deprecations\\\"...\" type=io.containerd.warning.v1\r\ntime=\"2024-01-11T15:38:21.927911033+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.blockfile\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.932812007+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.blockfile\\\"...\" error=\"no scratch file generator: skip plugin\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.933698169+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.btrfs\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.935859407+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.btrfs\\\"...\" error=\"path \/var\/lib\/docker\/containerd\/daemon\/io.containerd.snapshotter.v1.btrfs (ext4) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.936620986+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.devmapper\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.937326232+01:00\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.devmapper\" error=\"devmapper not configured\"\r\ntime=\"2024-01-11T15:38:21.937913396+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.native\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.939956551+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.overlayfs\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.943958363+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.zfs\\\"...\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.945899686+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.zfs\\\"...\" error=\"path \/var\/lib\/docker\/containerd\/daemon\/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin\" type=io.containerd.snapshotter.v1\r\ntime=\"2024-01-11T15:38:21.946626599+01:00\" level=info msg=\"loading plugin \\\"io.containerd.content.v1.content\\\"...\" type=io.containerd.content.v1\r\ntime=\"2024-01-11T15:38:21.948415172+01:00\" level=info msg=\"loading plugin \\\"io.containerd.metadata.v1.bolt\\\"...\" type=io.containerd.metadata.v1\r\ntime=\"2024-01-11T15:38:21.949305667+01:00\" level=warning msg=\"could not use snapshotter devmapper in metadata plugin\" error=\"devmapper not configured\"\r\ntime=\"2024-01-11T15:38:21.949893914+01:00\" level=info msg=\"metadata content store policy set\" policy=shared\r\ntime=\"2024-01-11T15:38:21.952659649+01:00\" level=info msg=\"loading plugin \\\"io.containerd.gc.v1.scheduler\\\"...\" type=io.containerd.gc.v1\r\ntime=\"2024-01-11T15:38:21.958987365+01:00\" level=info msg=\"loading plugin \\\"io.containerd.differ.v1.walking\\\"...\" type=io.containerd.differ.v1\r\ntime=\"2024-01-11T15:38:21.961949182+01:00\" level=info msg=\"loading plugin \\\"io.containerd.lease.v1.manager\\\"...\" type=io.containerd.lease.v1\r\ntime=\"2024-01-11T15:38:21.963992338+01:00\" level=info msg=\"loading plugin \\\"io.containerd.streaming.v1.manager\\\"...\" type=io.containerd.streaming.v1\r\ntime=\"2024-01-11T15:38:21.965150415+01:00\" level=info msg=\"loading plugin \\\"io.containerd.runtime.v1.linux\\\"...\" type=io.containerd.runtime.v1\r\ntime=\"2024-01-11T15:38:21.966403825+01:00\" level=info msg=\"loading plugin \\\"io.containerd.monitor.v1.cgroups\\\"...\" type=io.containerd.monitor.v1\r\ntime=\"2024-01-11T15:38:21.975362943+01:00\" level=info msg=\"loading plugin \\\"io.containerd.runtime.v2.task\\\"...\" type=io.containerd.runtime.v2\r\ntime=\"2024-01-11T15:38:21.978285760+01:00\" level=info msg=\"loading plugin \\\"io.containerd.runtime.v2.shim\\\"...\" type=io.containerd.runtime.v2\r\ntime=\"2024-01-11T15:38:21.979054923+01:00\" level=info msg=\"loading plugin \\\"io.containerd.sandbox.store.v1.local\\\"...\" type=io.containerd.sandbox.store.v1\r\ntime=\"2024-01-11T15:38:21.979820835+01:00\" level=info msg=\"loading plugin \\\"io.containerd.sandbox.controller.v1.local\\\"...\" type=io.containerd.sandbox.controller.v1\r\ntime=\"2024-01-11T15:38:21.980567248+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.containers-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.981509743+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.content-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.982589820+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.diff-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.983522565+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.images-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.984277644+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.introspection-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.985006724+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.namespaces-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.985720637+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.snapshots-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.986838631+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.tasks-service\\\"...\" type=io.containerd.service.v1\r\ntime=\"2024-01-11T15:38:21.987780042+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.containers\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.988535121+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.content\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.989250118+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.diff\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.990049613+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.events\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.990809026+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.images\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.991608521+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.introspection\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.992331101+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.leases\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.993419845+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.namespaces\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.994200924+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.sandbox-controllers\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.995095752+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.sandboxes\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.995847582+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.snapshots\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.996552828+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.streaming\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.997553822+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.tasks\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:21.998365235+01:00\" level=info msg=\"loading plugin \\\"io.containerd.transfer.v1.local\\\"...\" type=io.containerd.transfer.v1\r\ntime=\"2024-01-11T15:38:21.999289313+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.transfer\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:22.000027059+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.version\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:22.000784305+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.restart\\\"...\" type=io.containerd.internal.v1\r\ntime=\"2024-01-11T15:38:22.002011715+01:00\" level=info msg=\"loading plugin \\\"io.containerd.tracing.processor.v1.otlp\\\"...\" type=io.containerd.tracing.processor.v1\r\ntime=\"2024-01-11T15:38:22.003113459+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.tracing.processor.v1.otlp\\\"...\" error=\"no OpenTelemetry endpoint: skip plugin\" type=io.containerd.tracing.processor.v1\r\ntime=\"2024-01-11T15:38:22.003777539+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.tracing\\\"...\" type=io.containerd.internal.v1\r\ntime=\"2024-01-11T15:38:22.004470868+01:00\" level=info msg=\"skipping tracing processor initialization (no tracing plugin)\" error=\"no OpenTelemetry endpoint: skip plugin\"\r\ntime=\"2024-01-11T15:38:22.005766528+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.healthcheck\\\"...\" type=io.containerd.grpc.v1\r\ntime=\"2024-01-11T15:38:22.006338525+01:00\" level=info msg=\"loading plugin \\\"io.containerd.nri.v1.nri\\\"...\" type=io.containerd.nri.v1\r\ntime=\"2024-01-11T15:38:22.006744773+01:00\" level=info msg=\"NRI interface is disabled by configuration.\"\r\ntime=\"2024-01-11T15:38:22.011387914+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd-debug.sock\r\ntime=\"2024-01-11T15:38:22.012968489+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd.sock.ttrpc\r\ntime=\"2024-01-11T15:38:22.014401731+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd.sock\r\ntime=\"2024-01-11T15:38:22.015235893+01:00\" level=info msg=\"containerd successfully booted in 1.192351s\"\r\ntime=\"2024-01-11T15:38:23.716939668+01:00\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\r\ntime=\"2024-01-11T15:38:23.771135291+01:00\" level=info msg=\"Loading containers: start.\"\r\ntime=\"2024-01-11T15:38:25.499868086+01:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0\/16. Daemon option --bip can be used to set a preferred IP address\"\r\ntime=\"2024-01-11T15:38:26.027095395+01:00\" level=info msg=\"Loading containers: done.\"\r\ntime=\"2024-01-11T15:38:26.195469399+01:00\" level=warning msg=\"WARNING: No memory limit support\"\r\ntime=\"2024-01-11T15:38:26.196900474+01:00\" level=warning msg=\"WARNING: No swap limit support\"\r\ntime=\"2024-01-11T15:38:26.198133301+01:00\" level=warning msg=\"WARNING: No cpuset support\"\r\ntime=\"2024-01-11T15:38:26.199584960+01:00\" level=info msg=\"Docker daemon\" commit=f417435e5f6216828dec57958c490c4f8bae4f98 containerd-snapshotter=false storage-driver=overlay2 version=25.0.3\r\ntime=\"2024-01-11T15:38:26.201833948+01:00\" level=info msg=\"Daemon has completed initialization\"\r\ntime=\"2024-01-11T15:38:27.023632076+01:00\" level=info msg=\"API listen on \/var\/run\/docker.sock\"\r\ntime=\"2024-01-11T15:38:37.743467130+01:00\" level=info msg=\"Listening for connections\" addr=\"[::]:2377\" module=node node.id=qj3t5xsw04ej9usriymv1ce5i proto=tcp\r\ntime=\"2024-01-11T15:38:37.744535291+01:00\" level=info msg=\"Listening for local connections\" addr=\/var\/run\/docker\/swarm\/control.sock module=node node.id=qj3t5xsw04ej9usriymv1ce5i proto=unix\r\ntime=\"2024-01-11T15:38:37.772356223+01:00\" level=info msg=\"233836300a5a8674 switched to configuration voters=()\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.773125386+01:00\" level=info msg=\"233836300a5a8674 became follower at term 0\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.773794882+01:00\" level=info msg=\"newRaft 233836300a5a8674 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.774320296+01:00\" level=info msg=\"233836300a5a8674 became follower at term 1\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.775227041+01:00\" level=info msg=\"233836300a5a8674 switched to configuration voters=(2537837969983309428)\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.797855585+01:00\" level=info msg=\"233836300a5a8674 switched to configuration voters=(2537837969983309428)\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.798779663+01:00\" level=info msg=\"233836300a5a8674 is starting a new election at term 1\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.799329994+01:00\" level=info msg=\"233836300a5a8674 became candidate at term 2\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.799858657+01:00\" level=info msg=\"233836300a5a8674 received MsgVoteResp from 233836300a5a8674 at term 2\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.800432821+01:00\" level=info msg=\"233836300a5a8674 became leader at term 2\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.800972318+01:00\" level=info msg=\"raft.node: 233836300a5a8674 elected leader 233836300a5a8674 at term 2\" module=raft node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.805777959+01:00\" level=info msg=\"Creating default ingress network\" module=node node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.814945076+01:00\" level=info msg=\"leadership changed from not yet part of a raft cluster to qj3t5xsw04ej9usriymv1ce5i\" module=node node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:37.822411369+01:00\" level=info msg=\"dispatcher starting\" module=dispatcher node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:38.165685341+01:00\" level=info msg=\"manager selected by agent for new session: { }\" module=node\/agent node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:38.167968995+01:00\" level=info msg=\"waiting 0s before registering session\" module=node\/agent node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:38.289265921+01:00\" level=info msg=\"worker qj3t5xsw04ej9usriymv1ce5i was successfully registered\" method=\"(*Dispatcher).register\"\r\ntime=\"2024-01-11T15:38:38.317895016+01:00\" level=info msg=\"initialized VXLAN UDP port to 4789 \" module=node node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:38.319895922+01:00\" level=info msg=\"Initializing Libnetwork Agent\" advertise-addr=10.0.0.200 data-path-addr= listen-addr=0.0.0.0 local-addr=10.0.0.200 network-control-plane-mtu=1500 remote-addr-list=\"[]\"\r\ntime=\"2024-01-11T15:38:38.321285831+01:00\" level=info msg=\"New memberlist node - Node:rpi0 will use memberlist nodeID:792ec8ec68d6 with config:&{NodeID:792ec8ec68d6 Hostname:rpi0 BindAddr:0.0.0.0 AdvertiseAddr:10.0.0.200 BindPort:0 Keys:[[17 228 183 97 101 212 27 4 138 203 245 32 125 244 63 132] [102 159 9 215 55 38 171 6 86 161 126 2 80 1 167 66] [39 25 199 23 249 76 208 255 197 170 204 239 176 135 26 82]] PacketBufferSize:1400 reapEntryInterval:1800000000000 reapNetworkInterval:1825000000000 rejoinClusterDuration:10000000000 rejoinClusterInterval:60000000000 StatsPrintPeriod:5m0s HealthPrintPeriod:1m0s}\"\r\ntime=\"2024-01-11T15:38:38.326951633+01:00\" level=info msg=\"Node 792ec8ec68d6\/10.0.0.200, joined gossip cluster\"\r\ntime=\"2024-01-11T15:38:38.341700053+01:00\" level=info msg=\"Node 792ec8ec68d6\/10.0.0.200, added to nodes list\"\r\ntime=\"2024-01-11T15:38:38.447014649+01:00\" level=info msg=\"cluster update event\" module=dispatcher node.id=qj3t5xsw04ej9usriymv1ce5i\r\ntime=\"2024-01-11T15:38:39.048870220+01:00\" level=error msg=\"set bridge default vlan failed\" error=\"failed to enable default vlan on bridge br0: open \/sys\/class\/net\/br0\/bridge\/default_pvid: permission denied\"\r\ntime=\"2024-01-11T15:38:51.344737647+01:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\ntime=\"2024-01-11T15:38:51.347630131+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\ntime=\"2024-01-11T15:38:51.349401371+01:00\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\ntime=\"2024-01-11T15:38:51.354780092+01:00\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.pause\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\ntime=\"2024-01-11T15:38:54.981258849+01:00\" level=warning msg=\"error from *cgroupsv2.Manager.EventChan\" error=\"failed to add inotify watch for \\\"\/sys\/fs\/cgroup\/docker\/7dc9766d0d2352ac86cfa804c80ae20640b53001443696e65b4df33aa5b978ab\/memory.events\\\": no such file or directory\"\r\n```\r\n\r\nNmap query\r\n`80\/tcp open|filtered http no-response`\r\n","comments":[],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/d\/overlay"]},{"title":"docker-bake: Add `windows\/arm64` to binary-cross","body":"While we currently don't support `windows\/arm64` it's still valuable to have a \"it builds\" check in the CI.\r\n\r\n\r\n","comments":["I think it will be hard without aarch64 cross pkgs of mingw32 :sweat_smile:\r\n\r\nMaybe we could have a mingw-w64 wrapper in xx for this to install external [Cygwin packages](https:\/\/cygwin.com\/packages\/package_list.html) like `xx-cygwin` or https:\/\/mxe.cc\/#packages.","Would this work better if we had native arm64 runners? I guess we need to be patient then for those to arrive.","Hmm right \ud83d\ude48 \r\n(it works for me when building on Mac, so my guess is that native arm64 runners would also solve this)","> Would this work better if we had native arm64 runners? I guess we need to be patient then for those to arrive.\r\n\r\nYes I think that's fine but would break people invoking `bake binary-cross` on x86_64"],"labels":["platform\/windows","status\/2-code-review","area\/testing"]},{"title":"rootless (non-dpkg): update docs and `dockerd-rootless-setuptool.sh check` for Ubuntu 24.04  (`kernel.apparmor_restrict_unprivileged_userns`)","body":"### Description\n\nUbuntu 24.04 will enable [`kernel.apparmor_restrict_unprivileged_userns`](https:\/\/ubuntu.com\/blog\/ubuntu-23-10-restricted-unprivileged-user-namespaces) by default ([LP 2046477](https:\/\/bugs.launchpad.net\/ubuntu\/+source\/apparmor\/+bug\/2046477), [LP 2046844](https:\/\/bugs.launchpad.net\/ubuntu\/+source\/apparmor\/+bug\/2046844)), so rootless setup with `https:\/\/get.docker.com\/rootless` (non-dpkg) needs the following steps:\r\n```bash\r\nif [ ! -e \"\/etc\/apparmor.d\/usr.local.bin.rootlesskit\" ] && [ -e \"\/etc\/apparmor.d\/abi\/4.0\" ] && [ -e \"\/proc\/sys\/kernel\/apparmor_restrict_unprivileged_userns\" ]; then\r\n  cat >\"\/etc\/apparmor.d\/usr.local.bin.rootlesskit\" <<EOF\r\n# Ubuntu 23.10 introduced kernel.apparmor_restrict_unprivileged_userns\r\n# to restrict unsharing user namespaces:\r\n# https:\/\/ubuntu.com\/blog\/ubuntu-23-10-restricted-unprivileged-user-namespaces\r\n#\r\n# kernel.apparmor_restrict_unprivileged_userns is still opt-in in Ubuntu 23.10,\r\n# but it is expected to be enabled in future releases of Ubuntu.\r\nabi <abi\/4.0>,\r\ninclude <tunables\/global>\r\n\r\n\/usr\/local\/bin\/rootlesskit flags=(unconfined) {\r\n  userns,\r\n\r\n  # Site-specific additions and overrides. See local\/README for details.\r\n  include if exists <local\/usr.local.bin.rootlesskit>\r\n}\r\nEOF\r\n  systemctl restart apparmor.service\r\n```\r\n\r\nThis additional step is *not* needed for dpkg setup (`apt-get install docker-ce-rootless-extras`), as the `apparmor` package is shipped with the profile for `\/usr\/bin\/rootlesskit`:\r\nhttps:\/\/packages.ubuntu.com\/noble\/amd64\/apparmor\/filelist\r\n\r\n<details><summary><code>\/etc\/apparmor.d\/rootlesskit<\/code><\/summary><p>\r\n\r\n```\r\n# This profile allows everything and only exists to give the\r\n# application a name instead of having the label \"unconfined\"\r\n\r\nabi <abi\/4.0>,\r\ninclude <tunables\/global>\r\n\r\nprofile rootlesskit \/usr\/bin\/rootlesskit flags=(unconfined) {\r\n  userns,\r\n\r\n  # Site-specific additions and overrides. See local\/README for details.\r\n  include if exists <local\/rootlesskit>\r\n}\r\n```\r\n\r\n<\/p><\/details>","comments":["Reopening, as we haven't added docs yet\r\n","cc @dvdksn "],"labels":["impact\/documentation","area\/rootless"]},{"title":"Chrome behaves differently in Docker than on regular Alpine install; hangs when loading large image","body":"### Description\r\n\r\nWhen capturing a screenshot of an HTML page with a large jpeg image in it, instead of capturing a screenshot, Chrome will hang and refuse to respond to any further commands. This does not happen when running the same commands as in the Dockerfile on the host OS, this is only reproducible in the alpine-chrome Docker container.\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n[chrome-bug.zip](https:\/\/github.com\/moby\/moby\/files\/14454969\/chrome-bug.zip)\r\n\r\n1. Unzip included chrome-bug.zip file\r\n3. Run docker compose build ; docker compose up\r\n\r\n\r\n### Expected behavior\r\n\r\nThe line \"Screenshot captured\" will eventually be printed to the console\r\n\r\nActual behavior: Chrome will hang at some point in the loading process or when trying to take the screenshot itself. Once this happens the Chrome process seems to become totally unresponsive to any command sent by the remote interface.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:09 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:13:09 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```\r\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.6\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 12\r\n  Running: 10\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 16\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-97-generic\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 1.912GiB\r\n Name: edcom-lite\r\n ID: 28153a0b-6f21-4541-974f-6bef90078722\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nIf you edit the page.html file and change \"large.jpg\" to \"small.jpg\", the script runs as expected. This issue only seems to happen when viewing larger images, around 1MB or greater.\r\n\r\nThis bug happens both on ARM and x64 and on every recent version of Docker.","comments":["This might be related to differences between ulimits on host and container environment.\r\n\r\nPlease see: https:\/\/github.com\/moby\/moby\/issues\/47321#issuecomment-1946093183\r\n","I'm trying to set the ulimits to the host ulimits but it doesn't seem to want to increase the memlock limit past a set amount:\r\n\r\n```\r\n# cat docker-compose.yml \r\nservices:\r\n  screenshot:\r\n    image: chrome-bug\/screenshot\r\n    container_name: chrome-bug-screenshot\r\n    build:\r\n      context: .\r\n      dockerfile: screenshot.Dockerfile\r\n    ulimits:\r\n      core: 0\r\n      memlock: 250624\r\n      nofile: 1024\r\n      nproc: 8192\r\n  proxy:\r\n    image: nginx:1.23.3-alpine\r\n    container_name: chrome-bug-proxy\r\n    volumes: \r\n      - .:\/usr\/share\/nginx\/html\r\n\r\n# ulimit -a\r\nreal-time non-blocking time  (microseconds, -R) unlimited\r\ncore file size              (blocks, -c) 0\r\ndata seg size               (kbytes, -d) unlimited\r\nscheduling priority                 (-e) 0\r\nfile size                   (blocks, -f) unlimited\r\npending signals                     (-i) 7379\r\nmax locked memory           (kbytes, -l) 250624\r\nmax memory size             (kbytes, -m) unlimited\r\nopen files                          (-n) 1024\r\npipe size                (512 bytes, -p) 8\r\nPOSIX message queues         (bytes, -q) 819200\r\nreal-time priority                  (-r) 0\r\nstack size                  (kbytes, -s) 8192\r\ncpu time                   (seconds, -t) unlimited\r\nmax user processes                  (-u) 7379\r\nvirtual memory              (kbytes, -v) unlimited\r\nfile locks                          (-x) unlimited\r\n\r\n# docker exec -it chrome-bug-screenshot sh -c 'ulimit -a'\r\ncore file size (blocks)         (-c) 0\r\ndata seg size (kb)              (-d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size (blocks)              (-f) unlimited\r\npending signals                 (-i) 7379\r\nmax locked memory (kb)          (-l) 244\r\nmax memory size (kb)            (-m) unlimited\r\nopen files                      (-n) 1024\r\nPOSIX message queues (bytes)    (-q) 819200\r\nreal-time priority              (-r) 0\r\nstack size (kb)                 (-s) 8192\r\ncpu time (seconds)              (-t) unlimited\r\nmax user processes              (-u) 7379\r\nvirtual memory (kb)             (-v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\nThe ulimit \"max locked memory (kb)\" which is 64 by default has increased to 244, but isn't the value I'm setting (250624). \r\n","OK, it turns out the memlock setting in docker-compose.yml is actually in bytes for some reason, as setting it to 256638976 (250624 * 1024) works:\r\n\r\n```\r\n# cat docker-compose.yml \r\nservices:\r\n  screenshot:\r\n    image: chrome-bug\/screenshot\r\n    container_name: chrome-bug-screenshot\r\n    build:\r\n      context: .\r\n      dockerfile: screenshot.Dockerfile\r\n    ulimits:\r\n      core: 0\r\n      memlock: 256638976\r\n      nofile: 1024\r\n      nproc: 7379\r\n  proxy:\r\n    image: nginx:1.23.3-alpine\r\n    container_name: chrome-bug-proxy\r\n    volumes: \r\n      - .:\/usr\/share\/nginx\/html\r\n# docker exec -it chrome-bug-screenshot sh -c 'ulimit -a'\r\ncore file size (blocks)         (-c) 0\r\ndata seg size (kb)              (-d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size (blocks)              (-f) unlimited\r\npending signals                 (-i) 7379\r\nmax locked memory (kb)          (-l) 250624\r\nmax memory size (kb)            (-m) unlimited\r\nopen files                      (-n) 1024\r\nPOSIX message queues (bytes)    (-q) 819200\r\nreal-time priority              (-r) 0\r\nstack size (kb)                 (-s) 8192\r\ncpu time (seconds)              (-t) unlimited\r\nmax user processes              (-u) 7379\r\nvirtual memory (kb)             (-v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\nUnfortunately, even with the ulimits set to the host ulimits, the buggy behavior in Chrome persists :-(.","I also seem to remember (but I may be very wrong here!) that Chrome itself can use containers (sandboxes) on Linux for isolation. If it would, that could mean it's effectively doing a \"docker-in-docker\", which requires additional privileges, or would have to be disabled inside containers.\r\n\r\nI'd _expect_ that chrome base image you're using to be taking that into account, but just in case it's related \ud83d\ude48 ","> I also seem to remember (but I may be very wrong here!) that Chrome itself can use containers (sandboxes) on Linux for isolation. If it would, that could mean it's effectively doing a \"docker-in-docker\", which requires additional privileges, or would have to be disabled inside containers.\r\n\r\nGood point, but the Chrome process is being run with --no-sandbox, for whatever that's worth.\r\n\r\n","Ah! That's probably the option I had in mind (it's been a long time since I saw that discussion), so in that case ignore my previous comment!","One thing still worth testing (if you're comfortable with that in your setup) is to run the container with `seccomp` disabled (`--security-opt seccomp=unconfined`). There's been cases where seccomp-filtering blocked some syscall, which caused issues in combinations of container's userland and the kernel on the host.  (just in case that's related here).","Thank you for the suggestion. I added the seccomp option to docker-compose.yml but not observing any change in the behavior:\r\n\r\n```\r\nservices:\r\n  screenshot:\r\n    image: chrome-bug\/screenshot\r\n    container_name: chrome-bug-screenshot\r\n    build:\r\n      context: .\r\n      dockerfile: screenshot.Dockerfile\r\n    ulimits:\r\n      core: 0\r\n      memlock: 256638976\r\n      nofile: 1024\r\n      nproc: 7379\r\n    security_opt:\r\n      - seccomp:unconfined\r\n```","Update, AlecBlance figured this out:\r\n\r\nhttps:\/\/github.com\/Zenika\/alpine-chrome\/issues\/240#issuecomment-2015296658"],"labels":["status\/0-triage","kind\/bug"]},{"title":"[RFC] Dropping legacy networking mode","body":"### Description\r\n\r\nDuring the last couple weeks, we had a number of discussions on what should be done to remove legacy networking mode, and more particularly about legacy links. As there were many proposals made by various maintainers, this RFC try to summarize the various ideas floating around.\r\n\r\n- [Goals](#goals)\r\n- [Consensus](#consensus)\r\n- [Points of contention](#points-of-contention)\r\n    1. [Reimplement legacy links in terms of DNS records](#reimplement-legacy-links-in-terms-of-DNS-records)\r\n    2. [Disable ICC altogether](#disable-ICC-altogether)\r\n    3. [Enable ICC](#enable-ICC)\r\n- [Deprecation plan](#deprecation-plan)\r\n\r\n## <a name=\"goals\" \/> Goals\r\n\r\n1. Deprecate and remove a feature marked as legacy for almost 10 years (finally!)\u00a0;\r\n2. Reduce the complexity exposed to end-users and make it easier for them to transition from the default network to a custom network. In that regard, the default network should be a subset \/ \"diminished version\" of, or exactly the same as what custom networks provide. Currently, that's not the case because of the links feature having different semantics\u00a0;\r\n3. Lastly, get rid of the current snowflake implementation used by the default network. This would reduce the complexity of libnetwork.\r\n\r\n## <a name=\"consensus\" \/> Consensus\r\n\r\nI think there're two things we all agree about:\r\n\r\n1. `\/etc\/hosts`-based discovery should be let go and replaced by DNS-based discovery. It's recognized that this file isn't an API -- we don't mind breaking users who parse it\u00a0;\r\n2. The current env vars mechanism is obsolete and should be removed. They're barely useful anyway since you've to parse `_NAME` env vars and know the port you want to reach to look up the right env vars:\r\n\r\n```\r\n$ docker run --rm -d --name=c0 -p 80:80 alpine top\r\n$ docker run --rm -it --name=c1 --link=c0 alpine \/bin\/sh -c export\r\nexport C0_NAME='\/c1\/c0'\r\nexport C0_PORT='tcp:\/\/172.17.0.4:80'\r\nexport C0_PORT_80_TCP='tcp:\/\/172.17.0.4:80'\r\nexport C0_PORT_80_TCP_ADDR='172.17.0.4'\r\nexport C0_PORT_80_TCP_PORT='80'\r\nexport C0_PORT_80_TCP_PROTO='tcp'\r\n```\r\n\r\nMostly as a side note (it probably requires a dedicated proposal), a replacement mechanism was briefly discussed. That mechanism could be used to solve other issues: #3778 (5th most upvoted networking issue, 25th overall) and #8427. This could take the form of:\r\n\r\n1. A filesystem mount, allowing discoverability by walking the file tree. Another pro: it solves the notification issue (think Traefik use-case), and can be expanded to non-networking needs -- killing two birds with one stone. But there was some pushback on this design\u00a0;\r\n2. Using SRV records. Cons:\r\n  - The notification issue isn't solved\u00a0;\r\n  - Thus (most probably) limited to ports discovery (eg. host port when publishing to an ephemeral port)\u00a0;\r\n  - Ports don't have labels right now -- how to leverage RFC2782?\r\n\r\n## <a name=\"points-of-contention\" \/> Points of contention\r\n\r\nThe current point of contention is about what the default bridge should look like, and what should be the semantics of links (on that network).\r\n\r\n### <a name=\"reimplement-legacy-links-in-terms-of-dns-records\" \/> 1. Reimplement legacy links in terms of DNS records\r\n\r\nThe default network would be icc=false, and links would be used to declare which containers could reach each other.\r\n\r\nFrom a user standpoint, the links semantics would still be unique to the default bridge, hardly hitting goals 2. and 3. However, we could change the semantics of links on custom networks to make it look like the current legacy semantics. But, this doesn't seem user-driven, making it hard to justify time investment and making the overall design more complex.\r\n\r\nThe new semantics would be:\r\n\r\n- When icc=true, links are container-specific CNAME records\u00a0;\r\n- When icc=false, same as icc=true plus they also define which containers can reach each other\u00a0;\r\n\r\nFrom a technical standpoint, this is currently not easily feasible because non-legacy links are  implemented as container-specific CNAME records pointing to network-wide A records. When the DNS resolver is disabled, no A records are currently populated (**needs confirmation**).\r\n\r\n### <a name=\"disable-icc-altogether\" \/> 2. Disable ICC altogether\r\n\r\nBasically make the default bridge a north-south network -- ie. no inter-container connectivity, only a gateway to host's LAN\/WAN. Legacy links should be replaced with custom networks, in line with what the docs currently advertise: https:\/\/docs.docker.com\/network\/links\/.\r\n\r\nThere was some pushback on this idea as this would be a _massive_ breaking change -- this was already tried in the past and reverted.\r\n\r\n### <a name=\"enable-icc\" \/> 3. Enable ICC\r\n\r\nAll containers connected to the default bridge can resolve and communicate with each other. No more legacy links -- links on the default bridge behave like on custom networks.\r\n\r\nFrom a user standpoint, the default bridge network is just like any other bridge network. From a technical standpoint, no more snowflake implementation. This is probably the easiest solution, and the one which best fit goals 1-3.\r\n\r\nThis has the advantage of unifying Linux & Windows behaviors (the embedded DNS is enabled on Windows).\r\n\r\nIf users are unhappy about icc=true, we could still allow them to recreate the default bridge with whatever options they need, eg. like `docker_gwbridge`. There have been some discussions around unifying the way predefined networks are created \/ configured (eg. deprecate top-level daemon flags, unify the experience, etc...), so I'll keep those details for another proposal.\r\n\r\nThere was some pushback too but that's still too fuzzy for me, so I'll let anyone on the fence chime in. One potential blocker is that containers disconnected from Swarm networks end up being reattached to the default network -- but that looks like a silly bug warranting a proper fix (_I can't reproduce this issue_).\r\n\r\n## <a name=\"deprecation-plan\" \/> Deprecation plan\r\n\r\nThis mostly depends on what alternative we adopt, but it'd be a variation of something like:\r\n\r\n1. Show a WARNING on `docker create` \/ `docker run` whenever links are used on the default bridge network for a release or two\u00a0;\r\n2. Disable legacy networking \/ links by default, and add a feature flag to re-enable them if needed\u00a0;\r\n4. After a few releases, get totally rid of them.\r\n","comments":[],"labels":["kind\/enhancement","area\/networking","area\/ux"]},{"title":"No event when building an untagged image","body":"### Description\n\nBuilding an image with a tag results in `ActionTag` event being sent:\r\n\r\n```bash\r\n$ docker events &\r\n$ printf 'FROM alpine\\nRUN apk add vim' | docker build -t asdf -\r\n...\r\n2024-02-28T10:37:54.624937133Z image tag sha256:cdd546523a48d2c0932a32faa8c01abc3ebea2724d2571f713175964444d62bd (name=asdf:latest)\r\n ```\r\n\r\n\r\nWhen building an untagged image:\r\n```bash\r\n$ printf 'FROM alpine\\nRUN apk add vim' | docker build -t asdf -\r\n```\r\n\r\nthe event is not sent, because there's no tagging operation.\r\n\r\nThis makes it impossible to get a notification via events system when such image is built.\n\n### Reproduce\n\n```bash\r\n$ printf 'FROM alpine\\nRUN apk add vim' | docker build -t asdf -\r\n```\r\n\n\n### Expected behavior\n\nBuilding an untagged image should produce an event\n\n### docker version\n\n```bash\nClient:\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Thu Feb  1 00:22:01 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.45 (minimum version 1.24)\r\n  Go version:       go1.21.7\r\n  Git commit:       b37f8c8070832036d709bcf7d45bce8370b11321\r\n  Built:            Wed Feb 28 10:38:12 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.13\r\n  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/local\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: dev\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc crun\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 6.6.16-linuxkit\r\n Operating System: Debian GNU\/Linux 12 (bookworm) (containerized)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 6\r\n Total Memory: 7.755GiB\r\n Name: 9300895d92be\r\n ID: 8f527aca-3d67-48c8-b911-e5e87981ac83\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 28\r\n  Goroutines: 47\r\n  System Time: 2024-02-28T11:06:40.026886584Z\r\n  EventsListeners: 0\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["We can consider introducing  an image _create_ event, which would be produced when creating the digest. If we do, we need to look at places where this happens (`load`, `import` and such may of course also create a new digest).\r\n\r\nIf tagging happens, it would generate 2 events (create -> tag).\r\n\r\nIt's possible though that no create happens (consider `docker image build` that's fully cached and produces the same digest); I guess we already have checks for this, but mentioning just in case.\r\n\r\nFor the containerd integration, this somewhat brings back the discussion we had some time ago where we discussed that to \"match\" the pre-snapshotter behaviour, and to allow automatic garbage collection, we can consider _always_ creating a `dangling` or \"digested\" image name.\r\n\r\nLOL; when looking for what image events we currently produce, this was a bit confusing; two `LogImageEvent` functions, but they're separate things;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/8517c3386c382bad825852610f2cfbad01ba7a9e\/daemon\/containerd\/image_events.go#L12\r\nhttps:\/\/github.com\/moby\/moby\/blob\/8517c3386c382bad825852610f2cfbad01ba7a9e\/daemon\/containerd\/image_events.go#L32\r\n\r\n"],"labels":["area\/builder","kind\/bug","area\/builder\/classic-builder","area\/builder\/buildkit"]},{"title":"Container disk space limit","body":"### Description\n\nI think it would be great to add disk space constraints on runtime like cpu and memory limits, to prevent people who have access to a container from filling a whole server's disk space.","comments":["We already support this, but it depends on the backing filesystem.\r\nThese is done using quotas.\r\n\r\nThis can be done with:\r\n\r\n- overlay2 when backed by xfs (and projectqutoa is enabled on the xfs mount)\r\n- btrfs\r\n\r\n`docker run --storage-opt size=<size>` will get you there.\r\n"],"labels":["status\/0-triage","kind\/feature"]},{"title":"Empty string not allowed for \"Isolation\" in Swagger specs","body":"### Description\n\nThe swagger.yaml says that Isolation must be one of default|process|hyperv\r\n```\r\n          # Applicable to Windows\r\n          Isolation:\r\n            type: \"string\"\r\n            description: |\r\n              Isolation technology of the container. (Windows only)\r\n            enum:\r\n              - \"default\"\r\n              - \"process\"\r\n              - \"hyperv\"\r\n```\r\nBut on non-windows systems, this value can (will always?) be empty leading to invalid states for code auto-generated from the api specs.\n\n### Reproduce\n\n1. generate code e.g. with `openapi-generator-cli`\r\n2. Inspect container on non-windows system\r\n3. get error about invalid value for Isolation (`\"\"`)\n\n### Expected behavior\n\n`\"\"` should be marked as valid value for Isolation, or non-windows system should return \"default\".\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:09 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:13:09 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 70\r\n  Running: 30\r\n  Paused: 0\r\n  Stopped: 40\r\n Images: 131\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-18-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 32\r\n Total Memory: 124.9GiB\r\n Name: jkubuntu2204\r\n ID: 82d03fb7-29c1-45cf-9643-d36f566767ae\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Syslog entry: \"superfluous response.WriteHeader call\" in Docker 25.x","body":"### Description\n\nWhen retrieving the logs for a container (via `docker logs` for example), a log entry is made in `\/var\/log\/syslog`:\r\n\r\n```\r\nFeb 26 16:54:53 be-docker dockerd[410]: 2024\/02\/26 16:54:53 http: superfluous response.WriteHeader call from go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)\r\n```\r\n\r\nThis appears to have been introduced alongside the OpenTelemetry addition in Docker 25.0.0 as this log message is not created on previous versions of Docker (for example 24.0.6).\n\n### Reproduce\n\n1. Install Docker 25.x\r\n2. Create a container (any will do)\r\n3. Monitor `\/var\/log\/syslog` (via `tail -f` for example)\r\n4. While monitoring, run `docker logs containername` \r\n5. Note the entry in syslog\n\n### Expected behavior\n\nNo \"superfluous response\" message.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:14:17 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:17 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 7\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 33\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: gelf\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.4.0-171-generic\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.925GiB\r\n Name: be-docker\r\n ID: HJDW:MIDK:VVF2:ZZC5:CCHA:6KOE:CCZU:TXJ3:2YUM:2KHI:NKJX:7ID3\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: jcarppe\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No swap limit support\n```\n\n\n### Additional Info\n\nWhile a single log line entry doesn't seem like much, when using a separate tool (such as Portainer) to view container logs that auto-refreshes, this can result in repeated syslog entries.","comments":["docker compose stats will also create syslog messages","This line currently accounts for 8% of my total server syslog\/journal logs (out of about 120k lines).\r\nPlease either suppress it if it's innocuous or fix the underlying problem, it's hard to look at system logs with this spam (journalctl does not support filters; inverse grepping works but breaks --follow for example).","Wonder if this is a bug in OTEL here; I see it keeps track whether it already wrote headers, but then proceeds calling `w.ResponseWriter.WriteHeader(statusCode)` to write headers anyway; https:\/\/github.com\/moby\/moby\/blob\/330d777c53fbbf734178d6a35c9dc0a5070ba4ac\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/wrap.go#L93-L99\r\n\r\nIt looks like `Write` has a protection for that (it will only call `WriteHeader` if it wasn't written yet), but the same does not apply when `WriteHeader` is called directly (see above); https:\/\/github.com\/moby\/moby\/blob\/330d777c53fbbf734178d6a35c9dc0a5070ba4ac\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/wrap.go#L76-L80\r\n\r\nThat said, the log itself seems to be coming from `het\/http` in Go stdlib from either one of these two, and stdlib doesn't _prevent_ it being called multiple times, so maybe in this case something hits the wrapper's `Write()` (writing the header), before existing code is executed and calling `WriteHeader()` :thinking:;\r\n\r\n- https:\/\/github.com\/golang\/go\/blob\/a73af5d91c4c335fb44ae99517d4c41d5f3960e0\/src\/net\/http\/server.go#L1157\r\n- https:\/\/github.com\/golang\/go\/blob\/a73af5d91c4c335fb44ae99517d4c41d5f3960e0\/src\/net\/http\/server.go#L3675\r\n"],"labels":["status\/0-triage","kind\/bug","area\/metrics","area\/metrics\/otel","version\/25.0"]},{"title":"Add configs.XXX.content support to Docker swarm compose files","body":"### Description\n\nhttps:\/\/docs.docker.com\/compose\/compose-file\/08-configs\/#example-2 shows how embed the configuration into the compose file, but it only works in `docker compose` it would be nice to have it on Docker swarm as well.","comments":[],"labels":["status\/0-triage","kind\/feature"]},{"title":"Attempting to use containerd-snapshotter mode with nix-snapshotter","body":"### Description\r\n\r\nI was trying to use [nix-snapshotter](https:\/\/github.com\/pdtpartners\/nix-snapshotter) with Docker but ran into a few issues. We have it working no problem with containerd + rootless containerd so I believe the issues is with Docker's containerd integration.\r\n\r\n```sh\r\n[root@nixos:~]# docker run --rm ghcr.io\/pdtpartners\/hello\r\ndocker: Error response from daemon: failed to mount : mkdir \/var\/lib\/docker\/rootfs\/nix\/07b044aabc1d3fb1da00a927e2f269bae39b1e04366434f88bc408331261558b: no such file or directory.\r\nSee 'docker run --help'.\r\n```\r\n\r\nManually creating the directory gets me a bit further but then I run into chmod error:\r\n\r\n```sh\r\n[root@nixos:~]# mkdir -p \/var\/lib\/docker\/rootfs\/nix\r\n[root@nixos:~]# docker run --rm ghcr.io\/pdtpartners\/hello\r\ndocker: Error response from daemon: failed to mount : chmod \/var\/lib\/docker\/rootfs\/nix\/b3f74ea450ba2ef592208478828bae37d18dfbb7822a0869e12ce65d1f3279ba: read-only file system.\r\n```\r\n\r\nPerhaps related to this part of the code:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/v25.0.3\/daemon\/snapshotter\/mount.go#L123-L131\r\n\r\nAs an aside, I've contributed this PR to containerd back in Dec 2022, which added the `Target` field to containerd's `mount.Mount` struct:\r\nhttps:\/\/github.com\/containerd\/containerd\/pull\/7840\r\n\r\nI needed it to construct bind mounts to subdirectorys of the target mountpath. Does the `refCountMounter` still make sense in this case?\r\n\r\n```go\r\n\tif count := m.rc.Increment(target); count > 1 {\r\n\t\treturn target, nil\r\n\t}\r\n```\r\n\r\nTLDR around nix-snapshotter mounts is its an overlayfs on `\/` + many bind mounts under `\/nix\/store\/<package-hash>`, see the [architecture docs](https:\/\/github.com\/pdtpartners\/nix-snapshotter\/blob\/main\/docs\/architecture.md) for more details.\r\n\r\n### Reproduce\r\n\r\nI've prepared a docker image that runs QEMU VM with NixOS pre-configured with this reproduction (docker + containerd + nix-snapshotter):\r\n\r\n```sh\r\ndocker run --rm -it ghcr.io\/pdtpartners\/nix-snapshotter:docker-24.0.5\r\n\r\nnixos login: root # (Ctrl-a then x to quit)\r\nPassword: root\r\n\r\n[root@nixos:~]# docker image ls\r\nREPOSITORY                        TAG       IMAGE ID       CREATED         SIZE\r\nghcr.io\/pdtpartners\/hello         latest    2971ba7e1fad   4 minutes ago   1.78kB\r\nghcr.io\/pdtpartners\/redis-shell   latest    6c6c48068af3   4 minutes ago   20.5kB\r\nghcr.io\/pdtpartners\/redis         latest    fac949281ac3   4 minutes ago   14.5kB\r\n\r\n[root@nixos:~]# docker run --rm ghcr.io\/pdtpartners\/hello\r\n```\r\n\r\nDocker, containerd, and nix-snapshotter are all in their respective systemd units:\r\n\r\n```sh\r\nsystemctl status docker\r\nsystemctl status containerd\r\nsystemctl status nix-snapshotter\r\n```\r\n\r\n### Expected behavior\r\n\r\nDocker run should work with snapshotters that work with containerd.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.21.4\r\n Git commit:        v24.0.5\r\n Built:             Thu Jan  1 00:00:00 1970\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.21.4\r\n  Git commit:       v24.0.5\r\n  Built:            Tue Jan  1 00:00:00 1980\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.11\r\n  GitCommit:        v1.7.11\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/nix\/store\/gdyxcsv9w1aghvgjy1j4c4mmdh35hi93-docker-plugins\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.23.1\r\n    Path:     \/nix\/store\/gdyxcsv9w1aghvgjy1j4c4mmdh35hi93-docker-plugins\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: 24.0.5\r\n Storage Driver: nix\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: v1.7.11\r\n runc version:\r\n init version:\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.69\r\n Operating System: NixOS 23.11 (Tapir)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 3.839GiB\r\n Name: nixos\r\n ID: 724cf269-930a-4bc5-9da5-f93c46987016\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n```json\r\n# daemon.json\r\n{\r\n  \"containerd\": \"\/run\/containerd\/containerd.sock\",\r\n  \"features\": {\r\n    \"containerd-snapshotter\": true\r\n  },\r\n  \"group\": \"docker\",\r\n  \"hosts\": [\r\n    \"fd:\/\/\"\r\n  ],\r\n  \"live-restore\": true,\r\n  \"log-driver\": \"journald\",\r\n  \"storage-driver\": \"nix\"\r\n}\r\n```\r\n\r\n```toml\r\n# containerd.toml\r\nversion = 2\r\n[plugins]\r\n[plugins.\"io.containerd.grpc.v1.cri\"]\r\n[plugins.\"io.containerd.grpc.v1.cri\".cni]\r\nbin_dir = \"\/nix\/store\/afw4hbr9fy6983hph8q5isrwa6pwfrrq-cni-plugins-1.3.0\/bin\"\r\n\r\n[plugins.\"io.containerd.grpc.v1.cri\".containerd]\r\nsnapshotter = \"nix\"\r\n\r\n[[plugins.\"io.containerd.transfer.v1.local\".unpack_config]]\r\nplatform = \"linux\/amd64\"\r\nsnapshotter = \"nix\"\r\n\r\n[proxy_plugins.nix]\r\naddress = \"\/run\/nix-snapshotter\/nix-snapshotter.sock\"\r\ntype = \"snapshot\"\r\n```","comments":["Thanks, that's a really good and detailed report!\r\n\r\nYeah this code was mostly based on the previous graphdriver implementation and it assumed that these mounts are writable.\r\n\r\nI'll need some more time to dig into it deeper \ud83d\udc40 "],"labels":["area\/runtime","kind\/bug","containerd-integration"]},{"title":"API will be slow when not \"User-Agent\" in header","body":"### Description\n\nAPI will be slow when not \"User-Agent\" in header.\n\n### Reproduce\n\n1. Execute curl without header.\r\n    ```sh\r\n    $ time curl -s --unix-socket \/var\/run\/docker.sock \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null \r\n\r\n\r\n    ========================\r\n    Program : curl -s --unix-socket \/var\/run\/docker.sock  > \/dev\/null\r\n    CPU     : 0%\r\n    user    : 0.007s\r\n    system  : 0.005s\r\n    total   : 3.146s\r\n    ========================\r\n    ```\r\n\r\n2. Execute curl with \"User-Agent\" header.\r\n    ```sh\r\n    $ time curl -s --unix-socket \/var\/run\/docker.sock -H \"User-Agent: Docker-Client\/25.0.3\" \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null \r\n\r\n\r\n    ========================\r\n    Program : curl -s --unix-socket \/var\/run\/docker.sock -H   > \/dev\/null\r\n    CPU     : 19%\r\n    user    : 0.012s\r\n    system  : 0.010s\r\n    total   : 0.112s\r\n    ========================\r\n\r\n    ```\r\n\n\n### Expected behavior\n\nWithout header not be slow.\n\n### docker version\n\n```bash\n$ docker version                   \r\nClient:\r\n Cloud integration: v1.0.35+desktop.10\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:26 2024\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.27.2 (137060)\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435e5f6216828dec57958c490c4f8bae4f98\r\n  Built:            Wed Feb  7 00:39:16 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\n$ docker info\r\nClient:\r\n Version:    25.0.3\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1-desktop.4\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5-desktop.1\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-compose\r\n  debug: Get a shell into any image or container. (Docker Inc.)\r\n    Version:  0.0.24\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-debug\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  v1.0.4\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v1.0.0\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-sbom\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.4.1\r\n    Path:     \/Users\/skanehira\/.docker\/cli-plugins\/docker-scout\r\nWARNING: Plugin \"\/Users\/skanehira\/.docker\/cli-plugins\/docker-scan\" is not valid: failed to fetch metadata: fork\/exec \/Users\/skanehira\/.docker\/cli-plugins\/docker-scan: no such file or directory\r\n\r\nServer:\r\n Containers: 11\r\n  Running: 6\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 22\r\n Server Version: 25.0.3\r\n Storage Driver: stargz\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.6.12-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 10\r\n Total Memory: 4.808GiB\r\n Name: docker-desktop\r\n ID: de692d8b-bfab-4b8c-ad5f-3b294c40137b\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\n```\n\n\n### Additional Info\n\n_No response_","comments":["Hm.. I'm not seeing this on a Linux machine, so wondering if this is something specific to Docker Desktop (which has a proxy for the API for forwarding requests into the Docker Desktop VM);\r\n\r\n```bash\r\ntime curl -s --unix-socket \/var\/run\/docker.sock \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null\r\n\r\nreal\t0m0.013s\r\nuser\t0m0.005s\r\nsys\t0m0.004s\r\n\r\ntime curl -s --unix-socket \/var\/run\/docker.sock -H \"User-Agent: Docker-Client\/25.0.3\" \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null\r\n\r\nreal\t0m0.012s\r\nuser\t0m0.004s\r\nsys\t0m0.004s\r\n```","Actually, not seeing this on my Docker Desktop as well \ud83e\udd14 \r\n\r\n```bash\r\ntime curl -s --unix-socket \/var\/run\/docker.sock \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null\r\n\r\nreal\t0m0.139s\r\nuser\t0m0.003s\r\nsys\t0m0.006s\r\n\r\ntime curl -s --unix-socket \/var\/run\/docker.sock -H \"User-Agent: Docker-Client\/25.0.3\" \"http:\/\/localhost\/containers\/json?all=true\" > \/dev\/null\r\n\r\nreal\t0m0.137s\r\nuser\t0m0.003s\r\nsys\t0m0.004s\r\n```\r\n"],"labels":["area\/api","status\/0-triage","kind\/bug","platform\/mac","version\/25.0"]},{"title":"ADD command changing mtime cases layer cannot be shared in multi arch image","body":"### Description\r\n\r\n## Description\r\nADD will change the mtime of \/path\/to, whether this directory exists or not. So the artifact will contains three(count of arch) different layer but has same files ( only mtime of \/path\/to are not different.)\r\n\r\n## Steps to reproduce\r\n```Dockerfile\r\nFROM busybox:latest\r\n\r\n# mkdir dir && dd if=\/dev\/random of=dir\/file bs=1M count=10 && tar --mtime='UTC 2019-01-01' -cf files.tar dir\r\n\r\nRUN mkdir \/path\/to -p\r\n# Make the execution time of the next line inconsistent (this is normal if the dockfile has a command to execute)\r\nRUN sleep $((RANDOM % 10))\r\nADD files.tar \/path\/to\r\n```\r\n\r\n```bash\r\ndocker buildx build --platform linux\/amd64,linux\/arm64,linux\/arm\/v7 .\r\n```\r\n\r\n```\r\ntotal 31M\r\n-rw------- 1 root root  11M 2\u6708  23 17:41 0a17504001ae34a6881157bc2f2d3bdae0a4835c8392bd9aa22b935b0ff1812b\r\n-rw------- 1 root root 1.1K 2\u6708  23 17:41 0f6f81dbb7c0cc1988871921659d3b613f6949ff85dd0b9b28bbb967aab7d7b2\r\n-rw-r--r-- 1 root root 1.3K 2\u6708  23 17:41 1613d839de077505a5e8c12b0c06eca6019b6268d7b1ba3ff0aad837d96429df\r\n-rw-r--r-- 1 root root  167 2\u6708  23 17:41 37d7787c1ec17eaf62b5ab92908a4c9689951a03262a774e932a6a8b57f5cd3d\r\n-rw------- 1 root root   32 2\u6708  23 17:41 4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1\r\n-rw-r--r-- 1 root root  566 2\u6708  23 17:41 6e7b9e0ccf661b0f0d892b912ff87f6a7c52aa422c88f5f94ec4c501d086f5d8\r\n-rw-r--r-- 1 root root  167 2\u6708  23 17:41 71d7732e6f926947ba7a28339f6d6093fe6a9345b2dc2c65ec0a0ab415399c89\r\n-rw-r--r-- 1 root root  566 2\u6708  23 17:41 8f9bacee795e8df352c8354c3e4f1c8923b7647726118650f6045b79e7e50f24\r\n-rw------- 1 root root 1.1K 2\u6708  23 17:41 a53e46a1b3eceecd511da46860b5bea40e792093e8bb167f2ae557ab6e0a2a5c\r\n-rw------- 1 root root  136 2\u6708  23 17:41 a6109caf6041b07335c1d9be21aaf80f3099a1a2382221db5fc1d1acd0f1393c\r\n-rw------- 1 root root  11M 2\u6708  23 17:41 abf346ca4d502895b58927b1e344928ca7e68ea21f5922f03148705572c1dece\r\n-rw-r--r-- 1 root root 1.1K 2\u6708  23 17:41 acb435bd5973855bb53e61c8050228cfced2ae668e76113e677182aa2d886e11\r\n-rw-r--r-- 1 root root 1.3K 2\u6708  23 17:41 b618edb1071bdb255872ea201841feae1293362ab2b3fa6fb729a3c0fea62d5a\r\n-rw-r--r-- 1 root root 1.3K 2\u6708  23 17:41 b61f0c436dbc516cc651a6dc980fe1444f3e897a26edb0d91202e067cc6a974b\r\n-rw------- 1 root root 795K 2\u6708  23 17:41 c504311cf8e3973c432ab0a5b264fbaa35b68757c7084ecd43fad4364f61561e\r\n-rw-r--r-- 1 root root 1.1K 2\u6708  23 17:41 d0f6722b33de2fc62da6eeaf6d4fca21dee73737dd635323855ee3317cd9ba2d\r\n-rw------- 1 root root 1.1K 2\u6708  23 17:41 d193d50fceece91d63586b5f04bbb6760f7038c3cf3dd66b4e46f2bdeebf9903\r\n-rw-r--r-- 1 root root 1.1K 2\u6708  23 17:41 d40f7820983c501fb0e00d1e5d4c06c4618cda5bf0da42cf03b8920573fa338b\r\n-rw------- 1 root root  11M 2\u6708  23 17:41 d699b229b71e2548427e4bd5a53b9ea30f4d3d2411189aa91abcfd97f7ec767b\r\n-rwxr-xr-x 1 root root 2.4K 2\u6708  23 17:41 d7bc9b157a2cebd54941a0a2a066661c23a61b54a07c7018b5d52e7c93e93dbf*\r\n-rw-r--r-- 1 root root  167 2\u6708  23 17:41 dcdc629de2cfd79882cd87746c56bbc38e7b68a79b5e620ce7084809ee251653\r\n-rw-r--r-- 1 root root  566 2\u6708  23 17:41 e9aa37347c11f3c849357fc16e9f819f54a7482f25fdc36b4cea78b80eba327e\r\n(there are three 11M layer)\r\n```\r\n\r\n\r\n`COPY --from` has the same behavior.\r\n\r\n**This will cause registry to have greater storage pressure.**\r\n\r\nOnly add to `\/` won't have this problem.\r\n\r\n\r\n## Problem\r\nMaybe we need  `ADD --mtime='1970-01-01' files.tar \/path\/to` ?\r\n","comments":["The only way I can think of to avoid it is `ADD files.tar \/`","Try setting `SOURCE_DATE_EPOCH` in your build args (newer buildx's will automatically propagate it from env).\r\nThis does impact other file times too, though.\r\n\r\nhttps:\/\/github.com\/moby\/buildkit\/blob\/8e3fe35738c229a6dc125829319ea1f1d0d411ba\/docs\/build-repro.md?plain=1#L40-L70","Seems like part of this is because adding a file to a directory on Linux will change the directory; e.g., here's before \/ after adding afile to the `\/path\/to\/` directory;\r\n\r\n```bash\r\ndocker run --rm alpine sh -c 'mkdir -p \/path\/to; touch foo.txt; ls -la --full-time \/path\/to; sleep 3; mv foo.txt \/path\/to\/; ls -la --full-time \/path\/to'\r\ntotal 8\r\ndrwxr-xr-x    2 root     root          4096 2024-02-26 14:49:07 +0000 .\r\ndrwxr-xr-x    3 root     root          4096 2024-02-26 14:49:07 +0000 ..\r\ntotal 8\r\ndrwxr-xr-x    2 root     root          4096 2024-02-26 14:49:10 +0000 .\r\ndrwxr-xr-x    3 root     root          4096 2024-02-26 14:49:07 +0000 ..\r\n-rw-r--r--    1 root     root             0 2024-02-26 14:49:07 +0000 foo.txt\r\n```\r\n\r\nThe `\/` case may be slightly different here because (ISTR) the root directory itself is not stored in the image's data (or at least, I recall discussions about that related to the root-directory not persisting permissions \/ ownership in the image)","> SOURCE_DATE_EPOCH\r\n\r\n~SOURCE_DATE_EPOCH can effect the timestamp of the files from tar. but the mtime of target directory will always be changed~\r\n\r\nI make a mistask.\r\n\r\nSOURCE_DATE_EPOCH will only changes the created time (in config).\r\nSOURCE_DATE_EPOCH with output opt rewrite-timestamp=true will rewrite mtime of all layers (except base image) to the given time.\r\n\r\n","Yeah, moving \/ adding a file or a dir to `\/path\/to` makes no difference there; in both cases, an entry is created inside `path\/to`, which is modifying the timestamp; here's myu example from above, but now using a `dir` that is moved into `path\/to`;\r\n\r\n```bash\r\ndocker run --rm alpine sh -c 'mkdir -p \/path\/to; mkdir -p dir; touch dir\/file; ls -la --full-time \/path\/to; sleep 3; mv dir \/path\/to\/; ls -la --full-time \/path\/to'\r\ntotal 8\r\ndrwxr-xr-x    2 root     root          4096 2024-02-27 13:08:54 +0000 .\r\ndrwxr-xr-x    3 root     root          4096 2024-02-27 13:08:54 +0000 ..\r\ntotal 12\r\ndrwxr-xr-x    3 root     root          4096 2024-02-27 13:08:57 +0000 .\r\ndrwxr-xr-x    3 root     root          4096 2024-02-27 13:08:54 +0000 ..\r\ndrwxr-xr-x    2 root     root          4096 2024-02-27 13:08:54 +0000 dir\r\n```\r\n\r\nThe tricky bit there is very likely that neither Docker or BuildKit actively modify `path\/to`, as this change is happening implicitly. The only possible solution there would (probably?) be for it to;\r\n\r\n1. check the current metadata of the parent directory _before_ copying\r\n2. perform the copy (or `ADD`)\r\n3. actively reset the metadata to the previous values\r\n\r\nAnd wondering if `3.` could risk modifying the _parent_ of the dir that's modified, but perhaps that's not the case.\r\n","I think, at least in the case of SOURCE_DATE_EPOCH, the timestamp of the target directory should be reset to previous value (or it should be the value specified by SOURCE_DATE_EPOCH when ADD command create a new directory)","> > SOURCE_DATE_EPOCH\r\n> \r\n> ~SOURCE_DATE_EPOCH can effect the timestamp of the files from tar. but the mtime of target directory will always be changed~\r\n> \r\n> I make a mistask.\r\n> \r\n> SOURCE_DATE_EPOCH will only changes the created time (in config). SOURCE_DATE_EPOCH with output opt rewrite-timestamp=true will rewrite mtime of all layers (except base image) to the given time.\r\n\r\n\r\nBased on this, I think `SOURCE_DATE_EPOCH with output opt rewrite-timestamp=true` will be a temporary solution, although it has not been released, but soon.\r\n\r\n\r\n\r\n> 1. check the current metadata of the parent directory _before_ copying\r\n> 2. perform the copy (or `ADD`)\r\n> 3. actively reset the metadata to the previous values\r\n> \r\n> And wondering if `3.` could risk modifying the _parent_ of the dir that's modified, but perhaps that's not the case.\r\n\r\n@thaJeztah Will this be realized? This may be the most perfect solution."],"labels":["area\/builder","status\/0-triage","kind\/feature","area\/builder\/buildkit"]},{"title":"OTEL: update code to remove use of some deprecated options","body":"### Description\n\n- tracking ticket for https:\/\/github.com\/moby\/moby\/pull\/47245#issuecomment-1919535921\r\n- related to https:\/\/github.com\/moby\/buildkit\/issues\/4681\r\n\r\nLooks like this needs some local changes (either temporary `\/\/nolint` for the release branches if we want to backport, or changes in code);\r\n\r\n```\r\napi\/server\/router\/grpc\/grpc.go:24:69: SA1019: otelgrpc.StreamServerInterceptor is deprecated: Use [NewServerHandler] instead. (staticcheck)\r\n\tstream := grpc.StreamInterceptor(grpc_middleware.ChainStreamServer(otelgrpc.StreamServerInterceptor(), grpcerrors.StreamServerInterceptor))\r\n\t                                                                   ^\r\napi\/server\/router\/grpc\/grpc.go:49:15: SA1019: otelgrpc.UnaryServerInterceptor is deprecated: Use [NewServerHandler] instead. (staticcheck)\r\n\twithTrace := otelgrpc.UnaryServerInterceptor()\r\n\t             ^\r\nlibcontainerd\/supervisor\/remote_daemon.go:304:32: SA1019: otelgrpc.UnaryClientInterceptor is deprecated: Use [NewClientHandler] instead. (staticcheck)\r\n\t\t\t\t\tgrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\r\n\t\t\t\t\t                          ^\r\nlibcontainerd\/supervisor\/remote_daemon.go:305:33: SA1019: otelgrpc.StreamClientInterceptor is deprecated: Use [NewClientHandler] instead. (staticcheck)\r\n\t\t\t\t\tgrpc.WithStreamInterceptor(otelgrpc.StreamClientInterceptor()),\r\n\t\t\t\t\t                           ^\r\ndaemon\/daemon.go:965:29: SA1019: otelgrpc.UnaryClientInterceptor is deprecated: Use [NewClientHandler] instead. (staticcheck)\r\n\t\tgrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\r\n\t\t                          ^\r\ndaemon\/daemon.go:966:30: SA1019: otelgrpc.StreamClientInterceptor is deprecated: Use [NewClientHandler] instead. (staticcheck)\r\n\t\tgrpc.WithStreamInterceptor(otelgrpc.StreamClientInterceptor()),\r\n\t\t                           ^\r\n```\r\n\r\n`StreamServerInterceptor`, `UnaryServerInterceptor`, and `StreamClientInterceptor` were deprecated in https:\/\/github.com\/open-telemetry\/opentelemetry-go-contrib\/commit\/23bd4ed41fe30ae1e9a5b11b57e902743f94490b\r\n\r\n- https:\/\/github.com\/open-telemetry\/opentelemetry-go-contrib\/pull\/4534\r\n","comments":["@thaJeztah Thank you for working to fix the Otel vulnerabilities\r\n\r\nhttps:\/\/github.com\/moby\/moby\/issues\/47246#issuecomment-1967544074\r\nRegarding above comment from @neersighted \r\n\r\nI understand and respect the position of the Moby project. However, I wanted to provide some context for our urgency. The requirement to have zero vulnerabilities is a mandate from the US Federal government to its contractors. This particular vulnerability, flagged by scanner tool, has been present since October. It's been a significant amount of time and the issue is still unresolved.\r\n\r\nGiven the timeline and the government mandate, we are now compelled to remove the `dockerd` dependency from our application. The vulnerability must be addressed in our application by March 4th.","Greetings @raghu017,\r\n\r\nI am sorry to say that the position of the project is very clear, and that further agitation to perform a backport that the maintainers have concluded is unnecessary or dangerous is indicative of a lack of understanding and respect for this project.\r\n\r\nIt sounds to me as if you have successfully built a business on top of this project (and implicitly, the community\/contributors, processes, and source code within). I'm glad that you are able to do so -- certainly as a maintainer, I would like to encourage and see more diverse and successful usage of this project, both commercially and by hobbyists.\r\n\r\nHowever, it seems I must remind you of the warranty and guarantees that this project (and its contributors) make to you, the consumer of the source code:\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/81428bf11b122325fd855d5c59fa83016f82986a\/LICENSE#L144-L152\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/81428bf11b122325fd855d5c59fa83016f82986a\/LICENSE#L166-L175\r\n\r\nYou must understand that any promises, guarantees, or SLAs you have made regarding your products are yours alone. This project is open source, open to contributions from all sources, and driven by the needs of its contributors. Of course, this is at times imperfect and messy, but we try our best to steward the project and look after smaller user's needs as well.\r\n\r\nAs the project is open source, you are of course welcome to contribute, or to make modifications to the code, shared with others (or not) as long as you follow the terms of the license (see above). If you are unhappy with a decision made by the maintainers, such as not performing a backport you desire, you are welcome to (and indeed encouraged to) perform your own backport and deliver the resulting product to your customers.\r\n\r\nI must once again highlight to you that it falls upon you, not this project, to ensure that the artifacts you deliver to your customers are suitable for their purposes, and compliant with the terms of any contract you may have signed with them.\r\n\r\nYou have received an official response from this project, both as to the substance of the [CVE-2023-47108](https:\/\/github.com\/advisories\/GHSA-8pgv-569h-w5rw) finding (note: this is a **finding**, not a vulnerability, as dockerd is not vulnerable according to the [established security model](https:\/\/docs.docker.com\/engine\/security\/#docker-daemon-attack-surface)), and as to the suitability of #47245 for backport to a release branch.\r\n\r\nPlease demonstrate your understanding and respect for the project, its contributors, and its license by ceasing to agitate upstream for an action that will not be taken, and instead direct your energies to resolving your customer's concerns (and your apparent contractual obligations) according to the terms of the license, and the remedies you have as a user (and indeed, vendor) of open source software.\r\n\r\nPlease also understand that future agitation along this line (having been asked to cease two times, and having exhausted any productive discussion) may result in the removal of your ability to interact with, contribute to, and fully participate in the project.\r\n"],"labels":["area\/metrics","area\/metrics\/otel"]},{"title":"Error message \"raft message is too large and can't be sent\" when trying to remove a node from a Docker swarm cluster","body":"### Description\n\nI have a cluster of 3 nodes (See them below). I am unable to remove the node that shows as down and it's breaking my cluster.\r\n\r\n```\r\nID                            HOSTNAME                               STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\npcefq43rapf8mw8887inkztvi *   <CENSORED>llbprmmid01.<CENSORED>.net   Ready     Active         Reachable        25.0.3\r\nxl8kfa7y56uu3vz1xsw0lxb61     <CENSORED>llbprmmid02.<CENSORED>.net   Ready     Active         Reachable        25.0.3\r\nd31t8u7reuky3kjyym84smsl4     <CENSORED>llbprmmid03.<CENSORED>.net   Ready     Active         Leader           25.0.3\r\nhdresbrj592dpjlh80gwsuy9h     <CENSORED>llbprmmid03.<CENSORED>.net   Down      Drain                           25.0.2\r\n```\r\nWhen I do a `docker node ls | wc -l` it returns\r\n```\r\n5\r\n```\r\n\r\nI found out that there was a similar issue reported before, https:\/\/forums.docker.com\/t\/removing-node-from-the-swarm-issue-raft-message-is-too-large-and-cant-be-sent\/139518, I went through it, tried what they advise, but still no luck.\r\n\r\nAny idea how I can fix this? This actually broke a production environment!\r\n\n\n### Reproduce\n\n1. docker node rm -f hdresbrj592dpjlh80gwsuy9h\n\n### Expected behavior\n\ndocker node rm should remove the node that I want removed from the cluster without issues.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:15:16 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:12 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: active\r\n  NodeID: pcefq43rapf8mw8887inkztvi\r\n  Is Manager: true\r\n  ClusterID: nixy9iaupmii3yn6uidkw2l10\r\n  Managers: 3\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 3\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: true\r\n  Node Address: 192.168.32.110\r\n  Manager Addresses:\r\n   192.168.32.110:2377\r\n   192.168.32.111:2377\r\n   192.168.32.112:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-477.21.1.el8_8.x86_64\r\n Operating System: Red Hat Enterprise Linux 8.8 (Ootpa)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.39GiB\r\n Name: <CENSORED>llbprmmid01.<CENSORED>.net\r\n ID: a7918f44-224e-45dd-abf4-3c95d61e0f6f\r\n Docker Root Dir: \/u02\/docker\r\n Debug Mode: false\r\n HTTP Proxy: <CENSORED>\r\n HTTPS Proxy: <CENSORED>\r\n No Proxy: <CENSORED>\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nBelow is a snippet of the the logs, from `journalctl -u docker.service -f`, after enabling debug and trying to remove node3 from node1\r\n\r\n```\r\nFeb 22 18:10:53 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:53.346690739Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:53 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:53.347704874Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.305433283s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.465861670Z\" level=debug msg=\"Calling HEAD \/_ping\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.466267559Z\" level=debug msg=\"Calling DELETE \/v1.44\/nodes\/hdresbrj592dpjlh80gwsuy9h?force=1\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.806858949Z\" level=debug msg=\"error handling rpc\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" rpc=\/docker.swarmkit.v1.Control\/RemoveNode\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807027288Z\" level=debug msg=\"Error removing node\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" node-id=hdresbrj592dpjlh80gwsuy9h\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807091867Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" error_type=\"*status.Error\" module=api\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807104059Z\" level=error msg=\"Handler for DELETE \/v1.44\/nodes\/hdresbrj592dpjlh80gwsuy9h returned error: rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\"\r\nFeb 22 18:10:56 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:56.807116950Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"rpc error: code = Unknown desc = raft: raft message is too large and can't be sent\" error_type=\"*status.Error\" module=api\r\nFeb 22 18:10:58 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:58.653777697Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:10:58 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:10:58.654635872Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.339470303s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:03 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:03.994666787Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:03 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:03.995801446Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 4.774354673s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:05 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:05.980417978Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.112:57678\"\r\nFeb 22 18:11:05 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:05.980569597Z\" level=debug msg=\"<CENSORED>llbprmmid01.<CENSORED>.net(54699a50f58e): Initiating  bulk sync for networks [lp9jegtxlrr1ojaijz43kr233] with node 2d3fae1bf26e\"\r\nFeb 22 18:11:07 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:07.057352324Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.111:51210\"\r\nFeb 22 18:11:07 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:07.057502628Z\" level=debug msg=\"<CENSORED>llbprmmid01.<CENSORED>.net(54699a50f58e): Initiating  bulk sync for networks [lp9jegtxlrr1ojaijz43kr233] with node 67d13d52bcbe\"\r\nFeb 22 18:11:08 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:08.770571281Z\" level=debug msg=\"sending heartbeat to manager { } with timeout 5s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:08 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:08.771434267Z\" level=debug msg=\"heartbeat successful to manager { }, next heartbeat period: 5.469749527s\" method=\"(*session).heartbeat\" module=node\/agent node.id=pcefq43rapf8mw8887inkztvi session.id=72z12be65dn88tkuxtckrxxmj sessionID=72z12be65dn88tkuxtckrxxmj\r\nFeb 22 18:11:09 <CENSORED>llbprmmid01.<CENSORED>.net dockerd[674413]: time=\"2024-02-22T18:11:09.545754661Z\" level=debug msg=\"memberlist: Stream connection from=192.168.32.112:42024\"\r\n```","comments":["Update: Today I tried to remove the node again, and it worked... It'd be great though to understand what happened. Because I've had this same issue before, and it got resolved on its own after I tried to remove the node again a couple of days later"],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/25.0"]},{"title":"api: normalize the default NetworkMode","body":"Related to:\r\n- https:\/\/github.com\/moby\/moby\/issues\/19421\r\n\r\n**- What I did**\r\n\r\nThe NetworkMode \"default\" is now normalized into the value it aliases (\"bridge\" on Linux and \"nat\" on Windows) by the ContainerCreate endpoint, and by the restore codepath.\r\n\r\nGoing forward, this will make maintenance easier as there's one less NetworkMode to care about.\r\n\r\nThis is purposefully done in the `api` package for a couple reasons: 1. to make sure it's done as early as possible; 2. because the `default` `NetworkMode` only exists to have cross-OS compatibility and thus is effectively just an API concern.\r\n\r\n2nd commit remove all the calls to `NetworkMode.IsDefault()` as this is now unneeded.\r\n\r\n**- How to verify it**\r\n\r\nFor the restore code path:\r\n\r\n1. Start the engine with `--live-restore` without this patch applied ;\r\n2. Create a container: `docker run -d --name c0 alpine top` ;\r\n3. Stop the engine and apply this patch ;\r\n4. Restart the engine ;\r\n5. `docker inspect c0`\r\n\r\nFor the `ContainerCreate` code path: step 2. and 5. above.","comments":["Would this be an API-breaking change? If I'm understanding correctly, inspecting a container created with the network mode \"default\" would be changed to return a `HostConfig.NetworkMode` of \"bridge\" or \"nat\" for all API versions. (Thankfully it looks like the `NetworkSettings.Networks` map keys are already normalized on container create, so that wouldn't be changing.) Clients which inspect arbitrary containers would already have to know about the default\/bridge\/nat equivalence so probably wouldn't be negatively impacted by this change, but clients which create containers might not handle it so gracefully.\r\n\r\nWould it be possible to achieve the same goal of making maintenance easier by improving the `NetworkMode` methods instead of normalizing the network mode string? With us normalizing the `NetworkSettings.Networks` map key, having `func (NetworkMode) NetworkName` return `\"default\"` is worse than useless. Similarly, `func (NetworkMode) UserDefined` returning the empty string is unhelpful. I suspect adding a method to `NetworkMode` which always returns the correct string key for `NetworkSettings.Networks` given any network mode which could accept network settings (all but host, container, none) would get us 90% of the way to the goal.","While normalizing the NetworkMode would change the output of the container inspect endpoint, and thus could be qualified as an API breaking-change, I think we have to weigh in the following:\r\n\r\n* As you pointed out, this normalization already happen at the `NetworkSettings` level once a container has been started. For any other `State` than `created`, there is a mismatch between `NetworkMode` and `NetworkSettings` and clients have to know about it ;\r\n* Unlike `host`, `bridge`, `null` and custom values, `default` is a network alias that can't be inspected (`GET \/networks\/default` returns `network default not found`) ;\r\n* Thus, `NetworkMode` itself doesn't provide any useful information unless used to look up `NetworkSettings` ;\r\n* The only field from `NetworkSettings` that can be set for the default network is `MacAddress` (at least on Linux, didn't check on Windows yet). All other fields are a zero value. Thus, you get pretty much nothing useful when inspecting a container right after creating it ;\r\n* The `MacAddress` field has been moved to `NetworkSettings` only in the latest API version. For older API versions, that field is still available in `Config` ;\r\n\r\nMy conclusion is that this would be a breaking change affecting clients that just get no value from what they do. I'm not sure whether we should care about such use case.\r\n\r\nOn the other hand, I think we also need to consider the benefits for us. I think having new methods characterizing the semantics of each `NetworkMode` could help, but that's orthogonal to this change. It seems that too often, when we work on libnet integration, we have to consider whether `NetworkSettings` entries and `NetworkMode` have been normalized yet (last example: https:\/\/github.com\/moby\/moby\/pull\/47375#discussion_r1487726357), making any code change in that area harder to write, test, review, etc...\r\n\r\nI wish more often than not we could rely exclusively on canonicalized values in the daemon so that we don't have to bother about these user-facing details. This would make the logic flow less contorted. Another way to achieve the same result, while preserving API compatibility, would be to first improve the separation between user-specified config and daemon state -- something we need to do anyway. That would allow to have a two-way tranform scheme between canonicalized and non-canonicalized forms."],"labels":["area\/api","status\/2-code-review","kind\/enhancement","area\/networking"]},{"title":"Error messages when attempting to use reserved IP addresses are unclear","body":"### Description\r\n\r\n\"In IPv6, all zeros and all ones are legal values for any field, unless specifically excluded.\" ([RFC 4291, section 2](https:\/\/www.rfc-editor.org\/rfc\/rfc4291#section-2))\r\n\r\n\"This anycast address is syntactically the same as a unicast address for an interface on the link with the interface identifier set to zero.\" ([RFC 4291, section 2.6.1](https:\/\/www.rfc-editor.org\/rfc\/rfc4291.html#section-2.6.1))\r\n\r\nWhen attempting to use an all-zero host part in an IPv6 address, an unclear error message is returned  (`Address already in use`). It should be made reference to `Subnet-Router anycast address`.\r\n\r\nWhen attempting to use a broadcast IPv4 address, an unclear error message is returned: `Address already in use` instead of for example `Broadcast address`\r\n\r\nThis is unclear what behavior for all-zero host part is expected on IPv4 as well since the introduction of Classless Inter-Domain Routing. This may sound counter intuitive, but I believe preventing all-zero host part is useless with modern systems, and in practice this works fine with `iproute2`.\r\n\r\n### Reproduce\r\n\r\n1. `docker network create --driver=bridge --subnet=fd00::\/80 --gateway=fd00:: test1`\r\n```\r\nError response from daemon: failed to allocate gateway (fd00::): Address already in use\r\n```\r\n\r\nor\r\n1. `docker network create --driver=bridge --subnet=fd00::\/80 --gateway=fd00::1 test2`\r\n2. `docker run --network=test2 --ip6=fd00:: nginx`\r\n```\r\ndocker: Error response from daemon: Address already in use.\r\nERRO[0000] error waiting for container: context canceled `\r\n```\r\n\r\nor\r\n1. `docker network create --driver=bridge --subnet=10.0.1.0\/24 --gateway=10.0.1.255 test3`\r\n```\r\nError response from daemon: failed to allocate gateway (10.0.1.255): Address already in use\r\n```\r\n\r\n### Expected behavior\r\n\r\n- For IPv6, the error message should be instead `Subnet-Router anycast address`.\r\n- For IPv4, the error message should be instead `Broadcast address`.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:09:46 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:09:46 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community      \r\n Version:    25.0.1                               \r\n Context:    default             \r\n Debug Mode: false       \r\n Plugins:                                                    \r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1 \r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2                                      \r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n                                                                    \r\nServer:                                                     \r\n Containers: 4                                                      \r\n  Running: 0  \r\n  Paused: 0          \r\n  Stopped: 4\r\n Images: 56            \r\n Server Version: 25.0.1\r\n Storage Driver: btrfs                   \r\n  Btrfs:                         \r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2        \r\n Plugins:              \r\n  Volume: local             \r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive                                  \r\n Runtimes: io.containerd.runc.v2 runc                                                                                                   \r\n Default Runtime: runc             \r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-15-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.29GiB\r\n Name: CometTail\r\n ID: b72da995-0839-4c58-afc5-a42b17d9b1ae\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 10.0.0.0\/8, Size: 24\r\n   Base: fd00:d0cc:e700:1111::\/64, Size: 80\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["I edited the issue because I just remembered that this field is specifically excluded (section 2.6.1). This demonstrate why precise error messages are important."],"labels":["status\/0-triage","kind\/bug","area\/networking","version\/25.0"]},{"title":"ci\/actions\/go-setup: Use `vendor.sum` as `go.sum`","body":"actions\/go-setup tries to restore the go cache, but expects to read the `go.sum` which we don't have:\r\n```\r\nRun actions\/setup-go@v5\r\n...\r\nFound in cache @ \/opt\/hostedtoolcache\/go\/1.21.7\/x64\r\n...\r\n\/opt\/hostedtoolcache\/go\/1.21.7\/x64\/bin\/go env GOMODCACHE\r\n\/opt\/hostedtoolcache\/go\/1.21.7\/x64\/bin\/go env GOCACHE\r\n\/home\/runner\/go\/pkg\/mod\r\n\/home\/runner\/.cache\/go-build\r\nWarning: Restore cache failed: Dependencies file is not found in \/home\/runner\/work\/moby\/moby. Supported file pattern: go.sum\r\n```\r\n\r\nMake it use `vendor.sum` instead.\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Do we know what it's trying to restore? Given that we use vendoring, I'm wondering what module cache it would try to use outside of that. If there's none, perhaps we need to disable the cache \ud83e\udd14 ","It also restores the go build cache (`go env GOCACHE`). Basically it's using the hash of `go.sum` file to make a cache key, so the cache is invalidated whenever `go.sum` changes.","I don't see any meaningful impact on the jobs execution time though. So maybe we can just disable that.."],"labels":["kind\/enhancement","area\/testing","area\/performance"]},{"title":"daemon: add a deprecation warning about links on the default nw","body":"**- What I did**\r\n\r\n\r\nThis feature is deprecated and removed in a couple releases, when we make the default bridge network just another bridge network.\r\n\r\n**- How to verify it**\r\n\r\n```\r\n$ docker run --rm -d --name=c0 alpine top\r\n$ docker run --rm -it --link=c0 --name=c1 alpine true\r\nWARNING: Links on the default bridge network are deprecated and will be removed in a future release. You should use a custom network instead.\r\n```\r\n\r\n**- Description for the changelog**\r\n\r\n* The Engine now returns a deprecation warning when a container connected to the default bridge is created with links specified.\r\n","comments":["@akerouanton PTAL"],"labels":["status\/2-code-review","area\/networking","impact\/changelog","impact\/deprecation","area\/daemon"]},{"title":"docker run & docker-compose run fails sharing nvidia-gpu capabilities","body":"### Description\r\n\r\nI've been trying to share nvidia-gpu (for cuda\/compute) to docker-container as described in:\r\n- https:\/\/docs.docker.com\/compose\/gpu-support\/\r\n- https:\/\/docs.docker.com\/config\/containers\/resource_constraints\/\r\n- https:\/\/medium.com\/@jared.ratner2\/setting-up-docker-and-docker-compose-with-nvidia-gpu-support-on-linux-716db95c0f7c\r\n\r\nI'm using Ubuntu 22.04 and installed recent docker and nvidia-docker2 + nvidia-container-toolkit:\r\n\r\n### Reproduce\r\n\r\n1. sudo apt-get install nvidia-docker2 nvidia-container-toolkit\r\n2. I've added nvidia-runtime to host's docker daemon.json and restarter docker-service.\r\n```\r\n  {\r\n  \"runtimes\": {\r\n    \"sysbox-runc\": {\r\n       \"path\": \"\/usr\/bin\/sysbox-runc\"\r\n    },\r\n    \"nvidia\": {\r\n       \"path\": \"nvidia-container-runtime\",\r\n       \"runtimeArgs\": []\r\n    }\r\n  },\r\n  \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]\r\n  }\r\n```\r\n\r\n3. docker run --rm -ti --gpus all --entrypoint nvidia-smi nvidia\/cuda:12.3.1-runtime-ubuntu22.04\r\nFailed to initialize NVML: Unknown Error\r\n\r\n4. inside container: ls -lah \/dev\/nvidia* shows up nvidia-devices\r\n\r\n5. Trying with docker-compose.yml results in the same problem:\r\n```\r\nversion: '3.8'\r\n\r\nservices:\r\n  nvidia-smi:\r\n    image: nvidia\/cuda:12.3.1-runtime-ubuntu22.04\r\n    runtime: nvidia\r\n    environment:\r\n      - NVIDIA_VISIBLE_DEVICES=all\r\n      - NVIDIA_DRIVER_CAPABILITIES=all\r\n    command: nvidia-smi\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - capabilities:\r\n                - gpu\r\n                - compute\r\n            - driver: nvidia\r\n              #device_ids: ['0']\r\n              capabilities: [gpu]\r\n```\r\n\r\n### Expected behavior\r\n\r\nNVIDIA\/Compute sharing should work as documented in docker docs!\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Thu Feb  1 00:23:03 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.2\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       fce6e0c\r\n  Built:            Thu Feb  1 00:23:03 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n16:39:33\r\nClient: Docker Engine - Community\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.3\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 17\r\n  Running: 14\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 106\r\n Server Version: 25.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: local\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 nvidia runc sysbox-runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-10022-tuxedo\r\n Operating System: Ubuntu 23.04\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 32\r\n Total Memory: 62.53GiB\r\n Name: Gabriel-Tuxedo\r\n ID: 68d1049d-4416-42f0-a884-8fcbc24145ce\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: typoworx\r\n Experimental: false\r\n Insecure Registries:\r\n  registry-api.php.docker\r\n  registry.php.docker\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 192.168.165.0\/24, Size: 24\r\n   Base: 172.30.0.0\/16, Size: 24\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nnvidia-container-cli --load-kmods info                                                                                                                                                                                                                                                                                                                                      16:51:59\r\n[sudo] password for gabriel: \r\nNVRM version:   535.146.02\r\nCUDA version:   12.2\r\n\r\nDevice Index:   0\r\nDevice Minor:   0\r\nModel:          NVIDIA GeForce RTX 4060 Laptop GPU\r\nBrand:          GeForce\r\nGPU UUID:       GPU-46eb0b05-a309-1169-2f8d-e076379b85a3\r\nBus Location:   00000000:01:00.0\r\nArchitecture:   8.9","comments":["Actually I tried this tutorial which shows a repository with updates I installed for nvidia-docker2\r\nhttps:\/\/docs.nvidia.com\/datacenter\/cloud-native\/container-toolkit\/latest\/install-guide.html#installing-with-apt\r\n\r\nAfter upgrading the packages I ran:\r\n`sudo nvidia-ctk runtime configure --runtime=docker`\r\n\r\nBut nvidia-smi still refuses to work:\r\nFailed to initialize NVML: Unknown Error","Obviously it worked after using privileged capabilitiies. This is nowhere documentated and I even think it should be avoided if possible only exposing nvidia-gpu not exposing full privileges."],"labels":["status\/0-triage","kind\/bug"]},{"title":"Mv fails in buildx when running on zfs","body":"### Description\n\nI\u2019m using buildx to build a docker container.\r\n\r\nMy docker file has been working fine until I added a second drive to my zfs rpool (not mirrored).\r\n\r\nNow when I try to run a \u2018mv\u2019 command in my docker file it fails:\r\n\r\n#15 0.592 mv: cannot move '\/tmp\/webp\/unzipped\/libwebp-1.3.2-linux-x86-64\/bin\/cwebp' to a subdirectory of itself, '\/usr\/bin\/cwebp'\r\n#15 ERROR: process \"\/bin\/sh -c mv \/tmp\/webp\/unzipped\/libwebp-1.3.2-linux-x86-64\/bin\/cwebp \/usr\/bin\/cwebp\" did not complete successfully: exit code: 1\r\n------\r\n > [10\/21] RUN mv \/tmp\/webp\/unzipped\/libwebp-1.3.2-linux-x86-64\/bin\/cwebp \/usr\/bin\/cwebp:\r\nAs you can see the target directory isn\u2019t a subdirectory of the source.\r\n\r\nIs there some way of fixing this - now that I\u2019ve added the second drive to the rpool the only way to to back is to completely rebuild my system :D.\n\n### Reproduce\n\ncreate zfs volume with two drives in rpool (non-mirrored)\r\nTry use the 'RUN mv' command to move a file within the container\r\n\r\nMy zpool\r\n\r\n```\r\nzpool status\r\n  pool: bpool\r\n state: ONLINE\r\n  scan: scrub repaired 0B in 00:00:00 with 0 errors on Sun Jan 14 00:24:01 2024\r\nconfig:\r\n\r\n\tNAME         STATE     READ WRITE CKSUM\r\n\tbpool        ONLINE       0     0     0\r\n\t  nvme0n1p3  ONLINE       0     0     0\r\n\r\nerrors: No known data errors\r\n\r\n  pool: rpool\r\n state: ONLINE\r\nstatus: Some supported and requested features are not enabled on the pool.\r\n\tThe pool can still be used, but some features are unavailable.\r\naction: Enable all features using 'zpool upgrade'. Once this is done,\r\n\tthe pool may no longer be accessible by software that does not support\r\n\tthe features. See zpool-features(7) for details.\r\n  scan: scrub repaired 0B in 00:06:55 with 0 errors on Sun Jan 14 00:30:56 2024\r\nconfig:\r\n\r\n\tNAME         STATE     READ WRITE CKSUM\r\n\trpool        ONLINE       0     0     0\r\n\t  nvme0n1p4  ONLINE       0     0     0\r\n\t  nvme1n1p1  ONLINE       0     0     0\r\n\r\n```\r\nFile system\r\n```\r\ndf -h\r\nFilesystem                                        Size  Used Avail Use% Mounted on\r\ntmpfs                                             3.2G  324M  2.9G  11% \/run\r\nrpool\/ROOT\/ubuntu_c520d1                          512G   20G  492G   4% \/\r\ntmpfs                                              16G  244M   16G   2% \/dev\/shm\r\ntmpfs                                             5.0M  4.0K  5.0M   1% \/run\/lock\r\ntmpfs                                              16G     0   16G   0% \/run\/qemu\r\nbpool\/BOOT\/ubuntu_c520d1                          1.1G  434M  643M  41% \/boot\r\n\/dev\/nvme0n1p1                                    511M   19M  493M   4% \/boot\/efi\r\nrpool\/USERDATA\/root_b4334o                        493G  846M  492G   1% \/root\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/log                  493G  999M  492G   1% \/var\/log\r\nrpool\/ROOT\/ubuntu_c520d1\/usr\/local                493G  581M  492G   1% \/usr\/local\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/lib                  507G   15G  492G   3% \/var\/lib\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/games                492G  128K  492G   1% \/var\/games\r\nrpool\/ROOT\/ubuntu_c520d1\/srv                      492G  128K  492G   1% \/srv\r\nrpool\/USERDATA\/xxxxx_b4334o                     560G   68G  492G  13% \/home\/xxxxx\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/snap                 492G   13M  492G   1% \/var\/snap\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/mail                 492G  128K  492G   1% \/var\/mail\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/spool                492G  2.4M  492G   1% \/var\/spool\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/www                  492G  128K  492G   1% \/var\/www\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/lib\/NetworkManager   492G  256K  492G   1% \/var\/lib\/NetworkManager\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/lib\/AccountsService  492G  128K  492G   1% \/var\/lib\/AccountsService\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/lib\/apt              492G   99M  492G   1% \/var\/lib\/apt\r\nrpool\/ROOT\/ubuntu_c520d1\/var\/lib\/dpkg             492G   76M  492G   1% \/var\/lib\/dpkg\r\nrpool\/var\/lib\/docker                              506G   14G  492G   3% \/var\/lib\/docker\r\ntmpfs                                             3.2G  176K  3.2G   1% \/run\/user\/1000\r\n\r\n```\n\n### Expected behavior\n\nThe mv command should work as expected.\n\n### docker version\n\n```bash\ndocker version\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.3\r\n Git commit:        24.0.5-0ubuntu1~22.04.1\r\n Built:             Mon Aug 21 19:50:14 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.3\r\n  Git commit:       24.0.5-0ubuntu1~22.04.1\r\n  Built:            Mon Aug 21 19:50:14 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.2\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.7-0ubuntu1~22.04.2\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\n```\n\n\n### docker info\n\n```bash\ndocker info\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n\r\nServer:\r\n Containers: 7\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 78\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: zfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-39-generic\r\n Operating System: Ubuntu 22.04.4 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 31.28GiB\r\n Name: slayer4\r\n ID: 97782504-1188-4e80-b500-ce4d4eb402b6\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: onepub\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Dockerfile cache mount mode=0777 does not set go+w permissions","body":"### Description\r\n\r\nThe `RUN --mount=type=cache,mode=0777` instruction in a  Dockerfile does not set write permissions for group and other for that directory (so you get `drwxr-xr-x`). The documentation does not mention that this is not supposed to work.\r\n\r\n### Reproduce\r\n\r\n1. Create a Dockerfile:\r\n```Dockerfile\r\nFROM busybox\r\n\r\nRUN --mount=type=cache,target=\/cache,mode=0777  ls -al \/cache && exit 1\r\n```\r\n2. Run docker build -t test .\r\n\r\nThis produces:\r\n\r\n```\r\n => ERROR [stage-0 2\/2] RUN --mount=type=cache,target=\/cache,mode=0777 ls -al \/cache && exit 1                                                                       0.3s\r\n------                                                                                                                                                                                \r\n > [stage-0 2\/2] RUN --mount=type=cache,target=\/cache,mode=0777 ls -al \/cache && exit 1:                                                                                  \r\n0.237 total 0                                                                                                                                                                         \r\n0.237 drwxr-xr-x    1 root     root             0 Feb 20 14:34 .\r\n0.237 drwxr-xr-x    1 root     root           104 Feb 20 15:16 ..\r\n------\r\nDockerfile:3\r\n--------------------\r\n   1 |     FROM busybox\r\n   2 |     \r\n   3 | >>> RUN --mount=type=cache,target=\/cache,mode=0777 ls -al \/cache && exit 1\r\n   4 |     \r\n--------------------\r\nERROR: failed to solve: process \"\/bin\/sh -c ls -al \/cache && exit 1\" did not complete successfully: exit code: 1\r\n```\r\n\r\n### Expected behavior\r\n\r\nI would have expected `drwxrwxrwx` permissions to be printed for `.`, instead I get `drwxr-xr-x` for `.`.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629222\r\n Built:             Thu Feb  1 10:50:44 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          25.0.2\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       fce6e0ca9b\r\n  Built:            Thu Feb  1 10:50:44 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          v1.7.13\r\n  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8.m\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.12.1\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.24.6\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 25.0.2\r\n Storage Driver: btrfs\r\n  Btrfs: \r\n Logging Driver: local\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7c3aca7a610df76212171d200ca3811ff6096eb8.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  userns\r\n  cgroupns\r\n Kernel Version: 6.7.5-arch1-1\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 30.84GiB\r\n Name: nac38760\r\n ID: 54a1e9c2-c4df-45c1-ab0c-6ee40ceaff88\r\n Docker Root Dir: \/var\/lib\/docker\/231072.231072\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Thanks for reporting; I suspect the default umask (0022) is applied here, which makes the 0777 -> 0755.\r\n\r\nChanging the mode to 0700 shows that those permissions are applied, so the `mode` is passed, but limited by the umask;\r\n\r\n```\r\n#7 [stage-0 2\/2] RUN --mount=type=cache,target=\/cache,mode=0700  ls -al \/cache && exit 1\r\n#7 0.102 total 8\r\n#7 0.102 drwx------    2 root     root          4096 Feb 20 15:16 .\r\n#7 0.102 drwxr-xr-x    1 root     root          4096 Feb 20 15:17 ..\r\n```\r\n\r\nInterestingly it looks to work with a container-builder (but in that case it defaults to skipping dockerd and containerd for the containers, and directly using runc);\r\n\r\n```bash\r\ndocker builder create --name mybuilder\r\ndocker build --builder=mybuilder --progress=plain .\r\n# ...\r\n#8 [stage-0 2\/2] RUN --mount=type=cache,target=\/cache,mode=0777  ls -al \/cache && exit 1\r\n#8 0.069 total 8\r\n#8 0.069 drwxrwxrwx    2 root     root          4096 Feb 20 15:21 .\r\n#8 0.069 drwxr-xr-x    1 root     root          4096 Feb 20 15:21 ..\r\n#8 ERROR: process \"\/bin\/sh -c ls -al \/cache && exit 1\" did not complete successfully: exit code: 1\r\n```","\/cc @tonistiigi @crazy-max in case you have ideas where the difference could originate from."],"labels":["area\/builder","status\/0-triage","kind\/bug","area\/builder\/buildkit","version\/25.0"]},{"title":"Docker container stops resolve names","body":"### Description\r\n\r\nIn RHEL 8.9 (Docker (```v25.0.1```), docker compose (```v2.24.2```) creates a container by docker-compose.yml:\r\n```\r\nversion: '3.4'\r\nservices:\r\n  <service>:\r\n    image: <image>\r\n    restart: always\r\n    privileged: true\r\n    environment:\r\n      HTTPS_PROXY: \"<HTTPS_PROXY>\"\r\n    volumes:\r\n      - \/var\/run\/docker.sock:\/var\/run\/docker.sock\r\n      - \/home\/\\<user\\>\/\\<folder\\>\/config:\/etc\/\\<folder\\>\r\n```\r\nAfter some time container stops resolving names:\r\n```\r\ndocker exec -it <container> nslookup google.com\r\n;; connection timed out; no servers could be reached\r\n```\r\n\r\n### Reproduce\r\n\r\n1. Docker compose up docker-compose.yml\r\n2. (After some time): docker exec -it <container> nslookup google.com\r\n\r\n### Expected behavior\r\nDocker container doesn't stop resolving names.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:10:32 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:09:31 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\ndocker info\r\nClient: Docker Engine - Community\r\n Version:    25.0.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 15\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 14\r\n Images: 12\r\n Server Version: 25.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-513.11.1.el8_9.x86_64\r\n Operating System: Red Hat Enterprise Linux 8.9 (Ootpa)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 31.22GiB\r\n Name: <server_name>\r\n ID: b0678201-93f2-4768-831c-bc35cef26a5b\r\n Docker Root Dir: \/data\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http:\/\/<server_name>:80\/\r\n HTTPS Proxy: http:\/\/<server_name>:80\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nDocker logs from container:\r\n```\r\n...\r\nWARNING: Checking for jobs... failed    runner=<runner> status=couldn't execute POST against https:\/\/gitlab.dx1.<name>.com\/api\/v4\/jobs\/request: Post \"https:\/\/gitlab.dx1.<name>.com\/api\/v4\/jobs\/request\": proxyconnect tcp: dial tcp: lookup <server_name> on 127.0.0.11:53: read udp 127.0.0.1:43286->127.0.0.11:53: i\/o timeout\r\nWARNING: Checking for jobs... failed    runner=<runner> status=couldn't execute POST against https:\/\/gitlab.dx1.<name>.com\/api\/v4\/jobs\/request: Post \"https:\/\/gitlab.dx1.<name>.com\/api\/v4\/jobs\/request\": proxyconnect tcp: dial tcp: lookup <server_name> on 127.0.0.11:53: read udp 127.0.0.1:58358->127.0.0.11:53: i\/o timeout\r\n...\r\n```","comments":["Hi @fireman777, thanks for reporting. Can you try turning on the daemon debug logs (see https:\/\/docs.docker.com\/config\/daemon\/logs\/#enable-debugging) and paste the log lines about DNS stuff written around the time resolution starts failing please?","> Hi @fireman777, thanks for reporting. Can you try turning on the daemon debug logs (see https:\/\/docs.docker.com\/config\/daemon\/logs\/#enable-debugging) and paste the log lines about DNS stuff written around the time resolution starts failing please?\r\n\r\n\r\nHi @akerouanton, thanks for the response. Here is an attached file with docker logs. It's not in debug mode, because if I enable this mode, I should restart Docker container and after this the issue dissappers.\r\n[docker_logs.txt](https:\/\/github.com\/moby\/moby\/files\/14347720\/docker_logs.txt)\r\n","Logs seem to indicate that the docker internal DNS is unable to connect to the DNS server(s) configured on the host;\r\n\r\n```\r\n[resolver] failed to query DNS server: 10.115.11.146:53, query: ;webproxy.hzl.mgmt.services.\\tIN\\t A\" error=\"read udp 172.19.0.2:46361->10.115.11.146:53: i\/o timeout\r\n[resolver] failed to query DNS server: 10.44.139.225:53, query: ;webproxy.hzl.mgmt.services.\\tIN\\t A\" error=\"read udp 172.19.0.2:53991->10.44.139.225:53: i\/o timeout\r\n```\r\n","Yes, @thaJeztah, the problem with the network doesn't allow us to resolve names.\r\nI can't ping any host from the container (previously I could do it).\r\nAfter the docker service restarts it disappears for some time.","@fireman777 Can you paste the output of `docker network inspect` for your network please?","Yes, sure (file is attached).\r\n[network_inspect.txt](https:\/\/github.com\/moby\/moby\/files\/14348314\/network_inspect.txt)\r\nLabel \"com.docker.compose.version\": \"**1.29.2**\" still has a version of a previous version, but I don't think it's critical, since previously (before docker update) the problem was the same. We updated docker\/docker compose  to fix this problem, but it didn't help.","I've started docker in debug logging mode and restarted it. As soon as docker will face network connectivity issue, I'll add debug logs. "],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/dns"]},{"title":"cmd\/dockerd: assorted changes to improve context-passing, fix Windows config loading","body":"### cmd\/dockerd: move Windows code for data-root\r\n\r\nUnlike Linux, which uses fixed locations as default, the Windows daemon uses\r\npaths relative to the data-root as defaults for storing both the PIDFile, and\r\nthe daemon configuration file (daemon.json).\r\n\r\nThe data-root is configurable both through command-line options (`--data-root`),\r\nand through the daemon configuration file (daemon.json). Unfortunately, this\r\nmeans that we have a chicken-and-egg situation at hand (we don't know where\r\nto look for the daemon.json file, but the daemon.json file itself may be\r\nchanging the path where to look for it).\r\n\r\nThis patch moves Windows-specific config for the config-file location to config-\r\nrelated code to help discoverability.\r\n\r\nFor the PIDfile, additional changes will be needed, as using a PIDfile depends\r\non whether the daemon is run as a service or not.\r\n\r\n### cmd\/dockerd: remove unused error-returns\r\n\r\ngetDefaultDaemonConfigDir would never return an error and because of that,\r\nneither would getDefaultDaemonConfigFile, so we can remove these error returns.\r\n\r\n### cmd\/dockerd: getDefaultDaemonConfigFile: add GoDoc for Windows implementation\r\n\r\nDocument why we cannot return a default on Windows.\r\n\r\n### cmd\/dockerd: set default configfile location as part of newDaemonOptions\r\n\r\nMake creating the options slightly more atomic, and set the defaults when\r\ninstancing the options.\r\n\r\n### cmd\/dockerd: rename loadCLIPlatformConfig to setPlatformOptions\r\n\r\nMake it more explicit that this function is mutating the passed\r\nconfiguration.\r\n\r\n### cmd\/dockerd: apply options when creating daemonCLI, not when starting\r\n\r\nValidate and apply options when creating the CLI, so that starting the\r\nCLI does not have to mutate the config, and to have a clearer separation\r\nbetween \"creating\", \"validating\", and starting the daemon.\r\n\r\nThis also allows skipping the service-registration code in situations\r\nwhere we only want to validate the config.\r\n\r\n\r\n### cmd\/dockerd: windows: move setting PIDFile location to setPlatformOptions\r\n\r\nUnlike Linux, which uses fixed locations as default, the Windows daemon uses\r\npaths relative to the data-root as defaults for storing both the PIDFile, and\r\nthe daemon configuration file (daemon.json).\r\n\r\nThe data-root is configurable both through command-line options (`--data-root`),\r\nand through the daemon configuration file (daemon.json). This patch moves Windows-\r\nspecific config handling to config-related code.\r\n\r\n### cmd\/dockerd: construct context in main\r\n\r\nConstruct the context we use in the main function, and set it as context\r\nfor the root-command.\r\n\r\n### cmd\/dockerd: un-export DaemonCli, NewDaemonCli\r\n\r\nThey're only used within this package, and are not expected to be used\r\nexternally. Some exported functions also take non-exported types as\r\nargument, so would not be usable outside of this package either way.\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n```\r\n- Fix location of `daemon.json` on Windows if a custom `--data-root` location is configured.\r\n```\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/cli","status\/2-code-review","area\/daemon","kind\/refactor"]},{"title":"overlay merge mountpoint disappear\uff0cerror starting setns process: fork\/exec \/proc\/self\/exe: no such file or directory: unknown","body":"### Description\r\n\r\n```\r\n$ docker ps -a | grep xxx-keep\r\nf7e6580b6359   xxx.xxx.lan:5000\/xxx\/keepalived:v2.2.0                                                             \"\/entrypoint.sh --do\u2026\"   2 weeks ago      Up 56 minutes                         xxx-keepalived-ipvs\r\n$ docker top xxx-keepalived-ipvs \r\nUID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD\r\nroot                1212761             1212741             0                   10:52               ?                   00:00:00            keepalived --dont-fork --log-console --log-detail --use-file=\/etc\/keepalived\/keepalived.conf\r\nroot                1213048             1212761             0                   10:52               ?                   00:00:05            keepalived --dont-fork --log-console --log-detail --use-file=\/etc\/keepalived\/keepalived.conf\r\n$ docker exec xxx-keepalived-ipvs bash\r\nOCI runtime exec failed: exec failed: unable to start container process: error starting setns process: fork\/exec \/proc\/self\/exe: no such file or directory: unknown\r\n$ docker inspect xxx-keepalived-ipvs | grep Pid\r\n            \"Pid\": 1212761,\r\n            \"PidMode\": \"\",\r\n            \"PidsLimit\": null,\r\n$ ls -l \/proc\/1212761\/cmdline \r\n-r--r--r-- 1 root root 0  2\u6708 20 10:52 \/proc\/1212761\/cmdline\r\n```\r\n\r\ncontainer is running ,but `merge` is disappear\r\n```\r\n$docker inspect xxx-keepalived-ipvs | grep -i merge\r\n                \"MergedDir\": \"\/data\/kube\/docker\/overlay2\/71668496b88bc08fd8523376e81054883d02f1dca1ebf15b3969774911413c75\/merged\",\r\n$ ls -l \/data\/kube\/docker\/overlay2\/71668496b88bc08fd8523376e81054883d02f1dca1ebf15b3969774911413c75\/merged\r\nls: cannot access '\/data\/kube\/docker\/overlay2\/71668496b88bc08fd8523376e81054883d02f1dca1ebf15b3969774911413c75\/merged': No such file or directory\r\n$ mount | grep 71668496b88bc08fd8523376e81054883d02f1dca1ebf15b3969774911413c75\r\n$ ls -l \/data\/kube\/docker\/overlay2\/71668496b88bc08fd8523376e81054883d02f1dca1ebf15b3969774911413c75\/\r\n\u603b\u7528\u91cf 8\r\ndrwxr-xr-x 7 root root  74  2\u6708  4 15:40 diff\r\n-rw-r--r-- 1 root root  26  2\u6708  4 15:40 link\r\n-rw-r--r-- 1 root root 115  2\u6708  4 15:40 lower\r\ndrwx------ 3 root root  18  2\u6708 20 10:52 work\r\n```\r\ncontainer process is working\r\n```\r\n[root@kylin10-sp2 ~]# nsenter -t 1212761 --mount\r\nkylin10-sp2:\/# ls -l\r\ntotal 4\r\ndrwxr-xr-x    2 root     root            25 Feb  4 15:40 always-initsh.d\r\ndrwxr-xr-x    1 root     root            67 Apr 20  2023 bin\r\ndrwxr-xr-x   14 root     root          3160 Feb 20 10:52 dev\r\n-rwxr-xr-x    1 root     root          1777 Apr 20  2023 entrypoint.sh\r\ndrwxr-xr-x    1 root     root            54 Feb  4 15:40 etc\r\ndrwxr-xr-x    2 root     root             6 Feb 11  2023 home\r\ndrwxr-xr-x    1 root     root            21 Feb  4 15:40 lib\r\ndrwxr-xr-x    5 root     root            44 Feb 11  2023 media\r\ndrwxr-xr-x    2 root     root             6 Feb 11  2023 mnt\r\ndrwxr-xr-x    2 root     root             6 Feb 11  2023 opt\r\ndr-xr-xr-x  394 root     root             0 Feb 20 10:52 proc\r\ndrwx------    2 root     root             6 Feb 11  2023 root\r\ndrwxr-xr-x    1 root     root            44 Feb 20 10:52 run\r\ndrwxr-xr-x    1 root     root           131 Feb  4 15:40 sbin\r\ndrwxr-xr-x    2 root     root             6 Feb 11  2023 srv\r\ndr-xr-xr-x   13 root     root             0 Feb 19 14:30 sys\r\ndrwxrwxrwt    2 root     root             6 Feb 11  2023 tmp\r\ndrwxr-xr-x    1 root     root            66 Feb 11  2023 usr\r\ndrwxr-xr-x    1 root     root            30 Feb 11  2023 var\r\nkylin10-sp2:\/# \r\nlogout\r\n[root@kylin10-sp2 ~]# nsenter -t  1212761  --mount --uts --ipc --net --pid\r\nkeepalived-1:\/# ps aux\r\nPID   USER     TIME  COMMAND\r\n    1 root      0:00 keepalived --dont-fork --log-console --log-detail --use-file=\/etc\/keepalived\/keepalived.conf\r\n   56 root      0:05 keepalived --dont-fork --log-console --log-detail --use-file=\/etc\/keepalived\/keepalived.conf\r\n   57 root      0:00 -bash\r\n   58 root      0:00 ps aux\r\nkeepalived-1:\/# \r\n```\r\n\ud83e\udd14 \r\n\r\n### Reproduce\r\n\r\nhaven't found a stable way to reproduce yet\r\nThis appears to have occurred after the container was restarted concurrently\r\n\r\n### Expected behavior\r\n\r\ndocker exec works\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:34:09 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:24 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.1\r\n  GitCommit:        1677a17964311325ed1c31e2c0a3589ce6d5c30d\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 41\r\n  Running: 40\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 241\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1677a17964311325ed1c31e2c0a3589ce6d5c30d\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.19.90-24.4.v2101.ky10.aarch64\r\n Operating System: Kylin Linux Advanced Server V10 (Sword)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 8\r\n Total Memory: 30.62GiB\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nhttps:\/\/github.com\/moby\/moby\/issues\/43402","comments":["I see you're running quite old versions of containerd and runc; are you still able to reproduce this when updating docker, containerd, and runc?\r\n\r\nWondering if this might be related to https:\/\/github.com\/containerd\/containerd\/pull\/9828 as well, but it would be useful to verify if this still reproduces on current versions.","> I see you're running quite old versions of containerd and runc; are you still able to reproduce this when updating docker, containerd, and runc?\r\n> \r\n> Wondering if this might be related to [containerd\/containerd#9828](https:\/\/github.com\/containerd\/containerd\/pull\/9828) as well, but it would be useful to verify if this still reproduces on current versions.\r\n\r\nIt has a very small chance of happening. I will try the latest version after I find the steps to reproduce it stably."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/24.0"]},{"title":"Allocate IPv6 addresses after detecting IPv6 support","body":"**- What I did**\r\n\r\nIf IPv6 is disabled for a container, do not allocate an IPv6 address when it's attached to an IPv6 network.\r\n\r\nFix https:\/\/github.com\/moby\/moby\/issues\/47055\r\n\r\n**- How I did it**\r\n\r\nThere are a few commits - it's probably easiest to review them separately ...\r\n\r\n_**Unconditionally update container.NetworkSettings**_\r\n\r\n In `daemon.allocateNetwork()`, flag `updateSettings` was set if `container.NetworkSettings.Networks` was empty. That flag was passed down through the call stack to avoid a call to `daemon.updateNetworkSettings()` in `daemon.updateNetworkConfig()`.\r\n\r\nOther calls to `daemon.updateNetworkSettings()`, which happen when connecting an existing container to another network, unconditionally made the NetworkSettings update.\r\n\r\nI don't think there's a circumstance where NetworkSettings is not empty during container creation. Even if there is, the update is harmless and cheap. Eliminating the `updateSettings` flag simplifies the code a little and, in the next commit where `allocateNetwork()` is split out of `initializeNetworking()`, it avoids the need to pass that flag between the two.\r\n\r\n_**Separate Sandbox\/Endpoint construction**_\r\n\r\nPreviously, the `libnetwork.Sandbox` was created in `daemon.allocateNetwork()` if there was no network - otherwise it happened while connecting an Endpoint in `daemon.connectToNetwork()`. If there was an endpoint in the default bridge, it had to be connected first, because extra configuration is needed in the Sandbox for legacy links and that could only happen during Sandbox construction.\r\n\r\nNow, config for legacy links is added to the Sandbox when constructing the Endpoint that needs it - removing the constraint on ordering of Endpoint construction, and the dependency between Endpoint and Sandbox construction.\r\n\r\nSo, now a Sandbox can be constructed in one place, before the first Endpoint.\r\n\r\nAlso, replaces some legacy-link specific Sandbox option-setters with Sandbox methods for updating `\/etc\/hosts`, and updates a legacy-link parent's IPv6 address as well as its IPv4 address when a child container restarts.\r\n\r\n_**Configure network endpoints after creating a container**_\r\n\r\nThis commit splits the bulk of `daemon.allocateNetwork()` out of `daemon.initializeNetworking()`.\r\n\r\n`daemon.initializeNetworking()` hasn't moved, but it now only prepares configuration and doesn't do any Endpoint construction or configuration. Sandbox construction still happens here.\r\n\r\nEndpoint construction is in `daemon.allocateNetwork()` which, on non-Windows, is called after the container task has been created (before it's started).\r\n\r\nOn Windows, Endpoint construction still happens before the container task is created. If it's created afterwards, some DNS lookups for the container don't seem to end up in our resolver - see the TODO comment in the code. (But, Windows can't benefit from the delayed assignment of IPv6 addresses anyway.)\r\n\r\nDoing the Endpoint construction after the `osSbox` has been set up makes it possible to probe the container's network for IPv6 support first - enabling the next commit ...\r\n\r\n_**Only assign IPv6 addresses if required**_\r\n\r\nIf a Sandbox is provided to `CreateEndpointForSandbox`, check whether it has IPv6 before allocating IPv6 addresses.\r\n\r\nBecause no IPv6 address is added to the DNS when the Sandbox doesn't support it, a lookup for a container on an IPv6 network can now have no IPv6 address. The resolver is updated to treat a missing IPv6 address on an IPv4 hit as `ipv6miss` even if the network has IPv6 enabled. (So, an empty DNS response is returned, avoiding the upstream DNS request and NXDOMAIN.)\r\n\r\nThe `macvlan` driver expected an endpoint on a network with IPv6 subnets to have an IPv6 address - it searched the subnets for the address assigned to the endpoint, to work out which gateway address to use (crashing if there was no address). Now, it only makes that search if there is an IPv6 address.\r\n\r\n**- How to verify it**\r\n\r\nExisting regression tests for the refactoring.\r\n\r\nNew integration test checks that a container with IPv6 disabled via `sysctl` on an IPv6 network has no IPv6 address.\r\n\r\n**- Description for the changelog**\r\n\r\nIf IPv6 is disabled for a container, do not allocate an IPv6 address when it's attached to an IPv6 network.","comments":["I've un-merged CreateEndpoint\/Endpoint.Join, so that the Sandbox can be configured for legacy links between the two.\r\n\r\nCreateEndpoint still needs to know whether to set up IPv6 addresses, so I've added CreateEndpointForSandbox that takes the Sandbox as a param. Once we get rid of legacy links perhaps we'll make it do the Join as well, but there's no need to do that as part of this change.\r\n\r\nMoving address assignment to Join would mean CreateEndpoint doesn't need to see the Sandbox, but it wouldn't help with legacy links ... Join would need a callback after allocating addresses, before doing the Join.\r\n\r\nThe final commit isn't intended to be merged in its current state, if we want to do it or something like it, the changes will need to be pushed down into the other commits - it's just an experiment\/proposal for removing references to legacy-links from the libnetwork code, while making the Sandbox update for legacy links type safe.\r\n\r\n> I do not want libnetwork to have any specific knowledge of containers or the daemon. The default bridge network is a daemon concept. Legacy links are a containers concept: containers have legacy links; sandboxes do not. The legacy links daemon feature happens to be implemented in terms of a combination of Sandbox options, but libnetwork does not need to know that.\r\n\r\nThat sounds good. But, the Sandbox options to implement legacy links are very legacy-linky, not generic things that we'll want to leave behind for other purposes when we drop support for legacy links. For example ...\r\n\r\n```\r\n\/\/ OptionParentUpdate function returns an option setter for parent container\r\n\/\/ which needs to update the IP address for the linked container.\r\nfunc OptionParentUpdate(cid string, name, ip string) SandboxOption {\r\n```\r\n\r\nEven `libnetwork.OptionGeneric` is only used for legacy links, but because it's disguised it's harder to reason about and the structure of the options is more complicated than it needs to be.\r\n\r\nSo, I don't think removing references to legacy links gives us anything but obfuscation. But, I know there are strong feelings on the subject - so, can do something like the change in the final commit? Or, is there a better approach?","> CreateEndpoint still needs to know whether to set up IPv6 addresses, so I've added CreateEndpointForSandbox that takes the Sandbox as a param.\r\n\r\nPlease, please, **please** just make it the daemon's responsibility to pass `libnetwork.CreateOptionDisableIPv6()` to `CreateEndpoint`, same as the way you had it before I said anything. `func buildCreateEndpointOptions` already derives other endpoint-create options from the sandbox and everything. Keep in mind that I wanted the Endpoint's IPv6-ness to be determined _at Join time_ so that the Endpoint config could not get out of sync with the Sandbox. `CreateEndpointForSandbox` does not address that concern, _but superficially appears to do so_ because it takes a `*Sandbox` parameter.\r\n\r\n--------\r\n\r\n> But, the Sandbox options to implement legacy links are very legacy-linky, not generic things that we'll want to leave behind for other purposes when we drop support for legacy links.\r\n\r\nMost of the Sandbox options to implement legacy links are very `\/etc\/hosts`-y. `OptionExtraHost` adds an entry to the sandbox's `\/etc\/hosts` file. It could just as easily be implemented as a `(*Sandbox)` method. `OptionParentUpdate` adds\/updates an entry in another sandbox's `\/etc\/hosts` file. The meat of the implementation, `func (*Sandbox) updateParentHosts`, only uses its receiver parameter for two things: to get the list of updates to apply, and to get the controller handle so it can look up the other sandboxes to apply the updates to. If provided with a `(*Sandbox)` method to update a host entry, the daemon could just as easily implement that behaviour itself! It's already enumerating all the sandbox IDs which need updating and the entries to update anyway.\r\n\r\n`OptionGeneric` is the only exception. The option data gets added to the data returned by `(*Sandbox).Labels()`, which in turn are passed into `(driverapi.Driver).Join` and `(driverapi.Driver).ProgramExternalConnectivity` because sandbox labels are endpoint driver options, obviously. (Sigh.) Oddly enough, the bridge driver respects the port binding and exposed ports options passed into `ProgramExternalConnectivity` but not `\"ParentEndpoints\"` or `\"ChildEndpoints\"`, despite the method reconfiguring parent\/child link iptables rules.\r\n\r\n`func (*Sandbox) AddLegacyLinks` therefore does three different things. It updates the \/etc\/hosts files of the receiver sandbox, it updates the \/etc\/hosts files of other sandboxes as specified by the `OptionParentUpdate` args, and it also updates the sandbox labels. The method call doesn't actually _do_ anything with the updated sandbox labels, however. Three distinct things should be three distinct functions.\r\n\r\n```go\r\n\/\/ strawman:\r\n\r\nfunc (*Sandbox) AddHostsEntry(name, ip string) \/\/ instead of OptionExtraHost\r\nfunc (*Sandbox) UpdateHostsEntry(name, ip string) \/\/ instead of OptionParentUpdate\r\nfunc (*Sandbox) UpdateLabels(map[string]interface{}) \/\/ instead of OptionGeneric\r\n```\r\n\r\nThere you go: type-safe and not obfuscated, just like you wanted. I'd even argue that it would _clarify_ legacy link behaviour since it makes explicit (on the daemon side) that a parent update is just an update to an entry in \/etc\/hosts of another sandbox. And it does not add any new references to legacy links to libnetwork, just like I wanted.\r\n\r\nNote that with my strawman, the equivalent daemon code to\r\n```go\r\nlibnetwork.OptionParentUpdate(parent.ID, alias, bridgeSettings.IPAddress)\r\n```\r\nwould be (approximately)\r\n```go\r\npsb, _ := daemon.netController.GetSandbox(parent.ID)\r\npsb.UpdateHostsEntry(alias, bridgeSettings.IPAddress)\r\n```\r\n\r\n`UpdateLabels` is still hard to reason about, but I don't see any better way that does not involve worse hacks or fundamentally rearchitecting core parts of libnetwork. There's no getting around the fact that the daemon has to do something else after updating the sandbox labels before the updated parent and child link lists take effect. Mentioning legacy links anywhere in the vicinity of such a method definition would imply behaviour it does not implement.\r\n\r\nAs for making `UpdateLabels` more type-safe, how about something like this?\r\n```go\r\npackage bridge \/\/ import \"github.com\/docker\/docker\/libnetwork\/drivers\/bridge\"\r\n\r\nfunc LegacyContainerLinkOptions(parentEndpoints, childEndpoints []string) map[string]interface{}\r\n```\r\n```go\r\nsb.UpdateLabels(bridge.LegacyContainerLinkOptions(parents, children))\r\n```\r\n\r\nI'm not picky about the name, but I feel pretty strongly about it living in the bridge driver package as the bridge driver is the only driver which consumes the parent\/child endpoint lists.","Ok - I think that's done.\r\n","Thank you for humouring me! I skimmed the diff and it's looking much, much better. I am especially happy with how `func (*Daemon) addLegacyLinks` turned out! I will do a full review pass this week."],"labels":["area\/networking","kind\/bugfix","area\/networking\/dns"]},{"title":"Ephemeral instances (--rm) block flush dirty cache on termination unnecessarily","body":"### Description\r\n\r\nI run ephemeral Docker containers on very large machines with an enormous quantity of RAM.\r\n\r\nIt is quite common to see 120GB or more of Dirty pages in \/proc\/meminfo at the end of the life of one of my containers.\r\n\r\nWhen the containers are killed by Docker CLI, or their entrypoint exits, Docker appears to block for upwards of 5-10 minutes while the entire Dirty page cache is written out to disk. This is entirely unnecessary for an ephemeral (--rm) container, because the resulting data is immediately deleted after being written out.\r\n\r\nIn the meantime I cannot start new containers, my SSD's are getting burnt out, and system load is crazy.\r\n\r\nI have tried:\r\nMounting a volume instead of overlays\r\nrm -rf \/ inside the container before termination\r\n\r\n### Reproduce\r\n\r\n```\r\necho 210000 > \/proc\/sys\/vm\/dirty_expire_centisecs\r\necho 210000 > \/proc\/sys\/vm\/dirty_writeback_centisecs\r\n\r\necho 85 > \/proc\/sys\/vm\/dirty_ratio\r\necho 65 > \/proc\/sys\/vm\/dirty_background_ratio\r\n```\r\n\r\n```\r\ndocker run --rm <some container that does a lot of writes on a machine that has lots of RAM>\r\n```\r\n\r\n```\r\ndocker kill <container> OR wait for container to die naturally\r\n```\r\n\r\nWatch \/proc\/meminfo 'Dirty' slowly trickle down to 0MB at the speed of your disks (actually quite a bit slower for some reason), before your instance is finally terminated and docker is functional again.\r\n\r\n### Expected behavior\r\n\r\nBecause the container is ephemeral and Docker knows it, it should immediately terminate and discard all Dirty pages.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:09:23 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:09:23 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 4\r\n Server Version: 25.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: nvidia runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-15-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 128\r\n Total Memory: 251.5GiB\r\n Name: \r\n ID:\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug","area\/performance","version\/25.0"]},{"title":"Docker swarm hangs 30 seconds on `bulk sync to node XXX timed out` (no container pops up on the node during this delay)","body":"### Description\n\nWhen undraining the node and [immediately] placing a docker service on this node the container doesn't show up in `docker ps` command for 30 sec \/ 1 min \/ 1.5 min. It correlates 100% with a `bulk sync to node XXX timed out` log in `journalctl -f -u docker`, even though I can see that all networks needed for container are already in place.\n\n### Reproduce\n\n1. Add Node to the swarm cluster \r\n2. Drain the node\r\n3. Start a docker service that will go to the new, drained node\r\n4. Undrain the node\r\n5. SSH into the node and run `watch docker ps`\r\n6. Observe that container (from the service) pops up with 30 sec delay every time you see `bulk sync to node XXX timed out` in `journalctl -f -u docker`\r\n    * it is not consistent. \r\n    * The delay of 30 sec only occurs when the `bulk sync to Node XXX timed out` log occurs. \r\n    * If this log occurs 2 times, the delay will be 30 sec * 2 = 1 minute, if 3 times --> 1.5 minutes\n\n### Expected behavior\n\nThe container will pop up immediately or as soon as its dependencies (network, volumes, configs) are loaded. I can see that all the networks are in place but still the container will not show up for 30 sec (or 1min\/1.5min) when `bulk sync to node XXX timed out` occurs.\n\n### docker version\n\n```bash\nlient: Docker Engine - Community\r\n Version:           20.10.14\r\n API version:       1.41\r\n Go version:        go1.16.15\r\n Git commit:        a224086\r\n Built:             Thu Mar 24 01:48:02 2022\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.14\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.15\r\n  Git commit:       87a90dc\r\n  Built:            Thu Mar 24 01:45:53 2022\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.12.1)\r\n  compose: Docker Compose (Docker Inc., v2.21.0)\r\n  scan: Docker Scan (Docker Inc., v0.23.0)\r\n\r\nServer:\r\n Containers: 61\r\n  Running: 43\r\n  Paused: 0\r\n  Stopped: 18\r\n Images: 50\r\n Server Version: 20.10.14\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: gelf\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 9v2qq2onq3hclsmaac59spva3\r\n  Is Manager: true\r\n  ClusterID: 6gidq8hutx5w9xxy5eakqxpqt\r\n  Managers: 1\r\n  Nodes: 6\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.3.45\r\n  Manager Addresses:\r\n   10.0.3.45:2377\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.15.0-1053-aws\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 30.83GiB\r\n Name: stagingManager1\r\n ID: USJP:CYGH:KIXW:Q3BP:JS5X:FH6Q:VTJ3:ZQPK:2UWR:FLP4:3MQT:C4F7\r\n Docker Root Dir: \/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0\/12, Size: 20\r\n   Base: 192.168.0.0\/16, Size: 24\n```\n\n\n### Additional Info\n\nThis is not a pulling image delay because the image is already on the node.\r\n\r\nIssues I created during this 1 week of struggling:\r\n* https:\/\/github.com\/moby\/swarmkit\/issues\/3166\r\n* https:\/\/github.com\/moby\/moby\/discussions\/47372","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"`docker swarm leave` removes port mapping from standalone containers started via docker-compose","body":"### Description\r\n\r\nLeaving a swarm (doesn't matter if as a manager or worker) causes port mappings of standalone containers that were started via docker-compose to be removed. The affected containers continue to run, and restarting either the containers or the docker daemon will fix the issue. A bit surprisingly, standalone containers started via docker commandline seem not affected by this issue.\r\n\r\n### Reproduce\r\n\r\n1. Start a docker-compose unit that has port mappings: `docker-compose up -d`\r\n2. `docker ps` will yield e.g. `0.0.0.0:80->8080\/tcp, :::80->8080\/tcp` under PORTS\r\n3. Run `docker swarm init`\r\n4. `docker ps` will still yield e.g. `0.0.0.0:80->8080\/tcp, :::80->8080\/tcp` under PORTS\r\n5. Run `docker swarm leave --force`\r\n6. `docker ps` will now yield an empty string under PORTS\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd expect standalone containers to stay unaffected by swarm, so ports shall stay mapped.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.3\r\n Git commit:        24.0.5-0ubuntu1~22.04.1\r\n Built:             Mon Aug 21 19:50:14 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.3\r\n  Git commit:       24.0.5-0ubuntu1~22.04.1\r\n  Built:            Mon Aug 21 19:50:14 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.7.2\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.7-0ubuntu1~22.04.2\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.20.2+ds1-0ubuntu1~22.04.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 52\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 51\r\n Images: 93\r\n Server Version: 24.0.5\r\n Storage Driver: btrfs\r\n  Btrfs: \r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-17-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 20\r\n Total Memory: 30.95GiB\r\n Name: darkly\r\n ID: af93a94e-b8ad-4377-bb2a-ee82460f7175\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: xxx\r\n Experimental: true\r\n Insecure Registries:\r\n  xxx\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"dns resolves to incorrect containers","body":"### Description\n\nDNS resolution of container name points to every container with the same name, instead of just the one that is associated with the stack\n\n### Reproduce\n\nsetup multiple stacks\r\nincluding a service that will be named the same on each stack.\r\ncreate an external network\r\nsetup traefik routing to containers\r\nresolve DNS for service name on container that is attached to traefik network\n\n### Expected behavior\n\nto resolve ip address of container without ip addresses of other containers on other stacks.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Thu Feb  1 00:24:30 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.2\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       fce6e0c\r\n  Built:            Thu Feb  1 00:22:57 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 37\r\n  Running: 34\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 36\r\n Server Version: 25.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: loki\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.14.0-362.18.1.el9_3.x86_64\r\n Operating System: AlmaLinux 9.3 (Shamrock Pampas Cat)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 9.42GiB\r\n Name: i-87815947\r\n ID: 26da125d-6f1d-466c-9dbd-f3b54d672f3d\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 329\r\n  Goroutines: 321\r\n  System Time: 2024-02-15T13:28:09.19064291Z\r\n  EventsListeners: 1\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0\/12, Size: 20\r\n   Base: 192.168.0.0\/16, Size: 24\n```\n\n\n### Additional Info\n\n_No response_","comments":["If all of those are connected to the same network, I think this is currently expected, as doing so puts them in the same networking \"sandbox\",  which means those services are allowed to connect, and resolve each-other.\r\n\r\nISTR there was a proposal to allow a network to be connected without DNS resolution, but we should look into that.\r\n","cc @akerouanton @robmry "],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/dns","version\/25.0"]},{"title":"Docker Swarm init ignores \u2013data-path-addr argument","body":"### Description\r\n\r\nIt seems `docker swarm init` ignores `--data-path-addr` argument when using it like \r\n```\r\ndocker swarm init --advertise-addr wg0 --listen-addr wg0 --data-path-addr wg0\r\n```\r\n\r\nWhen looking at `netstats`, the port 4789 is listened to on all IPs: \r\n```\r\n# netstat -tulpn\r\nActive Internet connections (only servers)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID\/Program name\r\ntcp        0      0 10.1.3.1:7946           0.0.0.0:*               LISTEN      21029\/dockerd\r\ntcp        0      0 10.1.3.1:2377           0.0.0.0:*               LISTEN      21029\/dockerd\r\nudp        0      0 10.1.3.1:7946           0.0.0.0:*                           21029\/dockerd\r\nudp        0      0 0.0.0.0:4789            0.0.0.0:*                           -\r\n```\r\n\r\n### Reproduce\r\n\r\nStart a fresh Debian VM\r\n\r\n```\r\n# Update Debian\r\nsudo apt update\r\nsudo apt -y upgrade\r\n\r\n# Install Wireguard\r\nsudo apt -y install wireguard\r\n\r\n# Generate private and public keys\r\nwg genkey | tee \/etc\/wireguard\/privatekey | wg pubkey > \/etc\/wireguard\/publickey\r\n\r\n# Read private key into a variable\r\nPRIVATE_KEY=$(cat \/etc\/wireguard\/privatekey)\r\n\r\n# Create WireGuard configuration\r\ncat <<EOF | sudo tee \/etc\/wireguard\/wg0.conf\r\n[Interface]\r\nPrivateKey = $PRIVATE_KEY\r\nAddress = 10.0.0.1\/24\r\nListenPort = 51820\r\nEOF\r\n\r\n# Adjust permissions for the configuration file\r\nsudo chmod 600 \/etc\/wireguard\/{wg0.conf,privatekey}\r\n\r\n# Start WireGuard interface\r\nsudo wg-quick up wg0\r\n\r\n# Enable WireGuard interface to start on boot\r\nsudo systemctl enable wg-quick@wg0\r\n\r\n# Install Docker\r\ncurl -fsSL https:\/\/get.docker.com | sh -\r\n\r\n# Init Docker Swarm\r\ndocker swarm init --advertise-addr wg0 --listen-addr wg0 --data-path-addr wg0\r\n\r\n# Check ports\r\nnetstat -tulpn | grep -E \"(2377|7946|4789)\"\r\n```\r\n\r\n### Expected behavior\r\n\r\nAll ports used by Docker should only be listening on the specified interface and it's IP addresses.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:14:25 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:25 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 25.0.3\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: active\r\n  NodeID: nfbe1z2ydd83flqna9lfb8nq5\r\n  Is Manager: true\r\n  ClusterID: jb2a4x51gj3epmjxmyi67wtne\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.0.1\r\n  Manager Addresses:\r\n   10.0.0.1:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-18-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 3\r\n Total Memory: 3.729GiB\r\n Name: debian-4gb-fsn1-1\r\n ID: 008bd6c7-9a1c-49ba-9c3a-bc0e855087bf\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nAs discussed on Docker Forum ([link](https:\/\/forums.docker.com\/t\/swarm-init-ignores-data-path-addr-argument\/139820)).","comments":["I shall add that a `docker swarm join --advertise-addr wg0 --listen-addr wg0 --data-path-addr wg0` yields a similar result.","Hi @thaJeztah, can we add a \"security issue\" label, too? \r\n\r\nI think it is quit an issue if I tell Docker to listen only on an internal interface, and instead it listens on all interfaces, on the public IP and is unexpectedly exposed to the whole Internet. "],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/swarm","version\/25.0"]},{"title":"[BUG]tftp not works with ipv6 only ipv4 with bridge mode","body":"### Description\r\n\r\n```bash\r\ndocker run -d \\\r\n  --name tftp_server1 \\\r\n  --restart unless-stopped \\\r\n  -p 69:69\/udp \\\r\n  -v ${PWD}\/dhv\/tftp:\/data \\\r\n  --network=my-ipv6-networktesttftp \\\r\n  mcchae\/tftp-server   in.tftpd -L -vvv -u root --secure --create -a 0.0.0.0:69 -a [::]:69 \/data\r\n```\r\n\r\nafter make this configuration i see my server only send and recieve on ipv4 address not ipv6 in bridge mode  changing the network to host work with ipv6\/ipv4   any possible bugs   ?  \r\n\r\n```\r\ntcpdump port 69 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 08:38:48.632633 IP 192.168.1.12.43681 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:38:55.963123 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:01.967434 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:07.972401 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:13.975605 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:19.980811 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 \r\n```     \r\n\r\n```bash\r\noot@mohamed:\/home\/roote\/docker-tftp-server# docker inspect 4fdec26bdd5a\r\n[\r\n    {\r\n        \"Id\": \"4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\",\r\n        \"Created\": \"2024-02-14T08:31:52.521885623Z\",\r\n        \"Path\": \"in.tftpd\",\r\n        \"Args\": [\r\n            \"-L\",\r\n            \"-vvv\",\r\n            \"-u\",\r\n            \"root\",\r\n            \"--secure\",\r\n            \"--create\",\r\n            \"-a\",\r\n            \"0.0.0.0:69\",\r\n            \"-a\",\r\n            \"[::]:69\",\r\n            \"\/data\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 189481,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2024-02-14T08:31:53.379596385Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:8d5994eaf723e7b52e8df205f7e07c87586a2c623415b02fef5554d41949a22d\",\r\n        \"ResolvConfPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/hostname\",\r\n        \"HostsPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/hosts\",\r\n        \"LogPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d-json.log\",\r\n        \"Name\": \"\/tftp_server1\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"docker-default\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": [\r\n                \"\/home\/roote\/docker-tftp-server\/dhv\/tftp:\/data\"\r\n            ],\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"my-ipv6-networktesttftp\",\r\n            \"PortBindings\": {\r\n                \"69\/udp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"69\"\r\n                    }\r\n                ]\r\n            },\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"unless-stopped\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"ConsoleSize\": [\r\n                47,\r\n                181\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"private\",\r\n            \"Dns\": [],\r\n            \"DnsOptions\": [],\r\n            \"DnsSearch\": [],\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": false,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": null,\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": [],\r\n            \"BlkioDeviceReadBps\": [],\r\n            \"BlkioDeviceWriteBps\": [],\r\n            \"BlkioDeviceReadIOps\": [],\r\n            \"BlkioDeviceWriteIOps\": [],\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": [],\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": null,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": [\r\n                \"\/proc\/asound\",\r\n                \"\/proc\/acpi\",\r\n                \"\/proc\/kcore\",\r\n                \"\/proc\/keys\",\r\n                \"\/proc\/latency_stats\",\r\n                \"\/proc\/timer_list\",\r\n                \"\/proc\/timer_stats\",\r\n                \"\/proc\/sched_debug\",\r\n                \"\/proc\/scsi\",\r\n                \"\/sys\/firmware\"\r\n            ],\r\n            \"ReadonlyPaths\": [\r\n                \"\/proc\/bus\",\r\n                \"\/proc\/fs\",\r\n                \"\/proc\/irq\",\r\n                \"\/proc\/sys\",\r\n                \"\/proc\/sysrq-trigger\"\r\n            ]\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a-init\/diff:\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/cfa4e80486591c668023cc379f9cf35571ae5fffb230ba4a711363feff063004\/diff:\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/1ab19feebb31692bc2e697b99b014182f18f0be3e41b105e5ec261291bb40eca\/diff\",\r\n                \"MergedDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/merged\",\r\n                \"UpperDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/diff\",\r\n                \"WorkDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/home\/roote\/docker-tftp-server\/dhv\/tftp\",\r\n                \"Destination\": \"\/data\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            }\r\n        ],\r\n        \"Config\": {\r\n            \"Hostname\": \"4fdec26bdd5a\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"ExposedPorts\": {\r\n                \"69\/udp\": {}\r\n            },\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n            ],\r\n            \"Cmd\": [\r\n                \"in.tftpd\",\r\n                \"-L\",\r\n                \"-vvv\",\r\n                \"-u\",\r\n                \"root\",\r\n                \"--secure\",\r\n                \"--create\",\r\n                \"-a\",\r\n                \"0.0.0.0:69\",\r\n                \"-a\",\r\n                \"[::]:69\",\r\n                \"\/data\"\r\n            ],\r\n            \"Image\": \"mcchae\/tftp-server\",\r\n            \"Volumes\": {\r\n                \"\/data\": {}\r\n            },\r\n            \"WorkingDir\": \"\",\r\n            \"Entrypoint\": null,\r\n            \"OnBuild\": null,\r\n            \"Labels\": {}\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"1aff0bbf380c1645348da962f5c028d389198f59ea344d58591da1680124e96c\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {\r\n                \"69\/udp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"69\"\r\n                    },\r\n                    {\r\n                        \"HostIp\": \"::\",\r\n                        \"HostPort\": \"69\"\r\n                    }\r\n                ]\r\n            },\r\n            \"SandboxKey\": \"\/run\/snap.docker\/netns\/1aff0bbf380c\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"\",\r\n            \"Gateway\": \"\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"\",\r\n            \"IPPrefixLen\": 0,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"\",\r\n            \"Networks\": {\r\n                \"my-ipv6-networktesttftp\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"4fdec26bdd5a\"\r\n                    ],\r\n                    \"NetworkID\": \"63411640649c06ce8632847a23f5775a3d55525d093a3a3ee601540a0a61c97d\",\r\n                    \"EndpointID\": \"693adab5dffd584dada77f528219f02dff8dc0ab337613c449edd08bc110aef9\",\r\n                    \"Gateway\": \"192.168.176.1\",\r\n                    \"IPAddress\": \"192.168.176.2\",\r\n                    \"IPPrefixLen\": 20,\r\n                    \"IPv6Gateway\": \"fd00:cafe:face:feea::1\",\r\n                    \"GlobalIPv6Address\": \"fd00:cafe:face:feea::2\",\r\n                    \"GlobalIPv6PrefixLen\": 64,\r\n                    \"MacAddress\": \"02:42:c0:a8:b0:02\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n### Reproduce\r\n\r\n```bash\r\ndocker run -d \\\r\n  --name tftp_server1 \\\r\n  --restart unless-stopped \\\r\n  -p 69:69\/udp \\\r\n  -v ${PWD}\/dhv\/tftp:\/data \\\r\n  --network=my-ipv6-networktesttftp \\\r\n  mcchae\/tftp-server   in.tftpd -L -vvv -u root --secure --create -a 0.0.0.0:69 -a [::]:69 \/data\r\n```\r\n\r\nafter make this configuration i see my server only send and recieve on ipv4 address not ipv6 in bridge mode  changing the network to host work with ipv6\/ipv4   any possible bugs   ?  \r\n\r\n```\r\ntcpdump port 69 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 08:38:48.632633 IP 192.168.1.12.43681 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:38:55.963123 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:01.967434 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:07.972401 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:13.975605 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 08:39:19.980811 IP6 mohamed.local.52760 > 4fdec26bdd5a.69:  49 RRQ\r\n \"hello.txt\" octet tsize 0 blksize 1468 timeout 5 \r\n```     \r\n\r\n```bash\r\nroot@mohamed:\/home\/roote\/docker-tftp-server# docker inspect 4fdec26bdd5a\r\n[\r\n    {\r\n        \"Id\": \"4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\",\r\n        \"Created\": \"2024-02-14T08:31:52.521885623Z\",\r\n        \"Path\": \"in.tftpd\",\r\n        \"Args\": [\r\n            \"-L\",\r\n            \"-vvv\",\r\n            \"-u\",\r\n            \"root\",\r\n            \"--secure\",\r\n            \"--create\",\r\n            \"-a\",\r\n            \"0.0.0.0:69\",\r\n            \"-a\",\r\n            \"[::]:69\",\r\n            \"\/data\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 189481,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2024-02-14T08:31:53.379596385Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:8d5994eaf723e7b52e8df205f7e07c87586a2c623415b02fef5554d41949a22d\",\r\n        \"ResolvConfPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/hostname\",\r\n        \"HostsPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/hosts\",\r\n        \"LogPath\": \"\/var\/snap\/docker\/common\/var-lib-docker\/containers\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d\/4fdec26bdd5ac367cca9db2c723ee1cee03bd0ecbd4b6a90c150b340548a841d-json.log\",\r\n        \"Name\": \"\/tftp_server1\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"docker-default\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": [\r\n                \"\/home\/roote\/docker-tftp-server\/dhv\/tftp:\/data\"\r\n            ],\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"my-ipv6-networktesttftp\",\r\n            \"PortBindings\": {\r\n                \"69\/udp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"69\"\r\n                    }\r\n                ]\r\n            },\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"unless-stopped\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"ConsoleSize\": [\r\n                47,\r\n                181\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"private\",\r\n            \"Dns\": [],\r\n            \"DnsOptions\": [],\r\n            \"DnsSearch\": [],\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": false,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": null,\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": [],\r\n            \"BlkioDeviceReadBps\": [],\r\n            \"BlkioDeviceWriteBps\": [],\r\n            \"BlkioDeviceReadIOps\": [],\r\n            \"BlkioDeviceWriteIOps\": [],\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": [],\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": null,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": [\r\n                \"\/proc\/asound\",\r\n                \"\/proc\/acpi\",\r\n                \"\/proc\/kcore\",\r\n                \"\/proc\/keys\",\r\n                \"\/proc\/latency_stats\",\r\n                \"\/proc\/timer_list\",\r\n                \"\/proc\/timer_stats\",\r\n                \"\/proc\/sched_debug\",\r\n                \"\/proc\/scsi\",\r\n                \"\/sys\/firmware\"\r\n            ],\r\n            \"ReadonlyPaths\": [\r\n                \"\/proc\/bus\",\r\n                \"\/proc\/fs\",\r\n                \"\/proc\/irq\",\r\n                \"\/proc\/sys\",\r\n                \"\/proc\/sysrq-trigger\"\r\n            ]\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a-init\/diff:\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/cfa4e80486591c668023cc379f9cf35571ae5fffb230ba4a711363feff063004\/diff:\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/1ab19feebb31692bc2e697b99b014182f18f0be3e41b105e5ec261291bb40eca\/diff\",\r\n                \"MergedDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/merged\",\r\n                \"UpperDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/diff\",\r\n                \"WorkDir\": \"\/var\/snap\/docker\/common\/var-lib-docker\/overlay2\/2fffebb95e1a191f9e38a89c1f019f10dafb5f4473aed6c238a2885ae1983d1a\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/home\/roote\/docker-tftp-server\/dhv\/tftp\",\r\n                \"Destination\": \"\/data\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            }\r\n        ],\r\n        \"Config\": {\r\n            \"Hostname\": \"4fdec26bdd5a\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"ExposedPorts\": {\r\n                \"69\/udp\": {}\r\n            },\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n            ],\r\n            \"Cmd\": [\r\n                \"in.tftpd\",\r\n                \"-L\",\r\n                \"-vvv\",\r\n                \"-u\",\r\n                \"root\",\r\n                \"--secure\",\r\n                \"--create\",\r\n                \"-a\",\r\n                \"0.0.0.0:69\",\r\n                \"-a\",\r\n                \"[::]:69\",\r\n                \"\/data\"\r\n            ],\r\n            \"Image\": \"mcchae\/tftp-server\",\r\n            \"Volumes\": {\r\n                \"\/data\": {}\r\n            },\r\n            \"WorkingDir\": \"\",\r\n            \"Entrypoint\": null,\r\n            \"OnBuild\": null,\r\n            \"Labels\": {}\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"1aff0bbf380c1645348da962f5c028d389198f59ea344d58591da1680124e96c\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {\r\n                \"69\/udp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"69\"\r\n                    },\r\n                    {\r\n                        \"HostIp\": \"::\",\r\n                        \"HostPort\": \"69\"\r\n                    }\r\n                ]\r\n            },\r\n            \"SandboxKey\": \"\/run\/snap.docker\/netns\/1aff0bbf380c\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"\",\r\n            \"Gateway\": \"\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"\",\r\n            \"IPPrefixLen\": 0,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"\",\r\n            \"Networks\": {\r\n                \"my-ipv6-networktesttftp\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"4fdec26bdd5a\"\r\n                    ],\r\n                    \"NetworkID\": \"63411640649c06ce8632847a23f5775a3d55525d093a3a3ee601540a0a61c97d\",\r\n                    \"EndpointID\": \"693adab5dffd584dada77f528219f02dff8dc0ab337613c449edd08bc110aef9\",\r\n                    \"Gateway\": \"192.168.176.1\",\r\n                    \"IPAddress\": \"192.168.176.2\",\r\n                    \"IPPrefixLen\": 20,\r\n                    \"IPv6Gateway\": \"fd00:cafe:face:feea::1\",\r\n                    \"GlobalIPv6Address\": \"fd00:cafe:face:feea::2\",\r\n                    \"GlobalIPv6PrefixLen\": 64,\r\n                    \"MacAddress\": \"02:42:c0:a8:b0:02\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```   \r\n\r\n### Expected behavior\r\n\r\ntftp should respond to ipv6 and ipv4 packets in bridge mode\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Cloud integration: v1.0.28\r\n Version:           24.0.0\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        98fdcd7\r\n Built:             Mon May 15 18:49:22 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.8\r\n  Git commit:       a61e2b4\r\n  Built:            Sat Oct  7 00:14:30 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0   ```\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.4\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.7.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-compose\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.8\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-extension\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.17.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 17\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 14\r\n Images: 30\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-17-generic\r\n Operating System: Ubuntu Core 22\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 3.78GiB\r\n Name: mohamed.com\r\n ID: 66227d64-325b-4678-8246-ef2d65e4e81e\r\n Docker Root Dir: \/var\/snap\/docker\/common\/var-lib-docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\ni have other services working fine with ipv6 and ipv4 so the issue only in tftp i found ","comments":["@thaJeztah  any help please ?","any help please i still didnt get response yet"],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/ipv6","version\/24.0"]},{"title":"Registry host configuration cleanup","body":"First commit copies the resolver code from buildkit with fix for insecure registries and could be backported.\r\nSecond commit cleans up the resolve logic using containerd's host config.\r\n\r\ncloses #47240 \r\n","comments":["Couple of failures that look related. Could be that the test was written for a specific error output though (these are the integration-cli tests, which use a fixed, old version of the CLI), or perhaps we're missing some conversion for errors returned :thinking:\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestBuildFromAuthenticatedRegistry (0.34s)\r\n    docker_cli_build_test.go:4988: assertion failed: \r\n        Command:  \/usr\/local\/cli-integration\/docker push 127.0.0.1:5000\/baseimage\r\n        ExitCode: 1\r\n        Error:    exit status 1\r\n        Stdout:   The push refers to repository [127.0.0.1:5000\/baseimage]\r\n        76df9210b28c: Waiting\r\n        \r\n        Stderr:   unexpected status from HEAD request to http:\/\/127.0.0.1:5000\/v2\/baseimage\/blobs\/sha256:76df9210b28cbd4bc127844914d0a23937ed213048dc6289b2a2d4f7d675c75e: 401 Unauthorized\r\n        \r\n        \r\n        Failures:\r\n        ExitCode was 1 expected 0\r\n        Expected no error\r\n    check_test.go:532: [da6a45cfad359] daemon is not started\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestBuildFromAuthenticatedRegistry (0.34s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestBuildWithExternalAuth (0.31s)\r\n    docker_cli_build_test.go:5024: assertion failed: \r\n        Command:  \/usr\/local\/cli-integration\/docker --config \/tmp\/integration-cli-1513645647 push 127.0.0.1:5000\/dockercli\/busybox:authtest\r\n        ExitCode: 1\r\n        Error:    exit status 1\r\n        Stdout:   The push refers to repository [127.0.0.1:5000\/dockercli\/busybox]\r\n        76df9210b28c: Waiting\r\n        \r\n        Stderr:   unexpected status from HEAD request to http:\/\/127.0.0.1:5000\/v2\/dockercli\/busybox\/blobs\/sha256:76df9210b28cbd4bc127844914d0a23937ed213048dc6289b2a2d4f7d675c75e: 401 Unauthorized\r\n        \r\n        \r\n        Failures:\r\n        ExitCode was 1 expected 0\r\n        Expected no error\r\n    check_test.go:532: [d254f721c0d6c] daemon is not started\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestBuildWithExternalAuth (0.31s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestLogoutWithExternalAuth (0.87s)\r\n    docker_cli_logout_test.go:53: assertion failed: error is not nil: exit status 1\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestLogoutWithExternalAuth (0.87s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestPullWithExternalAuth (0.29s)\r\n    docker_cli_pull_local_test.go:439: assertion failed: \r\n        Command:  \/usr\/local\/cli-integration\/docker --config \/tmp\/integration-cli-2597430029 push 127.0.0.1:5000\/dockercli\/busybox:authtest\r\n        ExitCode: 1\r\n        Error:    exit status 1\r\n        Stdout:   The push refers to repository [127.0.0.1:5000\/dockercli\/busybox]\r\n        76df9210b28c: Waiting\r\n        \r\n        Stderr:   unexpected status from HEAD request to http:\/\/127.0.0.1:5000\/v2\/dockercli\/busybox\/blobs\/sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807: 401 Unauthorized\r\n        \r\n        \r\n        Failures:\r\n        ExitCode was 1 expected 0\r\n        Expected no error\r\n    check_test.go:532: [d37782d3d4dbc] daemon is not started\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestPullWithExternalAuth (0.29s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestPullWithExternalAuthLoginWithScheme (0.29s)\r\n    docker_cli_pull_local_test.go:397: assertion failed: \r\n        Command:  \/usr\/local\/cli-integration\/docker --config \/tmp\/integration-cli-846080656 push 127.0.0.1:5000\/dockercli\/busybox:authtest\r\n        ExitCode: 1\r\n        Error:    exit status 1\r\n        Stdout:   The push refers to repository [127.0.0.1:5000\/dockercli\/busybox]\r\n        76df9210b28c: Waiting\r\n        \r\n        Stderr:   unexpected status from HEAD request to http:\/\/127.0.0.1:5000\/v2\/dockercli\/busybox\/blobs\/sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807: 401 Unauthorized\r\n        \r\n        \r\n        Failures:\r\n        ExitCode was 1 expected 0\r\n        Expected no error\r\n    check_test.go:532: [df1a930ceacf0] daemon is not started\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestPullWithExternalAuthLoginWithScheme (0.29s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite\/TestPushNoCredentialsNoRetry (0.17s)\r\n    docker_cli_push_test.go:225: assertion failed: expression is false: strings.Contains(out, \"no basic auth credentials\")\r\n    check_test.go:532: [da543621fde40] daemon is not started\r\n    --- FAIL: TestDockerRegistryAuthHtpasswdSuite\/TestPushNoCredentialsNoRetry (0.17s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerRegistryAuthHtpasswdSuite (2.88s)\r\n```","Marked this as ready to go. I fixed the authorizer with the containerd image service resolver. The other failures now don't seem related, look like flakes."],"labels":["area\/distribution","status\/2-code-review","area\/daemon","kind\/bugfix","kind\/refactor"]},{"title":"Improve the predicate for a network that uses an internal DNS server","body":"### Description\n\nFollow up from https:\/\/github.com\/moby\/moby\/pull\/47375 ...\r\n\r\nWe should have an easy way to test for a network that need configuration for an internal DNS server. Hopefully we can use the same test to decide whether to start the internal nameserver.\r\n\r\n`IsUserDefined()` is close, but misses some cases ... in the PR\/issue linked above, it missed the default `nat` network on Windows (called \"nat\"), which does have an internal resolver listening on the bridge's address. As a consequence, DNS names for containers were not populated in that network.\r\n\r\nIn [setupPathsAndSandboxOptions](https:\/\/github.com\/moby\/moby\/blob\/97478c99f8dfe5f59d80932efb17bb699f4d7030\/daemon\/container_operations_unix.go#L427-L469) it misses swarm-scoped networks that do have an internal resolver. So, these networks end up using systemd's underlying nameservers instead of `127.0.0.53`. (So, if the host's nameservers are updated, containers need a restart to pick up the new servers.)\r\n\r\nThere may be other places where the test could be improved.","comments":[],"labels":["kind\/enhancement","area\/networking"]},{"title":"Unable to build a simple Dockerfile with buildx where userns-remap and the containerd backend is enabled","body":"### Description\r\n\r\nWe are unable to build a simple image using buildx, with the docker buildkit driver, where the docker daemon is running with the following configuration:\r\n- userns remap is enabled\r\n- storage driver is set to overlayfs\r\n- the containerd snapshotter is enabled\r\n- using the built in docker buildx driver\r\n\r\nThe problem can be reproduced relatively easily with a simple image such as:\r\n\r\n`\r\nFROM alpine:latest\r\nRUN echo \"hello world\"\r\n`\r\n\r\n\r\n### Reproduce\r\n\r\nCommand:\r\n`docker buildx build -f Dockerfile .`\r\n\r\nResult:\r\n```\r\ndocker buildx build -f Dockerfile .\r\n\r\n#0 building with \"default\" instance using docker driver\r\n#1 [internal] load build definition from Dockerfile.alpine.buildx\r\n#1 transferring dockerfile: 92B done\r\n#1 DONE 0.0s\r\n#2 [internal] load metadata for docker.io\/library\/alpine:latest\r\n#2 DONE 0.3s\r\n#3 [internal] load .dockerignore\r\n#3 transferring context: 2B done\r\n#3 DONE 0.0s\r\n#4 [1\/2] FROM docker.io\/library\/alpine:latest@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b\r\n#4 resolve docker.io\/library\/alpine:latest@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b done\r\n#4 sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 3.41MB \/ 3.41MB 0.1s done\r\n#4 extracting sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 0.1s done\r\n#4 DONE 0.2s\r\n#5 [2\/2] RUN echo \"hello world\"\r\n#5 0.027 runc run failed: unable to start container process: error during container init: error mounting \"\/var\/lib\/docker\/165536.165536\/buildkit\/executor\/resolv.conf\" to rootfs at \"\/etc\/resolv.conf\": open \/var\/lib\/docker\/165536.165536\/buildkit\/executor\/30g5og94pc1it3ymc8ymjdpd8\/rootfs\/etc\/resolv.conf: permission denied\r\n#5 ERROR: process \"\/bin\/sh -c echo \\\"hello world\\\"\" did not complete successfully: exit code: 1\r\n------\r\n > [2\/2] RUN echo \"hello world\":\r\n0.027 runc run failed: unable to start container process: error during container init: error mounting \"\/var\/lib\/docker\/165536.165536\/buildkit\/executor\/resolv.conf\" to rootfs at \"\/etc\/resolv.conf\": open \/var\/lib\/docker\/165536.165536\/buildkit\/executor\/30g5og94pc1it3ymc8ymjdpd8\/rootfs\/etc\/resolv.conf: permission denied\r\n------\r\n[Dockerfile.alpine.buildx](Dockerfile.alpine.buildx):2\r\n--------------------\r\n   1 |     FROM alpine:latest\r\n   2 | >>> RUN echo \"hello world\"\r\n--------------------\r\nERROR: failed to solve: process \"\/bin\/sh -c echo \\\"hello world\\\"\" did not complete successfully: exit code: 1\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nThe container should be built successfully.\r\n\r\n\r\n### docker version\r\n\r\n```bash\r\n+ docker version\r\nClient:\r\n Version:           25.0.3\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        4debf41\r\n Built:             Tue Feb  6 21:13:00 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:13:08 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.13\r\n  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    25.0.3\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/local\/libexec\/docker\/cli-plugins\/docker-compose\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 25.0.3\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Authorization: pipelines\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 7c3aca7a610df76212171d200ca3811ff6096eb8\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\nWARNING: API is accessible on http:\/\/0.0.0.0:2375 without encryption.\r\n         Access to the remote API is equivalent to root access on the host. Refer\r\n         to the 'Docker daemon attack surface' section in the documentation for\r\n         more information: https:\/\/docs.docker.com\/go\/attack-surface\/\r\n  seccomp\r\n   Profile: builtin\r\n  userns\r\n Kernel Version: 5.15.0-1052-aws\r\n Operating System: Alpine Linux v3.19 (containerized)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 30.89GiB\r\n Name: 371bfe59-1b94-4f27-a7ab-c2e3a417c200-rnbdb\r\n ID: 0f3438f6-24c2-4ed2-b1df-96ff9cdb8cbc\r\n Docker Root Dir: \/var\/lib\/docker\/165536.165536\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Registry Mirrors:\r\n  http:\/\/10.201.201.163:5000\/\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\nI've ran an experiment and found if run the following on the daemon, the problem disappears, however my knowledge here is currently lacking and I don't yet understand why this is the case without diving deeper:\r\n\r\n```\r\n chown -R 165536:165536 \/var\/lib\/docker\/165536.165536\/containerd\/daemon\/io.containerd.snapshotter.v1.overlayfs\/snapshots\r\n```\r\n\r\nWe launch our daemon with the following options. There is an auth plugin we have wired up, however than can be disregarded here.\r\n```\r\n\"--authorization-plugin=<redacted>\r\n    --storage-driver=overlayfs \r\n    --registry-mirror http:\/\/<host>:<port> \r\n    --userns-remap=default \r\n    --log-level warn\"\r\n```\r\n\r\ndaemon.json:\r\n```\r\n{\r\n  \"features\": {\r\n    \"containerd-snapshotter\": true\r\n  }\r\n}\r\n```\r\n\r\nI was able to find this runc log file:\r\n\r\n```\r\n\/var\/lib\/docker\/165536.165536\/buildkit\/executor # cat runc-log.json\r\n{\"level\":\"error\",\"msg\":\"runc run failed: unable to start container process: error during container init: error mounting \\\"\/var\/lib\/docker\/165536.165536\/buildkit\/executor\/resolv.conf\\\" to rootfs at \\\"\/etc\/resolv.conf\\\": open \/var\/lib\/docker\/165536.165536\/buildkit\/executor\/jmzvvv7hgk3inuk6w35gp2pyu\/rootfs\/etc\/resolv.conf: permission denied\",\"time\":\"2024-02-13T05:00:49Z\"}\r\n{\"level\":\"error\",\"msg\":\"container does not exist\",\"time\":\"2024-02-13T05:00:49Z\"}\r\n```\r\n\r\n","comments":["I recall some recent issue with `rootless`, but don't think it's _directly_ related here;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/47327\r\n\r\n\r\nISTR I've seen a discussion elsewhere though, but maybe it was in the BuildKit repo \ud83e\udd14 \/cc @AkihiroSuda do you recall?","Thanks! I reproduced this on my side and I'm digging into it. I think I have an initial idea of what's wrong, but still need to confirm it (if that's true, it will probably involve some changes on the buildkit side though).","So, the lesser issue is that the containerd worker created by buildkit always sets a nil identity mapping (which means root): https:\/\/github.com\/moby\/buildkit\/blob\/47d6583cdf58b952c3cb9c719f2f9b45be825c1f\/worker\/containerd\/containerd.go#L156\r\n\r\nBut, even if it set the desired mapping, it still doesn't change the fact, that all unpacked image rootfs snapshots are stored as-is without any user remapping. The remapping is performed in a separate layer added on top of the image layer when creating a new snapshot for the container to run: https:\/\/github.com\/moby\/moby\/blob\/a60546b084eef87910345876e32c3db46e7c3bbc\/daemon\/containerd\/image_snapshot.go#L74-L76\r\n\r\nThis is fine for `docker run\/create`, but not for buildkit as it creates container directly via runc so it doesn't go the same code path.\r\n\r\nNot sure what's the best way to solve this, but there might be a couple of options. The easiest solution might be to wrap the buildkit executor to remap users before executing the `Run` method. It could work, because each executor has its own directory like: `\/var\/lib\/docker\/165536.165536\/buildkit\/executor\/kh28zau1oz1p6l53lht2r8u67\/`.\r\n\r\nNot sure if that's the best option though.. cc @tonistiigi @crazy-max","ping @tonistiigi @crazy-max ","If moby with containerd storage keeps the storage pre-mapped like before then correct identity mapping needs to be passed during invocation. If it keeps files as full root and remaps on mount (weaker security but more flexible for volume access) then such mode does not exist in BuildKit atm and would need to be implemented before."],"labels":["area\/builder","status\/0-triage","kind\/bug","area\/security\/userns","area\/builder\/buildkit","containerd-integration","version\/25.0"]},{"title":"Raw error output","body":"### Description\n\n$ docker inspect .\/\r\n[]\r\njson: cannot unmarshal array into Go value of type types.ContainerJSON\n\n### Reproduce\n\nRun docker inspect .\/\n\n### Expected behavior\n\n`docker inspect` with inappropriate args should output something like help info with error message, that such args are unexpected and why (such object does not exist)\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.0\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        e758fe5\r\n Built:             Thu Jan 18 17:09:59 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.0\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       615dfdf\r\n  Built:            Thu Jan 18 17:09:59 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 25.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-17-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 31.22GiB\r\n Name: Faraday\r\n ID: fac8ed41-56b4-46ec-936f-3c9cd401ec6e\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Thanks for reporting! Hm, yes, this should produce a more sensible error, either for \"wrong format\", or \"no such container\".\r\n\r\n\r\nInitially, I wondered if this was related to the `inspect` command being a \"global\" inspect, i.e., it can return any type of \"object\" (container, volume, network, etc), depending on what it finds, but it looks to be the same for `docker container inspect` here;\r\n\r\n```bash\r\ndocker inspect .\/\r\n[]\r\njson: cannot unmarshal array into Go value of type types.ContainerJSON\r\n\r\ndocker container inspect .\/\r\n[]\r\njson: cannot unmarshal array into Go value of type types.ContainerJSON\r\n```\r\n\r\nHaving a quick look at this case, it looks like the underlying issue here is related to a poor design decision in the past where the API does not use URL-escaping for arguments, which leads to the `.\/` being \"normalized\" (`\/v1.44\/containers\/.\/json` -> `\/v1.44\/containers\/json`), which causes it to hit the `\/containers\/json` endpoint (i.e., `container ls` \/ `container ps`); \r\n\r\n```\r\nDEBU[2024-02-13T10:36:52.474831844Z] Calling GET \/v1.44\/containers\/json\r\n```\r\n\r\nWhen using another value (in this case one that does not return any results), things work as expected, and (for the global `docker inspect`), all object types \/ endpoints are hit;\r\n\r\n\r\n```bash\r\ndocker inspect nosuchcontainer\r\n[]\r\nError: No such object: nosuchcontainer\r\n```\r\n\r\n```\r\nDEBU[2024-02-13T10:37:16.847493383Z] Calling GET \/v1.44\/containers\/nosuchcontainer\/json\r\nDEBU[2024-02-13T10:37:16.848518174Z] Calling GET \/v1.44\/images\/nosuchcontainer\/json\r\nDEBU[2024-02-13T10:37:16.849937299Z] Calling GET \/v1.44\/networks\/nosuchcontainer\r\nDEBU[2024-02-13T10:37:16.851340966Z] Calling GET \/v1.44\/volumes\/nosuchcontainer\r\nDEBU[2024-02-13T10:37:16.851413299Z] Probing all drivers for volume with name: nosuchcontainer\r\nDEBU[2024-02-13T10:37:16.851986258Z] Calling GET \/v1.44\/info\r\nDEBU[2024-02-13T10:37:16.859828508Z] Calling GET \/v1.44\/plugins\/nosuchcontainer\/json\r\n```\r\n\r\nThere's been prior discussion on that, and some (partial) fixes;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/29126\r\n- https:\/\/github.com\/moby\/moby\/pull\/32127\r\n- https:\/\/github.com\/moby\/moby\/issues\/16563\r\n\r\nHowever, there were concerns at the time around breaking backward-compatibility when switching to use URL-encoded values or other handling https:\/\/github.com\/moby\/moby\/pull\/16577#issuecomment-152507967, because (IIRC) some endpoints accept values that could be ambiguous. Perhaps, now that we use API-version dependent handling in more situations, we could revisit that decision though.\r\n\r\nHere's some changes related to that, and I see there's one PR still pending. Perhaps good to bring that up  again in our next maintainers call;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/16577\r\n- https:\/\/github.com\/moby\/moby\/pull\/29131\r\n- https:\/\/github.com\/moby\/moby\/pull\/30087\r\n- https:\/\/github.com\/moby\/moby\/pull\/37857\r\n- related: https:\/\/github.com\/moby\/moby\/pull\/10117"],"labels":["area\/cli","status\/0-triage","kind\/enhancement"]},{"title":"reduce named (error)-returns, naked returns and some minor linting-fixes","body":"Reduce the use to named (error) returns, and when used, use a distinctive name for the error-return, instead of `err`, which is error-prone, as it's easy to shadow, which could cause issues when handled in a defer.\r\n\r\nSee individual commits for details.\r\n\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Hm.. need to look at this failure;\r\n\r\n```\r\n=== FAIL: github.com\/docker\/docker\/distribution TestLayerAlreadyExists (0.00s)\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [find existing blob] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"apple\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [find existing blob with different hmac] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"apple\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [overwrite media types] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"apple\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [find existing blob among many] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"pear\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [stat single digest just once] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"pear\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n    push_v2_test.go:678: progress update: progress.Progress{ID:\"5f70bf18a086\", Message:\"\", Action:\"Layer already exists\", Current:0, Total:0, HideCounts:false, Units:\"\", Aux:interface {}(nil), LastUpdate:false}\r\n    push_v2_test.go:412: [with and without SourceRepository] got unexpected descriptor: distribution.Descriptor{MediaType:\"\", Size:0, Digest:\"\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)} != distribution.Descriptor{MediaType:\"application\/vnd.docker.image.rootfs.diff.tar.gzip\", Size:0, Digest:\"1\", URLs:[]string(nil), Annotations:map[string]string(nil), Platform:(*v1.Platform)(nil)}\r\n```","Had this in a branch for some time, and noticed I never pushed it, but looks like some commits may need updating; will have a look"],"labels":["status\/2-code-review","kind\/refactor"]},{"title":"cmd\/dockerd: add basic support for sd_notify \"reload\" notifications","body":"- relates to https:\/\/github.com\/moby\/moby\/pull\/22446\r\n- addresses https:\/\/github.com\/moby\/moby\/issues\/47353\r\n\r\n\r\ncommit f74b856e1ac2805fe48ceb52bc83cd7a3cec870c added an ExecReload to the systemd unit, which allows users to signal the daemon to reload its config through systemd (`systemctl reload docker.service`).\r\n\r\nWhile reloading works, systemd expects the `ExecReload` command to be synchronous, so that it knows when the reload completes, and can account for this when managing dependent services.\r\n\r\n> Note however that reloading a daemon by enqueuing a signal (...) is usually\r\n> not a good choice, because this is an asynchronous operation and hence not\r\n> suitable when ordering reloads of multiple services against each other.\r\n\r\nSystemd 253 introduced a new Type (Type=notify-reload, see [systemd#25916]), which allows setting a \"ReloadSignal\" instead, in addition to sending a [MONOTONIC_USEC].\r\n\r\nWe currently still support distros that do not provide systemd 253, so cannot use this new feature, but sending \"RELOADING=1 \/ \"READY=1\" should at least provide more information to systemd for the time being.\r\n\r\nThis patch:\r\n\r\n- adds reload notifications to the daemon to notify when the daemon got signaled to reload its configuration see [sd_notify(3)].\r\n- adds a [notifyReady] to notify when the reload finished, which we currently send regardless if the reload was successful or failed (which could be due to an invalid config).\r\n\r\nWe can re-implement this using `Type=notify-reload` once we no longer have to support systemd versions before 253.\r\n\r\n[sd_notify(3)]: https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#RELOADING=1\r\n[systemd.service(5)]: https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.service.html#Type=\r\n[ExecReload]: https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.service.html#ExecReload=\r\n[MONOTONIC_USEC]: https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#MONOTONIC_USEC=\u2026\r\n[systemd#25916]: https:\/\/github.com\/systemd\/systemd\/pull\/25916\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n```\r\n- Send sd_notify [\"READY\"](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#READY=1) and [\"STOPPING\"](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#STOPPING=1) synchronously to make sure they are sent before we proceed.\r\n- Add sd_notify [\"RELOADING\"](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#RELOADING=1) notifications when signalling the daemon to reload its configuration.\r\n```\r\n\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["> We currently still support distros that do not provide systemd 253, so cannot use [Type=notify-reload]\r\n\r\n[As I commented above,](https:\/\/github.com\/moby\/moby\/pull\/47358#discussion_r1488129879) the daemon could implement everything necessary to support `Type=notify-reload` without breaking compatibility with older versions of systemd as the sd_notify API is forwards-compatible. The only compatibility concern is with the unit file itself. But that's just a packaging concern. Package authors for systemd 253+ distros could ship a modified unit file, or users could create a drop-in for `Type=notify-reload`. We could even provide multiple systemd unit files compatible with different systemd versions for distro-package authors to pick from!\r\n\r\nI see no reason to wait until we drop support for systemd < 253 to make the daemon fully compatible with notify-reload, irrespective of whether the unit files shipped in Docker CE packages leverage such compatibility."],"labels":["status\/2-code-review","kind\/enhancement","area\/systemd","area\/daemon"]},{"title":"Add sd_notify \"reload\" notifications to assist with `systemctl reload`","body":"### Description\r\n\r\nWe have been using 2 lines back to back in our install scripts for past year without issues. Reloading the configuration with a new runtime and then immediately using the new runtime.\r\n\r\nIt looks like as of yesterday, there is a race condition where we can no longer count on the configuration to be loaded when the systemctl command returns.\r\n\r\n**Before, no issues running commands back to back:**\r\n<install runsc>...\r\nsystemctl reload docker.service\r\ndocker run --rm --runtime=runsc hello-world\r\n\r\n**In past few days, it is now consistently failing if we run the commands back to back:**\r\ndocker: Error response from daemon: unknown or invalid runtime name: runsc\r\n\r\nIf we add a sleep for a few seconds, then it can find the runsc runtime okay.\r\n\r\nLooking at the systemctl logs, it seems like the systemd thinks reload is done, but the docker runtime is reporting reload success afterwards:\r\nFeb 07 15:55:53 systemd[1]: Started Docker Application Container Engine.\r\nFeb 07 15:55:58 systemd[1]: Reloading Docker Application Container Engine.\r\nFeb 07 15:55:58 dockerd[7004]: time=\"2024-02-07T15:55:58.156917959Z\" level=info msg=\"Got signal to relo>\r\nFeb 07 15:55:58 systemd[1]: **Reloaded Docker Application Container Engine.**\r\nFeb 07 15:55:58 dockerd[7004]: time=\"2024-02-07T15:55:58.214678240Z\" level=info msg=\"**Reloaded configuration**>\r\n\r\nDid something change with how `systemctl reload docker.service` behaves recently to consistently have this behavior? Or possibly some other reason the race condition has gotten worse, if it existed already?\r\n\r\n\r\n### Reproduce\r\n\r\nEnsure docker version is 25.0.3:\r\n\r\n1. Run `systemctl reload docker.service`\r\n2. Run `systemctl status docker.service`\r\n3. See from logs that systemctl command completes before docker reports reload is complete\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\nVersion:           25.0.3\r\nAPI version:       1.44\r\nGo version:        go1.21.6\r\nGit commit:        4debf41\r\nBuilt:             Tue Feb  6 21:14:17 2024\r\nOS\/Arch:           linux\/amd64\r\nContext:           default\r\n \r\nServer: Docker Engine - Community\r\nEngine:\r\n  Version:          25.0.3\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       f417435\r\n  Built:            Tue Feb  6 21:14:17 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\ncontainerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\nrunc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\ndocker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\nVersion:    25.0.3\r\nContext:    default\r\nDebug Mode: false\r\nPlugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n \r\nServer:\r\nContainers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\nImages: 0\r\nServer Version: 25.0.3\r\nStorage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nCgroup Version: 1\r\nPlugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\nSwarm: inactive\r\nRuntimes: io.containerd.runc.v2 runc runsc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\nrunc version: v1.1.12-0-g51d5e94\r\ninit version: de40ad0\r\nSecurity Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\nKernel Version: 5.15.0-1054-azure\r\nOperating System: Ubuntu 20.04.6 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 31.34GiB\r\nName: resources-estimation-001000120\r\nID: 7e461b39-7cdf-4292-99ce-e1495d595b7d\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode: false\r\nExperimental: false\r\nInsecure Registries:\r\n  127.0.0.0\/8\r\nLive Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Hm, I don't think there's ever been a guarantee that the configuration would be reloaded immediately. The reload signals the daemon, which in its turn starts to reload the config. https:\/\/github.com\/moby\/moby\/blob\/9e075f3808a5a5fd54f0217ac114d3a02b16af2f\/contrib\/init\/systemd\/docker.service#L14\r\n\r\nThere's been some changes wrt config handling in recent releases, so possibly timing is slightly different. Do you know which version of docker \"works\" and which version introduced the change in behavior?","It looks like it started consistently failing during our VM re-images only in the past 48 hours, so most likely, it would be in a release that shipped very recently in past week or so. We've since re-imaged all of our VMs to hotfix the install scripts, so I unfortunately don't have the exact version of docker where this was working, but up until this week, it was very reliable.\r\n\r\nFor now, we put a 10 second sleep in between to be safe, but interesting that it started failing very consistently!","Yes possibly related to refactoring, however this has always been impossible due to it being implemented using signals, which are asynchronous.","I see, okay, we will add some waits\/sleeps! Thanks for confirming that it's not a regression (and that it never waited on completion).","> It looks like it started consistently failing during our VM re-images only in the past 48 hours, so most likely, it would be in a release that shipped very recently in past week or so\r\n\r\nIf any big changes related to this area, those would've been part of the v25.0.0 release, which was [three weeks ago on January 19](https:\/\/github.com\/moby\/moby\/releases\/tag\/v25.0.0). Patch releases for v25.0 after that should not (AFAIK) have changes in this area, so it's also possible that behavior in something separate from the Docker Engine itself (e.g. systemd updates).\r\n\r\n> however this has always been impossible due to it being implemented using signals, which are asynchronous.\r\n\r\nYes, that was roughly my thinking as well in https:\/\/github.com\/moby\/moby\/issues\/47353#issuecomment-1932711865\r\n\r\nI _just_ got thinking though; we implement `sd_notify` for the daemon, and I'm wondering if we handle _reload_ there as well. I'm not sure if we do, but _if_ we do, I wonder if something changed there, and if we're sending notifications _too early_ ?\r\n\r\nhttps:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#RELOADING=1\r\n\r\n```markdown\r\nRELOADING=1\r\n\r\n    Tells the service manager that the service is beginning to reload its configuration.\r\n    This is useful to allow the service manager to track the service's internal state,\r\n    and present it to the user. Note that a service that sends this notification must\r\n    also send a \"READY=1\" notification when it completed reloading its configuration.\r\n    Reloads the service manager is notified about with this mechanisms are propagated\r\n    in the same way as they are when originally initiated through the service manager.\r\n    This message is particularly relevant for Type=notify-reload services, to inform\r\n    the service manager that the request to reload the service has been received\r\n    and is now being processed.\r\n\r\n    Added in version 217.\r\n```\r\n\r\n> I see, okay, we will add some waits\/sleeps!\r\n\r\nIn case it's useful, you could do this through a systemd drop-in \/ override file; that way you don't have to update scripts.\r\n\r\nYou can do so using `systemctl edit`, which creates a template \/ annotated systemd unit in your editor. In this case we want to _keep_ the existing `ExecReload`, but add a second one that does a `sleep` (or whatever you want to execute); adding a `[Service]` section to the file, and a custom `ExecReload`, will _append_ a new `ExecReload`;\r\n\r\n\r\n```systemd\r\n### Editing \/etc\/systemd\/system\/docker.service.d\/override.conf\r\n### Anything between here and the comment below will become the new contents of the file\r\n\r\n[Service]\r\n\r\nExecReload=\/usr\/bin\/sleep 5s\r\n\r\n### Lines below this comment will be discarded\r\n```\r\n\r\nAfter editing and saving, you can verify if the options look correct with `systemctl show`;\r\n\r\n```bash\r\nsystemctl show docker.service -p ExecReload\r\nExecReload={ path=\/bin\/kill ; argv[]=\/bin\/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n\/a] ; stop_time=[n\/a] ; pid=0 ; code=(null) ; status=0\/0 }\r\nExecReload={ path=\/usr\/bin\/sleep ; argv[]=\/usr\/bin\/sleep 5s ; ignore_errors=no ; start_time=[n\/a] ; stop_time=[n\/a] ; pid=0 ; code=(null) ; status=0\/0 }\r\n```\r\n\r\nWith that change, `systemctl reload docker.service` itself will add the sleep (after it signaled), so will not return until that expires;\r\n\r\n```bash\r\ntime systemctl reload docker.service\r\n\r\nreal\t0m5.030s\r\nuser\t0m0.005s\r\nsys\t0m0.007s\r\n```\r\n","\u261d\ufe0f I should mention that I did not test the above (i.e. how it behaves, other than \"reload now waits for 5 seconds before it completes).\r\n\r\n\r\nw.r.t. `sd_notify` in my previous comment; I just did a quick check, and it looks like the only occurence of `RELOADING` is in the `sdnotify` library we use;\r\n\r\n```bash\r\ngit grep RELOADING\r\nvendor\/github.com\/coreos\/go-systemd\/v22\/daemon\/sdnotify.go:     SdNotifyReloading = \"RELOADING=1\"\r\n```\r\n\r\nAnd it looks like that const is not used in our code;\r\n\r\n```bash\r\ngit grep SdNotifyReloading\r\nvendor\/github.com\/coreos\/go-systemd\/v22\/daemon\/sdnotify.go:     \/\/ SdNotifyReloading tells the service manager that this service is\r\nvendor\/github.com\/coreos\/go-systemd\/v22\/daemon\/sdnotify.go:     SdNotifyReloading = \"RELOADING=1\"\r\n```\r\n\r\nThe only occurrence is in the library itself, but not used https:\/\/github.com\/moby\/moby\/blob\/23d80f729e9660ffc1df390aeeb3b86dcec70100\/vendor\/github.com\/coreos\/go-systemd\/v22\/daemon\/sdnotify.go#L34-L36\r\n\r\nI guess we could explore that as an option, and to implement `sd_notify RELOADING=1` for this.\r\n","Oh! Forgot to post; \r\n\r\nSo it looks like `Type=notify-reload` was added in systemd 253, and not supported by older versions. Also see https:\/\/github.com\/systemd\/systemd\/issues\/6162\r\n\r\n> It is strongly recommended to set `ExecReload=` to a command that not only triggers a configuration reload of the daemon, but also synchronously waits for it to complete.\r\n\r\nAs reloading through a signal is common, the new `Type=notify-reload` was added in https:\/\/github.com\/systemd\/systemd\/pull\/25916.\r\n\r\nAlso worth noting from that last one; https:\/\/github.com\/systemd\/systemd\/pull\/25916#issuecomment-1383775480\r\n\r\n>> (...) Why the new type notify-reload is necessary, instead of silently extending `Type=notify`?\r\n> We need to know if services implement reload or not, so that we can implement things such as \"systemctl reload-or-restart\" properly. `Type=notify` means no reload is implemented (or implemented via `ExecReload=`), while `Type=notify-reload` indicates that reload is implemented.\r\n\r\nThe `Type=notify-reload` option does not require a `ExecReload`, but instead can use the [`ReloadSignal`](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.service.html#Type=), which was also added in systemd 253, but (IIUC) must be accompanied with [MONOTONIC_USEC](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/sd_notify.html#MONOTONIC_USEC=\u2026), which is not currently supported by the go-systemd library.\r\n\r\n\r\nWe currently still need to support distros that do not provide systemd 253, so cannot use this new feature, but perhaps \"RELOADING=1 \/ \"READY=1\" might be the best we can do currently, and _perhaps_ would still give systemd a bit more information.\r\n","Opened a PR for discussion;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47358"],"labels":["kind\/enhancement","area\/systemd","area\/daemon","version\/25.0"]},{"title":"\"Use Rosetta\": Certain node.js\/amd64 workloads cause container to become unresponsive\/100% CPU","body":"### Description\r\n\r\nI installed Docker on an Ubuntu virtual machine:\r\n\r\n```bash\r\n$> uname -a\r\nLinux ubuntu-linux-22-04-02-desktop 5.15.0-92-generic #102-Ubuntu SMP Wed Jan 10 09:37:39 UTC 2024 aarch64 aarch64 aarch64 GNU\/Linux\r\n```\r\nI use Parallel desktop pro 19 (with Rosetta activated)\r\n\r\n\r\n\r\n\r\n### Reproduce\r\n\r\nJust run on a VM with Rosetta activated:\r\n```bash\r\ndocker run --platform linux\/amd64 jbinto\/rosetta-what\r\n# Should never respond\r\n```\r\n\r\nMore details explained [here](https:\/\/github.com\/jbinto\/rosetta-what)\r\n\r\nOnly one CPU is used, and this goes on indefinitely.  \r\n\r\nNOTE: I do not have this problem when I don't use Rosetta.\r\n\r\n### Expected behavior\r\n\r\nResponse on 10~15 seconds\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.2\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Thu Feb  1 00:23:31 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.2\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       fce6e0c\r\n  Built:            Thu Feb  1 00:23:31 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.28\r\n  GitCommit:        ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc:\r\n  Version:          1.1.12\r\n  GitCommit:        v1.1.12-0-g51d5e94\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 8\r\n Server Version: 25.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n runc version: v1.1.12-0-g51d5e94\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-92-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 8\r\n Total Memory: 19.03GiB\r\n Name: ubuntu-linux-22-04-02-desktop\r\n ID: 16015445-fbfd-4f60-b143-ee162fca1ffe\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nIt seems that I am encountering exactly the same issue as described in https:\/\/github.com\/docker\/for-mac\/issues\/6998. \r\n\r\n\ud83d\udc49 I opened a similar issue in Docker For Linux here https:\/\/github.com\/docker\/for-linux\/issues\/1483; I reopened it here because I'm not quite sure where the problem comes from, please excuse me.\r\n\ud83d\udc49 The support team for Parallel Desktop (the software that allows for virtualizing Ubuntu 22 on macOS) has also been contacted, but for now, there has been no response.","comments":["Your reproducer is pretty minimal (thank you!) and does only one thing: generate an RSA key pair using the `keypair` package. That package only has one import\u2014`require('crypto')`\u2014and it is only used for its cryptographically secure random number generator. My hunch is that under Rosetta emulation, `require('crypto').randomBytes()` returns low-entropy bytes (all zeroes, perhaps?) which is resulting in the keypair generator entering an infinite loop factoring the same number over and over vainly trying to find a prime. Could you test what `require('crypto').randomBytes(32).toString()` returns natively and under Rosetta? I don't have access to an Apple Silicon mac to test on so I cannot check for myself.\r\n\r\nAnd just to make sure this isn't simply a case of Rosetta under Parallels being really slow in emulating numerically-heavy code, does it complete if you try to generate a really short RSA key, like 128 bits?","let me also \/cc @dgageot and @rumpl who recently looked into some odd quirks and performance issues with Rosetta (in case this reproducer is useful).","> Only one CPU is used, and this goes on indefinitely.\r\n\r\nThis is likely because of libc's `sched_getaffinity` returning the wrong result when called from node subprocesses: https:\/\/github.com\/docker\/for-mac\/issues\/7184","@corhere Thanks for your comment, I have make these following update of  https:\/\/github.com\/jbinto\/rosetta-what based on your explanation:\r\n\r\n```js\r\n\/\/ Update index.js\r\nlet r = require('crypto').randomBytes(32).toString()\r\nconsole.log(r)\r\n```\r\n\r\nI have build the image `rosetta-what-crypto` on a native amd64 machine. And I run it in parallel with Rosetta activated on a MacBook M3:\r\n\r\n```bash\r\ndocker run --platform linux\/amd64 rosetta-what-crypto\r\n\ufffd\ufffd\ufffd}d\ufffd^\ufffd\u06c8M\ufffdD\ufffd\ufffd@\ufffd\ufffd\ufffdUX\ufffd\ufffdw\ufffdhW\ufffd\ufffd\r\n```\r\n\r\nThe response take less than 1 seconds ( very fast )"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug"]},{"title":"validate min\/max values for CPU-shares, similar to Windows","body":"### Description\n\n- relates to https:\/\/github.com\/moby\/moby\/pull\/47155#discussion_r1477957126\r\n\r\nOn Windows, we have code to validate if the given number of CPUs is correct for the host we're running on (or absolute limits?);  https:\/\/github.com\/moby\/moby\/blob\/27e85c7b6885c2d21ae90791136d9aba78b83d01\/daemon\/daemon_windows.go#L97-L99\r\n\r\n\r\nIt looks like we don't have a corresponding check on Linux. We should look if we can \/ should apply checks for this option to make sure it's within the acceptable range. https:\/\/git.kernel.org\/cgit\/linux\/kernel\/git\/tip\/tip.git\/tree\/kernel\/sched\/sched.h?id=8cd9234c64c584432f6992fe944ca9e46ca8ea76#n269\r\n\r\nTBD:\r\n\r\n- If we do, do we need to account for the kernel _potentially_ changing such limits? \r\n- Do we want this check to be implemented in containerd (and us consuming it) or implement locally?\r\n","comments":["Looks like we have similar checks here:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/27e85c7b6885c2d21ae90791136d9aba78b83d01\/daemon\/daemon_unix.go#L127","Ah, thanks! Yes, looks like there's some checks there; maybe that was the code I was looking at when adding that \"TODO\", noticing we were only checking for some boundaries, but not the ones we \"adjusted\" (so I thought; before I remove these, now unused, consts -  perhaps we actually need to use them again!). \ud83d\ude05 \r\n\r\nWe need to check where the best place is for this (also making sure we migrate\/adjust existing container configs on-disk if we decide to make this more strict)"],"labels":["area\/runtime","kind\/enhancement"]},{"title":"Dockerfile: test containerd v2.0.0-beta.2","body":"- related (vendoring); https:\/\/github.com\/moby\/moby\/pull\/47255\r\n\r\nTest CI using containerd v2.0.0-beta.2\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/runtime","status\/2-code-review","area\/testing","containerd-integration"]},{"title":"VPN-Container RDP issues after updating docker packages","body":"### Description\r\n\r\nWe use ubuntu containers as clients for various VPN-Clients for RDP-Sessions. Since an update of the apt package, the connection is very unstable. The VPN Connection is established, and remote servers and the port for RDP are reachable but it will loose connection sporadically. The only thing that has changed is the version of the packages mentioned below.\r\n\r\n### Reproduce\r\n\r\nOn Debian 12, install the following packages:\r\n\r\n```\r\ncontainerd.io\/bookworm 1.6.27-1 amd64 [upgrade from 1.6.26-1].\r\ndocker-buildx-plugin\/bookworm 0.12.1-1~debian.12~bookworm amd64 [upgraded from 0.11.2-1~debian.12~bookworm].\r\ndocker-ce-cli\/bookworm 5.25.0.1-1~debian.12~bookworm amd64 [upgrade from 5.24.0.7-1~debian.12~bookworm].\r\ndocker-ce-rootless-extras\/bookworm 5.25.0.1-1~debian.12~bookworm amd64 [upgrade from 5.24.0.7-1~debian.12~bookworm].\r\ndocker-ce\/bookworm 5.25.0.1-1~debian.12~bookworm amd64 [upgrade from 5.24.0.7-1~debian.12~bookworm].\r\ndocker-compose-plugin\/bookworm 2.24.2-1~debian.12~bookworm amd64 [upgrade from 2.21.0-1~debian.12~bookworm].\r\n```\r\nThe _upgrade from_ versions are the last working ones.\r\n\r\nSet up a ubuntu:latest container and a vpn connection e.g. OpenVPN:\r\n\r\n```dockerfile\r\nFROM ubuntu:latest\r\n\r\n# Set the timezone\r\nENV TZ=\"Europe\/Berlin\r\n\r\n# Install Linux packages\r\n\r\nRUN apt-get update -y \\\r\n    && DEBIAN_FRONTEND=non-interactive apt-get upgrade -y \\\r\n    && DEBIAN_FRONTEND=non-interactive apt-get install -y --no-install-recommends \\\r\n    ca-certificates \\\r\n    tzdata \\\r\n    expect \\\r\n    net-tools \\\r\n    openssh-client \\\r\n    iproute2 \\\r\n    iptables \\\r\n    wget \\\r\n    iputils-ping \\ \r\n    socat \\\r\n    netcat \\\r\n    openvpn \\\r\n    tini \\\r\n    && apt-get clean \\\r\n    && apt-get autoremove --purge \\\r\n    \\ && rm -rf \/var\/lib\/apt\/lists\/*\r\n```\r\n\r\nExpose private port 3389 and have container forward all traffic from mapped public port to private IP (remote Windows server). This will cause massive dropped connections when opening an RDP session from a client.\r\n\r\n### Expected behavior\r\n\r\nStable RDP-Sessions\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:08:02 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:02 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.26\r\n  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 82\r\n  Running: 25\r\n  Paused: 0\r\n  Stopped: 57\r\n Images: 27\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-13-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 15.58GiB\r\n Name: <thehostname>\r\n ID: 2870ce1d-4882-4ff7-b2e7-f4df32fabddf\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Hi @hakimdotdev, I was going to answer your report but you just closed it. Could you confirm this was an issue on your side (eg. faulty network, etc...)?","Hi @akerouanton, I'm pretty sure it wasn't, i just thought its the wrong repo to post this. I was about to post it into the docker-ce or containerd packaging repo","What version of docker were you running before updating?","Oh! The `docker info` output is from _before_ updating, not the version you ran into issues with, is that correct?\r\n\r\n```\r\ndocker-ce-cli\/bookworm 5.25.0.1-1~debian.12~bookworm amd64 [upgrade from 5.24.0.7-1~debian.12~bookworm].\r\n```","@thaJeztah Yes you're right i missed out on that one, i can't reproduce right now, as the replica system now got a even newer version via auto updates. I'll have to reinstall the mentioned versions and get a `version` and `info` output","Thanks! Sorry for adding some noise there; I saw the `24.0.7` in the `docker info` \/ `docker version`, and got curious because I didn't recall any significant changes in patch releases for 24.0.x in networking, but then I spotted your comment that it was when upgrading _to_ 25.0.x (from 24.0.x) \ud83d\ude05 .","Could you confirm you updated straight from 24.0.7 to 25.0.1, without going through 25.0.0? Your report makes me think it could be a duplicate of https:\/\/github.com\/moby\/moby\/issues\/47211. It's worth checking you don't have any duplicate MAC address in your docker networks.\r\n\r\nIf it's inconclusive, could you try running `ping` continuously from both your containers and your host (targetting the remote system you usually connect to) until you see the sporadic issue re-happen again? If you observe packet losses only from the container, it'd rule out any external factors (eg. flaky link, etc...).\r\n\r\nCould you enable debug logs (see here: https:\/\/docs.docker.com\/config\/daemon\/logs\/) and check if the issue is correlated to any particular API calls made to the Engine?\r\n\r\nFinally, could you check with tcpdump what really happens (eg. packets dropped, ICMP error code returned, TCP RST, etc...)?","Hi @akerouanton, \r\n\r\n> Could you confirm you updated straight from 24.0.7 to 25.0.1, without going through 25.0.0? \r\n\r\nThe system had auto updates enabled but bookworm seems to have skipped a few versions and I'm not sure why, so yes straight from 24.0.7 to 25.0.1\r\n\r\n>Your report makes me think it could be a duplicate of https:\/\/github.com\/moby\/moby\/issues\/47211. It's worth checking you don't have any duplicate MAC address in your docker networks.\r\n\r\nI'm pretty sure but i can't confirm it right now, on the System with the old versions, there are no duplicates but I'll have to spin up the staging environment to test it.\r\n\r\n> If it's inconclusive, could you try running ping continuously from both your containers and your host (targetting the remote system you usually connect to) until you see the sporadic issue re-happen again? If you observe packet losses only from the container, it'd rule out any external factors (eg. flaky link, etc...).\r\n\r\nI've tried this when the issue occured. There is no packet losses, IPs and Ports are reachable from container to remote server while RDP says re-establishing.\r\n\r\n> Could you enable debug logs (see here: https:\/\/docs.docker.com\/config\/daemon\/logs\/) and check if the issue is correlated to any particular API calls made to the Engine?\r\n\r\n> Finally, could you check with tcpdump what really happens (eg. packets dropped, ICMP error code returned, TCP RST, etc...)?\r\n\r\nI'll spin up the staging env and try that.","The System has a nginx on it but no affected containers are running behind it ([grafana#81258](https:\/\/github.com\/grafana\/grafana\/issues\/81258)) and all VPN-Containers are on a created bridged Network #47211 but I don't recall any `no route to host`."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","area\/networking","version\/25.0"]},{"title":"Dockerd panic: runtime error: index out of range","body":"### Description\r\n\r\n```\r\nFeb  4 17:34:33 mgmt-1 esfdaemon[2884]: 0\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: panic: runtime error: index out of range [-1]\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: goroutine 1692014149 [running]:\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.encodeVarintLogbroker(...)\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1712\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.(*SubscriptionMessage).MarshalToSizedBuffer(0xc8c0149530, {0xc713d29100, 0x3e, 0x3e})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1620 +0x32a\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.(*SubscriptionMessage).Marshal(0xc36229f800?)\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1566 +0x56\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/internal\/impl.legacyMarshal({{}, {0x55754929e268, 0xc4389d07f0}, {0x0, 0x0, 0x0}, 0x0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/internal\/impl\/legacy_message.go:402 +0xa2\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/proto.MarshalOptions.marshal({{}, 0x80?, 0x0, 0x0}, {0x0, 0x0, 0x0}, {0x55754929e268, 0xc4389d07f0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/proto\/encode.go:166 +0x27b\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/proto.MarshalOptions.MarshalAppend({{}, 0x60?, 0x38?, 0x1a?}, {0x0, 0x0, 0x0}, {0x55754926a140?, 0xc4389d07f0?})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/protobuf\/proto\/encode.go:125 +0x79\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/golang\/protobuf\/proto.marshalAppend({0x0, 0x0, 0x0}, {0x7f21e46bc158?, 0xc8c0149530?}, 0x2?)\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/golang\/protobuf\/proto\/wire.go:40 +0xa5\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/golang\/protobuf\/proto.Marshal(...)\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/golang\/protobuf\/proto\/wire.go:23\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\/encoding\/proto.codec.Marshal({}, {0x5575491a3860, 0xc8c0149530})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\/encoding\/proto\/proto.go:45 +0x4e\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/grpc.encode({0x7f21f4258900?, 0x55754a738d10?}, {0x5575491a3860?, 0xc8c0149530?})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\/rpc_util.go:594 +0x44\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/grpc.prepareMsg({0x5575491a3860?, 0xc8c0149530?}, {0x7f21f4258900?, 0x55754a738d10?}, {0x0, 0x0}, {0x0, 0x0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\/stream.go:1692 +0xd2\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/google.golang.org\/grpc.(*serverStream).SendMsg(0xc232615260, {0x5575491a3860?, 0xc8c0149530})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\/stream.go:1569 +0xf3\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/grpc-ecosystem\/go-grpc-prometheus.(*monitoredServerStream).SendMsg(0xc21413e1e0, {0x5575491a3860?, 0xc8c0149530?})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/grpc-ecosystem\/go-grpc-prometheus\/server_metrics.go:156 +0x33\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.(*logBrokerListenSubscriptionsServer).Send(0xc4f1d51ad0?, 0xc4f1d51ad0?)\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1149 +0x2b\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/manager\/logbroker.(*LogBroker).ListenSubscriptions(0xc0013c82a0, 0xc3045081e0?, {0x557549292df0, 0xc2139b41e0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/manager\/logbroker\/broker.go:367 +0x90b\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.(*authenticatedWrapperLogBrokerServer).ListenSubscriptions(0xc0011adb78, 0x557549288660?, {0x557549292df0, 0xc2139b41e0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:658 +0xca\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api.(*raftProxyLogBrokerServer).ListenSubscriptions(0xc00156ae10, 0x55754919cbc0?, {0x557549292710, 0xc29e9fc540})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1933 +0x19b\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api._LogBroker_ListenSubscriptions_Handler({0x557548f7ba60?, 0xc00156ae10}, {0x55754928ea50, 0xc21413e1e0})\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: #011\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/api\/logbroker.pb.go:1136 +0xbc\r\nFeb  4 17:34:34 mgmt-1 dockerd[3765]: github.com\/docker\/docker\/vendor\/github.com\/grpc-ecosystem\/go-grpc-prometheus.(*ServerMetrics).StreamServerInterceptor.func1({0x557548f7ba60, 0xc00156ae10}, {0x55754928f428?, 0xc232615260}, 0x557546d3b019?, 0x557549254678)\r\n```\r\n\r\n### Reproduce\r\n\r\nswarm cluster manager dockerd crashed,  don't find a reproduce step.\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:36:32 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:32 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n[root@mgmt-1 harbor]# docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 36\r\n  Running: 13\r\n  Paused: 0\r\n  Stopped: 23\r\n Images: 40\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan jhinno\/network-plugin:v5.3 macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: pholttqef3uduodr2ykn4os35\r\n  Is Manager: true\r\n  ClusterID: ww1yjms50uodh200irzqfmnt0\r\n  Managers: 5\r\n  Nodes: 83\r\n  Default Address Pool: 12.0.0.0\/16  \r\n  SubnetSize: 16\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: \r\n...\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-305.el8.x86_64\r\n Operating System: Red Hat Enterprise Linux 8.4 (Ootpa)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 64\r\n Total Memory: 503.5GiB\r\n Name: mgmt-1\r\n ID: 84fc58cd-a6bb-4c15-a028-9ca9f47b3d75\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Registry Mirrors:\r\n  ...\r\n Live Restore Enabled: false\r\n\r\nWARNING: API is accessible on http:\/\/0.0.0.0:2375 without encryption.\r\n         Access to the remote API is equivalent to root access on the host. Refer\r\n         to the 'Docker daemon attack surface' section in the documentation for\r\n         more information: https:\/\/docs.docker.com\/go\/attack-surface\/\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["@corhere @dperny looks like some panic in Swarm code (related to logging?)"],"labels":["area\/logging","status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"Memory Leak issue on Docker Engine 24.0.6","body":"### Description\r\n\r\nDocker related issue: Here we are running our Linux based application inside docker container & when we start the process it holds 1.4GB of memory we haven\u2019t explicitly allocated the memory neither it\u2019s memory leakage from app side .\r\nOn normal VMs this process doesn\u2019t hold any memory if we compared with Docket instance .\r\nThis causes Out Of Memory for docker container\r\n\r\nThe state of Process is idle still its using the High RAM. Later when we start the processes it runs in Out of memory.\r\n\r\nExpected behavior - the reported process sp_cop should not hold 1.4G of memory.\r\n\r\nWe are using below docker version\r\n\r\n```bash\r\n[root@pslrhel7docker01 .ssh]# docker info\r\nClient: Docker Engine - Community\r\nVersion: 24.0.6\r\nContext: default\r\nDebug Mode: false\r\nPlugins:\r\nbuildx: Docker Buildx (Docker Inc.)\r\nVersion: v0.12.1\r\nPath: \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\ncompose: Docker Compose (Docker Inc.)\r\nVersion: v2.24.2\r\nPath: \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\nContainers: 1\r\nRunning: 1\r\nPaused: 0\r\nStopped: 0\r\nImages: 1\r\nServer Version: 24.0.6\r\nStorage Driver: overlay2\r\nBacking Filesystem: xfs\r\nSupports d_type: true\r\nUsing metacopy: false\r\nNative Overlay Diff: true\r\nuserxattr: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nCgroup Version: 1\r\nPlugins:\r\nVolume: local\r\nNetwork: bridge host ipvlan macvlan null overlay\r\nLog: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc io.containerd.runc.v2\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\nrunc version: v1.1.11-0-g4bccb38\r\ninit version: de40ad0\r\nSecurity Options:\r\nseccomp\r\nProfile: builtin\r\nKernel Version: 3.10.0-1160.el7.x86_64\r\nOperating System: Red Hat Enterprise Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.51GiB\r\nName: pslrhel7docker01\r\nID: 7c1b0533-f16f-4ac1-b981-39ed9b9c00e8\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode: false\r\nExperimental: false\r\n```\r\n\r\n### Reproduce\r\n\r\nNo Steps to reproduce.\r\n\r\n### Expected behavior\r\n\r\nThe process not suppose to use the memory more than 0.2% of total memory when nothing is running on the docker container.\r\nBelow screenshot is for Docker container process --> where it is reserving 1.4G of space to process which is idle and not executing anything.\r\n![image (1)](https:\/\/github.com\/moby\/moby\/assets\/62471221\/facc1056-cd8a-4724-a0a6-7c4fc8f57deb)\r\n\r\nThe below image is for reference which is started on Linux Local VM and it is in Idle state reserved very less memory 0.01% of total memory.\r\n<img width=\"708\" alt=\"image\" src=\"https:\/\/github.com\/moby\/moby\/assets\/62471221\/92b2e532-85f8-4568-9a33-e0e97c61fd06\">\r\n\r\n\r\n### docker version\r\n\r\n```bash\r\n[root@pslrhel7docker01 ~]# docker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:35:25 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:34:28 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n[root@pslrhel7docker01 .ssh]# docker info\r\nClient: Docker Engine - Community\r\nVersion: 24.0.6\r\nContext: default\r\nDebug Mode: false\r\nPlugins:\r\nbuildx: Docker Buildx (Docker Inc.)\r\nVersion: v0.12.1\r\nPath: \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\ncompose: Docker Compose (Docker Inc.)\r\nVersion: v2.24.2\r\nPath: \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\nContainers: 1\r\nRunning: 1\r\nPaused: 0\r\nStopped: 0\r\nImages: 1\r\nServer Version: 24.0.6\r\nStorage Driver: overlay2\r\nBacking Filesystem: xfs\r\nSupports d_type: true\r\nUsing metacopy: false\r\nNative Overlay Diff: true\r\nuserxattr: false\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nCgroup Version: 1\r\nPlugins:\r\nVolume: local\r\nNetwork: bridge host ipvlan macvlan null overlay\r\nLog: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc io.containerd.runc.v2\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\nrunc version: v1.1.11-0-g4bccb38\r\ninit version: de40ad0\r\nSecurity Options:\r\nseccomp\r\nProfile: builtin\r\nKernel Version: 3.10.0-1160.el7.x86_64\r\nOperating System: Red Hat Enterprise Linux\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 8\r\nTotal Memory: 15.51GiB\r\nName: pslrhel7docker01\r\nID: 7c1b0533-f16f-4ac1-b981-39ed9b9c00e8\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode: false\r\nExperimental: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["At a first glance, I don't think there's a bug at hand here, but running software containerised may in some cases affect how software is running.\r\n\r\n> The below image is for reference which is started on Linux Local VM and it is in Idle state reserved very less memory 0.01% of total memory.\r\n\r\nContainers, by default, may have different limits set as the host environment, for example, `ulimits`  may have different values, and some software is known to adjust their behavior based on available resources and limits;\r\n\r\nOn the host;\r\n\r\n```bash\r\nulimit -a\r\ncore file size          (blocks, -c) 0\r\ndata seg size           (kbytes, -d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size               (blocks, -f) unlimited\r\npending signals                 (-i) 3768\r\nmax locked memory       (kbytes, -l) 64\r\nmax memory size         (kbytes, -m) unlimited\r\nopen files                      (-n) 1024\r\npipe size            (512 bytes, -p) 8\r\nPOSIX message queues     (bytes, -q) 819200\r\nreal-time priority              (-r) 0\r\nstack size              (kbytes, -s) 8192\r\ncpu time               (seconds, -t) unlimited\r\nmax user processes              (-u) 3768\r\nvirtual memory          (kbytes, -v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\nIn a container;\r\n\r\n```bash\r\ndocker run --rm alpine sh -c 'ulimit -a'\r\ncore file size (blocks)         (-c) unlimited\r\ndata seg size (kb)              (-d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size (blocks)              (-f) unlimited\r\npending signals                 (-i) 3768\r\nmax locked memory (kb)          (-l) 64\r\nmax memory size (kb)            (-m) unlimited\r\nopen files                      (-n) 1048576\r\nPOSIX message queues (bytes)    (-q) 819200\r\nreal-time priority              (-r) 0\r\nstack size (kb)                 (-s) 8192\r\ncpu time (seconds)              (-t) unlimited\r\nmax user processes              (-u) unlimited\r\nvirtual memory (kb)             (-v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\n> No Steps to reproduce.\r\n\r\nWithout reproduction steps, I'm not sure if this ticket is really actionable. The above information _may_ be able for you to help narrow down the differences you see, but without a (ideally: minimal) reproducer to look into, I'm not sure if there's much we can do to look into.\r\n\r\n\r\n","Thanks for the update.\r\n\r\nI will shortly share the steps to repro the issue.","I am afraid, as per the policy we cannot share the repro steps.\r\nHowever during the investigation, the output of pmap command identified the anon process which is allocating\/reserving 1.4G for each application process."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/24.0"]},{"title":"Allow multiple macvlan networks to share a parent","body":"Closes #47317\r\n\r\nThe only case where macvlan interfaces are unable to share a parent is when the macvlan mode is passthru. This change tightens the check to that situation.\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n1. If two macvlan networks have the same parent and id -> already exists\r\n2. If two macvlan networks have the same parent, different ids, and at least one is passthru -> error\r\n3. If two macvlan networks have the same parent, different ids -> create network\r\n4. If two macvlan networks have different parents -> create network\r\n\r\n**- How I did it**\r\n\r\nA new if and continue to skip marking as \"found existing\".\r\n\r\n**- How to verify it**\r\n\r\nCreate two macvlan networks with the same parent.\r\n\r\n**- Description for the changelog**\r\n\r\nAllow multiple macvlan networks with the same parent.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/597549\/98442533-9507-43ab-a3dd-3903bffabd4b)\r\n\r\n[Man sets fire to smoke out opossums, burns down house instead](http:\/\/www.cbsnews.com\/news\/man-tries-to-scare-off-opossums-by-setting-fire-burns-down-own-house-instead\/)","comments":["Just a note, I haven't managed to get tests running locally.  I built this with nix, which I think _might_ run tests as part of the build.  I tested it manually with with two macvlan interfaces with the same parent (`eth0`) and at least the interfaces were created with no errors.\r\n\r\nEdit: I also added more specificity to the error message. I think it would have been possible to accidentally have an interface set to passthru, see the error message, and become confused thinking that sharing parents is never correct.  I'm not 100% sure that the ids used for the message are appropriate though, I think I would have used the network name but I'll wait for clarity on that.","Thank you for contributing! It appears your commit message is missing a DCO sign-off,\r\ncausing the DCO check to fail.\r\n\r\nWe require all commit messages to have a `Signed-off-by` line with your name\r\nand e-mail (see [\"Sign your work\"](https:\/\/github.com\/moby\/moby\/blob\/v20.10.11\/CONTRIBUTING.md#sign-your-work)\r\nin the CONTRIBUTING.md in this repository), which looks something like:\r\n\r\n```\r\nSigned-off-by: YourFirsName YourLastName <yourname@example.org>\r\n```\r\n\r\nThere is no need to open a new pull request, but to fix this (and make CI pass),\r\nyou need to _amend_ the commit(s) in this pull request, and \"force push\" the amended\r\ncommit.\r\n\r\nUnfortunately, it's not possible to do so through GitHub's web UI, so this needs\r\nto be done through the git commandline.\r\n\r\nYou can find some instructions in the output of the DCO check (which can be found\r\nin the \"checks\" tab on this pull request), as well as in the [Moby contributing guide](https:\/\/github.com\/moby\/moby\/blob\/v20.10.11\/docs\/contributing\/set-up-git.md).\r\n\r\nSteps to do so \"roughly\" come down to:\r\n\r\n1. Set your name and e-mail in git's configuration:\r\n\r\n    ```bash\r\n    git config --global user.name \"YourFirstName YourLastName\"\r\n    git config --global user.email \"yourname@example.org\"\r\n    ```\r\n\r\n    (Make sure to use your _real_ name (**not your GitHub username\/handle**) and e-mail)\r\n\r\n\r\n2. Clone your fork locally\r\n3. Check out the branch associated with this pull request\r\n4. Sign-off and amend the existing commit(s)\r\n\r\n    ```bash\r\n    git commit --amend --no-edit --signoff\r\n    ```\r\n\r\n    If your pull request contains multiple commits, either squash the commits (if\r\n    needed) or sign-off each individual commit.\r\n\r\n5. _Force push_ your branch to GitHub (using the `--force` or [`--force-with-lease`](https:\/\/stackoverflow.com\/questions\/52823692\/git-push-force-with-lease-vs-force) flags) to update the pull request.\r\n\r\n\r\nSorry for the hassle (I wish GitHub would make this a bit easier to do), and let me know if you need help or more detailed instructions!","Oh! If you're updating; could you remove the `Closes #47317` from the commit message? Unfortunately, GitHub has a tendency to either close related tickets prematurely (or producing \"noice \/ notifications\") when commits are merged in various places (which could be in forks of this repository). For that reason, we try to avoid referencing tickets in _commit_ messages (and only put them in the description on GitHub, not the commit message). ","Oh sure, and sorry about the DCO thing! I should re-read contributing more carefully...","No worries! iIt happens frequently (hence my \"canned reply\" above, which I wrote so that I didn't have to come up with a good way to describe the steps every time \ud83d\ude02 \u2764\ufe0f)","Oh! Looks like there's no e-mail address in your sign-off; \r\n\r\n```\r\nSigned-off-by: Andrew Baxter <>\r\n```","\/cc @akerouanton @robmry ","Thanks for the review! And good point on the logs, I applied the suggestion.\r\n\r\n> Thank you for looking at this, it looks like the right approach - but there are a couple of issues ...\r\n> \r\n> Integration test, `TestDockerNetworkMacvlan\/OverlapParent` in [macvlan_test.go](https:\/\/github.com\/moby\/moby\/blob\/master\/integration\/network\/macvlan\/macvlan_test.go) was supposed to be checking it's not possible to create two macvlan networks with the same parent.\r\n> \r\n> Unfortunately, the test hasn't been running for a while. But, when that's fixed (#47382), it'll fail with this change.\r\n> \r\n\r\nRoger, I'll try to update that.\r\n\r\n> Also, with a sequence like this ...\r\n> \r\n> ```\r\n> # docker network create -d macvlan mv1\r\n> # docker network create -d macvlan -o parent=dm-8a604e8c9839 mv2\r\n> # docker network rm mv1\r\n> # docker run --rm -ti --network mv2 alpine\r\n> docker: Error response from daemon: the requested parent interface dm-8a604e8c9839 was not found on the Docker host.\r\n> ```\r\n> \r\n> ... the first 'create' doesn't specify a parent interface, so a dummy link is created (\"dm-8a604e8c9839\"). That's remembered in `config.CreatedSlaveLink`, and the link is deleted by `driver.DeleteNetwork()` when the network's removed.\r\n> \r\n> That was ok when the dummy link could only be the parent for one macvlan network but now, deleting `mv1` leaves `mv2` without a parent (and so, would break connectivity for containers on `mv2`).\r\n\r\nI need to dig into this further, but before I drop off -- off the top of my head, three ways to handle this would be:\r\n1. Keep track of children and only delete the interface when no more children exist\r\n2. Don't delete the dummy(\/vlan) parent interface, but instead clean it up in system prune\r\n3. Re-restrict this so that sharing parents is only allowed for existing interfaces\r\n\r\nFor 1 I guess iterating all the networks and counting parent users before deleting would probably be the simplest\/safest. Reference counting, associating a set of children, deleting the parent in a finalizer are other options but have probably worse drawbacks.\r\n\r\nFor 2, if someone wanted to share dummy parent interfaces they might want it to keep existing even if they delete all the users. If they're really going to do that, would it be easier to do it outside of moby (i.e. create the interface with their own tooling, the same that would look up the dummy interface id)? If so maybe 3 would be better, otherwise for 2 maybe the dummy interface could be turned into a normal network that could be managed with network tools.\r\n\r\nSorry, I'm probably missing some context here so my guesses may be wildly off.\r\n\r\nDid you have any thoughts on how this should be handled?\r\n\r\n> \r\n> Are you up for dealing with that and updating the integration test? I think we should also have a new tests for the pass-through special case, and the create\/delete sequence above - if that's permitted.\r\n\r\nRight, makes sense.  I'll look into doing those too.\r\n\r\nThanks again for the review!","> 1. Keep track of children and only delete the interface when no more children exist\r\n> 2. Don't delete the dummy(\/vlan) parent interface, but instead clean it up in system prune\r\n> 3. Re-restrict this so that sharing parents is only allowed for existing interfaces\r\n\r\nLeaving it to system-prune might be tricky, we'd need to be certain that the interface is a docker-created dummy (just deleting interfaces named \"dm-nnnn\" when there's no \"nnnn\" network in-use might zap something it shouldn't). And, it'd rely on people running prune to declutter, when we could have dealt with when the last network went-away.\r\n\r\nDisallowing sharing of a moby-created dummy interface (a variant on '3') would solve the problem, and this PR would still be a step forward in terms of being able to share real interfaces. It'd be easy to do by just raising an error if `nw.config.CreatedSlaveLink` is set on the existing interface, as another `else if` when checking for `passthru` networks. But, there might be a reason to want to share one of those interfaces for testing... although I'm not convinced and, as you say, it'd be possible to create the dummy parent outside of moby as a workaround.\r\n\r\nSo, some form of option 1 might be best ...\r\n\r\nI agree that explicitly linking the networks or keeping a reference count could get messy.\r\n\r\nMaybe set `config.CreatedSlaveLink` on the new network, if an existing network with the same parent already has the flag set. Then, everything sharing that moby-created dummy parent ends up with the flag set.\r\n\r\nThen - in the delete, if the flag is set, search for another network with the same parent - only removing the dummy interface if none are found. That way, the search is only necessary if the flag is set, the order of deletions doesn't matter, and it's not necessary to work out whether the interface is a dummy created for some other already-deleted network.\r\n\r\n> Right, makes sense. I'll look into doing those too.\r\n\r\nBrilliant, thank you!","> So, some form of option 1 might be best ...\r\n\r\nI was thinking about this a bunch and I think for vlan parents we'd need 1 either way, so probably I was focusing on the dummy interfaces too much.\r\n\r\nI'll probably need some time for your updates to sink in to my brain too but that all makes sense.\r\n\r\nI'll start by trying 1 iterating the networks and counting uses at delete time and see where that gets me.","> I was thinking about this a bunch and I think for vlan parents we'd need 1 either way, so probably I was focusing on the dummy interfaces too much.\r\n\r\nOh, yes - my description's also focussed on the dummy interfaces ... but it applies equally to the moby-created VLAN interfaces.\r\n\r\nThe suggestion is just to make sure `CreatedSlaveLink` is set on all the networks sharing a moby-created parent, not just the one that actually created it. Then it doesn't matter which order networks are deleted in, the parent interface for a network with `CreatedSlaveLink` set can be removed when no other networks have the same parent.","DOH! Looks like the \"accept suggestion\" added a separate commit, which.. of course doesn't get a DCO sign-off.\r\n\r\nIt's probably fine to squash the commits into a single one (you can include the co-authored-by, but we're not always very fussy about that if it's suggestions from maintainers - it's usually fine to skip \ud83d\ude05)","> The suggestion is just to make sure `CreatedSlaveLink`\r\n\r\nRight, that makes sense.\r\n\r\n> It's probably fine to squash the commits into a single one\r\n\r\nAh sorry, I was intending to do that when I got back to this.  I'll do that now.","Okay, I think I've taken care of everything: counting other users, integration tests.\r\n\r\nNotes on the use counting - I'm not sure if the `ok := ...; ok` is a convention, but I'd have to add another if\/level of nesting to check the use count so I collapsed the 3 clauses into a single if. The counting could be delayed until it was confirmed to be a dummy\/vlan parent, but the vlan code has the \"is this a vlan device\" checks within the delete\/create functions and I'm not sure how hot this path is in real life so I went with the less disruptive change.\r\n\r\nNotes on the tests - I think these were the key things to check:\r\n- Sharing a dummy\/vlan\/physical\/etc interface\r\n- Deleting the original interface first\r\n- Deleting the 2nd interface first\r\n- Existing interfaces don't get deleted\r\n- Sharing a parent with a passthru\r\n- Creating a passthru with a shared parent\r\n\r\nI didn't see explicit tests for ex: implicitly creating a dummy interface, and the existing overlap test would test one of the delete cases.  I'm not sure if it's better to have one test per thing to check or if because integration tests are slower it's better to combine them when reasonably possible, but I believe all of the new code paths are reasonably covered with the new test cases.","@akerouanton PTAL"],"labels":["status\/2-code-review","kind\/feature","area\/networking","area\/networking\/d\/macvlan"]},{"title":"[macvlan] Same parent and gateway for multiple networks","body":"### Description\n\nI'm remaking this from https:\/\/github.com\/moby\/libnetwork\/issues\/2384\r\n\r\nIt's still relevant to me (esp with ipv6 networking) and I'd be happy to try working on it, but wanted to check first if the solution proposed there had any issues.","comments":["Some quick questions:\r\n\r\n- Is the check there for technical reasons or guidance (preventing people from making common mistakes)?\r\n- I couldn't find any documentation on macvlan, but on source said there can only be one macvlan with passthrough.  I didn't see any other restrictions.  Rather than check `CreatedSlaveLink` maybe it would be better to check if either interface sharing the parent are `passthru`?","I said that I wanted to check first but I figured it'd be better to just go for it than try to come up with some local workaround.  PR #47318 ","Thanks for reporting, and for working on an implementation. I left some comments on your PR, but I'm not an expert in this area, so will ask others to look at your changes (there may be subtle details, as there tend to be when networking is involved, so it's always good to have more eyes!)"],"labels":["status\/0-triage","kind\/feature","area\/networking","area\/networking\/d\/macvlan"]},{"title":"chore: add godoc to MTU tests","body":"- relates to https:\/\/github.com\/moby\/moby\/pull\/47309\r\n\r\nFor a follow-up we could add a GoDoc to these tests to mention where these specific ranges came from (what's \"magical\" about Bigger than 1500, or bigger than 64k?\r\n\r\nAnd perhaps combining both to a test-table \/ sub-tests would allow adding a \"docs\" to the tests (and more easily allow more cases to be added)\r\n\r\n_Originally posted by @thaJeztah in https:\/\/github.com\/moby\/moby\/pull\/47309#discussion_r1477043118_\r\n            ","comments":["\/cc @akerouanton @robmry "],"labels":["area\/networking","area\/testing"]},{"title":"[24.0 backport] gha: update actions to account for node 16 deprecation","body":"partial (\u26a0\ufe0f ) backport; some parts diverged on master, so I had to manually adjust:\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47250\r\n- https:\/\/github.com\/moby\/moby\/pull\/47263\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Looks like I missed a patch somewhere;\r\n\r\n```\r\nactions\/upload-artifact@v4\r\n  with:\r\n    name: integration-reports\r\n    path: \/tmp\/reports\/*\r\n    if-no-files-found: warn\r\n    compression-level: 6\r\n    overwrite: false\r\n  env:\r\n    GO_VERSION: 1.20.13\r\n    GOTESTLIST_VERSION: v0.3.1\r\n    TESTSTAT_VERSION: v0.1.3\r\n    ITG_CLI_MATRIX_SIZE: 6\r\n    DOCKER_EXPERIMENTAL: 1\r\n    DOCKER_GRAPHDRIVER: overlay2\r\n    CACHE_DEV_SCOPE: dev\r\nWith the provided path, there will be 215 files uploaded\r\nArtifact name is valid!\r\nRoot directory input is valid!\r\nError: Failed to CreateArtifact: Received non-retryable error: Failed request: (409) Conflict: an artifact with this name already exists on the workflow run\r\n```"],"labels":["status\/2-code-review","area\/testing"]},{"title":"v25.0.1 : Network not work correctly in project start","body":"Original issue: https:\/\/github.com\/docker\/cli\/issues\/4842\r\n\r\n### Description\r\n\r\nSince updating to Docker version 25.0.1, I've encountered an error with my Traefik saying, 'Could not resolve host: xxx.xxxxxxxx.local' for 'http:\/\/xxx.xxxxx.local\/me'.\r\n\r\nI spent the entire day trying to solve this problem, and finally found a solution. By creating a new temp network in the project and reinstalling it with docker compose up -d, my original network started working again and I resolved the 'Could not resolve host' issue.\r\n\r\nHowever, the problem is not consistently present. In some of my projects, the issue persists, while in others, Traefik and the network function normally.\r\n\r\n\r\n\r\n### Reproduce\r\n\r\nSetting Up Traefik: Configure Traefik to use your existing Docker network. It is after this setup that you might encounter the mentioned error (\"Could not resolve host\").\r\nReproducing the Problem: Attempt to access a specific service or route managed by Traefik. At this point, you should encounter the \"Could not resolve host\" error.\r\nCreating a New Temporary Network: Create a new Docker network named temp. This can be done using Docker commands or Docker Compose.\r\nReinstalling with Docker Compose: Run docker compose up -d to redeploy your services using the newly created temp network.\r\nTesting the Solution: After redeployment, try accessing the same service or route through Traefik again. If the solution is effective, the \"Could not resolve host\" error should no longer occur. This step verifies whether the new network setup resolves the issue.\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:09:23 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:09:23 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 32\r\n  Running: 28\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 49\r\n Server Version: 25.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.5.0-15-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 30.78GiB\r\n Name: LINUX25\r\n ID: d5f34c19-4626-4449-8e44-ee960d2641b5\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nIn this exemple, `admin` can't resolve host `api.xxx.local`\r\n\r\nFirst project, the `docker-compose.yml` of `Admin` : \r\n```yml\r\nversion: \"3.8\"\r\n\r\nservices:\r\n  app:\r\n    image: ardeveloppement\/system:2\r\n    container_name: connect-admin_app\r\n    restart: always\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.docker.network=traefik-proxy\"\r\n      - \"traefik.http.routers.connect-admin.rule=Host(`admin.xxx.local`)\"\r\n      - \"traefik.http.routers.connect-admin.entrypoints=web\"\r\n      - \"traefik.http.routers.connect-admin.service=connect-admin\"\r\n      - \"traefik.http.services.connect-admin.loadbalancer.server.port=80\"\r\n    environment:\r\n      ENVIRONMENT: 'development'\r\n      APP: nuxt\r\n      NUXT_COMMAND: yarn dev\r\n    env_file:\r\n      - .env\r\n    volumes:\r\n      - \".:\/srv\/app\/\"\r\n      - \".\/var\/logs\/nginx:\/var\/log\/nginx\"\r\n    networks:\r\n      - default\r\n      - traefik-proxy\r\n    external_links:\r\n      - reverse-proxy:auth.xxx.local\r\n      - reverse-proxy:api.xxx.local\r\n      - reverse-proxy:static.xxx.local\r\n      - reverse-proxy:customizr.xxx.local\r\n      - reverse-proxy:customizer-api.xxx.local\r\n      - reverse-proxy:catalog.xxx.local\r\n      - reverse-proxy:universe-api.xxx.local\r\n\r\nnetworks:\r\n  traefik-proxy:\r\n    external: true\r\n```\r\n\r\n----\r\n\r\nThe second project, the `docker-compose.yml` of  `core-api`: \r\n\r\n```yml\r\nversion: '3.8'\r\n\r\nservices:\r\n  app:\r\n    image: core-api:dev\r\n    build:\r\n      context: .\r\n      dockerfile: docker\/code\/Dockerfile.local\r\n    container_name: core-api_app\r\n    restart: always\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.docker.network=traefik-proxy\"\r\n      - \"traefik.http.routers.core-api.rule=Host(`api.xxx.local`)\"\r\n      - \"traefik.http.routers.core-api.entrypoints=web\"\r\n      - \"traefik.http.routers.core-api.service=core-api\"\r\n      - \"traefik.http.services.core-api.loadbalancer.server.port=80\"\r\n    environment:\r\n      PHP_FPM_MEMORY_LIMIT: 512M\r\n      PHP_FPM_MAX_EXECUTION_TIME: \"120\"\r\n      SYMFONY_ENV: dev\r\n      # docker inspect -f '{{range .NetworkSettings.Networks}}{{.Gateway}}{{end}}' core-api_app\r\n      XDEBUG_CONFIG: \"remote_host=172.18.0.1\"\r\n      PHP_XDEBUG_REMOTE_PORT: \"9001\"\r\n      PHP_IDE_CONFIG: \"serverName=xdebug-core-api\"\r\n    env_file:\r\n      - blackfire.env\r\n      - system.env\r\n    volumes:\r\n      - \".:\/srv\/app\"\r\n    networks:\r\n      - default\r\n      - traefik-proxy\r\n      - behat\r\n      - amqp\r\n    external_links:\r\n      - reverse-proxy:smtp.xxx.local\r\n      - reverse-proxy:auth.xxx.local\r\n      - reverse-proxy:customizr.xxx.local\r\n      - reverse-proxy:customizer.xxx.local\r\n      - reverse-proxy:customizer-api.xxx.local\r\n      - reverse-proxy:static.xxx.local\r\n      - reverse-proxy:catalog.xxx.local\r\n      - reverse-proxy:api.xxx.local\r\n      - reverse-proxy:digital-media.xxx.local\r\n      - reverse-proxy:mercure.xxx.local\r\n      - reverse-proxy:universe-api.xxx.local\r\n      - reverse-proxy:universe.xxx.local\r\n    ports:\r\n      - 9001\r\n\r\n  db:\r\n    image: mysql:5.6 # Be Careful in 5.7 passwords expire\r\n    container_name: core-api_db\r\n    restart: always\r\n    labels:\r\n      - \"traefik.enable=false\"\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD: root\r\n      MYSQL_DATABASE: core\r\n      MYSQL_USER: core\r\n      MYSQL_PASSWORD: core\r\n    volumes:\r\n      - \".\/:\/srv\/app\"\r\n      - \".\/data\/mysql\/db:\/var\/lib\/mysql\"\r\n      - \".\/docker\/mysql\/my.cnf:\/etc\/mysql\/conf.d\/my.cnf\"\r\n    networks:\r\n      - default\r\n      - monitoring\r\n    ports:\r\n      - \"127.0.0.1:4406:3306\"\r\n\r\n  redis:\r\n    image: redis:alpine\r\n    restart: always\r\n    labels:\r\n      - \"traefik.enable=false\"\r\n    volumes:\r\n      - \".\/data\/redis:\/data\"\r\n    networks:\r\n      - default\r\n      - redis\r\n    ports:\r\n      - 6359:6379\r\n\r\n  blackfire:\r\n    container_name: blackfire\r\n    image: blackfire\/blackfire\r\n    restart: always\r\n    env_file:\r\n      - blackfire.env\r\n    networks:\r\n      - default\r\n\r\n  chrome:\r\n    image: selenium\/standalone-chrome-debug:3.141.59-fluorine\r\n    environment:\r\n      VNC_NO_PASSWORD: 1\r\n    volumes:\r\n      - \/dev\/shm:\/dev\/shm\r\n    networks:\r\n      - behat\r\n    external_links:\r\n      - app:api.cospirit-connect.local\r\n    ports:\r\n      - \"4444:4444\"\r\n      - \"5901:5900\"\r\n\r\n  novnc:\r\n    image: ardeveloppement\/novnc:1.0.0\r\n    networks:\r\n      - behat\r\n    ports:\r\n      - \"6080:8081\"\r\n\r\nnetworks:\r\n  behat:\r\n  traefik-proxy:\r\n    external: true\r\n  monitoring:\r\n    external: true\r\n  redis:\r\n    external: true\r\n  amqp:\r\n    external: true\r\n```\r\n\r\n----\r\n\r\n`$ docker network inspect traefik-proxy`\r\n\r\n```bash\r\n[\r\n    {\r\n        \"Name\": \"traefik-proxy\",\r\n        \"Id\": \"901e5dcad04f9e3b402e71e9dad651fc951fe47f27e113a3946e276a2dfe47e6\",\r\n        \"Created\": \"2024-01-31T13:47:24.803364696+01:00\",\r\n        \"Scope\": \"local\",\r\n        \"Driver\": \"bridge\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": {},\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"192.168.112.0\/20\",\r\n                    \"Gateway\": \"192.168.112.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {\r\n            \"0ade53b1d7b4573e15942177caa7c06b16abe0fde613ee5246bcd78bf791ec0e\": {\r\n                \"Name\": \"universe-client_app\",\r\n                \"EndpointID\": \"08f75e5c3fb539c4858295900d53c536401975721f34247504cd850548075319\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:06\",\r\n                \"IPv4Address\": \"192.168.112.6\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\": {\r\n                \"Name\": \"connect-admin_app\",\r\n                \"EndpointID\": \"2a19aa8b13902c8a005ad13f9631b696b88c7d53eb4121a82e355cfd9c81f5b7\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:04\",\r\n                \"IPv4Address\": \"192.168.112.4\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"27a33a6d662a760d51810310bdef804a3bd71f3e690391f5f0fd7839f15bdb6c\": {\r\n                \"Name\": \"static_app\",\r\n                \"EndpointID\": \"93a6ba41d9e2f4d0ef3c39b0f9da2470c1562a1987e661cb15d06433dc7688ce\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:05\",\r\n                \"IPv4Address\": \"192.168.112.5\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"9610ffab83044a4e194223510e76df8ca7d7cac6c3a54339ef25a4f18d644171\": {\r\n                \"Name\": \"core-api_app\",\r\n                \"EndpointID\": \"291de55b57e85d07344c8581fcfc5cb676c7287b92a954e21c3d38585ca369d3\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:02\",\r\n                \"IPv4Address\": \"192.168.112.2\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"aa52d0308c5c6051a7d06d5bbd01a794468a2ce335c9d03f3a9477a981ce85c7\": {\r\n                \"Name\": \"customizer-client_app\",\r\n                \"EndpointID\": \"6ce46cd20fa1eca89dc203aea2fe958121a7f8de8d56f1698ed352ec3f21d165\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:03\",\r\n                \"IPv4Address\": \"192.168.112.3\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"ab34d9e223cc208958a9b3a5b341d0d5bdc3d6e86b02e5504fa00c83e303ae8a\": {\r\n                \"Name\": \"influxdb\",\r\n                \"EndpointID\": \"ff3a776bdcfabb1d85c8fff179339ddfe63db07c5370b6c7e49b585c2ece8f7c\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:08\",\r\n                \"IPv4Address\": \"192.168.112.8\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"adb2261610d9d8118fbe59cace0edb98082fc4f94d432e65bfc1546d3a9188b1\": {\r\n                \"Name\": \"universe-api_app\",\r\n                \"EndpointID\": \"f5cb315ce871e3c5e8fc341f25475059b98a498b6befe5cc5fb0ac11c4609d35\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:07\",\r\n                \"IPv4Address\": \"192.168.112.7\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"bdac4aed83531cbddb7e50debc9a90e9fde9c883c0ffb349c2cf248e0b66292a\": {\r\n                \"Name\": \"auth_app\",\r\n                \"EndpointID\": \"b25c75b7b3a21e174bc76582cee25d9bbf6296f542f765a4aaf16d46c21e32a3\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:0a\",\r\n                \"IPv4Address\": \"192.168.112.10\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"c4e8350147d9462118fedac95fced5cd0c61c837fcfccc20f421c04c92b915ac\": {\r\n                \"Name\": \"selenium-hub\",\r\n                \"EndpointID\": \"3762e8f90b5a484cd8f38ef4c00c5d983f685aa9398f0eef98f979bf1bc8fe92\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:09\",\r\n                \"IPv4Address\": \"192.168.112.9\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"d13e8dc48521529887d3a6a41dcc8d542e00d62a6d93028d1b2a8b73e09922d1\": {\r\n                \"Name\": \"reverse-proxy\",\r\n                \"EndpointID\": \"a82fcd4fec80324b903bf4e8ecd1ea8e83a1d1b74fd6f586a5ed36d63db83134\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:0c\",\r\n                \"IPv4Address\": \"192.168.112.12\/20\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"e2f3c14db667c146b1bc2a0df78659eb9e1b42f37691d4c77c3424d3500f4229\": {\r\n                \"Name\": \"customizer-api_app\",\r\n                \"EndpointID\": \"51f61cc850e5b67c91d221b7bb43c2b90d8b3cea2a7e600bc06943564996e0d1\",\r\n                \"MacAddress\": \"02:42:c0:a8:70:0b\",\r\n                \"IPv4Address\": \"192.168.112.11\/20\",\r\n                \"IPv6Address\": \"\"\r\n            }\r\n        },\r\n        \"Options\": {},\r\n        \"Labels\": {}\r\n    }\r\n]\r\n```\r\n---- \r\n`$ docker inspect 231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f`\r\n\r\n```bash\r\n[\r\n    {\r\n        \"Id\": \"231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\",\r\n        \"Created\": \"2024-02-01T08:58:22.13189286Z\",\r\n        \"Path\": \"\/root\/entrypoint\",\r\n        \"Args\": [\r\n            \"supervisord\",\r\n            \"--configuration\",\r\n            \"\/etc\/supervisor\/app.conf\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 82938,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2024-02-01T10:35:53.76616273Z\",\r\n            \"FinishedAt\": \"2024-02-01T10:35:53.225351739Z\"\r\n        },\r\n        \"Image\": \"sha256:889773541c27679548ac00e85c15aca94c75e04c4a491aac00f0abe7241c1a1e\",\r\n        \"ResolvConfPath\": \"\/var\/lib\/docker\/containers\/231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/lib\/docker\/containers\/231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\/hostname\",\r\n        \"HostsPath\": \"\/var\/lib\/docker\/containers\/231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\/hosts\",\r\n        \"LogPath\": \"\/var\/lib\/docker\/containers\/231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f\/231244b41ea4d6696ee2ec0f4e514e5adca797c97807427295b63c18207d460f-json.log\",\r\n        \"Name\": \"\/connect-admin_app\",\r\n        \"RestartCount\": 3,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"docker-default\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": [\r\n                \"\/home\/nicolas\/projects\/admin:\/srv\/app:rw\",\r\n                \"\/home\/nicolas\/projects\/admin\/var\/logs\/nginx:\/var\/log\/nginx:rw\"\r\n            ],\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"admin_default\",\r\n            \"PortBindings\": {},\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"always\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": [],\r\n            \"ConsoleSize\": [\r\n                0,\r\n                0\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"private\",\r\n            \"Dns\": null,\r\n            \"DnsOptions\": null,\r\n            \"DnsSearch\": null,\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": false,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": null,\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": null,\r\n            \"BlkioDeviceReadBps\": null,\r\n            \"BlkioDeviceWriteBps\": null,\r\n            \"BlkioDeviceReadIOps\": null,\r\n            \"BlkioDeviceWriteIOps\": null,\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": null,\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": null,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": [\r\n                \"\/proc\/asound\",\r\n                \"\/proc\/acpi\",\r\n                \"\/proc\/kcore\",\r\n                \"\/proc\/keys\",\r\n                \"\/proc\/latency_stats\",\r\n                \"\/proc\/timer_list\",\r\n                \"\/proc\/timer_stats\",\r\n                \"\/proc\/sched_debug\",\r\n                \"\/proc\/scsi\",\r\n                \"\/sys\/firmware\",\r\n                \"\/sys\/devices\/virtual\/powercap\"\r\n            ],\r\n            \"ReadonlyPaths\": [\r\n                \"\/proc\/bus\",\r\n                \"\/proc\/fs\",\r\n                \"\/proc\/irq\",\r\n                \"\/proc\/sys\",\r\n                \"\/proc\/sysrq-trigger\"\r\n            ]\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/lib\/docker\/overlay2\/863130bf7c882eb25a100a9b6b13a700f76cbf71a9766f0d11587d250e8bfcde-init\/diff:\/var\/lib\/docker\/overlay2\/6d3400eefa28d79df01253c2efcd848eb4ea1473fda6b25102ef0191601c4a17\/diff:\/var\/lib\/docker\/overlay2\/2563caf1528f5fa33cde9064c592c951a98b10ee3ca5ef0ac8031fcfea06f28f\/diff:\/var\/lib\/docker\/overlay2\/f54f6948a1487210fdcd68b77d9a990ca7fe6f5549fc9c3e659c2a9f593d4194\/diff:\/var\/lib\/docker\/overlay2\/e3c857806b6a4d9cffd1f32b876043f80308a9d26f0a5d0612c9a06fb4ea6f64\/diff:\/var\/lib\/docker\/overlay2\/77a3d67c4bcb1a41885adcc177b1d65ed3b869e8c6b7a5214c18e92a62a189a3\/diff:\/var\/lib\/docker\/overlay2\/19480c464bc543cd9542aa7c718004d23d49af4584bc686c7fe3b0ef81ee9bcf\/diff:\/var\/lib\/docker\/overlay2\/69f2e54779311504cfcece2fa2c50ffc9e15171056cc9a889b721fff48e06b1a\/diff:\/var\/lib\/docker\/overlay2\/f79f4a5719a91c66b65df235e5d943b74650ebd70421111b799fb7629cff1186\/diff:\/var\/lib\/docker\/overlay2\/aa58153123ad121702a519f516e6c287ba75db68c478032cf3f0baa2b15dadf1\/diff\",\r\n                \"MergedDir\": \"\/var\/lib\/docker\/overlay2\/863130bf7c882eb25a100a9b6b13a700f76cbf71a9766f0d11587d250e8bfcde\/merged\",\r\n                \"UpperDir\": \"\/var\/lib\/docker\/overlay2\/863130bf7c882eb25a100a9b6b13a700f76cbf71a9766f0d11587d250e8bfcde\/diff\",\r\n                \"WorkDir\": \"\/var\/lib\/docker\/overlay2\/863130bf7c882eb25a100a9b6b13a700f76cbf71a9766f0d11587d250e8bfcde\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/home\/nicolas\/projects\/admin\",\r\n                \"Destination\": \"\/srv\/app\",\r\n                \"Mode\": \"rw\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            },\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/home\/nicolas\/projects\/admin\/var\/logs\/nginx\",\r\n                \"Destination\": \"\/var\/log\/nginx\",\r\n                \"Mode\": \"rw\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            }\r\n        ],\r\n        \"Config\": {\r\n            \"Hostname\": \"231244b41ea4\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"NUXT_ENV_API_URL=http:\/\/customizer-api.cospirit-connect.local\",\r\n                \"NUXT_ENV_UNIVERSE_URL=http:\/\/universe-api.cospirit.local\",\r\n                \"NUXT_ENV_UNIVERSE_CLIENT_URL=http:\/\/universe.cospirit.local\",\r\n                \"NUXT_ENV_CATALOG_API_URL=http:\/\/catalog.cospirit-connect.local\",\r\n                \"NUXT_ENV_CORE_API_URL=http:\/\/api.cospirit-connect.local\/app_dev.php\",\r\n                \"NUXT_ENV_AUTH_URL=http:\/\/auth.cospirit-connect.local\",\r\n                \"NUXT_ENV_ADMIN_URL=http:\/\/admin.cospirit-connect.local\",\r\n                \"NUXT_ENV_STATIC_URL=http:\/\/static.cospirit-connect.local\/index_dev.php\",\r\n                \"NUXT_ENV_CUSTOMER_URL=http:\/\/customer.cospirit-connect.local\/app_dev.php\",\r\n                \"NUXT_ENV_CUSTOMIZER_API_URL=http:\/\/customizer-api.cospirit-connect.local\",\r\n                \"NUXT_ENV_CUSTOMIZER_URL=http:\/\/customizer.cospirit-connect.local\",\r\n                \"NUXT_ENV_STATIC_APPLICATION_ID=arconnect-doccustomizer\",\r\n                \"NUXT_ENV_STATIC_ORDERABLE_FOLDER=arconnect\",\r\n                \"APP_CORS_ALLOW_ORIGIN=^https?:\\\\\/\\\\\/[a-z0-9\\\\-]+\\\\.cospirit\\\\-connect\\\\.local$\",\r\n                \"APP_LOG_PATH=stdout\",\r\n                \"APP_VERSION_SERVICES_URL=http:\/\/services.cospirit-connect.local\",\r\n                \"CUSTOMIZER_URL=http:\/\/customizer.cospirit-connect.local\",\r\n                \"SELENIUM_HUB_URL=selenium-hub.cospirit.local\",\r\n                \"NIGHTWATCH_ENV=chromeHeadless\",\r\n                \"DEBUG=nuxt:*\",\r\n                \"USER_ID=1000\",\r\n                \"GROUP_ID=1000\",\r\n                \"ENVIRONMENT=development\",\r\n                \"APP=nuxt\",\r\n                \"NUXT_COMMAND=yarn dev\",\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\",\r\n                \"PHP_VERSION=7.4\",\r\n                \"PHP_MODULES_EXTRA=\",\r\n                \"PHP_DATE_TIMEZONE=UTC\",\r\n                \"NGINX_DIRECTIVES=[\\\"gzip\\\", \\\"error\\\", \\\"assets\\\"]\",\r\n                \"SYSTEM_TIMEZONE=Etc\/UTC\"\r\n            ],\r\n            \"Cmd\": [\r\n                \"supervisord\",\r\n                \"--configuration\",\r\n                \"\/etc\/supervisor\/app.conf\"\r\n            ],\r\n            \"Image\": \"ardeveloppement\/system:2\",\r\n            \"Volumes\": {\r\n                \"\/srv\/app\": {},\r\n                \"\/var\/log\/nginx\": {}\r\n            },\r\n            \"WorkingDir\": \"\/srv\/app\",\r\n            \"Entrypoint\": [\r\n                \"\/root\/entrypoint\"\r\n            ],\r\n            \"MacAddress\": \"02:42:ac:19:00:02\",\r\n            \"OnBuild\": null,\r\n            \"Labels\": {\r\n                \"com.docker.compose.config-hash\": \"a3d5f3fc02f180756a3eb0b61d36cc46c969acb6f5e7d6e4910bba772dcd8bb6\",\r\n                \"com.docker.compose.container-number\": \"1\",\r\n                \"com.docker.compose.oneoff\": \"False\",\r\n                \"com.docker.compose.project\": \"admin\",\r\n                \"com.docker.compose.project.config_files\": \"docker-compose.yml\",\r\n                \"com.docker.compose.project.working_dir\": \"\/home\/nicolas\/projects\/admin\",\r\n                \"com.docker.compose.service\": \"app\",\r\n                \"com.docker.compose.version\": \"1.27.4\",\r\n                \"maintainer\": \"AR Developpement <support-arconnect@cospirit.com>\",\r\n                \"traefik.docker.network\": \"traefik-proxy\",\r\n                \"traefik.enable\": \"true\",\r\n                \"traefik.http.routers.connect-admin.entrypoints\": \"web\",\r\n                \"traefik.http.routers.connect-admin.rule\": \"Host(`admin.cospirit-connect.local`)\",\r\n                \"traefik.http.routers.connect-admin.service\": \"connect-admin\",\r\n                \"traefik.http.services.connect-admin.loadbalancer.server.port\": \"80\"\r\n            }\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"977debc6b493955b976247849afb36fd089fab401844d3685f6cb8b5798c6c40\",\r\n            \"SandboxKey\": \"\/var\/run\/docker\/netns\/977debc6b493\",\r\n            \"Ports\": {},\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"\",\r\n            \"Gateway\": \"\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"\",\r\n            \"IPPrefixLen\": 0,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"\",\r\n            \"Networks\": {\r\n                \"admin_default\": {\r\n                    \"IPAMConfig\": {},\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"app\",\r\n                        \"231244b41ea4\"\r\n                    ],\r\n                    \"MacAddress\": \"02:42:ac:19:00:02\",\r\n                    \"NetworkID\": \"25413d11b1b9cdd23b1178e47d41b3fc8155decd97c58ef59ae3533a974644b1\",\r\n                    \"EndpointID\": \"b90c9b5505561db4f5edbe26bedd1cbd8ca95ee7f98bfb2266afdb751b7f1e4d\",\r\n                    \"Gateway\": \"172.25.0.1\",\r\n                    \"IPAddress\": \"172.25.0.2\",\r\n                    \"IPPrefixLen\": 16,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"DriverOpts\": null,\r\n                    \"DNSNames\": [\r\n                        \"connect-admin_app\",\r\n                        \"app\",\r\n                        \"231244b41ea4\"\r\n                    ]\r\n                },\r\n                \"traefik-proxy\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": [\r\n                        \"reverse-proxy:api.cospirit-connect.local\",\r\n                        \"reverse-proxy:auth.cospirit-connect.local\",\r\n                        \"reverse-proxy:catalog.cospirit-connect.local\",\r\n                        \"reverse-proxy:customizer-api.cospirit-connect.local\",\r\n                        \"reverse-proxy:customizr.cospirit-connect.local\",\r\n                        \"reverse-proxy:static.cospirit-connect.local\",\r\n                        \"reverse-proxy:universe-api.cospirit.local\"\r\n                    ],\r\n                    \"Aliases\": [\r\n                        \"app\",\r\n                        \"231244b41ea4\"\r\n                    ],\r\n                    \"MacAddress\": \"02:42:c0:a8:70:04\",\r\n                    \"NetworkID\": \"901e5dcad04f9e3b402e71e9dad651fc951fe47f27e113a3946e276a2dfe47e6\",\r\n                    \"EndpointID\": \"2a19aa8b13902c8a005ad13f9631b696b88c7d53eb4121a82e355cfd9c81f5b7\",\r\n                    \"Gateway\": \"192.168.112.1\",\r\n                    \"IPAddress\": \"192.168.112.4\",\r\n                    \"IPPrefixLen\": 20,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"DriverOpts\": null,\r\n                    \"DNSNames\": [\r\n                        \"connect-admin_app\",\r\n                        \"app\",\r\n                        \"231244b41ea4\"\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n----\r\n\r\n`The content of \/etc\/resolv.conf, from the container where Traefik is running, when the issue occurs ;`\r\n```\r\nsearch xxx.org\r\nnameserver 127.0.0.11\r\noptions ndots:0\r\n```","comments":["@neerfix Thanks for recreating the issue here.\r\n\r\nFew things; could you please reduce your reproducer to its most minimal version? That is:\r\n\r\n- Try removing containers one by one until the issue becomes irreproducible or until you've only two containers left (Traefik and another one) ;\r\n- Use a single, hermetic compose file ;\r\n- Rely only on simple \/ official images ;\r\n- etc...\r\n\r\n\r\nThere're way too many moving parts here for us to investigate without spending _a lot_ of time going through the same process. Plus, we can't run the reproducer as-is because it relies on bind-mounts, external networks, etc... Basically, we can't copy\/paste it and observe the issue -- that's what we need to investigate.\r\n\r\nOnce you have a minimal reproducer, please update your original post.\r\n\r\n> Since updating to Docker version 25.0.1, I've encountered an error with my Traefik saying, 'Could not resolve host: xxx.xxxxxxxx.local' for 'http:\/\/xxx.xxxxx.local\/me'.\r\n\r\nUnfortunately you're blanking out the most interesting part of the error message: the exact domain name that couldn't be resolved. Could you also update your original post to include that please?\r\n\r\nBtw, one thing that strikes me: `.local` TLD has some special meaning (see https:\/\/en.wikipedia.org\/wiki\/.local). Could you try switching to something else?","It seems to be working; I can't reproduce the issue in any other way."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","area\/networking","version\/25.0"]},{"title":"Data race (`libnetwork.(*Controller).getNetworksFromStore`)","body":"### Description\n\n```\r\n==================\r\nWARNING: DATA RACE\r\nWrite at 0x00c004908e00 by goroutine 5045:\r\n  github.com\/docker\/docker\/libnetwork.(*Controller).getNetworksFromStore()\r\n      \/go\/src\/github.com\/docker\/docker\/libnetwork\/store.go:99 +0x566\r\n  github.com\/docker\/docker\/libnetwork.(*Controller).Networks()\r\n      \/go\/src\/github.com\/docker\/docker\/libnetwork\/controller.go:818 +0x5e\r\n  github.com\/docker\/docker\/daemon.(*Daemon).getAllNetworks()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:144 +0x1ba\r\n  github.com\/docker\/docker\/daemon.(*Daemon).clearAttachableNetworks()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:765 +0x3e\r\n  github.com\/docker\/docker\/daemon.(*Daemon).DaemonLeavesCluster()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/daemon.go:744 +0x3e\r\n  github.com\/docker\/docker\/daemon\/cluster.(*Cluster).Leave()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/cluster\/swarm.go:427 +0xd72\r\n  github.com\/docker\/docker\/api\/server\/router\/swarm.(*swarmRouter).leaveCluster()\r\n      \/go\/src\/github.com\/docker\/docker\/api\/server\/router\/swarm\/cluster_routes.go:59 +0xaa\r\n  github.com\/docker\/docker\/api\/server\/router\/swarm.(*swarmRouter).leaveCluster-fm()\r\n      <autogenerated>:1 +0x84\r\n  github.com\/docker\/docker\/api\/server\/middleware.(*ExperimentalMiddleware).WrapHandler.ExperimentalMiddleware.WrapHandler.func1()\r\n      \/go\/src\/github.com\/docker\/docker\/api\/server\/middleware\/experimental.go:26 +0xc2\r\n  github.com\/docker\/docker\/api\/server\/middleware.(*VersionMiddleware).WrapHandler.VersionMiddleware.WrapHandler.func1()\r\n      \/go\/src\/github.com\/docker\/docker\/api\/server\/middleware\/version.go:62 +0x3d0\r\n  github.com\/docker\/docker\/pkg\/authorization.(*Middleware).WrapHandler.func1()\r\n      \/go\/src\/github.com\/docker\/docker\/pkg\/authorization\/middleware.go:59 +0xb9e\r\n  github.com\/docker\/docker\/api\/server.(*Server).makeHTTPHandler.func1()\r\n      \/go\/src\/github.com\/docker\/docker\/api\/server\/server.go:55 +0x25e\r\n  net\/http.HandlerFunc.ServeHTTP()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:2136 +0x47\r\n  go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*middleware).serveHTTP()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/handler.go:217 +0x1b18\r\n  go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.NewMiddleware.func1.1()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/handler.go:81 +0x67\r\n  net\/http.HandlerFunc.ServeHTTP()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:2136 +0x47\r\n  net\/http.Handler.ServeHTTP-fm()\r\n      <autogenerated>:1 +0x67\r\n  net\/http.HandlerFunc.ServeHTTP()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:2136 +0x47\r\n  github.com\/gorilla\/mux.(*Router).ServeHTTP()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/gorilla\/mux\/mux.go:212 +0x361\r\n  net\/http.serverHandler.ServeHTTP()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:2938 +0x2a1\r\n  net\/http.(*conn).serve()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:2009 +0xc24\r\n  net\/http.(*Server).Serve.func3()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:3086 +0x4f\r\n\r\nPrevious read at 0x00c004908e00 by goroutine 386:\r\n  github.com\/docker\/docker\/libnetwork.(*Network).createLoadBalancerSandbox()\r\n      \/go\/src\/github.com\/docker\/docker\/libnetwork\/network.go:2145 +0x2aa\r\n  github.com\/docker\/docker\/libnetwork.(*Controller).NewNetwork()\r\n      \/go\/src\/github.com\/docker\/docker\/libnetwork\/controller.go:692 +0x1d27\r\n  github.com\/docker\/docker\/daemon.(*Daemon).createNetwork()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:372 +0x1424\r\n  github.com\/docker\/docker\/daemon.(*Daemon).setupIngress()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:214 +0x1d5\r\n  github.com\/docker\/docker\/daemon.(*Daemon).startIngressWorker.func1()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:167 +0x18b\r\n\r\nGoroutine 5045 (running) created at:\r\n  net\/http.(*Server).Serve()\r\n      \/usr\/local\/go\/src\/net\/http\/server.go:3086 +0x86c\r\n  main.(*DaemonCli).start.func4()\r\n      \/go\/src\/github.com\/docker\/docker\/cmd\/dockerd\/daemon.go:335 +0x1fa\r\n  main.(*DaemonCli).start.func8()\r\n      \/go\/src\/github.com\/docker\/docker\/cmd\/dockerd\/daemon.go:346 +0x4f\r\n\r\nGoroutine 386 (running) created at:\r\n  github.com\/docker\/docker\/daemon.(*Daemon).startIngressWorker()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:161 +0xe4\r\n  github.com\/docker\/docker\/daemon.(*Daemon).startIngressWorker-fm()\r\n      <autogenerated>:1 +0x33\r\n  sync.(*Once).doSlow()\r\n      \/usr\/local\/go\/src\/sync\/once.go:74 +0xf0\r\n  sync.(*Once).Do()\r\n      \/usr\/local\/go\/src\/sync\/once.go:65 +0x44\r\n  github.com\/docker\/docker\/daemon.(*Daemon).enqueueIngressJob()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:182 +0x20a\r\n  github.com\/docker\/docker\/daemon.(*Daemon).SetupIngress()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/network.go:194 +0xd1\r\n  github.com\/docker\/docker\/daemon\/cluster\/executor\/container.(*executor).Configure()\r\n      \/go\/src\/github.com\/docker\/docker\/daemon\/cluster\/executor\/container\/executor.go:238 +0x148a\r\n  github.com\/moby\/swarmkit\/v2\/agent.(*Agent).handleSessionMessage()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/agent\/agent.go:433 +0x6f6\r\n  github.com\/moby\/swarmkit\/v2\/agent.(*Agent).run()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/agent\/agent.go:299 +0x1e24\r\n  github.com\/moby\/swarmkit\/v2\/agent.(*Agent).Start.func1.1()\r\n      \/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/swarmkit\/v2\/agent\/agent.go:83 +0x4f\r\n==================\r\n```\n\n### Reproduce\n\n1. Build a daemon with the race detector enabled\r\n2. `while :; do docker swarm init & docker swarm leave --force & done`\n\n### Expected behavior\n\nNo data races\n\n### docker version\n\n```bash\nsome dirty tree close to 8a81b9d35fb89251437bd046d2154f6c73b22f7c\n```\n\n\n### docker info\n\n```bash\nN\/A\n```\n\n\n### Additional Info\n\n_No response_","comments":["`(*Controller).getNetworksFromStore` fetches each (potentially cached) network from the datastore, locks it, then does some initialization to fully hydrate the network object. It is called whenever networks are enumerated, including the `GET \/networks` Engine API.\r\n\r\n`(*Controller).NewNetwork` creates a new network object, does a bunch of initialization, adds it to the datastore, then does some more initialization. In particular, it calls `(*Network).createLoadBalancerSandbox` without holding the network's lock, and `createLoadBalancerSandbox` accesses network fields without locking either.\r\n\r\nThis data race can happen whenever a network is being created concurrently with networks being enumerated. In this backtrace it happened because `docker swarm init` and `docker swarm leave` were racing, but it could just as easily happen with concurrent `docker network create` and `docker network ls`."],"labels":["kind\/bug","area\/networking"]},{"title":"Add a Windows regression test for `--mac-address`","body":"### Description\r\n\r\nTry to implement a Windows regression test for the fix in https:\/\/github.com\/moby\/moby\/pull\/47233 (as described in its \"how to verify\").,\r\n\r\n### Reproduce\r\n\r\nSee the PR.\r\n\r\n### Expected behavior\r\n\r\nThere's a regression test.\r\n\r\n### docker version\r\n\r\n```bash\r\nn\/a\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nn\/a\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["This turned out to be quite tricky ... the Windows tests all run under a single instance of the daemon, there's no existing way to set up a new daemon for a test (which, in this case, could be restarted). And, it's not simple to do - perhaps for many reasons, but at-least because on startup the Windows daemon looks for networks on the host and tidies up any networks it doesn't recognise."],"labels":["kind\/enhancement","area\/networking"]},{"title":"api\/build: Use v2 builder by default","body":"Use Buildkit based image builder by default when building via the API.\r\n\r\n(A draft PR to see what CI thinks)\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/api","area\/builder","area\/builder\/classic-builder","area\/builder\/buildkit"]},{"title":"[23.0] Vendor rootlesskit v2","body":"Vendor rootlesskit v2 and all dependencies\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":["\ud83e\udd26 looked from my phone and didn't consider who opened the PR. Do we need similar backports for 24 and 25? Otherwise newer versions use an older version of rootlesskit?","> \ud83e\udd26 looked from my phone and didn't consider who opened the PR. Do we need similar backports for 24 and 25? Otherwise newer versions use an older version of rootlesskit?\r\n\r\nSure, I'll take care about it","Also heads-up that there was a regression in v2.0.0; we merged a follow-up that updates it to v2.0.1;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47332","I think we should hold off until #47248 is resolved as I suspect it is also a rootlesskit\/v2 regression","> I think we should hold off until #47248 is resolved as I suspect it is also a rootlesskit\/v2 regression\r\n\r\nBeing fixed in:\r\n- https:\/\/github.com\/moby\/moby\/pull\/47558\r\n\r\nThis was not a regression in rootlesskit v2, but rather related to changing the state dir from `\/tmp\/...` to `\/run\/...`.\r\n\r\n"],"labels":["area\/rootless"]},{"title":"Documentation: Amend commands to run unit tests on windows","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\nUpdate documentation on command to run unit tests on windows\r\n\r\n**- How I did it**\r\nAdd `-e DOCKER_GITCOMMIT=$DOCKER_GITCOMMIT`\r\n**- How to verify it**\r\nConfirm that the following error is not reported\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/28774968\/79de8e3c-d7cd-4fa5-bcf2-e7251ac41709)\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/3-docs-review","area\/docs","area\/project"]},{"title":"`docker system df` results in \"Error response from daemon: failed to retrieve image list\"","body":"### Description\n\nI'm attempting to view the disk space used by Docker Desktop for Mac, but running\u2026\r\n\r\n```\r\n$ docker system df\r\n```\r\n\r\n\u2026 results in the error\u2026\r\n\r\n```\r\nError response from daemon: failed to retrieve image list: snapshot sha256:ff480b454b3df027c4130a7c65e097edb38ba7056168d786377c10a16f7eeaf5 does not exist: not found\r\n```\r\n\r\nI'm guessing this SHA256 is an image layer that's causing problemd, but not sure how to work out which image it's part of\n\n### Reproduce\n\n1. docker system df\n\n### Expected behavior\n\nIt shoud return information about the available disk space for the Docker for Desktop VM.\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:04:20 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.26.1 (131620)\r\n Engine:\r\n  Version:          master\r\n  API version:      1.44 (minimum version 1.12)\r\n  Go version:       go1.21.3\r\n  Git commit:       54fcd40aa4de94cd75aedc5f6ebf38c6d8f92082\r\n  Built:            Wed Nov 22 07:43:39 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.25\r\n  GitCommit:        d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.7\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.0-desktop.2\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.3-desktop.2\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  0.1\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.10\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.2.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 24\r\n  Running: 22\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 51\r\n Server Version: master\r\n Storage Driver: stargz\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: zgkgf9kdlj1j77t2rrqtnathl\r\n  Is Manager: true\r\n  ClusterID: 7r7pg6i1j3omb8qt8vldrhhl6\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.65.4\r\n  Manager Addresses:\r\n   192.168.65.4:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.5.11-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 12\r\n Total Memory: 7.662GiB\r\n Name: docker-desktop\r\n ID: 8767e463-13dd-4457-952d-e5546062bfa5\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nI also noticed, when I ran `docker info` that this was reported\u2026\r\n\r\n```\r\nWARNING: daemon is not using the default seccomp profile\r\n```\r\n\r\nIs this normal?","comments":["Thanks for reporting. ISTR there's been some fixes in this area for the containerd-snapshotter, that are included in the v25.xx releases (now included in Docker Desktop v4.27), but I'd have to check if those fixes _prevent_ incorrect state, or also apply when this state is encountered (if not, it may require relevant state\/images to be removed).\r\n\r\nI should also mention that we changed the default snapshotter with containerd integration to `overlayfs`, and made stargz an opt-in (in practice, stargz didn't provide many benefits for _most_ scenarios, and there were some corner-cases where it didn't behave fully as expected); if you happen to do a \"factory\" reset (or to remove data) after updating to Docker Desktop 4.27, it should pick `overlayfs`  as a default.\r\n\r\n> I also noticed, when I ran docker info that this was reported\u2026\r\n>\r\n> WARNING: daemon is not using the default seccomp profile\r\n\r\nThis is expected (although we're considering to remove the warning). Docker Desktop provides additional security boundaries (the Daemon and Containers running inside a VM, as well as the option to enable [hardened-desktop](https:\/\/docs.docker.com\/desktop\/hardened-desktop\/)). For this reason, seccomp filtering did not provide significant benefits in Docker Desktop, but is known to cause additional overhead, affecting performance. Docker Desktop therefore is configured with seccomp disabled by default, but (with docker v25.0; see https:\/\/github.com\/docker\/cli\/pull\/4754) allows opting-in to enable for situations where you want to replicate a setup running outside of Docker Desktop.\r\n\r\n"],"labels":["status\/0-triage","kind\/bug","containerd-integration","version\/24.0"]},{"title":"libnetwork: clean up ovmanager pseudo-driver","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\nThe ovmanager driver does not do very much. Its primary function is to ensure that each subnet of each overlay network in the Swarm has a unique VXLAN ID assigned. It only cares about the number of IPAM pools, not the specifics. Stop the ovmanager driver from holding unnecessary state.\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\nTODO TODO TODO\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/networking","kind\/refactor","area\/networking\/d\/overlay"]},{"title":"Docker 25 cannot use unsecured private s3 registry","body":"### Description\r\n\r\nDocker v25 is confused when pulling images through unsecured registry and all pulls end in failure.\r\n\r\nMy unsecured registry on loopback (127.0.01) is using S3 in secured mode (`secure: true`) so that while loopback traffic is unencrypted, the registry \"talks\" to S3 over https.\r\n\r\nIn logs, the registry urls all indicate `url=https:\/127.0.0.1:5000\/...` even though the loopback registry does not serve over TLS. \r\nThe pull ends in failure with `failed to copy: httpReadSeeker: failed to open: failed to do request: Get \"https:\/\/[url to a blob]\": dial tcp 52.217.75.76:80 i\/o timeout`.\r\nNote that it is trying to connect to port 80 when it should use 443.\r\n\r\n(`52.217.75.76` is the IP of `s3-1-w.amazonaws.com`).\r\n\r\nWhen setting up a dummy self-signed certificate and making the loopback registry use TLS instead, then everything starts working as expected.\r\n\r\nThis seems related, or at least similar, to #36263 .\r\nThis too seems similar. https:\/\/github.com\/docker\/buildx\/issues\/2030\r\n\r\n### Reproduce\r\n\r\n1. Use docker-distribute to setup a S3 backed registry listening on loopback 127.0.0.1:5000. Be sure to enable `secure: true` in the storage.s3 section\r\n2. Try to pull an image with docker v25 through the local registry, `docker pull 127.0.0.1:5000\/some_image`.\r\n\r\nSwitch to secured registry to make it work\r\n1. Update the `http.addr` for the registry to \"localhost:5000\"\r\n1. Generate a self signed certificate for \"localhost\" and update the docker-distribute config to have a TLS section (see https:\/\/github.com\/wking\/docker-distribution\/blob\/master\/docs\/configuration.md for more info)\r\n \t``` yaml\r\n        http:\r\n            addr: localhost:5000\r\n            tls:\r\n                certificate: \/path\/to\/x509\/locahost.crt\r\n                key: \/path\/to\/x509\/locahost.key\r\n        ```\r\n3. Pull the same image `docker pull localhost:5000\/some_image` and it now works\r\n\r\n### Expected behavior\r\n\r\ndocker pull should function with unsecured private registries just as well as with secured registries.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.24+dfsg1\r\n API version:       1.41\r\n Go version:        go1.19.8\r\n Git commit:        297e128\r\n Built:             Thu May 18 08:38:34 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          v25.0.0\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.21.6\r\n  Git commit:       311b9ff0aa93aa55880e1e5f8871c4fb69583426\r\n  Built:            Fri Jan 26 20:46:21 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.20~ds1\r\n  GitCommit:        1.6.20~ds1-1+b1\r\n runc:\r\n  Version:          1.1.5+ds1\r\n  GitCommit:        1.1.5+ds1-1+b1\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: v25.0.0\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1.6.20~ds1-1+b1\r\n runc version: 1.1.5+ds1-1+b1\r\n init version:\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-15-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 7.753GiB\r\n Name: bookworm\r\n ID: e432cf75-f542-4c5c-99a2-7c5fc146ce5c\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["> The pull ends in failure with failed to copy: httpReadSeeker: failed to open: failed to do request: Get \"https:\/\/[url to a blob]\": dial tcp 52.217.75.76:80 i\/o timeout\r\n\r\nCan you share more logs about the request, such as the host after the `GET` (or just the format, feel free to keep the path redacted). This particular error is coming up from the Go HTTP library, which is handling the redirect coming from the registry. It seems as though the redirect is happening just fine without any issue connecting to the registry. There maybe something in the configured transport that is missing some TLS configuration, but odd that the result is an io timeout. Is this happening every time?","Yes, this is happening every time.\r\n\r\nNote, in case this was not clear in my first write up, that even though the registry is unsecured (regular http), the docker logs show `url=https:\/\/127.0.0.1:5000\/...` (note `https:\/\/`) as it iterates through the image manifest. Since it works and my registry, as configured, is not capable of talking https, clearly the logs' `url=https` is bogus.\r\n\r\nThen, when it gets to retrieving the first blob of the image, it still indicate in the logs the `url=https:\/\/<some s3 alias>`, which is good since we asked for secured S3. But then it fails with `dial tcp 52.217.75.76:80 i\/o timeout` **on port 80**, or the default port for plain http, which explains the timeout.\r\n\r\nI used curl to make sure that was not a go http client problem. curl'ing the s3 url on https works, but using the same url with http it times out as well.\r\n\r\nI think the issue is that, despite what it says the logs, that docker is trying to talk to the s3 backend over plain http.  It is almost as if, when talking to an unsecured registry, it made turned its https client into a plain http client and kept on using the same \"dumb down\" client instance when retrieving from S3.\r\n\r\nOne more tidbit of information: I am using the containerd snapshotter backend and if I try to pull the image with `ctr i pull`, it works just fine.\r\n\r\n\r\nAs for providing full requests, I'll try to do that a little later as I must spend some anonymizing our internal hostnames, S3 tokens etc.\r\n\r\n\r\nCheers,","I see the problem, the fallback mechanism Moby is using right now for HTTP\/HTTPS is setting HTTP for the whole transport which Go will use for redirect, https:\/\/github.com\/moby\/moby\/blob\/0f507ef62421f57aae674d2d7282d88453a3099a\/vendor\/github.com\/moby\/buildkit\/util\/resolver\/resolver.go#L222.\r\n\r\nWe should either switch to containerd's which does not store the state https:\/\/github.com\/moby\/moby\/blob\/0f507ef62421f57aae674d2d7282d88453a3099a\/vendor\/github.com\/containerd\/containerd\/remotes\/docker\/resolver.go#L714 or the buildkit one needs to be smarter about only falling back to HTTP for the same host. ","Hi, while I do see the builkit resolver's http fallback state being kept in there, please note that this is not an issue with docker v24.0.7 which was using an older release of buildkit","Is there anything else I can provide to help the investigation?\r\nI tried to revert to buildkit v0.11, as used in v24 which works, but failed to resolve all the conflicts thus far.\r\n\r\n"],"labels":["area\/distribution","status\/0-triage","kind\/bug","area\/builder\/buildkit","containerd-integration","version\/25.0"]},{"title":"Set containerd container image ref","body":"This populates the \"Image\" field on containerd containers, but only when \r\nusing the containerd image store.\r\nThis allows containerd clients to look up the image information.\r\n\r\nIn order to be able to test this change I have also added the containerd connection info to the `\/info` endpoint.\r\nThis also just generally helpful for debugging purposes.\r\n\r\nI found that we weren't setting this while reviewing https:\/\/github.com\/containerd\/runwasi\/pull\/405\r\nIn that PR the shim is traversing the container object to get the image info (as mentioned it in the PR comments it would be nice if there was an immutable ref available, but that's a whole other thing).","comments":["LGTM","Upon talking in the maintainer's call; this may not be a good idea to backport as we'd have to extend `\/info` and that is technically a breaking change in the Go APIs due to the \"one huge struct\" implementation.\r\n\r\nIt's nice to have as it makes Wasm containers faster to start; but it's not broken in 26.0 per-se."],"labels":["area\/runtime","status\/2-code-review","containerd-integration"]},{"title":"libnet: remove the `EndpointInfo` interface ","body":"- Temporarily stacked on top of #47216 to make `TestSRVServiceQuery` pass\r\n\r\n**- What I did**\r\n\r\n`EndpointInfo` was introduced in https:\/\/github.com\/moby\/libnetwork\/commit\/d8ba1e231. The commit's message states:\r\n\r\n> This sort of deisgn hides the libnetwork specific type details from drivers.\r\n\r\n9 years later it turns out the `Endpoint` struct is the only implementor of that interface, and `libnetwork` and `daemon` packages are the only consumers. This is adding an unneeded level of indirection.\r\n\r\n`(*Endpoint).Info()` is the only function that returns an `EndpointInfo`. It's basically 12 lines of cursed indirection packed together. But, as noted by thaJeztah in https:\/\/github.com\/moby\/moby\/commit\/210abfaef6d67eb6e404e88d7e20c7ac30820a1f, we need to make sure the `*Endpoint` receiver is always fully hydrated to get rid of it. So here we go.\r\n\r\nAs the diff shows, most of the time `(*Endpoint).Info()` is called right after a call to `(*Network).Endpoints()` (sometimes through an indirection). That method in turns calls `getEndpointsFromStore`, which is calling `(*Store).List()`, which is either retrieving all objects of a given type from its internal cache (if it was populated), or retrieve them all from boltdb. In that case, `*Endpoint`s are fully hydrated.\r\n\r\n- `disconnectFromNetwork`: calls `WalkEndpoints`, which calls `(*Network).Endpoints()`, right before ;\r\n- `buildContainerAttachments`: calls `(*Network).Endpoints()` right before ;\r\n- `buildEndpointResource`: `ep` and `info` are actually both the same thing ;\r\n- `clearAttachableNetworks`: calls `(*Network).Endpoints()` right before ;\r\n- `findLBEndpointSandbox`: calls `(*Network).Endpoints()` right before ;\r\n- `addLBBackend`: calls `(*Network).Endpoints()` right before ;\r\n- `deleteLoadBalancerSandbox`: calls `(*Network).EndpointByName()`, which calls `WalkEndpoint` (see above).\r\n\r\nWe're left with the following, less obvious cases:\r\n\r\n- `updateJoinInfo`: called from `(*Daemon).connectToNetwork()`, where `*Endpoint`s are created ;\r\n- `buildEndpointInfo`: called from `(*Daemon).updateEndpointNetworkSettings()`, again called from `(*Daemon).connectToNetwork()` ;\r\n\r\nMeaning, we can now safely assume `*Endpoint`s are always fully hydrated before calling its `Info()` method. We can finally ditch it! \ud83c\udf89\r\n\r\n**- How I did it**\r\n\r\nStatic analysis of the code\r\n\r\n**- How to verify it**\r\n\r\nCI is green.\r\n","comments":["I was surprised I didn't remove this one yet, but looking at https:\/\/github.com\/moby\/moby\/commit\/210abfaef6d67eb6e404e88d7e20c7ac30820a1f (https:\/\/github.com\/moby\/moby\/pull\/46314), I see I left a comment at the time;\r\n\r\n> We should remove this interface, and the related Info() function, but it's currently acting as a \"gate\" to prevent accessing the Endpoint's accessors without making sure it's fully hydrated.\r\n\r\nWondering if there's cases where the information may not be propagated (and if there's a risk involved in removing that part).\r\n\r\n\r\nOtherwise, a big \"+1\" for removing.","I'll convert this one to draft for now as there're a few CI jobs broken -- I might have missed something."],"labels":["status\/2-code-review","area\/networking","kind\/refactor"]},{"title":"libnet: don't lock `*Endpoint` in the void","body":"**- What I did**\r\n\r\n`(*Endpoint).sbJoin()`, `(*Endpoint).sbLeave()` start by calling `(*Network).getEndpointFromStore()` before doing anything else. In turn this method calls `(*Store).GetObject()` with a freshly created `*Endpoint` instance, meaning the `*Endpoint` instance returned by `getEndpointFromStore` is a totally different reference than the one already living in memory. Thus, any mutex locks on these new instances actually lock *nothing* at all.\r\n\r\nAnother property of `GetObject()` is to fully hydrate a given object based on the \"canonical\" reference that lives in its cache.\r\n\r\nSince both `sbJoin()` and `sbLeave()` are always called on an `*Endpoint` receiver which were either just created, or just retrieved from the datastore, there's no chance that the receiver was partially hydrated.\r\n\r\n**- How I did it**\r\n\r\nBy conducting a static analysis of `sbJoin()`, `sbLeave()` and their callsites.\r\n\r\n**- How to verify it**\r\n\r\nCI\r\n\r\n","comments":["@akerouanton I think the failure may be related (not a flaky);\r\n\r\n```\r\n=== Failed\r\n=== FAIL: libnetwork TestSandboxAddMultiPrio (0.29s)\r\ntime=\"2024-01-25T14:34:34Z\" level=error msg=\"Could not add route to IPv6 network fe90::1\/64 via device test_nw_0: network is down\"\r\n    sandbox_unix_test.go:179: another container is attached to the same network endpoint\r\n\r\n=== FAIL: libnetwork TestResolvConf (0.10s)\r\n    libnetwork_linux_test.go:2049: another container is attached to the same network endpoint\r\n    libnetwork_linux_test.go:1977: network testnetwork id cdc1acb28253c687c212affa09a2183ef96a76907577fc23701f6ee2e63cba13 has active endpoints\r\n```\r\n","Yep, they're. I'll convert this one as a draft for now.","@thaJeztah Turns out, the same fix had to be applied to `sbLeave()` too. This one's ready now.","> Since both sbJoin() and sbLeave() are always called on an *Endpoint receiver which were either just created, or just retrieved from the datastore, there's no chance that the receiver was partially hydrated.\r\n\r\nSilly question; are there other code-paths that could have been executed concurrently that could have modified the endpoint (so the hydrated copy we have being a stale \/ outdated copy)? (as there's no lock _before_ we retrieved the hydrated copy, so curious if there's any synchronisation to prevent that from happening).","> Since both `sbJoin()` and `sbLeave()` are always called on an `*Endpoint` receiver which were either just created, or just retrieved from the datastore, there's no chance that the receiver was partially hydrated.\r\n\r\nConsider the call chain `(*Endpoint).sbJoin` <- `(*Endpoint).Join` <- `(*Sandbox).Refresh` <- `(*Daemon).updateNetwork`. The sandbox is fetched from the controller's `c.sandboxes` map, the endpoints are fetched from the sandbox's `sb.endpoints` map, and `ep.Join(sb)` is called on all of those endpoints which were neither just created nor just retrieved from the datastore.\r\n\r\nThe endpoint receiver being partially hydrated is not the most important concern; if it is out of date w.r.t. the KVObject in the datastore, the `n.getController().updateToStore(ep)` call in `sbJoin` will fail and there is no rollback-retry loop. So long as `func (*Network) getEndpointFromStore` exists in its current form, I cannot trust that `Endpoint` objects in memory are unique."],"labels":["status\/2-code-review","area\/networking","kind\/refactor"]},{"title":"High system load following 2 recent Docker upgrades","body":"### Description\n\nWe've experienced really high CPU load on our server twice recently, both times immediatly at the point that `yum` starts the upgrade of Docker packages.\r\n\r\nWe are running on CentOS\n\n### Reproduce\n\n1. Install Plesk 18.0.58;\r\n2. Install Docker via Plesk;\r\n3. Configure automatic upgrades;\r\n\r\nI suspect there may be a 'start these Docker services' step also that might make a difference, but I've not Idea what is causing the high load.\n\n### Expected behavior\n\nDocker should be upgraded and conters restart without any issues, and we should only see brief CPU usge spikes as the containers start up.\n\n### docker version\n\n```bash\nAfter last night's upgrade\u2026\r\n\r\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:12:51 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:11:50 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 193\r\n  Running: 70\r\n  Paused: 0\r\n  Stopped: 123\r\n Images: 503\r\n Server Version: 25.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: loki\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\nSwarm: active\r\n  NodeID: l603mylmj6hnh95at639cy4w0\r\n  Is Manager: true\r\n  ClusterID: vsayqmcuktjv67jgmtgjqm5kk\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 31.193.2.114\r\n  Manager Addresses:\r\n   31.193.2.114:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 3.10.0-1160.95.1.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 25.34GiB\r\n Name: id79706\r\n ID: THRG:QZBK:5427:DLKF:M4HF:Y6TQ:32CN:PTR3:IWUG:HOEC:C5T7:TTNI\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nHere is our System Load graph with the upgrade times marked.  This shows last nights issue as more severe.\r\n\r\nThe initial upgrade issue was causing quite a bit of issues running things like `ps` and `htop`, but fortunately the web server was still responding OK at that point.  The cliff edge marked in blue was a server reboot we performed to get things under control, and even that took far longer than typical to come back up again, and I had to contact our support team to get it back online.\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/4817007\/ef24d548-8fc7-457d-9ca1-7a16cc3a6d78)\r\n\r\nSystem info\u2026\r\nCentOS Linux release 7.9.2009 (Core)\r\n\r\nPlesk\u2026\r\n18.0.58.2\r\nPlesk Obsidian 18.0\r\n\r\nThe 2 recent yum upgrades\u2026\r\n\r\n```\r\nLoaded plugins: fastestmirror\r\nTransaction ID : 2367\r\nBegin time     : Thu Jan 25 03:25:21 2024\r\nBegin rpmdb    : 1258:c9f9a40e049bf4d96c3bf8f46d1daabc1cc1f718\r\nEnd time       :            03:30:30 2024 (5 minutes)\r\nEnd rpmdb      : 1258:94fbfead98d6a40bb9bd43a96bdee5b52ce5fca5\r\nUser           : System <unset>\r\nReturn-Code    : Success\r\nTransaction performed with:\r\n    Installed     rpm-4.11.3-48.el7_9.x86_64                      @updates\r\n    Installed     yum-3.4.3-168.el7.centos.noarch                 @base\r\n    Installed     yum-metadata-parser-1.1.4-10.el7.x86_64         @anaconda\r\n    Installed     yum-plugin-fastestmirror-1.1.31-54.el7_8.noarch @updates\r\nPackages Altered:\r\n    Updated docker-ce-3:25.0.0-1.el7.x86_64               @plesk-ext-docker\r\n    Update            3:25.0.1-1.el7.x86_64               @plesk-ext-docker\r\n    Updated docker-ce-cli-1:25.0.0-1.el7.x86_64           @plesk-ext-docker\r\n    Update                1:25.0.1-1.el7.x86_64           @plesk-ext-docker\r\n    Updated docker-ce-rootless-extras-25.0.0-1.el7.x86_64 @plesk-ext-docker\r\n    Update                            25.0.1-1.el7.x86_64 @plesk-ext-docker\r\n    Updated docker-compose-plugin-2.24.1-1.el7.x86_64     @plesk-ext-docker\r\n    Update                        2.24.2-1.el7.x86_64     @plesk-ext-docker\r\nhistory info\r\n```\r\n\r\n```\r\nLoaded plugins: fastestmirror\r\nTransaction ID : 2362\r\nBegin time     : Sat Jan 20 03:13:20 2024\r\nBegin rpmdb    : 1258:aa696fc965824607ba376bcba2c42cedf14780cc\r\nEnd time       :            03:17:44 2024 (264 seconds)\r\nEnd rpmdb      : 1258:38f5b6c8b8b47fbdb03d19a8d62799c88e36c680\r\nUser           : System <unset>\r\nReturn-Code    : Success\r\nTransaction performed with:\r\n    Installed     rpm-4.11.3-48.el7_9.x86_64                      @updates\r\n    Installed     yum-3.4.3-168.el7.centos.noarch                 @base\r\n    Installed     yum-metadata-parser-1.1.4-10.el7.x86_64         @anaconda\r\n    Installed     yum-plugin-fastestmirror-1.1.31-54.el7_8.noarch @updates\r\nPackages Altered:\r\n    Updated docker-buildx-plugin-0.11.2-1.el7.x86_64      @plesk-ext-docker\r\n    Update                       0.12.1-1.el7.x86_64      @plesk-ext-docker\r\n    Updated docker-ce-3:24.0.7-1.el7.x86_64               @plesk-ext-docker\r\n    Update            3:25.0.0-1.el7.x86_64               @plesk-ext-docker\r\n    Updated docker-ce-cli-1:24.0.7-1.el7.x86_64           @plesk-ext-docker\r\n    Update                1:25.0.0-1.el7.x86_64           @plesk-ext-docker\r\n    Updated docker-ce-rootless-extras-24.0.7-1.el7.x86_64 @plesk-ext-docker\r\n    Update                            25.0.0-1.el7.x86_64 @plesk-ext-docker\r\n    Updated docker-compose-plugin-2.21.0-1.el7.x86_64     @plesk-ext-docker\r\n    Update                        2.24.1-1.el7.x86_64     @plesk-ext-docker\r\nhistory info\r\n```\r\n\r\nHappy to provide more info if you let me know what you need.","comments":["It's probable worth mentioning that I shut down some non-essential Docker services following the initial issue on Saturday morning (between the first Docker upgrade and my implementing resource limits), and these were all redeployed when the resource limits were added that evening, so someting there could possibly be causing the initial ramp, perhaps.\r\n\r\nYou can also see that the System Load (& also the CPU, not on the graph) remained low following the reboot, but all the Docker services were definitelty running during most of this time.\r\n\r\nAny suggestions on how we can determine the cause of the System Load would be appreciated as well.","Is the high CPU load coming directly from the `dockerd` process or the service containers?","@vvoland I couldn't check at the time.  `htop` was just showing a blank screen and hanging, and this time around I couldn't ssh in at all.\r\n\r\nAre there other tools to view the CPU load, other than htop?","You could try to have high priority shell waiting in background or use some simple high priority snmp exporters for system stats or even start simple script (again high priority) to start collecting data on host to say just some local file you can retrieve later.\r\n\r\nOne of these usually works with stuff that might lock you out and are rather simple to set up. Well, bit more complicated than just running a command but there's a lot of info available.\r\n\r\nBasically straightforward workaround to get some CPU time is to simply adjust priority of your\/other processes."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/25.0"]},{"title":"Pulling from insecure registry failed when docker build","body":"### Description\n\nI have a local insecure registry listen on port 80. And I added the `--insecure-registry=192.168.1.100` argument.\r\nWhen I do `docker pull 192.168.1.100\/my\/image` it works fine.\r\nBut when I used this image in Dockerfile: `FROM 192.168.1.100\/my\/image` and run docker build, it fails with\r\n```\r\nERROR: failed to solve: 192.168.1.100\/my\/image: failed to do request: Head \"https:\/\/192.168.1.100\/v2\/my\/image\/manifests\/latest\": dial tcp 192.168.1.100:443: connect: connection refused\r\n```\r\nthe error message said it tried to use https instead of http. \r\n\r\nAfter more tests I found if I change the address to `--insecure-registry=192.168.1.100:80`  the both `docker pull 192.168.1.100:80\/my\/image` and `docker build` of `FROM 192.168.1.100:80\/my\/image` works fine.\r\n\r\nMaybe there is a bug when no port is specified.\n\n### Reproduce\n\n1. make an insecure registry listen on port 80.\r\n2. docker pull works fine.\r\n3. docker build failed trying to connect to port 443.\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient:\r\n Version:           25.0.0\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        e758fe5\r\n Built:             Thu Jan 18 17:09:01 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.0\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       615dfdf\r\n  Built:            Thu Jan 18 17:10:34 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.12\r\n  GitCommit:        71909c1814c544ac47ab91d2e8b84718e517bb99\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    25.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/local\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/local\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 25.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 71909c1814c544ac47ab91d2e8b84718e517bb99\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-91-generic\r\n Operating System: Alpine Linux v3.19 (containerized)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 15.44GiB\r\n Name: builder\r\n ID: 83698aaf-14ce-4731-94f7-fcf8676b01b8\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  192.168.1.100:80\r\n  192.168.1.100\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\n```\n\n\n### Additional Info\n\n_No response_","comments":["I think this is currently expected (at least I recall prior discussions about this); some of this was related to certificates being stored with the registry _and port_ included in the storage locations, and to disambiguate  `host` being either `host:443` (TLS) or `host:80` (non-TLS).\r\n\r\nA quick search for some (somewhat) related tickets;\r\n\r\n- https:\/\/github.com\/distribution\/distribution\/issues\/2511\r\n- https:\/\/github.com\/moby\/moby\/issues\/36263\r\n- https:\/\/github.com\/moby\/moby\/issues\/33564\r\n\r\nIt's definitely not ideal, and the way insecure registries (as well as registry-mirrors) are configured are in dire need of an overhaul, but no work has been done yet in that area.","Thanks, @thaJeztah \r\n\r\nI know that if there's no `.` (or a` :port`) in the hostname, it causes ambiguity. \r\nBut for hostname with `.`  (or IP in my case), `docker pull` respect `--insecure-registry` settings but `docker build` does not.\r\nIn older versions, I can overcome this by adding `http:\/\/` prefix like `--insecure-registry=http:\/\/192.168.1.100`, it'll lead the buildkit to use http on registry `192.168.1.100`. But now it's not working anymore.\r\n\r\nIs there any other way to hint docker to use http not https even though I've told it the registry is insecure?","I just found https:\/\/github.com\/docker\/buildx\/issues\/2030\r\nIt seems that the bug is supposed to be fixed in 25.0 but still exists.","I have saw serveral reports for this issue. Any plan to fix it?"],"labels":["area\/distribution","kind\/question","status\/0-triage","version\/25.0"]},{"title":"Networking between containers in custom network breaks after time","body":"### Description\n\nContainers will be able to communicate, but at some point I start getting \"no route to host\" seemingly randomly.\r\n\r\nSometimes this will resolve itself, but most of the time I have to redeploy the containers or restart the system.\n\n### Reproduce\n\n1. docker-compose up -d\r\n\r\n```\r\nversion: '3.7'\r\n\r\nservices:\r\n  nginx:\r\n    restart: unless-stopped\r\n    build: ..\/nginx\r\n    ports:\r\n      - \"80:80\"\r\n      - \"443:443\"\r\n    volumes:\r\n      - static_files:\/var\/www\/html\r\n\r\n  django:\r\n    restart: unless-stopped\r\n    build: ..\/django\r\n    env_file:\r\n      - .env\r\n    volumes:\r\n      - static_files:\/static\r\n    ports:\r\n      - \"8000:8000\"\r\n    depends_on:\r\n      - crate\r\n      - redis\r\n\r\n  redis:\r\n    restart: unless-stopped\r\n    image: redis:7.2.4\r\n    ports:\r\n      - \"6379:6379\"\r\n    command: redis-server --save \"\" --appendonly no\r\n\r\n  crate:\r\n    restart: unless-stopped\r\n    image: crate:5.4.7\r\n    environment:\r\n      - CRATE_HEAP_SIZE=8192m\r\n    ports:\r\n      - \"4200:4200\"\r\n      - \"5442:5432\"\r\n    volumes:\r\n      - crate_data:\/data\r\n    command: crate -Cnetwork.host=_site_ -Cpath.repo=\/data\/backup\r\n\r\n  events:\r\n    restart: unless-stopped\r\n    build: ..\/events\r\n    ports:\r\n      - \"8081:8081\"\r\n    env_file:\r\n      - .env\r\n    volumes:\r\n      - \/home\/ubuntu\/sql_logs:\/debug\r\n\r\n  postgres:\r\n    image: postgres:16-bullseye\r\n    volumes:\r\n      - pg_data:\/var\/lib\/postgresql\/data\r\n\r\nvolumes:\r\n  crate_data:\r\n  static_files:\r\n  pg_data:\r\n```\n\n### Expected behavior\n\nNetwork is reliable\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.1\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        29cf629\r\n Built:             Tue Jan 23 23:09:55 2024\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.1\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       71fa3ab\r\n  Built:            Tue Jan 23 23:09:55 2024\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.1\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 32\r\n Server Version: 25.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-1018-aws\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 2\r\n Total Memory: 3.746GiB\r\n Name: ip-172-30-30-160\r\n ID: GMOE:YHEH:24T3:2O6J:XF44:XZNY:OMFP:7RQC:7ALL:7633:PJ2O:HX2Y\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nThis is running on Ubuntu 22.04 LTS using kernel 6.2.0-1018-aws on ARM64","comments":["Hi @robd003 - thank you for reporting ...\r\n\r\nNo ideas about what could be happening at the moment. Would it be possible to boil it down to a minimal reproducible example, using standard images, so that we can take a look?\r\n\r\nHow long does it run for before you see the problem, and then how long does it take to recover if it's going to?\r\n\r\nDo the issues correspond with anything unusual in container or system logs?\r\n\r\nIs it all running in the background, or as a compose foreground process?","Hi,\r\nWe have experienced similar symptoms that might be related to the same root cause. Hope this will help to get closer to understand what happens.\r\n\r\nOur environment:\r\nCentos 7 fully updated to latest patches. One ethernet connection on the host with a simple IPv4 address.\r\nWe have multiple local scoped bridge networks. The different nets are used to communicate between containers on the host (like access to a db container), finally we publish frontend facing services to fixed ports on the host network.\r\nAfter the update to v25 there were connection errors to published ports from otside, randomly giving \"No route to host\" errors on TCP connect.\r\n\r\nDuring troubleshooting I was able to reproduce the problem inside the docker host. To reproduce the problem I started to connect from the host having IP 192.168.1.1 to a local published port via `telnet 192.168.1.1 8080` I was able to reproduce the problem, sometimes the connection was established normally, sometimes the there was random delay, and sometimes got the \"No route to host\" error. I have tested `telnet localhost 8080` as well, in that case it was working reliably.\r\nThe problem appeared on multiple host with many containers images and ports.\r\n\r\nI also took a packet trace on eth0 during test connection from outside, there I saw that sometimes there were retries sent from the client and the connection proceeded later on, other times the initial SYN pack and it's retries were unanswered for some time and after that an ICMP \"Destination unreachable\" was sent out of the docker host.\r\n\r\nWe have not have problems like this before the update and undoing the update fixed the problem in all cases.\r\n\r\nHope this helps.","Thank you @remetegabor ... I've had a go at reproducing, with a few alpine containers pinging each other on different networks, and a couple of nginx containers with published ports - but, no luck.\r\n\r\nIt'd be really good to have an example config based on standard images that reproduces the problem for you. Then we'll be able to investigate.\r\n\r\n(Also, could you confirm you're using 25.0.1? And, how long it takes to see the problem, or how frequently it's happening?)","I experienced the same problem on 4 different hosts when upgrading to 25.0.1 yesterday. 2x Debian 12 (aarch64) and 2x Ubuntu 22.04 (amd64). A few minutes after updating, containers started having trouble reaching each other. They recovered by themselves, but had trouble again a few minutes later and this repeated.\r\n\r\nThe problems disappeared for me after restarting the host, or after restarting all containers (using `docker compose restart`). Hope this information is of some use here!","@robmry Is there anything I can do to \"snapshot\" the state when the issue occurs?\r\n\r\nWould a docker inspect of every container be useful? iptables-save output?\r\n\r\nThe issue occurs between all containers. This is most easily observed with the Django container since it's connecting to redis & cratedb, but I've also seen it with nginx connecting to django.\r\n\r\nThe most annoying part is that it's intermittent.\r\n\r\nThese issues started with the v25.0.0 release, before that everything was working perfectly. Could it be an issue with `docker-proxy`?","@robd003 There're a few things that could help us better understand what's going on:\r\n\r\n- Turn on debug logs as per documented here: https:\/\/docs.docker.com\/config\/daemon\/logs\/#enable-debugging, and provide any logs emitted around the time the bug starts ;\r\n- The output of both `iptables-save` and `conntrack -L` when this occurs ;\r\n- You can also try running `conntrack -E` to see live conntrack events, until the bug appears.\r\n- The output of `docker network inspect` where this bug happens (ie. needs to match the other item asked here), and the IP addresses of affected containers on that network.\r\n- And also, did you update any other software around the same time as the Engine? (eg. kernel, systemd, etc...)","@robmry my test were done on 25.0.0. Unfortunately today i won't be able to put together a minimal repro env. I will try to make it on Monday. \r\n\r\nIn the mean time based on the other reports and what I saw I think there is no such thing as go wrong and recover on the engine, kernel or network stack side. Packets regarding tcp sessions are dropped randomly, and this makes higher level apps to fail and recover.\r\nThe simple test to make connections and close immediately resulting delays and connection failures randomly suggest this to be the root cause.\r\n\r\nI will try to collect logs requested by @akerouanton.","I can confirm the behaviour on our side as well. Our Docker environment experiences random networking issues since upgrading to 25.0.0, later to 25.0.1. The problem even remains even after downgrading to 24.0.7. We restarted docker service with no effect, and restartet the whole machine and recreated the containers, still no changes.\r\n\r\nThe server is a slightly larger installation, a 72GB VM running on Ubuntu 22.04.3 LTS x86_64 with Kernel 5.15.0-92-generic, the physical base system is running on Proxmox VE 8.1.4 x86_64.\r\n\r\ndocker info\r\n```\r\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 65\r\n  Running: 63\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 70\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-92-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 20\r\n Total Memory: 70.67GiB\r\n Name: vm1\r\n ID: OSMC:FQFW:242B:3UQE:QNZY:U5L2:EQQB:D35Y:IR3M:LFPN:PCEW:KSIN\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\nOn this machine there's a public facing nginx container that proxies requests to ~25 different applications, all running as docker container. Mostly everything seems up and running, however, rather randomly the nginx service is not able to connect to the internal application \r\n\r\n> connect() failed (113: Host is unreachable) while connecting to upstream, client: 47.128.113.xxx, server: vaultwarden\r\n\r\nHere's the example of our nginx docker-compose file:\r\n\r\n```\r\nversion: \"3\"\r\nservices:\r\n  nginx-proxy:\r\n    container_name: nginx-proxy\r\n    image: nginx:latest\r\n    networks:\r\n      - proxy-network\r\n    ports:\r\n      - 443:443\/tcp\r\n      - 80:80\/tcp\r\n    restart: always\r\n    volumes:\r\n      - <internal volumes>\r\nnetworks:\r\n  proxy-network:\r\n    external: true\r\n```\r\n\r\nAnd this an example of one of the internal applications that are connected from the nginx service above:\r\n```\r\nversion: \"3.6\"\r\nservices:\r\n  vaultwarden:\r\n    container_name: \"vaultwarden\"\r\n    environment:\r\n      - \"ADMIN_TOKEN=xxx\"\r\n    image: \"vaultwarden\/server:latest\"\r\n    networks:\r\n      - \"proxy-network\"\r\n    restart: \"unless-stopped\"\r\n    volumes:\r\n      - \"vaultwarden-data:\/data\"\r\nnetworks:\r\n  proxy-network:\r\n    external: true\r\n    name: \"proxy-network\"\r\nvolumes:\r\n  vaultwarden-data:\r\n    external: true\r\n```\r\n\r\nBtw: Vaultwarden is a fork of Bitwarden, which mentions the same issue in their own community forum:\r\nhttps:\/\/community.bitwarden.com\/t\/docker-v25-networking-issues-self-hosted-only\/62633\r\n\r\n\r\n\r\nI hope this helps to track down the issue fast.","Thanks @dkatheininger for the details. Unfortunately I fear we can't do much without the iptables and conntrack dumps, and docker debug logs I requested here: https:\/\/github.com\/moby\/moby\/issues\/47211#issuecomment-1910886922. That'd be great if you can provide that in addition.","Thank you @akerouanton for your quick response. We'll need some time to provide iptables and conntrack output.\r\nAs suggested in your comment we produced also `docker network inspect proxy-network` and analysed the output.\r\nWe found duplicate MAC-Addresses which appears to be odd. Here's an example:\r\n\r\n```\r\n[\r\n    {\r\n        \"Name\": \"proxy-network\",\r\n        \"Id\": \"521df80402ad53ea66e38c1a19863ed841cc0e46b559f58d1bf5f057e03a1e6f\",\r\n        \"Created\": \"2022-12-13T09:38:48.247757267Z\",\r\n        \"Scope\": \"local\",\r\n        \"Driver\": \"bridge\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"172.19.0.0\/16\",\r\n                    \"Gateway\": \"172.19.0.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": false,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {\r\n            ...\r\n            \"739edae05c7a76db8b1d75b5b1e2837ee435a3af7a3d8d19fa2e66da9bc91507\": {\r\n                \"Name\": \"container16\",\r\n                \"EndpointID\": \"5bcdbd8b7c3f2edea430646fb7218b2da0bca8865b32a03fde0c31d617109bcb\",\r\n                \"MacAddress\": \"02:42:ac:13:00:18\",\r\n                \"IPv4Address\": \"172.19.0.24\/16\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n\r\n            ...\r\n\r\n            \"a390a500c776ad011898bf0e6a662ba3e2caffc8e535ae7e4c70c1a4648d07aa\": {\r\n                \"Name\": \"container27\",\r\n                \"EndpointID\": \"b7f3d2a4688dbafa35b27bc9a950542c6588688a860890472b803b2486c99bc8\",\r\n                \"MacAddress\": \"02:42:ac:13:00:18\",\r\n                \"IPv4Address\": \"172.19.0.18\/16\",\r\n                \"IPv6Address\": \"\"\r\n            }\r\n            ...\r\n        },\r\n        \"Options\": {},\r\n        \"Labels\": {}\r\n    }\r\n...\r\n```\r\n\r\nWe even found mutiple duplicates. When we recreated the affected containers so that the received unique MAC addresses the problems seemed to disappear.","@akerouanton another observation we made:\r\nwe are running 5 servers, all of them with Docker 25.0.1, same nginx \/ backend application setup as described above. But only two of them showed the networking issue. The affected systems are running Ubuntu 22.04.3 LTS, the other ones (that weren'r affected) are running Debian 12 (bookworm).","Thank you @dkatheininger ... I think the duplicate mac addresses you found were the clue we needed.\r\n\r\nThere was a bug in 25.0.0 (https:\/\/github.com\/moby\/moby\/pull\/45905) that could result in duplicate MAC addresses when one container was stopped, another started, then the first was restarted. We thought we'd fixed it in 25.0.1 (https:\/\/github.com\/moby\/moby\/pull\/47168) ... but duplicate MAC addresses created by 25.0.0 are retained when the containers are started with 25.0.1.\r\n\r\nI think the only workaround for that is to re-create the containers with 25.0.1, which shouldn't generate duplicate addresses.\r\n\r\nBecause of the way the configuration and running-state of the container are stored - we may not be able to remove the need for that re-creation once duplicate MAC addresses have been generated.\r\n\r\nAnother problem is that 25.0.1 doesn't respect a `--mac-address` option over a container restart.\r\n\r\nSo - still investigating, but it would be good to know if this description fits with what's been observed, and whether re-creating the containers solves the problem in all of these cases.","@robmry we can confirm that your description of the issue and the mitigation worked. Recreating the affected containers with 25.0.1 helped to prevent the duplicate MAC addresses","Hi @dkatheininger ... thank you for confirming."],"labels":["status\/0-triage","kind\/bug","area\/networking","version\/25.0"]},{"title":"Service level network name (default alias)","body":"### Description\n\nI would like to have the possibility to overwrite the default alias of a service on network level to avoid duplicates in external networks.\r\n\r\n\r\nThis could look like:\r\n```\r\nservices:\r\n  db:\r\n    networks:\r\n      some-network:\r\n        name: my-db-to-override-db-default-alias\r\n        aliases:\r\n          - db_123_213\r\n        \r\n```\r\nif you would inspect the service it should no longer be aliased as `db`\r\n```\r\n(...)\r\n\"Networks\": {\r\n  \"my_network\": {\r\n    \"Aliases\": [\r\n      \"8c2da47d1e8e\",\r\n      \"db_123_213\",\r\n      \"my-db-to-override-db-default-alias\"\r\n     ],\r\n  }\r\n}\r\n\r\n```\r\n\r\nOne use case would be if I have an external proxy network and several docker compose projects which each have a service X (e.g. app or pgsql). sometimes it would be useful to change the default alias for the external network to avoid a conflict.\r\n\r\nREF: https:\/\/forums.docker.com\/t\/feature-request-service-level-network-name-default-alias\/139498\/3","comments":[],"labels":["status\/0-triage","kind\/feature"]},{"title":"libnet: remove retry logic from a few data management code paths","body":"**- What I did**\r\n\r\nAll four commits have explanations about why they're safe to carry. But the common trait is that all retry logic was added to work around issues posed by either:\r\n\r\n1. Classic Swarm and the eventually consistent data store it supported ;\r\n2. Requests to the local datastore not being serialized, leading to consistency issues between the underlying cache and the on-disk state ;\r\n3. Because `datastore.GetObject()` method was (and still is) hydrating the `KVObject` parameter passed as argument instead of returning the object in the cache (if any). As a consequence, two instances of the same object could live in memory at the same. This would defeat the purpose of mutex, and one instance could lag behind the other (in terms of dbIndex, the version ID used for optimistic locking).\r\n\r\nWhile both issue 1. and 2. are a thing of the past, 3. remains. Fortuantely, after analyzing the code, it seems there's no case where that could happen -- making these changes safe.\r\n\r\n**- How I did it**\r\n\r\nBy carrying out a static analysis of the code.\r\n\r\n**- How to verify it**\r\n\r\nCI.\r\n\r\n","comments":["I'm converting to a draft for now as there are more occurences of that sort of logic and I'd like to get rid of them all at once.","Putting out of draft mode. @corhere PTAL.","This one needs a rebase","Untangling the `epCnt` knot is going to be tricky, and I think it deserves a dedicated PR. How about focusing on the comical `DeleteObjectAtomic` retry loops in this PR? They could all be replaced with idempotent datastore delete calls if we can prove that none of the callers depend on the side effect of refreshing the in-memory object from the store."],"labels":["status\/2-code-review","area\/networking","kind\/refactor"]},{"title":"Make sure buildkit worker is registered with host-gateway-ip label set","body":"- Alternative to https:\/\/github.com\/moby\/moby\/pull\/47187\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["We need to have a look which option is best (this PR or my other PR)\r\n\r\nI was thinking about this approach, but then considered that the possible downside could be that the daemon config changes at runtime later on. Also tried to keep the diff small \/ limited.\r\n\r\nWe could (possibly) have a look at updating the buildkit-code to allow for things like `reload` of the daemon config (and maybe request the IP-address \u201cwhen needed\u201d)"],"labels":["area\/builder","status\/2-code-review","area\/networking","area\/daemon","kind\/bugfix","area\/builder\/buildkit"]},{"title":"c8d: ImageService.GetImage: fix filtering by platform","body":"- extracting from https:\/\/github.com\/moby\/moby\/pull\/47114\r\n- see https:\/\/github.com\/moby\/moby\/pull\/47114#discussion_r1458826051\r\n- also related: https:\/\/github.com\/moby\/moby\/pull\/47167\r\n\r\n\r\nBefore this patch, it would find all images (and platform) are found, and return the first image (or the old default: \"linux\/amd64\");\r\n\r\nWith this patch, an error is returned if a platform is set, and no results were found:\r\n\r\n    No such image: alpine:latest for platform: windows\/s390x\r\n\r\nLooking at consumers of this function, there may be something to fix though; https:\/\/github.com\/moby\/moby\/blob\/178651733852a26bc11e6fa272b48ff37ecc7447\/daemon\/create.go#L85-L86\r\n\r\n    if opts.params.Platform == nil && opts.params.Config.Image != \"\" {\r\n        img, err := daemon.imageService.GetImage(ctx, opts.params.Config.Image, imagetypes.GetImageOpts{Platform: opts.params.Platform})\r\n\r\nThe code above looks to check for _NO_ platform to be passed in parameters, but then to pass that platform as `imagetypes.GetImageOpts.Platform`.\r\n\r\nThat code was added in 300c11c7c9143ca077890050deb83917e67fe250 but that was a refactor, combining multiple branches, so before that we passed platform unconditionally (but only used the image in some cases, so that was an optimization).\r\n\r\nShould we;\r\n\r\n- always lookup the image?\r\n- should `GetImage` use OnlyPlatformWithFallback as default (if no platform is specified)? To select any image, but _prefer_ local platform?\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/2-code-review","area\/images","kind\/bugfix"]},{"title":"macvlan, ipvlan: generate unsolicited ARP\/ND advertisements on link up?","body":"### Description\n\n- Related to #42746\r\n\r\nWe should get the kernel to send unsolicited IPv4 ARP and IPv6 Neighbor Discovery advertisements when bringing up macvlan and ipvlan container links. That would allow (at least) L2 switches to eagerly update their forwarding tables when a container link's MAC address changes, or a container with a static MAC address is rescheduled onto a node connected to a different switch port. Otherwise the containers will be unreachable from the network until the stale forwarding entries expire.\r\n\r\n(The following is all speculation as the user has not yet responded.) I suspect that the author of the linked PR is using either the macvlan or ipvlan driver, tried setting the `net.ipv6.conf.all.ndisc_notify` sysctl in the container config and found it to be effective only when the interface's IPv6 address is added with the optimistic DAD option. Optimistic DAD would have worked around the issue of user-specified container sysctls getting applied after `libnetwork-setkey` by delaying the sending of unsolicited ND advertisements until after the container runtime sets `ndisc_notify`. The kernel sends an unsolicited ND advertisement when `ndisc_notify` is enabled and an address becomes no longer tentative, which happens after duplicate address detection completes. Optimistic DAD likely delays things long enough that the container runtime can win the race to set `ndisc_notify` before the kernel sets the address as not-tentative most of the time.\r\n\r\nWith #47062 changing the order of operations such that user sysctls are applied before any network links are brought up, users would be able to opt in using `docker run --sysctl net.ipv6.default.ndisc_notify=1` without any workarounds or hacks. However, I posit that users should not have to opt in; `net.ipv4.conf.<iface>.arp_notify=1` and `net.ipv6.conf.<iface>.ndisc_notify=1` should be unconditionally set by libnetwork. I cannot think of any downside. Either the unsolicited advertisements are accepted by peers and a restarted\/moved\/rescheduled container's networking Just Works, or they are ignored and nothing happens differently from today.\r\n\r\n(cc @psaab)","comments":[],"labels":["kind\/enhancement","area\/networking\/d\/ipvlan","area\/networking\/d\/macvlan"]},{"title":"testing: runconfig: add new fixtures to TestDecodeContainerConfig","body":"### Description\n\n- relates to https:\/\/github.com\/moby\/moby\/pull\/47155\r\n\r\n\r\nThe last update to the fixtures looks to have been in https:\/\/github.com\/moby\/moby\/pull\/16433, many years ago; we should add some more recent ones.","comments":[],"labels":["kind\/enhancement","area\/testing"]},{"title":"Mounting read-only \/dev behaviour changed in v25","body":"### Description\r\n\r\nSince Docker upgrade to v25 the sub-filesystems in \/dev (in particular \/dev\/shm) get mounted read-only if the Docker container has a volume specified that mounts \/dev as read-only, which is a change from behaviour in v24.\r\n\r\nUpgrading from docker-ce version `5:24.0.7-1~ubuntu.22.04~jammy` to `5:25.0.0-1~ubuntu.22.04~jammy` (and containerd.io version `1.6.15-1` to `1.6.27-1`) causes the change in permissions of \/dev\/shm that breaks a lot of software, in out case Python multiprocessing module fails to create a new instance of https:\/\/docs.python.org\/3.10\/library\/multiprocessing.html#multiprocessing.Value as it is unable to create a file in \/dev\/shm\r\n\r\n### Reproduce\r\n\r\n1. docker run --rm -d -v \/dev:\/dev:ro --name test123 ubuntu:jammy sleep 3600\r\n2. docker exec -it test123 mount | grep \/dev\/shm\r\n3. `tmpfs on \/dev\/shm type tmpfs (ro,nosuid,nodev,inode64)`\r\n \r\nOther file systems are also affected:\r\n```\r\ndevpts on \/dev\/pts type devpts (ro,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\r\ntmpfs on \/dev\/shm type tmpfs (ro,nosuid,nodev,inode64)\r\nmqueue on \/dev\/mqueue type mqueue (ro,nosuid,nodev,noexec,relatime)\r\nhugetlbfs on \/dev\/hugepages type hugetlbfs (ro,relatime,pagesize=2M)\r\n```\r\n\r\n### Expected behavior\r\n\r\ndocker exec -it test123 mount | grep \/dev\/shm\r\n`tmpfs on \/dev\/shm type tmpfs (rw,nosuid,nodev,inode64)`\r\n\r\n```\r\ndevpts on \/dev\/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\r\ntmpfs on \/dev\/shm type tmpfs (rw,nosuid,nodev,inode64)\r\nmqueue on \/dev\/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)\r\nhugetlbfs on \/dev\/hugepages type hugetlbfs (rw,relatime,pagesize=2M)\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           25.0.0\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        e758fe5\r\n Built:             Thu Jan 18 17:09:49 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.0\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       615dfdf\r\n  Built:            Thu Jan 18 17:09:49 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    25.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 23\r\n  Running: 22\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 45\r\n Server Version: 25.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-79-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 7.532GiB\r\n Name: xxx\r\n ID: WJDD:VNV2:5UUD:M5GY:AVR6:4GFI:WFDT:V66E:OPBY:CGB5:DQ3A:DTPM\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["v25 introduced support for recursive readonly mount and this is enabled by default.\r\n\r\nTo keep the old behavior on the 25.0+ version you should use a set the `bind-recursive` option to `writable`.\r\n\r\n```\r\ndocker run --rm -d \\\r\n    --mount type=bind,src=\/dev,dst=\/dev,readonly,bind-recursive=writable \\\r\n    -it --name test123 ubuntu:jammy sleep 3600\r\n```\r\n\r\nNote that this is not supported for the short `-v src:dst,ro` syntax and the long `--mount` syntax must be used instead.\r\n\r\nMore information:\r\n- https:\/\/github.com\/moby\/moby\/pull\/45278\r\n- https:\/\/github.com\/moby\/moby\/pull\/46037\r\n- https:\/\/docs.docker.com\/storage\/bind-mounts\/#recursive-mounts","Question: what workload needs `-v \/dev:\/dev:ro` ?","In our case that was used to give access to serial ports from the container. There were extra --device mappings that were granting write access to serial port devices, but base level read=-only access to the \/dev folder was needed as well because the udev rules that would be mapping devices to human-readable names would be creating symlinks in \/dev and not renaming devices.\r\n\r\nIt might be feasibly surmised that \/dev\/shm is a special case. That it should actually get a dedicated tmpfs mount *unless* the user explicitly mounts it from the host system (for inter-container shared memory usage).\r\n\r\nI see the benefits on the more predictable security front from the general change. It was just a very unexpected failure in a such a generic component.","> It might be feasibly surmised that \/dev\/shm is a special case. That it should actually get a dedicated tmpfs mount unless the user explicitly mounts it from the host system (for inter-container shared memory usage).\r\n\r\nHm.. yes. Curious; in your case (`\/dev:\/dev`) did it inherit `\/dev\/tmpfs` from the host, or does it get mounted over by a `tmpfs`  set for the container?\r\n\r\nI know we had some special logic to check for user-provided mounts overlapping with the default ones that are set for containers (but not sure from the top of my head if we take \"all of `\/dev` is a user-mount\" into account (I'd have to check the code).\r\n\r\n","When \/dev is mounted into the container (either ro or rw) the \/dev\/shm is inherited from the host.","Even specifying --shm-size parameter does not override this - you still get the \/dev\/shm from host, mounted read-only","I would expect that quite a few people would end up with \/dev:\/dev:ro volume mount in their configs over the years after trying all kinds of approaches to giving their containers access to various hardware devices and finally finding a configuration that works for them, then adding \":ro\" as it made them feel more secure and it did not break anything. The *actual* access might be going via --device or --gpus even via --privileged , but they'd still have that line in their config as \"it doesn't hurt anything\".","> I see the benefits on the more predictable security front from the general change. It was just a very unexpected failure in a such a generic component.\r\n\r\nFor transparency; there was some back-and-forth on that topic. The intent for `:ro` was always meant to be \"fully read-only\", and there's definitely been cases where users were caught off-guard discovering that it _wasn't_ (some nested mounts still writable). We had to make the trade-off between keeping the old (limited; could be _partially_ writable) behavior, knowing it has been a bit of a footgun to users, or make `:ro` align more with the original intent (read-only), knowing that there could be some advanced use-cases where sub-mounts are _intentionally_ writable.\r\n\r\nWe ultimately decided to align with the most common use-case and most common _expectation_, but to provide more granular control in the advanced (`--mount`) syntax to allow for more advanced \/ specific use-cases. We also took into account that there's numerous examples that use `:ro`; having to force users to rewrite all their uses to (e.g.) `:rro` to get _actual_ read-only was something we tried to avoid.\r\n\r\n- `:ro` (and `--mount readonly=true`) will be read-only on a \"best effort\" base; for current kernels that's recursively read-only. On older kernels, that's the previous read-only (knowing that older kernels will eventually phase out). \r\n- the `--mount` syntax adds additional options to _require_ recursive-readonly (MUST be fully read-only, otherwise error out (e.g. if the kernel doesn't support it))\r\n- the `--mount` syntax to have an option to explicitly disable read-only for nested mounts; for advanced use-cases where the user _opts in_ to have writable paths in the mount.\r\n- the `--mount` syntax to have an option to _disable_ nested mounts, for advanced use-cases where the source location contains mounts that must not be inherited in the container\r\n","> Even specifying --shm-size parameter does not override this - you still get the \/dev\/shm from host, mounted read-only\r\n\r\nAh, that's an interesting combination; wondering if we need to do something with that to make it clear we don't \"own\" `\/dev\/shm` in that case, hence cannot set it \ud83e\udd14 ","To me, personally, (after figuring out what \/dev\/shm actually is) it would have been least surprising if containers *always* got their own copy of \/dev\/shm, *unless* the user explicitly mounted something else into \/dev\/shm *directly*. Documenting that alongside the --shm-size option would be a natural fit. Along with option to set --shm-size to zero to disable \/dev\/shm overmounting.","Actually it looks like it's a special case where we explicitly ignore any mounts under `\/dev\/...` if user mounts `\/dev`\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/c87e0ad209929135a2264ad6d10e6012bcd312f6\/daemon\/oci_linux.go#L546-L559"],"labels":["kind\/question","status\/0-triage","version\/25.0"]},{"title":"api: remove support for legacy filter format (api < v1.22)","body":"- extracted from https:\/\/github.com\/moby\/moby\/pull\/47155, but it looks like docker-py uses the legacy format.\r\n\r\nrelated to\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/34756\r\n- https:\/\/github.com\/moby\/moby\/commit\/93d1dd8036d57f5cf1e5cbbbad875ae9a6fa6180 \/ https:\/\/github.com\/moby\/moby\/pull\/18266\r\n\r\n\r\nRemove tha fallback code for the legacy (api < v1.22) filter format.\r\n\r\nAPI v1.23 and older are deprecated, so we can remove support for plain-text error responses.\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/api","status\/2-code-review","impact\/deprecation"]},{"title":"[BUG] ALL containers which use NFS volumes do not start after reboot","body":"### Description\n\nThis is the ported Bug-Report from: https:\/\/github.com\/docker\/compose\/issues\/11354\r\n\r\n\r\nI am currently facing an issue with my Intel NUC running Debian SID that has persisted for about a year. Despite trying various solutions, I have been unable to resolve it satisfactorily.\r\n\r\nMy configuration is as follows:\r\n\r\n```\r\nNUC ==(NFS - docker-compose volume)==> SYNO\r\n```\r\n\r\nI run numerous containers within my docker-compose stack, all of which are set to restart with the `restart unless-stopped` policy. However, upon system reboot, all containers with NFS volumes mapped fail to start automatically. They remain inactive unless manually initiated. Interestingly, initiating a manual start or restart at any other time works seamlessly, and everything functions as expected.\r\n\r\nI anticipate that all containers should initiate during a system reboot, and suspect there may be an underlying hidden race condition that eludes my detection.\r\n\r\nAlso it seems this is the very same issue as the issues mentioned here:\r\n1. [LINK1](https:\/\/forums.docker.com\/t\/containers-with-nfs-volumes-wont-start-after-reboot\/137604\/5)\r\n2. [LINK2](https:\/\/forums.docker.com\/t\/docker-compose-up-stuck-on-starting-when-using-nfs-volume\/137374\/2)\r\n3. [LINK3](https:\/\/www.reddit.com\/r\/docker\/comments\/16ez2bb\/containers_with_nfs_volumes_wont_start_after\/)\r\n4. [LINK4](https:\/\/github.com\/docker\/compose\/issues\/11354)\n\n### Reproduce\n\n1. use docker-compose (or `docker cli` as proven in the old Bug-Report https:\/\/github.com\/docker\/compose\/issues\/11354#issuecomment-1902235822)\r\n2. configure any container\r\n3. configure a NFS Volume like this:\r\n```\r\nvolumes:\r\n\r\n  share:\r\n    name: share\r\n    driver_opts:\r\n      type: \"nfs\"\r\n      o: \"addr=192.168.178.2,nfsvers=4\"\r\n      device: \":\/volume1\/NFS_SHARE\/\"\r\n```\r\n4. use the named NFS-Volume in the configured container\r\n5. do the restart test (`docker restart container_name`) after restart (`docker ps`)\r\n6. do the reboot test (`reboot`) after reboots (`docker ps`)\n\n### Expected behavior\n\nThere shall not be any race-condition and all containers (also the ones having NFS-based volumes mounted to) shall restart.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           25.0.0\r\n API version:       1.44\r\n Go version:        go1.21.6\r\n Git commit:        e758fe5\r\n Built:             Thu Jan 18 17:09:59 2024\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.0\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       615dfdf\r\n  Built:            Thu Jan 18 17:09:59 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    25.0.0\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.24.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 25.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.6.11-amd64\r\n Operating System: Debian GNU\/Linux trixie\/sid\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 30.88GiB\r\n Name: h0tmann\r\n ID: 670157fc-30a9-4aa4-807c-4fa28aec7ec7\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n1. [LINK1](https:\/\/forums.docker.com\/t\/containers-with-nfs-volumes-wont-start-after-reboot\/137604\/5)\r\n2. [LINK2](https:\/\/forums.docker.com\/t\/docker-compose-up-stuck-on-starting-when-using-nfs-volume\/137374\/2)\r\n3. [LINK3](https:\/\/www.reddit.com\/r\/docker\/comments\/16ez2bb\/containers_with_nfs_volumes_wont_start_after\/)\r\n4. [LINK4](https:\/\/github.com\/docker\/compose\/issues\/11354)","comments":["Are these NFS devices available before the docker service is started? I wonder if the systemd unit needs a custom \"After\" condition added \ud83e\udd14","Yes the SYNO runs 24\/7 and is not rebooted at all.\r\n\r\nThats what I read somewhere else aswell.\r\nIt waits for network, but somehow there is a race-condition when it comes to:\r\n\r\n- Network itself\r\n- NFS mount actually connected\r\n\r\nBut in this regards, I am not an expert, so do not qoute me :)","Hm, right, so looking at the default systemd unit for the docker service; https:\/\/github.com\/moby\/moby\/blob\/5a3a101af2ff6fae24605107b1fbcf53fbb5c38e\/contrib\/init\/systemd\/docker.service#L4-L5\r\n\r\nIt currently waits for;\r\n\r\n- [`network-online.target`](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.special.html#network-online.target) (networking)\r\n- `docker.socket` (the socket-activation service)\r\n- `firewalld.service` (firewalld)\r\n- `containerd.service` (containerd)\r\n- [`time-set.target`](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.special.html#time-set.target) (clock set)\r\n\r\n\r\nThe containerd service already has a [`local-fs.target`](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.special.html#local-fs.target) to make sure local filesystems are mounted; https:\/\/github.com\/containerd\/containerd\/blob\/b66830ff6f8375ce8c7a583eaa03549eaa6707c4\/containerd.service#L18\r\n\r\nWhich means that (because the docker service has `After=containerd.service`), local filesystems at least _should be_ mounted.\r\n\r\nI think what's needed in your setup is to have the [`remote-fs.target`](https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.special.html#remote-fs.target);\r\n\r\n> **remote-fs.target**\r\n> \r\n> Similar to `local-fs.target`, but for remote mount points.\r\n>\r\n> systemd automatically adds dependencies of type After= for this target unit to all SysV init script service units with an LSB header referring to the \"$remote_fs\" facility.\r\n\r\nGiven that remove filesystems are not something that's used by default by the Docker Engine, I don't think we should add this to the default systemd unit; doing so likely would delay startup of the service, which would be a regression for setups that don't use remove filesystems, but are running on a system that _does_ have them (but perhaps can be discussed).\r\n\r\nTo add that target, you can use `systemctl edit docker.service`. This will create an override file that allows you to extend or override properties of the default systemd unit (Flatcar has a great page on [describing this in more depth](https:\/\/www.flatcar.org\/docs\/latest\/setup\/systemd\/drop-in-units\/));\r\n\r\n```bash\r\nsudo systemctl edit docker.service\r\n```\r\n\r\nThat command will create a systemd \"override\" (or \"drop-in\") file, and open it in your default editor. You can add your overrides in the file, and save it. By default, the `After` you specify in your override file is _appended_ to the existing list of options in the `After` of the default systemd unit (which should not be edited).\r\n\r\n```bash\r\n## Editing \/etc\/systemd\/system\/docker.service.d\/override.conf\r\n### Anything between here and the comment below will become the new contents of the file\r\n[Unit]\r\nAfter=remote-fs.target\r\n\r\n\r\n### Lines below this comment will be discarded\r\n\r\n### \/lib\/systemd\/system\/docker.service\r\n# [Unit]\r\n# ...\r\n```\r\n\r\nAfter you edited and saved, you need to reload systemd to make it re-read the configuration;\r\n\r\n```bash\r\nsudo systemctl daemon-reload\r\n```\r\n\r\nYou can check the new settings using `systemctl show`, which should now show the `remote-fs.target` included;\r\n\r\n```bash\r\nsudo systemctl show docker.service | grep ^After\r\nAfter=containerd.service systemd-journald.socket docker.socket sysinit.target time-set.target network-online.target remote-fs.target system.slice firewalld.service basic.target\r\n```","Thanks for the detailed explanation.\r\n\r\nI have followed your commands. Here is the check-command:\r\n\r\n```\r\n$ systemctl show docker.service | grep ^After\r\nAfter=network-online.target basic.target sysinit.target firewalld.service docker.socket system.slice time-set.target containerd.service remote-fs.target systemd-journald.socket\r\n```\r\n\r\nI also reloaded the systemd daemon (this should automatically be done by reboot - but did it manually anyway) and rebooted the server.\r\n\r\nAgain all containers with NFS Volumes are down. They do not start up again.\r\n\r\n> Given that remove filesystems are not something that's used by default by the Docker Engine, I don't think we should add this to the default systemd unit; doing so likely would delay startup of the service, which would be a regression for setups that don't use remove filesystems, but are running on a system that does have them (but perhaps can be discussed).\r\n\r\nIs there a possibility to add this just if NFS (any remoteFS) is getting used anywhere in any docker container?\r\n\r\nBut like mentioned above, this apparently did not fix the issue. Thanks for your help :)","> But like mentioned above, this apparently did not fix the issue.\r\n\r\n\ud83d\ude22 that's a shame; thanks for trying! I was hoping this would make sure that those remote filesystem mounts were up-and-running.\r\n\r\n_Possibly_ it requires a stronger dependency defined; more than `After` \ud83e\udd14 \r\n\r\nReading the documentation for `After` https:\/\/www.freedesktop.org\/software\/systemd\/man\/latest\/systemd.unit.html#Before=\r\n\r\n> Note that those settings are independent of and orthogonal to the requirement dependencies as configured by `Requires=`, `Wants=`, `Requisite=`, or `BindsTo=`.\r\n> \r\n> It is a common pattern to include a unit name in both the `After=` and `Wants=` options, in which case the unit listed will be started before the unit that is configured with these options.\r\n\r\nPerhaps that second line applies here; might be worth trying if adding `remote-fs.service` to `Wants` helps \ud83e\udd14 \r\n\r\n\r\n","I did the following:\r\n\r\n1. `systemctl edit docker.service`\r\n2. added `remote-fs.service` also to `Wants`:\r\n\r\n```\r\n### Editing \/etc\/systemd\/system\/docker.service.d\/override.conf\r\n### Anything between here and the comment below will become the contents of the drop-in file\r\n\r\n[Unit]\r\nAfter=remote-fs.target\r\nWants=remote-fs.target\r\n\r\n### Edits below this comment will be discarded\r\n```\r\n3. `systemctl daemon-reload`\r\n4. `systemctl show docker.service | grep ^After`:\r\n```\r\nAfter=docker.socket network-online.target system.slice sysinit.target containerd.service systemd-journald.socket remote-fs.target basic.target firewalld.service time-set.target\r\n```\r\n5. `systemctl show docker.service | grep ^Wants`\r\n```\r\nWants=network-online.target remote-fs.target containerd.service\r\n```\r\n6. `reboot`\r\n\r\nStill - the containers with NFS Volumes do not start automatically.","@thaJeztah are there any news, or is there a specific user to tag on this one?\r\n\r\nThanks in advance! :)","Is there any related error message in dockerd log?\r\n\r\n```bash\r\nsudo journalctl -e --no-pager  -u docker -g ' error while mounting volume '\r\n# or if the above doesn't yield anything useful, try searching for the NFS server address you use\r\nsudo journalctl -e --no-pager  -u docker -g '192.168.178.2'\r\n```","@vvoland thanks - I will reply, once I am at home and executed the commands.","@vvoland thanks, the search for the IP itself returned something:\r\n\r\n```\r\nJan 25 18:41:19 hostname dockerd[705]: time=\"2024-01-25T18:41:19.637798586+01:00\" level=error msg=\"failed to start container\" container=822210342a705a345accd6bfa16b69507b832bf01aec77ea4439f4b6d375c390 error=\"error while mounting volume '\/var\/lib\/docker\/volumes\/share\/_data': failed to mount local volume: mount :\/volume1\/NFS_SHARE\/:\/var\/lib\/docker\/volumes\/share\/_data, data: addr=192.168.178.2,nfsvers=4,hard,timeo=600,retrans=3: network is unreachable\"\r\n```\r\n\r\nbasically `network is unreachable`, but yet it is a standard debian installation, following the official docs: https:\/\/docs.docker.com\/engine\/install\/debian\/\r\n\r\nand this is my systemctl file:\r\n\r\n```\r\n# \/usr\/lib\/systemd\/system\/docker.service\r\n[Unit]\r\nDescription=Docker Application Container Engine\r\nDocumentation=https:\/\/docs.docker.com\r\nAfter=network-online.target docker.socket firewalld.service containerd.service time-set.target\r\nWants=network-online.target containerd.service\r\nRequires=docker.socket\r\n\r\n[Service]\r\nType=notify\r\n# the default is not to use systemd for cgroups because the delegate issues still\r\n# exists and systemd currently does not support the cgroup feature set required\r\n# for containers run by docker\r\nExecStart=\/usr\/bin\/dockerd -H fd:\/\/ --containerd=\/run\/containerd\/containerd.sock\r\nExecReload=\/bin\/kill -s HUP $MAINPID\r\nTimeoutStartSec=0\r\nRestartSec=2\r\nRestart=always\r\n\r\n# Note that StartLimit* options were moved from \"Service\" to \"Unit\" in systemd 229.\r\n# Both the old, and new location are accepted by systemd 229 and up, so using the old location\r\n# to make them work for either version of systemd.\r\nStartLimitBurst=3\r\n\r\n# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.\r\n# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make\r\n# this option work for either version of systemd.\r\nStartLimitInterval=60s\r\n\r\n# Having non-zero Limit*s causes performance problems due to accounting overhead\r\n# in the kernel. We recommend using cgroups to do container-local accounting.\r\nLimitNPROC=infinity\r\nLimitCORE=infinity\r\n\r\n# Comment TasksMax if your systemd version does not support it.\r\n# Only systemd 226 and above support this option.\r\nTasksMax=infinity\r\n\r\n# set delegate yes so that systemd does not reset the cgroups of docker containers\r\nDelegate=yes\r\n\r\n# kill only the docker process, not all processes in the cgroup\r\nKillMode=process\r\nOOMScoreAdjust=-500\r\n\r\n[Install]\r\nWantedBy=multi-user.target\r\n\r\n# \/etc\/systemd\/system\/docker.service.d\/override.conf\r\n[Unit]\r\nAfter=remote-fs.target\r\nWants=remote-fs.target\r\n```\r\n\r\nHope this helps debugging it :)","Just wanted to add:\r\n\r\nthe very same also happens when using `SMB`\/`CIFS` mounts\/volumes.\r\nI assume this applies to all network-based mounts\/volumes.\r\n\r\nI also confirmed the very same on another server. Just to make sure, it is not because of any special config on my end.","Hm... so I just realised that my suggestion of using systemd for this would work if the _host_ had NFS mounts for these filesystems, but if the host does not have those, systemd would not be aware of them, so won't take them into account.\r\n\r\nDoes this work if your host has a mount from these server(s)? (also see https:\/\/geraldonit.com\/2023\/02\/25\/auto-mount-nfs-share-using-systemd\/)\r\n\r\n In that case, it's also worth considering setting up the NFS mount on the host, and instead of using an NFS mount for the _container_;\r\n\r\n```yaml\r\n    driver_opts:\r\n      type: \"nfs\"\r\n      o: \"addr=192.168.178.2,nfsvers=4\"\r\n      device: \":\/volume1\/NFS_SHARE\/\"\r\n```\r\n\r\nTo use a \"bind\" mount, using the NFS mounted path from the host. This could be a regular bind-mount, or a volume with the relevant mount-options set https:\/\/github.com\/moby\/moby\/issues\/19990#issuecomment-248955005, something like;\r\n\r\n\r\n```yaml\r\n    driver_opts:\r\n      type: \"none\"\r\n      o: \"bind\"\r\n      device: \"\/path\/to\/nfs-mount\/on-host\/\"\r\n```\r\n","> Does this work if your host has a mount from these server(s)?\r\n\r\nSorry I don't understand this question.\r\n\r\nBut here something I have tried before and it worked:\r\n\r\n1. setting up NFS-Mount to a volume `\/mnt\/NFS_MOUNT\/`  (with `\/etc\/fstab`)\r\n2. mapping it into the container just like any other folder.\r\n\r\nThis works, but this is not what I desire, since I want the mount and the whole connection also be transferable via docker compose etc.\r\n\r\nI feel like this volumes in docker do have a general\/structural problem of not waiting for the mount to be active.\r\nCan btw anyone of you confirm AND replicate this bug on your side?","Same problem here. In my case, Proxmox with a Debian VM (docker\/portainer) connecting to a Synology NAS over NFSv4. After reboot, 8 of 30 containers fail to start and unsurprisingly they're all the ones with NFS mounts. The containers spin right up when I click Start in portainer though.\r\n\r\nSo far I've tried setting different restart: settings and depends-on:, neither of which is working. I really don't want to touch the host, much rather get it working in Docker alone.\r\n\r\nStill scouring the internet for a solution :)","@thaJeztah @vvoland are there any news on this, or is there something I can do to help?"],"labels":["kind\/question","status\/0-triage","area\/systemd","area\/volumes","version\/25.0"]},{"title":"[WIP] Migrate to github.com\/containerd\/platforms module","body":"- [x] depends on \/ stacked on https:\/\/github.com\/moby\/moby\/pull\/47141\r\n- [x] depends on \/ stacked on https:\/\/github.com\/moby\/moby\/pull\/47143\r\n- [x] depends on \/ stacked on https:\/\/github.com\/moby\/moby\/pull\/47147\r\n- [ ] relates to \/ depends on https:\/\/github.com\/containerd\/containerd\/pull\/9662\r\n- [ ] depends on above being backported\r\n- [ ] depends on (ideally) module requirements to be lowered\r\n\r\n### Migrate to github.com\/containerd\/platforms module\r\n\r\nContainerd's \"platforms\" package is being moved to a separate module. This\r\nallows updating the platforms parsing independent of the containerd module\r\nitself.\r\n\r\nFor existing containerd versions (1.6, 1.7), the package in containerd will\r\nbe an alias for the new module.\r\n\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Ah, looks like some updates are needed;\r\n\r\n```\r\n# github.com\/docker\/docker\/libcontainerd\/local\r\nlibcontainerd\/local\/local_windows.go:944:34: cannot use p[i].CreateTimestamp (variable of type time.Time) as *timestamppb.Timestamp value in struct literal\r\n```","- opened https:\/\/github.com\/containerd\/platforms\/pull\/5 to prevent having to upgrade hcsshim","Two tests failing that look related, looks like there's some change in behavior in the package;\r\n\r\n\r\n```\r\n=== FAIL: github.com\/docker\/docker\/integration\/build TestBuildPlatformInvalid (0.06s)\r\n    build_test.go:685: assertion failed: expression is false: errdefs.IsInvalidParameter(err)\r\n```\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/27e85c7b6885c2d21ae90791136d9aba78b83d01\/integration\/build\/build_test.go#L680-L685\r\n\r\n\r\n```\r\n=== FAIL: amd64.integration.image TestImagePullPlatformInvalid (0.01s)\r\n    pull_test.go:37: assertion failed: expression is false: errdefs.IsInvalidParameter(err)\r\n```\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/27e85c7b6885c2d21ae90791136d9aba78b83d01\/integration\/image\/pull_test.go#L34-L37","Improved the test-assert a bit, and it looks like it's returning a 500 \/  internal server error;\r\n\r\n```\r\n=== RUN   TestImagePullPlatformInvalid\r\n    pull_test.go:37: assertion failed: invalid type for expected: func(error) error: errdefs.errSystem: Error response from daemon: \"foobar\": unknown operating system or architecture: invalid argument\r\n--- FAIL: TestImagePullPlatformInvalid (0.01s)\r\n```\r\n\r\n","Daemon logs show that it returns an untyped error, hence using the default (`500` \/ Internal Server error);\r\n\r\n```console\r\ntime=\"2024-01-21T00:18:31.260074294Z\" level=debug msg=\"Calling POST \/v1.45\/images\/create?fromImage=hello-world&platform=foobar&tag=latest\"\r\ntime=\"2024-01-21T00:18:31.260220544Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"\\\"foobar\\\": unknown operating system or architecture: invalid argument\" error_type=\"*fmt.wrapError\" module=api\r\ntime=\"2024-01-21T00:18:31.260320336Z\" level=error msg=\"Handler for POST \/v1.45\/images\/create returned error: \\\"foobar\\\": unknown operating system or architecture: invalid argument\"\r\ntime=\"2024-01-21T00:18:31.260407169Z\" level=debug msg=\"FIXME: Got an API for which error does not match any expected type!!!\" error=\"\\\"foobar\\\": unknown operating system or architecture: invalid argument\" error_type=\"*fmt.wrapError\" module=api\r\n```","Found the likely cause; the new module doesn't use containerd errdefs;\r\n\r\n<img width=\"2268\" alt=\"Screenshot 2024-01-21 at 01 25 19\" src=\"https:\/\/github.com\/moby\/moby\/assets\/1804568\/9acd2135-4ee2-45c3-b479-e3449bf13388\">\r\n"],"labels":["status\/2-code-review","kind\/refactor"]},{"title":"Problems adding files to image following 25.0.0 upgrade","body":"### Description\r\n\r\nWe have a build process ad Dockerfile for our project images that has not changed for some time, yet this morning I'm unable to build our images due to a problem with adding the content of a .tar.gz file to the image\u2026\r\n\r\n```\r\n => ERROR [setup api_platform_php_production 1\/1] ADD .\/files.tar.gz \/srv\/api\/files                                                                                                                     0.5s\r\n------\r\n > [setup api_platform_php_production 1\/1] ADD .\/files.tar.gz \/srv\/api\/files:\r\n------\r\nfailed to solve: lsetxattr \/sftp_key.ppk: operation not supported\r\n```\r\n\r\nHere is our Dockerfile\u2026\r\n\r\n```Dockerfile\r\nFROM php:8.2-fpm-alpine\r\n\r\nADD .\/files.tar.gz \/srv\/api\/files\r\n```\r\n\r\nI can't share the content of the ZIP file as it's sensitive data, but it was created on MacOS and contans 1 environment variables file and one private key (.ppk) file.\r\n\r\n### Reproduce\r\n\r\n1. `docker build --target=api_platform_php_production .\/api`\r\n\r\n### Expected behavior\r\n\r\nBuild should complete, with the files in the `.tar.gz` file in the image, but I recieve the following error\u2026\r\n\r\n```\r\n => ERROR [2\/2] ADD .\/files.tar.gz \/srv\/api\/files                                                                                                                                     0.3s\r\n------\r\n > [2\/2] ADD .\/files.tar.gz \/srv\/api\/files:\r\n------\r\nDockerfile.test:3\r\n--------------------\r\n   1 |     FROM php:8.2-fpm-alpine\r\n   2 |\r\n   3 | >>> ADD .\/files.tar.gz \/srv\/api\/files\r\n   4 |\r\n--------------------\r\nERROR: failed to solve: lsetxattr \/sftp_key.ppk: operation not supported\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:04:20 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          25.0.0\r\n  API version:      1.44 (minimum version 1.24)\r\n  Go version:       go1.21.6\r\n  Git commit:       615dfdf\r\n  Built:            Thu Jan 18 17:12:10 2024\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.27\r\n  GitCommit:        a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc:\r\n  Version:          1.1.11\r\n  GitCommit:        v1.1.11-0-g4bccb38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.0-desktop.2\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.3-desktop.2\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  0.1\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.10\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.2.0\r\n    Path:     \/Users\/toby\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 252\r\n  Running: 76\r\n  Paused: 0\r\n  Stopped: 176\r\n Images: 492\r\n Server Version: 25.0.0\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: loki\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\r\n Swarm: active\r\n  NodeID: l603mylmj6hnh95at639cy4w0\r\n  Is Manager: true\r\n  ClusterID: vsayqmcuktjv67jgmtgjqm5kk\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 31.193.2.114\r\n  Manager Addresses:\r\n   31.193.2.114:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a1496014c916f9e62104b33d1bb5bd03b0858e59\r\n runc version: v1.1.11-0-g4bccb38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 3.10.0-1160.95.1.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 25.34GiB\r\n Name: id79706\r\n ID: THRG:QZBK:5427:DLKF:M4HF:Y6TQ:32CN:PTR3:IWUG:HOEC:C5T7:TTNI\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nIf I unpack the `tar.gz` file and copy the files individualy then this works fine, and the build can complete.\r\n\r\nExample that works\u2026\r\n\r\n```Dockerfile\r\nFROM php:8.2-fpm-alpine\r\n\r\nCOPY .\/files\/key_passphrase \/srv\/api\/files\/key_passphrase\r\nCOPY .\/files\/sftp_key.ppk \/srv\/api\/files\/sftp_key.ppk\r\n```\r\n\r\nAnd here's the yum upgrade history with versions\u2026\r\n\r\n```\r\n$ yum history info 2362\r\nLoaded plugins: fastestmirror\r\nTransaction ID : 2362\r\nBegin time     : Sat Jan 20 03:13:20 2024\r\nBegin rpmdb    : 1258:aa696fc965824607ba376bcba2c42cedf14780cc\r\nEnd time       :            03:17:44 2024 (264 seconds)\r\nEnd rpmdb      : 1258:38f5b6c8b8b47fbdb03d19a8d62799c88e36c680\r\nUser           : System <unset>\r\nReturn-Code    : Success\r\nTransaction performed with:\r\n    Installed     rpm-4.11.3-48.el7_9.x86_64                      @updates\r\n    Installed     yum-3.4.3-168.el7.centos.noarch                 @base\r\n    Installed     yum-metadata-parser-1.1.4-10.el7.x86_64         @anaconda\r\n    Installed     yum-plugin-fastestmirror-1.1.31-54.el7_8.noarch @updates\r\nPackages Altered:\r\n    Updated docker-buildx-plugin-0.11.2-1.el7.x86_64      @plesk-ext-docker\r\n    Update                       0.12.1-1.el7.x86_64      @plesk-ext-docker\r\n    Updated docker-ce-3:24.0.7-1.el7.x86_64               @plesk-ext-docker\r\n    Update            3:25.0.0-1.el7.x86_64               @plesk-ext-docker\r\n    Updated docker-ce-cli-1:24.0.7-1.el7.x86_64           @plesk-ext-docker\r\n    Update                1:25.0.0-1.el7.x86_64           @plesk-ext-docker\r\n    Updated docker-ce-rootless-extras-24.0.7-1.el7.x86_64 @plesk-ext-docker\r\n    Update                            25.0.0-1.el7.x86_64 @plesk-ext-docker\r\n    Updated docker-compose-plugin-2.21.0-1.el7.x86_64     @plesk-ext-docker\r\n    Update                        2.24.1-1.el7.x86_64     @plesk-ext-docker\r\nhistory info\r\n```","comments":["I've checked through recent upgrades and containerd upgraded a few days back, in case this is relevant\u2026\r\n\r\n```\r\nyum history info 2356\r\nLoaded plugins: fastestmirror\r\nTransaction ID : 2356\r\nBegin time     : Tue Jan 16 03:25:55 2024\r\nBegin rpmdb    : 1258:3c2cc2b5207d86a83dad34fe173281d7d78cc6ed\r\nEnd time       :            03:26:06 2024 (11 seconds)\r\nEnd rpmdb      : 1258:413aeb381e195a11eac855b120c78b2273896044\r\nUser           : System <unset>\r\nReturn-Code    : Success\r\nTransaction performed with:\r\n    Installed     rpm-4.11.3-48.el7_9.x86_64                      @updates\r\n    Installed     yum-3.4.3-168.el7.centos.noarch                 @base\r\n    Installed     yum-metadata-parser-1.1.4-10.el7.x86_64         @anaconda\r\n    Installed     yum-plugin-fastestmirror-1.1.31-54.el7_8.noarch @updates\r\nPackages Altered:\r\n    Updated containerd.io-1.6.26-3.1.el7.x86_64 @plesk-ext-docker\r\n    Update                1.6.27-3.1.el7.x86_64 @plesk-ext-docker\r\n```","Looks like it's failing on extended attributes on one of the files (`sftp_key.ppk`), and either the host's filesystem or the attribute is not suported.\r\n\r\nGiven that that's likely a private key, I guess it's not really an option to send that archive for debugging \/ checking, but perhaps you're able to provide a reproducible example that doesn't have sensitive data?","@thaJeztah Yeah, sorry, it's a private key.  I'll try to find time to test with another archive file with a other files or a dummy key, but finally getting away from my desk after a full day of firefighting!","Thanks yes, if you're able to provide a minimal reproducer, that would be very helpful (so that we can use it for debugging). I have some _potential_ candidates in mind (some changes in this area; some were actual bug fixes where failing to write important attributes were silently ignored), but having a reproducer would be very helpful to confirm that.\r\n\r\nI would also recommend (if this is a production server) pinning packages to prevent unattended updates before having tested them in a staging environment.\r\n\r\nAnd be careful adding private keys to your images; adding them to your image means that anyone with access to the image (or registry containing the image) could potentially get access to your keys \ud83d\ude48  ","macOS `tar` adds all file xattrs as `SCHILY.xattr` PAX records by default, based on some quick testing on my macOS Sonoma system.\r\n```console\r\n$ tar --version\r\nbsdtar 3.5.3 - libarchive 3.5.3 zlib\/1.2.12 liblzma\/5.0.5 bz2lib\/1.0.8\r\n```\r\n\r\nI whipped up a quick Python script to dump the xattr names:\r\n```python\r\n#!\/usr\/bin\/env python3\r\n\r\nimport tarfile\r\nimport sys\r\n\r\nif len(sys.argv) < 2 or sys.argv[1] == '-':\r\n    f = sys.stdin.buffer\r\nelse:\r\n    f = open(sys.argv[1], 'rb')\r\n\r\nwith tarfile.open(fileobj=f) as tf:\r\n    for m in tf.getmembers():\r\n        print(m.name)\r\n        for k in m.pax_headers.keys():\r\n            if k.startswith('SCHILY.xattr.'):\r\n                print(f\"\\t{k}\")\r\n        print()\r\n```\r\n\r\nWhen I point it at a tarball I created of a random screenshot, I get the following output:\r\n```\r\n._Screen Shot 2021-01-07 at 3.10.03 PM.png\r\n\r\nScreen Shot 2021-01-07 at 3.10.03 PM.png\r\n\tSCHILY.xattr.com.apple.metadata:kMDItemScreenCaptureType\r\n\tSCHILY.xattr.com.apple.metadata:kMDItemScreenCaptureGlobalRect\r\n\tSCHILY.xattr.com.apple.metadata:kMDItemIsScreenCapture\r\n\tSCHILY.xattr.com.apple.macl\r\n\tSCHILY.xattr.com.apple.lastuseddate#PS\r\n\tSCHILY.xattr.com.apple.FinderInfo\r\n```\r\n\r\n`com.` is not a valid [Linux xattr namespace prefix](https:\/\/man7.org\/linux\/man-pages\/man7\/xattr.7.html#:~:text=Extended%20attribute%20namespaces), so none of those xattrs could be successfully applied to a file on Linux. I would not be surprised if the tarball @toby-griffiths is having trouble with also contains `com.apple.foo` xattrs.\r\n\r\nWhat should we do when an xattr can't be applied? Clearly it's (at least) an ergonomics issue to treat all xattrs as mandatory in Dockerfile instructions. The most backwards-compatible choice would be to revert to restore best-effort xattr behaviour for Dockerfile `ADD` instructions. It would also be nice to log xattr errors to the build output so it's visible to the user, instead of just the daemon log. We could possibly also make the behaviour configurable, like GNU tar's `--xattrs`, `--no-xattrs`, `--xattrs-exclude=PATTERN`, `--xattrs-include=PATTERN` flags.\r\n\r\nOther things of note:\r\n- macOS tar always archives xattrs by default, but only extracts xattrs by default when run as root\r\n- GNU tar, on the other hand, always defaults to `--no-xattrs`\r\n- If GNU tar is unable to apply an xattr to a file during extraction, it prints a warning and continues","@tonistiigi @AkihiroSuda any thoughts?","Yup, looks indeed that files created on macOS can have those attributes, and adding a tar produces the error. Adding non-compressed files (or at least, files that are not extracted by `ADD`) works;\r\n\r\n\r\nDockerfile:\r\n\r\n```dockerfile\r\nFROM alpine\r\n\r\nWORKDIR \/foo\r\nADD screenshot.png .\r\nADD archive.tar.gz .\r\n```\r\n\r\n```bash\r\nls -la\r\ntotal 1776\r\ndrwxr-xr-x    4 thajeztah  staff     128 Jan 23 12:32 .\/\r\ndrwx------  321 thajeztah  staff   10272 Jan 23 12:27 ..\/\r\n-rw-r--r--    1 thajeztah  staff      68 Jan 23 12:33 Dockerfile\r\n-rw-r--r--@   1 thajeztah  staff  905087 Jan 23 12:27 screenshot.png\r\n```\r\n\r\nThe screenshot contains extended-attributes added by macOS (same may be the case when downloading files from the internet);\r\n\r\n```bash\r\nxattr screenshot.png\r\ncom.apple.FinderInfo\r\ncom.apple.lastuseddate#PS\r\ncom.apple.macl\r\ncom.apple.metadata:kMDItemIsScreenCapture\r\ncom.apple.metadata:kMDItemScreenCaptureGlobalRect\r\ncom.apple.metadata:kMDItemScreenCaptureType\r\n```\r\n\r\nCreate a tar with the screenshot in it (using default options, which will preserve extended attributes in the archive);\r\n\r\n```bash\r\ntar -czf archive.tar.gz screenshot.png\r\n```\r\n\r\n```bash\r\ndocker build -t foo --no-cache --progress=plain .\r\n#0 building with \"desktop-linux\" instance using docker driver\r\n# ...\r\n\r\n#7 [3\/4] ADD screenshot.png .\r\n#7 DONE 0.0s\r\n\r\n#8 [4\/4] ADD archive.tar.gz .\r\n#8 ERROR: lsetxattr \/screenshot.png: operation not supported\r\n------\r\n > [4\/4] ADD archive.tar.gz .:\r\n------\r\nDockerfile:5\r\n--------------------\r\n   3 |     WORKDIR \/foo\r\n   4 |     ADD screenshot.png .\r\n   5 | >>> ADD archive.tar.gz .\r\n   6 |\r\n--------------------\r\nERROR: failed to solve: lsetxattr \/screenshot.png: operation not supported\r\n```\r\n\r\n\r\n### Workaround\r\n\r\nUsing the `--no-xattrs` option when creating the archive fixes the problem, and can at least be used as a workaround;\r\n\r\n```bash\r\nrm archive.tar.gz\r\n\r\ntar --no-xattrs -czf archive.tar.gz screenshot.png\r\n\r\ndocker build -t foo --no-cache --progress=plain .\r\n#0 building with \"desktop-linux\" instance using docker driver\r\n\r\n#8 [4\/4] ADD archive.tar.gz .\r\n#8 DONE 0.0s\r\n\r\n#9 exporting to image\r\n#9 exporting layers 0.0s done\r\n#9 exporting manifest sha256:be68c788f273f8f722d27a80f359482205928f51ea47195e34929b7e6a73f919 done\r\n#9 exporting config sha256:b7227af8c802980742f79b45a60e05277a4df41475b898892e81eebda1ca1bba done\r\n#9 exporting attestation manifest sha256:a7715c956724758591cec3092fc5d190e315d33063255ff9a6742b9daf8d9c76 done\r\n#9 exporting manifest list sha256:6ed881625fb028fad314d172b316fbf1d9b5217388262cef0f8bee738222f140\r\n#9 exporting manifest list sha256:6ed881625fb028fad314d172b316fbf1d9b5217388262cef0f8bee738222f140 done\r\n#9 naming to docker.io\/library\/foo:latest done\r\n#9 unpacking to docker.io\/library\/foo:latest 0.0s done\r\n#9 DONE 0.1s\r\n```\r\n\r\n\r\n","> And be careful adding private keys to your images; adding them to your image means that anyone with access to the image (or registry containing the image) could potentially get access to your keys \ud83d\ude48\r\n\r\nYes, of course. I was aware of this, but we're a tiny organisation with just a couple of us with access to the images\u2026 however, because of this I've actually done the work I was planning to move these to secrets, and am now cleaning up the old images.","Ah, great!\r\n\r\nI know there's many \"don't do that!\" things, that may be \"fine\" _if you know what you're doing_ (who hasn't `chmod -r 777` just to make things work \ud83d\ude07 \ud83d\ude02).\r\n\r\nBut I try to be cautious when I see things mentioned on GitHub, and leave  a comment; mostly for \"other readers\". Not everyone understands the risks, and people may arrive here through Google and read the example and copy it without knowing \ud83d\ude48\r\n"],"labels":["area\/builder","area\/storage","status\/0-triage","status\/more-info-needed","kind\/bug","area\/builder\/buildkit","version\/25.0"]},{"title":"TestDiskUsage is flaky with snapshotters","body":"### Description\n\nIt sometimes fails when run with c8d snapshotters:\r\n\r\n```\r\n=== FAIL: amd64.integration.system TestDiskUsage\/empty (0.00s)\r\n    disk_usage_test.go:41: assertion failed: \r\n        --- du\r\n        +++ \u2192\r\n          types.DiskUsage{\r\n        - \tLayersSize: 4096,\r\n        + \tLayersSize: 0,\r\n          \tImages:     {},\r\n          \tContainers: {},\r\n          \t... \/\/ 3 identical fields\r\n          }\r\n        \r\n    --- FAIL: TestDiskUsage\/empty (0.00s)\r\n\r\n=== FAIL: amd64.integration.system TestDiskUsage (2.60s)\r\n\r\n```\r\n\r\nIt seems to be related to the same thing that was happening in some other test: https:\/\/github.com\/moby\/moby\/pull\/46591\/commits\/28d057cb0e7946906c656d82dd6293f8d7e28947\r\n\r\nThis test is parallel to other tests from the same suite so other tests are running at the same time. \r\nMy hypothesis is that  even this test spawns a new docker daemon, the containerd daemon and its storage is still shared and some other tests cause the creation of empty directory which produces the overall size of 4096 (minimum allocation unit on `ext4`).\r\nIf that's true, removing `t.Parallel` should help, but perhaps we should look into it and investigate if we need more separation between docker and containerd daemons in tests.\n\n### Reproduce\n\n-\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nv25.0.0\n```\n\n\n### docker info\n\n```bash\n-\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["kind\/bug","exp\/intermediate","area\/testing","containerd-integration"]},{"title":"containerd integration: No extracting progress when pulling an image","body":"### Description\n\nWith containerd image store enabled, pulling an image doesn't show the unpacking progress after the download has finished. This in combination with big images\/slow machines gives the impression that the pull is stuck.\n\n### Reproduce\n\n$ docker pull <big image>\n\n### Expected behavior\n\ndocker pull with big images should show extraction progress\n\n### docker version\n\n```bash\nmaster @ 4f9c865eddc09da0c0cb9fe08c76b81b804093f5\n```\n\n\n### docker info\n\n```bash\n-\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["kind\/bug","area\/images","containerd-integration","area\/ux"]},{"title":"containerd integration: Fix remaining Windows failures","body":"\r\n\r\nThere are 2 remaining tests that are failing:\r\n----------------------------------------------\r\n- [ ] TestCopyFromContainerPathIsNotDir\r\n\r\n```\r\n=== FAIL: github.com\/docker\/docker\/integration\/container TestCopyFromContainerPathIsNotDir (0.52s)\r\n    copy_test.go:47: assertion failed: expected an error, got nil\r\n```\r\n\r\nThis test verifies that copying the file from container that was specified by a path with a trailing slash (suggesting a directory) errors.\r\n\r\nIn this case, it succeeds, you can also use `docker cp` to verify:\r\n```\r\nPS> docker cp \"asdf:c:\/windows\/system32\/drivers\/etc\/hosts\" test\r\nSuccessfully copied 2.56kB to C:\\Users\\Pawel\\Desktop\\test\r\nPS> type test\r\n# Copyright (c) 1993-2009 Microsoft Corp.\r\n#\r\n# This is a sample HOSTS file used by Microsoft TCP\/IP for Windows.\r\n#\r\n(...)\r\n```\r\n\r\n\r\n-----------------------------------------------\r\n\r\n- [ ] TestBuildSymlinkBreakout\r\n```\r\n=== FAIL: github.com\/docker\/docker\/integration-cli TestDockerCLIBuildSuite\/TestBuildSymlinkBreakout (0.88s)\r\n    docker_cli_build_test.go:3636: assertion failed: \r\n        Command:  D:\\a\\moby\\moby\\out\\docker.exe build -t testbuildsymlinkbreakout --no-cache .\r\n        ExitCode: 1\r\n        Error:    exit status 1\r\n        Stdout:   Sending build context to Docker daemon  7.168kB\r\n\r\n        Step 1\/3 : from busybox\r\n         ---> bad3991f6d60\r\n        Step 2\/3 : add symlink.tar \/\r\n        \r\n        Stderr:   failed to copy files: invalid symlink \"\\\\\\\\?\\\\C:\\\\Windows\\\\SystemTemp\\\\rootfs-mount3502585150\\\\symlink2\" -> \"\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/\"\r\n        \r\n        \r\n        Failures:\r\n        ExitCode was 1 expected 0\r\n        Expected no error\r\n```\r\n\r\n","comments":["That second test is weird. It looks like it expects the breakout symlink to be (silently?) skipped or neutered, but a quick look at the logic starting from [`func (b *Builder) performCopy`](https:\/\/github.com\/moby\/moby\/blob\/4a40d10b6044fc1dc733e03799b2404ef45d2cc0\/builder\/dockerfile\/internals.go#L178) (where the outer error happens) through to [`func createTarFile`](https:\/\/github.com\/moby\/moby\/blob\/4a40d10b6044fc1dc733e03799b2404ef45d2cc0\/pkg\/archive\/archive.go#L753) where the inner error happens doesn't show any alternative paths through this except the error we just saw.\r\n\r\nI note `TestBuildAddBadLinks` in the same file is marked \"Not for Windows\", so perhaps it's actually always been faulty, but passed by accident in graphdriver? Even though that one's marked as not-for-Windows, it has some Windows-specific logic for the symlink path (Windows-style slashes, which might actually be the causal underlying issue, but why wouldn't it fail in the graphdriver in this case?)\r\n\r\nI'm mildly suspicious of this because it's a negative test (\"Target file doesn't exist\") but that is an open invitation for false-positives... But I also have to admit I don't know what the expected behaviour of the Docker legacy builder is for such cases, nor where any such logic is implemented.","Sounds plausible that it may not even have worked on Windows.\r\n\r\nWe also need to really review all the `t.Skip()` for Windows; I know there's many that were added originally by the Microsoft contributors; some were skipped because the platform (Windows) did not support things, others because they could never work on Windows, and then there's test that _could_ work, but may need conditional changes to adopt them for Windows.\r\n\r\nUnfortunatly of those `t.Skip()` don't have a comment with details (reason why failing \/ can't work \/ will never work)"],"labels":["platform\/windows","kind\/bug","containerd-integration"]},{"title":"rootless: support detach-netns mode (Support real localhost registries and `--net=host`)","body":"Now `dockerd-rootless.sh` launches RootlessKit with `--detach-netns`\r\nso as to run the daemon in the host network namespace.\r\n\r\nThe libnetwork namespaces are allocated inside the \"detached\" netns\r\n(`$ROOTLESSKIT_STATE_DIR\/netns`) that is associated with slirp4netns,\r\nvpnkit, pasta, etc., as the rootless daemon has no `CAP_NET_ADMIN` for\r\nthe host network namespace.\r\n\r\nThis will enable:\r\n- Accelerated (and deflaked) `docker pull`, `docker push`, `docker build`, etc\r\n- Proper support for `docker pull 127.0.0.1:...\/...`\r\n- Proper support for `dockern run --net=host`\r\n\r\nSee also:\r\n- rootless-containers\/rootlesskit#379\r\n- containerd\/nerdctl#2723\r\n\r\n- - -\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n- Accelerated (and deflaked) `docker pull`, `docker push`, `docker build`, etc\r\n- Proper support for `docker pull 127.0.0.1:...\/...`\r\n- Proper support for `dockern run --net=host`\r\n\r\n**- How I did it**\r\nExecute the rootless daemon in the host network namespace.\r\n\r\nPrior to this PR, the daemon was executed in a network namespace created by RootlessKit.\r\n\r\n**- How to verify it**\r\nCI should be green\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\nrootless: support detach-netns mode (Support real localhost registries and `--net=host`)\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\ud83d\udc27 ","comments":["https:\/\/github.com\/moby\/moby\/actions\/runs\/7570750383\/job\/20616830214?pr=47103\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.capabilities TestNoNewPrivileges\/CapabilityRequested=true (0.00s)\r\n    capabilities_linux_test.go:83: assertion failed: error is not nil: Error response from daemon: No such image: captest:latest\r\n    --- FAIL: TestNoNewPrivileges\/CapabilityRequested=true (0.00s)\r\n\r\n=== FAIL: amd64.integration.capabilities TestNoNewPrivileges\/CapabilityRequested=false (0.00s)\r\n    capabilities_linux_test.go:83: assertion failed: error is not nil: Error response from daemon: No such image: captest:latest\r\n    --- FAIL: TestNoNewPrivileges\/CapabilityRequested=false (0.00s)\r\n\r\n=== FAIL: amd64.integration.capabilities TestNoNewPrivileges (0.29s)\r\n```\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/asdfasdf:latest (0.00s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/asdfasdf:latest (0.00s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/library\/asdfasdf:latest (0.00s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/library\/asdfasdf:latest (0.00s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/asdfasdf (0.00s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/asdfasdf (0.00s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/asdfasdf:foobar (0.01s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/asdfasdf:foobar (0.01s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/library\/asdfasdf (0.00s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/library\/asdfasdf (0.00s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting\/library\/asdfasdf:foobar (0.00s)\r\n    pull_test.go:187: assertion failed: expected error to contain \"pull access denied for asdfasdf, repository does not exist or may require 'docker login'\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n    --- FAIL: TestImagePullNonExisting\/library\/asdfasdf:foobar (0.00s)\r\n\r\n=== FAIL: amd64.integration.image TestImagePullNonExisting (0.01s)\r\n```\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.system TestLoginFailsWithBadCredentials (0.39s)\r\n    login_test.go:28: assertion failed: expected error to contain \"unauthorized: incorrect username or password\", got \"Error response from daemon: Get \\\"https:\/\/registry-1.docker.io\/v2\/\\\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\"\r\n        Error response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp: lookup registry-1.docker.io on 168.63.129.16:53: dial udp 168.63.129.16:53: connect: network is unreachable\r\n```\r\n\r\n- - -\r\n\r\n```\r\n$ docker --context=rootless pull hello-world\r\n                         \r\nUsing default tag: latest\r\nlatest: Pulling from library\/hello-world\r\nDigest: sha256:4bd78111b6914a99dbc560e6a20eab57ff6655aea4a80c50b0c5491968cbc2e6\r\nStatus: Image is up to date for hello-world:latest\r\ndocker.io\/library\/hello-world:latest\r\n\r\n$ docker --context=rootless run --rm hello-world\r\n\r\nHello from Docker!\r\n...\r\n\r\n$ docker --context=rootless pull hello-world\r\nUsing default tag: latest\r\nError response from daemon: Get \"https:\/\/registry-1.docker.io\/v2\/\": dial tcp [2600:1f18:2148:bc00:41e1:f57f:e2e2:5e54]:443: connect: network is unreachable\r\n```\r\n\r\nIt looks like the daemon accidentally leaves the host network namespace after running a container"],"labels":["kind\/enhancement","area\/networking","impact\/changelog","area\/rootless"]},{"title":"update docker-py to current master","body":"- fixes https:\/\/github.com\/moby\/moby\/issues\/46967\r\n- [x] depends on \/ stacked on https:\/\/github.com\/moby\/moby\/pull\/47091\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/46965\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/46966\r\n- relates to https:\/\/github.com\/docker\/docker-py\/pull\/3203\r\n\r\n\r\nfull diff: https:\/\/github.com\/docker\/docker-py\/compare\/7.0.0...bd164f928ab82e798e30db455903578d06ba2070\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["We should undraft this if we want to deprecate Container related fields in image inspect in v26: https:\/\/github.com\/docker\/cli\/pull\/4893","Ah, yeah; I think we had `skips` for those set, or we don't?\r\n\r\nLooking at https:\/\/github.com\/docker\/docker-py\/commit\/2a5f354b502fe0624239f8b2607cc624857027cb, perhaps we need to do another update there to update it to v25, and we could ask them to tag a new version (could be a \"patch\" or \"minor\" release; looks like it's mostly bug fixes, but also compatibility with newer API versions(?) so that could mean \"minor\" version update.\r\n","Hmm looks like we have the skips but ~they don't work? \ud83e\udd14~ they're only for the containerd integration.\r\n\r\nAt least https:\/\/github.com\/moby\/moby\/pull\/47430 shows related docker-py failures.","> they're only for the containerd integration.\r\n\r\nAh, right, I guess we could still skip unconditionally. That said; not 100% against updating to \"master\"; mostly was trying to stay on tagged versions, but it's not _critical_"],"labels":["status\/2-code-review","area\/testing"]},{"title":"Constantly growing \/var\/lib\/docker\/overlay2\/","body":"### Description\n\nWe have a set of job-runners that perform various build- and deployment-related tasks using docker. After some time (weeks), all of them end up with critical disk usage and have to be re-provisioned as there does not appear to be a way to release the used disk space via the docker CLI:\r\n\r\n```\r\n$ du -sh \/var\/lib\/docker\/*\r\n72.4M\t\/var\/lib\/docker\/buildkit\r\n71.2M\t\/var\/lib\/docker\/containers\r\n4.0K\t\/var\/lib\/docker\/engine-id\r\n293.9M\t\/var\/lib\/docker\/image\r\n132.0K\t\/var\/lib\/docker\/network\r\n65.7G\t\/var\/lib\/docker\/overlay2\r\n0\t\/var\/lib\/docker\/plugins\r\n0\t\/var\/lib\/docker\/runtimes\r\n0\t\/var\/lib\/docker\/swarm\r\n0\t\/var\/lib\/docker\/tmp\r\n5.0G\t\/var\/lib\/docker\/volumes\r\n$ ls -l \/var\/lib\/docker\/overlay2\/ | wc -l\r\n4204\r\n$ docker system df\r\nTYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\r\nImages          2         2         123.7MB   0B (0%)\r\nContainers      2         2         168B      0B (0%)\r\nLocal Volumes   62        0         5.292GB   5.292GB (100%)\r\nBuild Cache     0         0         0B        0B\r\n$ docker builder ls\r\nNAME\/NODE DRIVER\/ENDPOINT STATUS  BUILDKIT             PLATFORMS\r\ndefault * docker                                       \r\n  default default         running v0.11.7+d3e6c1360f6e linux\/amd64, linux\/amd64\/v2, linux\/amd64\/v3, linux\/386\r\n$ docker builder du\r\nReclaimable:\t0B\r\nTotal:\t\t0B\r\n$ docker system prune -a -f\r\nTotal reclaimed space: 0B\r\n$ docker builder prune -a -f\r\nTotal:\t0B\r\n```\r\n\r\nRelated to #32420, #43586\n\n### Reproduce\n\nI'm not sure as it appears to be building up slowly over time.\n\n### Expected behavior\n\nThe used disk space under `\/var\/lib\/docker\/overlay2\/` shows up in any diagnostic `docker ...` command and can be released with any (other) `docker ...` command.\r\nI would expect it to show up under `docker system df` and\/or `docker builder du` and for it t be releaseable with `docker system prune` or `docker builder prune`.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:09:13 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:45 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.18\r\n  GitCommit:        2456e983eb9e37e47538f59ea18f2043c9a73640\r\n runc:\r\n  Version:          1.1.4\r\n  GitCommit:        v1.1.4-0-g5fd4c4d\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 2456e983eb9e37e47538f59ea18f2043c9a73640\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.14.0-70.13.1.el9_0.x86_64\r\n Operating System: Rocky Linux 9.1 (Blue Onyx)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 15.28GiB\r\n Name: xxx\r\n ID: c603ff4f-a1fb-4a1c-b323-75ef6e588528\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: xxx\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["If a lot of \"docker build\" is happening on those machines, I wonder if this is related to;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/46136\r\n\r\nWhich should be fixed in BuildKit v0.12 (the version used in the upcoming docker v25.0 release). If you have a system to test on (and are able to reproduce on), it may be worth trying if it reproduces with the v25.0.0 release-candidate. Packages for v25.0 are available in the \"test\" channel in our package repositories on download.docker.com. Those are pre-releases, so make sure to test them in a test-environment you're comfortable with trying on, but a \"ga\" release will be available soon (finishing up some  final bits, but we may be able to do a v25.0.0 release this week).","Thanks for the quick response!\r\n\r\nThe described reproducer with parallel builds from that issue is indeed analogous to our use-case. I would assume this to be the same issue.\r\n\r\nWe will see if we can confirm this with the RC release.","Thanks! Yes, happy to hear if that resolved the issue. From discussion I had with the BuildKit team, it was \"too complicated\" to backport related fixes to the v0.11 release, so hoping it's fixed with the v0.12 (used in v25).\r\n\r\nThere's for sure situations where content _could_ be left behind (unclean shutdown etc), but some reports caused us some head-scratching and we couldn't place where the content came from (or why it wouldn't be cleaned up), so hoping this is the reason (and it to be fixed).\r\n"],"labels":["area\/storage\/overlay","status\/0-triage","kind\/bug","version\/24.0"]},{"title":"Docker Attach call causing possible hardware damage","body":"### Description\r\n\r\nI have been messing around with Stable-Diffusion in a docker container, and I do have a good idea as to why this happened since I have been researching and trying to fix the issue on my GPU since now it is potentially unuseable.\r\n\r\nThe steps to reproduces are well described in the other issue with rocm I have opened, since now the card this happened to fails out, while an identical card in the same system and with the same configuration doesnt.\r\n\r\nI had an open tty running the some 'diffusers' nased python scripts just a loop generating images.\r\nI figured id login to the container and run something else on the second GPU because... why not!\r\nI had done this before but didnt record how to, and in the docker exec i ommited -it, I had seen docker attach and and ran it, to my dismay it totally locked the system and required an entire reboot, resulting in that card never to work properly again. \r\n\r\nMy figures is, after doing some more research was the container itself didnt have rocm or amdgpu install, the host did, but I had succesfully installed everything and had generated more then 100 images in the configurations listed in the other issue. I had some OOM issues ive since resolved because of not having drivers and changing up versions of things, but when the \"docker attach\" when to take control of the tty that was still open and running, whilst also running on the gpu, without proper drivers, it run a muck with the gpu. that isnt resolved. what i did resolve and why i believe this is after i did get the software setup in another configuration(not the final configuration Im using now) I wasnt able to reproduce the specific error anymore, as i was able to consitently before, but i was unable to get it to spit out a stack trace i needed for the ROCM team, so i was like what they hell lets attach again and cause the crash with all the enviroment variables set and see if i can get it to spit it out. To my surprise it attached and both tty windows were displaying the progress bar with no issue. and on the bunch card. this is were my assertion is coming from that i believe it had to do with the fact the drivers werent in the container, since after they were installed there wasnt any issues. \r\n\r\nnow all im really posting this for is the say, or ask, for there to be some sanity checks in here. Is it busy?, Is there a current tty attached? container configurations aside(accept for preproduction of the issue hence the reference), I do believe \r\n\r\n$docker attach <container>\r\nwarning: cannot attach, currently attached :: use '--force' if you know its safe to (cause there was this guy right, and he literally blew up a GPU because of this)\r\n\r\nwould be pretty awsome lol, and acctually i didnt completely blow up the gpu, but pretty close to being bricked\r\n\r\nI'd also me more the happy to do this, if obviously its low concern. I think i might be able to replicate a docker container, running a docker container.... but i dunno if i wanna blow up another gpu sooo lol\r\n\r\nthanks\r\n\r\n### Reproduce\r\n\r\nrefer to \r\nhttps:\/\/github.com\/ROCm\/ROCm\/issues\/2804 \r\n\r\n### Expected behavior\r\n\r\nto not blow up my gpu cause im a rookie\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.21.0\r\n Git commit:        %{shortcommit_cli}\r\n Built:             Sun Aug 27 16:45:40 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.21.0\r\n  Git commit:       %{shortcommit_moby}\r\n  Built:            Sun Aug 27 16:45:40 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.23\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 3\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: \/usr\/libexec\/docker\/docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  selinux\r\n  cgroupns\r\n Kernel Version: 6.6.9-200.fc39.x86_64\r\n Operating System: Fedora Linux 39 (Thirty Nine)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 78.48GiB\r\n Name: paragon.home\r\n ID: 74597b7c-5e51-4760-96b3-b919fe1b02d1\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nhttps:\/\/github.com\/ROCm\/ROCm\/issues\/2804 ","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Fix empty descriptors in layerStore","body":"- related to: https:\/\/github.com\/moby\/moby\/pull\/47068\r\n\r\n(fixes the root cause of https:\/\/github.com\/moby\/moby\/issues\/47065)\r\n\r\nFix empty layer descriptors.\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/distribution","status\/2-code-review","area\/images","kind\/bugfix"]},{"title":"Swarm: External DNS queries made from a service fails when the service has a port mapping","body":"### Description\n\nIn swarm mode, external DNS queries made from a service fails when the service has a port mapping.\r\nThe affected service is still able to reach an external webpage via IP address - DNS resolution is the issue here.\r\n\r\n**Note: This bug only appears when there is more than 1 non-leader swarm node.**\n\n### Reproduce\n\ndocker-compose.yml\r\n```\r\nversion: \"3.8\"\r\n\r\nservices:\r\n  curl-unmapped:\r\n    image: alpine\/curl:latest\r\n    hostname: curl-unmapped.xyz.com\r\n    networks:\r\n      - my-test-network\r\n    command: sleep infinity\r\n\r\n  curl-mapped:\r\n    image: alpine\/curl:latest\r\n    hostname: curl-mapped.xyz.com\r\n    ports:\r\n      - 4027:4027\r\n    networks:\r\n      - my-test-network\r\n    command: sleep infinity\r\n\r\nnetworks:\r\n  my-test-network:\r\n    driver: overlay\r\n    attachable: true\r\n```\r\n0. Set up swarm stack at least 1 worker (in addition to manager\/leader node)\r\n1. `docker stack deploy dnsResolution -c docker-compose.yml`\r\n2. Wait for stack to be deployed\r\n3. Run `docker exec -it dnsResolution-curl-unmapped.<rest> curl -Ik www.google.com` (This will work)\r\n4. Run `docker exec -it dnsResolution-curl-mapped.<rest> curl -Ik www.google.com` (This will fail: `curl: (6) Could not resolve host: www.google.com`)\r\n\r\nSee the _Additional Info_ section below for Docker service error logs which are logged when the DNS query fails in Step 4.\n\n### Expected behavior\n\nExternal DNS queries made from a service should work regardless of whether the service has a port mapping.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.26\r\n  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 16\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: nh7lq6y8n93r1yhk80najtvwa\r\n  Is Manager: true\r\n  ClusterID: yd0f9fufrv3qnr9z4zw41vckk\r\n  Managers: 1\r\n  Nodes: 3\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.1.50\r\n  Manager Addresses:\r\n   10.0.1.50:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.19.0-1029-aws\r\n Operating System: Ubuntu 22.04.2 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.773GiB\r\n Name: ip-10-0-1-50\r\n ID: 6bdd917e-00c0-4ff6-8dad-1bd7d247bc31\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nFrom `journalctl -xeu docker.service -f`\r\n\r\n```\r\nJan 11 06:20:52 ip-10-0-1-50 dockerd[42089]: time=\"2024-01-11T06:20:52.549371398Z\" level=error msg=\"[resolver] failed to query DNS server: 10.0.0.2:53, query: ;www.google.com.\\tIN\\t A\" error=\"write udp 10.0.0.48:36589->10.0.0.2:53: write: operation not permitted\"\r\n```\r\n\r\n**Note:** Error message states `write: operation not permitted`. This might be an indication of the issue.","comments":["This sounds more like an issue with your host machine, not Moby. The port mapping might be red herring; have you confirmed that the `curl-mapped` task is running on the same Swarm node as the `curl-unmapped` one?\r\n\r\n\"write: operation not permitted\" is `EPERM`, which can happen if your firewall is configured to drop the outbound packets or [your conntrack table is full](https:\/\/blog.cloudflare.com\/conntrack-tales-one-thousand-and-one-flows#fullconntrackcauseswitheperm). Are you able to successfully make DNS queries from the affected host outside of a container?\r\n```console\r\n$ dig google.com 10.0.0.2\r\n```","Apologies for the delayed response, I was on holidays from Jan 19th. \r\n\r\n---\r\n\r\n>  have you confirmed that the curl-mapped task is running on the same Swarm node as the curl-unmapped one?\r\n\r\nIn the original test I performed above in the issue description, both tasks were running on the samw Swarm node - in this case the manager node.\r\n\r\n> if your firewall is configured to drop the outbound packets\r\n\r\nI'm by no means an `iptables` expert and don't really have a full understanding of how the Docker Swarm network interacts with the host IP tables. However, here is a snippet which might be useful:\r\n\r\n```\r\nsudo iptables -vL | grep DROP\r\n\r\nChain FORWARD (policy DROP 103 packets, 6052 bytes)\r\n    0     0 DROP       all  --  docker_gwbridge docker_gwbridge  anywhere             anywhere            \r\n    0     0 DROP       all  --  any    docker0  anywhere             anywhere            \r\n    0     0 DROP       all  --  any    docker_gwbridge  anywhere             anywhere   \r\n```\r\n\r\nNote: The test stack described in the issue description **does not** specify any custom IP table rules.\r\n\r\n>  Are you able to successfully make DNS queries from the affected host outside of a container?\r\n\r\nFrom the host machine, I **am** able to successfully make DNS queries.\r\n\r\n```\r\ndig google.com\r\n\r\n; <<>> DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu <<>> google.com\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32766\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 65494\r\n;; QUESTION SECTION:\r\n;google.com.                    IN      A\r\n\r\n;; ANSWER SECTION:\r\ngoogle.com.             182     IN      A       142.250.204.14\r\n\r\n;; Query time: 4 msec\r\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\r\n;; WHEN: Mon Feb 19 23:00:06 UTC 2024\r\n;; MSG SIZE  rcvd: 55\r\n```\r\n\r\nand\r\n\r\n```\r\ndig google.com 10.0.0.2\r\n\r\n; <<>> DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu <<>> google.com 10.0.0.2\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 49269\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 65494\r\n;; QUESTION SECTION:\r\n;google.com.                    IN      A\r\n\r\n;; ANSWER SECTION:\r\ngoogle.com.             164     IN      A       142.250.204.14\r\n\r\n;; Query time: 0 msec\r\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\r\n;; WHEN: Mon Feb 19 23:00:24 UTC 2024\r\n;; MSG SIZE  rcvd: 55\r\n\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 45016\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 65494\r\n;; QUESTION SECTION:\r\n;10.0.0.2.                      IN      A\r\n\r\n;; ANSWER SECTION:\r\n10.0.0.2.               0       IN      A       10.0.0.2\r\n\r\n;; Query time: 0 msec\r\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\r\n;; WHEN: Mon Feb 19 23:00:24 UTC 2024\r\n;; MSG SIZE  rcvd: 53\r\n\r\n```","My apologies, I messed up the `dig` command in my previous comment. It should be `dig google.com @10.0.0.2` to query the server at 10.0.0.2 directly instead of going through systemd-resolved at 127.0.0.53. Could you please try running that command on the affected host?","I'm not sure whether you intended to add a space between `google.com` and `10.0.0.2`. Nevertheless I've tried both. Here are the results.\r\n\r\n```\r\ndig google.com @10.0.0.2\r\n\r\n; <<>> DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu <<>> google.com @10.0.0.2\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 39666\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 4096\r\n;; QUESTION SECTION:\r\n;google.com.                    IN      A\r\n\r\n;; ANSWER SECTION:\r\ngoogle.com.             79      IN      A       142.250.66.238\r\n\r\n;; Query time: 4 msec\r\n;; SERVER: 10.0.0.2#53(10.0.0.2) (UDP)\r\n;; WHEN: Wed Feb 21 01:05:16 UTC 2024\r\n;; MSG SIZE  rcvd: 55\r\n```\r\nand\r\n\r\n```\r\ndig google.com@10.0.0.2\r\n\r\n; <<>> DiG 9.18.12-0ubuntu0.22.04.2-Ubuntu <<>> google.com@10.0.0.2\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 672\r\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\r\n\r\n;; OPT PSEUDOSECTION:\r\n; EDNS: version: 0, flags:; udp: 65494\r\n;; QUESTION SECTION:\r\n;google.com\\@10.0.0.2.          IN      A\r\n\r\n;; AUTHORITY SECTION:\r\n.                       1357    IN      SOA     a.root-servers.net. nstld.verisign-grs.com. 2024022002 1800 900 604800 86400\r\n\r\n;; Query time: 4 msec\r\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\r\n;; WHEN: Wed Feb 21 01:05:26 UTC 2024\r\n;; MSG SIZE  rcvd: 123\r\n```"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","area\/networking\/dns","version\/24.0"]},{"title":"dockerd makes DNS lookups when a container name is not resolvable inside a custom network","body":"### Description\n\nI recently notices that some of my hosts was spamming my dns server with lookups for mongo. After investigating this I found that it was all my hosts running docker and docker-compose.\r\n\r\nEven with no containers running, and all containers deleted the issues is still there.\r\n\r\nIf I stop the docker service \"systemctl stop docker\" the dns lookups stop.\n\n### Reproduce\n\nAs I do not really have access to install docker anywhere else I do not know if it is enough to just install docker to see the issue, but you can check by using an iptables rule to catch the lookups.\r\n\r\niptables -I OUTPUT -p UDP --dport 53 -m string --algo bm --string mongo -j LOG --log-uid --log-prefix \"MONGO_MATCH: \"\r\njournalctl -t kernel -f\r\n\r\nEXAMPLE OUTPUT:\r\nJan 02 21:34:30 hostname kernel: MONGO_MATCH: IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.53 LEN=62 TOS=0x00 PREC=0x00 TTL=64 ID=46862 DF PROTO=UDP SPT=40725 DPT=53 LEN=42 UID=0 GID=0\r\nJan 02 21:34:30 hostname kernel: MONGO_MATCH: IN= OUT=lo SRC=127.0.0.1 DST=127.0.0.53 LEN=82 TOS=0x00 PREC=0x00 TTL=64 ID=51075 DF PROTO=UDP SPT=40453 DPT=53 LEN=62 UID=0 GID=0\r\nJan 02 21:34:30 hostname kernel: MONGO_MATCH: IN= OUT=eth0 SRC=192.168.0.51 DST= LEN=82 TOS=0x00 PREC=0x00 TTL=64 ID=15031 PROTO=UDP SPT=37809 DPT=53 LEN=62 UID=101 GID=103\r\nJan 02 21:34:30 hostname kernel: MONGO_MATCH: IN= OUT=eth0 SRC=192.168.0.51 DST= LEN=71 TOS=0x00 PREC=0x00 TTL=64 ID=15032 PROTO=UDP SPT=37809 DPT=53 LEN=51 UID=101 GID=103\n\n### Expected behavior\n\nI expect dockerd to NOT looking mongo all the time in DNS as it is not something I use.\n\n### docker version\n\n```bash\nroot@hostname# cat \/etc\/lsb-release \r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=22.04\r\nDISTRIB_CODENAME=jammy\r\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\r\nroot@hostname# docker version\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.41 (downgraded from 1.43)\r\n Go version:        go1.20.3\r\n Git commit:        24.0.5-0ubuntu1~22.04.1\r\n Built:             Mon Aug 21 19:50:14 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.24\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       5d6db84\r\n  Built:            Wed Aug 23 20:55:00 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.6.20\r\n  GitCommit:        2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc:\r\n  Version:          1.1.5\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\ndocker info\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 9\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-1027-oem\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 20\r\n Total Memory: 62.46GiB\r\n Name: hostname\r\n ID: 6685d9f8-d581-4c52-b1b4-caf9cdcf86ef\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Your report is mentioning two different environments; one running 20.10;\r\n\r\n```\r\nServer:\r\n Engine:\r\n  Version:          20.10.24\r\n```\r\n\r\nAnd one running 24.0;\r\n\r\n```\r\nServer:\r\n Containers: 6\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 9\r\n Server Version: 24.0.5\r\n```\r\n\r\nDo you have information about containers that are running, and how they are configured?\r\n\r\n\r\nDocker provides an embedded DNS that allows you to resolve containers by their name or alias (e.g. a `mongo` container on the same \"docker network\" can be resolved using `mongo` as hostname). Internal names that cannot be resolved by the embedded DNS will be forwarded to external resolvers configured on the host. On the default (\"bridge\") network, there's (currently) no use of and embedded DNS resolver; some host-names may be included in the container's `\/etc\/hosts`, but DNS lookups will otherwise be handled directly by the DNS that's configured on the host (or the DNS that's configured for the container through the `--dns` option).\r\n\r\nAs may be clear from the above, it may depend on the configuration of the containers that are running what's causing these lookups to happen (e.g. if a container is configured to connect to a `mongo` container, but no such container exists, or the container is not running, it may fail to resolve it in DNS, and therefore fall back to resolve using the upstream DNS resolvers).","No containers are running, or stopped. Totally clean machine. All the information is also from the same machine.","> No containers are running, or stopped. Totally clean machine. All the information is also from the same machine.\r\n\r\n\r\nThe `docker info` output shows that there's 6 containers (1 running), and the version of the `docker info` output shows that it's a docker engine v24.0.5, but your `docker version` output shows that it's connected to a `20.10.24` daemon, so they most definitely don't look like the same machines \/ daemons;\r\n\r\n```\r\nServer:\r\n Containers: 6\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 9\r\n```"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","area\/networking\/dns"]},{"title":"cpu-rt-runtime does not behave as expected for systemd cgroup drive when a different slice used","body":"### Description\r\n\r\nAs I know in the following codes, initCPURtController will set cpu.rt_runtime_us for parent path, \r\nBut for systemd cgroup driver, if I specify a cgroup parent such as test.slice or a sub-slice test-a.slice rather than system.slice, it seemd that cpu.rt_runtime_us of the parent path was never set.\r\nI wonder if this is a bug, or should I alway ensure that cpu.rt_runtime_us of the parent path wat set myself?\r\n\r\n\r\n\r\n\t\tif c.HostConfig.CgroupParent != \"\" {\r\n\t\t\tparent = c.HostConfig.CgroupParent\r\n\t\t} else if daemonCfg.CgroupParent != \"\" {\r\n\t\t\tparent = daemonCfg.CgroupParent\r\n\t\t}\r\n\r\n\t\tif useSystemd {\r\n\t\t\tcgroupsPath = parent + \":\" + scopePrefix + \":\" + c.ID\r\n\t\t\tlog.G(ctx).Debugf(\"createSpec: cgroupsPath: %s\", cgroupsPath)\r\n\t\t} else {\r\n\t\t\tcgroupsPath = filepath.Join(parent, c.ID)\r\n\t\t}\r\n\t\tif s.Linux == nil {\r\n\t\t\ts.Linux = &specs.Linux{}\r\n\t\t}\r\n\t\ts.Linux.CgroupsPath = cgroupsPath\r\n\r\n\t\t\/\/ the rest is only needed for CPU RT controller\r\n\r\n\t\tif daemonCfg.CPURealtimePeriod == 0 && daemonCfg.CPURealtimeRuntime == 0 {\r\n\t\t\treturn nil\r\n\t\t}\r\n\r\n\t\tp := cgroupsPath\r\n\t\tif useSystemd {\r\n\t\t\tinitPath, err := cgroups.GetInitCgroup(\"cpu\")\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn errors.Wrap(err, \"unable to init CPU RT controller\")\r\n\t\t\t}\r\n\t\t\t_, err = cgroups.GetOwnCgroup(\"cpu\")\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn errors.Wrap(err, \"unable to init CPU RT controller\")\r\n\t\t\t}\r\n\t\t\tp = filepath.Join(initPath, s.Linux.CgroupsPath)\r\n\t\t}\r\n\r\n\t\t\/\/ Clean path to guard against things like ..\/..\/..\/BAD\r\n\t\tparentPath := filepath.Dir(p)\r\n\t\tif !filepath.IsAbs(parentPath) {\r\n\t\t\tparentPath = filepath.Clean(\"\/\" + parentPath)\r\n\t\t}\r\n\r\n\t\tmnt, root, err := cgroups.FindCgroupMountpointAndRoot(\"\", \"cpu\")\r\n\t\tif err != nil {\r\n\t\t\treturn errors.Wrap(err, \"unable to init CPU RT controller\")\r\n\t\t}\r\n\t\t\/\/ When docker is run inside docker, the root is based of the host cgroup.\r\n\t\t\/\/ Should this be handled in runc\/libcontainer\/cgroups ?\r\n\t\tif strings.HasPrefix(root, \"\/docker\/\") {\r\n\t\t\troot = \"\/\"\r\n\t\t}\r\n\t\tmnt = filepath.Join(mnt, root)\r\n\r\n\t\tif err := daemon.initCPURtController(daemonCfg, mnt, parentPath); err != nil {\r\n\t\t\treturn errors.Wrap(err, \"unable to init CPU RT controller\")\r\n\t\t}\r\n\t\treturn nil\r\n\t}\r\n}\r\n\r\n### Reproduce\r\n\r\ndocker run -itd --cap-add=sys_nice --cpu-rt-runtime=950000 --cgroup-parent=test-a.slice busybox sh -c \"while true; do sleep 10; done\"\r\n\r\nb2af9a25eda3cd8ec426d2e5452f0511bc065522c4ef8ce5d2cd285037f1d3c8\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused \"process_linux.go:449: container init caused \\\"process_linux.go:415: setting cgroup config for procHooks process caused \\\\\\\"failed to write 950000 to cpu.rt_runtime_us: write \/sys\/fs\/cgroup\/cpu,cpuacct\/test.slice\/test-a.slice\/docker-b2af9a25eda3cd8ec426d2e5452f0511bc065522c4ef8ce5d2cd285037f1d3c8.scope\/cpu.rt_runtime_us: invalid argument\\\\\\\"\\\"\": unknown.\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           18.09.0\r\n EulerVersion:      18.09.0.323\r\n API version:       1.39\r\n Go version:        go1.17.3\r\n Git commit:        172f8da\r\n Built:             Mon Apr 17 23:41:30 2023\r\n OS\/Arch:           linux\/amd64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.09.0\r\n  EulerVersion:     18.09.0.323\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.17.3\r\n  Git commit:       172f8da\r\n  Built:            Thu Apr  6 00:00:00 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nContainers: 3\r\n Running: 1\r\n Paused: 0\r\n Stopped: 2\r\nImages: 39\r\nServer Version: 18.09.0\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: systemd\r\nHugetlb Pagesize: 2MB, 2MB (default is 2MB)\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 1c90a442489720eec95342e1789ee8a5e1b9536f\r\nrunc version: d736ef14f0288d6993a1845745d6756cfc9ddd5a-dirty\r\ninit version: N\/A (expected: )\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 5.10.0-60.18.0.50.oe2203.x86_64\r\nOperating System: openEuler 22.03 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.264GiB\r\nName: localhost.localdomain\r\nID: VOEB:6HJ6:REWM:GSFM:PJB2:OFGS:UHX5:XMIK:4E7I:F75B:UYFQ:UHGN\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https:\/\/index.docker.io\/v1\/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0\/8\r\nLive Restore Enabled: true\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"IPv4 only container are resolved with an IPv6 address on an IPv6 enabled network","body":"### Description\n\nHi there, there's something I don't fully understand about docker networking and ipv6.\r\n\r\nI have a dual-stack network where some containers are v4+v6 and some are v4 only.\r\nI can see that when a dual-stack container revolves an ipv4-only, it gets ans ipv4 AND an ipv6 address from the resolver.\r\nLooking at the interface in the ipv4-only container, I don't see any ipv6 address, so I don't understand how the resolver can get an ipv6 address for it, especially if specified as ipv6-disabled.\r\n\r\nPlease find below the reproduction steps.\r\n\r\nI'm probably missing something...\r\n\r\nThanks.\n\n### Reproduce\n\n```\r\n# create dual stack network\r\ndocker network create \\\r\n    --subnet 192.168.123.0\/24 --gateway 192.168.123.1 --ip-range 192.168.123.128\/26 \\\r\n    --subnet=\"fde0:725c:19d8:9704::\/64\" --gateway=\"fde0:725c:19d8:9704::1\" --ipv6 \\\r\n    test-dual-stack-network\r\n\r\n# start and ipv4-only and a dual-stack container\r\ndocker run -d --network test-dual-stack-network --name dual-stack node:19-alpine tail -f \/dev\/null \r\ndocker run -d --network test-dual-stack-network --name ipv4-only --sysctl net.ipv6.conf.all.disable_ipv6=1 node:19-alpine tail -f \/dev\/null \r\n\r\n# check that dual-stack container has ipv4 + ipv6\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 state UNKNOWN qlen 1000\r\n    inet6 ::1\/128 scope host\r\n       valid_lft forever preferred_lft forever\r\n136: eth0@if137: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 state UP\r\n    inet6 fde0:725c:19d8:9704::2\/64 scope global flags 02\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::42:c0ff:fea8:7b80\/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n\r\n# check that ipv4-only container has no ipv6\r\n$ docker exec -ti ipv4-only ip -6 addr\r\n(no output)\r\n\r\n# resolve ipv4-only from dual-stack\r\n$ docker exec -ti dual-stack nslookup ipv4-only\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11:53\r\n\r\nNon-authoritative answer:\r\nName:\tipv4-only\r\nAddress: 192.168.123.129\r\n\r\nNon-authoritative answer:\r\nName:\tipv4-only\r\nAddress: fde0:725c:19d8:9704::3\r\n```\n\n### Expected behavior\n\n`docker exec -ti dual-stack nslookup ipv4-only` to return only ipv4.\r\n\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35-desktop+001\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:32:30 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.22.0 (117440)\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:38 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.1\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     \/Users\/tomav\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 67\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 63\r\n Images: 28\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 5.15.49-linuxkit-pr\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.668GiB\r\n Name: docker-desktop\r\n ID: 7fbae3e1-bac7-4317-85cb-9e58ca837f7f\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\n```\n\n\n### Additional Info\n\n```\r\n$ docker inspect ipv4-only\r\n[\r\n    {\r\n        \"Id\": \"24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5\",\r\n        \"Created\": \"2024-01-10T15:06:21.784037625Z\",\r\n        \"Path\": \"docker-entrypoint.sh\",\r\n        \"Args\": [\r\n            \"tail\",\r\n            \"-f\",\r\n            \"\/dev\/null\"\r\n        ],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 34415,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2024-01-10T15:06:21.972918417Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:309fc9aaf0d7d138831483c33eae396be3e1b5bd83ac1e57e1b124f9300df1fc\",\r\n        \"ResolvConfPath\": \"\/var\/lib\/docker\/containers\/24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/lib\/docker\/containers\/24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5\/hostname\",\r\n        \"HostsPath\": \"\/var\/lib\/docker\/containers\/24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5\/hosts\",\r\n        \"LogPath\": \"\/var\/lib\/docker\/containers\/24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5\/24ce3d7b171d825dc3d01cb8eee5370a3bedef539249cf476b98154fd052abb5-json.log\",\r\n        \"Name\": \"\/ipv4-only\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": null,\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"test-dual-stack-network\",\r\n            \"PortBindings\": {},\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"no\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"ConsoleSize\": [\r\n                46,\r\n                187\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"private\",\r\n            \"Dns\": [],\r\n            \"DnsOptions\": [],\r\n            \"DnsSearch\": [],\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": false,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": null,\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Sysctls\": {\r\n                \"net.ipv6.conf.all.disable_ipv6\": \"1\"\r\n            },\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": [],\r\n            \"BlkioDeviceReadBps\": [],\r\n            \"BlkioDeviceWriteBps\": [],\r\n            \"BlkioDeviceReadIOps\": [],\r\n            \"BlkioDeviceWriteIOps\": [],\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": [],\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": null,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": [\r\n                \"\/proc\/asound\",\r\n                \"\/proc\/acpi\",\r\n                \"\/proc\/kcore\",\r\n                \"\/proc\/keys\",\r\n                \"\/proc\/latency_stats\",\r\n                \"\/proc\/timer_list\",\r\n                \"\/proc\/timer_stats\",\r\n                \"\/proc\/sched_debug\",\r\n                \"\/proc\/scsi\",\r\n                \"\/sys\/firmware\"\r\n            ],\r\n            \"ReadonlyPaths\": [\r\n                \"\/proc\/bus\",\r\n                \"\/proc\/fs\",\r\n                \"\/proc\/irq\",\r\n                \"\/proc\/sys\",\r\n                \"\/proc\/sysrq-trigger\"\r\n            ]\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/lib\/docker\/overlay2\/3cb2228c038a0ce0f19f1bd7a25c207fbac3e4e210a8feb4c6c1b97e14c2fc69-init\/diff:\/var\/lib\/docker\/overlay2\/551179578b1a8cc7cb5ef2d33912fbea4835946b83b14ef41d2bda84142ef175\/diff:\/var\/lib\/docker\/overlay2\/10a77529225f16c055942f8f3b7ad886e78d609f863db0adb7c0f2401d53b060\/diff:\/var\/lib\/docker\/overlay2\/3576624d09a3c441806ddb645291a31f9369d89815ffecde48d08dc1ba74d0ec\/diff:\/var\/lib\/docker\/overlay2\/382768c536559fb944f7131debd4748554762e32ac25fb6eebde1ae775dade8e\/diff\",\r\n                \"MergedDir\": \"\/var\/lib\/docker\/overlay2\/3cb2228c038a0ce0f19f1bd7a25c207fbac3e4e210a8feb4c6c1b97e14c2fc69\/merged\",\r\n                \"UpperDir\": \"\/var\/lib\/docker\/overlay2\/3cb2228c038a0ce0f19f1bd7a25c207fbac3e4e210a8feb4c6c1b97e14c2fc69\/diff\",\r\n                \"WorkDir\": \"\/var\/lib\/docker\/overlay2\/3cb2228c038a0ce0f19f1bd7a25c207fbac3e4e210a8feb4c6c1b97e14c2fc69\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [],\r\n        \"Config\": {\r\n            \"Hostname\": \"24ce3d7b171d\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\",\r\n                \"NODE_VERSION=19.7.0\",\r\n                \"YARN_VERSION=1.22.19\"\r\n            ],\r\n            \"Cmd\": [\r\n                \"tail\",\r\n                \"-f\",\r\n                \"\/dev\/null\"\r\n            ],\r\n            \"Image\": \"node:19-alpine\",\r\n            \"Volumes\": null,\r\n            \"WorkingDir\": \"\",\r\n            \"Entrypoint\": [\r\n                \"docker-entrypoint.sh\"\r\n            ],\r\n            \"OnBuild\": null,\r\n            \"Labels\": {}\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"bbd1bed273e5a723dbd70bfc00de376bb5d2a5c1ed32993b2a8376eb89f38cec\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {},\r\n            \"SandboxKey\": \"\/var\/run\/docker\/netns\/bbd1bed273e5\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"\",\r\n            \"Gateway\": \"\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"\",\r\n            \"IPPrefixLen\": 0,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"\",\r\n            \"Networks\": {\r\n                \"test-dual-stack-network\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"24ce3d7b171d\"\r\n                    ],\r\n                    \"NetworkID\": \"3529611cf1da65a490712c4866cee7ec16058e39362d2a1fcfe0320d7e7f10fc\",\r\n                    \"EndpointID\": \"8865c25be193ce7e06e27284fa1f145afc798152add0f6817bf19d13259ccd3d\",\r\n                    \"Gateway\": \"192.168.123.1\",\r\n                    \"IPAddress\": \"192.168.123.129\",\r\n                    \"IPPrefixLen\": 24,\r\n                    \"IPv6Gateway\": \"fde0:725c:19d8:9704::1\",\r\n                    \"GlobalIPv6Address\": \"fde0:725c:19d8:9704::3\",\r\n                    \"GlobalIPv6PrefixLen\": 64,\r\n                    \"MacAddress\": \"02:42:c0:a8:7b:81\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```","comments":["\/cc @akerouanton @robmry ","Hi @tomav - thank you for reporting this, you're not missing anything.\r\n\r\nI'm about to make some changes to the way the IPv6-ness of a container gets decided, currently it's based on whether it's connected to a network with IPv6 enabled. The `--sysctl` option is applied in the container's namespace, but it's not taken into account in its configuration. \r\n\r\nSo, we'll change that, and use it to fix this problem.","Thanks @robmry! This will solve numerous issues. \ud83c\udf89\r\nIn my case, a nginx dual-stack container which acts as a reverse proxy to ipv4-only containers. It always tries first the ipv6   received from the resolver with default configuration before fallback on ipv4, which drastically increases time to first byte."],"labels":["kind\/question","status\/0-triage","kind\/bug","area\/networking","area\/networking\/ipv6","version\/24.0"]},{"title":"Ports: Short syntax no longer works?","body":"### Description\n\nAs recommended in [this Docker Forums post]([Ports: Short syntax no longer works?](https:\/\/forums.docker.com\/t\/ports-short-syntax-no-longer-works\/139155)), I'm posting here as well.\r\n\r\nAlso, an older GH issue thread that I had forgotten I created almost a year ago. [Here](https:\/\/github.com\/docker\/cli\/issues\/4062).\n\n### Reproduce\n\nFull steps are described in both links I added above.\n\n### Expected behavior\n\nShort syntax should behave the same as the long syntax; as they originally did.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\nVersion: 24.0.7\r\nAPI version: 1.43\r\nGo version: go1.20.10\r\nGit commit: afdd53b\r\nBuilt: Thu Oct 26 09:08:01 2023\r\nOS\/Arch: linux\/amd64\r\nContext: default\r\n\r\nServer: Docker Engine - Community\r\nEngine:\r\nVersion: 24.0.7\r\nAPI version: 1.43 (minimum version 1.12)\r\nGo version: go1.20.10\r\nGit commit: 311b9ff\r\nBuilt: Thu Oct 26 09:08:01 2023\r\nOS\/Arch: linux\/amd64\r\nExperimental: false\r\ncontainerd:\r\nVersion: 1.6.26\r\nGitCommit: 3dd1e886e55dd695541fdcd67420c2888645a495\r\nrunc:\r\nVersion: 1.1.10\r\nGitCommit: v1.1.10-0-g18a0cb0\r\ndocker-init:\r\nVersion: 0.19.0\r\nGitCommit: de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\nWARNING: Plugin \"\/usr\/libexec\/docker\/cli-plugins\/docker-app\" is not valid: failed to fetch metadata: fork\/exec \/usr\/libexec\/docker\/cli-plugins\/docker-app: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-compose\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-compose: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-dev\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-dev: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-extension\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-extension: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-sbom\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-sbom: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-scan\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-scan: no such file or directory\r\nWARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-scout\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-scout: no such file or directory\r\n\r\nServer:\r\n Containers: 47\r\n  Running: 47\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 49\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: gb1cto7gaq68fp4resr5hep\r\n  Is Manager: true\r\n  ClusterID: kki6av7j3sbtrknvd4u4ywi\r\n  Managers: 1\r\n  Nodes: 3\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 1\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: true\r\n  Node Address: 192.168.0.7\r\n  Manager Addresses:\r\n   192.168.0.7:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.133.1-microsoft-standard-WSL2\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 15.62GiB\r\n Name: domain-Cloud\r\n ID: 242f7a18-34b8-4e10-aad4-ca740d76fc24\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 589\r\n  Goroutines: 1168\r\n  System Time: 2024-01-08T21:26:42.916274752-06:00\r\n  EventsListeners: 49\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\n```\n\n\n### Additional Info\n\n_No response_","comments":["I'm not able to reproduce;\r\n\r\n\r\nUsing this compose-file:\r\n\r\n```yaml\r\nservices:\r\n  web:\r\n    image: nginx:alpine\r\n    ports:\r\n       - 9017:80\r\n```\r\n\r\n\r\nDeploying with docker compose;\r\n\r\n\r\n```bash\r\ndocker compose up -d\r\n[+] Running 2\/2\r\n \u2714 Network publish_default  Created                                         0.0s\r\n \u2714 Container publish-web-1  Started\r\n\r\ncurl -s localhost:9017 | grep Welcome\r\n<title>Welcome to nginx!<\/title>\r\n<h1>Welcome to nginx!<\/h1>\r\n\r\n\r\ndocker compose down -v\r\n[+] Running 2\/2\r\n \u2714 Container publish-web-1  Removed                                         0.1s\r\n \u2714 Network publish_default  Removed                                         0.0s\r\n```\r\n\r\nUsing stack deploy;\r\n\r\n```bash\r\ndocker stack deploy -c docker-compose.yaml mystack\r\nCreating network mystack_default\r\nCreating service mystack_web\r\n\r\n# accessing the mapped port will connect to the service's containers through the ingress network\r\ncurl -s localhost:9017 | grep Welcome\r\n<title>Welcome to nginx!<\/title>\r\n<h1>Welcome to nginx!<\/h1>\r\n\r\ndocker stack rm mystack\r\nRemoving service mystack_web\r\nRemoving network mystack_default\r\n```\r\n\r\nBut as commented on https:\/\/github.com\/docker\/cli\/issues\/4062#issuecomment-1450547215 - when using Swarm Services, the services are published through the ingress network by default; ports for containers backing the service (\"tasks\") are not individually mapped, but instead a port is mapped for the service as a whole, and requests to that port are load-balanced to task(s) for the service.\r\n\r\nIn the `stack deploy` example;\r\n\r\n\r\nCheck that the port is mapped for the service:\r\n\r\n```bash\r\ndocker service ls\r\nID             NAME          MODE         REPLICAS   IMAGE          PORTS\r\ngpu45bt3yyuc   mystack_web   replicated   1\/1        nginx:alpine   *:9017->80\/tcp\r\n```\r\n\r\nBut note that no ports are mapped for individual tasks (containers) as \"host-mode\" publishing is not used;\r\n\r\n```bash\r\ndocker service ps mystack_web\r\nID             NAME            IMAGE          NODE             DESIRED STATE   CURRENT STATE                ERROR     PORTS\r\nzywgoqxwfn6z   mystack_web.1   nginx:alpine   docker-desktop   Running         Running about a minute ago\r\n\r\ndocker ps\r\nCONTAINER ID   IMAGE                           COMMAND                  CREATED              STATUS              PORTS                  NAMES\r\n29246d7e23c8   nginx:alpine                    \"\/docker-entrypoint.\u2026\"   About a minute ago   Up About a minute   80\/tcp                 mystack_web.1.zywgoqxwfn6z9jpgil1sgklvd\r\n```\r\n\r\n\r\nThe advanced syntax you showed in the forum uses `host` mode publishing; in that case port-mapping is done for individual containers (similar to `docker run` or `docker compose`), so no load-balancer is involved, and ports are only accessible on the host where the task runs (and as a side-effect; only a single task can be deployed on a host).\r\n","@thaJeztah \r\n\r\nOK, thanks for the detailed explanation. It's a bit of a confusing method, since the short syntax literally means to publish a port to the host, and docker stack deploy doesn't cough up any errors \/warnings\/notices when attempting to use it.\r\n\r\nI also can't seem to find any official docs detailing that nuance; are you aware of any that you can link me to?"],"labels":["status\/0-triage","status\/more-info-needed","area\/networking\/portmapping","version\/24.0"]},{"title":"builder-next: builder uses ChecksumForGraphID, which always produces error (?)","body":"### Description\n\n- relates to https:\/\/github.com\/moby\/moby\/pull\/17924\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/37151\r\n\r\nI stumbled upon this code, and from the looks of it, this will _always_ produce an error, so either this code is completely broken, or it's just dead-code (the `ChecksumForGraphID` is in a \"migration\" file, which may have been related to migrating \"v1\" images to \"v2\", so perhaps all of that can be removed, but I'm not familiar with the code, so can't tell for sure (see https:\/\/github.com\/moby\/moby\/commit\/a8f88ef4036d22aa1feb1de8e86d92371bcd5b67); code was implemented as part of the \"v2\" image (content-addressable images) migration; https:\/\/github.com\/moby\/moby\/pull\/17924\r\n\r\nThe only use of this method is in `builder\/builder-next\/adapters\/snapshot.EnsureLayer()`; https:\/\/github.com\/moby\/moby\/blob\/7082aecd540db38ef48c730505ca3a37bdfca32a\/builder\/builder-next\/adapters\/snapshot\/layer.go#L81\r\n\r\nThat code always calls the function with an _empty_ `oldTarDataPath`, which means that the function terminates immediately, and returns `no tar-split file`; https:\/\/github.com\/moby\/moby\/blob\/7082aecd540db38ef48c730505ca3a37bdfca32a\/layer\/migration.go#L16-L26\r\n\r\nDoes that mean we'd always return an error here? https:\/\/github.com\/moby\/moby\/blob\/7082aecd540db38ef48c730505ca3a37bdfca32a\/builder\/builder-next\/adapters\/snapshot\/layer.go#L85-L87\r\n\r\nThe builder-next code was introduced in https:\/\/github.com\/moby\/moby\/commit\/27fa0e8a7b9b922dc3955cc2cee82b4fefc331c4 (part of the initial BuildKit integration; https:\/\/github.com\/moby\/moby\/pull\/37151)","comments":["@tonistiigi @dmcgowan PTAL","@rumpl had a good eye; the `no tar-split file` is never returned, because the error is _overwritten_ in the `defer`, which makes this function (for all our uses) an alias for `ls.checksumForGraphIDNoTarsplit`"],"labels":["area\/builder","area\/storage","area\/images","area\/builder\/buildkit"]},{"title":"gha: add CodeQL Analysis workflow","body":"copied from https:\/\/github.com\/docker\/cli\/blob\/88e6474350e644495a8009e9e1437332aa828a17\/.github\/workflows\/codeql.yml\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["This pull request sets up GitHub code scanning for this repository. Once the scans have completed and the checks have passed, the analysis results for this pull request branch will appear on [this overview](\/moby\/moby\/security\/code-scanning?query=pr%3A47034+is%3Aopen). Once you merge this pull request, the 'Security' tab will show more code scanning analysis results (for example, for the default branch). Depending on your configuration and choice of analysis tool, future pull requests will be annotated with code scanning analysis results. For more information about GitHub code scanning, check out [the documentation](https:\/\/docs.github.com\/code-security\/code-scanning\/automatically-scanning-your-code-for-vulnerabilities-and-errors\/about-code-scanning). ","Ah, this probably requires some additional trickery to make it work;\r\n\r\n```\r\n2024\/01\/07 12:00:22 Warning: encountered errors extracting package `github.com\/moby\/moby\/api\/server\/backend\/build`:\r\n2024\/01\/07 12:00:22   -: code in directory \/home\/runner\/work\/moby\/moby\/root\/src\/github.com\/moby\/moby\/api\/server\/backend\/build expects import \"github.com\/docker\/docker\/api\/server\/backend\/build\"\r\nError: 2024\/01\/07 12:00:22   \/home\/runner\/work\/moby\/moby\/root\/src\/github.com\/moby\/moby\/api\/server\/backend\/build\/backend.go:48:27: cannot use s (variable of type *\"github.com\/moby\/moby\/vendor\/google.golang.org\/grpc\".Server) as *\"github.com\/docker\/docker\/vendor\/google.golang.org\/grpc\".Server value in argument to b.buildkit.RegisterGRPC\r\n```"],"labels":["status\/2-code-review","area\/testing"]},{"title":"Unable to run image with older architecture arm v6 (docker engine v24.0.7)","body":"### Description\r\n\r\nFailed to run image built with arm v6 architecture. No issue running image built with arm64 v8 architecture. Encounter error message below when attempt to start the container:\r\n```\r\nbash: error while loading shared libraries: libtinfo.so.6: ELF load command address\/offset not page-aligned\r\n```\r\n\r\n### Reproduce\r\n\r\nEnvironment: Raspberry Pi 5 bookworm os 64bit\r\n```\r\ncat \/etc\/os-release\r\nPRETTY_NAME=\"Debian GNU\/Linux 12 (bookworm)\"\r\nNAME=\"Debian GNU\/Linux\"\r\nVERSION_ID=\"12\"\r\nVERSION=\"12 (bookworm)\"\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=\"https:\/\/www.debian.org\/\"\r\nSUPPORT_URL=\"https:\/\/www.debian.org\/support\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.debian.org\/\"\r\n```\r\n\r\nExecute `docker run -it --rm --entrypoint bash balenalib\/raspberry-pi-debian:bookworm` and the error below will appear. \r\n```\r\nbash: error while loading shared libraries: libtinfo.so.6: ELF load command address\/offset not page-aligned\r\n```\r\n\r\n### Expected behavior\r\n\r\nAfter execute `docker run -it --rm --entrypoint bash balenalib\/raspberry-pi-debian:bookworm`, we should be able to enter the container shell.\r\n```\r\npi@raspberry:\/ $ docker run -it --rm --entrypoint bash balenalib\/raspberry-pi-debian:bookworm\r\nroot@5a962c34caa3:\/#\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:08:15 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:15 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.26\r\n  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 4\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 6.1.0-rpi7-rpi-2712\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.861GiB\r\n Name: raspberrypi\r\n ID: d1e34987-e19b-4b68-ba91-717422f9ce9b\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["From a quick glance, this seems to be an issue with [libtinfo](https:\/\/packages.debian.org\/search?keywords=libtinfo) inside the container.\r\n\r\nI tried reproducing this on my m1 (arm64) mac with Docker Desktop, but it looks to be working there, however, it's possible that QEMU is used there. It _could_ be something to seccomp; could you try if it works if you run the container with seccomp  disabled (\"unconfined\")?\r\n\r\n```bash\r\ndocker run -it --rm --entrypoint bash --security-opt seccomp=unconfined balenalib\/raspberry-pi-debian:bookworm\r\n```\r\n\r\n\r\n\r\n\r\n\r\n","Hi @thaJeztah , I'm getting the same error message as reported in the original post.\r\n\r\n```\r\npi@raspberry:~ $ docker run -it --rm --entrypoint bash --security-opt seccomp=unconfined balenalib\/raspberry-pi-debian:bookworm\r\nbash: error while loading shared libraries: libtinfo.so.6: ELF load command address\/offset not page-aligned\r\n```"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/24.0"]},{"title":"docker: Error response from daemon: failed to mkdir \/var\/lib\/docker\/volumes\/...\/_data\/.cache\/...: mkdir \/var\/lib\/docker\/volumes\/...\/_data\/.cache\/...: file exists.","body":"### Description\n\nIt seems like there is race condition when overlying folders in the container with same named volume in multiple containers at once.\r\n\r\nThis doesn't seem to happen if the container image does not have `\/root\/.cache` folder already in there.\r\n\r\nSeems like others have already run into this issue as well https:\/\/github.com\/kubernetes-sigs\/kind\/issues\/974 , also in CI, since this is where concurrent use of the same volume is quite common.\n\n### Reproduce\n\n```\r\necho -e \"FROM ubuntu:22.04\\nRUN mkdir -p \/root\/.cache\/a\/b\/c\/d\/e\" > Dockerfile\r\ndocker build . -t mkdir_error\r\n\r\n# this is not deterministc, so we execute it multiple times until we see an error, for me it happens at least 1 per 10 tries\r\n# it also seems to happen ONCE per named volume, hence why I create a new one every time for each test\r\nwhile true;\r\ndo ; \r\n  export VOL_I=$(($VOL_I+1));\r\n  for i in {1..4}; do docker run --rm --volume test$VOL_I:\/root mkdir_error mkdir -p \/root\/.cache 2>\/dev\/stderr &; done;\r\n  wait;\r\ndone;\r\n```\n\n### Expected behavior\n\nIt should work every time without an error.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.26\r\n  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1708\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 1706\r\n Images: 42\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: zfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-39-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 23.87GiB\r\n Name: reef-vm\r\n ID: eb2f1665-586b-4027-a6ef-4aedecefbaee\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nFirst time I seen this error (and why I pursued to reproduce it) was on Github Actions CI.","comments":["Possibly related to #43068"],"labels":["status\/0-triage","kind\/bug"]},{"title":"Move StartedAt time to before starting the container","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\nFixes #45445\r\n\r\n**- What I did**\r\nMoby reports the StartedAt and FinishedAt times of containers. The StartedAt time is inaccurate as it is set too late in the container startup process, being set way past the moment the container is started. This not only makes the StartedAt be reported as relatively late, and the difference between StartedAt and FinishedAt be too optimistic, but with very short-lived containers the StartedAt is reported later than the FinishedAt, meaning the container took a negative amount of time to complete.\r\n\r\nThis MR moves the StartedAt time recording to right before the container is told to start. This makes the code slightly more complex, as the StartedAt time now has to be passed to the moment it can be recorded, but improves the accuracy of the StartedAt time.\r\n\r\nHello-world prior to the change:\r\n\"StartedAt\": \"2023-12-28T22:15:43.373989192Z\",\r\n\"FinishedAt\": \"2023-12-28T22:15:43.373677346Z\"\r\n\r\nHello-world after the change:\r\n\"StartedAt\": \"2023-12-30T14:19:39.241397794Z\",\r\n\"FinishedAt\": \"2023-12-30T14:19:39.423867882Z\"\r\n\r\n**- How I did it**\r\nI record the start time using time.Now() right before `ctr.Start`. I then pass this variable to the `SetRunning` function, which is called later and actually records the StartedAt time. Previously, this time was generated within the SetRunning function. For all unit tests and the health monitor, I simply generate the time right on the function call (So I pass time.Now() as a parameter). For unit tests, the StartedAt time doesn't matter, and for the monitor, I could not find a better place to record the time prior to SetRunning.\r\n\r\n**- How to verify it**\r\n- Run tests to verify that tests have been properly adapted to this new parameter.\r\n- Include the built version in dockerd using this process: https:\/\/github.com\/moby\/moby\/blob\/master\/docs\/contributing\/set-up-dev-env.md. Then you can run containers and see the StartedAt and FinishedAt times using `docker inspect`.\r\n\r\n**- Description for the changelog**\r\n```markdown changelog\r\nContainer's `StartedAt` property is now recorded before container startup, guaranteeing that the `FinishedAt` is always before `StartedAt`. \r\n```\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n![20170629_165245](https:\/\/github.com\/moby\/moby\/assets\/60571459\/367e7293-70c3-4c48-aef4-739d881278f2)\r\n","comments":["\/cc @cpuguy83 PTAL if this is what you had in mind; https:\/\/github.com\/moby\/moby\/issues\/45445#issuecomment-1529885622\r\n\r\n> It looks like the issue is that it sets the started at time (and generate said time) after it is started meanwhile the exit event may occur before that start time is even generated.\r\n\r\nI must  admit though that I'm not \"stoked\" about having to pass the extra arguments; do we know exactly what code-path triggers this? Is this tapering over a bug? (curious if this is because we're setting this state in 2 separate ways; once in `containerStart` but also in `monitor.go` (`daemon.ProcessEvent`))","I don't think we're tapering over a bug. If the monitor was setting the state and overriding the containerStart, then moving the containerStart time earlier wouldn't have fixed the bug, as the monitor would have still overridden it and we would have gotten the exact same results as before.\r\n\r\nFrom what I can see it's simply just the issue of there being a lot of code between the actual container start command, and the moment we generate the time prior to this MR. This code takes about 0.2 seconds to execute on my PC, which puts the starting time of the container about 0.2 seconds after the time the container is actually started.\r\n\r\nMy team is trying to time the execution time of arbitrary untrusted code for our product, so we would really like to use docker's internal timing as it's more accurate than the external measuring we're currently doing, but that's not possible when the results are inconsistent and return negative time differences.","> From what I can see it's simply just the issue of there being a lot of code between the actual container start command, and the moment we generate the time\r\n\r\nRight, but it looks like this PR changes the semantics of the \"StartedAt\" field; before this PR, the `StartedAt` would reflect the time at which we change the container's state (`SetRunning`) (\"container process is started\") was updated. After this PR, the field reflects the time at which the start was _initiated_.\r\n\r\nBoth can make sense on their own, but they _do_ carry a different meaning. Unfortunately these are areas that originated from early in the project, where intended behavior was not always documented (if _at all_), but other code in the same area seems to confirm that the intent was \"started\" (and running), as Prometheus counters, and container events are also updated there; https:\/\/github.com\/moby\/moby\/blob\/1f6c42c678b1879203fc116431da798cdefd6348\/daemon\/start.go#L214-L227\r\n\r\nHowever, and it looks like this is where there's a discrepancy; if the process exits _before_ it's considered started, then `FinishedAt` is set before (?), which is inconsistent as for that case (apparently) the container _is_ considered started (and exited). Which is also why I was looking at the `monitor` code which seems to have its own opinion on \"StartedAt\" (based on events), and that lead me to the question \"why are both trying to manage the same state?\"\r\n\r\n`SetRunning` looks to be doing _a lot_ - perhaps _too much?_, and the `initial` argument on `SetRunning()` at least looks dodgy (I'd have to dig why it was added); perhaps there's something to look into there as well \ud83e\udd14 ","Is there anything I can do to help out with this?","> Right, but it looks like this PR changes the semantics of the \"StartedAt\" field; before this PR, the `StartedAt` would reflect the time at which we change the container's state (`SetRunning`) (\"container process is started\") was updated. After this PR, the field reflects the time at which the start was _initiated_.\r\n> ..., but other code in the same area seems to confirm that the intent was \"started\" (and running)...\r\n\r\nLooking at the code (but not being an expert), it now seems like the `StartedAt` time reflects \"some time after we initiated the start and _surely_ it will be running now\", but it certainly is not the exact time at which the program actually started running. Since the current code isn't exactly correct with intended semantics anyway, I would err on the earlier side to at least ensure that `StartedAt` is always before `FinishedAt` - any program will always have a nonzero runtime, so a negative one just does not really make sense.","What's the status on this? What are the next steps to take here?","I would also be curious to the status of this PR","Personally it's LGTM after addressing https:\/\/github.com\/moby\/moby\/pull\/47003#pullrequestreview-1829536312\r\n\r\nBut @thaJeztah  @cpuguy83 might have other thoughts here","@vvoland would you be able to review this PR again? I implemented your feedback.","@vvoland I implemented your changes.","Thanks \u2764\ufe0f \r\nCode looks good now, just one last thing - can you please squash the commits into one?\r\n","Done :)"],"labels":["status\/2-code-review","impact\/changelog","area\/daemon","kind\/bugfix"]},{"title":"When using containerd snapshotter docker daemon doesn't check for \"Already Exists\"","body":"### Description\r\n\r\nWhen using the containerd snapshotter interface the `daemon\/create.go` code calls the `PrepareSnapshotter()` function which in turn invokes the containerd `Prepare()` API. It is possible to get a non-nil `err` value returned which is not a true error. That is, the snapshotter is able to return `ErrAlrExists` (already exists) to tell containerd that the snapshot has been prepared. In our case we have a caching snapshotter that has prepared snapshots in advance. \r\n\r\nI note the code in `daemon\/create.go` doesn't differentiate a \"bad\" err value from a \"good\" one:\r\n```\r\n        if daemon.UsesSnapshotter() {\r\n                if err := daemon.imageService.PrepareSnapshot(ctx, ctr.ID, opts.params.Config.Image, opts.params.Platform, setupInitLayer(daemon.idMapping)); err != nil {\r\n                        return nil, err\r\n                }\r\n```\r\n\r\nI am also seeing `docker images` does not report the newly pulled image for images in our content store. The way our content store works is that when containerd calls the `Info()` we check our cache and if it exists we symlink the cache entry into the content store and tell the caller the layer exists otherwise it goes through normal pull processing. So I am assuming that the docker pull is seeing a layer exists and just ignores the rest of the processing such that its own internal db is not fully updated. I say fully as while `docker image` doesn't show the image a `docker run` is fine.\r\n\r\nIn the following console log I pull an image that's in our cache (we have both a content store plugin and snapshotter plugin) and one that's not in our cache. I am able to run the cached image but `docker images` doesn't report it:\r\n```\r\n> docker pull fedora:latest\r\n06df381d697d: Download complete\r\ndfb5e6183f51: Download complete\r\n8404925a71fd: Download complete\r\ndocker.io\/library\/fedora:latest\r\n> docker images\r\nREPOSITORY   TAG       IMAGE ID   CREATED   SIZE\r\n> docker pull python:latest\r\n3733015cdd1b: Download complete\r\n553d2d59b9c4: Download complete\r\nfc7a60e86bae: Download complete\r\n107007f161d0: Download complete\r\nd685eb68699f: Download complete\r\nbc0734b949dc: Download complete\r\nb5de22c0f5cd: Download complete\r\n02b85463d724: Download complete\r\nb43bd898d5fb: Download complete\r\n7fad4bffde24: Download complete\r\n917ee5330e73: Download complete\r\ndocker.io\/library\/python:latest\r\n> docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED         SIZE\r\npython       latest    3733015cdd1b   4 seconds ago   1.46GB\r\n> docker run --rm -it fedora:latest pwd\r\n\/\r\n>\r\n```\r\n\r\n### Reproduce\r\n\r\nI am able to reproduce it on my system where our content store and snapshotter plugins reside but these are not public code (yet).\r\n\r\n### Expected behavior\r\n\r\n`docker images` should display images that are pulled when the snapshotter reports \"Already Exists\".\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:53:10 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:52:10 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.18.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: 24.0.2\r\n Storage Driver: mssnap\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-477.10.1.el8_8.x86_64\r\n Operating System: AlmaLinux 8.8 (Sapphire Caracal)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 1.894GiB\r\n Name: mssnap.devlab.sinenomine.net\r\n ID: 5bcd4245-c020-40e6-acb1-b8d26235ddcb\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Quick \"off-the-bat\" reply to the second part of your description;\r\n\r\n> I am also seeing docker images does not report the newly pulled image for images in our content store. The way our content store works is that when containerd calls the Info() we check our cache and if it exists we symlink the cache entry into the content store and tell the caller the layer exists otherwise it goes through normal pull processing.\r\n\r\nFor transparency; the current implementation of the containerd image-store integration is largely written with the assumption that the \"moby\" namespace(s) are \"owned\" by the docker engine, and not manipulated separately.\r\n\r\nWe're aware that this is not _ideal_, and have discussed making this more permissive in future (to allow for scenarios where multiple tools act on the same namespace), but it's not been our focus of attention for the initial implementation. As you mentioned, currently some state is kept in the Docker Engine itself; there is some synchronisation (e.g. container state), but there's likely many scenarios where this _currently_ won't work well with other tools managing the data.\r\n\r\nPerhaps it would be good to create a separate ticket for this with further details so that it can be tracked and discussed separately.\r\n\r\n(if \"making this work\" would require limited changes, then contributions on improving would be welcome though!)","> I note the code in daemon\/create.go doesn't differentiate a \"bad\" err value from a \"good\" one:\r\n\r\nThanks for reporting; (also at a quick glance), that _seems_ like an ok change to make. \r\n\r\nFor handling these error-types, we should have a look to either handle that error immediately, and handle containerd's error-type ([errdefs.ErrAlreadyExists](https:\/\/github.com\/containerd\/containerd\/blob\/v1.7.11\/errdefs\/errors.go#L45)) \r\n\r\nOr if we need to handle it further \"up the stack\", see if there's a suitable parallel in Moby's \"errdefs\" (although I don't think there's s direct 1:1 equivalent; https:\/\/github.com\/moby\/moby\/blob\/46f7ab808b9504d735d600e259ca0723f76fb164\/errdefs\/defs.go#L1\r\n\r\n\r\n@nealef As you already dug into the code, are you interested in contributing?\r\n","> > I note the code in daemon\/create.go doesn't differentiate a \"bad\" err value from a \"good\" one:\r\n> \r\n> Thanks for reporting; (also at a quick glance), that _seems_ like an ok change to make.\r\n> \r\n> For handling these error-types, we should have a look to either handle that error immediately, and handle containerd's error-type ([errdefs.ErrAlreadyExists](https:\/\/github.com\/containerd\/containerd\/blob\/v1.7.11\/errdefs\/errors.go#L45))\r\n> \r\n> Or if we need to handle it further \"up the stack\", see if there's a suitable parallel in Moby's \"errdefs\" (although I don't think there's s direct 1:1 equivalent;\r\n> \r\n> https:\/\/github.com\/moby\/moby\/blob\/46f7ab808b9504d735d600e259ca0723f76fb164\/errdefs\/defs.go#L1\r\n> \r\n> @nealef As you already dug into the code, are you interested in contributing?\r\n\r\nI was looking at something like:\r\n```\r\n--- a\/daemon\/create.go\r\n+++ b\/daemon\/create.go\r\n@@ -8,6 +8,7 @@ import (\r\n        \"time\"\r\n\r\n        \"github.com\/containerd\/containerd\/platforms\"\r\n+       ctrerr \"github.com\/containerd\/containerd\/errdefs\"\r\n        \"github.com\/containerd\/log\"\r\n        \"github.com\/docker\/docker\/api\/types\/backend\"\r\n        containertypes \"github.com\/docker\/docker\/api\/types\/container\"\r\n@@ -198,7 +199,7 @@ func (daemon *Daemon) create(ctx context.Context, daemonCfg *config.Config, opts\r\n        ctr.ImageManifest = imgManifest\r\n\r\n        if daemon.UsesSnapshotter() {\r\n-               if err := daemon.imageService.PrepareSnapshot(ctx, ctr.ID, opts.params.Config.Image, opts.params.Platform, setupInitLayer(daemon.idMapping)); err != nil {\r\n+               if err := daemon.imageService.PrepareSnapshot(ctx, ctr.ID, opts.params.Config.Image, opts.params.Platform, setupInitLayer(daemon.idMapping)); err != nil && err != ctrerr.ErrAlreadyExists {\r\n                        return nil, err\r\n                }\r\n        } else {\r\n```","> Quick \"off-the-bat\" reply to the second part of your description;\r\n> \r\n> > I am also seeing docker images does not report the newly pulled image for images in our content store. The way our content store works is that when containerd calls the Info() we check our cache and if it exists we symlink the cache entry into the content store and tell the caller the layer exists otherwise it goes through normal pull processing.\r\n> \r\n> For transparency; the current implementation of the containerd image-store integration is largely written with the assumption that the \"moby\" namespace(s) are \"owned\" by the docker engine, and not manipulated separately.\r\n> \r\n> We're aware that this is not _ideal_, and have discussed making this more permissive in future (to allow for scenarios where multiple tools act on the same namespace), but it's not been our focus of attention for the initial implementation. As you mentioned, currently some state is kept in the Docker Engine itself; there is some synchronisation (e.g. container state), but there's likely many scenarios where this _currently_ won't work well with other tools managing the data.\r\n> \r\n> Perhaps it would be good to create a separate ticket for this with further details so that it can be tracked and discussed separately.\r\n> \r\n> (if \"making this work\" would require limited changes, then contributions on improving would be welcome though!)\r\n\r\nI'll create a separate ticket and continue to look at the `docker images` processing to see if things are there and are just getting filtered out for some reason.\r\n\r\nI also wonder whether `moby` is over complicating the containerd stuff. k3s and k8s can also use containerd APIs to pull images but neither appears to need to go to the `Prepare()` or `Stat()` level. k3s uses the the imageserver Pull interface which will worry about unpacking and preparing and simply return the Image details back to the caller. However, I am new to the moby internals so there are evidently very good reasons for doing things this way. ","> I was looking at something like:\r\n\r\nHm.. wondering if perhaps that check should live somewhere in `imageservice.PrepareSnapshot()`. It feels overly broad to ignore _any_ `ctrerr.ErrAlreadyExists` there (there could be other _unexpected_ paths that return that error). It feels like it should be handled more specifically inside that function.\r\n\r\n> I also wonder whether moby is over complicating the containerd stuff. k3s and k8s can also use containerd APIs to pull images but neither appears to need to go to the Prepare() or Stat() level. k3s uses the the imageserver Pull interface which will worry about unpacking and preparing and simply return the Image details back to the caller. However, I am new to the moby internals so there are evidently very good reasons for doing things this way.\r\n\r\nI haven't looked into those details yet, but know that plans are to move more functionality to containerd code where possible. That said, there's been quite some handling specific to moby that had to be accounted for, and not all features required to handle that was available (yet) in containerd's code.\r\n\r\n\/cc @dmcgowan @vvoland who may have more ideas here.\r\n","> I also wonder whether `moby` is over complicating the containerd stuff. k3s and k8s can also use containerd APIs to pull images but neither appears to need to go to the `Prepare()` or `Stat()` level. k3s uses the the imageserver Pull interface which will worry about unpacking and preparing and simply return the Image details back to the caller. However, I am new to the moby internals so there are evidently very good reasons for doing things this way.\r\n\r\nIt looks like this mostly because we want the switch to the containerd image store to be as transparent as possible and not break anyone, which means we have to reimplement everything the same way it already works. And sometimes the containerd API just doesn't have what we need (reference counting the mounts for example).\r\n","As for the pulled image not being listed, could you restart dockerd with debug logging enabled and give us the logs after you run `docker images`? There are cases in image listing where we silently ingore things we can't list, logs could help","> As for the pulled image not being listed, could you restart dockerd with debug logging enabled and give us the logs after you run `docker images`? There are cases in image listing where we silently ingore things we can't list, logs could help\r\n\r\n```\r\nDec 27 18:06:17 snap.lab.sinenomine.net dockerd[120493]: time=\"2023-12-27T18:06:17.159563586-05:00\" level=debug msg=\"Calling HEAD \/_ping\"\r\nDec 27 18:06:17 snap.lab.sinenomine.net dockerd[120493]: time=\"2023-12-27T18:06:17.160959857-05:00\" level=debug msg=\"Calling GET \/v1.43\/images\/json\"\r\n```\r\nThat's all there is in the dockerd log. \r\n\r\nRunning an `strace` on the `docker images` command I see the GET command and the empty response:\r\n```\r\n124247 write(3, \"GET \/v1.43\/images\/json HTTP\/1.1\\r\\nHost: docker\\r\\nUser-Agent: Docker-Client\/24.0.2 (linux)\\r\\n\\r\\n\", 91) = 91\r\n: \r\n124246 <... read resumed>\"HTTP\/1.1 200 OK\\r\\nApi-Version: 1.44\\r\\nContent-Type: application\/json\\r\\nDocker-Experimental: false\\r\\nOstype: linux\\r\\nServer: Docker\/dev (linux)\\r\\nDate: Wed, 27 Dec 2023 23:09:00 GMT\\r\\nContent-Length: 3\\r\\n\\r\\n[]\\n\", 4096) = 200\r\n```","Strace on the cli won\u2019t get you any useful information no, the daemon is the one doing all the work. \r\n\r\nNot having any logs when image list is called is helpful though, we\u2019ll take a closer look. \r\n\r\nIf we don\u2019t find anything, would you mind helping us debug this issue? I understand your code is proprietary for now but if we gave you a patch with some extra logs in the daemon would that work for you? ","I am attempting to instrument `getImagesJSON` to see what it's seeing. It calls `ir.backend.Images` and I am trying to figure what that resolves to.","The following test is failed by the cached image:\r\n```\r\nfunc (im *ImageManifest) CheckContentAvailable(ctx context.Context) (bool, error) {\r\n        \/\/ The target is already a platform-specific manifest, so no need to match platform.\r\n        pm := cplatforms.All\r\n\r\n        available, _, _, missing, err := containerdimages.Check(ctx, im.ContentStore(), im.Target(), pm)\r\n        if err != nil {\r\n                return false, err\r\n        }\r\n\r\n        if !available || len(missing) > 0 {\r\n                for _, miss := range(missing) {\r\n                        log.G(ctx).Infof(\"missing: %v\", miss)\r\n                }\r\n                return false, nil\r\n        }\r\n\r\n        return true, nil\r\n}\r\n```\r\n`missing` is non-zero and contains:\r\n```\r\napplication\/vnd.docker.image.rootfs.diff.tar.gzip sha256:8237fce9fd6b5acc12e7010d84e6245c6b00937de832c1987c4769866af1573a 64872433 [] map[] [] <nil>\r\n```\r\nThis looks like my content store problem. This layer should exist in content store. It exists in my cache so when the content store server calls my content store's `Info()` function I should locate it in the cache and symlink it to the content store's area.","It appears that when an image doesn't exist we do the pull and then a commit. It is during the commit that the content store's `Info()` API is invoked. It is at this point my caching content store will setup the symlink to the cache (if present in the cache). However, if the image does exist pull just says image pulled and we don't commit. I wonder if I could add code to invoke `Info()` for its layers. This should have no effect if the image exists and its not cached (in fact if the symlink is already present then calling `Info()` on a cached image will have no effect either).\r\n\r\nI am seeing `Info()` being called for the list of manifests, the configuration, and amd64 manifest but not the diff layer.","Also, you are using the `client.Pull()` API. When unpacking is specified the snapshotter is invoked and the snapshotter will prepare a snapshot (see the `Unpack()` function in `containerd\/image.go`). So subsequent explicit invocation of the snapshotter seems redundant. Are you gathering information as well as creating a snapshot? Would a `Stat()` suffice?","What I believe is happening is that during the pull when it decides whether to pull in a diff it checks the snapshot area for a corresponding diff (for the 1st layer) or chainid (for subsequent layers), if it finds it (via the `Stat()` call) it doesn't bother to download the gzipped layer (or in the case of our content store create a symlink to the cache) because it assumes the compressed layer must be in the content store. Therefore the content store doesn't have that\/those layer(s) so when we try a `docker images` it does a `Check()` operation and it doesn't find the layers so does not return information for that image. \r\n\r\nSo this is not a problem with docker but the way containerd works. However, does docker need to do the manifest check before it will return details of the image? Could it do the same check(s) it does for `docker inspect` or `docker run` - both of which work without they layer existing in the content store?\r\n\r\nAs an experiment I commented out the `CheckContentAvailability()` related code in `daemon\/containerd\/image_list.go` and `docker images` worked fine. However, I am not familiar enough with the rest of the code to see what the side-effects of this may be though I note `image_list.go` is the only code that makes that call."],"labels":["status\/0-triage","kind\/bug","area\/images","containerd-integration"]},{"title":"Please cut a new release","body":"### Description\n\nTrying to upgrade from 20.10.24+incompatible to 24.0.7+incompatible (due to security vulnerabilities) we are unable to merge these updates because of incompatible changes that have since been addressed by #46628.\r\n\r\nCould you please cut a new release with this update so that we (and others) can upgrade our code base accordingly?\r\n\r\nSorry for using the bug form for reporting this, but I didn't feel that any of the other options were relevant.\n\n### Reproduce\n\nNot relevant.\n\n### Expected behavior\n\nNot relevant.\n\n### docker version\n\n```bash\nNot relevant.\n```\n\n\n### docker info\n\n```bash\nNot relevant.\n```\n\n\n### Additional Info\n\n_No response_","comments":["Thanks for reporting; I guess we didn't catch that this was a regression in 23.0.x \/ 24.0.x.\r\n\r\nWe have not (yet) planned a new patch release for 24.0.x, but I prepared backports of that PR;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46993\r\n- https:\/\/github.com\/moby\/moby\/pull\/46994\r\n\r\nThat patch should already be included in the v25.0.0-beta.3 pre-release, which could be an alternative.\r\n"],"labels":["status\/0-triage","kind\/bug","version\/24.0"]},{"title":"Build fails on Mac","body":"### Description\n\nSeems that ever since #44210 was merged in 24.0.0 build on Mac machines fail due to the missing `goInChroot` function.\n\n### Reproduce\n\n`main.go`:\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\t\"github.com\/docker\/docker\/layer\"\r\n)\r\n\r\nfunc main() {\r\n\tvar v layer.DiffID\r\n\t_ = v\r\n\tfmt.Println(\"OK\")\r\n}\r\n```\r\n\r\n```sh\r\n$ GOOS=darwin go build main.go\r\n# github.com\/docker\/docker\/pkg\/chrootarchive\r\n..\/..\/..\/go\/pkg\/mod\/github.com\/docker\/docker@v24.0.7+incompatible\/pkg\/chrootarchive\/archive_unix.go:31:8: undefined: goInChroot\r\n..\/..\/..\/go\/pkg\/mod\/github.com\/docker\/docker@v24.0.7+incompatible\/pkg\/chrootarchive\/archive_unix.go:53:8: undefined: goInChroot\r\n..\/..\/..\/go\/pkg\/mod\/github.com\/docker\/docker@v24.0.7+incompatible\/pkg\/chrootarchive\/diff_unix.go:45:8: undefined: goInChroot\r\n```\r\n\r\nBut on Linux it works just fine:\r\n```sh\r\n$ GOOS=linux go run main.go\r\nOK\r\n```\n\n### Expected behavior\n\nProject should compile on Darwin like it did before.\n\n### docker version\n\n```bash\nNot relevant, it's a build issue.\n```\n\n\n### docker info\n\n```bash\nNot relevant, it's a build issue.\n```\n\n\n### Additional Info\n\nCurrent workaround is to downgrade to v23:\r\n```\r\nreplace github.com\/docker\/docker => github.com\/docker\/docker v23.0.8+incompatible\r\n```","comments":["I see this was fixed by #45724 and #46094. Maybe these could be backported to v24 until v25 goes out of beta?","Do you have more information how this package is used in your code-base on macOS? While those linked PRs fix the compile issue, those packages are intended for internal use in the daemon, and never designed to be used on macOS, so I'm curious how this code ended up in macOS code in your project."],"labels":["status\/0-triage","kind\/enhancement","status\/more-info-needed"]},{"title":"Separate IP Masquerade setting for ipv4 and ipv6","body":"### Description\n\nCurrently we have a option for bridge network to control the ip masquerade for container public traffic:\r\n`com.docker.network.bridge.enable_ip_masquerade`\r\nAs we cannot have ipv6 only networks #32850 we are not able to disable masquerade only for ipv6 traffic.\r\nSetting this option to `false` causes container in this network to not being able to have ipv4 connections.\r\n\r\nThis is a common use case when we come to ipv6. Because we have a lot of ipv6 addresses and each container can speak with  own public ipv6 address but we have limited ipv4 addresses and we should use nat for container public ipv4 traffic(so we need masquerade).","comments":["I found this line is checking this option:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/46f7ab808b9504d735d600e259ca0723f76fb164\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go#L280\r\n\r\nAn idea is that we add another option with name `enable_ipv6_masquerade` and add something like below to this condition:\r\n```\r\n|| (ipver == iptables.IPv6 && config.EnableIPv6Masquerade)\r\n```"],"labels":["status\/0-triage","kind\/feature","area\/networking","area\/networking\/ipv6"]},{"title":"Improve a few comments in daemon, libnet & api pkgs","body":"**- What I did**\r\n\r\nThese comments were collected over the past few months while working on other branches. Improving thse comments make it a little easier to understand how all these pieces fit together.","comments":["cc @robmry "],"labels":["area\/api","status\/2-code-review","area\/networking","area\/daemon","kind\/refactor"]},{"title":"daemon: remove getNetworkID() helper","body":"**- What I did**\r\n\r\n- Related to https:\/\/github.com\/moby\/moby\/pull\/46251\r\n\r\nThis helper was originally introduced in commit cafed80 to address an issue related to duplicated network names. As we don't allow such duplicates anymore, this helper can be removed.\r\n","comments":["cc @robmry ","Looks like build is failing;\r\n\r\n```\r\n52.99 daemon\/container_operations.go:235:9: v declared and not used\r\n52.99 daemon\/container_operations.go:297:12: v declared and not used\r\n52.99 daemon\/container_operations.go:364:37: undefined: id\r\n52.99 daemon\/container_operations.go:365:45: undefined: id\r\n52.99 daemon\/container_operations.go:375:54: undefined: id\r\n52.99 daemon\/container_operations.go:381:31: undefined: id\r\n52.99 daemon\/container_operations.go:384:52: undefined: id\r\n```"],"labels":["status\/2-code-review","area\/networking","area\/daemon","kind\/refactor"]},{"title":"Unskip docker-py commit tests","body":"- needs: https:\/\/github.com\/docker\/docker-py\/pull\/3203\r\n\r\nThese tests were skipped temporarily. Restore them once we get a new docker-py version\r\n\r\nTODO:\r\n- [x] Merge https:\/\/github.com\/docker\/docker-py\/pull\/3203\r\n- [ ] Wait for the new docker-py release that includes the above\r\n- [ ] Update the docker-py in `hack\/make\/test-docker-py` to the new release\r\n- [ ] Revert https:\/\/github.com\/moby\/moby\/pull\/46966\r\n\r\n","comments":[],"labels":["kind\/bug"]},{"title":"client: deprecate old API versions","body":"- related to https:\/\/github.com\/moby\/moby\/pull\/46887, but for the client\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["\r\nNeed to looks at how we set API versions in some tests; looks like we do so _including_ a `v` prefix (which current validation doesn't accept)\r\n\r\n```\r\n\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === Failed\r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestGetAPIPath (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:180: assertion failed: error is not nil: API version must be provided without \"v\" prefix\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPIVersionEmpty (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:272: assertion failed: 1.25 (string) != 1.24 (expected string)\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPIVersion\/downgrade_from_default (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:338: assertion failed: 1.21 (tc.expectedVersion string) != 1.44 (string)\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPIVersion\/downgrade_legacy (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:338: assertion failed: 1.24 (tc.expectedVersion string) != 1.44 (string)\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPIVersion\/downgrade_old (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:338: assertion failed: 1.19 (tc.expectedVersion string) != 1.44 (string)\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPIVersion (0.00s)\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestNegotiateAPVersionOverride (0.00s)\r\n[2023-12-18T18:57:28.211Z]     client_test.go:350: assertion failed: error is not nil: maximum supported API version is 1.44: 9.99\r\n[2023-12-18T18:57:28.211Z] \r\n[2023-12-18T18:57:28.211Z] === FAIL: client TestOptionWithVersionFromEnv (0.00s)\r\n[2023-12-18T18:57:28.211Z]     options_test.go:59: assertion failed: error is not nil: maximum supported API version is 1.44: 2.9999\r\n\r\n```","Ah! \ud83e\udd26 I forgot that `DOCKER_API_VERSION` (by \"design\") should accept \"anything\", including completely bogus versions;\r\n\r\n```\r\n\r\n[2023-12-18T20:29:28.187Z] === FAIL: client TestNegotiateAPVersionOverride (0.00s)\r\n[2023-12-18T20:29:28.187Z]     client_test.go:360: assertion failed: error is not nil: maximum supported API version is 1.44: 9.99\r\n[2023-12-18T20:29:28.187Z] \r\n[2023-12-18T20:29:28.187Z] === FAIL: client TestOptionWithVersionFromEnv (0.00s)\r\n[2023-12-18T20:29:28.187Z]     options_test.go:59: assertion failed: error is not nil: maximum supported API version is 1.44: 2.9999\r\n```\r\n\r\nI guess (for now) I need to keep that behavior"],"labels":["area\/api","area\/cli","status\/2-code-review","impact\/api","impact\/changelog"]},{"title":"API Documentation documents deprecated (?) filter query format.","body":"### Description\n\nHi!\r\n\r\nIn all swagger [API docs](https:\/\/docs.docker.com\/engine\/api\/v1.41\/#tag\/Container\/operation\/ContainerList) since at least `v1.22`, the `filters` query parameter of the API has been documented as follows:\r\n\r\n```\r\nFilters to process on the container list, encoded as JSON (a map[string][]string). For example, {\"status\": [\"paused\"]} will only return paused containers.\r\n```\r\n\r\nor \r\n\r\n```\r\nFilters to process on the prune list, encoded as JSON (a map[string][]string).\r\n```\r\n\r\netc.\r\n\r\nIn the API implementation in https:\/\/github.com\/moby\/moby\/blob\/master\/api\/types\/filters\/parse.go, the `filters` type is described as `map[string]map[string]bool`, which expects e.g. `{\"status\": {\"paused\": true}}`.\r\n\r\nThe `map[string][]string` implementation is described in comments in this parser as deprecated\/legacy since API version `1.22`, see [here](https:\/\/github.com\/moby\/moby\/blob\/master\/api\/types\/filters\/parse.go#L67) or [here](https:\/\/github.com\/moby\/moby\/blob\/master\/api\/types\/filters\/parse.go#L322).\r\n\r\nThe official docker client, when the version has been negotiated `>1.22`, seems to use the `map[string]map[string]bool` type when communicating with the daemon.\r\n\r\nWhich version is preferred here? Some libs (such as the `bollard` lib in rust) [seem to be using the legacy `filters` query format](https:\/\/github.com\/fussybeaver\/bollard\/blob\/master\/src\/container.rs#L79). Will this deprecation at some point be forced?\r\n\r\nI feel that the API docs should at least mention both possible query parameter formats.\r\n\r\nThanks!\n\n### Reproduce\n\nno reproduction necessary, see provided links.\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.21.4\r\n Git commit:        v24.0.5\r\n Built:             Thu Jan  1 00:00:00 1970\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/nix\/store\/gdyxcsv9w1aghvgjy1j4c4mmdh35hi93-docker-plugins\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.23.1\r\n    Path:     \/nix\/store\/gdyxcsv9w1aghvgjy1j4c4mmdh35hi93-docker-plugins\/libexec\/docker\/cli-plugins\/docker-compose\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Prometheus Metric for Health Check Status","body":"### Description\n\nCurrently, there are only metrics for the total (or failed) number of healthchecks run by the engine. I would like these to have additional container labels to indicate which containers are currently unhealthy. At the same time, I can see this where this could cause problems due to being uninitialised at startup, or due to having many of these metrics. What alternative solutions are there for me to bring this healthcheck data into Grafana?\r\n\r\nMy current best idea is rolling my own Docker 'events' container that scrapes the healthcheck status and logs changes (and every 10 minutes at minimum, to reduce the amount of logs that queries need to search through). Does anyone have any cleaner suggestions?","comments":["> My current best idea is rolling my own Docker 'events' container that scrapes the healthcheck status and logs changes (and every 10 minutes at minimum, to reduce the amount of logs that queries need to search through). Does anyone have any cleaner suggestions?\r\n\r\nYou could [write a Prometheus exporter](https:\/\/prometheus.io\/docs\/instrumenting\/writing_exporters\/) which scrapes the Docker Engine API, like cAdvisor. [Unfortunately cAdvisor itself does not export the metrics you want.](https:\/\/github.com\/google\/cadvisor\/issues\/2166)"],"labels":["status\/0-triage","kind\/feature","area\/metrics","area\/metrics\/prometheus"]},{"title":"Restart docker daemon when docker performs health check  causing pipe leaks","body":"### Description\n\n1.start container with healthy check\uff0clike\r\ndocker run -itd --health-cmd \"sleep 5\" --health-interval 1s --health-retries 3 fd2d3e51789e bash\r\n\"sleep 5\" is an example, as long as it is a command that does not end immediately, it will be fine.\r\n2.execute systemctl start docker when health-cmd is executing\r\n3.lsof -p $(pidof containerd-shim) |grep pipe\r\n\n\n### Reproduce\n\n1.docker run -itd --health-cmd \"sleep 5\" --health-interval 1s --health-retries 3 fd2d3e51789e bash\r\n2.systemctl restart docker\r\n3.lsof -p $(pidof containerd-shim) |grep pipe\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\ndocker version\r\nClient:\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:06:50 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:17 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.6\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\ndocker version\r\nClient:\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:06:50 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:17 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.6\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n[root@localhost ~]# docker info\r\nClient:\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 091922f03c2762540fd057fba91260237ff86acb\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.10.0-136.12.0.86.h1388.eulerosv2r12.aarch64\r\n Operating System: EulerOS 2.0 (SP12)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 3.303GiB\r\n Name: localhost.localdomain\r\n ID: 308e0380-3d4e-474b-ab27-4547adf23fa6\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  k8s.gcr.io\r\n  10.175.125.206:80\r\n  docker-hub.tools.huawei.com\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n Product License: Community Engine\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["area\/runtime","status\/0-triage","kind\/bug","version\/24.0"]},{"title":"inconsistent single-l and double-l spellings in variable and function names for canceled\/cancelled etc.","body":"### Description\r\n\r\nBoth the single-l spellings (_canceled, canceling, cancelable, cancelation_) and double-l spelling (_cancelled, cancelling, cancellable, cancellation_) are prevalent and widely used in the English language (though as a side note, often the single-l form is associated with U.S. English). See various dictionaries:\r\n- https:\/\/www.merriam-webster.com\/dictionary\/cancel\r\n- https:\/\/dictionary.cambridge.org\/dictionary\/english\/cancel\r\n- https:\/\/en.wiktionary.org\/wiki\/cancel\r\n\r\nThe concern with having a mix of the single-l and double-l spellings in variable and function names is that it is all too easy for programmers to pick the \"wrong\" spelling for a particular variable or function name. In many cases, the \"wrong\" spelling will soon result in a syntax error. (But in some situations the error might be silent and\/or return an unexpected and unnoticed value.)\r\n\r\nHere are the number of hits on files for single-l and double-l spellings of the words _canceled\/cancelled, canceling\/cancelling, cancelable\/cancellable,_  and _cancelation\/cancellation_  in the `moby` repo. As you can see there is a mix between the two spellings in use:\r\n\r\n52 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22canceled%22&type=code\r\n7 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22canceling%22&type=code\r\n0 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancelable%22&type=code\r\n1 file: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancelation%22&type=code\r\n\r\n58 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancelled%22&type=code\r\n0 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancelling%22&type=code\r\n0 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancellable%22&type=code\r\n11 files: https:\/\/github.com\/search?q=repo%3Amoby%2Fmoby+%22cancellation%22&type=code\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n1. Search the moby repo for single-l and double-l spellings and observe that indeed there is a mix between the two spellings for various forms of the word \"cancel\" in variable and function names.\r\n\r\n### Expected behavior\r\n\r\n1. Consistent spelling of the same words when they are used in variable and function names. This will make these variables and functions easier to use and refer to in code and will likely prevent some errors that result from the use of the \"wrong\" spelling. In addition, it will improve the developer experience when searching the code base textually.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:53:15 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Desktop 4.21.0 (113844)\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:52:17 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nI understand that with regard to the refactoring required for this, the \"juice might not seem to be worth the squeeze.\" Nevertheless, using a consistent spelling for these variable and function names would improve the developer experience and increase the code quality of the `moby` repo.","comments":["I see golang stdlib uses single `l`, or at least, looking at [`context.Canceled`](https:\/\/pkg.go.dev\/context#Canceled). Changing non-exported variables and functions should not be problematic (although would result in some amount of code-churn, but it looks like there's some variants that made their way into exported functions, such as [`errdefs.Cancelled`](https:\/\/pkg.go.dev\/github.com\/docker\/docker@v24.0.7+incompatible\/errdefs#Cancelled) and [`errdefs.IsCancelled()`](https:\/\/pkg.go.dev\/github.com\/docker\/docker@v24.0.7+incompatible\/errdefs#IsCancelled), which is slightly more problematic, as renaming those will involve aliasing the old variant, and deprecating them.\r\n\r\nIf we want to converge on a single spelling for these, we should also have a linter that verifies the code and flags incorrect uses in CI. I see the `misspell` linter is configurable to use US spelling; https:\/\/golangci-lint.run\/usage\/linters\/#misspell, but not sure if it only checks for _comments_ or also for variable names.\r\n","I've tested `misspell` linter in this [repository](https:\/\/github.com\/martinjirku\/moby-linter-test). It checks  both, for  comments and for variables as well (actually, it's configurable):\r\n\r\n```sh\r\nmain.go:6:37: `cancelled` is a misspelling of `canceled` (misspell)\r\n  \/\/ this is misspelled comment for cancelled\r\n                                    ^\r\nmain.go:7:3: `cancelled` is a misspelling of `canceled` (misspell)\r\n  cancelled := true\r\n  ^\r\nmain.go:8:47: `cancelled` is a misspelling of `canceled` (misspell)\r\n  fmt.Printf(\"This is misspelled word: %t\\n\", cancelled)\r\n                                              ^\r\n```"],"labels":["kind\/enhancement"]},{"title":"make win does not build containerutility.exe","body":"### Description\n\nthe `make win` command does not build [the required containerutility.exe binary](https:\/\/github.com\/moby\/moby\/issues\/41776).\r\n\r\nas a workaround, we can use `docker buildx bake all --set *.platform=windows\/amd64` instead of `make win`.\n\n### Reproduce\n\n1. checkout the https:\/\/github.com\/moby\/moby\/releases\/tag\/v24.0.7 release\r\n2. make win\n\n### Expected behavior\n\nthe bundles\/binary\/ directory must contain the `containerutility.exe` binary.\n\n### docker version\n\n```bash\n24.0.7\n```\n\n\n### docker info\n\n```bash\nn\/a\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"[WIP] implement module compatibility check","body":"This package imports all \"importable\" packages, i.e., packages that:\r\n\r\n- are not applications (\"main\")\r\n- are not internal\r\n- and that have non-test go-files\r\n\r\nWe do this to verify that our code can be consumed as a dependency in \"module mode\". When using a dependency that does not have a go.mod (i.e.; is not a \"module\"), go implicitly generates a go.mod. Lacking information from the dependency itself, it assumes \"go1.16\" language (see [DefaultGoModVersion]). Starting with Go1.21, go downgrades the language version used for such dependencies, which means that any language feature used that is not supported by go1.16 results in a compile error;\r\n\r\n    # github.com\/docker\/cli\/cli\/context\/store\r\n    \/go\/pkg\/mod\/github.com\/docker\/cli@v25.0.0-beta.2+incompatible\/cli\/context\/store\/storeconfig.go:6:24: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    \/go\/pkg\/mod\/github.com\/docker\/cli@v25.0.0-beta.2+incompatible\/cli\/context\/store\/store.go:74:12: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n\r\nThese errors do NOT occur when using GOPATH mode, nor do they occur when using \"pseudo module mode\" (the \"-mod=mod -modfile=vendor.mod\" approach used in this repository).\r\n\r\nAs a workaround for this situation, we must include \"\/\/go:build\" comments in any file that uses newer go-language features (such as the \"any\" type or the \"min()\", \"max()\" builtins).\r\n\r\nFrom the go toolchain docs (https:\/\/go.dev\/doc\/toolchain):\r\n\r\n> The go line for each module sets the language version the compiler enforces\r\n> when compiling packages in that module. The language version can be changed\r\n> on a per-file basis by using a build constraint.\r\n>\r\n> For example, a module containing code that uses the Go 1.21 language version\r\n> should have a go.mod file with a go line such as go 1.21 or go 1.21.3.\r\n> If a specific source file should be compiled only when using a newer Go\r\n> toolchain, adding \/\/go:build go1.22 to that source file both ensures that\r\n> only Go 1.22 and newer toolchains will compile the file and also changes\r\n> the language version in that file to Go 1.22.\r\n\r\nThis file is a generated module that imports all packages provided in the repository, which replicates an external consumer using our code as a dependency in go-module mode, and verifies all files in those packages have the correct \"\/\/go:build <go language version>\" set.\r\n\r\nTo test this package:\r\n\r\n    make -C .\/internal\/gocompat\/\r\n    GO111MODULE=off go generate .\r\n    go mod tidy\r\n    go test -v\r\n    # github.com\/docker\/docker\/libnetwork\/options\r\n    ..\/..\/libnetwork\/options\/options.go:45:25: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    # github.com\/docker\/docker\/libnetwork\/internal\/setmatrix\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:13:16: type parameter requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:13:18: predeclared comparable requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:14:20: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:20:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:31:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:43:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:59:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:80:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:93:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:104:10: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/internal\/setmatrix\/setmatrix.go:104:10: too many errors\r\n    # github.com\/docker\/docker\/libnetwork\/config\r\n    ..\/..\/libnetwork\/config\/config.go:35:47: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/config\/config.go:47:41: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/config\/config.go:63:55: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/libnetwork\/config\/config.go:95:63: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    # github.com\/docker\/docker\/testutil\r\n    ..\/..\/testutil\/helpers.go:80:9: predeclared any requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    # github.com\/docker\/docker\/builder\/builder-next\/adapters\/containerimage\r\n    ..\/..\/builder\/builder-next\/adapters\/containerimage\/pull.go:72:4: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    ..\/..\/builder\/builder-next\/adapters\/containerimage\/pull.go:200:19: type instantiation requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    FAIL\tgocompat [build failed]\r\n    make: *** [Makefile:5: verify] Error 1\r\n\r\n[DefaultGoModVersion]: https:\/\/github.com\/golang\/go\/blob\/58c28ba286dd0e98fe4cca80f5d64bbcb824a685\/src\/cmd\/go\/internal\/gover\/version.go#L15-L24\r\n[2]: https:\/\/go.dev\/doc\/toolchain\r\n\r\n\r\n### temporarily skip some packages\r\n\r\none, or combinations of these still makes it fail, but unfortunately\r\nit doesn't print what file it is:\r\n    \r\n    go test -v\r\n    # github.com\/docker\/docker\/daemon\r\n    embedding interface element ~[]string requires go1.18 or later (-lang was set to go1.16; check go.mod)\r\n    FAIL\tgocompat [build failed]\r\n\r\nno external consumer _should_ be importing the daemon-code, so skipping\r\nchecks for these (for now).\r\n\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/2-code-review","area\/testing"]},{"title":"replace uses of distributions registry-client because it's deprecated and removed","body":"### Description\n\nrelates to\r\n\r\n- https:\/\/github.com\/distribution\/distribution\/pull\/4126\r\n- https:\/\/github.com\/distribution\/distribution\/issues\/4119\r\n\r\n\r\nThe distribution's client was moved to an internal package, and docker\/distribution (\"v2\") is about to be deprecated in favour of distribution\/distribution \"v3\".\r\n\r\nWe need to replace uses of the client package in our code (we can temporarily use an internal fork if needed), and migrate to github.com\/distribution\/distribution v3 for the remaining parts we need (if any).\r\n\r\n\r\n\r\n","comments":["\/cc @corhere @dmcgowan ","What's the rush? v2 isn't going to disappear when v3.0.0 gets tagged, so we don't have to change anything unless bugs come up that need fixing. And our use of the distribution client is legacy anyway: the containerd backend uses containerd's distribution client. We will be able to drop the dependency much more easily when we get rid of the graphdrivers. If bugs in the distribution client come up which need fixing before then, I think we should copy the client code into the repo, like we did with libnetwork. Carrying an internal fork in a separate repo would be just as much of an ongoing pain as merging fixes to the v2 branch of `distribution\/distribution`.","> What's the rush? v2 isn't going to disappear when v3.0.0 gets tagged, so we don't have to change anything unless bugs come up that need fixing\r\n\r\nYes, I think there's  quite a strong urge from the distribution maintainers to EOL v2 when v3 is released. There's already a (rather unfortunate) bug in v2 (introduced by yours truly) that has a fix merged, but it's uncertain if another v2.x release will see light of day; https:\/\/github.com\/distribution\/distribution\/pull\/4114\r\n\r\nFrom that perspective I considered it would be better to \"own\" the client, so that we have control (specifically for LTS branches), or at least it could make life easer in cases where there _is_ a bug to fix, and we either need to fork \"under pressure\" at that moment. "],"labels":["area\/distribution","kind\/refactor"]},{"title":"README: Docker EE -> Docker Desktop or Mirantis Container Engine","body":null,"comments":["Thank you for contributing! It appears your commit message is missing a DCO sign-off,\r\ncausing the DCO check to fail.\r\n\r\nWe require all commit messages to have a `Signed-off-by` line with your name\r\nand e-mail (see [\"Sign your work\"](https:\/\/github.com\/moby\/moby\/blob\/v20.10.11\/CONTRIBUTING.md#sign-your-work)\r\nin the CONTRIBUTING.md in this repository), which looks something like:\r\n\r\n```\r\nSigned-off-by: YourFirsName YourLastName <yourname@example.org>\r\n```\r\n\r\nThere is no need to open a new pull request, but to fix this (and make CI pass),\r\nyou need to _amend_ the commit(s) in this pull request, and \"force push\" the amended\r\ncommit.\r\n\r\nUnfortunately, it's not possible to do so through GitHub's web UI, so this needs\r\nto be done through the git commandline.\r\n\r\nYou can find some instructions in the output of the DCO check (which can be found\r\nin the \"checks\" tab on this pull request), as well as in the [Moby contributing guide](https:\/\/github.com\/moby\/moby\/blob\/v20.10.11\/docs\/contributing\/set-up-git.md).\r\n\r\nSteps to do so \"roughly\" come down to:\r\n\r\n1. Set your name and e-mail in git's configuration:\r\n\r\n    ```bash\r\n    git config --global user.name \"YourFirstName YourLastName\"\r\n    git config --global user.email \"yourname@example.org\"\r\n    ```\r\n\r\n    (Make sure to use your _real_ name (**not your GitHub username\/handle**) and e-mail)\r\n\r\n\r\n2. Clone your fork locally\r\n3. Check out the branch associated with this pull request\r\n4. Sign-off and amend the existing commit(s)\r\n\r\n    ```bash\r\n    git commit --amend --no-edit --signoff\r\n    ```\r\n\r\n    If your pull request contains multiple commits, either squash the commits (if\r\n    needed) or sign-off each individual commit.\r\n\r\n5. _Force push_ your branch to GitHub (using the `--force` or [`--force-with-lease`](https:\/\/stackoverflow.com\/questions\/52823692\/git-push-force-with-lease-vs-force) flags) to update the pull request.\r\n\r\n\r\nSorry for the hassle (I wish GitHub would make this a bit easier to do), and let me know if you need help or more detailed instructions!","I guess this should also be updated as part of;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46772\r\n\r\n\/cc @neersighted ","Thanks, I'll do that soon. You could make that a bit easier for web-based commits like the one I did here though: https:\/\/github.blog\/changelog\/2022-06-08-admins-can-require-sign-off-on-web-based-commits\/ <- This setting will make GitHub automatically sign off commits through the web UI."],"labels":["status\/3-docs-review","dco\/no","area\/project"]},{"title":"DNS reverse lookup does timeout if container name is very long","body":"### Description\n\nIf the container name is very long the DNS server will timeout for reverse lookups. This can create hard to find problems in CI pipelines due to a Gitlab bug (see below).\r\n\r\nRelated issues:\r\n\r\n- https:\/\/gitlab.com\/gitlab-org\/gitlab-runner\/-\/issues\/27763\r\n- https:\/\/github.com\/mhale\/smtpd\/issues\/40\r\n- https:\/\/forums.docker.com\/t\/reverse-dns-fails-dns-bad-rdata-if-container-name-is-larger-than-62-characters\/107330\n\n### Reproduce\n\nCreate a custom bridge network, then perform reverse lookup inside a container where the target has a very long name.\r\n\r\nHere is a small shell snippet that reliably reproduces the issue:\r\n\r\n```sh\r\ndocker network create test\r\ndocker run --rm -it \\\r\n  --name runner-jl9klc-project-30504-concurrent-0-fd6217607b395033-build-3.runner-jl9klc-project-30504-concurrent-0-job-1297211-network \\\r\n  --network test \\\r\n  alpine \\\r\n  sh -c 'apk add -q bind-tools && dig +short -x $(ip -f inet addr show eth0 | awk '\\''\/inet \/ {print $2}'\\'' | cut -d \"\/\" -f 1)'\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\n;; communications error to 127.0.0.11#53: timed out\r\n;; communications error to 127.0.0.11#53: timed out\r\n;; communications error to 127.0.0.11#53: timed out\r\n;; no servers could be reached\r\n```\n\n### Expected behavior\n\nThere should be no timeout. The DNS should return no result or maybe an alias that is valid if any. There should still be a log entry.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:09:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:20 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.26\r\n  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 18\r\n  Running: 15\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 43\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dd1e886e55dd695541fdcd67420c2888645a495\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 4.18.0-513.9.1.el8_9.x86_64\r\n Operating System: Rocky Linux 8.9 (Green Obsidian)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.504GiB\r\n Name: bn-vl-rs-intern-01\r\n ID: 425fd49d-5ce2-41c3-9727-fc8ddbcf760d\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["@jannschu Thanks for reporting. As noted in the original Gitlab issue, the real problem comes from Gitlab which is using a too lengthy name. IMO the proper move is to fix Gitlab CI. On the other hand we might consider doing the following:\r\n\r\n1. Return a warning to the client when a container is created with a name \/ hostname \/ network-alias that doesn't comply with RFC1034 ;\r\n2. Improve the error message logged by our DNS server to make the issue more obvious to understand ;\r\n3. And we might consider adding a hard limit in the future (eg. return an error instead of a warning). But we might break people by doing so, thus we'd need to be extra cautious. I think we'd need to tackle https:\/\/github.com\/moby\/moby\/issues\/46890 first.","Since names longer than 63 chars cannot fit into DNS questions or answers, forward or reverse lookups on those named containers never could have worked. That gives us some wiggle room for improving the API and daemon behaviour. We could exclude user-configured names which are too long from the list of `DNSNames`, handling them as if they were left unset. Returning the container's shortid for the reverse lookup has got to be better than not answering or answering `SERVFAIL`. Replying `NXDOMAIN` (read: no reverse DNS record for the offending container) could also be an option.\r\n\r\nWith the rule that container names longer than 63 chars don't get automatically get added as DNS names, I don't think it is necessary to set a hard limit in future API versions. A warning would be sufficient. On the other hand, I am all for refusing a hostname or alias that is invalid as a DNS name.","> @jannschu Thanks for reporting. As noted in the original Gitlab issue, the real problem comes from Gitlab which is using a too lengthy name. IMO the proper move is to fix Gitlab CI. On the other hand we might consider doing the following:\r\n> \r\n> 1. Return a warning to the client when a container is created with a name \/ hostname \/ network-alias that doesn't comply with RFC1034 ;\r\n> 2. Improve the error message logged by our DNS server to make the issue more obvious to understand ;\r\n> 3. And we might consider adding a hard limit in the future (eg. return an error instead of a warning). But we might break people by doing so, thus we'd need to be extra cautious. I think we'd need to tackle [client: deprecate unbounded API version auto-negotiation\u00a0#46890](https:\/\/github.com\/moby\/moby\/issues\/46890) first.\r\n\r\n\r\nGreat plan, @akerouanton! \r\n\r\n1. Return warning to the client, would be a great addition, to save other peoples time troubleshooting.\r\n\r\n2. How is it possible to see the DNS error message you talk about in point nr 2?\r\nI am on Ubuntu. Did not see it with `\"debug\": true` in `\/etc\/docker\/daemon.json` and following with `tail -f \/var\/log\/syslog`\r\n\r\nAlso, I answered about this on Stackoverflow. Maybe less people end up here in this thread due to that. Hopefully your plan will be implemented despite that and thereby help save future users from some frustration.\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/42642561\/docker-restrictions-regarding-naming-container\/78156660#78156660\r\n\r\nPS GitLab is not at fault in my case. The long and descriptive container names were invented by myself."],"labels":["kind\/enhancement","kind\/bug","area\/networking","area\/daemon","area\/networking\/dns"]},{"title":"tests: migrate integration-cli stats tests to integration","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\nMove API stats tests from `\/integration-cli` to `\/integration`, refactoring appropriately.\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\nRun the tests! :)\r\n\r\n```\r\nmake TEST_FILTER=<insert test name here> TEST_IGNORE_CGROUP_CHECK=1 test-integration\r\n```\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\nn\/a\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n<img width=\"692\" alt=\"Screenshot 2023-12-12 at 12 30 21\" src=\"https:\/\/github.com\/moby\/moby\/assets\/70572044\/8a969a87-5e48-416b-b62f-3f5e1651f547\">\r\n","comments":[],"labels":["status\/2-code-review","area\/testing","kind\/refactor"]},{"title":"docker service update --publish-rm <port> not working?","body":"### Description\n\nI'm using caprover, which uses docker to create an deployment server.\r\n\r\nIt's using swarm and the relevant services can be checked etc. with docker service inspect <service>.\r\n\r\nBy default, the main container publishes 3 ports (80\/443 & 3000), but I need to remove 3000 (due to a conflict).\r\n\r\nThe author has advised me that previously \"docker service update --publish-rm <port> <container>\" should work, but it doesn't for some reason.\r\n\r\n\n\n### Reproduce\n\nhttps:\/\/github.com\/caprover\/caprover\/issues\/1898\n\n### Expected behavior\n\nSee github issue above\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.25\r\n  GitCommit:        d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc:\r\n  Version:          1.1.10\r\n  GitCommit:        v1.1.10-0-g18a0cb0\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 4\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: xioj7fr69cm8hb5y5a8zyke7n\r\n  Is Manager: true\r\n  ClusterID: 6n65422bxw4b70suubjtkm4ri\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 138.197.70.177\r\n  Manager Addresses:\r\n   138.197.70.177:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-91-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.752GiB\r\n Name: prod\r\n ID: e5786d16-677c-47b4-b599-bf460dd41f0a\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["What port did you specify to remove the published port? Currently the `--publish-rm` option uses the _container_ port, not the _host_ port; for example;\r\n\r\n\r\n```bash\r\n$ docker service create -p 8080:80 -p 8090:80 -p 9090:90 --name foo nginx:alpine\r\nxjjylntk2wf0mqwc1a59ny2qz\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n\r\n\r\n$ docker service ls\r\nID             NAME      MODE         REPLICAS   IMAGE          PORTS\r\nxjjylntk2wf0   foo       replicated   1\/1        nginx:alpine   *:8080->80\/tcp, *:8090->80\/tcp, *:9090->90\/tcp\r\n```\r\n\r\nRemoving the published port(s) for port 80 will remove all ports mapped to that container port;\r\n\r\n```bash\r\n$ docker service update --publish-rm 80 foo\r\nfoo\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n\r\n$ docker service ls\r\nID             NAME      MODE         REPLICAS   IMAGE          PORTS\r\nxjjylntk2wf0   foo       replicated   1\/1        nginx:alpine   *:9090->90\/tcp\r\n```\r\n\r\n\r\nI should add that there was a discussion to make this option more granular, so that individual port-mappings for the same container-port could be removed, but I don't think work has been done to implement that; see\r\n\r\n- https:\/\/github.com\/moby\/swarmkit\/issues\/1396\r\n- https:\/\/github.com\/moby\/moby\/issues\/25338\r\n\r\n\r\nI also tried the `3000:300` mapping from the linked issue, but wasn't able to reproduce that case;\r\n\r\n```bash\r\n$ docker service create --name foo -p 3000:3000 nginx:alpine\r\n8yvgloe7ojv7pyzbg11kr07pf\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n\r\n$ docker service ls\r\nID             NAME      MODE         REPLICAS   IMAGE          PORTS\r\n8yvgloe7ojv7   foo       replicated   1\/1        nginx:alpine   *:3000->3000\/tcp\r\n\r\n $ docker service update --publish-rm 3000 foo\r\nfoo\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n\r\n$ docker service ls\r\nID             NAME      MODE         REPLICAS   IMAGE          PORTS\r\n8yvgloe7ojv7   foo       replicated   1\/1        nginx:alpine\r\n```\r\n","Hi @thaJeztah I've just noticed something interesting...\r\n\r\nIn answer to your question, the port mapping is 3000:3000\/tcp, however during debugging, I noticed that this port mapping only shows duing an docker ps:\r\n\r\n```\r\nCONTAINER ID   IMAGE                              COMMAND                  CREATED        STATUS        PORTS                                                                    NAMES\r\nd03fffdb176f     caprover\/certbot-sleeping:v1.6.0   \"\/bin\/sh -c 'sleep 9\u2026\"   13 hours ago   Up 13 hours   80\/tcp, 443\/tcp                                                            captain-certbot.1.6tjj7c56ahzxjkww6rf7jp3ph\r\ncfdb746bef57   nginx:1.24                         \"\/docker-entrypoint.\u2026\"   13 hours ago   Up 13 hours   0.0.0.0:80->80\/tcp, :::80->80\/tcp, 0.0.0.0:443->443\/tcp, :::443->443\/tcp   captain-nginx.1.keoe7teyvmlwignj6kowarjd9\r\n25d3776d322c   caprover\/caprover:1.11.1           \"docker-entrypoint.s\u2026\"   13 hours ago   Up 13 hours   0.0.0.0:3000->3000\/tcp, :::3000->3000\/tcp                                  captain-captain.1.radz1k93vd5jl3vtgfh4ms6oq\r\n```\r\n\r\nand not docker service ls:\r\n\r\n```\r\nID             NAME              MODE         REPLICAS   IMAGE                              PORTS\r\nmdfwkvqxnw3v   captain-captain   replicated   1\/1        caprover\/caprover:1.11.1           \r\nlhfw0f6cmx6h   captain-certbot   replicated   1\/1        caprover\/certbot-sleeping:v1.6.0   \r\nlwb6vs8onlj8   captain-nginx     replicated   1\/1        nginx:1.24                         \r\n```\r\n\r\nBut the docker service inspect shows:\r\n\r\n```\r\nID:             mdfwkvqxnw3vkgo74vmk84dp6\r\nName:           captain-captain\r\nService Mode:   Replicated\r\n Replicas:      1\r\nPlacement:\r\n Constraints:   [node.id == xioj7fr69cm8hb5y5a8zyke7n]\r\nUpdateConfig:\r\n Parallelism:   1\r\n On failure:    pause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Update order:      stop-first\r\nRollbackConfig:\r\n Parallelism:   1\r\n On failure:    pause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Rollback order:    stop-first\r\nContainerSpec:\r\n Image:         caprover\/caprover:1.11.1\r\n Env:           IS_CAPTAIN_INSTANCE=1 \r\nMounts:\r\n Target:        \/captain\r\n  Source:       \/captain\r\n  ReadOnly:     false\r\n  Type:         bind\r\n Target:        \/var\/run\/docker.sock\r\n  Source:       \/var\/run\/docker.sock\r\n  ReadOnly:     false\r\n  Type:         bind\r\nSecrets:\r\n Target:        captain-salt\r\n  Source:       captain-salt\r\nLog Driver:\r\n Name:          json-file\r\n LogOpts:\r\n  max-size:       512m\r\n\r\nResources:\r\nNetworks: captain-overlay-network \r\nEndpoint Mode:  vip\r\nPorts:\r\n PublishedPort = 3000\r\n  Protocol = tcp\r\n  TargetPort = 3000\r\n  PublishMode = host \r\n\r\n```\r\n","Ah! Looks like the service uses \"host-mode\" publishing for the ports;\r\n\r\n```\r\nPublishMode = host \r\n```\r\n\r\nThe \"normal\" mode for publishing is to publish a port for the _service_, which uses internal load-balancing that forwards traffic to containers (tasks) backing the service (and ports can be accessed from any node in the cluster). In that mode, ports are not mapped to individual containers.\r\n\r\nWith host-mode publishing, port-mapping works similar to \"standalone\" (non-swarm) containers, and port-mapping is happening per-container (but those ports can only be accessed from the node on which that specific instance runs).\r\n\r\nThat _could_ explain differences yes. Although I _would_ expect the same code-path to be hit (service-spec updated -> create new tasks with the updated config) \ud83e\udd14 \r\n\r\n","Yes, here's the print for the json service.\r\n\r\n\"EndpointSpec\": {\r\n                \"Mode\": \"vip\",\r\n                \"Ports\": [\r\n                    {\r\n                        \"Protocol\": \"tcp\",\r\n                        \"TargetPort\": 3000,\r\n                        \"PublishedPort\": 3000,\r\n                        \"PublishMode\": \"host\"\r\n                    }\r\n                ]\r\n            }\r\n\r\nI did try the host-rm <port> but that did not work either... Does the author of caprover need to make changes to use service ports?","I had a quick peek at the code that handles updating port-mappings (the code is in https:\/\/github.com\/docker\/cli); https:\/\/github.com\/docker\/cli\/blob\/623d9b1f68171e3ff9899b21392ac4f102ba533b\/cli\/command\/service\/update.go#L1050\r\n\r\nAnd from that code, it looks like there's a a check if the port-mapping to be added\/removed has the same protocol (tcp\/udp..) and \"mode\" (`host` or `ingress`); https:\/\/github.com\/docker\/cli\/blob\/623d9b1f68171e3ff9899b21392ac4f102ba533b\/cli\/command\/service\/update.go#L1069-L1073\r\n\r\n```go\r\nif equalProtocol(port.Protocol, pConfig.Protocol) &&\r\n\tport.TargetPort == pConfig.TargetPort &&\r\n\tequalPublishMode(port.PublishMode, pConfig.PublishMode) {\r\n\tcontinue portLoop\r\n}\r\n```\r\n\r\n`tcp` (protocol) and `ingress` (mode) are the defaults, which is why `--publish-rm` worked in the earlier examples, but when host-mode is used, it has to be specified.\r\n\r\nTo verify that; create a service that uses host-mode publishing;\r\n\r\n```bash\r\ndocker service update --publish-rm 3000 foo\r\nfoo\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n```\r\n\r\nCheck the service's config (which includes the \"host-mode\" port mapping;\r\n\r\n```bash\r\ndocker service inspect --format=pretty foo\r\n\r\nID:\t\ts0cf9elemckkai0jrj44ej291\r\nName:\t\tfoo\r\nService Mode:\tReplicated\r\n Replicas:\t1\r\nPlacement:\r\nUpdateConfig:\r\n Parallelism:\t1\r\n On failure:\tpause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Update order:      stop-first\r\nRollbackConfig:\r\n Parallelism:\t1\r\n On failure:\tpause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Rollback order:    stop-first\r\nContainerSpec:\r\n Image:\t\tnginx:alpine@sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc\r\n Init:\t\tfalse\r\nResources:\r\nEndpoint Mode:\tvip\r\nPorts:\r\n PublishedPort = 3000\r\n  Protocol = tcp\r\n  TargetPort = 3000\r\n  PublishMode = host\r\n```\r\n\r\nRemove the mapped port; specifying the publish-mode (host);\r\n\r\n```bash\r\ndocker service update --publish-rm mode=host,target=3000 foo\r\nfoo\r\noverall progress: 1 out of 1 tasks\r\n1\/1: running   [==================================================>]\r\nverify: Service converged\r\n```\r\n\r\nCheck that the port-mapping was removed;\r\n\r\n```bash\r\ndocker service inspect --format=pretty foo\r\n\r\nID:\t\ts0cf9elemckkai0jrj44ej291\r\nName:\t\tfoo\r\nService Mode:\tReplicated\r\n Replicas:\t1\r\nUpdateStatus:\r\n State:\t\tcompleted\r\n Started:\t16 minutes ago\r\n Completed:\t16 minutes ago\r\n Message:\tupdate completed\r\nPlacement:\r\nUpdateConfig:\r\n Parallelism:\t1\r\n On failure:\tpause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Update order:      stop-first\r\nRollbackConfig:\r\n Parallelism:\t1\r\n On failure:\tpause\r\n Monitoring Period: 5s\r\n Max failure ratio: 0\r\n Rollback order:    stop-first\r\nContainerSpec:\r\n Image:\t\tnginx:alpine@sha256:3923f8de8d2214b9490e68fd6ae63ea604deddd166df2755b788bef04848b9bc\r\n Init:\t\tfalse\r\nResources:\r\nEndpoint Mode:\tvip\r\n```","So from the above, it looks like this is working as intended, but perhaps the CLI should print a warning or error when specifying a port to remove that is not found. I think the intent here was to make the `--publish-rm` and `--publish-add` options idempotent (don't error when removing things that weren't there), but perhaps it'd be useful to print a warning for these cases (not an error, but at least inform the user); that would be related to the other ticket I linked earlier;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/25338"],"labels":["kind\/question","status\/more-info-needed","area\/swarm","version\/24.0"]},{"title":"c8d: evaluate expected behavior for looking up users and groups","body":"- relates to https:\/\/github.com\/moby\/moby\/issues\/46903#issuecomment-1842722565\r\n\r\n I'm also curious what it's trying to look up in this case; It looks like `GIDFromPath` unconditionally reads `\/etc\/group`, then takes the first entry it finds as `group` and uses its ID;\r\n\r\n```bash\r\ndocker run --rm alpine cat \/etc\/group\r\nroot:x:0:root\r\nbin:x:1:root,bin,daemon\r\n...\r\n```\r\n\r\nSo even if no user is set, it's reading those files to (surprise!) find out that it must run as `0` (`root`)?\r\n\r\nWhich makes me curious;\r\n\r\n- MUST the first entry always be for `root` ?\r\n- Is it possible \/ allowed to have a different group ID for the first entry? (i.e., `root` does not have `0` as group?)\r\n\r\nAll of that at least assumes the container is running a \"distro\" image, and does not take `from scratch` images into account;\r\n\r\n- if no user is defined (`\"\"`); should it just bail out and use `0`?\r\n- if `\/etc\/group` and\/or `\/etc\/password` are not present, should it just use `0` for both? (i.e.: `from scratch` containers)\r\n- for numeric \"user\" or \"group\", should it even look-up anything? (there was some debate bout that, and possibly that was changed already as part of a \"CVE\" (fun fact: Linux userland tools DO allow a user-name or group-name that's a number))\r\n\r\n_Originally posted by @thaJeztah in https:\/\/github.com\/moby\/moby\/issues\/46903#issuecomment-1842722565_\r\n            ","comments":[],"labels":["area\/runtime","containerd-integration"]},{"title":"Output of `docker ps --format json` is not valid json - it's not an array","body":"### Description\n\nWhen more than one docker container is running the output of `docker ps --format json` does not include the leading '[' and trailing ']' needed for the output to represent an array of items.\r\nThus constructs like `docker ps --format json | jq '.[] | select(.Names == \"localstack\")'` do not work as a workaround for https:\/\/github.com\/moby\/moby\/issues\/32985.\r\n\r\nThe output I see looks like:\r\n`\r\n{... container1 info}\r\n{... container2 info}\r\n`\r\n\r\nNote two things.\r\n- There is no comma (,) separator between the container infos.\r\n- And there is no wrapping square brackets to indicate that this is an array of dictionaries.\r\n-\r\nTo be considered proper json this needs to be (indentation added for clarity):\r\n`\r\n[\r\n  {... container1 info},\r\n  {... container2 info}\r\n]\r\n`\r\n\n\n### Reproduce\n\n1. `docker run --name python38 -it python:3.8 bash &`\r\n2. `docker run --name python39 -it python:3.9 bash &`\r\n3. `docker ps --format json`\r\n\r\nThe output looks like:\r\n`\r\n{\"Command\":\"\\\"bash\\\"\",\"CreatedAt\":\"2023-12-06 14:53:23 -0600 CST\",\"ID\":\"a577b8f2e163\",\"Image\":\"python:3.8\",\"Labels\":\"\",\"LocalVolumes\":\"0\",\"Mounts\":\"\",\"Names\":\"python38\",\"Networks\":\"bridge\",\"Ports\":\"\",\"RunningFor\":\"37 seconds ago\",\"Size\":\"0B\",\"State\":\"running\",\"Status\":\"Up 37 seconds\"}\r\n{\"Command\":\"\\\"bash\\\"\",\"CreatedAt\":\"2023-12-06 14:53:12 -0600 CST\",\"ID\":\"b7558d12c69e\",\"Image\":\"python:3.9\",\"Labels\":\"\",\"LocalVolumes\":\"0\",\"Mounts\":\"\",\"Names\":\"python39\",\"Networks\":\"bridge\",\"Ports\":\"\",\"RunningFor\":\"48 seconds ago\",\"Size\":\"0B\",\"State\":\"running\",\"Status\":\"Up 48 seconds\"}\r\n`\n\n### Expected behavior\n\n`docker ps --format json` should output valid json so downstream tools like `jq` can parse it\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:28:49 2023\r\n OS\/Arch:           darwin\/amd64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.25.2 (129061)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:16 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.6\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.5\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.0-desktop.1\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.9\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.0.9\r\n    Path:     \/Users\/jimmeyer\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 24\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 20\r\n Images: 154\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.4.16-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 15.62GiB\r\n Name: linuxkit-e62b622ed039\r\n ID: 26c72089-82e4-4620-b3b5-8900190e88e6\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 193\r\n  Goroutines: 394\r\n  System Time: 2023-12-06T20:42:05.817133057Z\r\n  EventsListeners: 10\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: true\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nRelates to https:\/\/github.com\/moby\/moby\/issues\/32985","comments":["@jim-meyer  - `--format` does not define output format for entire result but for each container. Also, it works on templating so that you can choose which columns you want in the final output.\r\n\r\nFor example `docker ps --format='{{json .}}'` prints all columns for the container while `docker ps --format='{{json .ID}}'` prints only ID column of each container. See [format documentation](https:\/\/docs.docker.com\/engine\/reference\/commandline\/ps\/#format) for more details.\r\n\r\n\r\nAs for your use-case I think `docker ps --format='{{json .}}' | jq . -s` works fine.\r\n\r\nP.S. This issue belongs to https:\/\/github.com\/docker\/cli repo :)","For compatibility reason, this can't be fixed.\r\n","@AkihiroSuda - What kind of compatibility issue(s) we are talking here? \r\n\r\nI mean, if we know what are the issues then there might be solution to fix it or a workaround those issue(s). IMO, correct behaviour is to return valid json output as it is easy to operate on in different scripting language and tools. ","It is likely that users have been depending on the current behavior, and changing the behavior will break their existing scripts.\r\n","> For compatibility reason, this can't be fixed.\r\n\r\nPerhaps a _new_ format (say \"jsonArray\" or something) can be added?"],"labels":["kind\/question"]},{"title":"Missing image tag event on build","body":"### Description\n\nBuilding an image doesn't emit any \"image tag\" events\n\n### Reproduce\n\nIn one terminal run `docker events` and in another run\r\n\r\n```console\r\n$ cat << EOF | docker build -f- -t test \/var\/empty\r\nFROM scratch\r\nEOF\r\n```\n\n### Expected behavior\n\nWe shoud see an `image tag` event in the first terminal:\r\n\r\n```\r\n2023-12-06T14:35:37.810440001+01:00 image tag sha256:54ac6bb475f018dd78804ac491a2dc0e0e3331f8b6347e56b75c61fffee81011 (name=test)\r\n```\n\n### docker version\n\n```bash\nmaster\n```\n\n\n### docker info\n\n```bash\nmaster\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug","containerd-integration"]},{"title":"Image in use not visible after re-pull","body":"### Description\r\n\r\nRetagging and removing an image makes it not show in `docker images` even though it's still in use.\r\n\r\n### Reproduce\r\n\r\nWith graph drivers\r\n\r\n```console\r\n$ docker pull debian:oldstable-20200514\r\n$ docker create debian:oldstable-20200514\r\n$ docker tag debian:oldstable-20200514 debian:stable\r\n$ docker rmi -f debian:oldstable-20200514\r\n$ docker pull debian:stable\r\n$ docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\r\ndebian       stable    c505deff3653   2 weeks ago   139MB\r\ndebian       <none>    476d4a705c2f   3 years ago   95.8MB\r\n```\r\n\r\nWith containerd:\r\n\r\n```console\r\n$ docker pull debian:oldstable-20200514\r\n$ docker create debian:oldstable-20200514\r\n$ docker tag debian:oldstable-20200514 debian:stable\r\n$ docker rmi -f debian:oldstable-20200514\r\n$ docker pull debian:stable\r\n$ docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\r\ndebian       stable    ed69ce2733ba   2 weeks ago   205MB\r\n$ docker ps -a\r\nCONTAINER ID   IMAGE          COMMAND   CREATED          STATUS    PORTS     NAMES\r\nb6c163481b58   14fe8cfc8730   \"bash\"    25 seconds ago   Created             upbeat_leavitt\r\n```\r\n\r\nThe `upbeat_leavitt` ends up running and pointing to an image that doesn't exist any more.\r\n\r\n### Expected behavior\r\n\r\nNot sure really what's the expected behavior we want here. Not showing the old image after is fine if it's not used any more. But in the case where there is already a container using it it could seem weird, you end up with a container that uses an image that you can't see\/inspect any more \r\n\r\n### docker version\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["This isn't really an `tag+rmi` issue:\r\n\r\n```\r\n$ docker images -a\r\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\r\nalpine       latest    34871e729050   5 days ago   11.7MB\r\n\r\n$ docker run -d alpine sleep 3600\r\nc0faf9092cc5824af00fca5ec659ad0a914d4be4dbe8b4032cf8c7a26e2a015d\r\n\r\n$ docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\r\nalpine       latest    34871e729050   5 days ago   11.7MB\r\n\r\n$ docker rmi alpine\r\nError response from daemon: conflict: unable to delete alpine:latest (must be forced) - container c0faf9092cc5 is using its referenced image 34871e729050\r\n\r\n$ docker rmi -f alpine\r\nUntagged: alpine:latest\r\n\r\n$ docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\r\n<none>       <none>    34871e729050   5 days ago   11.7MB\r\n\r\n```\r\n\r\n\r\nIt's only the `tag` (also used by `build`) not doing having the same untag\/remove behavior as `rmi` for the destination image:\r\n\r\n```\r\n$ docker pull busybox\r\nREPOSITORY   TAG       IMAGE ID       CREATED        SIZE\r\nalpine       latest    34871e729050   5 days ago     11.7MB\r\nbusybox      latest    1ceb872bcc68   19 hours ago   6.09MB\r\n\r\n$ docker rmi alpine\r\nError response from daemon: conflict: unable to delete alpine:latest (must be forced) - container c0faf9092cc5 is using its referenced image 34871e729050\r\n\r\n$ docker tag busybox alpine\r\n\r\n$ docker images\r\nREPOSITORY   TAG       IMAGE ID       CREATED        SIZE\r\nalpine       latest    1ceb872bcc68   19 hours ago   6.09MB\r\nbusybox      latest    1ceb872bcc68   19 hours ago   6.09MB\r\n\r\n$ docker ps\r\nCONTAINER ID   IMAGE          COMMAND        CREATED         STATUS         PORTS     NAMES\r\nc0faf9092cc5   34871e729050   \"sleep 3600\"   6 minutes ago   Up 6 minutes             frosty_hoover\r\n\r\n```\r\n\r\nEasiest solution would probably be to make tag first perform the destination image removal?","I updated the description and the reproducer\r\n"],"labels":["status\/0-triage","kind\/bug","containerd-integration"]},{"title":"Slow exec error when no \/etc\/group","body":"### Description\r\n\r\nTrying to exec into a container that doesn't have `\/etc\/group` shows an error but then just lingers there for 10 seconds:\r\n\r\n\r\n### Reproduce\r\n\r\n```console\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# ID=$(docker run -d docker\/desktop-kubernetes-pause:3.7)\r\nUnable to find image 'docker\/desktop-kubernetes-pause:3.7' locally\r\n3.7: Pulling from docker\/desktop-kubernetes-pause\r\naff472d3f83e: Download complete\r\nDigest: sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c\r\nStatus: Downloaded newer image for docker\/desktop-kubernetes-pause:3.7\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# docker ps\r\nCONTAINER ID   IMAGE                                 COMMAND    CREATED         STATUS         PORTS     NAMES\r\nd005f8037c37   docker\/desktop-kubernetes-pause:3.7   \"\/pause\"   5 seconds ago   Up 5 seconds             musing_meninsky\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# time docker exec -it -u 0:root $ID \/bin\/sh\r\nunable to find group root: no matching entries in group file\r\n\r\n\r\nreal    0m10.046s\r\nuser    0m0.025s\r\nsys     0m0.018s\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected more something like this:\r\n\r\n```console\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# ID=$(docker run -d docker\/desktop-kubernetes-pause:3.7)^C\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# docker ps\r\nCannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# ID=$(docker run -d docker\/desktop-kubernetes-pause:3.7)\r\nroot@b4f40d17b559:\/go\/src\/github.com\/docker\/docker# time docker exec -it $ID \/bin\/sh\r\nunable to find group root: no matching entries in group file\r\n\r\nreal    0m0.067s\r\nuser    0m0.017s\r\nsys     0m0.011s\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n","comments":["The 10 second timeout comes from:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/debcb769393dd31cb1569b0cf5a80e0985fa5c6d\/daemon\/resize.go#L40-L61\r\n\r\nIt doesn't happen if you use `-d`:\r\n```bash\r\n$ ID=$(docker run -d docker\/desktop-kubernetes-pause:3.7)\r\n$ time docker exec -dit $ID \/bin\/sh\r\nError response from daemon: open \/var\/lib\/docker\/rootfs\/overlayfs\/e4ed54e27b236d511975d609c8c435bcc0168782e999ca842e0bee41e0256b6a\/etc\/group: no such file or directory\r\n\r\nreal    0m0.019s\r\nuser    0m0.010s\r\nsys     0m0.006s\r\n```\r\n\r\n\r\nFor some reason, the reason is not returned on exec start and the CLI already is already at a later stage where it monitors the tty resize signals:\r\nhttps:\/\/github.com\/docker\/cli\/blob\/cddd1869a8d1f9e0f1350e805340c09f65daf81c\/cli\/command\/container\/exec.go#L202-L206","Hmm, looks like the API returns the error differently with `Detach`:\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/debcb769393dd31cb1569b0cf5a80e0985fa5c6d\/api\/server\/router\/container\/exec.go#L152-L159","Is it executing the group lookup (`GIDFromPath`)? https:\/\/github.com\/containerd\/containerd\/blob\/194a1fdd2cde35bc019ef138f30485e27fe0913e\/oci\/spec_opts.go#L1069-L1083\r\n\r\nAlso wondering if that should special handling for \"not found\" errors for those files.\r\n\r\n\r\nThere's also another possibility; is there a codepath where something may end up trying `getent` ? https:\/\/github.com\/moby\/moby\/blob\/debcb769393dd31cb1569b0cf5a80e0985fa5c6d\/pkg\/idtools\/idtools_unix.go#L103-L116 ","Yes, `WithAppendAdditionalGroups` explicitly reads the `\/etc\/group` and errors out if it's not accessible:\r\nhttps:\/\/github.com\/containerd\/containerd\/blob\/194a1fdd2cde35bc019ef138f30485e27fe0913e\/oci\/spec_opts.go#L884-L897\r\n\r\nFun fact, it uses a function from `moby\/sys` to parse the groups file:\r\nhttps:\/\/github.com\/moby\/sys\/blob\/4950d7687cf6c9b138dc0e18c2c7351e1f6ed497\/user\/user.go#L156-L164 ","> Fun fact, it uses a function from moby\/sys to parse the groups file:\r\n\r\nYes, that's the code that was originally in runc, and was moved to a separate module","I'm also curious what it's trying to look up in this case; It looks like `GIDFromPath` unconditionally reads `\/etc\/group`, then takes the first entry it finds as `group` and uses its ID;\r\n\r\n```bash\r\ndocker run --rm alpine cat \/etc\/group\r\nroot:x:0:root\r\nbin:x:1:root,bin,daemon\r\n...\r\n```\r\n\r\nSo even if no user is set, it's reading those files to (surprise!) find out that it must run as `0` (`root`)?\r\n\r\nWhich makes me curious;\r\n\r\n- MUST the first entry always be for `root` ?\r\n- Is it possible \/ allowed to have a different group ID for the first entry? (i.e., `root` does not have `0` as group?)\r\n\r\nAll of that at least assumes the container is running a \"distro\" image, and does not take `from scratch` images into account;\r\n\r\n- if no user is defined (`\"\"`); should it just bail out and use `0`?\r\n- if `\/etc\/group` and\/or `\/etc\/password` are not present, should it just use `0` for both? (i.e.: `from scratch` containers)\r\n- for numeric \"user\" or \"group\", should it even look-up anything? (there was some debate bout that, and possibly that was changed already as part of a \"CVE\" (fun fact: Linux userland tools DO allow a user-name or group-name that's a number))\r\n","Opened a PR in containerd https:\/\/github.com\/containerd\/containerd\/pull\/9494","- opened https:\/\/github.com\/moby\/moby\/issues\/46917 for my comment above","Updated this issue: removed the `containerd-integration` label because it happens with graph drivers too\r\n"],"labels":["status\/0-triage","kind\/bug"]},{"title":"integration-cli: Update plugin test install image error","body":"Check for the error returned by the plugin pull logic based off the containerd pull code. This code is used regardless of graphdriver or snapshotter.\r\n\r\nLocally this test is failing for me on overlay2 and overlayfs snapshotter. The message which was previously being checked is in the old pull code which the plugin logic moved away from a few years ago. Let's see is CI handles it differently.\r\n\r\n","comments":["I don't see any failures but I also don't know which job would be running `TestDockerPluginSuite\/TestPluginInstallImage`. It definitely should have been failing before this change as well.","> I don't see any failures but I also don't know which job would be running `TestDockerPluginSuite\/TestPluginInstallImage`. It definitely should have been failing before this change as well.\r\n\r\nHeh.... I _may_ know why (\ud83d\ude22); \r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/41559"],"labels":["status\/2-code-review","area\/testing","containerd-integration"]},{"title":"client: deprecate unbounded API version auto-negotiation","body":"The Engine API version affects the semantics of API requests and responses, not just their shapes. The client package cannot always faithfully convert between the shape and semantics of the Go API between the client package and the software which uses it for every API version supported by the client. Some of those semantics necessarily leak into the software. The most obvious examples of this are cases where functionality is not supported outright over a particular API version. I suspect there are other, more subtle cases as well, particularly around API responses.\r\n\r\nThe `client.WithAPIVersionNegotiation()` option ostensibly makes it easy to write software that targets a wide range of daemon versions by selecting the newest API version supported by both the API client and the daemon. Trouble is, the API version supported by the software _using_ the client is not taken into consideration. Similarly, `client.WithVersionFromEnv()` gives the user running the software the ability to force an API version that the software is not expecting. For user-agent software such as docker\/cli these are not necessarily problems as the differences in API versions are bubbled up to the human or script using the software. But for automated client software which ultimately has to handle the API requests and responses itself, such as cri-dockerd, an unexpected API version could lead to subtle or disastrous issues. Providing unbounded version negotiation and overriding API version from environment variables as features built into the client package encourages users to opt in, even when it would be inappropriate and ill-advised for their application.\r\n\r\nConfiguring the Engine API client with the API version fixed to that of the oldest daemon supported by the client software would be the best choice for the majority of client software as it minimizes the number of engine versions that need to be tested and maximizes forward compatibility. More advanced client software could support multiple API versions without blowing up the testing matrix by performing more controlled negotiation from a small set of API versions, e.g. by using `(*Client).Ping()` to introspect the daemon's API version. We could even provide utilities to assist with such negotiation if we wanted to, e.g.:\r\n```go\r\nc, err := client.NewClientWithOpts(client.WithAPIVersionSelection(\"1.44\", \"1.41\", \"1.36\"))\r\n```\r\n\r\nTo keep users away from the pit of failure, unbounded version selection (auto-negotiating any API version between 1.24 and latest without restriction, or allowing the end-user to force an arbitrary API version) should not be easy to opt into by any consumer of the `client` package. I think we should deprecate (or completely remove) the `WithAPIVersionNegotiation`, `WithVersionFromEnv` and `FromEnv` options from the client package, and move the auto-negotiation logic used by docker\/cli into the docker\/cli repo. Any third-party client software which wants unbounded API version selection would still be free to implement it themself, which should hopefully give them the time to reflect on why that would be an unwise choice.","comments":["Maintainer call:\r\n- `client.WithAPIVersionNegotiation()` has not shot enough feet off yet (C: that we know of) to justify taking the footgun away\r\n  - They would find more creative ways to shoot themselves in the foot anyway if we took away the footgun, so there is no point trying(???)\r\n    - Drive-by contributors \"helpfully\" extending the list of supported API versions in client applications without the contributor or app maintainers doing their due diligence\r\n    - App maintainers upgrading the client package version but forgetting to update the API version selection list could limit our ability to drop support for old API versions in the Engine\r\n      - C: So? We drop support anyway, the client software fails fast when pointed at a newer Engine version, client software fixes their API version selection list and validates their client against the newest Engine.\r\n      - C: Not automatically raising the max negotiated version when the client is updated is a feature. It would prevent surprises if the client package was updated in an application due to a transitive dependency.\r\n- People don't want to think about version negotiation\r\n  - Abstract away the version differences entirely with a (hypothetical) SDK\r\n    - C: How could we abstract away the lack of features which only exist in newer API versions?\r\n  - Until we provide an SDK which abstracts away versioning differences, we should not take away people's ability to negotiate API versions their apps are unprepared to handle(???)\r\n- \"The client already abstracts away some version differences.\" C: [The client code in question](https:\/\/github.com\/moby\/moby\/blob\/65973c6c40a32380f4f7318de92eb6c8161742f0\/client\/container_create.go#L23-L44) does nothing of the sort. Its purpose is to fail fast when the negotiated API version does not support some parameters, avoiding a round-trip.\r\n\r\nAdding `client.WithAPIVersionSelection` and deprecating `client.WithVersionFromEnv` were much less controversial than deprecating `client.WithAPIVersionNegotiation`."],"labels":["area\/api","status\/0-triage","kind\/enhancement"]},{"title":"Deprecate unversioned Engine API usage","body":"When API requests are made without a version prefix, the daemon treats the behaves as if the request specified the newest API version supported by the daemon. Any client software which makes unversioned API requests can therefore be subject to the semantics of API versions newer than the version it was tested against, potentially leading to breakage and compatibility issues which could have been avoided if the client was explicit about the API version it wanted to operate under.\r\n\r\nI propose that we freeze the API version implied by unversioned API requests so that unversioned API clients tested against existing daemon versions will continue to work across daemon upgrades. Any client which wants to make use of functionality introduced in API versions introduced after the freeze would have to opt into it by versioning the API. The unversioned API would effectively be deprecated in that software using it would continue to work as it always has, but no new functionality would be available without first migrating to a versioned API.","comments":["Freezing the version will have the side effect of putting an upper bound on API versions we could remove support for. We would have to remove unversioned API support completely at some future point after adopting this proposal."],"labels":["area\/api","status\/0-triage","kind\/enhancement","area\/daemon"]},{"title":"Timeout Function","body":"### Description\n\n# RFC: Implementation of a Timeout Function in Docker CLI\r\n\r\n## Introduction\r\n\r\nThis RFC proposes the implementation of a timeout function in the Docker CLI. The function would allow a user to specify a timeout period after which the execution of a Docker container would be automatically terminated.\r\n\r\n## Motivation\r\n\r\nCurrently, Docker does not have a built-in command to set a timeout for containers. This can be problematic in scenarios where a container is expected to run for a specific duration, but might continue to run indefinitely due to an error or an unhandled exception. A timeout function would provide a way to prevent such situations, ensuring that containers do not consume resources indefinitely.\r\n\r\n## Proposed Solution\r\n\r\nThe proposed solution is to add a new command-line option, `--timeout`, to the `docker run` command. This option would accept a time duration as an argument, after which the execution of the container would be terminated. The time duration would be specified in a format similar to the `timeout` command in Unix-based systems.\r\n\r\nHere's an example of how the `docker run` command with the `--timeout` option might look:\r\n\r\n```bash\r\ndocker run --timeout 30s ubuntu sleep infinity\r\n```\r\n\r\nIn this example, the `ubuntu` container would be automatically stopped after 30 seconds, even though the `sleep infinity` command inside the container is designed to run indefinitely.\r\n\r\n## Implementation Details\r\n\r\nThe implementation of the `--timeout` option would involve changes to the Docker CLI and the Docker daemon. The Docker CLI would need to accept the `--timeout` option and pass it to the Docker daemon. The Docker daemon would then need to start a timer when the container is started, and send a `SIGTERM` signal to the container when the timer expires.\r\n\r\nThe Docker CLI and Docker daemon would need to handle the case where the container is already stopped when the timer expires. In such cases, the Docker CLI and Docker daemon would need to ignore the `SIGTERM` signal.\r\n\r\nIn addition, we can get a proof of concept (POC) by setting the `Cmd` field in the `ExecConfig` structure located in `api\/types\/configs.go`. The `Cmd` field is a slice of strings that represents the execution commands and arguments for the container. By controlling this field, we can control the command that is run in the container, which can be used to implement the timeout functionality.\r\n\r\n``` go\r\n\t\/\/ ExecConfig is a small subset of the Config struct that holds the configuration\r\n\t\/\/ for the exec feature of docker.\r\ntype ExecConfig struct {\r\n\tUser string \/\/ User that will run the command\r\n\tPrivileged bool \/\/ Is the container in privileged mode\r\n\tTty bool \/\/ Attach standard streams to a tty.\r\n\tConsoleSize *[2]uint `json:\",omitempty\"` \/\/ Initial console size [height, width]\r\n\tAttachStdin bool \/\/ Attach the standard input, makes possible user interaction\r\n\tAttachStderr bool \/\/ Attach the standard error\r\n\tAttachStdout bool \/\/ Attach the standard output\r\n\tDetach bool \/\/ Execute in detach mode\r\n\tDetachKeys string \/\/ Escape keys for detach\r\n\tEnv []string \/\/ Environment variables\r\n\tWorkingDir string \/\/ Working directory\r\n\tCmd []string \/\/ Execution commands and args\r\n}\r\n```\r\n\r\n## Necessary Changes\r\n\r\nFrom our understanding, there would need to be a change in the `Cmd` attribute of `ExecConfig` inside the `create` funciton in the `daemon\/create.go` file. This function is responsible for creating a new contianer from the configuration. A change following this logic seems to be enough at least for Docker desktop. (Of course, there would need to keep in mind the current OS and more, as to follow the design of the rest of Docker.)\r\n\r\n```go\r\nif len(opts.params.Config.Cmd) > 0 {\r\n\t\topts.params.Config.Cmd = append([]string{\"\/bin\/sh\", \"-c\"}, opts.params.Config.Cmd...)\r\n\t\topts.params.Config.Cmd[len(opts.params.Config.Cmd)-1] += \" &\"\r\n\t}\r\n\topts.params.Config.Cmd = strslice.StrSlice(append(opts.params.Config.Cmd, \"sleep\", \"60\", \";\", \"exit\"))\r\n```\r\n\r\nThis would guarantee that the `sleep` command runs asynchrony with the users CMD, and being followed by the `exit` statement that will finish execution of the contianer, killing the main process.\r\n\r\n## Impact on Existing Systems\r\n\r\nThe proposed changes would not impact existing Docker containers, as the `--timeout` option would be optional. Existing Docker containers would continue to run until they are explicitly stopped or they crash.\r\n\r\n## Conclusion\r\n\r\nThe proposed changes would provide a robust and convenient way to set a timeout for Docker containers. The implementation of the `--timeout` option would require changes to both the Docker CLI and the Docker daemon, but these changes would be relatively straightforward and would not impact existing Docker containers. The proposed changes would be a valuable addition to the Docker platform, improving its robustness and usability.","comments":["- related \/ previous discussion: https:\/\/github.com\/moby\/moby\/issues\/1905","+1"],"labels":["status\/0-triage","kind\/feature"]},{"title":"Old image lost on image pull","body":"### Description\n\nWhen a `docker pull` updates an image we are losing the old image.\n\n### Reproduce\n\n```console\r\n$ docker pull debian:oldstable-20200514\r\n$ docker tag debian:oldstable-20200514 debian:stable\r\n$ docker image rm -f debian:oldstable-20200514\r\n$ docker pull debian:stable\r\n$ docker images\r\n```\r\n\r\nWith graph drivers the last command shows\r\n```console\r\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\r\ndebian       stable    c505deff3653   8 days ago    139MB\r\ndebian       <none>    476d4a705c2f   3 years ago   95.8MB\r\n```\r\n\r\nAnd with containerd we end up with \r\n```console\r\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\r\ndebian       stable    ed69ce2733ba   8 days ago   205MB\r\n```\r\n\r\nThe old image was lost.\n\n### Expected behavior\n\nAfter the second pull the old image should still be present\n\n### docker version\n\n```bash\nmaster\n```\n\n\n### docker info\n\n```bash\nmaster\n```\n\n\n### Additional Info\n\n_No response_","comments":["This sounds related to the discussion we had on \"always creating a `dangling` image\" to keep a reference (with digest). In that case, removing the tag from the image would keep the dangling image (`<none>`)","Oh, actually, I guess the old case wasn't creating a dangling image, only removing the `tag` (but  keeping the `debian` repo)\r\n\r\nAlso somewhat considering if we should consider this desired behavior; the old image effectively is replaced with a new one, so should we consider the old one candidate for garbage collection? (question there is if we want to do that _immediately_)","I asked myself the same, if there is a container already using the image that will be updated after a `pull` then yeah, we want to keep that image a little while longer. But once the container is removed, is there really a need to keep the old image?...\r\n\r\nBut then again, you could imaging someone pulling a new version of the image, test it out and decide they want to rollback, if we remove the image they would need to re-pull the old image, if it's still there then a `tag` is enough.\r\n\r\nI'm not sure how useful keeping the old image is useful... Removing the old image if a pull updates it would help in the most painful point people have with docker: images taking too much space.","Yes; this needs some thinking;\r\n\r\n- FWIW, restarting container should not update image (IIRC, we resolve digest, and persist that, to prevent that happening; in that case we refer to the image by digest once the tag is gone)\r\n- for rollback; that's indeed the tricky one: it may be unexpected for the image to be fully gone _immediately_, but perhaps \"less unexpected\" for it to be garbage-collected _eventually_ \r\n- similarly: `docker build` multiple times; do we want to keep the \"previous build\" (but untagged)? To some extent we have build-cache to build the image again (but the source material to build it may no longer be present)\r\n- \u261d\ufe0f both are also cases where it would be beneficial to distinguish \"pulled\" images from \"locally built\" (pulled images can be pulled again, so usually \"no harm done\" if the image is removed from the cache) \r\n- \u261d\ufe0f I think eventually, we want to make managing the image cache easier, and less \"manual\" (`docker system prune`), so that we can more depend on automatic garbage-collecting unused content\r\n\r\n\r\nBut finding the right balance is hard (and may differ between use-cases), so perhaps some amount of \"tweak-ability\" (config) may be needed. As well as (possibly) more polished UX to manage content ready for garbage collection (separate `trash` namespace that keeps X versions of an image, and a way to \"empty the trash\"?)"],"labels":["status\/0-triage","kind\/bug","area\/images","containerd-integration"]},{"title":"BuildKit gc (garbage-collection) config is complicated, and doesn't allow for multiple filters in daemon.json","body":"### Description\r\n\r\n- This relates to https:\/\/github.com\/moby\/moby\/pull\/46861\r\n- slightly related: https:\/\/github.com\/moby\/buildkit\/issues\/1359\r\n\r\n\r\nDefault on my Docker Desktop is;\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"features\": {\r\n      \"buildkit\": true\r\n    },\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GB\",\r\n      \"enabled\": true\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhich, when using `docker builder inspect` (`docker buildx inspect`), showed;\r\n\r\n```\r\nGC Policy rule#0:\r\n All:           false\r\n Filters:       type==source.local,type==exec.cachemount,type==source.git.checkout\r\n Keep Duration: 172.8\u00b5s\r\n Keep Bytes:    790.6MiB\r\nGC Policy rule#1:\r\n All:           false\r\n Keep Duration: 5.184ms\r\n Keep Bytes:    5.588GiB\r\nGC Policy rule#2:\r\n All:        false\r\n Keep Bytes: 5.588GiB\r\nGC Policy rule#3:\r\n All:        true\r\n Keep Bytes: 5.588GiB\r\n```\r\n\r\nThe size values appear to be because it's calculating percentages (in addition to presenting as GiB? 5.588GiB = 6GB). However, `GiB`\r\n\r\nSo to try fixing the issue from https:\/\/github.com\/moby\/moby\/pull\/46861 by manually configuring the daemon to have the proper values, I tried writing the defaults in the `daemon.json` (as documented in https:\/\/docs.docker.com\/build\/cache\/garbage-collection\/); https:\/\/github.com\/moby\/moby\/blob\/f6533a1df1604d4e1fa2cabb1e66640187e25dba\/builder\/builder-next\/worker\/gc.go#L29-L50\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GB\",\r\n      \"enabled\": true,\r\n      \"policy\": [\r\n        {\r\n          \"keepStorage\": \"829MB\",\r\n          \"filter\": [\"type==source.local\",\"type==exec.cachemount\",\"type==source.git.checkout\",\"unused-for=48h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"filter\": [\"unused-for=1440h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"all\": true\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nBut that didn't work; it produces an error;\r\n\r\n```bash\r\n$ dockerd\r\n...\r\ncould not get builder GC policy: filters expect only one value\r\n```\r\n\r\nInitially I thought that would be if `filter` was a _string_, not an array of strings (so just a comma-separated list of filters), so I updated;\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"features\": {\r\n      \"buildkit\": true\r\n    },\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GB\",\r\n      \"enabled\": true,\r\n      \"policy\": [\r\n        {\r\n          \"keepStorage\": \"829MB\",\r\n          \"filter\": \"type==source.local,type==exec.cachemount,type==source.git.checkout,unused-for=48h\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"filter\": \"unused-for=1440h\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"all\": true\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThat produces an ugly error;\r\n\r\n```bash\r\n$ dockerd\r\n...\r\nunable to configure the Docker daemon with file \/etc\/docker\/daemon.json: json: cannot unmarshal string into Go struct field BuilderGCRule.builder.GC.Policy.Filter of type map[string]map[string]bool\r\n```\r\n\r\nSo, perhaps an array, but single value with the options comma-separated?\r\n\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"features\": {\r\n      \"buildkit\": true\r\n    },\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GB\",\r\n      \"enabled\": true,\r\n      \"policy\": [\r\n        {\r\n          \"keepStorage\": \"829MB\",\r\n          \"filter\": [\"type==source.local,type==exec.cachemount,type==source.git.checkout,unused-for=48h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"filter\": [\"unused-for=1440h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"all\": true\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhich produced an ugly stack trace;\r\n\r\n```\r\nERRO[2023-11-29T09:43:34.279200420Z] gc error: filters: parse error: [type >|=|< ==source.local,type==exec.cachemount,type==source.git.checkout,unused-for=48h]: unsupported operator \"===\": invalid argument\r\nfailed to parse prune filters [type===source.local,type==exec.cachemount,type==source.git.checkout,unused-for=48h]\r\ngithub.com\/moby\/buildkit\/cache.(*cacheManager).pruneOnce\r\n\t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/buildkit\/cache\/manager.go:1023\r\ngithub.com\/moby\/buildkit\/cache.(*cacheManager).Prune\r\n\t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/buildkit\/cache\/manager.go:1003\r\ngithub.com\/docker\/docker\/builder\/builder-next\/worker.(*Worker).Prune\r\n\t\/go\/src\/github.com\/docker\/docker\/builder\/builder-next\/worker\/worker.go:246\r\ngithub.com\/moby\/buildkit\/control.(*Controller).gc.func2.1\r\n\t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/moby\/buildkit\/control\/control.go:551\r\ngolang.org\/x\/sync\/errgroup.(*Group).Go.func1\r\n\t\/go\/src\/github.com\/docker\/docker\/vendor\/golang.org\/x\/sync\/errgroup\/errgroup.go:75\r\nruntime.goexit\r\n\t\/usr\/local\/go\/src\/runtime\/asm_arm64.s:1172\r\n```\r\n\r\nGoing back to the documentation, it shows an example to set up this config in BuildKit's config file, which _DOES_ appear to support the multiple filters (but also shows a field for `keepDuration` (and `keepBytes`), so no need to do the messy conversion to `unused-for=48h` ?;\r\n\r\n```toml\r\n[worker.oci]\r\n  gc = true\r\n  gckeepstorage = 10000\r\n  [[worker.oci.gcpolicy]]\r\n    keepBytes = 512000000\r\n    keepDuration = 172800\r\n    filters = [ \"type==source.local\", \"type==exec.cachemount\", \"type==source.git.checkout\"]\r\n  [[worker.oci.gcpolicy]]\r\n    all = true\r\n    keepBytes = 1024000000\r\n```\r\n\r\nFrom the above, it looks like we're missing some handling in dockerd to allow for additional filters to be configured.\r\n\r\n\r\n\r\n### docker version\r\n\r\n```\r\nClient:\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:30:04 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:30 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.3\r\n  GitCommit:        7880925980b188f4c97b462f709d0db8e8962aff\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n","comments":["This config was accepted, but is missing the filters;\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GB\",\r\n      \"enabled\": true,\r\n      \"policy\": [\r\n        {\r\n          \"keepStorage\": \"829MB\",\r\n          \"filter\": [\"unused-for=48h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"filter\": [\"unused-for=1440h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GB\",\r\n          \"all\": true\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAbove produced:\r\n\r\n```\r\nGC Policy rule#0:\r\n All:           false\r\n Filters:\r\n Keep Duration: 48h0m0.626576672s\r\n Keep Bytes:    829MiB\r\nGC Policy rule#1:\r\n All:           false\r\n Filters:\r\n Keep Duration: 1440h0m0.626578339s\r\n Keep Bytes:    6GiB\r\nGC Policy rule#2:\r\n All:        false\r\n Filters:\r\n Keep Bytes: 6GiB\r\nGC Policy rule#3:\r\n All:        true\r\n Filters:\r\n Keep Bytes: 6GiB\r\n```\r\n","Also, it looks like there's a bug in either parsing, or presentation;\r\n\r\n\r\nThe config has `GB`;\r\n\r\n```\r\n\"keepStorage\": \"6GB\",\r\n```\r\n\r\nBut inspect shows `GiB`;\r\n\r\n```\r\nKeep Bytes:    6GiB\r\n```","Alright; looks like parsing considers `GiB` and `GB` to be the same; this JSON also works, but also gives `GiB`\r\n\r\n```json\r\n{\r\n  \"builder\": {\r\n    \"gc\": {\r\n      \"defaultKeepStorage\": \"20GiB\",\r\n      \"enabled\": true,\r\n      \"policy\": [\r\n        {\r\n          \"keepStorage\": \"829MiB\",\r\n          \"filter\": [\"unused-for=48h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GiB\",\r\n          \"filter\": [\"unused-for=1440h\"]\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GiB\"\r\n        },\r\n        {\r\n          \"keepStorage\": \"6GiB\",\r\n          \"all\": true\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n","FWIW, the `filters expect only one value` error is https:\/\/github.com\/moby\/moby\/blob\/718fafed263a094be88550414eb7504941419779\/builder\/builder-next\/builder.go#L44\r\n\r\nWhich is produced in `toBuildkitPruneInfo`: https:\/\/github.com\/moby\/moby\/blob\/718fafed263a094be88550414eb7504941419779\/builder\/builder-next\/builder.go#L656-L658\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/718fafed263a094be88550414eb7504941419779\/builder\/builder-next\/builder.go#L673-L675"],"labels":["area\/builder","kind\/enhancement","kind\/bug","area\/builder\/buildkit","version\/24.0"]},{"title":"containerd: Remove `unleaseSnapshotsFromDeletedConfigs`","body":"### Description\n\n\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/f6533a1df1604d4e1fa2cabb1e66640187e25dba\/daemon\/containerd\/image_delete.go#L128-L143\r\n\r\nThis was originally introduced by https:\/\/github.com\/moby\/moby\/pull\/45339\/commits\/52af6d957eee8640eff9a2bafb33b8de5f43537f as a workaround for [buildkit bug](https:\/\/github.com\/moby\/buildkit\/issues\/3797) (history entries were adding gc.ref labels to config blobs).\r\n\r\nThe buildkit bug is now fixed.\r\n\r\n- [ ] Verify that this workaround can be removed\r\n- [ ] Remove the code (partially revert 52af6d957eee8640eff9a2bafb33b8de5f43537f)\r\n\r\n\n\n### Reproduce\n\n-\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nAt the time of creating this ticket, the master was at f6533a1df1604d4e1fa2cabb1e66640187e25dba\n```\n\n\n### docker info\n\n```bash\n-\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["kind\/bug","kind\/refactor","containerd-integration"]},{"title":"Swarm manager address not bound to IPv4 but only IPv6","body":"### Description\r\n\r\nWe've currently a strange issue where one of our (automatically provisioned) swarm manager nodes is not bound to the (IPv4) address+port provided via  `--advertise-addr`  but to all IPv6-addresses instead:\r\n\r\n```\r\n[root@BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager core]# docker swarm init --advertise-addr 10.112.18.3:2377\r\nSwarm initialized: current node (87gknxe267r9xuq0ktfj7z2ta) is now a manager.\r\n\r\nTo add a worker to this swarm, run the following command:\r\n\r\n    docker swarm join --token SWMTKN-1-63qm1y8gcas04vl1h1f584aoimndhtvz12meqn4zqw8a75htw4-6f85k93sfh73vw07f0n9wdi8h 10.112.18.3:2377\r\n\r\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\r\n\r\n[root@BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager core]# netstat -lnt\r\nActive Internet connections (only servers)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State      \r\ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN     \r\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     \r\ntcp        0      0 127.0.0.54:53           0.0.0.0:*               LISTEN     \r\ntcp        0      0 0.0.0.0:5355            0.0.0.0:*               LISTEN     \r\ntcp        0      0 0.0.0.0:5666            0.0.0.0:*               LISTEN     \r\ntcp6       0      0 :::22                   :::*                    LISTEN     \r\ntcp6       0      0 :::2377                 :::*                    LISTEN     \r\ntcp6       0      0 :::5355                 :::*                    LISTEN     \r\ntcp6       0      0 :::5666                 :::*                    LISTEN     \r\ntcp6       0      0 :::7946                 :::*                    LISTEN     \r\n[root@BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager core]# ip a\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link\/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1\/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1\/128 scope host noprefixroute \r\n       valid_lft forever preferred_lft forever\r\n2: net0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\r\n    link\/ether 02:ef:e5:69:ba:30 brd ff:ff:ff:ff:ff:ff\r\n    altname enp11s0\r\n    inet 10.112.18.3\/32 scope global noprefixroute net0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::5224:d788:c1bd:e9f1\/64 scope link noprefixroute \r\n       valid_lft forever preferred_lft forever\r\n4: docker_gwbridge: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \r\n    link\/ether 02:42:79:c2:6a:79 brd ff:ff:ff:ff:ff:ff\r\n    inet 172.17.1.1\/24 brd 172.17.1.255 scope global docker_gwbridge\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::42:79ff:fec2:6a79\/64 scope link proto kernel_ll \r\n       valid_lft forever preferred_lft forever\r\n5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \r\n    link\/ether 02:42:3f:8a:27:df brd ff:ff:ff:ff:ff:ff\r\n    inet 172.17.0.1\/24 brd 172.17.0.255 scope global docker0\r\n       valid_lft forever preferred_lft forever\r\n    inet6 fe80::42:3fff:fe8a:27df\/64 scope link proto kernel_ll \r\n       valid_lft forever preferred_lft forever\r\n28: veth9489458@if27: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker_gwbridge state UP group default \r\n    link\/ether 2a:0d:9d:8b:71:9d brd ff:ff:ff:ff:ff:ff link-netnsid 1\r\n    inet6 fe80::280d:9dff:fe8b:719d\/64 scope link proto kernel_ll \r\n       valid_lft forever preferred_lft forever\r\n\r\n```\r\n\r\nThe problem was noticed as other nodes where not able to join the cluster via the intended IPv4 address.\r\n\r\nWe have not been successful so far to reproduce this on any other machine. Rebooting the machine does also not change the issue.\r\n\r\nMight be the same issue as discussed here: https:\/\/forums.docker.com\/t\/docker-swarm-service-do-not-bind-on-ipv4-only-on-ipv6\/137718\/23\r\n\r\n### Reproduce\r\n\r\nI'm not sure how to reliably reproduce this as other VMs with the same software versions and deployment-process do not have this issue. But on that exact VM we can reliably reproduce this by just running `docker swarm leave --force` and `docker swarm init --advertise-addr 10.112.18.3:2377` again.\r\n\r\n### Expected behavior\r\n\r\nSwarm manager is bound to interface provided via `--advertise-addr`.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.23\r\n API version:       1.41\r\n Go version:        go1.20rc3\r\n Git commit:        %{shortcommit_cli}\r\n Built:             Sun Jan 29 17:25:05 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.20rc3\r\n  Git commit:       %{shortcommit_moby}\r\n  Built:            Sun Jan 29 17:25:05 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.19\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 20.10.23\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: t3d8ypabcqg1qawo60oi1ynep\r\n  Is Manager: true\r\n  ClusterID: fkblhrelrqii3a79d212tdm5m\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.112.18.3\r\n  Manager Addresses:\r\n   10.112.18.3:2377\r\n Runtimes: io.containerd.runtime.v1.linux runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: \/usr\/libexec\/docker\/docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 6.5.8-200.fc38.x86_64\r\n Operating System: Fedora CoreOS 38.20231027.3.2\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.773GiB\r\n Name: BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager\r\n ID: TE2J:ARAT:ZZQN:5IUI:COEK:CZZX:SU4I:565M:UZOX:E4OM:QPJS:FRDI\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0\/16, Size: 24\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nThe VM is running [Fedora CoreOS 38.20231027.3.2](https:\/\/fedoraproject.org\/coreos\/release-notes?arch=x86_64&stream=stable) (the automatic updates via zincati are disabled so the software versions are what is stated there).\r\n\r\nLogs of the docker daemon on that host when it was provisioned and declared as a swarm manager node:\r\n\r\n```\r\n# journalctl -u docker.service\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager systemd[1]: Starting docker.service - Docker Application Container Engine...\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.885266449+01:00\" level=info msg=\"Starting up\"\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.889606717+01:00\" level=info msg=\"libcontainerd: started new containerd process\" pid=2204\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.889643724+01:00\" level=info msg=\"parsed scheme: \\\"unix\\\"\" module=grpc\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.889650303+01:00\" level=info msg=\"scheme \\\"unix\\\" not registered, fallback to default scheme\" module=grpc\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.889668654+01:00\" level=info msg=\"ccResolverWrapper: sending update to cc: {[{unix:\/\/\/var\/run\/docker\/containerd\/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}\" module=grpc\r\nNov 24 16:27:12 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:12.889675981+01:00\" level=info msg=\"ClientConn switching balancer to \\\"pick_first\\\"\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13+01:00\" level=warning msg=\"containerd config version `1` has been deprecated and will be removed in containerd v2.0, please switch to version `2`, see https:\/\/github.com\/containerd\/containerd\/blob\/main\/docs\/PLUGINS.md#version-header\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.152018990+01:00\" level=info msg=\"starting containerd\" revision= version=1.6.19\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.175036116+01:00\" level=info msg=\"loading plugin \\\"io.containerd.content.v1.content\\\"...\" type=io.containerd.content.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.175126482+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.aufs\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178549881+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.aufs\\\"...\" error=\"aufs is not supported (modprobe aufs failed: exit status 1 \\\"modprobe: FATAL: Module aufs not found in directory \/lib\/modules\/6.5.8-200.fc38.x86_64\\\\n\\\"): skip plugin\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178575327+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.btrfs\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178719364+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.btrfs\\\"...\" error=\"path \/var\/lib\/docker\/containerd\/daemon\/io.containerd.snapshotter.v1.btrfs (xfs) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178735034+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.devmapper\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178747200+01:00\" level=warning msg=\"failed to load plugin io.containerd.snapshotter.v1.devmapper\" error=\"devmapper not configured\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178756227+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.native\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.178882905+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.overlayfs\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.179089376+01:00\" level=info msg=\"loading plugin \\\"io.containerd.snapshotter.v1.zfs\\\"...\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.180574951+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.snapshotter.v1.zfs\\\"...\" error=\"path \/var\/lib\/docker\/containerd\/daemon\/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin\" type=io.containerd.snapshotter.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.180596573+01:00\" level=info msg=\"loading plugin \\\"io.containerd.metadata.v1.bolt\\\"...\" type=io.containerd.metadata.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.180641804+01:00\" level=warning msg=\"could not use snapshotter devmapper in metadata plugin\" error=\"devmapper not configured\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.180654535+01:00\" level=info msg=\"metadata content store policy set\" policy=shared\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184731038+01:00\" level=info msg=\"loading plugin \\\"io.containerd.differ.v1.walking\\\"...\" type=io.containerd.differ.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184758859+01:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.exchange\\\"...\" type=io.containerd.event.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184772435+01:00\" level=info msg=\"loading plugin \\\"io.containerd.gc.v1.scheduler\\\"...\" type=io.containerd.gc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184840895+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.introspection-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184859601+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.containers-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184872646+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.content-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184883903+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.diff-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184897110+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.images-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184919429+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.leases-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184930707+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.namespaces-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184942279+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.snapshots-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.184953853+01:00\" level=info msg=\"loading plugin \\\"io.containerd.runtime.v1.linux\\\"...\" type=io.containerd.runtime.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.185047735+01:00\" level=info msg=\"loading plugin \\\"io.containerd.runtime.v2.task\\\"...\" type=io.containerd.runtime.v2\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.185360019+01:00\" level=info msg=\"loading plugin \\\"io.containerd.monitor.v1.cgroups\\\"...\" type=io.containerd.monitor.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187518667+01:00\" level=info msg=\"loading plugin \\\"io.containerd.service.v1.tasks-service\\\"...\" type=io.containerd.service.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187556234+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.introspection\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187572299+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.restart\\\"...\" type=io.containerd.internal.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187678018+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.containers\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187694740+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.content\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187705850+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.diff\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187716256+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.events\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187778080+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.healthcheck\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187799599+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.images\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187813280+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.leases\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187823604+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.namespaces\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187835679+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.opt\\\"...\" type=io.containerd.internal.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187963249+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.snapshots\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187978681+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.tasks\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.187989706+01:00\" level=info msg=\"loading plugin \\\"io.containerd.grpc.v1.version\\\"...\" type=io.containerd.grpc.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.188001408+01:00\" level=info msg=\"loading plugin \\\"io.containerd.tracing.processor.v1.otlp\\\"...\" type=io.containerd.tracing.processor.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.188014673+01:00\" level=info msg=\"skip loading plugin \\\"io.containerd.tracing.processor.v1.otlp\\\"...\" error=\"no OpenTelemetry endpoint: skip plugin\" type=io.containerd.tracing.processor.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.188024682+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.tracing\\\"...\" type=io.containerd.internal.v1\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.188040690+01:00\" level=error msg=\"failed to initialize a tracing processor \\\"otlp\\\"\" error=\"no OpenTelemetry endpoint: skip plugin\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.189141985+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd-debug.sock\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.189197915+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd.sock.ttrpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.189277496+01:00\" level=info msg=serving... address=\/var\/run\/docker\/containerd\/containerd.sock\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:13.189293047+01:00\" level=info msg=\"containerd successfully booted in 0.060537s\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.209581531+01:00\" level=info msg=\"detected 127.0.0.53 nameserver, assuming systemd-resolved, so using resolv.conf: \/run\/systemd\/resolve\/resolv.conf\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.211473555+01:00\" level=info msg=\"parsed scheme: \\\"unix\\\"\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.211492031+01:00\" level=info msg=\"scheme \\\"unix\\\" not registered, fallback to default scheme\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.211508459+01:00\" level=info msg=\"ccResolverWrapper: sending update to cc: {[{unix:\/\/\/var\/run\/docker\/containerd\/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.211518299+01:00\" level=info msg=\"ClientConn switching balancer to \\\"pick_first\\\"\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.213400663+01:00\" level=info msg=\"parsed scheme: \\\"unix\\\"\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.213425529+01:00\" level=info msg=\"scheme \\\"unix\\\" not registered, fallback to default scheme\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.213437282+01:00\" level=info msg=\"ccResolverWrapper: sending update to cc: {[{unix:\/\/\/var\/run\/docker\/containerd\/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.213444748+01:00\" level=info msg=\"ClientConn switching balancer to \\\"pick_first\\\"\" module=grpc\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.256355498+01:00\" level=info msg=\"Loading containers: start.\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.449312203+01:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0\/24. Daemon option --bip can be used to set a preferred IP address\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.568198617+01:00\" level=info msg=\"Loading containers: done.\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.643370623+01:00\" level=info msg=\"Docker daemon\" commit=\"%{shortcommit_moby}\" graphdriver(s)=overlay2 version=20.10.23\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.643605805+01:00\" level=info msg=\"Daemon has completed initialization\"\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager systemd[1]: Started docker.service - Docker Application Container Engine.\r\nNov 24 16:27:13 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:13.681017937+01:00\" level=info msg=\"API listen on \/run\/docker.sock\"\r\nNov 24 16:27:19 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:19.522999624+01:00\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\nNov 24 16:27:19 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:19.524755690+01:00\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\nNov 24 16:27:19 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:19.524779118+01:00\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\nNov 24 16:27:19 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[2204]: time=\"2023-11-24T16:27:19.525665151+01:00\" level=info msg=\"starting signal loop\" namespace=moby path=\/run\/docker\/containerd\/daemon\/io.containerd.runtime.v2.task\/moby\/981d3e271af2e57ba7a8edea48de80aa4583a993a2a068c49a42c3d3b61b02df pid=2490 runtime=io.containerd.runc.v2\r\nNov 24 16:27:19 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager 981d3e271af2[1983]: VMware Tools daemon, version 11.3.0.29534 (build-18090558)\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.590520602+01:00\" level=info msg=\"parsed scheme: \\\"\\\"\" module=grpc\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.591040713+01:00\" level=info msg=\"scheme \\\"\\\" not registered, fallback to default scheme\" module=grpc\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.591065121+01:00\" level=info msg=\"ccResolverWrapper: sending update to cc: {[{\/var\/run\/docker\/swarm\/control.sock  <nil> 0 <nil>}] <nil> <nil>}\" module=grpc\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.591075827+01:00\" level=info msg=\"ClientConn switching balancer to \\\"pick_first\\\"\" module=grpc\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.591127955+01:00\" level=info msg=\"blockingPicker: the picked transport is not ready, loop back to repick\" module=grpc\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.594896635+01:00\" level=info msg=\"Listening for connections\" addr=\"[::]:2377\" module=node node.id=t3d8ypabcqg1qawo60oi1ynep proto=tcp\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.594975131+01:00\" level=info msg=\"Listening for local connections\" addr=\/var\/run\/docker\/swarm\/control.sock module=node node.id=t3d8ypabcqg1qawo60oi1ynep proto=unix\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.598285720+01:00\" level=info msg=\"31a4b8e2c38ac905 became follower at term 0\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.598426585+01:00\" level=info msg=\"newRaft 31a4b8e2c38ac905 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.598485512+01:00\" level=info msg=\"31a4b8e2c38ac905 became follower at term 1\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.603300685+01:00\" level=info msg=\"31a4b8e2c38ac905 is starting a new election at term 1\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.603463824+01:00\" level=info msg=\"31a4b8e2c38ac905 became candidate at term 2\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.603512040+01:00\" level=info msg=\"31a4b8e2c38ac905 received MsgVoteResp from 31a4b8e2c38ac905 at term 2\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.603551333+01:00\" level=info msg=\"31a4b8e2c38ac905 became leader at term 2\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.603584375+01:00\" level=info msg=\"raft.node: 31a4b8e2c38ac905 elected leader 31a4b8e2c38ac905 at term 2\" module=raft node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.605158735+01:00\" level=info msg=\"Creating default ingress network\" module=node node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.606978520+01:00\" level=info msg=\"leadership changed from not yet part of a raft cluster to t3d8ypabcqg1qawo60oi1ynep\" module=node node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:29 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:29.607038010+01:00\" level=info msg=\"dispatcher starting\" module=dispatcher node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.227043203+01:00\" level=info msg=\"manager selected by agent for new session: { }\" module=node\/agent node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.227478207+01:00\" level=info msg=\"waiting 0s before registering session\" module=node\/agent node.id=t3d8ypabcqg1qawo60oi1ynep\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.312846053+01:00\" level=info msg=\"worker t3d8ypabcqg1qawo60oi1ynep was successfully registered\" method=\"(*Dispatcher).register\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.316037611+01:00\" level=info msg=\"Initializing Libnetwork Agent Listen-Addr=0.0.0.0 Local-addr=10.112.18.3 Adv-addr=10.112.18.3 Data-addr= Remote-addr-list=[] MTU=1500\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.316134701+01:00\" level=info msg=\"New memberlist node - Node:BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager will use memberlist nodeID:0568c116d136 with config:&{NodeID:0568c116d136 Hostname:BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager BindAddr:0.0.0.0 AdvertiseAddr:10.112.18.3 BindPort:0 Keys:[[100 36 228 197 55 35 81 139 196 164 191 130 209 131 216 218] [132 22 75 192 182 194 103 31 17 99 213 58 9 167 245 224] [225 231 22 237 249 36 131 233 101 52 108 91 164 72 185 204]] PacketBufferSize:1400 reapEntryInterval:1800000000000 reapNetworkInterval:1825000000000 StatsPrintPeriod:5m0s HealthPrintPeriod:1m0s}\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.318069353+01:00\" level=info msg=\"Node 0568c116d136\/10.112.18.3, joined gossip cluster\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.319310763+01:00\" level=info msg=\"initialized VXLAN UDP port to 4789 \"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.319374763+01:00\" level=info msg=\"Node 0568c116d136\/10.112.18.3, added to nodes list\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566072888+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.conn_reuse_mode\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/conn_reuse_mode: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566121299+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_nodest_conn\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_nodest_conn: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566136557+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_quiescent_template\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_quiescent_template: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566151558+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.conn_reuse_mode\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/conn_reuse_mode: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566164799+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_nodest_conn\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_nodest_conn: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566182835+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_quiescent_template\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_quiescent_template: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566207688+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_nodest_conn\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_nodest_conn: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566221640+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_quiescent_template\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_quiescent_template: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566235318+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.conn_reuse_mode\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/conn_reuse_mode: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566248138+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.conn_reuse_mode\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/conn_reuse_mode: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566260698+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_nodest_conn\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_nodest_conn: no such file or directory\"\r\nNov 24 16:27:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:27:30.566276371+01:00\" level=error msg=\"error reading the kernel parameter net.ipv4.vs.expire_quiescent_template\" error=\"open \/proc\/sys\/net\/ipv4\/vs\/expire_quiescent_template: no such file or directory\"\r\nNov 24 16:32:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:32:30.321615456+01:00\" level=info msg=\"NetworkDB stats BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager(0568c116d136) - netID:rdj59y8ppg9hf8nlgfpjyjriw leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg\/s:0\"\r\nNov 24 16:37:30 BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager dockerd[1983]: time=\"2023-11-24T16:37:30.520753556+01:00\" level=info msg=\"NetworkDB stats BK-703a95a7c6ff69af8b16330b4533d88dd25d4f30-manager(0568c116d136) - netID:rdj59y8ppg9hf8nlgfpjyjriw leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg\/s:0\"\r\n...\r\n```","comments":["Thanks for reporting! Wondering if there's either some race-condition at hand here, but also if we're explicitly starting listeners on both `::` and `0.0.0.0` (and if we do, if that perhaps gets normalized somewhere and considered a \"duplicate\" :thinking_face:)\r\n\r\n\r\nI should note that docker no longer publishes updates for the 20.10.x version of the engine (we only maintain packages for the current stable release (24.0.x at time of writing)). \r\n\r\n> I'm not sure how to reliably reproduce this as other VMs with the same software versions and deployment-process do not have this issue.\r\n\r\nThat's unfortunate, otherwise I'd also have asked if you had a test-environment on which it reproduced (and if you'd still be able to reproduce on a current version).\r\n\r\n\/cc @akerouanton @robmry  (in case one of you happens to be stumbling upon possible causes in any related code)","I can't think of a specific reason to explain that. Maybe @corhere has some ideas?","> ### Expected behavior\r\n> Swarm manager is bound to interface provided via `--advertise-addr`.\r\n\r\nIncorrect. The Swarm manager binds to the interface provided via `--listen-addr`. [The CLI sets it to `0.0.0.0:2377` by default.](https:\/\/github.com\/docker\/cli\/blob\/c1455b6751586ee994c497013b7d49b6e4ced520\/cli\/command\/swarm\/opts.go#L18) I am not sure how the unspecified IPv4 address became the unspecified IPv6 address, but it does not matter. On Linux, <a href=\"https:\/\/man7.org\/linux\/man-pages\/man7\/ipv6.7.html#:~:text=IPV6_V6ONLY%20(since%20Linux,is%200%20(false).\">a socket listening on the unspecified IPv6 address is a dual-stack socket which will accept both IPv4 _and_ IPv6 connections, unless the `IPV6_V6ONLY` socket option is used or the file `\/proc\/sys\/net\/ipv6\/bindv6only` is frobbed.<\/a> Unless you went out of your way to set that procfs file, all the provided information suggests that the Swarm manager is listening on all interfaces, both IPv6 _and_ IPv4.\r\n\r\nYour physical network interface doesn't look so good. It has a \/32 netmask, no gateway and no default route; unless you are doing some funky things with networking and custom routes I don't see how any packets would route into or out of that interface. Are you sure other machines on the network can reach it using the address 10.112.18.3?\r\n\r\n> ```\r\n> 2: net0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\r\n>     link\/ether 02:ef:e5:69:ba:30 brd ff:ff:ff:ff:ff:ff\r\n>     altname enp11s0\r\n>     inet 10.112.18.3\/32 scope global noprefixroute net0\r\n>        valid_lft forever preferred_lft forever\r\n>     inet6 fe80::5224:d788:c1bd:e9f1\/64 scope link noprefixroute \r\n>        valid_lft forever preferred_lft forever\r\n> ```\r\n\r\n","Hello, I'm facing a similar issue where both my manager and workers are within the same network range. I believe I don't need a gateway in this scenario since they can communicate directly within the network. I've tested the ICMP protocol between them and it appears to be working fine.\r\nI'm encountering a problem, despite verifying that bind6only is disabled. Could you please assist me in resolving this issue? Can you suggest a solution for this situation?\r\nthank you in advance  \r\n@corhere @akerouanton @thaJeztah "],"labels":["area\/api","status\/0-triage","status\/more-info-needed","kind\/bug","area\/swarm","area\/daemon","version\/20.10"]},{"title":"The creation of a service using 'docker service create' is extremely slow.","body":"### Description\n\nWhy does Docker service create attempt to access the registry, and if it can't find it, keep retrying until 10 minutes have passed before starting the container? Is there a way to resolve this issue?\n\n### Reproduce\n\ndocker service create\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.4\r\n API version:       1.41 (downgraded from 1.43)\r\n Go version:        go1.20.5\r\n Git commit:        3713ee1\r\n Built:             Fri Jul  7 14:54:21 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.10\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.16.9\r\n  Git commit:       e2f740d\r\n  Built:            Mon Oct 25 07:43:13 2021\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.13\r\n  GitCommit:        9cc61520f4cd876b86e77edfeb88fbcd536d1f9d\r\n runc:\r\n  Version:          1.0.3\r\n  GitCommit:        v1.0.3-0-gf46b6ba\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.4\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.19.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 41\r\n Server Version: 20.10.10\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: ux6s4hb38o1pp0nskofuqs26t\r\n  Is Manager: true\r\n  ClusterID: 3eaundlb7khednfy3olq0hnos\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 5789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.101.7\r\n  Manager Addresses:\r\n   192.168.101.7:2377\r\n Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 9cc61520f4cd876b86e77edfeb88fbcd536d1f9d\r\n runc version: v1.0.3-0-gf46b6ba\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 3.10.0-1160.92.1.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.42GiB\r\n Name: CS-GeoPanelTest\r\n ID: MMOB:MPT3:VKI7:QIHZ:VTVS:6O6P:A4GQ:7XHQ:O6OY:WIAB:APU4:5TZW\r\n Docker Root Dir: \/data\/docker\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  zh-registry.xxxx.com.cn\r\n  127.0.0.0\/8\r\n Registry Mirrors:\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"c8d: unconditionally use ref-counting, or get info from snapshotter","body":"### Description\r\n\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/45698\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/45781\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/45781#discussion_r1235917043\r\n- slightly related to https:\/\/github.com\/moby\/moby\/issues\/46757\r\n\r\nCommit https:\/\/github.com\/moby\/moby\/commit\/21c0a54a6b2dc059fae62f46a23d338a35fe6d1d (https:\/\/github.com\/moby\/moby\/pull\/45698) introduced a hard-coded list of snapshotters (`refCountedFileSystems`) that we know need reference-counting. With containerd, the list of available snapshotters is not known (additional snapshotters may be installed on the system), so maintaining a hard-coded list is problematic to maintain.\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/54fcd40aa4de94cd75aedc5f6ebf38c6d8f92082\/daemon\/snapshotter\/mount.go#L15-L16\r\n\r\nWe should consider to either;\r\n\r\n- unconditionally use reference-counting (as suggested in https:\/\/github.com\/moby\/moby\/pull\/45781#discussion_r1236612727)\r\n- get this information from the snapshotter (also related: https:\/\/github.com\/moby\/moby\/issues\/46757)\r\n- :point_up: this may depend on containerd 2.0 though, so perhaps we need to go for the alternative in the meantime\r\n","comments":[],"labels":["area\/storage","kind\/refactor","containerd-integration"]},{"title":"add method to graph driver.RefCounter to get current refcount","body":"- relates to https:\/\/github.com\/moby\/moby\/pull\/46770#discussion_r1383114243\r\n\r\n\r\nThe current interface only allows Incrementing and Decrementing, but does not allow getting the current reference-count. As a result, we need to do implement confusing code to get the current count;\r\n\r\n```go\r\n\/\/ Check if the refcount is non-zero.\r\nm.rc.Increment(target)\r\nif m.rc.Decrement(target) > 0 {\r\n    \/\/ it's in use\r\n}\r\n```\r\n\r\nWe should look at a cleaner way to get this information without having to increment\/decrement. Perhaps we don't need the actual count (only if in use?), but we can discuss that when working on this.\r\n\r\n\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/54fcd40aa4de94cd75aedc5f6ebf38c6d8f92082\/daemon\/graphdriver\/counter.go#L10-L37","comments":["We should also look at moving this package elsewhere, because it's now used by `snapshotters` as well, so perhaps we should move it outside of the graphdriver package to live on its own.\r\n"],"labels":["kind\/enhancement","kind\/refactor"]},{"title":"dockerd-rootless-setuptool.sh broken AD Integration due to the latest release","body":"### Description\n\ndockerd-rootless-setuptool.sh install giving issues with Escape Sequence character if username contains \\t \\s, which was working before.\r\n\r\nI found the following pull request which makes changes to the dockerd-rootless-setuptool.sh script: which broken AD integrations https:\/\/github.com\/moby\/moby\/pull\/46407\/files\n\n### Reproduce\n\ndockerd-rootless-setuptool.sh install  if username contains xyz\\t12345 or xyz\\s12345 \n\n### Expected behavior\n\nAD Integrations fails with above steps \n\n### docker version\n\n```bash\ndocker version\r\nClient: Docker Engine - Community\r\nVersion: 24.0.7\r\nAPI version: 1.43\r\nGo version: go1.20.10\r\nGit commit: afdd53b\r\nBuilt: Thu Oct 26 09:08:01 2023\r\nOS\/Arch: linux\/amd64\r\nContext: default\r\nCannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\n```\n\n\n### docker info\n\n```bash\ndocker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\nERROR: Cannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\r\nerrors pretty printing info\n```\n\n\n### Additional Info\n\ndocker version\r\nClient:\r\nVersion: 24.0.4\r\nAPI version: 1.43\r\nGo version: go1.20.5\r\nGit commit: 3713ee1\r\nBuilt: Fri Jul 7 14:49:50 2023\r\nOS\/Arch: linux\/amd64\r\nContext: default\r\n\r\nServer: Docker Engine - Community\r\nEngine:\r\nVersion: 24.0.4\r\nAPI version: 1.43 (minimum version 1.12)\r\nGo version: go1.20.5\r\nGit commit: 4ffc614\r\nBuilt: Fri Jul 7 14:51:12 2023\r\nOS\/Arch: linux\/amd64\r\nExperimental: false\r\ncontainerd:\r\nVersion: v1.7.1\r\nGitCommit: 1677a17964311325ed1c31e2c0a3589ce6d5c30d\r\nrunc:\r\nVersion: 1.1.7\r\nGitCommit: v1.1.7-0-g860f061\r\ndocker-init:\r\nVersion: 0.19.0\r\nGitCommit: de40ad0\r\nrootlesskit:\r\nVersion: 1.1.0\r\nApiVersion: 1.1.1\r\nNetworkDriver: slirp4netns\r\nPortDriver: builtin\r\nStateDir: \/tmp\/rootlesskit1786230875\r\nslirp4netns:\r\nVersion: 0.4.3\r\nGitCommit: 2244b9b6461afeccad1678fac3d6e478c28b4ad6\r\n\r\nTheis docker version is not working with AD Users contains Escape sequence characters. ","comments":["Thanks for reporting; I recall there was some discussion on the original ticket around this; https:\/\/github.com\/moby\/moby\/issues\/44396#issuecomment-1300993995\r\n\r\n\r\nAre you able to provide an example what `id -un` looks like on such a system? It looks like the script takes that as input, after which the code from the PR you linked attempts to escape special characters.","> Thanks for reporting; I recall there was some discussion on the original ticket around this; [#44396 (comment)](https:\/\/github.com\/moby\/moby\/issues\/44396#issuecomment-1300993995)\r\n> \r\n> Are you able to provide an example what `id -un` looks like on such a system? It looks like the script takes that as input, after which the code from the PR you linked attempts to escape special characters.\r\n\r\nHi @thaJeztah  Here is the output to the command `id -un`. Output:  `abg\\t000cs`\r\n\r\n id -un","Thanks! I'm curious now if that `\\t` also plays a role here (`\\t` somewhere being extrapolated to a `<tab>` character).\r\n\r\nI don't have access to a Windows machine myself, so mainly trying to collect information that may help with a fix; do you have more details on the error that happens because of this? (logs \/ output from the daemon?)\r\n\r\n\r\nalso \/cc @AkihiroSuda @ameyag ","Hi @thaJeztah ,\r\nYes the `\\t` seems to be getting extrapolated to the `<tab>` character in certainly one and possibly more places. \r\nWe see this extrapolation in the recommendation provided when we run the dockerd-rootless-setuptool.sh install command as shown below:\r\n\r\n```bash\r\nABG\\t000cs@10-43-36-56:~$ export  USER_DOMAIN=$(whoami | cut -d'\\' -f1)\r\nABG\\t000cs@10-43-36-56:~$ export  USER_ID=$(whoami | cut -d'\\' -f2)\r\nABG\\t000cs@10-43-36-56:~$ export USER_LINUX_ID=$(id -u $USER_ID@$USER_DOMAIN)\r\nABG\\t000cs@10-43-36-56:~$ export XDG_RUNTIME_DIR=\/run\/user\/$USER_LINUX_ID\r\nABG\\t000cs@10-43-36-56:~$ export DOCKER_HOST=unix:\/\/\/run\/user\/$USER_LINUX_ID\/docker.sock\r\nABG\\t000cs@10-43-36-56:~$\r\nABG\\t000cs@10-43-36-56:~$ dockerd-rootless-setuptool.sh install\r\n[ERROR] Missing system requirements. Run the following commands to\r\n[ERROR] install the requirements and run this tool again.\r\n########## BEGIN ##########\r\nsudo sh -eux <<EOF\r\n# Add subuid entry for ABG 000cs\r\necho \"ABG 000cs:100000:65536\" >> \/etc\/subuid\r\n# Add subgid entry for ABG 000cs\r\necho \"ABG 000cs:100000:65536\" >> \/etc\/subgid\r\nEOF\r\n########## END ##########\r\n```\r\n\r\nHere\u2019s the contents of the \/etc\/subuid and \/etc\/subgid files. \r\n\r\n```bash\r\nABG\\t000cs@10-43-36-56:~$ cat \/etc\/subuid\r\nABGt000cs:100000:65536\r\nABG\\t000cs:100000:65536\r\nABG\\t000cs@10-43-36-56:~$ cat \/etc\/subgid\r\nABGt000cs:100000:65536\r\nABG\\t000cs:100000:65536\r\n``` \r\n \r\nNote that the `ABGt000cs:100000:65536` entries in the subuid and subgid files were needed in the rootless configuration of docker engine 24.0.4 which works for us. However we still do not understand why this is needed. `ABG\\t000cs:100000:65536` alone in those files should be sufficient.\r\n \r\nUnfortunately these subuid and subgid entries no longer work in the rootless configuration of the latest docker engine 24.0.7.\r\n\r\nalso \/cc @AkihiroSuda @ameyag"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","area\/rootless","version\/24.0"]},{"title":"Docker Daemon consuming extremely high memory on the host. 800+GB","body":"### Description\r\n\r\nWe have a kubernetes cluster running docker as container runtime and cri-dockerd to let kubernetes controle docker via CRI.\r\nPart of this cluster we have two machines,\r\n\r\nMachine 1 - 128CPUs and 1TB RAM\r\nMachine 2 - 128CPUs and 128GB RAM\r\n\r\nKubernetes - v1.26.1\r\ncri-dockerd -  0.3.0\r\nKernel Version: 5.19.0-38-generic\r\nOperating System: Ubuntu 22.04.1 LTS\r\n\r\n\r\nLately we are seeing scenarios, where when pods are running (20-30+) on the above hosts, after sometime the docker daemon starts consuming memory, untill OOM killed. On below attached picture, it took close 800GB memory on number of occasions.\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/35141195\/cd69452d-2d07-49a7-a621-0b63e83f03e0)\r\n\r\n\r\n\r\n### Reproduce\r\n\r\nOnly gets reproduced with specific set of pods being run\r\n\r\n### Expected behavior\r\n\r\ndockerd shouldn't consume all the memory available on the host\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           20.10.23\r\n API version:       1.41\r\n Go version:        go1.18.10\r\n Git commit:        7155243\r\n Built:             Thu Jan 19 17:45:08 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.18.10\r\n  Git commit:       6051f14\r\n  Built:            Thu Jan 19 17:42:57 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.15\r\n  GitCommit:        5b842e528e99d4d4c1686467debf2bd4b88ecd86\r\n nvidia:\r\n  Version:          1.1.4\r\n  GitCommit:        v1.1.4-0-g5fd4c4d\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.10.0-docker)\r\n  compose: Docker Compose (Docker Inc., v2.15.1)\r\n  scan: Docker Scan (Docker Inc., v0.23.0)\r\n\r\nServer:\r\n Containers: 125\r\n  Running: 37\r\n  Paused: 0\r\n  Stopped: 88\r\n Images: 744\r\n Server Version: 20.10.23\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc\r\n Default Runtime: nvidia\r\n Init Binary: docker-init\r\n containerd version: 5b842e528e99d4d4c1686467debf2bd4b88ecd86\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.19.0-38-generic\r\n Operating System: Ubuntu 22.04.1 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 128\r\n Total Memory: 1008GiB\r\n Name: hostname\r\n ID: some-id\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 245\r\n  Goroutines: 195\r\n  System Time: 2023-11-21T11:31:18.880900685Z\r\n  EventsListeners: 0\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Thanks for reporting. We'll probably need more information to triage this issue.\r\n\r\n> Lately we are seeing scenarios, where when pods are running (20-30+) on the above hosts, after sometime the docker daemon starts consuming memory, untill OOM killed.\r\n\r\nDo you have more information what led to this issue?\r\n\r\n- Was this an existing installation, and was this setup working before?\r\n- If it was an existing installation, did anything change that could lead to this issue \/ coincide with this issue starting to appear? Considering;\r\n   - Updates to your distro\r\n   - Kernel version updates\r\n   - Updates of docker or containerd\r\n   - Updates in kubernetes, or other tooling that may interact with the docker engine (monitoring, log-aggregating)\r\n   - Changes in workloads deployed\r\n- Do daemon- or system-logs show anything that could be of interest?\r\n\r\nNote that docker no longer maintains version 20.10 (we only maintain the current release, which is 24.0.x at time of writing), but commercially supported versions of 20.10 are available through Mirantis (as their Mirantis Container Runtime). If you have a test-environment in which you can reproduce the issue, it may be worth trying with a current release.\r\n\r\nI also notice that you're running an outdated versions of `containerd` and `runc` (1.6.15\/1.1.4; current patch releases are 1.6.25 and 1.1.10). I would recommend updating to current patch releases of those (also if you're unable to update to a more recent version of the engine itself)."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/20.10"]},{"title":"Port mapping sometimes incorrect after starting additional container","body":"### Description\n\nSometimes it happens that host port mapping goes into wrong container.\r\n\r\nWe have tests which are using docker compose to create containers and then they start them sometimes one by one and sometimes recreate them. We have in the compose files a few services that all have set ephemeral port mapping:\r\n```\r\n    ports:\r\n      - \"443\"\r\n```\r\nSometimes it happens that when we run tests on standard Ubuntu GitHub runner that http calls from a script that runs on host using port that he got from inspecting docker doesn't go into correct cotaniner. It looks like a newly started container broke mapping of cotnainer that was already running.\r\n\r\nWe have not experienced the issue on neither Windows GitHub runner nor on dev machine with Windows Desktop. We just tried replacing port mapping for \"443\" to \"127.0.0.1::443\" and so far we have not experienced the issue.\n\n### Reproduce\n\nI don't have clear simple reproduction, yet, but expected steps (what our tests do):\r\n\r\n1. Have a few services in docker compose with ephemeral ports\r\n2. `docker compose up --no-start --detach --force-recreate`\r\n3. `docker compose start service1`\r\n4. `docker compose ps`\r\n5. try to communicate with service1 on the mapped port - no problem here\r\n6. (maybe needed for reproduction: `docker compose up --no-start --detach --force-recreate service2 )\r\n7. `docker compose start service2`\r\n8. `docker compose ps`\r\n9. try to communicate with service1 on the mapped port for service1 - this sometimes fails and is dirrected into service2 instead of service 1. We can see that service2 is also accessible using the mapped port for service2 - so both host ports navigate there.\n\n### Expected behavior\n\nhost port mapping should not break during starting or recreating of a different container\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 24\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-1015-azure\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 6.76GiB\r\n Name: fv-az559-896\r\n ID: 5ee0a90d-9ba7-4120-a52c-7d8898402fd7\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: githubactions\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nDocker Compose version v2.23.0\r\n\r\nGitHub Runner Image\r\n  Image: ubuntu-22.04\r\n  Version: 20231030.2.0\r\n  Included Software: https:\/\/github.com\/actions\/runner-images\/blob\/ubuntu22\/20231030.2\/images\/linux\/Ubuntu2204-Readme.md\r\n  Image Release: https:\/\/github.com\/actions\/runner-images\/releases\/tag\/ubuntu22%2F20231030.2","comments":[],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/portmapping","version\/24.0"]},{"title":"feature: clear container logs from the api","body":"closes #44445\r\n\r\n**- What I did**\r\nAdd functionality to clear container logs with the API.\r\n\r\n**- How I did it**\r\nI added a route \/containers\/{id}\/logs\/clear route that gets the container.LogPath and opens the file and call Truncate(0) on it to make it empty.\r\nI also updated the go client and added test for it.\r\n\r\n**- How to verify it**\r\n- `docker run --name hello hello-world` run a hello-world container\r\n- `docker logs hello` (there is logs)\r\n- `curl -X 'POST' 0.0.0.0:2375\/containers\/hello\/logs\/clear` Call the new route and clear the logs\r\n- `docker logs hello` (no more logs)\r\n\r\n**- Notes**\r\nThis also works for container that are running.\r\nI plan to work on the docker CLI command in docker\/cli as well to add the command. something like `docker logs --clear hello` maybe.\r\n\r\n**- Description for the changelog**\r\nAdded functionality to clear the logs of a container with the API without needing to kill and remove the container.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\nsadly none on my pc right now. next time, I will have one.\r\n","comments":["Thanks for contributing. At a quick glance, I expect implementing this will require more changes, taking into account;\r\n\r\n- log rotation (logging for a container can have log-rotation enabled)\r\n- logging-drivers; current code appears to be for the JSON-file logging driver only; other logging-drivers will use their own storage\r\n- logging-drivers without read support; for those, \"dual logging\" is enabled, which means that the `local` logging driver is used as a ring-buffer for reading logs\r\n\r\n\r\n\r\n\/cc @cpuguy83 @corhere @neersighted ","roger, I will investigate more on it and see what I can do on the things you mentioned."],"labels":["area\/logging","status\/1-design-review"]},{"title":"feature request: would it be possible to bundle the respective Swagger files with the engine releases?","body":"### Description\n\nUse-case:\r\n\r\nI'm looking at a library that generates the function calls\/mappings from the swagger file at run time. (As opposed to using fixed versions that need to be manually updated.)\r\nObviously in gapped\/restricted environments, doing calls to the outside world could prove problematic\/challenging.\r\n\r\nBundling it in with the engine\/daemon and making it accessible via the Engine API seems like a great way to remove that external dependency and always have an up-to-date source for that specific deployment\/engine.\r\n\r\nApologies if this has already been implemented anywhere. I searched through the sources but did not find anything that hinted at this.","comments":["> Bundling it in with the engine\/daemon\r\n\r\nI know the `docker-ce` packages already contain docs, and some of the `contrib\/` files, so perhaps adding the swagger as part of the deb and rpm packages may be an option (not sure what the best location would be for it, but I guess we could look at some of that).\r\n\r\n> and making it accessible via the Engine API\r\n\r\nI'm not sure about this one; at a glance, this sounds like a chicken-and-egg situation; how could a tool generate an API client from the API itself? Or does Swagger \/ OpenAPI have a formal \"discovery\" definition for this? Do you have more information about that if there is?\r\n","Sorry, started a comment and that got lost to time.\r\n\r\nA cursory search only hints at using the an openapi json\/yml file.\r\n> It is RECOMMENDED that the root OpenAPI document be named: openapi.json or openapi.yaml.\r\n\r\nAnd the specification does provide mechanisms to describe file downloads.\r\n\r\nSo I think it could be feasible to provide a `\/openapi.yml` file that gets described in that file.\r\n\r\nBut I will research this and follow up."],"labels":["area\/api","status\/0-triage","kind\/feature","status\/more-info-needed","area\/packaging"]},{"title":"Docker swarm with ipv6 bridge breaks host ipv6 networking","body":"### Description\n\nWhen creating a docker swarm with an existing ipv6 ready `docker_gwbridge`, the host network's ipv6 fails.\r\n\r\nIPv6 does still work for the containers, but not via the gateway network, and not for the host.\n\n### Reproduce\n\n1. Clean Ubuntu or Debian machines\r\n2. Install Docker on all\r\n3. Enable IPv6 via [steps listed here](https:\/\/gist.github.com\/troykelly\/749b48712d0d777030e7dd5d64916744)\r\n4. Confirm IPv6 still works (ie curl -vvv https:\/\/ipv6.google.com)\r\n5. Create the swarm `docker swarm init --advertise-addr 10.0.0.1 --listen-addr 10.0.0.1`\r\n6. Watch IPv6 fail (ie curl -vvv https:\/\/ipv6.google.com)\n\n### Expected behavior\n\nIPv6 shouldn't be broken for the host\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:08:02 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:08:02 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: zxc3x54vaf43m9pzx6uj1snsn\r\n  Is Manager: true\r\n  ClusterID: h02lppa8vcgglxvdj94mq6xae\r\n  Managers: 1\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 103.135.98.16\r\n  Manager Addresses:\r\n   103.135.98.16:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-13-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 19.5GiB\r\n Name: controller-ghi.public-servers.sy3.aperim.net\r\n ID: f102c4d6-0878-4d78-aeb0-d7ea59d67362\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nI've tested this repeatedly with both Debian and Ubuntu, it's very easy to reproduce.\r\nThis method of enabling IPv6 was working with previous docker versions, I'm not sure however where\/when it broke - but it's pretty recent.","comments":["Without the IPv6 `docker_gwbridge` creating the swarm does not impact IPv6 on the host.","Any idea when Docker will support IPv6?"],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/swarm","area\/networking\/ipv6"]},{"title":"Docker Service Fails to Start in Swarm Mode After a Kernel Update","body":"### Description\n\nI upgraded the kernel of my ubuntu server 22.04 from 5.15.0-83-generic to 5.19.0-43-generic and got an error similar to issue [#29180](https:\/\/github.com\/moby\/moby\/issues\/29180).\r\n\r\nAs described in issue [#29180](https:\/\/github.com\/moby\/moby\/issues\/29180), I removed the node that was a manager from swarm, which worked fine, and deleted the \/var\/lib\/docker\/swarm folder on the corrupted node. After that, I was able to start the docker service and rejoin the node to the swarm.\r\n\r\nI am not really sure if the problem is related to the kernel update or the fact that I updated docker from 24.0.6 to 24.0.7, or something else.\n\n### Reproduce\n\n1. apt install linux-image-5.19.0-43-generic\r\n2. \/sbin\/reboot\r\n3. systemctl start docker.service\n\n### Expected behavior\n\nThe service should start, but there is always an error\r\n\r\n```\r\n# systemctl start docker.service\r\nJob for docker.service failed because the control process exited with error code.\r\nSee \"systemctl status docker.service\" and \"journalctl -xeu docker.service\" for details.\r\n```\r\n```\r\n# journalctl -xeu docker.service\r\nThe job identifier is 4066.\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: time=\"2023-11-17T07:03:40.468017035Z\" level=panic msg=\"tocommit(48669) is out of range [lastIndex(48650)]. Was the raft log corrupted, truncated, or lost?\" module=raft node.id=xsx92qih1r8hj53rms48763gb\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: panic: (*logrus.Entry) 0xc000792700\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: goroutine 300 [running]:\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: github.com\/sirupsen\/logrus.(*Entry).log(0xc0009e0af0, 0x0, {0xc000cc97a0, 0x63})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:260 +0x4d6\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: github.com\/sirupsen\/logrus.(*Entry).Log(0xc0009e0af0, 0x0, {0xc001276d10?, 0x2?, 0x2?})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:304 +0x4f\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: github.com\/sirupsen\/logrus.(*Entry).Logf(0xc0009e0af0, 0x0, {0x55bf0219b7f8?, 0x55bf004f0d45?}, {0xc004f13c40?, 0x55bf02644820?, 0x59f111f13956c200?})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:349 +0x85\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: github.com\/sirupsen\/logrus.(*Entry).Panicf(0xbe0a?, {0x55bf0219b7f8?, 0x550c7dc3243185be?}, {0xc004f13c40?, 0xc19bf4749bdc06a7?, 0x80deb1fe72be5d74?})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:387 +0x34\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: go.etcd.io\/etcd\/raft\/v3.(*raftLog).commitTo(0xc0009e0b60, 0xbe1d)\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/log.go:237 +0x103\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: go.etcd.io\/etcd\/raft\/v3.(*raft).handleHeartbeat(_, {0x8, 0x7035419add9d798e, 0x5e0e1edd6f878b0e, 0xc, 0x0, 0x0, {0x0, 0x0, 0x0}, ...})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/raft.go:1508 +0x45\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: go.etcd.io\/etcd\/raft\/v3.stepFollower(_, {0x8, 0x7035419add9d798e, 0x5e0e1edd6f878b0e, 0xc, 0x0, 0x0, {0x0, 0x0, 0x0}, ...})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/raft.go:1434 +0x3d8\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: go.etcd.io\/etcd\/raft\/v3.(*raft).Step(_, {0x8, 0x7035419add9d798e, 0x5e0e1edd6f878b0e, 0xc, 0x0, 0x0, {0x0, 0x0, 0x0}, ...})\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/raft.go:975 +0x1335\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: go.etcd.io\/etcd\/raft\/v3.(*node).run(0xc000a3b6e0)\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/node.go:356 +0x925\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]: created by go.etcd.io\/etcd\/raft\/v3.RestartNode\r\nNov 17 07:03:40 swarm-manager02 dockerd[344369]:         \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/node.go:244 +0x24a\r\nNov 17 07:03:40 swarm-manager02 systemd[1]: docker.service: Main process exited, code=exited, status=2\/INVALIDARGUMENT\r\n\u2591\u2591 Subject: Unit process exited\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 An ExecStart= process belonging to unit docker.service has exited.\r\n\u2591\u2591\r\n\u2591\u2591 The process' exit code is 'exited' and its exit status is 2.\r\nNov 17 07:03:40 swarm-manager02 systemd[1]: docker.service: Failed with result 'exit-code'.\r\n\u2591\u2591 Subject: Unit failed\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 The unit docker.service has entered the 'failed' state with result 'exit-code'.\r\nNov 17 07:03:40 swarm-manager02 systemd[1]: docker.service: Unit process 344547 (iptables) remains running after unit stopped.\r\nNov 17 07:03:40 swarm-manager02 systemd[1]: docker.service: Consumed 1.595s CPU time.\r\n\u2591\u2591 Subject: Resources consumed by unit runtime\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 The unit docker.service completed and consumed the indicated resources.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: docker.service: Scheduled restart job, restart counter is at 3.\r\n\u2591\u2591 Subject: Automatic restarting of a unit has been scheduled\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 Automatic restarting of the unit docker.service has been scheduled, as the result for\r\n\u2591\u2591 the configured Restart= setting for the unit.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: Stopped Docker Application Container Engine.\r\n\u2591\u2591 Subject: A stop job for unit docker.service has finished\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 A stop job for unit docker.service has finished.\r\n\u2591\u2591\r\n\u2591\u2591 The job identifier is 4149 and the job result is done.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: docker.service: Consumed 1.595s CPU time.\r\n\u2591\u2591 Subject: Resources consumed by unit runtime\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 The unit docker.service completed and consumed the indicated resources.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: docker.service: Start request repeated too quickly.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: docker.service: Failed with result 'exit-code'.\r\n\u2591\u2591 Subject: Unit failed\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 The unit docker.service has entered the 'failed' state with result 'exit-code'.\r\nNov 17 07:03:42 swarm-manager02 systemd[1]: Failed to start Docker Application Container Engine.\r\n\u2591\u2591 Subject: A start job for unit docker.service has failed\r\n\u2591\u2591 Defined-By: systemd\r\n\u2591\u2591 Support: http:\/\/www.ubuntu.com\/support\r\n\u2591\u2591\r\n\u2591\u2591 A start job for unit docker.service has finished with a failure.\r\n\u2591\u2591\r\n\u2591\u2591 The job identifier is 4149 and the job result is failed.\r\n```\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.7\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 1\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: oj8fjf7p5t6wuih969cnpsrmg\r\n  Is Manager: true\r\n  ClusterID: xlr3necmcq6a1b7kwpe44q5i8\r\n  Managers: 3\r\n  Nodes: 7\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.15.29.22\r\n  Manager Addresses:\r\n   10.15.29.21:2377\r\n   10.15.29.22:2377\r\n   10.15.29.23:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.19.0-43-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.831GiB\r\n Name: swarm-manager02\r\n ID: 5b9037f5-a566-4e43-ba7f-3fc530a3432f\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nIf there is any additional information that is missing, please let me know and I will try to provide as much as I can.","comments":["\/asign this issue to me please and let's discuss over it\r\n","Sorry, I don't know how to assign this to you. Also, because we needed this server, I reinstalled it and don't have a chance to test anything.\r\n\r\n> \/asign this issue to me please and let's discuss over it\r\n\r\n","Same issue here, after kernel update. Seems to be only on the manager node in our cluster. We have 1 and recreation of the swarm is necessary after the error.\r\n\r\nGot the following error running dockerD to start the Manager Node:\r\n\r\n```\r\nPANI[2024-01-20T09:13:02.545069278Z] 37429e02367a901c state.commit 8418 is out of range [10000, 10000]  module=raft node.id=unstg0yeiahpxk2atth88kfcq\r\npanic: (*logrus.Entry) 0xc0005a07e0\r\n\r\ngoroutine 314 [running]:\r\ngithub.com\/sirupsen\/logrus.(*Entry).log(0xc0005a03f0, 0x0, {0xc0003d9860, 0x41})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:260 +0x491\r\ngithub.com\/sirupsen\/logrus.(*Entry).Log(0xc0005a03f0, 0x0, {0xc000c70f90?, 0x4?, 0x4?})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:304 +0x48\r\ngithub.com\/sirupsen\/logrus.(*Entry).Logf(0xc0005a03f0, 0x0, {0x55b359744918?, 0x55b357a8fabb?}, {0xc000d4b480?, 0x55b359c5bc40?, 0x0?})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:349 +0x7c\r\ngithub.com\/sirupsen\/logrus.(*Entry).Panicf(0x2710?, {0x55b359744918?, 0x0?}, {0xc000d4b480?, 0x0?, 0x0?})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/sirupsen\/logrus\/entry.go:387 +0x2e\r\ngo.etcd.io\/etcd\/raft\/v3.(*raft).loadState(0xc000b70420, {0x1?, 0x1?, 0x0?})\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/raft.go:1698 +0x1a3\r\ngo.etcd.io\/etcd\/raft\/v3.newRaft(0xc000bd2f80)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/raft.go:355 +0x67a\r\ngo.etcd.io\/etcd\/raft\/v3.NewRawNode(0x55b359c5b380?)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/rawnode.go:48 +0x17\r\ngo.etcd.io\/etcd\/raft\/v3.RestartNode(0xc000cc2300?)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/etcd\/raft\/v3\/node.go:239 +0x1f\r\ngithub.com\/moby\/swarmkit\/v2\/manager\/state\/raft.(*Node).JoinAndStart(0xc000cc2300, {0x55b35a22d898?, 0xc000bdcaf0?})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/moby\/swarmkit\/v2\/manager\/state\/raft\/raft.go:421 +0x7d6\r\ngithub.com\/moby\/swarmkit\/v2\/manager.(*Manager).Run(0xc0008c9a40, {0x55b35a22d860?, 0xc000de2ba0?})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/moby\/swarmkit\/v2\/manager\/manager.go:574 +0x249f\r\ngithub.com\/moby\/swarmkit\/v2\/node.(*Node).runManager.func1(0x0?)\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/moby\/swarmkit\/v2\/node\/node.go:1032 +0x65\r\ncreated by github.com\/moby\/swarmkit\/v2\/node.(*Node).runManager in goroutine 307\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/moby\/swarmkit\/v2\/node\/node.go:1031 +0x518\r\n```"],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"enhancement: surface DNS in-use for container, and consider \"reload\" DNS","body":"### Description\n\n- relates to https:\/\/github.com\/moby\/moby\/pull\/46803#issuecomment-1807973319\r\n- slightly related: https:\/\/github.com\/moby\/moby\/pull\/35506\r\n\r\nWhile looking working on https:\/\/github.com\/moby\/moby\/pull\/46803, I noticed that we're not very consistent with \"defaults\" configured on the daemon.\r\n\r\n- For log-driver (and options), we bake the defaults into the container when it's created\r\n- We're not doing that for DNS; default DNS is not baked in the container, and thus updated if the daemon is restarted with new DNS settings\r\n\r\nI _think_ this was by design, so that a container without an explicit DNS configured can be updated to follow the daemon's defaults (or the host's DNS servers, if no default is set on the daemon). However, it makes it hard to discover what DNS is used for a container.\r\n\r\n\r\nIf the daemon was started with `--dns 1.1.1.1`, then the container _does_ use `1.1.1.1` (as can be seen for a container on the default `bridge` network, and checking its `\/etc\/resolv.conf`);\r\n\r\n```bash\r\ndocker run --rm busybox cat \/etc\/resolv.conf\r\nnameserver 1.1.1.1\r\n```\r\n\r\nBut _inspecting_ a container won't show that in the config;\r\n\r\n```bash\r\ndocker inspect --format '{{.HostConfig.Dns}}' defaultdns\r\n[]\r\n```\r\n\r\nWhereas manually specifying the `--dns` does get baked into the container;\r\n\r\n```bash\r\ndocker create --dns 2.2.2.2 --name customdns busybox\r\ndocker inspect --format '{{.HostConfig.Dns}}' customdns\r\n[2.2.2.2]\r\n```\r\n\r\nTo verify that the DNS is _not_ \"baked\" into containers without an explicit `--dns` set;\r\n\r\nOn a docker daemon started with `--dns 1.1.1.1`;\r\n\r\n```bash\r\ndocker run -d --restart=always --name mycontainer nginx:alpine\r\ndocker exec mycontainer cat \/etc\/resolv.conf\r\nnameserver 1.1.1.1\r\n```\r\n\r\nAfter stopping the container, and restarting with a different DNS (`--dns 8.8.8.8`), the existing container is updated with the new DNS;\r\n\r\n```bash\r\ndocker exec mycontainer cat \/etc\/resolv.conf\r\nnameserver 8.8.8.8\r\n```\r\n\r\nFor the default `bridge` network, it's possible for the user to verify what DNS is used, either by checking its `\/etc\/resolv.conf`, or from `nslookup` output;\r\n\r\n```bash\r\ndocker exec mycontainer nslookup google.com\r\nServer:\t\t8.8.8.8\r\nAddress:\t8.8.8.8:53\r\n\r\nNon-authoritative answer:\r\nName:\tgoogle.com\r\nAddress: 142.251.36.14\r\n\r\nNon-authoritative answer:\r\nName:\tgoogle.com\r\nAddress: 2a00:1450:400e:810::200e\r\n```\r\n\r\nHowever, when using a custom network, the resolver inside the container will be the embedded DNS, which makes it not possible to discover what \"upstream\" DNS servers are used;\r\n\r\n```bash\r\ndocker network create mynetwork\r\na527a8903e46fdbcf16e5aa4c40a2c38e823bfcaa1e47449de09e7ad737832e4\r\n\r\ndocker run --rm --network=mynetwork alpine cat \/etc\/resolv.conf\r\nnameserver 127.0.0.11\r\noptions ndots:0\r\n\r\ndocker run --rm --network=mynetwork alpine nslookup google.com\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11:53\r\n\r\nNon-authoritative answer:\r\nName:\tgoogle.com\r\nAddress: 142.251.36.14\r\n\r\nNon-authoritative answer:\r\nName:\tgoogle.com\r\nAddress: 2a00:1450:400e:801::200e\r\n```\r\n\r\n## Possible improvements\r\n\r\n### improve generated `\/etc\/resolv.conf`\r\n\r\nAs we are generating the container's `\/etc\/resolv.conf`, a \"quick win\" would be include some comments in the generated file;\r\n\r\n- include a comment that indicates the file was generated by docker; this can apply to _both_ the default `bridge` network _and_ for custom networks\r\n- include the list of \"upstream\" DNS servers that the daemon has configured\r\n- a comment describing the behavior if the file is updated inside the container (we allow the file to be edited, but _if edited_, we will no longer update the file (we check if the checksum changed))\r\n\r\nFor \"inspiration\", here's an example of `\/etc\/resolv.conf` on a Linux machine with `systemd-resolved`;\r\n\r\n```bash\r\ncat \/etc\/resolv.conf\r\n# This file is managed by man:systemd-resolved(8). Do not edit.\r\n#\r\n# This is a dynamic resolv.conf file for connecting local clients to the\r\n# internal DNS stub resolver of systemd-resolved. This file lists all\r\n# configured search domains.\r\n#\r\n# Run \"systemd-resolve --status\" to see details about the uplink DNS servers\r\n# currently in use.\r\n#\r\n# Third party programs must not access this file directly, but only through the\r\n# symlink at \/etc\/resolv.conf. To manage man:resolv.conf(5) in a different way,\r\n# replace this symlink by a static file or a different symlink.\r\n#\r\n# See man:systemd-resolved.service(8) for details about the supported modes of\r\n# operation for \/etc\/resolv.conf.\r\n```\r\n\r\n### include DNS information in \"inspect\" output\r\n\r\nHaving this information surface as part of `inspect` becomes more important if we decide to also use the embedded DNS for the default `bridge` network, as in that situation (as described above), it's more complicated for the user to obtain this information.\r\n\r\n:warning: This may be more involved, as it touches on issues with the current design, where there's poor separation between \"config\" and \"state\". To prevent ambiguity, we should introduce _new_ fields for this (\"actual state\") that is separate from the _configured_ (\"requested\") state\r\n\r\nWe should look if we can surface the DNS server(s) that are currently used for a container;\r\n\r\n- if a custom DNS was configured for the container (`docker run --dns ..`), these DNS servers would be the same as those in the config (\"desired state\") in `.HostConfig.Dns`\r\n- if _no_ custom DNS was configured, this would show the list of DNS servers that the daemon uses as default. In case the embedded DNS is used, those would be the embedded DNS's \"upstream\" resolvers\r\n- :question: do we need an indication if `\/etc\/resolv.conf` inside the container was manually modified? In that case, we stop managing the container's DNS config, so we may not be able to provide the \"correct\" information.\r\n\r\n### consider making DNS \"updatable\" through `docker container update`\r\n\r\nThe `docker container update` (`docker update` shorthand) command currently does not allow for the DNS settings of a container to be updated.\r\n\r\n- We can consider adding this to the list of options that can be updated for existing containers.\r\n- If we do, we must also provide a way to _unset_ the config (i.e., being able for a container to use the daemon's default)\r\n- For UX, we should look at `docker service update`, which has a better approach for mutating properties (through `--<some-option>-add` \/ `--<some-option>-rm`)\r\n\r\n### consider making DNS \"reloadable\"\r\n\r\nWe currently configure the list of DNS servers to use as default during daemon startup. This list can either originate from the host's resolvers (we parse `\/etc\/resolv.conf`), or from user-config (`daemon.json` or the `--dns` flags).\r\n\r\n- As we already have code in place to update container's DNS settings (which we do during restart), we should look if it's possible to update these settings _at runtime_ without requiring a full restart of the daemon.\r\n- This feature could potentially be used \"dynamically\"; on systems with \"dynamic\" networks (laptop switching WiFI networks) we could use this to automatically update container configs when detecting network changes. Perhaps that part may be best left to external systems to \"trigger\", but perhaps the daemon itself could also watch for changes\r\n- For manual changes to the config (updating `daemon.json`), this can be used by the user through `systemctl reload docker.service` or `SIGHUP dockerd`; https:\/\/github.com\/moby\/moby\/blob\/93fffa299c2cb23055bb54c380a02d53c9a7d525\/contrib\/init\/systemd\/docker.service#L14\r\n","comments":["Would reloading DNS be something that could also help fix issues on swarm where Service DNS entries are \"lost\" and we have to update the involved Services?\r\n\r\nI would be very happy about that as that could give us a way to manually resolve DNS issues in swarms without the current harsh solution.\r\n\r\nWe have DNS resolution monitoring already in place so this would give me a way to automatically reconcile these DNS issues. ","I forgot I created this one; looks like the \"surface configured DNS part\" will be addressed through a PR that @robmry is working on (?);\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/47041\r\n\r\n> Would reloading DNS be something that could also help fix issues on swarm where Service DNS entries are \"lost\" and we have to update the involved Services?\r\n\r\nDon't know from the top of my head, but perhaps @robmry has ideas","My PR makes the 'resolv.conf' generation more predictable and, when the internal resolver is used, it adds comments to that file listing the upstream nameservers. But, it doesn't make the DNS updatable\/reloadable, or add anything to the 'inspect' output ... those could be follow-up changes.\r\n\r\nSo, I don't think it'll have any effect on lost service entries in swarm. Is there an issue\/ticket describing that problem?","The issue I am facing might be part of other DNS errors that are discussed on moby\/moby, but for me it materializes mostly around us upgrading Docker Swarm clusters and cases where the network is a bit flaky between the nodes.\r\n\r\nThen, some services are not reachable from some nodes and dns lookups fail. The only thing that helps is to kill the task with a service redeploy via `docker service update`. If you want, I can try to formulate a new issue for this, but I am almost sure that this was already raised somewhere on the tracker, I just can't find it atm.","Thanks @s4ke - I can't find an open issue for DNS failures related to Swarm upgrades either, so it would probably be best to raise a new one to make sure it's tracked. (Even if it does turn out to be a duplicate, additional info about how to repro and the environment where you see the problem is sure to help.)","Could be that those are related to the original design decision to not register services in DNS until they're up (and healthy), so if a service is not healthy, the DNS entry is not present. (I _think_ for that one at least there may be a ticket ... somewhere \ud83d\ude48)","In my case this even happens if the Service is healthy. The reproducer will be a bit hard but I will try my best. I will create a Ticket. Thx. "],"labels":["kind\/enhancement","area\/networking","area\/ux","area\/networking\/dns"]},{"title":"FromEnv should work with an SSH transport set via DOCKER_HOST","body":"### Description\n\nWhen connecting using `DOCKER_HOST=ssh:\/\/...` any operations it performs gets \r\n```\r\ninvalid character 'F' looking for beginning of value\r\n```\n\n### Reproduce\n\nWhen connecting using `DOCKER_HOST=ssh:\/\/...`\r\n```go\r\ncli, err := client.NewClientWithOpts(client.FromEnv)\r\nif err != nil {\r\n  return nil, err\r\n}\r\nservices, err := cli.ServiceList(context.Background(), types.ServiceListOptions{Status: true})\r\nif err != nil {\r\n    fmt.Println(\"service list error\")\r\n    fmt.Println(err)\r\n    return nil, err\r\n}\r\n```\r\nThe it keeps on getting \r\n```\r\ninvalid character 'F' looking for beginning of value\r\n```\n\n### Expected behavior\n\nShould just work\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:32:48 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Desktop 4.25.0 (126437)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:16 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.5\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.0-desktop.1\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.9\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scan.exe\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.0.9\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 352\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: ic7lgvymnyg2vrxangdtfdeff\r\n  Is Manager: true\r\n  ClusterID: p2t7bh5gvs5xwmdw5wfhnmmnm\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.65.3\r\n  Manager Addresses:\r\n   192.168.65.3:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n Kernel Version: 5.15.90.1-microsoft-standard-WSL2\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 7.76GiB\r\n Name: noriko\r\n ID: 288e30d3-d348-459a-a693-34220e4ce4b9\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\nWARNING: daemon is not using the default seccomp profile\n```\n\n\n### Additional Info\n\n_No response_","comments":["Yes, this is something we should look at if we can make this easier to consume, or consider improving examples \/ docs around this.\r\n\r\nFor SSH connections, the `docker` CLI uses a connection-helper; which is set up here;\r\nhttps:\/\/github.com\/docker\/cli\/blob\/a6114fc42416d354745abc82aeba1a7f4da0cf0a\/cli\/command\/cli.go#L262-L272\r\n\r\n```go\r\nfunc newAPIClientFromEndpoint(ep docker.Endpoint, configFile *configfile.ConfigFile) (client.APIClient, error) {\r\n\topts, err := ep.ClientOpts()\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tif len(configFile.HTTPHeaders) > 0 {\r\n\t\topts = append(opts, client.WithHTTPHeaders(configFile.HTTPHeaders))\r\n\t}\r\n\topts = append(opts, client.WithUserAgent(UserAgent()))\r\n\treturn client.NewClientWithOpts(opts...)\r\n}\r\n```\r\n\r\nhttps:\/\/github.com\/docker\/cli\/blob\/a6114fc42416d354745abc82aeba1a7f4da0cf0a\/cli\/context\/docker\/load.go#L85-L93\r\n\r\n```go\r\n\/\/ ClientOpts returns a slice of Client options to configure an API client with this endpoint\r\nfunc (ep *Endpoint) ClientOpts() ([]client.Opt, error) {\r\n\tvar result []client.Opt\r\n\tif ep.Host != \"\" {\r\n\t\thelper, err := connhelper.GetConnectionHelper(ep.Host)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\t\tif helper == nil {\r\n\t\t\ttlsConfig, err := ep.tlsConfig()\r\n```\r\n\r\nThat `connhelper` package is currently part of the `docker\/cli` repository, but it's also used by other projects (such as `buildx`). Having it live in the CLI code makes it more complicated to use, and also means that we currently can't add this functionality to the API-client itself. https:\/\/github.com\/docker\/cli\/blob\/a6114fc42416d354745abc82aeba1a7f4da0cf0a\/cli\/connhelper\/connhelper.go#L21-L36\r\n\r\nWe have an internal ticket (but I should probably move that ticket to a public issue tracker) to move the `connhelper` package to a separate module so that it can more easily be re-used. Once that's in place, we could consider moving more of that logic into the client. We'd have to look if we want that to be _default_ (as part of `FromEnv`) or still to be opt-in though (passing additional options).\r\n\r\nIn the meantime, I'd recommend looking at the code in the CLI repository to see if you can implement a similar approach in your project.\r\n","\/cc @laurazard ","I think you guys had started it but abandoned in the end here https:\/\/github.com\/docker\/go-connections\/pull\/39","Somewhat; that was for the initial work; at the time it was still being decided where it would be implemented (daemon cli, docker cli)\r\n\r\nIf we move the code, it would definitely not be in that repository though; the go-connections repository has grown to be a bit of a kitchen-sink, and we're looking at potentially dismantling it.","Thanks @thaJeztah , still learning Go on the side to redo my [old docker shell function](https:\/\/gist.github.com\/trajano\/315fecc2986b5ac5f642d207d79303f8#file-docker-sh) to use Go rather than bash so I can use it in zsh\r\n\r\nJust did a hack for now where if I am using `DOCKER_HOST` since that's the way I switch contexts rather than `docker context` for my day to day\r\n\r\nhttps:\/\/github.com\/trajano\/docker-cli\/blob\/94dd7badb501a261dd0179f23c67052ebf9ad0ad\/cmd\/run_docker_command.go#L113-L188","Anyway I tried to see if I can pull from the CLI project but I couldn't make heads or tails for it.  But the primary purpose of my tool was just to give the Docker CLI some defaults that I generally use.","IIRC, you can use `docker system dial-stdio` with DOCKER_HOST set and use that as an API proxy if you want to get something up and running.\r\n\r\nExample:\r\n```console\r\n$ DOCKER_HOST=ssh:\/\/dev docker system dial-stdio <<EOF\r\nheredoc> GET \/_ping HTTP\/1.1\r\nheredoc> Host: example.com\r\nheredoc>\r\nheredoc> EOF\r\nHTTP\/1.1 200 OK\r\nApi-Version: 1.44\r\nBuilder-Version: 2\r\nCache-Control: no-cache, no-store, must-revalidate\r\nDocker-Experimental: false\r\nOstype: linux\r\nPragma: no-cache\r\nServer: Docker\/dev (linux)\r\nSwarm: inactive\r\nDate: Thu, 16 Nov 2023 17:43:43 GMT\r\nContent-Length: 2\r\nContent-Type: text\/plain; charset=utf-8\r\n\r\nOK%            \r\n```"],"labels":["area\/api","area\/cli","status\/0-triage","kind\/enhancement"]},{"title":"docker pull image failed with \"failed to register layer\"","body":"### Description\r\n\r\nRunning command `docker pull xxx` to pull an image.\r\nAnd the error output for it\r\n`failed to register layer: Error processing tar file(exit status 1): open \/bin\/[: permission denied`\r\n\r\n### Reproduce\r\n\r\n`reboot`\r\n`docker pull xxxx`\r\n\r\n### Expected behavior\r\n\r\n\"`docker pulll xxx` without error\"\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           18.09.8\r\n API version:       1.39\r\n Go version:        go1.10.8\r\n Git commit:        0dd43dd87f\r\n Built:             Wed Jul 17 17:38:58 2019\r\n OS\/Arch:           linux\/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.8\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.10.8\r\n  Git commit:       0dd43dd87f\r\n  Built:            Wed Jul 17 17:48:49 2019\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nContainers: 0\r\n Running: 0\r\n Paused: 0\r\n Stopped: 0\r\nImages: 1\r\nServer Version: 18.09.8\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: false\r\nLogging Driver: json-file\r\nCgroup Driver: systemd\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb\r\nrunc version: N\/A\r\ninit version: fec3683\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 3.10.0-514.el7.x86_64\r\nOperating System: CentOS Linux 7 (Core)\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.62GiB\r\nName: centos-1631607984\r\nID: B5A5:ZQNE:XUFN:XDS7:VECX:NUDW:6FKQ:2G2N:FTRO:MGCS:HYSJ:UN7X\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): true\r\n File Descriptors: 19\r\n Goroutines: 44\r\n System Time: 2023-11-14T11:44:26.880026904+08:00\r\n EventsListeners: 0\r\nRegistry: https:\/\/index.docker.io\/v1\/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0\/8\r\nLive Restore Enabled: false\r\nProduct License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n`ls -lh \/bin\/[`\r\n`-rwxr-xr-x. 1 root root 41K Nov  6  2016 \/bin\/[`\r\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"api: clean-up image-inspect output","body":"### Description\n\nrelates to:\r\n\r\n- https:\/\/github.com\/docker\/cli\/issues\/2868\r\n- https:\/\/github.com\/moby\/moby\/pull\/43461\r\n- https:\/\/github.com\/moby\/moby\/issues\/18880\r\n- https:\/\/github.com\/moby\/moby\/issues\/38762\r\n- https:\/\/github.com\/moby\/moby\/issues\/17780\r\n- (possibly others)\r\n\r\n\r\n\r\nWhile discussing changes \/ designs for the CLI to improve output for `docker image inspect` (and to look at designs for a potential `docker image info` command), I noticed that the image inspect output contains various fields that don't seem to be related to the image itself. Some of these are \"known\" as they were (are) used by the classic builder (such as `ContainerConfig`); however, it looks like the `Config` field has the same issue, and holds the _Container_ config.\r\n\r\n\r\n`Container` field; only used by classic builder, and holds the ID of the container that was used to produce the image https:\/\/github.com\/moby\/moby\/blob\/34e923e3e31be229c6ad72005416134fc2a1fcfe\/api\/types\/types.go#L77-L80\r\n\r\n\r\n`ContainerConfig` field; only used by classic builder, and holds the config of the container that was used to produce the image https:\/\/github.com\/moby\/moby\/blob\/34e923e3e31be229c6ad72005416134fc2a1fcfe\/api\/types\/types.go#L82-L87\r\n\r\n`Config` field; this one needs to be looked at https:\/\/github.com\/moby\/moby\/blob\/34e923e3e31be229c6ad72005416134fc2a1fcfe\/api\/types\/types.go#L97\r\n\r\nLooking at `Config`, we are using `container.Config`, which seems to be a _superset_ of what _should_ actually be part of the image.\r\n\r\n\r\nHere's examples of images built with BuildKit and the classic builder;\r\n\r\n```bash\r\ndocker buildx imagetools inspect --raw moby\/moby-bin@sha256:f8ae01a0e08159551e946cae5dad31675cfc47d29d3a851504ccf8adad8da3e4 | jq .config\r\n{\r\n  \"Env\": [\r\n    \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n  ],\r\n  \"WorkingDir\": \"\/\",\r\n  \"Labels\": {\r\n    \"org.opencontainers.image.created\": \"2023-11-13T11:22:54.120Z\",\r\n    \"org.opencontainers.image.description\": \"The Moby Project - a collaborative project for the container ecosystem to assemble container-based systems\",\r\n    \"org.opencontainers.image.licenses\": \"Apache-2.0\",\r\n    \"org.opencontainers.image.revision\": \"34e923e3e31be229c6ad72005416134fc2a1fcfe\",\r\n    \"org.opencontainers.image.source\": \"https:\/\/github.com\/moby\/moby\",\r\n    \"org.opencontainers.image.title\": \"moby\",\r\n    \"org.opencontainers.image.url\": \"https:\/\/github.com\/moby\/moby\",\r\n    \"org.opencontainers.image.version\": \"master\"\r\n  },\r\n  \"OnBuild\": null\r\n}\r\n```\r\n\r\n\r\n```bash\r\ndocker buildx imagetools inspect --raw docker.io\/library\/busybox:latest@sha256:a416a98b71e224a31ee99cff8e16063554498227d2b696152a9c3e0aa65e5824 | jq .config\r\n{\r\n  \"Hostname\": \"\",\r\n  \"Domainname\": \"\",\r\n  \"User\": \"\",\r\n  \"AttachStdin\": false,\r\n  \"AttachStdout\": false,\r\n  \"AttachStderr\": false,\r\n  \"Tty\": false,\r\n  \"OpenStdin\": false,\r\n  \"StdinOnce\": false,\r\n  \"Env\": [\r\n    \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n  ],\r\n  \"Cmd\": [\r\n    \"sh\"\r\n  ],\r\n  \"Image\": \"sha256:919de79e94ce464098cec8a4796f27aa2d0c93985e27f6a157c8bbb557568013\",\r\n  \"Volumes\": null,\r\n  \"WorkingDir\": \"\",\r\n  \"Entrypoint\": null,\r\n  \"OnBuild\": null,\r\n  \"Labels\": null\r\n}\r\n```\r\n\r\nNote that various fields are stored in the image, but are not part of an image's config, but only to be used at _runtime_ (set by a container), so are likely ignored when running the image, such as;\r\n\r\n- `AttachStdin`\r\n- `AttachStdout`\r\n- `AttachStderr`\r\n- `Tty`\r\n- `OpenStdin`\r\n- `StdinOnce`\r\n\r\nSome fields are also confusing, such as `Image` (see some tickets linked above), and `WorkingDir` looks to be inconsistent (BuildKit defaults to `\"\/\"`, whereas classic builder uses `\"\"` as default).\r\n\r\n\r\nWe should define a type separate from `container.Config` for this purpose, and remove fields that should not be persisted in the image. We may have to check what parts are used by the classic builder though.\r\n\r\n\r\n","comments":[],"labels":["area\/api","kind\/enhancement","area\/images","area\/ux"]},{"title":"Removing volumes","body":"### Description\n\nI'm not sure if this is a bug.\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/7c8d02d1bc8511fb05eb5074ec9b5bae54d2c5e2\/client\/volume_remove.go#L24\r\n\r\nShouldn't `volumeID` be escaped as it is used directly in URL?\r\nI mean something like this:\r\n```\r\nresp, err := cli.delete(ctx, \"\/volumes\/\"+url.QueryEscape(volumeID), query, nil)\r\n```\r\n\r\n\n\n### Reproduce\n\ndoes not apply\n\n### Expected behavior\n\ndoes not apply\n\n### docker version\n\n```bash\ndoes not apply\n```\n\n\n### docker info\n\n```bash\ndoes not apply\n```\n\n\n### Additional Info\n\n_No response_","comments":["Thanks for reporting. Yes, this is a known issue, and far from ideal; there's been some prior work on changing this, but it turned out to be move involved to prevent it being a backward-breaking change (as different combinations of client and daemon are possible, and changing behavior on either side can cause issues). At the time, there were also some issues with the gorilla\/mux library used, but I think those were fixed since. Some of the earlier issues and pull-requests; \r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/16577\r\n- https:\/\/github.com\/moby\/moby\/pull\/29131\r\n- https:\/\/github.com\/moby\/moby\/pull\/37857\r\n- https:\/\/github.com\/docker\/cli\/pull\/1331\r\n"],"labels":["area\/api","status\/0-triage","kind\/bug","area\/volumes"]},{"title":"chore: verify context cancellation for endpoints that mutate state","body":"### Description\n\nrelates to:\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46688\r\n- https:\/\/github.com\/moby\/moby\/pull\/46687\r\n- https:\/\/github.com\/moby\/moby\/pull\/45738\r\n- https:\/\/github.com\/moby\/moby\/pull\/46444\r\n\r\nNow that we're passing contexts in more places (some related to our work on OTEL), we must re-evaluate how we handle context cancellation; most notably for endpoints that mutate state (POST, PATCH, DELETE). These endpoints should in most cases be considered atomic operations; once a request is received, they should be executed until completion; _possibly_ with some safeguards (timeouts), but we must make sure that they are treated as a single unit of work (more so if the request involves multiple subsystems (e.g. removing an object and associated resources)).\r\n\r\nFor this;\r\n\r\n- We can use `compatcontext.WithoutCancel` where applicable (replacing `context.Background` in some cases)\r\n- As these contexts _should be_ bound to the request, we should move the `compatcontext.WithoutCancel()` to the calling site (which would be inside the API); there's some existing uses, which should be moved; see https:\/\/github.com\/moby\/moby\/pull\/46688#pullrequestreview-1693173937\r\n- _However_ some refactoring may be needed in some cases; _validation_ of requests _should_ (likely) remain \"cancellable\"; a _SUCCESSFUL_ request should proceed, whereas an incomplete (invalid \/ unsuccesful) request should still be cancelled, and errors during validation should not result in a request \"hanging\" forever.\r\n","comments":[],"labels":["area\/api","roadmap","kind\/enhancement","area\/metrics","area\/metrics\/otel"]},{"title":"External DNS forwarding is broken on Windows container NAT networks","body":"### Description\r\n\r\nA 'feature' was implemented back in 2016 that sets the default gateway as the primary DNS server for Windows containers. This is currently a bool 'DisableGatewayDNS' and the issue is that no DNS servers are at the address thus resulting in the primary failing and then making use of the secondary DNS server. This being the default behavior causes issues if the user does not have a DNS server listening on the node itself. \r\n\r\nLink to DisableGatewayDNS property: https:\/\/github.com\/moby\/moby\/blob\/7e66d9900c81e872e5ecc89aa0bb03e52901447f\/libnetwork\/drivers\/windows\/windows.go#L49C2-L49C19\r\n\r\nOriginal Commit:\r\nhttps:\/\/github.com\/moby\/moby\/commit\/5a5b7fee330867bc313a7b33a2e9611ffeea6328\r\n\r\nWe have several options here and I am willing to do the work to resolve this. \r\n\r\n1. Remove the feature entirely\r\n2. Change the name of property to EnableGatewayDNS\r\n3. If the option is not set, default to true\r\n4. Fix issue with DNS\r\n\r\nFor option 3:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/5a5b7fee330867bc313a7b33a2e9611ffeea6328\/libnetwork\/drivers\/windows\/windows_store.go#L154\r\n\r\nThe user still has the ability to set custom DNS servers via the \"com.docker.network.windowsshim.dnsservers\" option as a comma separated string on network creation or by using the --dns flag on docker run. Note that when using these options, the default gateway is still used as the primary however the fail behavior goes to the additional DNS servers. \r\n\r\nIssues: \r\nhttps:\/\/forums.docker.com\/t\/dns-broken-on-docker-desktop-for-windows\/130121\r\nhttps:\/\/github.com\/microsoft\/Windows-Containers\/issues\/216\r\n\r\nMore issues exist. It does seem that most people are setting \"com.docker.network.windowsshim.disable_gatewaydns=true\" as the default behavior of false is not applicable to 99% of users. \r\n\r\nIn this comment, I propose a work around: \r\nhttps:\/\/github.com\/microsoft\/Windows-Containers\/issues\/216#issuecomment-1775709793","comments":["Hello @corhere @akerouanton I was told by @neersighted to ping you. If you have any thoughts on which direction you want me to head I am open to it! ","[There is supposed to be a DNS resolver listening on the default gateway.](https:\/\/github.com\/moby\/libnetwork\/pull\/1412) That resolver is a requirement for [automatic service discovery.](https:\/\/docs.docker.com\/network\/network-tutorial-standalone\/#:~:text=On%20user%2Ddefined,service%20discovery.) Disabling gateway DNS by default will break that functionality, which when it works as intended, _is_ applicable to the majority of users. It would be analogous to disabling kube-dns by default on Windows k8s clusters.\r\n\r\nIf the DNS resolver on the default gateway is not resolving queries, that is a bug which needs to be addressed. And given the totality of evidence, with users on v20.10 daemon versions complaining about issues starting after a Windows Update, it seems to me that a bug in the [Host Networking Service](https:\/\/learn.microsoft.com\/en-us\/virtualization\/windowscontainers\/container-networking\/architecture) is the most likely root cause.\r\n\r\nWhat you have proposed is a backwards-incompatible change. If we were to do that as a workaround, either everyone would have to deal with automatic service discovery being disabled by default on Windows forever or we'd have to introduce a second backwards-incompatible change to revert back to the original defaults once gateway DNS is fixed.\r\n\r\nAs of v24.0.0, default network options can be set in daemon.json.\r\n\r\n- #43197\r\n\r\nImpacted users can therefore work around the issue by configuring their daemon like so:\r\n```go\r\n{\r\n    \"default-network-opts\": {\r\n        \"nat\": { \"com.docker.network.windowsshim.disable_gatewaydns\": \"true\" }\r\n    }\r\n}\r\n```","I have asked internally about this however this being 7 years ago I doubt anyone remembers. I have disabled it and have experienced no issues. My guess that this is no longer needed however I will sync with the dev lead on the HNS team. It seems that most people are disabling this. If the requirement has changed I will move forward with removing this. ","Thanks! Yes, some of these areas are deep, dark corners, so any guidance from the team is appreciated (also if there's documented changes around this that may be Windows version related)","> I have disabled it and have experienced no issues. My guess that this is no longer needed\r\n\r\nI doubt it. The DNS resolver I mentioned earlier [is part of dockerd.exe](https:\/\/github.com\/moby\/moby\/blob\/63b2a2138dacd5d141fabc5bc132e403f577ed38\/libnetwork\/network_windows.go). Unless DNS queries are routed to it, containers attached to the network will be unable to resolve each other by name. For example, if you have two containers named `mycontainer1` and `mycontainer2` connected to the same nat network, you should be able to successfully run `ping mycontainer2` from mycontainer1.","Hmm. So with the value set to true we can ping outside domains such as Microsoft \n.com hovever cannot use the container names to resolve. With the value set to false we can resolve container names however cannot resolve anything outside. This could be using nslookup or just pinging Microsoft.com. Something is definitely wrong here though. I'm going to create a chart of working and non working scenarios. I would expect if the value is false that all scenarios to work here. ","> With the value set to false we can resolve container names however cannot resolve anything outside.\r\n\r\nThat's actually good news: DNS queries are being received and replied to by Moby's (libnetwork's) DNS resolver. It's just not successfully forwarding queries to upstream DNS servers that it can't resolve on its own. As far as I can tell, that never would have worked on Windows as the resolver is instantiated with that functionality disabled (`proxyDNS=false`).\r\nhttps:\/\/github.com\/moby\/moby\/blob\/63b2a2138dacd5d141fabc5bc132e403f577ed38\/libnetwork\/network_windows.go#L51\r\n\r\nI can think of a few possible explanations for public DNS resolution could have not been broken for containers attached to NAT networks in the past:\r\n- HNS used to have some magic sitting in front of Moby's DNS resolver to resolve queries itself when Moby's replied NXDOMAIN?\r\n- HNS used to configure DNS inside the container compartment to only query the gateway DNS resolver for \"local\" names (no dots) and query other servers for everything else?\r\n- Windows' DNS resolver behaviour used to be such that it would try secondary servers after the first replied NXDOMAIN?\r\n- Moby's DNS resolver used to handle queries it could not resolve locally in such a way that would make Windows' DNS resolver try querying secondary DNS servers, but now behaves differently?\r\n- DNS forwarding for NAT networks never worked on Windows (Mandela Effect)?\r\n\r\nRegardless, I still think that the best path forward for all users is to fix DNS forwarding on Windows so that both container names and public DNS can be resolved by default. I see [a Win32 API function `DnsQueryRaw`](https:\/\/learn.microsoft.com\/en-us\/windows\/win32\/api\/windns\/nf-windns-dnsqueryraw) which would do exactly what we need, but it doesn't look to be in any released Windows versions yet. We already have a DNS forwarder implemented in userspace that we use on Linux; all we'd have to do to also use it on Windows is set `proxyDNS=true` and [configure the list of external DNS servers to forward to](https:\/\/github.com\/moby\/moby\/blob\/cff4f20c44a3a7c882ed73934dec6a77246c6323\/libnetwork\/resolver.go#L185-L187). Thankfully [that looks easy enough to query from the system](https:\/\/learn.microsoft.com\/en-us\/windows\/win32\/api\/windns\/nf-windns-dnsqueryconfig), at least for IPv4 servers. Do you know of a way to query the list of configured IPv6 DNS servers on the system?","I am now able to see the issue with the internal DNS resolution is broken now. Thanks for pointing that out! Since the list of DNS Servers is accurate, are you saying the work involved is setting the proxyDNS bool to true? \r\n","> Since the list of DNS Servers is accurate, are you saying the work involved is setting the proxyDNS bool to true? \n\nNot quite.  Merely setting proxyDNS to true is not going to be sufficient as the list of external DNS servers is _empty_. The `SetExtServers` method also needs to be called for DNS forwarding to be functional. ","History clarification: The `DisableGatewayDNS` network-creation flag was added in 2017 (via https:\/\/github.com\/moby\/libnetwork\/pull\/2021) to support _disabling_ the gateway being the default DNS, [relying on a new HNS API in Windows Server 1709](https:\/\/github.com\/moby\/libnetwork\/pull\/2021#issuecomment-350166832). As noted at https:\/\/github.com\/docker\/for-win\/issues\/397#issuecomment-273646131, the flow of \"SRV_FAIL from dockerd-internal-DNS, then fallback to secondary DNS\" was [the original design state](https:\/\/github.com\/moby\/libnetwork\/pull\/1412) for NAT networks.\r\n\r\nAs noted in that PR, there was already an option (`DisableDNS` added as part of [overlay network default gateway support](https:\/\/github.com\/moby\/libnetwork\/pull\/1541)) which could be set when creating an individual endpoint for the same effect. One of `DisableGatewayDNS`'s effects was to set `DisableDNS` on all subsequent endpoints created on that network. This leads to the actual HNS option, [`EnableInternalDNS` which was originally set `true` for NAT networks](https:\/\/github.com\/moby\/libnetwork\/pull\/1412\/files#diff-1517f737a45c2b49399bc0206cedef10bc4ce3d9b165d8f16ba6fa9deaef707fR484-R486), and `false` for others, and then when `DisableDNS` was introduced, [setting it forced `EnableInternalDNS` to false on `nat` networks](https:\/\/github.com\/moby\/libnetwork\/pull\/1541\/files#diff-1517f737a45c2b49399bc0206cedef10bc4ce3d9b165d8f16ba6fa9deaef707fL485-R520). So I'm _guessing_ that overlay networks required the network creator to provide a DNS server for that network, e.g. kube-and hence ignored \r\n\r\nPer https:\/\/github.com\/docker\/for-win\/issues\/397#issuecomment-311475521, it seems both [`nslookup`](https:\/\/github.com\/microsoft\/Windows-Containers\/issues\/216#issuecomment-1086211615) and [the .NET resolver (of some vintage)](https:\/\/github.com\/docker\/for-win\/issues\/397#issuecomment-310760523) did not fall-over to the secondary DNS server when the primary sent back SRV_FAIL. Although when .NET's `System.Net.Dns.GetHostAddresses` was [called through PowerShell](https:\/\/github.com\/moby\/moby\/issues\/30260#issuecomment-277027799), it worked, so maybe there was a specific third-party .NET resolver that had this issue, there's not enough detail in the failure report to know for sure. Ironically, the proxy DNS client in moby\/libnetwork didn't fail-over on SRV_FAIL from an upstream DNS server at the time, but [does now](https:\/\/github.com\/moby\/libnetwork\/pull\/2121).\r\n\r\nHowever, if `ping` (or [`Resolve-DNS`](https:\/\/github.com\/microsoft\/Windows-Containers\/issues\/216#issuecomment-1419601968)) is showing the same results as `nslookup`, then the problem is probably not the DNS resolver as it was back when the flag was added, but something else has gone wrong.\r\n\r\nI do wonder, I don't see in any of the current bugs an `ipconfig \/all` for the problematic case... Is the _secondary_ DNS address being set? Perhaps _that_ is what broke, and why dockerd DNS server results work, but the fail-over doesn't?\r\n\r\n----\r\n\r\nAll that said, making the dockerd DNS service able to proxy on Windows and _only_ using that from inside a container by default seems like a reasonable, and better-isolated result to me.\r\n\r\nI'm not sure why that wasn't done in the first place, but some questions leap out to me:\r\n* Can we stop HCN from setting up the secondary DNS server address, and _only_ use the dockerd proxy?\r\n* _Which_ DNS server or servers do we proxy to?\r\n  * DNS Server config in Windows is per-adapter, do we need to replicate [this process](https:\/\/serverfault.com\/a\/84293\/148812) ([Modulo changes in Windows 10](https:\/\/serverfault.com\/a\/805455\/148812)) so people see the expected DNS resolution results from the proxy when using VPN or otherwise multihoming?\r\n    * I haven't looked, can the Docker DNS proxy dynamically update as adapters are connected\/removed?\r\n  * Does ICS (or whatever supports NAT on Windows these days) by any chance provide a DNS server that already takes care of that, and we can just proxy to it?","The Docker DNS proxy could be easily extended to support dynamic updates to the server list by making `SetExtServers` safe to call concurrently.","> > Since the list of DNS Servers is accurate, are you saying the work involved is setting the proxyDNS bool to true?\r\n> \r\n> Not quite. Merely setting proxyDNS to true is not going to be sufficient as the list of external DNS servers is _empty_. The `SetExtServers` method also needs to be called for DNS forwarding to be functional.\r\n\r\nWe do have a list of DNS servers though. Should the SetExtServers that the current list of DNS Servers passed into it (Excluding the default gateway). ","> > > Since the list of DNS Servers is accurate, are you saying the work involved is setting the proxyDNS bool to true?\n> > \n> > Not quite. Merely setting proxyDNS to true is not going to be sufficient as the list of external DNS servers is _empty_. The `SetExtServers` method also needs to be called for DNS forwarding to be functional.\n> \n> We do have a list of DNS servers though. Should the SetExtServers that the current list of DNS Servers passed into it (Excluding the default gateway). \n\nWhich list, the user-supplied one? That's the easy answer for which servers `libnetwork.Resolver` should forward to. But what about when the user does not go out of their way to configure any DNS servers on the container network? DNS forwarding Just Works on Linux containers attached to a bridge network with no configuration. It should work on Windows containers attached to a nat network (the closest analogue), too.","> [There is supposed to be a DNS resolver listening on the default gateway.](https:\/\/github.com\/moby\/libnetwork\/pull\/1412) That resolver is a requirement for [automatic service discovery.](https:\/\/docs.docker.com\/network\/network-tutorial-standalone\/#:~:text=On%20user%2Ddefined,service%20discovery.) Disabling gateway DNS by default will break that functionality, which when it works as intended, _is_ applicable to the majority of users. It would be analogous to disabling kube-dns by default on Windows k8s clusters.\r\n> \r\n> If the DNS resolver on the default gateway is not resolving queries, that is a bug which needs to be addressed. And given the totality of evidence, with users on v20.10 daemon versions complaining about issues starting after a Windows Update, it seems to me that a bug in the [Host Networking Service](https:\/\/learn.microsoft.com\/en-us\/virtualization\/windowscontainers\/container-networking\/architecture) is the most likely root cause.\r\n> \r\n> What you have proposed is a backwards-incompatible change. If we were to do that as a workaround, either everyone would have to deal with automatic service discovery being disabled by default on Windows forever or we'd have to introduce a second backwards-incompatible change to revert back to the original defaults once gateway DNS is fixed.\r\n> \r\n> As of v24.0.0, default network options can be set in daemon.json.\r\n> \r\n> * [Introduce config option for default generic network options of newly created networks\u00a0#43197](https:\/\/github.com\/moby\/moby\/pull\/43197)\r\n> \r\n> Impacted users can therefore work around the issue by configuring their daemon like so:\r\n> \r\n> ```go\r\n> {\r\n>     \"default-network-opts\": {\r\n>         \"nat\": { \"com.docker.network.windowsshim.disable_gatewaydns\": \"true\" }\r\n>     }\r\n> }\r\n> ```\r\n\r\nI tried this option on Docker version 25.0.3, build 4debf41, and it did not work for the default \"nat\" network - it was still created with a gateway dns (which is not working).","@florianehmke As you discovered, these options don't apply to the default networks. You need to create a custom network."],"labels":["platform\/windows","kind\/bug","area\/networking","area\/networking\/dns"]},{"title":"if docker cp  container   hang will cause  docker inspect container hang","body":"### Description\r\n\r\ndocker cp use  containerExtractToDir\r\n```\r\nfunc (daemon *Daemon) containerExtractToDir(container *container.Container, path string, copyUIDGID, noOverwriteDirNonDir bool, content io.Reader) (err error) {\r\n\tcontainer.Lock()\r\n\tdefer container.Unlock()\r\n```\r\ndocker cp will hold a container lock and if archive.Untar func hang, docker inspect can't get this  container lock. docker inspect will hang too.\r\n\r\n### Reproduce\r\n\r\n\r\n1. add some delay to imitate docker cp hang\r\n```\r\nfunc (daemon *Daemon) containerExtractToDir(container *container.Container, path string, copyUIDGID, noOverwriteDirNonDir bool, content io.Reader) (err error) {\r\n\tcontainer.Lock()\r\n\tdefer container.Unlock()\r\n        time.Sleep(time.Second * 60)\r\n```\r\n2. run docker cp  to a container\r\n3. run docker inspect \r\n\r\n### Expected behavior\r\ndocker cp shouldn't hold a container lock.\r\ndocker inspect will not hang even though docker cp hang.\r\n\r\n### docker version\r\n\r\n```bash\r\n[root@localhost ~]# docker version\r\nClient:\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:04:00 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n[root@localhost ~]# docker version\r\nClient:\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:04:00 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n```\r\n\r\n\r\n### Additional Info","comments":["Thanks for reporting! I know we dealt with locking being \"too aggressive\" (or not fine-grained enough) in the past, so we should look if we can improve this if possible. I know that changes around this area have proven to not always be _easy_ to address due to some older design choices; things _improved_; similar issues were addressed in the past where `docker ps` (listing containers) also could be impacted by locks, but there's likely still room for improvement; some of which may require refactoring \/ redesigning code, but such changes impact various \"hot-paths\" in the code, so must be done very carefully.\r\n\r\nThe lock in this case looks to be part of `State`, which is embedded in the container struct; https:\/\/github.com\/moby\/moby\/blob\/311b9ff0aa93aa55880e1e5f8871c4fb69583426\/container\/container.go#L64-L65\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/311b9ff0aa93aa55880e1e5f8871c4fb69583426\/container\/state.go#L15-L19\r\n\r\n\r\nThat said, it _seems_ (just \"gut feeling\") like the Lock is treated more as \"global\" lock for the container; we'd have to check if the lock is used for the right reason everywhere (we should also consider if that mutex should be public, as it's generally preferable for locks to be \"internal\").\r\n\r\nFor the copy itself, we'd have to evaluate what locking is needed, but I can (without having dug into that) somewhat expect that some concurrency control is needed to prevent the container's state being changed _during_ the copy operation, as doing so may result in mounts\/unmounts, which would impact the copy (assuming we don't want a copy operation to be cancelled \/ fail).\r\n\r\nPerhaps we should look at the _inspect_ side of things though, as that should only require \"read\" access to information of the container.\r\n\r\nThe \"inspect\" code has a couple of implementations for older API versions https:\/\/github.com\/moby\/moby\/blob\/311b9ff0aa93aa55880e1e5f8871c4fb69583426\/daemon\/inspect.go#L23\r\n\r\nSo, looking at the implementation for the _current_ API version, locking happens around this code https:\/\/github.com\/moby\/moby\/blob\/311b9ff0aa93aa55880e1e5f8871c4fb69583426\/daemon\/inspect.go#L41-L79\r\n\r\nWhere `getInspectData` uses `Container.State` (which, per the above, would likely need the mutex for synchronisation). Not sure if the Lock is needed beyond that, but even if it would be just for `Container.State`, I don't think we would be able to get rid of the lock. Using an `RWMutex` instead could be an option (which would allow the `inspect` to proceed), but RWMutex is known to come with an overhead, so that must be outweighed.\r\n\r\nBased on my quick (and very incomplete) look at this;\r\n\r\n- We should document what the `State.mutex` is protecting\r\n- If it's _intended_ to only protect `State`, then it would be better to replace all occurences of `Container.Lock()` with an explicit `Container.State.Lock` to prevent ambiguity \/ wrong expectations on calling sites\r\n- (But _ideally_ un-export the mutex if possible)\r\n- Based on the above, evaluate if it's used _for the correct reason_ in all places (it's possible the `Container.Lock()` is used with the expectation to lock the container as a whole)\r\n","I changed the label to `enhancement` as I don't think this is necessarily a _bug_ (but as I wrote above; definitely something we should look at improving if possible).\r\n\r\nAlso \/cc @corhere @rumpl @laurazard if any of you feels like digging into some of these locking patterns.\r\n\r\n","Another option for reducing lock contention is to use the read-only replica of the container state for `docker inspect`, following up on\r\n- #31273","I discussed this issue with some other maintainers (@cpuguy83, @corhere); one thing pointed out was that `inspect` was called out as one of the remaining things to do after some of the locking issues were addressed in https:\/\/github.com\/moby\/moby\/pull\/31273\r\n\r\n> In the future, more read operations can be served from the in-memory ACID store, e.g.:\r\n> \r\n> - docker inspect (shouldn't be too hard, but will require deep copying some pointers currently being held by the container during Snapshot())\r\n> - healthcheck probe results\r\n> - stats collection\r\n> - container.RWLayer\r\n> - ExecConfigs\r\n\r\nOther things that have been discussed (on Slack, and a maintainers call);\r\n\r\n- We agree that the mutex being _exported_ is far from ideal; having the mutex exported, as well as it being used as `container.Lock()` (not `container.State.Lock()`) makes it unclear on _intent_.\r\n- Locking \/ synchronisation _is_ needed in various code-paths; both to synchronise `State`, but also to protect mounts that are used. Both of those will require investigating for each use, as making changes in those areas is not without risk.\r\n- _Potentially_ `docker inspect` can use the in-memory presentation of containers (as is done for `docker ps`), but this needs to be looked into (most notably to make sure it would properly reflect the current `State` of the container).\r\n- Using a `RWMutex` (and an `Rlock()` for `inspect`) _could_ be a short-term \/ temporary solution if this does not impact performance for \"hot paths\" (most notably; paths that act on \"batch\" operations, such as listing containers)\r\n\r\nIn the long term, we should look at a better separation of \"state\" and \"config\" (which will be a significant amount of work).","(I should add to the above that, If using a `RWMutex` won't significantly impact performance elsewhere, that I would personally be ok with taking that short-term solution while we look into better options).\r\n"],"labels":["kind\/enhancement","area\/daemon"]},{"title":"Improve documentation around maintenance, building, and packaging","body":"- Closes https:\/\/github.com\/moby\/moby\/issues\/46761\r\n\r\nAttempt to better document the state of the project. This is incremental progress, but I'm hoping this can help \"unstick\" us and get a little more momentum going on documentation and improved transparency.\r\n\r\nI'm only married to the intent of this PR, and not the specific phrasing, so please do suggest edits liberally. Likewise, I've guessed a bit as to intent\/content with the branch documentation; please do suggest improvements\/corrections if my mental model differs from reality (especially for those I have called out by name!).","comments":[],"labels":["area\/docs","area\/project","area\/packaging"]},{"title":"container: Remove RWLayer","body":"Don't store the RWLayer in the Container object - it's an implementation detail of the graphdriver ImageService and it shouldn't be exposed. The ID of the layer is always the same as the container ID so it can be used to move the direct RWLayer accesses to the graphdriver's ImageService implementation.\r\n\r\n**- What I did**\r\nRemoved `RWLayer` field from `Container` struct and removed some ugly `if daemon.UsesSnapshotter()` branches.\r\n\r\n**- How I did it**\r\nInstead of storing `RWLayer` and accessing it when needed, obtain it directly from the layerStore. The ID of the RW layer always matches the container ID.\r\n\r\n**- How to verify it**\r\nCI.\r\n\r\n**- Description for the changelog**\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/2-code-review","area\/daemon","kind\/refactor","containerd-integration"]},{"title":"docker swarm leave leaves services in an inconsistent state","body":"### Description\n\n`docker swarm leave` leaves services in a weird state: After a node has left the swarm many services are left showing strange replica counts such as \"5\/3\". This goes away only when the \"Down\" node is removed from the swarm.\r\n\r\nIf, for any reason, the pruning of Down nodes cannot be done immediately, this means any health or scaling logic that depends on the number of healthy replicas is being fed incorrect and inconsistent results.\r\n\n\n### Reproduce\n\n1. have a swarm with many tasks on many worker nodes\r\n2. run `docker container ls` on a worker node to verify there are at least a few task conainers running on this worker.\r\n3. run `docker swarm leave` on one of the workers and wait for the command to complete.\r\n4. run `docker container ls` to verify there are now no task container still running on this ex-worker.\r\n5. run `docker node ls` on one of the managers to confirm the worker has left and is now Down.\r\n6. run `docker service ls` on one of the managers once the worker has left.\n\n### Expected behavior\n\n1. the worker should - and does - show no containers running once it has left the swarm.\r\n2. the worker should show as \"Down\" on the list of nodes when the manager is queried. It does.\r\n3. service state on the swarm should be consistent. It is not.\r\n\r\nThis is what my Grafana service shows after I performed the process above:\r\n```bash\r\n> docker service ls\r\nID             NAME                                                           MODE         REPLICAS               IMAGE                                                                                                                                 PORTS\r\n...\r\n57lklq8ip9oe   grafana_grafana                                                replicated   2\/1                    grafana\/grafana:latest               \r\n> docker service ps 57l\r\nID             NAME                    IMAGE                    NODE                                          DESIRED STATE   CURRENT STATE           ERROR     PORTS\r\niaq4vvxhgst9   grafana_grafana.1       grafana\/grafana:latest   ip-100-64-84-87.eu-west-1.compute.internal    Running         Running 4 minutes ago             \r\nzral81ytn27s    \\_ grafana_grafana.1   grafana\/grafana:latest   ip-100-64-112-69.eu-west-1.compute.internal   Shutdown        Running 21 hours ago              \r\necexdtdbibe9    \\_ grafana_grafana.1   grafana\/grafana:latest   ip-100-64-84-87.eu-west-1.compute.internal    Shutdown        Shutdown 21 hours ago             \r\nsrmjc18oskfw    \\_ grafana_grafana.1   grafana\/grafana:latest   ip-100-64-73-37.eu-west-1.compute.internal    Shutdown        Shutdown 20 hours ago             \r\ngv42z97j02ef    \\_ grafana_grafana.1   grafana\/grafana:latest   ip-100-64-84-87.eu-west-1.compute.internal    Shutdown        Shutdown 21 hours ago         \r\n```\r\n\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ced0996\r\n Built:             Thu Aug 31 00:00:00 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       a61e2b4\r\n  Built:            Thu Aug 31 00:00:00 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.2\r\n  GitCommit:        0cae528dd6cb557f7201036e9f43420650207b58\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.0.0+unknown\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: awslogs\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 6366w3gyoqvyam0n78kh8qt2i\r\n  Is Manager: true\r\n  ClusterID: hdcgbncq0mhon3bum6t8nrlsr\r\n  Managers: 3\r\n  Nodes: 8\r\n  Default Address Pool: 172.24.0.0\/14  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 100.64.19.130\r\n  Manager Addresses:\r\n   100.64.115.121:2377\r\n   100.64.19.130:2377\r\n   100.64.31.70:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0cae528dd6cb557f7201036e9f43420650207b58\r\n runc version: f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.55-75.123.amzn2023.x86_64\r\n Operating System: Amazon Linux 2023\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.629GiB\r\n Name: ip-100-64-19-130.eu-west-1.compute.internal\r\n ID: f0803777-00db-42a0-bc4e-e56981f8b6be\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0\/16, Size: 16\r\n   Base: 172.18.0.0\/16, Size: 16\r\n   Base: 172.19.0.0\/16, Size: 16\r\n   Base: 172.20.0.0\/14, Size: 20\n```\n\n\n### Additional Info\n\nAmazon Linux 2023 running on AWS EC2.","comments":["This may be closely related to your older ticket; https:\/\/github.com\/moby\/moby\/issues\/46382\r\n\r\n`docker swarm leave` on a worker is a local operation, but the worker doesn't have privileges to modify swarm state, so from a cluster perspective, the node went down. If the node is down, the cluster does not update state of tasks that were running on the node before it went offline (so it assumes they are still running as they were). It would be nice though to have some indicator for this. \r\n\r\nThat said, I'm not sure why the `2\/1` happened; that would indicate that either the service was scaled down (but due to the node being offline, it not being able to scale it down), or that it created a new task (on a different machine) and (again, due to the worker being down) not able to shutdown the old task.\r\n\r\nDid these states happen without changing services (after the node went \"Down\"), or were services updated (with `--update-order` being \"start-first\")?\r\n\r\n\r\n\/cc @dperny @corhere ","Taking into account my lowered expectations from the previous ticket, `docker swarm leave` does still make some efforts to be a better neighbour than simply going \"Down\": On a node with many tasks it does take a while to return control, and when it does, all the local task containers have been stopped.\r\n\r\nHowever, no other actions were taken to get to the 2\/1 state. The docker ps list above indicates that the docker swarm managers still believe that Grafana is in a desired-state=stopped, but current-state=running on the now removed\/down node.\r\n\r\nI do not pretend to know the details of how, in normal operation, managers find out that workers are no longer running a task because it crashed or stopped or something: I do know that when I started `docker swarm leave` on a node on our dev swarm to test its behaviour, there were about 20 task containers. And when it finally returned they were all closed and recreated on other workers. But somewhere from 15 to 20 services were now, like Grafana, showing (x+1)\/x replica counts.","This might also be related to #45922, where a node, that just goes down leads to confusing outputs of `docker service ls`.\r\nBut I haven't studied what the \"swarm leave\" command will do in this particular case.\r\n\r\nI opened a PR #3146 in the swarm repo to potentially \"fix\" this issue by exlcuding \"running task\" on \"down nodes\" on the status ouputs of the \"service list endpoint\". \r\nSince I'm quite new I find it hard to figure out where it is best to change the information based on the state of the node.\r\nIdeas on this:\r\n- we could remove the running task on a removed node from the running tasks output in the list-service output from swarm\r\n- we could put the task into an \"orphaned state\" instead of just leaving it running. But I don't know how this affects other components acting on the desired state of shutdown. I also don't know what happens to nodes that are unresponsive for only a short amount of time, then come back with the task still running. They will have local state of the task RUNNING and this state is now inconsistent with the overall cluster state of \"ORPHANED\". But I guess orphaned has been initially implemented to deal with such tasks\r\n- we could check the state on the client and only output those tasks as running whose nodes are in a functioning state"],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"c8d: improve pull errors (non-matching platform, etc)","body":"- originally posted in https:\/\/github.com\/moby\/moby\/pull\/46517#discussion_r1332825725\r\n\r\nThe containerd error is less informative than the old one, because it doesn't show _WHAT_ we weren't able to match (which _could_ be obvious if a user explicitly passed a `--platform`, but less so if not, but could also be that user passed a _partial_ platform, and we normalized it).\r\n\r\nThe \"non-snapshotter\" error looks to be this one; https:\/\/github.com\/moby\/moby\/blob\/0d9da7367dd79e6e5c46ef4a37ca0fbe79d2bf8f\/distribution\/pull_v2.go#L939-L945\r\n\r\nFor containerd, the error is produced here; https:\/\/github.com\/moby\/moby\/blob\/0d9da7367dd79e6e5c46ef4a37ca0fbe79d2bf8f\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go#L310-L312\r\n\r\n\r\nI guess _ideally_ the containerd error would be somewhat more informative (_what_ were we trying to match, and couldn't find?), but it looks like that's less trivial, because that's fully up to the `HandlerFunc` that's passed (which does the filtering).\r\n\r\nThe alternative here would be to handle this on our side: when we get the result. The error returned is typed; it's a `errdefs.ErrNotFound`, so we can decorate the error (and perhaps we can decorate it regardless of what the reason was), i.e.;\r\n\r\n```go\r\nif cerrdefs.IsNotFound(err) {\r\n    err = fmt.Errorf(\"no matching manifest for %s: %w\", formatPlatform(e.platform), err)\r\n} else {\r\n    err = fmt.Errorf(\"failed to find matching manifest for %s: %w\", formatPlatform(e.platform), err)\r\n}\r\n```\r\n\r\n(I removed \"in the manifest list entries\" from the above, but maybe we should keep it, not sure if it's \"too much\")\r\n","comments":[],"labels":["kind\/enhancement","containerd-integration","area\/ux"]},{"title":"Ability to mount cgroupfs as read\/write with `--cgroupns=private`","body":"### Description\r\n\r\nCurrently (Docker 24.0.7), it does not appear to be possible to mount `\/sys\/fs\/cgroup` read\/write while also using cgroup2 namespaces. `\/sys\/fs\/cgroup` is always mounted readonly unless the container is privileged, which prevents normal containers from being able to use cgroups. It is possible to mount the host cgroupfs with `-v \/sys\/fs\/cgroup:\/sys\/fs\/cgroup:rw`, but this bind-mounts the host cgroupfs. A bind mount defeats the purpose of cgroup namespaces, as it would expose the host's full cgroups tree to the container instead of its own cgroup namespace.\r\n\r\nIt appears podman can mount `\/sys\/fs\/cgroup` read-write with the arg `--security-opt unmask=\/sys\/fs\/cgroup`, which is used with \"systemd mode\". Is it possible to add a similar flag to allow the container to access cgroups securely?","comments":[],"labels":["status\/0-triage","kind\/feature"]},{"title":"convert to go modules (go.mod\/go.sum)","body":"### Description\n\nCurrently, this repo has vendor.mod\/vendor.sum which look like they could be go.mod\/go.sum.\r\nHowever, I find the following comment in vendor.mod.\r\n\r\n```\r\n\/\/ 'vendor.mod' enables use of 'go mod vendor' to managed 'vendor\/' directory.\r\n\/\/ There is no 'go.mod' file, as that would imply opting in for all the rules\r\n\/\/ around SemVer, which this repo cannot abide by as it uses CalVer.\r\n```\r\n\r\nCan you comment on what CalVer is, and whether I can treat vendor.mod\/sum as go.mod\/sum  when I build this repo?\r\nAlso, I am curious about your plans. Are you planning to move to go.mod\/go.sum in this repo?\r\nMaking this move or being able to treat vendor.mod\/sum as go.mod\/sum down stream would make downstream building much easier.\r\n\r\nThanks for your time.\r\n\r\nWilliam\n\n### Reproduce\n\nn\/a\n\n### Expected behavior\n\nUsing go modules is the simplest way to build go programs these days.\n\n### docker version\n\n```bash\nn\/a\n```\n\n\n### docker info\n\n```bash\nn\/a\n```\n\n\n### Additional Info\n\n_No response_","comments":["This appears to be something of a frequent question; but the answer in short is that we are not a Go module, and we are not compatible (directly) with Go modules.\r\n\r\nThe project must be built in GOPATH mode; the `vendor.mod` file is merely an artifact of the tooling we use to generate the `vendor\/` folder, but should not be taken to be more. We are working to transition to a Go module (and I'm bugging @thaJeztah to open a tracking issue), but are not there yet, and until then will require GOPATH mode and be `+incompatible` in our Go module versions.\r\n\r\nhttps:\/\/github.com\/moby\/moby\/issues\/44618 has more background on this, but the long and short of it is that building with a `go.mod` present changes the semantics of the Go toolchain, and we don't test or support that upstream. It may be possible to get correct results, but that is at your own risk\/requires you to make the effort to ensure that.","Also, with regard to `CalVer`, that comment is no longer accurate; we have transitioned (back!!!) to SemVer as part of our Go module transition. I've opened https:\/\/github.com\/moby\/moby\/pull\/46772 to address this (and help answer this question) among other things.","Cool. I think a tracker for your module migration would be beneficial.\r\nI don't know how much longer gopath mode is going to be a thing; go upstream seems to strongly discourage using it at this point.","Removing GOPATH mode will break the Go 1.x compatibility promise; however, the language version will be frozen in GOPATH mode, so we are on something of a timer as it's inevitable that vendored code will require future language versions. "],"labels":["kind\/question","area\/packaging"]},{"title":"c8d: docker info \"storage driver\" section should provide more details","body":"### Description\n\nWhen using graph-drivers, the storage-driver section provides some additional details about the host's \"backing\" filesystem.\r\n\r\n```\r\nStorage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n```\r\n\r\nWhen using snapshotters, we only list the name of the default snapshotter (which is currently a \"global\" default, and we don't (yet) allow per-container snapshotters, but this may be something we add in future);\r\n\r\n```\r\nStorage Driver: stargz\r\n  driver-type: io.containerd.snapshotter.v1\r\n```\r\n\r\nThe additional information has often proven to be useful when triaging \/ debugging reported issues, so we should look at bringing (some of) that information back, but we need to look what the best location is to surface;\r\n\r\n- When using graph-drivers, the additional information is provided by the graph-driver, and the information shown differs between drivers, which can make sense because for a block-storage driver, the \"backing filesystem\" is not really relevant.\r\n- Does containerd (and snapshotters) have an API\/interface to provide additional information relevant to the snapshotter?\r\n- OTOH, some of that information could be considered \"general\" information, so perhaps it's fine to collect (and present) it separate from the active (default) snapshotter\r\n\r\nSomewhat related; if we support multiple snapshotters (in future), should we provide the list of available snapshotters? Containerd detects support for each snapshotter during startup (but not sure if there's a good way to get the list of snapshotter that can be used); also see https:\/\/github.com\/moby\/moby\/issues\/44076\r\n","comments":[],"labels":["area\/storage","kind\/enhancement","containerd-integration"]},{"title":"Networks can sometimes be created with overlapping IP ranges","body":"### Description\n\nMultiple networks sharing a subnet can be created with overlapping IP ranges, unless the ranges are identical.\n\n### Reproduce\n\n1. `docker network create --driver ipvlan --subnet 10.123.0.0\/16 --ip-range 10.123.0.0\/17 overlap1`\r\n    -> Success\r\n2. `docker network create --driver ipvlan --subnet 10.123.0.0\/16 --ip-range 10.123.0.0\/17 overlap2`\r\n    -> Error response from daemon: Pool overlaps with other one on this address space\r\n3. `docker network create --driver ipvlan --subnet 10.123.0.0\/16 --ip-range 10.123.0.0\/18 overlap3`\r\n    -> Success?!\r\n\r\nThe bridge driver has its own overlap checks: no bridge network can be created with a subnet that overlaps the subnet of any existing bridge network, irrespective of ip-range. But e.g. two ipvlan networks can share a subnet, as can a bridge and an ipvlan.\n\n### Expected behavior\n\nThe rules of when networks can have overlapping IPAM pools should be applied consistently: either both `overlap2` and `overlap3` can be created successfully, or neither.\n\n### docker version\n\n```bash\nConfirmed on v20.10.24, v23.0.6, v24.0.6\n```\n\n\n### docker info\n\n```bash\nN\/A\n```\n\n\n### Additional Info\n\n**Duplicate addresses will _not_ be allocated** even if pools overlap. All IPAM pools for the same subnet share the same allocation bitmap so an address allocated from one pool is implicitly reserved in all the other pools.","comments":["Discussed on the maintainer call.\r\n- Fixing the overlap checks to block overlapping ip-ranges could cause users grief: the daemon might fail to start after upgrade\r\n- Allowing network ip-ranges to overlap would give advanced users the flexibility they have been asking for, but would remove guard rails which catch accidental overlaps from e.g. copy-pasting a network config and forgetting to modify it. We could allow overlapping ip-ranges with a warning; the CreateNetwork API already has affordances for returning warnings."],"labels":["kind\/bug","area\/networking","status\/confirmed","version\/20.10","version\/23.0","area\/networking\/ipam","version\/24.0"]},{"title":"`docker run --rm` can end up not removing containers","body":"### Description\r\n\r\nIf a `docker run --rm` command is terminated early, it will leave a container in the `Created` state. This does not require a `SIGKILL`. A `SIGTERM` is sufficient to trigger this behavior.\r\n\r\n### Reproduce\r\n\r\n1. `docker pull debian:bullseye` (or any other image)\r\n2. `echo before:; docker ps -a; docker run --rm -a stderr -a stdout --name test debian:bullseye false & pid=$!; sleep .02; kill $pid; sleep 5; echo after:; docker ps -a`\r\n\r\nThe `sleep .02` might need some finagling since this depends on timing.\r\n\r\n### Expected behavior\r\n\r\n`docker run --rm` should start a new container and remove it in a transactional manner. Since it is a proxy process there will naturally be circumstances where that guarantee is not 100%, but at the least the above state should not be possible.\r\n\r\n### docker version\r\n\r\n```bash\r\ndocker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:31:44 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:44 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 577\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-87-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 30.9GiB\r\n Name: aim\r\n ID: 7RB2:TKWO:VY2S:FZIJ:KEVE:YSSD:HXRZ:2C6Y:TIXD:ZIPE:EWBI:7ZQ4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  localhost:32000\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/342823\/b06e8b36-2e5d-469e-b5e8-9b97977465f1)","comments":["Thanks for reporting!\r\n\r\nWondering if this would be something similar to\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46688\r\n\r\nI recall we also had some issues in containerd where an early exit of a container could be missed (but I think those were addressed).\r\n\r\nDid you find this issue as part of a workflow you already used, and is this something you ran into with older versions of the docker engine (23.0 or older), or did this start to become an issue with v24.0?","I ran into this on servers where a wrapper script handles some bridging between systemd, docker, and various launch params. There's a bash trap which kills the process. Though... now that I think about it, that particular invocation is actually a `--detach`, which means that... I ended up reproducing something completely different than what I was hunting for.... oh ffs \ud83e\udd26\u200d\u2642\ufe0f \r\n\r\nAnyways, I guess this also means that I have only reproduced this with v24. The one where `--detach` is used happens all the way back to v19."],"labels":["area\/runtime","status\/0-triage","kind\/bug","area\/daemon","version\/24.0"]},{"title":"Behaviour when container HealthStartInterval < HealthStartPeriod is unintuitive","body":"### Description\n\nThe daemon waits for `HealthStartPeriod` to elapse before it performs the next probe, even if `HealthStartInterval` ends before that time.\n\n### Reproduce\n\nRun a container with a health-check, e.g.:\r\n```\r\nStartPeriod: 2s\r\nStartInterval: 30s\r\nInterval: 2s\r\nRetries: 1\r\nTest: [\"CMD\", \"\/bin\/false\"]\r\n```\r\n\r\nThe container's health status will become `unhealthy` ~30s after the container is started.\n\n### Expected behavior\n\nThe container's health status becomes `unhealthy` within one `Interval` after `StartPeriod` has elapsed since the container has started.\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:28:49 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.24.2 (124339)\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.44 (minimum version 1.12)\r\n  Go version:       go1.20.8\r\n  Git commit:       HEAD\r\n  Built:            Tue Sep 26 11:52:32 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.6\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.5\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.22.0-desktop.2\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.8\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.0.7\r\n    Path:     \/Users\/tyler\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 30\r\n  Running: 24\r\n  Paused: 0\r\n  Stopped: 6\r\n Images: 77\r\n Server Version: dev\r\n Storage Driver: stargz\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.4.16-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 8\r\n Total Memory: 19.52GiB\r\n Name: docker-desktop\r\n ID: 36252aca-a483-4b9d-b39c-ea24c2c0d207\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\n```\n\n\n### Additional Info\n\n- Related to #46733","comments":[],"labels":["area\/api","area\/runtime","kind\/bug","version\/master"]},{"title":"c8d: Daemon crashes when using `ImageLoad`","body":"### Description\n\nFound by @natalieparellano when testing containerd beta. https:\/\/github.com\/buildpacks\/imgutil has this undocumented _fast path_ for `ImageLoad`. The idea, I believe, is that if we think the base layers should exist in the daemon we can send a 0 size tar header for those layers and the overlay storage system would end up finding the referenced layers by diffid I believe. The new containerd storage layer hard crashes the daemon with an EOF.\n\n### Reproduce\n\n```bash\r\ngit clone https:\/\/github.com\/buildpacks\/imgutil\r\ncd imgutil\r\ngit checkout bug\/crash-docker-containerd\r\ngo test -count=1 -v .\/acceptance\/reproducibility_test.go\r\n```\r\n\r\nResult:\r\n```\r\nDockerd panic: [2023-10-30T19:05:44.292642262Z][dockerd][I] panic: runtime error: invalid memory address or nil pointer dereference\r\n[2023-10-30T19:05:44.292697769Z][dockerd][I] [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x2add536]\r\n[2023-10-30T19:05:44.292705699Z][dockerd][I] \r\n[2023-10-30T19:05:44.292705963Z][dockerd][I] goroutine 297251 [running]:\r\n[2023-10-30T19:05:44.292711163Z][dockerd][I] io.(*PipeWriter).Write(...)\r\n[2023-10-30T19:05:44.292711417Z][dockerd][I] \t\/usr\/local\/go\/src\/io\/pipe.go:165\r\n[2023-10-30T19:05:44.292713012Z][dockerd][I] github.com\/containerd\/containerd\/remotes\/docker.(*pushWriter).Commit(0xc001565570, {0x0?, 0xc34e40?}, 0x0, {0xc017f28a00, 0x47}, {0xc1481d821119c3ee?, 0x7a38d2c3f5?, 0x3a605a0?})\r\n[2023-10-30T19:05:44.292714437Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/remotes\/docker\/pusher.go:441 +0x76\r\n[2023-10-30T19:05:44.292716408Z][dockerd][I] github.com\/containerd\/containerd\/content.Copy({0xc5cac0, 0xc018140c30}, {0xc63168, 0xc001565570}, {0xc3d3e0, 0xc0180fe090}, 0x0, {0xc017f28a00, 0x47}, {0x0, ...})\r\n[2023-10-30T19:05:44.292716948Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/content\/helpers.go:186 +0x3df\r\n[2023-10-30T19:05:44.292718684Z][dockerd][I] github.com\/containerd\/containerd\/remotes.push({0xc5cac0, 0xc018140c30}, {0x7f16504bff18, 0xc018087a70}, {0xc3e9c0?, 0xc0180ce8a0?}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, ...})\r\n[2023-10-30T19:05:44.292718930Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/remotes\/handlers.go:199 +0x5bc\r\n[2023-10-30T19:05:44.292720358Z][dockerd][I] github.com\/containerd\/containerd\/remotes.PushHandler.func1({0xc5ca18, 0xc017f47a90}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, 0x0}, ...})\r\n[2023-10-30T19:05:44.292720937Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/remotes\/handlers.go:166 +0x2a5\r\n[2023-10-30T19:05:44.292722473Z][dockerd][I] github.com\/containerd\/containerd\/images.HandlerFunc.Handle(0xc017f28a00?, {0xc5ca18?, 0xc017f47a90?}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, ...}, ...})\r\n[2023-10-30T19:05:44.292722818Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:59 +0x7f\r\n[2023-10-30T19:05:44.292724433Z][dockerd][I] github.com\/containerd\/containerd\/images.Handlers.func1({0xc5ca18, 0xc017f47a90}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, 0x0}, ...})\r\n[2023-10-30T19:05:44.292724687Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:69 +0x15e\r\n[2023-10-30T19:05:44.292762509Z][dockerd][I] github.com\/containerd\/containerd\/images.HandlerFunc.Handle(0xc3c901?, {0xc5ca18?, 0xc017f47a90?}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, ...}, ...})\r\n[2023-10-30T19:05:44.292763387Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:59 +0x7f\r\n[2023-10-30T19:05:44.292765983Z][dockerd][I] github.com\/containerd\/containerd\/images.Handlers.func1({0xc5ca18, 0xc017f47a90}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, 0x0}, ...})\r\n[2023-10-30T19:05:44.292766218Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:69 +0x15e\r\n[2023-10-30T19:05:44.292767842Z][dockerd][I] github.com\/containerd\/containerd\/images.HandlerFunc.Handle(0xc001121ec8?, {0xc5ca18?, 0xc017f47a90?}, {{0xc017f30990, 0x2c}, {0xc017f28a00, 0x47}, 0x0, {0x0, 0x0, ...}, ...})\r\n[2023-10-30T19:05:44.292768075Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:59 +0x7f\r\n[2023-10-30T19:05:44.292769615Z][dockerd][I] github.com\/containerd\/containerd\/images.Dispatch.func1()\r\n[2023-10-30T19:05:44.292770438Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/images\/handlers.go:168 +0xd6\r\n[2023-10-30T19:05:44.292772048Z][dockerd][I] golang.org\/x\/sync\/errgroup.(*Group).Go.func1()\r\n[2023-10-30T19:05:44.292772303Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/golang.org\/x\/sync\/errgroup\/errgroup.go:75 +0x64\r\n[2023-10-30T19:05:44.292774110Z][dockerd][I] created by golang.org\/x\/sync\/errgroup.(*Group).Go\r\n[2023-10-30T19:05:44.292774360Z][dockerd][I] \t\/go\/src\/github.com\/docker\/docker\/vendor\/golang.org\/x\/sync\/errgroup\/errgroup.go:72 +0xa5\r\n[2023-10-30T19:05:44.344824989Z][dockerd][I] EOF\r\n\r\n```\n\n### Expected behavior\n\nI would expect the `ImageLoad` to fail with an error, allowing the caller to handle the error. In this case `imgutil` has a slow path for downloading base layers if we fail to load the image into the daemon.\r\n\r\nObviously we would love to keep the faster behavior - but a crash should be resolved if nothing else. https:\/\/github.com\/moby\/moby\/issues\/44369 could allow us to correctly probe the daemon storage to see if we should do the work or not.\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:28:49 2023\r\n OS\/Arch:           darwin\/amd64\r\n Context:           default\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.5\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.22.0-desktop.2\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.8\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.0.7\r\n    Path:     \/Users\/jesse.brown\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\n```\n\n\n### Additional Info\n\n_No response_","comments":["We can fix the panic case for 0 length registry uploads, however, I'm not sure containerd will ever handle the workaround being done here cleanly. The issue is that containerd always converts the non-OCI layout (with only `manifest.json`) to an OCI layout on import. The layers referenced in the `manifest.json` are used to create an OCI image manifest, in this case, adding invalid 0 length entries. Even if the layers already exist locally, without including the original OCI image manifest which references those layers, there is no clean way to recreate this manifest. It could possibly be done by scanning all the content looking for the uncompressed digest labels which matches the diff id from the config or by searching the snapshotters and re-extracting the diff, but neither of those approaches are clean and probably won't be accepted upstream.\r\n\r\nMy suggestion for the future would be to use the OCI layout when importing the image. If the content already exists in the content store, then simply omitting it is fine. All import does for OCI layout is import all the blobs and return the `index.json` as the root object, if content is missing, the error would be on unpack, save, or push.","I guess _somewhat_ related; for v25, the `image save` output is now a hybrid format (both \"legacy\" docker format and OCI structure); https:\/\/github.com\/moby\/moby\/pull\/44598 (in case that PR would provide useful information on a possible approach) ","> My suggestion for the future would be to use the OCI layout when importing the image. If the content already exists in the content store, then simply omitting it is fine. All import does for OCI layout is import all the blobs and return the index.json as the root object, if content is missing, the error would be on unpack, save, or push.\r\n\r\nI'm curious for the original issue where the optimization is; is the optimization in this case \"not having to send over the bytes\", or is it on the daemon side (dockerd skips zero-length content, but when _NOT_ zero-length, it would still process them)?\r\n\r\nWe should probably also look if we can make the \"non-snapshotter\" case work if those blobs are _omitted_, to make it work similar to the containerd-snapshotter case (omitting feels like a \"cleaner\" approach than zero-length)).\r\n\r\n(Big disclaimer: I haven't dug in to the existing logic, but we were discussing some of this in our team Yesterday)\r\n\r\n","I think the optimization was to skip processing on the daemon. In our case we know the base layers should be there since we pulled them earlier in the build process. The 0 length trick I think meant it didn\u2019t have to process those layers. \r\n\r\nI wasn\u2019t around when this was written though. I could be mistaken. \r\n\r\nThe omitting would be nice and clean if it worked for both snapshotter and non-snapshotter. We call these \u201csparse\u201d images in buildpacks\/imgutil for oci layout. \r\n\r\nKnowing we can send oci layout tars now is super useful. This isn\u2019t code I\u2019ve touched in a while but we support writing oci layout so it might be time to reevaluate using it as the basis for saving images to the daemon. "],"labels":["status\/0-triage","kind\/bug","containerd-integration"]},{"title":"c8d: fix docker-py failures with snapshotter enabled","body":"### Description\n\nFrom a quick look, most of these may be expected, and the digest-mismatch ones could possibly be resolved if we adjust the tests to be aware of \"snapshotter\" vs \"graphdriver\".\r\n\r\n- [ ] `BuildTest.test_build_squash`: doesn't produce a squashed image\r\n- [ ] `BuildTest.test_build_with_cache_from`: fails on some count (0 != 3)\r\n- [ ] `CommitTest.test_commit`: looks like digest mismatch\r\n- [ ] `CommitTest.test_commit_with_changes`: looks like digest mismatch\r\n- [ ] `PruneImagesTest.test_prune_images`: looks like digest mismatch\r\n- [ ] `ImageCollectionTest.test_pull_multiple`: to be looked at: `500 Server Error: Internal Server Error for url: http+docker:\/\/localhost\/v1.44\/images\/create?fromImage=hello-world`\r\n- [ ] `ImageCollectionTest.test_save_and_load`: looks like digest mismatch\r\n- [ ] `ImageCollectionTest.test_save_and_load_repo_name`: looks like digest mismatch\r\n\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n_________________________ BuildTest.test_build_squash __________________________\r\ndocker\/api\/client.py:268: in _raise_for_status\r\n    response.raise_for_status()\r\n\/usr\/local\/lib\/python3.7\/site-packages\/requests\/models.py:1021: in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nE   requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker:\/\/localhost\/v1.44\/images\/squash\/json\r\n\r\nThe above exception was the direct cause of the following exception:\r\ntests\/integration\/api_build_test.py:374: in test_build_squash\r\n    squashed = build_squashed(True)\r\ntests\/integration\/api_build_test.py:371: in build_squashed\r\n    return self.client.inspect_image(tag)\r\ndocker\/utils\/decorators.py:19: in wrapped\r\n    return f(self, resource_id, *args, **kwargs)\r\ndocker\/api\/image.py:252: in inspect_image\r\n    self._get(self._url(\"\/images\/{0}\/json\", image)), True\r\ndocker\/api\/client.py:274: in _result\r\n    self._raise_for_status(response)\r\ndocker\/api\/client.py:270: in _raise_for_status\r\n    raise create_api_error_from_http_exception(e) from e\r\ndocker\/errors.py:39: in create_api_error_from_http_exception\r\n    raise cls(e, response=response, explanation=explanation) from e\r\nE   docker.errors.ImageNotFound: 404 Client Error for http+docker:\/\/localhost\/v1.44\/images\/squash\/json: Not Found (\"No such image: squash:latest\")\r\n_____________________ BuildTest.test_build_with_cache_from _____________________\r\ntests\/integration\/api_build_test.py:245: in test_build_with_cache_from\r\n    assert counter == 3\r\nE   assert 0 == 3\r\n____________________________ CommitTest.test_commit ____________________________\r\ntests\/integration\/api_image_test.py:89: in test_commit\r\n    assert img['Container'].startswith(id)\r\nE   AssertionError: assert False\r\nE    +  where False = <built-in method startswith of str object at 0x7fef878273b0>('939256c8dcb8a11612bd1899ba3d23bae97ac26481b65e9d5ae385ca2622a41b')\r\nE    +    where <built-in method startswith of str object at 0x7fef878273b0> = ''.startswith\r\n_____________________ CommitTest.test_commit_with_changes ______________________\r\ntests\/integration\/api_image_test.py:107: in test_commit_with_changes\r\n    assert img['Container'].startswith(cid['Id'])\r\nE   AssertionError: assert False\r\nE    +  where False = <built-in method startswith of str object at 0x7fef878273b0>('d661cc7c1e46825c3844ab6aee23da48e18349a8037e080b8bd1b4b974262136')\r\nE    +    where <built-in method startswith of str object at 0x7fef878273b0> = ''.startswith\r\n______________________ PruneImagesTest.test_prune_images _______________________\r\ntests\/integration\/api_image_test.py:328: in test_prune_images\r\n    assert img_id not in [\r\nE   AssertionError: assert 'sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d' not in [None, 'sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d', 'sha256:7e9b6e7ba2842c91cf49f3e214d0...fde787f63f9e016fdc3384500c2823d', 'sha256:719385e32844401d57ecfd3eacab360bf551a1491c05b85806ed8f1b08d792f6', None, ...]\r\n____________________ ImageCollectionTest.test_pull_multiple ____________________\r\ndocker\/api\/client.py:268: in _raise_for_status\r\n    response.raise_for_status()\r\n\/usr\/local\/lib\/python3.7\/site-packages\/requests\/models.py:1021: in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nE   requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http+docker:\/\/localhost\/v1.44\/images\/create?fromImage=hello-world\r\n\r\nThe above exception was the direct cause of the following exception:\r\ntests\/integration\/models_images_test.py:89: in test_pull_multiple\r\n    images = client.images.pull('hello-world', all_tags=True)\r\ndocker\/models\/images.py:466: in pull\r\n    repository, tag=tag, stream=True, all_tags=all_tags, **kwargs\r\ndocker\/api\/image.py:429: in pull\r\n    self._raise_for_status(response)\r\ndocker\/api\/client.py:270: in _raise_for_status\r\n    raise create_api_error_from_http_exception(e) from e\r\ndocker\/errors.py:39: in create_api_error_from_http_exception\r\n    raise cls(e, response=response, explanation=explanation) from e\r\nE   docker.errors.APIError: 500 Server Error for http+docker:\/\/localhost\/v1.44\/images\/create?fromImage=hello-world: Internal Server Error (\"failed to resolve reference \"docker.io\/library\/hello-world\": object required\")\r\n____________________ ImageCollectionTest.test_save_and_load ____________________\r\ntests\/integration\/models_images_test.py:112: in test_save_and_load\r\n    assert result[0].id == image.id\r\nE   AssertionError: assert 'sha256:e515a...875734c4eeb3c' == 'sha256:451ee...1c52a0fdd3e98'\r\nE     - sha256:451eee8bedcb2f029756dc3e9d73bab0e7943c1ac55cff3a4861c52a0fdd3e98\r\nE     + sha256:e515aad2ed234a5072c4d2ef86a1cb77d5bfe4b11aa865d9214875734c4eeb3c\r\n_______________ ImageCollectionTest.test_save_and_load_repo_name _______________\r\ntests\/integration\/models_images_test.py:131: in test_save_and_load_repo_name\r\n    assert result[0].id == image.id\r\nE   AssertionError: assert 'sha256:e515a...875734c4eeb3c' == 'sha256:451ee...1c52a0fdd3e98'\r\nE     - sha256:451eee8bedcb2f029756dc3e9d73bab0e7943c1ac55cff3a4861c52a0fdd3e98\r\nE     + sha256:e515aad2ed234a5072c4d2ef86a1cb77d5bfe4b11aa865d9214875734c4eeb3c\r\n------- generated xml file: \/src\/bundles\/test-docker-py\/junit-report.xml -------\r\n=========================== short test summary info ============================\r\n```","comments":["> `BuildTest.test_build_squash`: doesn't produce a squashed image\r\n\r\nWe had a similar test in the integration and we just skipped it for snapshotters: https:\/\/github.com\/moby\/moby\/pull\/46620\r\n\r\nI don't think we can easily skip it in the docker-py tests...\r\nPerhaps we should just implement squash - actually it shouldn't be that complicated \ud83d\ude04 ","Ideally we'd be able to skip some in upstream, but in the meantime, we can skip some of those on our side; we already skip some tests but those skips are unconditional; we could consider making some of those skips conditional (based on the \"use-snapshotter\" env-var; https:\/\/github.com\/moby\/moby\/blob\/dcf7287d647bcb515015e389df46ccf1e09855b7\/hack\/make\/test-docker-py#L15-L29"],"labels":["area\/testing","containerd-integration"]},{"title":"`ADD --checksum` doesn't work with windows builds","body":"### Description\r\n\r\nWhen building a windows image (`docker build`) from a `Dockerfile` containing `ADD --checksum`,\r\nthe build doesn't fail if the checksum is incorrect.\r\n\r\nSwitching to `linux` containers, `ADD --checksum` behaves as expected\r\n\r\n### Reproduce\r\n\r\n1. create a Dockerfile like so and build it\r\n```\r\nFROM mcr.microsoft.com\/dotnet\/framework\/sdk:4.8-windowsservercore-ltsc2019\r\nADD --checksum=c6bdf93f4b2de6dfa1a3a847e7c24ae10edf7f6318653d452cd4381415700ada https:\/\/www.python.org\/ftp\/python\/3.12.0\/python-3.12.0-amd64.exe python-installer.exe\r\n```\r\n\r\n2. now try again changing the content with the following\r\n```\r\nFROM mcr.microsoft.com\/dotnet\/framework\/sdk:4.8-windowsservercore-ltsc2019\r\nADD --checksum=IExpectedThisToBeCheckedAndFailButNothingFailsAtThisPoint https:\/\/www.python.org\/ftp\/python\/3.12.0\/python-3.12.0-amd64.exe python-installer.exe\r\n```\r\n\r\n### Expected behavior\r\n\r\n`docker build` should fail if the `--checksum` doesn't match\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35+desktop.5\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:32:48 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Desktop 4.24.2 (124339)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.24)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:39 2023\r\n  OS\/Arch:          windows\/amd64\r\n  Experimental:     true\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.5\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.22.0-desktop.2\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.8\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scan.exe\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.0.7\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 205\r\n Server Version: 24.0.6\r\n Storage Driver: windowsfilter\r\n  Windows:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent      \r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog Swarm: inactive\r\n Default Isolation: hyperv\r\n Kernel Version: 10.0 19045 (19041.1.amd64fre.vb_release.191206-1406)\r\n Operating System: Microsoft Windows Version 22H2 (OS Build 19045.3570)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.84GiB\r\n Name: FLuzzi\r\n ID: A7BQ:FSPJ:NFAO:6ZQF:C7SQ:ENLV:YPEV:U7TU:MMO7:GIPG:KCFJ:SR3A\r\n Docker Root Dir: C:\\ProgramData\\Docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["`--checksum` needs BuildKit, which is still not integrated to Docker for Windows.\r\n\r\nWe should print a warning to clarify that it is not supported though.","Yes, agreed; I thought we already did, but looks like we only do for some options, such as `RUN --mount`;\r\n\r\n```\r\nStep 2\/2 : RUN --mount=type=bind,dst=\/foo echo hello\r\nthe --mount option requires BuildKit. Refer to https:\/\/docs.docker.com\/go\/buildkit\/ to learn how to build images with BuildKit enabled\r\n```\r\n","Didn't know that, thanks!\r\nI'll open an issue to update the [docs](https:\/\/docs.docker.com\/engine\/reference\/builder\/#verifying-a-remote-file-checksum-add---checksumchecksum-http-src-dest) \ud83d\ude03 "],"labels":["platform\/windows","status\/0-triage","kind\/bug","area\/builder\/classic-builder"]},{"title":"DNM: PoC Always set a userns","body":"This is basically doing 2 things:\r\n\r\n1. Always put a container in a new user namespace\r\n2. Grant CAP_SYS_ADMIN by default\r\n\r\nThis makes it so all containers, unless specically requesting the host namespace with `--privileged` or `--userns=host`, are placed into a new user namespace. UIDs and GIDs are mapped 1:1, so uid 0 in the userns is uid 0 on the host.\r\n\r\nThe main thing this buys us is the containerized process no longer has privileges in the root userns which also allows us to safely(???) grant extra privileges to the container.\r\n\r\nSpecifically what's nice about this is now a containerized process can do bind and fuse mounts, and possibly overlay depending on which kernel you are running (ubuntu kernel allows unprivileged overlay mounts).\r\n\r\nThis *should* actually be more secure than allowing the container to run in the host namespace.\r\nEven if we don't grant CAP_SYS_ADMIN it may still be a good change because it removes all privilege from the root namespace.\r\n\r\n---\r\n\r\n\r\nAs noted in the test, this `mount` call still seems to be failing but *only* in the test environment (AFAICT). It seems to run fine for me if I just spin up a container and run the exact commands.\r\n","comments":["> Grant CAP_SYS_ADMIN by default\r\n\r\nI don't think we should do this, as the kernel had a bunch of vulns that can be trigged via `CAP_SYS_ADMIN` in UserNS.\r\n\r\n\r\n> Specifically what's nice about this is now a containerized process can do bind and fuse mounts, and possibly overlay depending on which kernel you are running (ubuntu kernel allows unprivileged overlay mounts).\r\n\r\nThis should probably be a new `security-opt`\r\n\r\n> ubuntu kernel allows unprivileged overlay mounts\r\n\r\nNot just Ubuntu, since kernel 5.11\r\n","A workaround to safely gain `CAP_SYS_ADMIN` in a container is to use UML (with the host kernel >= 4.8)\r\nhttps:\/\/github.com\/weber-software\/diuid"],"labels":["status\/1-design-review","kind\/enhancement","area\/security\/userns"]},{"title":"Swarm: Missing reverse DNS entry for virtual service ip in embedded DNS server causes software to hang until timeout when using internal networks only","body":"### Description\r\n\r\nIf a swarm service has an internal network, that service can not reach the internet. If such a service needs to reach a different service this usually happens via the service name which the embedded DNS resolver of the docker daemon resolves to a virtual IP.\r\n\r\nThis usually works but some software also does a reverse DNS lookup for that resolved virtual IP of the service name and the embedded DNS resolver of the docker daemon does not seem to know a reverse DNS name for the service virtual IP. Thus it tries to ask the docker daemon default DNS server 8.8.8.8 from within the context of the running task\/container. That request runs in a timeout because the internal network assigned to the service does not provide internet access. Thus the docker daemon can not reach 8.8.8.8.\r\n\r\nThe result is that such a software running as service hangs for several seconds.\r\n\r\nSome examples:\r\n- `ping <service name>`\r\n  - Immediately resolves <service name> to the virtual IP but then hangs because it tries to resolve the reverse DNS name for that virtual IP\r\n  - The time between individual pings is slow\r\n  - when using `ping -n <service name>` no reverse lookup is done and ping behaves fast and as expected\r\n- envoyproxy\r\n  - after starting envoy proxy with a cluster configuration of type `strict_dns` envoy does not accept connections for many seconds because reverse DNS lookup for the load balancer upstream DNS name fails (which is likely a different swarm service, e.g. some application server service)\r\n  - Assigning the envoy proxy swarm service an additional network with internet access, envoy behaves as expected\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n1. Initialize 1-node swarm cluster with default docker daemon DNS resolver 8.8.8.8\r\n2. `docker network create --internal --driver overlay missing-reverse-dns`\r\n3. `docker service create --network missing-reverse-dns --name network-multitool --replicas 2 wbitt\/network-multitool`\r\n4. Enter any task of that service: `docker exec -it <hash> \/bin\/bash`\r\n\r\nChecking A entry for service name:\r\n```\r\nnslookup -type=A network-multitool\r\n\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\tnetwork-multitool\r\nAddress: 10.0.2.2\r\n```\r\n\r\nChecking PTR entry for resolved VIP:\r\n```\r\nnslookup -type=PTR 10.0.2.2\r\n\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\n** server can't find 2.2.0.10.in-addr.arpa: SERVFAIL\r\n```\r\nNo PTR entry has been found for service name.\r\n\r\n```\r\nping network-multitool\r\n\r\nPING network-multitool (10.0.2.2) 56(84) bytes of data.\r\n\/\/ VIP has been resolved but output hangs now for some seconds because of missing PTR\r\n\/\/ After timeout ping lines now appear relatively slowly\r\n64 bytes from 10.0.2.2 (10.0.2.2): icmp_seq=1 ttl=64 time=0.047 ms\r\n```\r\n\r\nNow lets try a concrete service task\r\n```\r\nnslookup -type=A tasks.network-multitool\r\n\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\ttasks.network-multitool\r\nAddress: 10.0.2.4\r\nName:\ttasks.network-multitool\r\nAddress: 10.0.2.3\r\n```\r\nand PTR\r\n```\r\nnslookup -type=PTR 10.0.2.4\r\n\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\n4.2.0.10.in-addr.arpa\tname = network-multitool.2.rqf4ihe1ohdp0lzz7kyig9y15.missing-reverse-dns.\r\n\r\nAuthoritative answers can be found from:\r\n```\r\nSo a concrete task of the service does have a PTR entry. \r\n\r\nPinging IP `10.0.2.4` does not hang and works as expected.\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nThe embedded DNS server used in docker swarm services should provide a reverse DNS entry (PTR) for service names. Currently it only provides a PTR entry for service tasks.\r\n\r\nNot doing so results in potential lagging software if the swarm service uses internal networks only because no DNS server on the internet can be reached (by default 8.8.8.8).\r\n\r\n### docker version\r\n\r\nOutput of Docker Desktop for Mac. But issue also appears on traditional linux server installations, e.g. Ubuntu.\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35+desktop.4\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:28:49 2023\r\n OS\/Arch:           darwin\/amd64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.23.0 (120376)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:16 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\nOutput of Docker Desktop for Mac. But issue also appears on traditional linux server installations, e.g. Ubuntu.\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.6\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.4\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0-desktop.1\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.7\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.24.1\r\n    Path:     \/Users\/<USERNAME>\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 9\r\n  Running: 9\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 25\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: u2tn3lopfggk5xdo9cmmj8p38\r\n  Is Manager: true\r\n  ClusterID: m65bbme1vfi7pjve6u4ezm6z0\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.65.3\r\n  Manager Addresses:\r\n   192.168.65.3:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.3.13-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 7.671GiB\r\n Name: docker-desktop\r\n ID: 0429cc62-1032-4d49-9a2a-4d350452a617\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["By analysing how service bindings are established in libnetwork, I found out that PTR records are intentionally not created for service vip's, but I could not find out why.\r\n\r\nBy changing a boolean flag passed to one of the addSvcRecords calls, we can add this PTR record. The API for creating PTRs is already there, so the only thing I did was changing this boolean flag.\r\nSince I'm quite new to the libnetwork module, I have not yet understood fully how all the components work together, but I could test (via unittest and manually), that after this change the record seems to be there, i.e. `host vip` yielded an A record for <svcname>.<networkname>.\r\nSince the service record is only created once for every service, we will not end up with multiple PTR records for this vip.\r\n\r\nI could open a PR for this simple change + test and let someone check on it, but I wanted to discuss this approach before doing so @thaJeztah. Any ideas if this might be the way to go?\r\n\r\n\r\nA more detailed explanation of what I found out about the implementation:\r\nI studied this issue for a while and it seems that when a container is bound to a service via the network controllers `addServiceBinding` method, the method creates a new service if none is available in the controller's `serviceBindings` map. Then the controller's method `addEndpointNameResolution` is called with a flag indicating if a service was created or not. \r\nThe addEndpointNameResolution method mainly takes care of creating the 'tasks.serviceName' records, that jnehlmeier pointed out and also the A records for the single tasks.\r\nIf a service was previously created then an A service record is created for the service's vip as well, but this method explicitly opts out of creating a reverse mapping for this service by passing `ipMapUpdate` = false to the network's `addSvcRecords` method.\r\nAll this passing of flags to change behaviour of the methods is a little confusing, but if we simply pass ipMapUpdate = true for the A service record for the service's vip, then the reverse lookup is successfull.","@devchris123 I am curious: how does the `PTR` look like for a vip if you flip the boolean? I assume it is `<service-name>.<network-name>` similar to `PTR` entries of service tasks, given that multiple networks can be assigned to a service.\r\n\r\nAlso a service already has an `A` entry for each `<service-name>.<network-name>`, so it would be a consistent `PTR <-> A` tuple for each network of a service.\r\n\r\n```shell\r\n# no network specified. Not sure which rules apply to select the IP below\r\nnslookup -type=A network-multitool\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\tnetwork-multitool\r\nAddress: 10.0.2.2\r\n\r\n\r\n# via first network\r\nnslookup -type=A network-multitool.missing-reverse-dns\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\tnetwork-multitool.missing-reverse-dns\r\nAddress: 10.0.2.2\r\n\r\n\r\n# via second network\r\nnslookup -type=A network-multitool.missing-reverse-dns-2\r\nServer:\t\t127.0.0.11\r\nAddress:\t127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:\tnetwork-multitool.missing-reverse-dns-2\r\nAddress: 10.0.3.2\r\n```\r\n","@jnehlmeier Indeed the PTR record for your above example is network-multitool.missing-reverse-dns after exercising the change.\r\n\r\nAbout your question regarding multiple networks: \r\nI dived into the resolver call stack and what seems to happen is that when resolving the IP for a given service name it loops over all endpoints and checks endpoint's network name against the network name being looked up, the one we pass over via the dot notation, and if the endpoint's network name does not match our network it continues to loop over the endpoints until the desired network is found. Then it looks up and returns the ip.\r\n\r\nIf no network is given then this check is not being done and suprisingly (or to be expected?) the implementation just returns the first ip found for the first network instead of passing all the available ips. (This is still a list but I think that there is only one ip per endpoint expected, but it is a list for implementation reasons)\r\n\r\nI played around a bit and made a list of ips found for every endpoint. Then I recompiled and get the following output:\r\n\r\nhost network-multitool\r\nnetwork-multitool has address 10.0.1.2\r\nnetwork-multitool has address 10.0.2.2\r\n\r\nor with nslookup network-multitool\r\nServer:         127.0.0.11\r\nAddress:        127.0.0.11#53\r\n\r\nNon-authoritative answer:\r\nName:   network-multitool\r\nAddress: 10.0.2.2\r\nName:   network-multitool\r\nAddress: 10.0.1.2\r\n\r\nThis is when the service is initally created with the --network parameter for two networks: network-multitool.missing-reverse-dns and network-multitool.missing-reverse-dns-2"],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/dns","version\/24.0"]},{"title":"Docker logs crashes every couple of hours","body":"### Description\n\nEvery couple of hours docker compose logs crashes on my production server. In the system logs (\/var\/log\/docker) I find something like this:\r\n`error unmarshalling log entry (size=108554): proto: LogEntry: illegal tag 0 (wire type 6)`\r\n\r\nThis error is then followed by several errors that the log message is too large:\r\n`Error streaming logs: log message is too large (1937007727 > 1000000)`\r\n\r\nNote that the too large log message in this case is almost 2GB!\r\n\r\nManually running something like this also triggers the log message is too large error:\r\n`sudo docker logs abc123 --since \"2023-10-16T18:49:10\" --until \"2023-10-16T18:50:00\"`\r\n\r\n### The environment\r\nI am encountering this issue on two different production environments, both running on AWS EC2 instances:\r\n\r\n-  A Kong API gateway, which is an application built on top of nginx. It is third party software, we did not write code for this\r\n- Our own node.js application, that runs in a container but has a separate nginx container running next to it\r\n\r\nBoth seem to fail on the access logs. The kong gateway crashes every couple of hours, while the environment running our nodejs application crashes about once a week. We use the docker awslog driver to send the logs to cloudwatch. Cloudwatch seems to receive all logs, while manually running docker logs seems to miss up to multiple hours of logs every time after the error occurs.\r\n\r\n### What I have tried\r\n\r\n- I have looked into the cloudwatch logs for 'weird' log entries at the time of the error, and I have particularly looked for big log entries or weird characters but could not find any.\r\n- I ran docker logs manually in an open ssh session, which results in the same error as reported in the system logs\r\n- I have downloaded the cached log files and went through them using less, tail and other tools. Doing this I found out that the cached logs misses entries that do exist in Cloudwatch\r\n- I've tried copying these logs and make them available in a dummy container, so I have a more 'stable' environment to figure out the bug, but I cant get it to work\n\n### Reproduce\n\nI have tried to get a reproducible situation, but I am unable to do so. I cannot pinpoint the log that crashes the system, nor can I import cached logs in another container. But this is my situation:\r\n\r\n1. Run an application with nginx (this is quite an assumption)\r\n2. Log a lot of access logs\r\n3. Have the application crash\r\n\n\n### Expected behavior\n\nDocker logs should not crash\n\n### docker version\n\n```bash\nClient:\r\n Version:           20.10.23\r\n API version:       1.41\r\n Go version:        go1.18.9\r\n Git commit:        7155243\r\n Built:             Tue Apr 11 22:56:36 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.18.9\r\n  Git commit:       6051f14\r\n  Built:            Tue Apr 11 22:57:17 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.19\r\n  GitCommit:        1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., 0.0.0+unknown)\r\n  compose: Docker Compose (Docker Inc., v2.22.0)\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 3\r\n Server Version: 20.10.23\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc version: f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 4.14.326-245.539.amzn2.x86_64\r\n Operating System: Amazon Linux 2\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.781GiB\r\n Name: ip-172-31-17-252.eu-west-1.compute.internal\r\n ID: 5ELM:KYZM:7AEB:RH2I:FFL6:G2SA:4OOR:SGWE:6JOC:NXBS:U7HA:HA3Y\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nI am aware I have not much info to go on, since I am not capable of providing reproducible steps. I can however access the production environment and execute debugging steps if this helps.\r\n\r\n_Note: I am sent here after creating an issue in the the docker cli repo. (https:\/\/github.com\/docker\/cli\/issues\/4617). The commenter over there suggests this version of docker might be maintained by AWS. I did indeed make a ticket with them, their (business tier) support however remains unresponsive._","comments":["At the other issue I was asked if there is anything that could be interfering with the log files. Up until this error we left the environment itself very clean: we use AWS beanstalk to deploy the instances. After the crashes started to occur, we have made some customizations to the environment with the most crashes, such as configuring a cloudwatch agent. We only did this after issues occurred with the environments in question, so I must assume any conflicting tooling is a default of AWS.","I took a quick look and Amazon is applying some patches to Docker that change the log behavior.  In particular, their patch `docker-20.10.4-Limit-logger-errors-logged-into-daemon-logs.patch` changes the behavior of the ring buffer and appears to attempt to log the entire failed message (2GB??) into the daemon log.\r\n\r\n\/cc @stewartsmith\r\n\r\n<details>\r\n<summary>How to see patches in Amazon's SRPM<\/summary>\r\n\r\nDo this inside an `amazonlinux:2` container\r\n\r\n```\r\nbash-4.2# yum install yum-utils -y\r\n...\r\nbash-4.2# amazon-linux-extras enable docker\r\n...\r\nbash-4.2# yum --showduplicates search docker | grep 20.10.23\r\ndocker-20.10.23-1.amzn2.0.1.x86_64 : Automates deployment of containerized\r\nbash-4.2# yumdownloader --source docker-20.10.23-1.amzn2.0.1.x86_64\r\n...\r\nbash-4.2# rpm2cpio docker-20.10.23-1.amzn2.0.1.src.rpm > docker.src.cpio\r\nbash-4.2# mkdir docker\r\nbash-4.2# cpio -D docker -dimv < docker.src.cpio\r\n...\r\nbash-4.2#  cat docker\/docker.spec | grep ^Patch\r\nPatch2001:      docker-20.10.4-sysvinit-use-nohup.patch\r\nPatch2002:      docker-20.10.4-sysvinit-add-storage-opts.patch\r\nPatch2004:      docker-20.10.4-sysvinit-increase-daemon-maxfiles.patch\r\nPatch2007:      docker-20.10.4-sysvinit-stop-before-network.patch\r\nPatch2010:      docker-20.10.4-sysvinit-configurable-start-timeout.patch\r\nPatch2103:      docker-20.10.4-Skip-devmapper-tests-that-don-t-work-in-a-buildroot.patch\r\nPatch2104:      docker-20.10.4-Skip-mutating-vfs-tests.patch\r\nPatch2108:      docker-20.10.4-Skip-pkg-sysinfo-tests-that-require-root.patch\r\nPatch2109:      docker-20.10.4-Skip-mutating-volume-local-tests.patch\r\nPatch2113:      docker-20.10.4-Skip-pkg-authorization-tests-that-create-sockets.patch\r\nPatch2114:      docker-20.10.4-Skip-pkg-idtools-tests-that-require-root.patch\r\nPatch2115:      docker-20.10.4-Skip-distribution-tests-that-network.patch\r\nPatch2117:      docker-20.10.4-Skip-builder-tests-that-require-root.patch\r\nPatch2118:      docker-20.10.7-Skip-overlay-tar-untar-test.patch\r\nPatch2120:      no-private-mnt-namespace.patch\r\nPatch2123:      runc-allow-git-sha-override.patch\r\nPatch2125:      tini-allow-git-sha-override.patch\r\nPatch2126:      docker-20.10.4-Skip-cli-tests-that-require-network.patch\r\nPatch2130:      docker-20.10.4-Skip-auth-middleware-test.patch\r\nPatch2133:      docker-20.10.4-Skip-quota-tests.patch\r\nPatch2135:      docker-20.10.4-Skip-engine-layer-tests.patch\r\nPatch2140:      docker-20.10.4-Skip-volume-store-tests.patch\r\nPatch2141:      docker-20.10.4-systemd-unit-sysconfig.patch\r\nPatch2143:     docker-20.10.4-Skip-get-source-mount.patch\r\nPatch2144:     docker-20.10.4-systemd-unit-runtimes.d.patch\r\nPatch2145:      docker-20.10.4-Skip-git-tests.patch\r\nPatch2146:      docker-20.10.4-Skip-compression-tests-that-require-root.patch\r\nPatch2148:      docker-20.10.4-Skip-btrfs-graphdriver-tests-if-not-root.patch\r\nPatch2149:      docker-20.10.4-Restore-containerd-dependency-restart-policy-and-nof.patch\r\nPatch3005:      docker-20.10.7-Skip-pkg-archive-tests-that-require-root.patch\r\nPatch3008:      docker-20.10.7-Add-test-skip-helpers-to-testutil.patch\r\nPatch3009:      docker-20.10.7-Add-test-skip-helpers-to-cli.patch\r\nPatch3013:\tdocker-20.10.4-Limit-logger-errors-logged-into-daemon-logs.patch\r\nPatch3014:      docker-20.10.17-Skip-kubeconfig-and-loadcontext-tests-that-require-root.patch\r\n```\r\n\r\n<\/details>","I've executed the same commands on my instance and indeed the Limit-logger-errors-logged-into-daemon-logs.patch is there (see below).\r\n\r\nRegarding the 2GB log message, my underbelly feeling is that it is the result of a concatenation of many access log messages which get combined after a serialization error. This one to be precise: ` error unmarshalling log entry (size=108554): proto: LogEntry: illegal tag 0 (wire type 6)`. This might explain the missing couple of hours in the daemon logs that do exist in cloudwatch. But to be honest, I am out of my depth here.\r\n\r\n```\r\n[ec2-user@... ~]$ cpio -D docker -dimv < docker.src.cpio\r\nREADME-docker-runtimes.d\r\ncli-20.10.23.tar.gz\r\ndocker-20.10.17-Skip-kubeconfig-and-loadcontext-tests-that-require-root.patch\r\ndocker-20.10.23.tar.gz\r\ndocker-20.10.4-Limit-logger-errors-logged-into-daemon-logs.patch\r\ndocker-20.10.4-Restore-containerd-dependency-restart-policy-and-nof.patch\r\ndocker-20.10.4-Skip-auth-middleware-test.patch\r\ndocker-20.10.4-Skip-btrfs-graphdriver-tests-if-not-root.patch\r\ndocker-20.10.4-Skip-builder-tests-that-require-root.patch\r\ndocker-20.10.4-Skip-cli-tests-that-require-network.patch\r\ndocker-20.10.4-Skip-compression-tests-that-require-root.patch\r\ndocker-20.10.4-Skip-devmapper-tests-that-don-t-work-in-a-buildroot.patch\r\ndocker-20.10.4-Skip-distribution-tests-that-network.patch\r\ndocker-20.10.4-Skip-engine-layer-tests.patch\r\ndocker-20.10.4-Skip-get-source-mount.patch\r\ndocker-20.10.4-Skip-git-tests.patch\r\ndocker-20.10.4-Skip-mutating-vfs-tests.patch\r\ndocker-20.10.4-Skip-mutating-volume-local-tests.patch\r\ndocker-20.10.4-Skip-pkg-authorization-tests-that-create-sockets.patch\r\ndocker-20.10.4-Skip-pkg-idtools-tests-that-require-root.patch\r\ndocker-20.10.4-Skip-pkg-sysinfo-tests-that-require-root.patch\r\ndocker-20.10.4-Skip-quota-tests.patch\r\ndocker-20.10.4-Skip-volume-store-tests.patch\r\ndocker-20.10.4-systemd-unit-runtimes.d.patch\r\ndocker-20.10.4-systemd-unit-sysconfig.patch\r\ndocker-20.10.4-sysvinit-add-storage-opts.patch\r\ndocker-20.10.4-sysvinit-configurable-start-timeout.patch\r\ndocker-20.10.4-sysvinit-increase-daemon-maxfiles.patch\r\ndocker-20.10.4-sysvinit-stop-before-network.patch\r\ndocker-20.10.4-sysvinit-use-nohup.patch\r\ndocker-20.10.7-Add-test-skip-helpers-to-cli.patch\r\ndocker-20.10.7-Add-test-skip-helpers-to-testutil.patch\r\ndocker-20.10.7-Skip-overlay-tar-untar-test.patch\r\ndocker-20.10.7-Skip-pkg-archive-tests-that-require-root.patch\r\ndocker-setup-runtimes.sh\r\ndocker-storage.sysconfig\r\ndocker.spec\r\ndocker.sysconfig\r\ngo-md2man-2.0.1.tar.gz\r\nlibnetwork-05b93e0.tar.gz\r\nno-private-mnt-namespace.patch\r\nrunc-allow-git-sha-override.patch\r\ntini-allow-git-sha-override.patch\r\ntini-de40ad0.tar.gz\r\nv0.10.4.tar.gz\r\n128367 blocks\r\n```","I am seeing a similar problem and my vm is not in AWS:\r\n`error from daemon in stream: Error grabbing logs: log message is too large (168194932 > 1000000)`\r\n\r\nI'm using local logging driver and have rotation configured a certain way. In the container directory I have several files with the rotated logs but executing docker logs on the affected containers produce a huge output that suddely ends with the error message and logs are truncated (latest logs are lost).\r\n\r\n\r\nversion:\r\n```\r\nxxx@imm-gd-1xi8afbouye44-ins:~$ docker -v\r\nDocker version 24.0.5, build ced0996\r\n```\r\n\r\nError:\r\n```\r\nxxx@imm-gd-1xi8afbouye44-ins:~$ docker logs 38b3a20c1812\r\n...\r\n...\r\n...\r\nerror from daemon in stream: Error grabbing logs: log message is too large (108229732 > 1000000)\r\n```\r\n\r\nContainer logs dir:\r\n```\r\nxxx@imm-gd-1xi8afbouye44-ins:~$ sudo ls \/var\/lib\/docker\/containers\/38b3a20c18122a8585a79cd3ef11b0b68a7e4481a3bc2f1d755946ab930c92a0\/local-logs\r\ncontainer.log  container.log.1.gz  container.log.2.gz  container.log.3.gz  container.log.4.gz  container.log.5.gz  container.log.6.gz  container.log.7.gz\r\n```\r\n\r\nDocker logging conf:\r\n```\r\nxxx@imm-gd-1xi8afbouye44-ins:~$ sudo cat \/etc\/docker\/daemon.json \r\n{\r\n\"log-driver\": \"local\",\r\n\"log-opts\": {\r\n  \"compress\": \"true\",\r\n  \"max-size\": \"20m\",\r\n  \"max-file\": \"8\"\r\n},\r\n\"iptables\": false,\r\n\"bridge\": \"none\"\r\n}\r\n```\r\n","@lbergesio have you checked `\/var\/log\/docker` as well? I think that is the default place for docker errors to be logged, and for me the errors always start with something along the lines of `Error streaming logs: error unmarshalling log entry (size=1)`, and then a bunch of the 'too large' errors like you have as well.","> @lbergesio have you checked `\/var\/log\/docker` as well? I think that is the default place for docker errors to be logged, and for me the errors always start with something along the lines of `Error streaming logs: error unmarshalling log entry (size=1)`, and then a bunch of the 'too large' errors like you have as well.\r\n\r\nI don't have`\/var\/log\/docker`, I `grep` for \"too large\" in `\/var\/log` and had nothing. I remember seeing errors with \"unmarshalling\" when I was using the json log driver, but not with local.","@wouthoekstra but I do have `sudo journalctl -fu docker.service` (mind this is for a different container, but same issue):\r\n\r\n```\r\nOct 26 23:20:05 imm-gd-1xi8afbouye44-ins dockerd[1227]: time=\"2023-10-26T23:20:05.515920863Z\" level=error msg=\"Error streaming logs: log message is too large (1952739189 > 1000000)\" container=d7f8c9792ea361fcd09d65edac9ac24e99d5bc63654d7457335c77623900e336 method=\"(*Daemon).ContainerLogs\" module=daemon\r\nOct 26 23:20:16 imm-gd-1xi8afbouye44-ins dockerd[1227]: time=\"2023-10-26T23:20:16.009040105Z\" level=error msg=\"Error streaming logs: log message is too large (1952739189 > 1000000)\" container=d7f8c9792ea361fcd09d65edac9ac24e99d5bc63654d7457335c77623900e336 method=\"(*Daemon).ContainerLogs\" module=daemon\r\nOct 27 10:43:33 imm-gd-1xi8afbouye44-ins dockerd[1227]: time=\"2023-10-27T10:43:33.165421282Z\" level=error msg=\"Error streaming logs: log message is too large (1952739189 > 1000000)\" container=d7f8c9792ea361fcd09d65edac9ac24e99d5bc63654d7457335c77623900e336 method=\"(*Daemon).ContainerLogs\" module=daemon\r\n```","Similar issue here. \r\nThe app is a Ruby on Rails web app configured via a docker-compose.yml, and uses the `awslogs` driver to log to Cloudwatch. It's deployed on Elastic Beanstalk.\r\n\r\nThe issue only started occurring after upgrading the Beanstalk image from `Docker running on 64bit Amazon Linux 2\/3.5.9` to  `Docker running on 64bit Amazon Linux 2\/3.6.x`\r\nThe biggest change I see between these versions is the update to `docker compose V2`.\r\nI'm working around it by downgrading back to 3.5.9.\r\n\r\nI've tried disabling colorized logs as I could see some unexpected Unicode characters in the `\/var\/lib\/docker\/containers\/<container id>\/container-cached.log` file, which didn't resolve the issue.\r\n\r\nRelevant configs:\r\nOn the failing instance:\r\n```\r\n> docker-compose --version\r\nDocker Compose version v2.23.0\r\n\r\n> docker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., v0.0.0+unknown)\r\n  compose: Docker Compose (Docker Inc., v2.23.0)\r\n\r\nServer:\r\n Server Version: 20.10.25\r\n Logging Driver: json-file\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Plugins:\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Default Runtime: runc\r\n containerd version: 1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc version: f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n Kernel Version: 4.14.326-245.539.amzn2.x86_64\r\n Operating System: Amazon Linux 2\r\n Labels:\r\n Experimental: false\r\n```\r\n\r\nWorking instance:\r\n\r\n```\r\n> docker-compose --version\r\ndocker-compose version 1.29.2, build unknown\r\n\r\n> docker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., 0.0.0+unknown)\r\n\r\nServer:\r\n Server Version: 20.10.23\r\n Logging Driver: json-file\r\n Plugins:\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n containerd version: 1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc version: f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n Kernel Version: 4.14.318-240.529.amzn2.x86_64\r\n Operating System: Amazon Linux 2\r\n Labels:\r\n Experimental: false\r\n```\r\n\r\n### Update\r\n- looks like the `docker-20.10.4-Limit-logger-errors-logged-into-daemon-logs.patch` patch is installed in both the working and failing instance \r\n- issue occurs when a large number of logs are being sent to STDOUT\r\n- `error from daemon in stream: Error grabbing logs: error unmarshalling log entry (size=73482): proto: LogEntry: illegal tag 0 (wire type 6)` log entry seems to coincide with the issue.\r\n- `Error streaming logs: log message is too large` is also present in the logs when the log service crashes ","To sum up, there are two factors that are making this issue so confusing.\r\n1. For environments that are running on AWS Elastic Beanstalk, `Docker running on 64bit Amazon Linux 2\/3.5.9` is using `docker-compose version 1` which was written in Python so it won't bubble up the `docker logs` issue to crash `docker compose logs` service. But for `Docker running on 64bit Amazon Linux 2\/3.6.x` which is using `docker-compose version 2`, it catches the log streamming error from `docker logs` and crashes. Therefore, the issue is not introduced by docker compose V2 but existed before and then exposed by docker compose V2.\r\n\r\n2. Amazon Linux has a `docker-20.10.4-Limit-logger-errors-logged-into-daemon-logs.patch` to lower the rate of writing log driver issue which may ultimately cause the `log message too large` when the buffer is full.\r\n\r\nBut the problem here is why we would get `error unmarshalling log entry (size=73482): proto: LogEntry: illegal tag 0 (wire type 6)`? Are there something in our applications breaking the protocol buffer serialization?","@PeterCai7 would it help if we removed the patch from our most problematic instance? On a particular instance compose logs crashes within a couple of hours most of the time. If it does not within a day after removing the patch, we have a stronger case in blaming the patch.\r\n\r\nI would need some instructions about removing the patch if you desire this, either through this issue or the case I created with AWS support.","@wouthoekstra \r\nIf you would like to test an Amazon Linux Docker version(with this patch removed), you would have to rebuild the SRPM file and replace the original `docker-20.10.23-1.amzn2.0.1.src.rpm` with your new built one. Then build RPM binaries with this new SRPM. Uninstall old docker version and install with new RPM files. Overall, it takes sort of extra efforts. But given that this issue is also occuring in non-AL instance. I highly doubt if this is an efficient approach of digging in.\r\n\r\n","Good point. I was not aware of how much work this was going to be, and I rather not do all of that on our production environment. I'll just await your progress.","@wouthoekstra \r\n\r\nAfter taking a further look, my theory is that this issue could be caused by unfitted encode\/decode mechanism between `awslog` log driver and `local` log driver. As you mentioned, you configured `awslog` driver in your application and the log messages would be processed by [`cloudwatchlogs.go`](https:\/\/github.com\/moby\/moby\/blob\/master\/daemon\/logger\/awslogs\/cloudwatchlogs.go) which works fine and all your logs are available in cloudwatch. However, since `awslog` does not provide `read` method for local logs reading, the [dual logging](https:\/\/docs.docker.com\/config\/containers\/logging\/dual-logging\/) would be kicked off by default. Your log entries would be cached to local machine and docker daemon would invoke `local` logdriver to read when you hit `docker log`(and when beanstalk execute `docker compose log` as well). As we can see in `awslog`, this log driver does not encode log entries with protocol buffer standard defined in [moby](https:\/\/github.com\/moby\/moby\/blob\/master\/api\/types\/plugins\/logdriver\/io.go) but the `local` driver would read local cached logs with the protocol buffer standard. This could be the cause of `error unmarshalling log entry (size=73482): proto: LogEntry: illegal tag 0 (wire type 6)` and then this error could occur repeatedly and get stacked until the rate limit configured by Amazon Linux is reached then we see the other error  `error from daemon in stream: Error grabbing logs: log message is too large`.\r\n\r\nCould you try the following steps as a walkaround?\r\n1) Switch log driver from `awslog` to `local`, this would help making sure the log encoding compliant with protocol buffer standard.\r\n2) Turn on `logstreaming` in [Elastic Beanstalk](https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/dg\/AWSHowTo.cloudwatchlogs.html), this would help uploading local logs to cloudwatch(CW) through beanstalk controlled path. And we should be able to see them in CW under `\/aws\/elasticbeanstalk\/environment_name\/var\/log\/eb-docker\/containers\/eb-current-app\/stdouterr.log`.\r\n\r\nIf this issue is gone after this walkaround, it would be a support of my hypothesis and we would be more clear on what is our next step.\r\n\r\n\r\n","@PeterCai7 I switched the driver from `awslog` to `local`. It is running for two hours now without issues, which seems promising.\r\n\r\nI've enabled log streaming for the instance, and i've set the required permissions to do so. The only logs that make it into cloudwatch currently are:\r\n`\/aws\/elasticbeanstalk\/<<instance>>\/environment-health.log`\r\n\r\nI am missing my access logs and error logs. That's no issue for a short period, and I guess it is not an issue for debugging this issue either? I did ssh into the machine just now and manually ran docker logs, and the logs are all there.\r\n\r\nI will report back tomorrow and let you know if the instance remained healthy overnight.","About twenty hours later, docker compose log is still running and the instance is still healthy.","@PeterCai7 my case above I was already using the local log driver and my vm was not running on aws","@wouthoekstra According to the documentation, there should be a CloudWatch log group named ` \/aws\/elasticbeanstalk\/{environment_name}`(environment name, not the instance name) if you configured everything right. But this is the different issue if you cannot find the log group in CW. At this moment, we probably just make sure the log entry error does not occur any more after you switched to `local` driver.","@lbergesio Yes, your case could be different. I see you mentioned there was no `Error streaming logs: error unmarshalling log entry (size=1)` issue in your case. I believe things went well for log reading\/decoding. The `log message is too large` may  related to your rotation config. Are you using Amazon Linux series of OS? Or you are using other Linux distributions since you mentioned your VM is not running on AWS. Just trying to clarify before we dive deep.","> Error streaming logs\r\n\r\nI dont have access to the vm anymore :(. But it is an ubuntu 20.04 based image running on IBM cloud.This is my `daemon.json` with the log config:\r\n\r\n```\r\n{                        \r\n\"log-driver\": \"local\",   \r\n\"log-opts\": {            \r\n  \"compress\": \"true\",    \r\n  \"max-size\": \"20m\",     \r\n  \"max-file\": \"8\"        \r\n},                       \r\n\"iptables\": false,       \r\n\"bridge\": \"none\"         \r\n}                        \r\n```\r\n\r\nAlso, although I think it is unrelated, I am filtering out some docker logs with this in `\/etc\/rsyslog.d\/01-blocklist.conf`:\r\n\r\n```\r\nif $msg contains \"run-docker-runtime\" and $msg contains \".mount: Succeeded.\" then {\r\n    stop                                                                        \r\n}                                                                               \r\n```","@PeterCai7 Is there any progress on this issue? I can understand if the issue is hard to solve, but if there is no solution in sight I can discuss with my team about moving several of our servers away from awslog for now.","Can someone provide a sample of the logs in question? Log files are in `\/var\/lib\/docker\/containers\/<id>\/local-logs`\r\nI understand this can be sensitive information and may be difficult\/impossible to provide.\r\n\r\nI am trying to find a way to reproduce the issue in the meantime.","@cpuguy83 I can send you log files. I've just downloaded some recent logs, and they do look weird. less considers it a binary file. I see you have an email address in your github profile, should I send it to you over there?","> @PeterCai7 Is there any progress on this issue? I can understand if the issue is hard to solve, but if there is no solution in sight I can discuss with my team about moving several of our servers away from awslog for now.\r\n\r\nYes, the beanstalk team did not develop the `awslog` driver in Docker repo here. Therefore, I think the first thing we need to figure out is shall we make changes to `awslog` for follwing encoding mechanisims as what \"local\" driver is doing. Otherwise, I would say putting servers with 'local' driver has better integration with Docker platform of beanstalk.","> Can someone provide a sample of the logs in question? Log files are in `\/var\/lib\/docker\/containers\/<id>\/local-logs` I understand this can be sensitive information and may be difficult\/impossible to provide.\r\n> \r\n> I am trying to find a way to reproduce the issue in the meantime.\r\n\r\n@cpuguy83 in the (possibly related) IoT Edge issue, we provided some logs \r\n\r\nhttps:\/\/github.com\/Azure\/iotedge\/issues\/7074#issuecomment-1907253928\r\n\r\nEDIT: also tagging https:\/\/github.com\/Azure\/iotedge\/issues\/7177 as possibly related","Thanks @jlian \r\n\r\nI was referring to the log files of the container.\r\n\r\n@wouthoekstra \r\n\r\nSure you can send it there.\r\n","I've been away for a couple of days, and I just wanted to make a fresh export of logs for @cpuguy83, when I noticed all my environments at AWS are reported to be healthy. Has there been an update?","Sadly no, I don't think there has been any changes to this logging code.","That is odd. Nothing changed on our side as far as I am aware off. And we had several environments with completely different tech stacks that were impacted by this issue, all of them currently reporting to be healthy. Nonetheless, I've send you docker container logs from one of the environments from when we did have issues.","@wouthoekstra\r\nDid you mean nothing changed from your side but `awslog` driver started working for your environments and reporing healthy?","@PeterCai7 Yes that is correct. We did not update our tech stacks, nor did we change our logging mechanism. We do have automatic updates on our AWS servers, and our environments are all at \"Docker running on 64bit Amazon Linux 2\/3.7.1\" now."],"labels":["area\/logging","status\/0-triage","kind\/bug","version\/20.10"]},{"title":"Docker won't start.","body":"### Description\r\n\r\nHi,\r\n\r\nI have small Debian virtual machine(on VMware ESXi) with docker and in it I have few containers. Basic stuff really as I'm learning how to use Docker and containers. I was not using this VM for few months, and today, when I wanted to do more testing on it, I found out that docker deamon is not starting. I tried to start it via systemctl and it failed.\r\n\r\nI tried to do upgrade of all linux packages(apt update && apt upgrade) as I thought it might be some known bug that was fixed in newer versions. Even after this upgrade and VM restart docker is still not starting up.\r\n\r\nHere is what I get when I try to run dockerd as root:\r\n```\r\nroot@docker:~# dockerd -D\r\nINFO[2023-10-20T19:24:35.925590878+02:00] Starting up\r\nDEBU[2023-10-20T19:24:35.925955782+02:00] Listener created for HTTP on unix (\/var\/run\/docker.sock)\r\nDEBU[2023-10-20T19:24:35.926543490+02:00] Golang's threads limit set to 13860\r\nDEBU[2023-10-20T19:24:35.926894337+02:00] metrics API listening on \/var\/run\/docker\/metrics.sock\r\nDEBU[2023-10-20T19:24:35.929172038+02:00] Using default logging driver json-file\r\nDEBU[2023-10-20T19:24:35.929471417+02:00] processing event stream                       module=libcontainerd namespace=plugins.moby\r\nDEBU[2023-10-20T19:24:35.929764625+02:00] No quota support for local volumes in \/var\/lib\/docker\/volumes: Filesystem does not support, or has not enabled quotas\r\nDEBU[2023-10-20T19:24:35.930740049+02:00] [graphdriver] priority list: [overlay2 fuse-overlayfs btrfs zfs devicemapper vfs]\r\nDEBU[2023-10-20T19:24:35.933088295+02:00] successfully detected metacopy status         storage-driver=overlay2 usingMetacopy=false\r\nDEBU[2023-10-20T19:24:35.934324796+02:00] backingFs=extfs, projectQuotaSupported=false, usingMetacopy=false, indexOff=\"index=off,\", userxattr=\"\"  storage-driver=overlay2\r\nINFO[2023-10-20T19:24:35.934343952+02:00] [graphdriver] using prior storage driver: overlay2\r\nDEBU[2023-10-20T19:24:35.934353370+02:00] Initialized graph driver overlay2\r\nDEBU[2023-10-20T19:24:35.937311776+02:00] Max Concurrent Downloads: 3\r\nDEBU[2023-10-20T19:24:35.937334268+02:00] Max Concurrent Uploads: 5\r\nDEBU[2023-10-20T19:24:35.937338637+02:00] Max Download Attempts: 5\r\nINFO[2023-10-20T19:24:35.937354186+02:00] Loading containers: start.\r\nDEBU[2023-10-20T19:24:35.937671249+02:00] processing event stream                       module=libcontainerd namespace=moby\r\nDEBU[2023-10-20T19:24:35.938464768+02:00] loaded container                              container=5c3fb96d20ca739cd430655d16f9c9aabd5ec349afb9f218c08561c61c2866eb paused=false running=false\r\nDEBU[2023-10-20T19:24:35.938596138+02:00] loaded container                              container=0b9d85dae40f26b0f388deed4ad2a06001595b95e340e09539d99a68ba531af1 paused=false running=false\r\nDEBU[2023-10-20T19:24:35.942886086+02:00] restoring container                           container=5c3fb96d20ca739cd430655d16f9c9aabd5ec349afb9f218c08561c61c2866eb paused=false restarting=false running=false\r\nDEBU[2023-10-20T19:24:35.943010633+02:00] restoring container                           container=0b9d85dae40f26b0f388deed4ad2a06001595b95e340e09539d99a68ba531af1 paused=false restarting=false running=false\r\nDEBU[2023-10-20T19:24:35.943751752+02:00] done restoring container                      container=0b9d85dae40f26b0f388deed4ad2a06001595b95e340e09539d99a68ba531af1 paused=false restarting=false running=false\r\nDEBU[2023-10-20T19:24:35.943764787+02:00] done restoring container                      container=5c3fb96d20ca739cd430655d16f9c9aabd5ec349afb9f218c08561c61c2866eb paused=false restarting=false running=false\r\nDEBU[2023-10-20T19:24:35.944077171+02:00] Option DefaultDriver: bridge\r\nDEBU[2023-10-20T19:24:35.944102920+02:00] Option DefaultNetwork: bridge\r\nDEBU[2023-10-20T19:24:35.944114702+02:00] Network Control Plane MTU: 1500\r\nDEBU[2023-10-20T19:24:35.947574691+02:00] \/usr\/sbin\/iptables, [--wait -t filter -C FORWARD -j DOCKER-ISOLATION]\r\nDEBU[2023-10-20T19:24:35.948499009+02:00] \/usr\/sbin\/iptables, [--wait -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER]\r\nDEBU[2023-10-20T19:24:35.949539177+02:00] \/usr\/sbin\/iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0\/8 -j DOCKER]\r\nDEBU[2023-10-20T19:24:35.951134450+02:00] \/usr\/sbin\/iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER]\r\nDEBU[2023-10-20T19:24:35.952296099+02:00] \/usr\/sbin\/iptables, [--wait -t nat -D PREROUTING]\r\nDEBU[2023-10-20T19:24:35.953160413+02:00] \/usr\/sbin\/iptables, [--wait -t nat -D OUTPUT]\r\nDEBU[2023-10-20T19:24:35.953946718+02:00] \/usr\/sbin\/iptables, [--wait -t nat -F DOCKER]\r\nDEBU[2023-10-20T19:24:35.955240789+02:00] \/usr\/sbin\/iptables, [--wait -t nat -X DOCKER]\r\nDEBU[2023-10-20T19:24:35.956192278+02:00] \/usr\/sbin\/iptables, [--wait -t filter -F DOCKER]\r\nDEBU[2023-10-20T19:24:35.957034409+02:00] \/usr\/sbin\/iptables, [--wait -t filter -X DOCKER]\r\nDEBU[2023-10-20T19:24:35.958301849+02:00] \/usr\/sbin\/iptables, [--wait -t filter -F DOCKER-ISOLATION-STAGE-1]\r\nDEBU[2023-10-20T19:24:35.959178456+02:00] \/usr\/sbin\/iptables, [--wait -t filter -X DOCKER-ISOLATION-STAGE-1]\r\nDEBU[2023-10-20T19:24:35.960404447+02:00] \/usr\/sbin\/iptables, [--wait -t filter -F DOCKER-ISOLATION-STAGE-2]\r\nDEBU[2023-10-20T19:24:35.961308597+02:00] \/usr\/sbin\/iptables, [--wait -t filter -X DOCKER-ISOLATION-STAGE-2]\r\nDEBU[2023-10-20T19:24:35.962494081+02:00] \/usr\/sbin\/iptables, [--wait -t filter -F DOCKER-ISOLATION]\r\nDEBU[2023-10-20T19:24:35.963281438+02:00] \/usr\/sbin\/iptables, [--wait -t filter -X DOCKER-ISOLATION]\r\nDEBU[2023-10-20T19:24:35.964447425+02:00] \/usr\/sbin\/iptables, [--wait -t nat -n -L DOCKER]\r\nDEBU[2023-10-20T19:24:35.965263467+02:00] \/usr\/sbin\/iptables, [--wait -t nat -N DOCKER]\r\nDEBU[2023-10-20T19:24:35.965998455+02:00] \/usr\/sbin\/iptables, [--wait -t filter -n -L DOCKER]\r\nDEBU[2023-10-20T19:24:35.966759462+02:00] \/usr\/sbin\/iptables, [--wait -t filter -N DOCKER]\r\nDEBU[2023-10-20T19:24:35.967936200+02:00] \/usr\/sbin\/iptables, [--wait -t filter -n -L DOCKER-ISOLATION-STAGE-1]\r\nDEBU[2023-10-20T19:24:35.968994893+02:00] \/usr\/sbin\/iptables, [--wait -t filter -N DOCKER-ISOLATION-STAGE-1]\r\nDEBU[2023-10-20T19:24:35.969733417+02:00] \/usr\/sbin\/iptables, [--wait -t filter -n -L DOCKER-ISOLATION-STAGE-2]\r\nDEBU[2023-10-20T19:24:35.971047235+02:00] \/usr\/sbin\/iptables, [--wait -t filter -N DOCKER-ISOLATION-STAGE-2]\r\nDEBU[2023-10-20T19:24:35.971899065+02:00] \/usr\/sbin\/iptables, [--wait -t filter -C DOCKER-ISOLATION-STAGE-1 -j RETURN]\r\nDEBU[2023-10-20T19:24:35.973222221+02:00] \/usr\/sbin\/iptables, [--wait -A DOCKER-ISOLATION-STAGE-1 -j RETURN]\r\nDEBU[2023-10-20T19:24:35.974073610+02:00] \/usr\/sbin\/iptables, [--wait -t filter -C DOCKER-ISOLATION-STAGE-2 -j RETURN]\r\nDEBU[2023-10-20T19:24:35.974960827+02:00] \/usr\/sbin\/iptables, [--wait -A DOCKER-ISOLATION-STAGE-2 -j RETURN]\r\npanic: page 3 already freed\r\n\r\ngoroutine 1 [running, locked to thread]:\r\ngo.etcd.io\/bbolt.(*freelist).free(0xc0009f9900, 0x339, 0x7f5694f1d000)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/freelist.go:175 +0x2c8\r\ngo.etcd.io\/bbolt.(*node).spill(0xc0005f4cb0)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/node.go:363 +0x1a8\r\ngo.etcd.io\/bbolt.(*node).spill(0xc0005f4c40)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/node.go:350 +0xa9\r\ngo.etcd.io\/bbolt.(*Bucket).spill(0xc00007f680)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/bucket.go:584 +0x33f\r\ngo.etcd.io\/bbolt.(*Bucket).spill(0xc0004aa638)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/bucket.go:551 +0x107\r\ngo.etcd.io\/bbolt.(*Tx).Commit(0xc0004aa620)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/tx.go:163 +0x10a\r\ngo.etcd.io\/bbolt.(*DB).Update(0xc0007705f0?, 0xc0011be640)\r\n        \/root\/build-deb\/engine\/vendor\/go.etcd.io\/bbolt\/db.go:869 +0xe5\r\ngithub.com\/docker\/libkv\/store\/boltdb.(*BoltDB).Put(0xc0007705f0, {0xc000c749c0, 0x23}, {0x562560076ef0, 0x0, 0x0}, 0xc0006da390?)\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/docker\/libkv\/store\/boltdb\/boltdb.go:187 +0x225\r\ngithub.com\/docker\/docker\/libnetwork\/datastore.(*datastore).ensureParent(0xc000988d40, {0xc000c749c0, 0x23})\r\n        \/root\/build-deb\/engine\/libnetwork\/datastore\/datastore.go:476 +0x90\r\ngithub.com\/docker\/docker\/libnetwork\/datastore.(*datastore).iterateKVPairsFromStore(0xc000988d40, {0xc000c749c0, 0x23}, {0x56255edb4500?, 0xc0006da390?}, 0xc0011be7c8)\r\n        \/root\/build-deb\/engine\/libnetwork\/datastore\/datastore.go:508 +0x8d\r\ngithub.com\/docker\/docker\/libnetwork\/datastore.(*datastore).Map(0xc000988d40, {0xc000c749c0, 0x23}, {0x56255edb4500, 0xc0006da390})\r\n        \/root\/build-deb\/engine\/libnetwork\/datastore\/datastore.go:547 +0x15b\r\ngithub.com\/docker\/docker\/libnetwork.(*Controller).getNetworksFromStore(0xc000894000)\r\n        \/root\/build-deb\/engine\/libnetwork\/store.go:104 +0x30a\r\ngithub.com\/docker\/docker\/libnetwork.(*Controller).Networks(0x56255eda5b20?)\r\n        \/root\/build-deb\/engine\/libnetwork\/controller.go:786 +0x1d\r\ngithub.com\/docker\/docker\/libnetwork.(*Controller).WalkNetworks(0x56255ed90460?, 0x56255ed6f5a8)\r\n        \/root\/build-deb\/engine\/libnetwork\/controller.go:798 +0x1e\r\ngithub.com\/docker\/docker\/libnetwork.New({0xc0008f4230, 0x8, 0xe})\r\n        \/root\/build-deb\/engine\/libnetwork\/controller.go:153 +0x55a\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).initNetworkController(0xc000d12480, 0xc000541980)\r\n        \/root\/build-deb\/engine\/daemon\/daemon_unix.go:853 +0x4e\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).restore(0xc000d12480)\r\n        \/root\/build-deb\/engine\/daemon\/daemon.go:508 +0x6bd\r\ngithub.com\/docker\/docker\/daemon.NewDaemon({0x56255eda4f18?, 0xc000940320}, 0xc000948500, 0xc000c82d20, 0xc000c80280)\r\n        \/root\/build-deb\/engine\/daemon\/daemon.go:1135 +0x2fde\r\nmain.(*DaemonCli).start(0xc00093cec0, 0xc0009397a0)\r\n        \/root\/build-deb\/engine\/cmd\/dockerd\/daemon.go:232 +0xae9\r\nmain.runDaemon(...)\r\n        \/root\/build-deb\/engine\/cmd\/dockerd\/docker_unix.go:14\r\nmain.newDaemonCommand.func1(0xc00089a000?, {0xc000932e50?, 0x1?, 0x1?})\r\n        \/root\/build-deb\/engine\/cmd\/dockerd\/docker.go:39 +0x94\r\ngithub.com\/spf13\/cobra.(*Command).execute(0xc00089a000, {0xc000052070, 0x1, 0x1})\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/spf13\/cobra\/command.go:916 +0x862\r\ngithub.com\/spf13\/cobra.(*Command).ExecuteC(0xc00089a000)\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/spf13\/cobra\/command.go:1044 +0x3bd\r\ngithub.com\/spf13\/cobra.(*Command).Execute(...)\r\n        \/root\/build-deb\/engine\/vendor\/github.com\/spf13\/cobra\/command.go:968\r\nmain.main()\r\n        \/root\/build-deb\/engine\/cmd\/dockerd\/docker.go:109 +0x18c\r\nroot@docker:~#\r\n```\r\n\r\nIn the log, last action before it starts printing trace is related to iptables. I tried to run last commend manually and it is executed correctly, no error is given so I don;t think it is iptables related.\r\n\r\n\r\n### Reproduce\r\n\r\nI don't know how to reproduce it.\r\n\r\n### Expected behavior\r\n\r\nDocker should start automatically during VM boot.\r\n\r\n### docker version\r\n\r\n```bash\r\nroot@docker:~# docker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:32:16 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\nCannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\r\nroot@docker:~#\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nroot@docker:~# docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\nERROR: Cannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\r\nerrors pretty printing info\r\nroot@docker:~#\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Trying to debug this further, I tried to run docker via strace debugger and last action before it start throwing error is reading network config form file: \/var\/lib\/docker\/network\/files\/local-kv.db :\r\n\r\n```\r\n...\r\nclose(11)                               = 0\r\nopenat(AT_FDCWD, \"\/var\/lib\/docker\/network\/files\/local-kv.db\", O_RDWR|O_CREAT|O_CLOEXEC, 0644) = 11\r\nfcntl(11, F_GETFL)                      = 0x8002 (flags O_RDWR|O_LARGEFILE)\r\nfcntl(11, F_SETFL, O_RDWR|O_NONBLOCK|O_LARGEFILE) = 0\r\nepoll_ctl(4, EPOLL_CTL_ADD, 11, {EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, {u32=3126504184, u64=140255283555064}}) = -1 EPERM (Operation not permitted)\r\nfcntl(11, F_GETFL)                      = 0x8802 (flags O_RDWR|O_NONBLOCK|O_LARGEFILE)\r\nfcntl(11, F_SETFL, O_RDWR|O_LARGEFILE)  = 0\r\nflock(11, LOCK_EX|LOCK_NB)              = 0\r\nfstat(11, {st_mode=S_IFREG|0644, st_size=131072, ...}) = 0\r\npread64(11, \"\\0\\0\\0\\0\\0\\0\\0\\0\\4\\0\\0\\0\\0\\0\\0\\0\\355\\332\\f\\355\\2\\0\\0\\0\\0\\20\\0\\0\\0\\0\\0\\0\"..., 4096, 0) = 4096\r\nfstat(11, {st_mode=S_IFREG|0644, st_size=131072, ...}) = 0\r\nmmap(NULL, 131072, PROT_READ, MAP_SHARED, 11, 0) = 0x7f8fb8167000\r\nmadvise(0x7f8fb8167000, 131072, MADV_RANDOM) = 0\r\nmunmap(0x7f8fb8167000, 131072)          = 0\r\nflock(11, LOCK_UN)                      = 0\r\nclose(11)                               = 0\r\nfutex(0xc000078d48, FUTEX_WAKE_PRIVATE, 1) = 1\r\nunlinkat(AT_FDCWD, \"\/var\/run\/docker.pid\", 0) = 0\r\ngetpid()                                = 2294\r\ntgkill(2294, 2298, SIGURG)              = 0\r\nnanosleep({tv_sec=0, tv_nsec=1000000}, NULL) = 0\r\nnanosleep({tv_sec=0, tv_nsec=1000000}, NULL) = 0\r\nnanosleep({tv_sec=0, tv_nsec=1000000}, NULL) = 0\r\nwrite(2, \"panic: \", 7panic: )                  = 7\r\n...\r\n```\r\n\r\nI'm attaching mentioned file - can it be corrupted(maybe previous unclean stop of docker?)?\r\n\r\n[local-kv.zip](https:\/\/github.com\/moby\/moby\/files\/13057355\/local-kv.zip)\r\n"],"labels":["status\/0-triage","kind\/bug","area\/networking"]},{"title":"c8d: Lost history after image delete","body":"### Description\r\n\r\nWhile working on https:\/\/github.com\/moby\/moby\/pull\/46634 I found that deleting an image that was built with a `--cache-from=IMG` will break `IMG`s history.\r\n\r\nThis is due to the fact that we don't have a check for the present children so `rmi` happily deletes the image and all of its parents. \r\n\r\nNote: while we do remove images we shouldn't this doesn't break any image because the content itself is note removed, only the image (in containerd).\r\n\r\nTalking with @thaJeztah, @laurazard and others we agree that we might not want to bring back the children check when deleting an image because it doesn't really make sense with containerd.\r\n\r\nSo we can either:\r\n- ignore this, this is only related to the classic builder that is deprecated \r\n- fix this by re-adding the children check when we delete an image\r\n- fix this in some other way\r\n\r\n### Reproduce\r\n\r\n```console\r\n$ DOCKER_BUILDKIT=0 docker -t first .\r\n\r\n... output ...\r\n\r\nSuccessfully built b96945b77088\r\nSuccessfully tagged first:latest\r\n$ DOCKER_BUILDKIT=0 docker build -t second --cache-from=first -f Dockerfile.after .\r\n\r\n... output ...\r\n\r\nSuccessfully built ba20a593e9e5\r\nSuccessfully tagged second:latest\r\n$ docker history first\r\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\r\nb96945b77088   12 seconds ago   \/bin\/sh -c touch baz                            4.1kB     \r\n9426bb522ce8   13 seconds ago   \/bin\/sh -c #(nop) ADD file:d7a2ef4e435ddef6d\u2026   4.1kB     \r\n431523535996   13 seconds ago   \/bin\/sh -c #(nop)  ENV FOO=bar                  0B        \r\n3fbc63216742   3 months ago     \/bin\/sh -c #(nop)  CMD [\"sh\"]                   0B        \r\n<missing>      3 months ago     \/bin\/sh -c #(nop) ADD file:7e9002edaafd4e457\u2026   4.39MB    \r\n$ docker rmi second \r\nUntagged: second:latest\r\nDeleted: sha256:ba20a593e9e5d0a207557f5bde462f796566596ad4838d1de8781e2ed203da97\r\nDeleted: sha256:9426bb522ce8bfec856594b33bc6aac0ec566848db8b0d87b6d7114f2cf37928\r\nDeleted: sha256:431523535996213be734425ae93f5cb41afa1f29fb23b9d540a1722b1f046705\r\n$ docker history first\r\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\r\nb96945b77088   20 seconds ago   \/bin\/sh -c touch baz                            4.1kB     \r\n<missing>      21 seconds ago   \/bin\/sh -c #(nop) ADD file:d7a2ef4e435ddef6d\u2026   4.1kB     \r\n<missing>      21 seconds ago   \/bin\/sh -c #(nop)  ENV FOO=bar                  0B        \r\n<missing>      3 months ago     \/bin\/sh -c #(nop)  CMD [\"sh\"]                   0B        \r\n<missing>      3 months ago     \/bin\/sh -c #(nop) ADD file:7e9002edaafd4e457\u2026   4.39MB  \r\n```\r\n\r\n### Expected behavior\r\n\r\nThe history of `first` shouldn't change after `second` is deleted.\r\n\r\n### docker version\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nmaster\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["> fix this by re-adding the children check when we delete an image\r\n\r\nI think we could reimplement the children check, but only using the \"parent label\" (not the initial parent-search implementation which compared diffids). The check would only be performed on images that have the \"parent label\" (so we don't make it impossible to delete images built with buildkit) and it would check if `getParentsByBuilderLabel` returns anything.\r\nThis way we'd still have this check for classic builder images and reuse the code we already have for history.","We can, and that's what I did in my other PR before removing it. Issue is, we would be adding a check that doesn't really make sense in containerd world. We _can_ safely remove those images, the content is not removed because it's still used by another image"],"labels":["status\/0-triage","kind\/bug","area\/images","containerd-integration"]},{"title":"`docker:dind` on Rootless Docker host (`can't get final child's PID from pipe`)","body":"# Description\r\n\r\nHi there,\r\n\r\nIn `docker:dind` on Rootless Docker host (Debian 12), `docker pull` succeeds, and `docker run` fails with the error `can't get final child's PID from pipe`.\r\nIn `docker:dind` on Rootless Docker host (Ubuntu 22.04), `docker pull` succeeds, and `docker run` also succeeds.\r\n\r\n`docker:dind-rootless` on Rootless Docker host (Debian 12) does not up with the error `newuidmap: write to uid_map failed`.\r\n`docker:dind-rootless` on Rootless Docker host (Ubuntu 22.04) does not up with the error `newuidmap: write to uid_map failed`.\r\n\r\n# Reproduce\r\n\r\n## `docker:dind`\r\n\r\n### on Rootless Docker host (Debian 12) - `docker pull` succeeds, `docker run` fails\r\n\r\n```\r\nrootless@debian12:~$ uname -a\r\nLinux debian12 6.1.0-13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.55-1 (2023-09-29) x86_64 GNU\/Linux\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind$ cat compose.yml\r\nversion: \"3\"\r\n\r\nservices:\r\n  dind:\r\n    image: docker:24.0.6-dind\r\n    privileged: true\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind$ docker compose up -d\r\n[+] Running 2\/2\r\n \u2714 Network dind_default   Created                                                                                  0.1s\r\n \u2714 Container dind-dind-1  Started                                                                                  0.0s\r\n\r\nrootless@debian12:~\/dind$ docker compose ps\r\nNAME          IMAGE                COMMAND                   SERVICE   CREATED         STATUS         PORTS\r\ndind-dind-1   docker:24.0.6-dind   \"dockerd-entrypoint.sh\"   dind      6 seconds ago   Up 5 seconds   2375-2376\/tcp\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind$ docker compose exec dind sh\r\n\/ # \r\n\r\n\/ # docker pull ubuntu:latest\r\nlatest: Pulling from library\/ubuntu\r\naece8493d397: Pull complete\r\nDigest: sha256:2b7412e6465c3c7fc5bb21d3e6f1917c167358449fecac8176c6e496e5c1f05f\r\nStatus: Downloaded newer image for ubuntu:latest\r\ndocker.io\/library\/ubuntu:latest\r\n\r\n\/ # docker run --rm -it ubuntu:latest bash\r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown.\r\nERRO[0000] error waiting for container:\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind$ docker compose logs dind\r\ndind-dind-1  | Certificate request self-signature ok\r\ndind-dind-1  | subject=CN = docker:dind server\r\ndind-dind-1  | \/certs\/server\/cert.pem: OK\r\ndind-dind-1  | Certificate request self-signature ok\r\ndind-dind-1  | subject=CN = docker:dind client\r\ndind-dind-1  | \/certs\/client\/cert.pem: OK\r\ndind-dind-1  | mount: permission denied (are you root?)\r\ndind-dind-1  | Could not mount \/sys\/kernel\/security.\r\ndind-dind-1  | AppArmor detection and --privileged mode might break.\r\ndind-dind-1  | time=\"2023-10-19T14:51:46.367691053Z\" level=info msg=\"Starting up\"\r\ndind-dind-1  | time=\"2023-10-19T14:51:46.373233400Z\" level=warning msg=\"could not change group \/var\/run\/docker.sock to docker: group docker not found\"\r\ndind-dind-1  | time=\"2023-10-19T14:51:46.373487600Z\" level=info msg=\"containerd not running, starting managed containerd\"\r\ndind-dind-1  | time=\"2023-10-19T14:51:46.375119974Z\" level=info msg=\"started new containerd process\" address=\/var\/run\/docker\/containerd\/containerd.sock module=libcontainerd pid=57\r\n\r\n...snip...\r\n\r\ndind-dind-1  | time=\"2023-10-19T14:51:47.229973833Z\" level=info msg=\"API listen on [::]:2376\"\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.392478666Z\" level=info msg=\"No non-localhost DNS nameservers are left in resolv.conf. Using default external servers: [nameserver 8.8.8.8 nameserver 8.8.4.4]\"\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.392619707Z\" level=info msg=\"IPv6 enabled; Adding default IPv6 external servers: [nameserver 2001:4860:4860::8888 nameserver 2001:4860:4860::8844]\"\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.491483074Z\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.493372565Z\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.pause\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.493494554Z\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.493571581Z\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.545901194Z\" level=info msg=\"shim disconnected\" id=d34b28a8f39a732517dc5e9cff41b5e5220569eeaaa34bc8268240dbd53ea1f6 namespace=moby\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.546667559Z\" level=warning msg=\"cleaning up after shim disconnected\" id=d34b28a8f39a732517dc5e9cff41b5e5220569eeaaa34bc8268240dbd53ea1f6 namespace=moby\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.546752340Z\" level=info msg=\"cleaning up dead shim\" namespace=moby\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.568219191Z\" level=warning msg=\"cleanup warnings time=\\\"2023-10-19T14:52:39Z\\\" level=warning msg=\\\"failed to read init pid file\\\" error=\\\"open \/run\/docker\/containerd\/daemon\/io.containerd.runtime.v2.task\/moby\/d34b28a8f39a732517dc5e9cff41b5e5220569eeaaa34bc8268240dbd53ea1f6\/init.pid: no such file or directory\\\" runtime=io.containerd.runc.v2\\n\" namespace=moby\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.569698590Z\" level=error msg=\"copy shim log\" error=\"read \/proc\/self\/fd\/13: file already closed\" namespace=moby\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.579784182Z\" level=error msg=\"stream copy error: reading from a closed fifo\"\r\ndind-dind-1  | time=\"2023-10-19T14:52:39.703295992Z\" level=error msg=\"Handler for POST \/v1.43\/containers\/d34b28a8f39a732517dc5e9cff41b5e5220569eeaaa34bc8268240dbd53ea1f6\/start returned error: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown\"\r\n```\r\n\r\n### on Rootless Docker host (Ubuntu 22.04) - `docker pull` succeeds, `docker run` succeeds\r\n\r\n```\r\nrootless@ubuntu2204:~$ uname -a\r\nLinux ubuntu2204 5.15.0-87-generic #97-Ubuntu SMP Mon Oct 2 21:09:21 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind$ cat compose.yml\r\nversion: \"3\"\r\n\r\nservices:\r\n  dind:\r\n    image: docker:24.0.6-dind\r\n    privileged: true\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind$ docker compose up -d\r\n[+] Running 2\/2\r\n \u2714 Network dind_default   Created                                                                                  0.1s\r\n \u2714 Container dind-dind-1  Started                                                                                  0.1s\r\n\r\nrootless@ubuntu2204:~\/dind$ docker compose ps\r\nNAME          IMAGE                COMMAND                   SERVICE   CREATED              STATUS              PORTS\r\ndind-dind-1   docker:24.0.6-dind   \"dockerd-entrypoint.sh\"   dind      About a minute ago   Up About a minute   2375-2376\/tcp\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind$ docker compose exec dind sh\r\n\/ # \r\n\r\n\/ # docker pull debian:stable\r\nstable: Pulling from library\/debian\r\n98d7f57ba433: Pull complete\r\nDigest: sha256:fd865da5f887f2fdb7af19c3a33fe9c98cb00a26859df66508cadadc35cb8676\r\nStatus: Downloaded newer image for debian:stable\r\ndocker.io\/library\/debian:stable\r\n\r\n\/ # docker run --rm -it debian:stable bash\r\nroot@44986f5f32dc:\/# \r\n\r\nroot@44986f5f32dc:\/# cat \/etc\/os-release\r\nPRETTY_NAME=\"Debian GNU\/Linux 12 (bookworm)\"\r\nNAME=\"Debian GNU\/Linux\"\r\nVERSION_ID=\"12\"\r\nVERSION=\"12 (bookworm)\"\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=\"https:\/\/www.debian.org\/\"\r\nSUPPORT_URL=\"https:\/\/www.debian.org\/support\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.debian.org\/\"\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind$ docker compose logs dind\r\ndind-dind-1  | Certificate request self-signature ok\r\ndind-dind-1  | subject=CN = docker:dind server\r\ndind-dind-1  | \/certs\/server\/cert.pem: OK\r\ndind-dind-1  | Certificate request self-signature ok\r\ndind-dind-1  | subject=CN = docker:dind client\r\ndind-dind-1  | \/certs\/client\/cert.pem: OK\r\ndind-dind-1  | mount: permission denied (are you root?)\r\ndind-dind-1  | Could not mount \/sys\/kernel\/security.\r\ndind-dind-1  | AppArmor detection and --privileged mode might break.\r\ndind-dind-1  | time=\"2023-10-19T14:43:57.967092773Z\" level=info msg=\"Starting up\"\r\ndind-dind-1  | time=\"2023-10-19T14:43:57.974216729Z\" level=warning msg=\"could not change group \/var\/run\/docker.sock to docker: group docker not found\"\r\ndind-dind-1  | time=\"2023-10-19T14:43:57.974358332Z\" level=info msg=\"containerd not running, starting managed containerd\"\r\n\r\n...snip...\r\n\r\ndind-dind-1  | time=\"2023-10-19T14:43:58.396372212Z\" level=info msg=\"API listen on [::]:2376\"\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.012664672Z\" level=info msg=\"No non-localhost DNS nameservers are left in resolv.conf. Using default external servers: [nameserver 8.8.8.8 nameserver 8.8.4.4]\"\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.012813115Z\" level=info msg=\"IPv6 enabled; Adding default IPv6 external servers: [nameserver 2001:4860:4860::8888 nameserver 2001:4860:4860::8844]\"\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.087850115Z\" level=info msg=\"loading plugin \\\"io.containerd.internal.v1.shutdown\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.internal.v1\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.088112853Z\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.pause\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.088218992Z\" level=info msg=\"loading plugin \\\"io.containerd.event.v1.publisher\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.event.v1\r\ndind-dind-1  | time=\"2023-10-19T14:44:58.088291501Z\" level=info msg=\"loading plugin \\\"io.containerd.ttrpc.v1.task\\\"...\" runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1\r\n```\r\n\r\n## `docker:dind-rootless` does not up\r\n\r\n\r\n### on Rootless Docker host (Debian 12)\r\n\r\n```\r\nrootless@debian12:~$ uname -a\r\nLinux debian12 6.1.0-13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.55-1 (2023-09-29) x86_64 GNU\/Linux\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind-rootless$ cat compose.yml\r\nversion: \"3\"\r\n\r\nservices:\r\n  dind-rootless:\r\n    image: docker:24.0.6-dind-rootless\r\n    privileged: true\r\n    environment:\r\n      - DOCKER_HOST=unix:\/\/\/var\/run\/user\/1000\/docker.sock\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind-rootless$ docker compose up -d\r\n[+] Running 2\/2\r\n \u2714 Network dind-rootless_default            Created                                                                0.1s\r\n \u2714 Container dind-rootless-dind-rootless-1  Started                                                                0.1s\r\n\r\nrootless@debian12:~\/dind-rootless$ docker compose ps\r\nNAME      IMAGE     COMMAND   SERVICE   CREATED   STATUS    PORTS\r\n```\r\n\r\n```\r\nrootless@debian12:~\/dind-rootless$ docker compose logs dind-rootless\r\ndind-rootless-dind-rootless-1  | Certificate request self-signature ok\r\ndind-rootless-dind-rootless-1  | subject=CN = docker:dind server\r\ndind-rootless-dind-rootless-1  | \/certs\/server\/cert.pem: OK\r\ndind-rootless-dind-rootless-1  | Certificate request self-signature ok\r\ndind-rootless-dind-rootless-1  | subject=CN = docker:dind client\r\ndind-rootless-dind-rootless-1  | \/certs\/client\/cert.pem: OK\r\ndind-rootless-dind-rootless-1  | Device \"ip_tables\" does not exist.\r\ndind-rootless-dind-rootless-1  | ip_tables              36864  2 iptable_nat,iptable_filter\r\ndind-rootless-dind-rootless-1  | x_tables               61440  9 iptable_nat,iptable_filter,xt_nat,xt_tcpudp,xt_conntrack,xt_MASQUERADE,xt_addrtype,nft_compat,ip_tables\r\ndind-rootless-dind-rootless-1  | modprobe: can't change directory to '\/lib\/modules': No such file or directory\r\ndind-rootless-dind-rootless-1  | [rootlesskit:parent] error: failed to setup UID\/GID map: newuidmap 55 [0 1000 1 1 100000 65536] failed: newuidmap: write to uid_map failed: Operation not permitted\r\ndind-rootless-dind-rootless-1  | : exit status 1\r\n```\r\n\r\n### on Rootless Docker host (Ubuntu 22.04)\r\n\r\n```\r\nrootless@ubuntu2204:~$ uname -a\r\nLinux ubuntu2204 5.15.0-87-generic #97-Ubuntu SMP Mon Oct 2 21:09:21 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind-rootless$ cat compose.yml\r\nversion: \"3\"\r\n\r\nservices:\r\n  dind-rootless:\r\n    image: docker:24.0.6-dind-rootless\r\n    privileged: true\r\n    environment:\r\n      - DOCKER_HOST=unix:\/\/\/var\/run\/user\/1000\/docker.sock\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind-rootless$ docker compose up -d\r\n[+] Running 2\/2\r\n \u2714 Network dind-rootless_default            Created                                                                0.1s\r\n \u2714 Container dind-rootless-dind-rootless-1  Started                                                                0.1s\r\n\r\nrootless@ubuntu2204:~\/dind-rootless$ docker compose ps\r\nNAME      IMAGE     COMMAND   SERVICE   CREATED   STATUS    PORTS\r\n```\r\n\r\n```\r\nrootless@ubuntu2204:~\/dind-rootless$ docker compose logs dind-rootless\r\ndind-rootless-dind-rootless-1  | Certificate request self-signature ok\r\ndind-rootless-dind-rootless-1  | subject=CN = docker:dind server\r\ndind-rootless-dind-rootless-1  | \/certs\/server\/cert.pem: OK\r\ndind-rootless-dind-rootless-1  | Certificate request self-signature ok\r\ndind-rootless-dind-rootless-1  | subject=CN = docker:dind client\r\ndind-rootless-dind-rootless-1  | \/certs\/client\/cert.pem: OK\r\ndind-rootless-dind-rootless-1  | Device \"ip_tables\" does not exist.\r\ndind-rootless-dind-rootless-1  | ip_tables              32768  2 iptable_nat,iptable_filter\r\ndind-rootless-dind-rootless-1  | x_tables               53248  9 iptable_nat,iptable_filter,xt_nat,xt_tcpudp,xt_conntrack,xt_MASQUERADE,xt_addrtype,nft_compat,ip_tables\r\ndind-rootless-dind-rootless-1  | modprobe: can't change directory to '\/lib\/modules': No such file or directory\r\ndind-rootless-dind-rootless-1  | [rootlesskit:parent] error: failed to setup UID\/GID map: newuidmap 56 [0 1000 1 1 100000 65536] failed: newuidmap: write to uid_map failed: Operation not permitted\r\ndind-rootless-dind-rootless-1  | : exit status 1\r\n```\r\n\r\n# Expected behavior\r\n\r\nI want `docker:dind` to work correctly on Debian 12 Rootless Docker hosts.\r\nIf possible, `docker:dind-rootless` should also work.\r\n\r\n\r\n# docker version\r\n\r\n## `docker version` on Debian 12 host\r\n\r\n```\r\nrootless@debian12:~$ docker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:32:10 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           rootless\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:10 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          1.1.1\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    slirp4netns\r\n  PortDriver:       builtin\r\n  StateDir:         \/tmp\/rootlesskit41920960\r\n slirp4netns:\r\n  Version:          1.2.0\r\n  GitCommit:        656041d45cfca7a4176f6b7eed9e4fe6c11e8383\r\n```\r\n\r\n## `docker version` on Ubuntu 22.04 host\r\n\r\n```\r\nrootless@ubuntu2204:~$ docker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:31:44 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           rootless\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:44 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n rootlesskit:\r\n  Version:          1.1.1\r\n  ApiVersion:       1.1.1\r\n  NetworkDriver:    slirp4netns\r\n  PortDriver:       builtin\r\n  StateDir:         \/tmp\/rootlesskit4088568616\r\n slirp4netns:\r\n  Version:          1.0.1\r\n  GitCommit:        6a7b16babc95b6a3056b33fb45b74a6f62262dd4\r\n```\r\n\r\n# docker info\r\n\r\n## `docker info` on Debian 12 host\r\n\r\n```\r\nrootless@debian12:~$ docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    rootless\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 6.1.0-13-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 1.921GiB\r\n Name: debian12\r\n ID: 34a061b9-ae74-41b6-8211-bed2fd9e4f8f\r\n Docker Root Dir: \/home\/rootless\/.local\/share\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No cpuset support\r\nWARNING: No io.weight support\r\nWARNING: No io.weight (per device) support\r\nWARNING: No io.max (rbps) support\r\nWARNING: No io.max (wbps) support\r\nWARNING: No io.max (riops) support\r\nWARNING: No io.max (wiops) support\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```\r\n\r\n## `docker info` on Ubuntu 22.04 host\r\n\r\n```\r\nrootless@ubuntu2204:~$ docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    rootless\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 5\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  rootless\r\n  cgroupns\r\n Kernel Version: 5.15.0-87-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 1.918GiB\r\n Name: ubuntu2204\r\n ID: c1f3005e-61ca-427e-be10-c7a56cfd21b5\r\n Docker Root Dir: \/home\/rootless\/.local\/share\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No cpu cfs quota support\r\nWARNING: No cpu cfs period support\r\nWARNING: No cpu shares support\r\nWARNING: No cpuset support\r\nWARNING: No io.weight support\r\nWARNING: No io.weight (per device) support\r\nWARNING: No io.max (rbps) support\r\nWARNING: No io.max (wbps) support\r\nWARNING: No io.max (riops) support\r\nWARNING: No io.max (wiops) support\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n```\r\n","comments":["Probably a duplicate of:\r\n- https:\/\/github.com\/moby\/moby\/issues\/46563\r\n\r\nThe fix will be included in v24.0.7:\r\n- https:\/\/github.com\/moby\/moby\/pull\/46626","Reproduced it as commented.\r\n\r\n```\r\nrootless@debian12:~\/dind$ docker compose exec dind sh\r\n\/ # docker pull ubuntu:latest\r\nlatest: Pulling from library\/ubuntu\r\naece8493d397: Pull complete\r\nDigest: sha256:2b7412e6465c3c7fc5bb21d3e6f1917c167358449fecac8176c6e496e5c1f05f\r\nStatus: Downloaded newer image for ubuntu:latest\r\ndocker.io\/library\/ubuntu:latest\r\n\r\n\/ # docker run --rm -it ubuntu:latest bash\r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't get final child's PID from pipe: EOF: unknown.\r\nERRO[0000] error waiting for container:\r\n\r\n\/ # docker run --oom-score-adj=100 --rm -it ubuntu:latest bash\r\nroot@7b0ba3b17e07:\/# cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n```","I would appreciate comments about `docker:dind-rootless` as well.\r\nIs it not supposed to run `docker:dind-rootless` on a Rootless Docker host?"],"labels":["status\/0-triage","kind\/bug","area\/rootless","kind\/duplicate","version\/24.0"]},{"title":"[24.0 backport] vendor: github.com\/docker\/distribution v2.8.3","body":"backports of;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46555\r\n- https:\/\/github.com\/moby\/moby\/pull\/46542\r\n\r\n\r\n- relates to https:\/\/github.com\/moby\/moby\/issues\/46006\r\n- fixes https:\/\/github.com\/moby\/moby\/issues\/46006\r\n\r\nDiff is different, because the 24.0 branch still uses the deprecated reference package, and doesn't contain;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46376\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Interesting failures; they look like actual failures, but curious how the failures relate to these changes \ud83e\udd14 \r\n\r\n```\r\nfailed to start daemon: insecure registry insecurehost.com:5000 is not valid: invalid host \"insecurehost.com\"\r\n```\r\n\r\n```\r\n=== Failed\r\n=== FAIL: daemon TestDaemonReloadAllowNondistributableArtifacts (0.00s)\r\n    reload_test.go:71: allow-nondistributable-artifacts registry docker1.com is not valid: invalid host \"docker1.com\"\r\n\r\n=== FAIL: daemon TestDaemonReloadInsecureRegistries (0.00s)\r\n    reload_test.go:228: insecure registry docker1.example.com is not valid: invalid host \"docker1.example.com\"\r\n\r\n=== FAIL: registry TestLoadAllowNondistributableArtifacts (0.00s)\r\n    config_test.go:125: expect error 'allow-nondistributable-artifacts registry myregistry.example.com:500000 is not valid: invalid port \"500000\"', got 'allow-nondistributable-artifacts registry myregistry.example.com:500000 is not valid: invalid host \"myregistry.example.com\"'\r\n\r\n=== FAIL: registry TestLoadInsecureRegistries (0.00s)\r\ntime=\"2023-10-18T16:01:47Z\" level=warning msg=\"insecure registry http:\/\/myregistry.example.com should not contain 'http:\/\/' and 'http:\/\/' has been removed from the insecure registry config\"\r\n    config_test.go:244: expect no error, got 'insecure registry myregistry.example.com is not valid: invalid host \"myregistry.example.com\"'\r\n\r\n=== FAIL: registry TestNewIndexInfo (0.00s)\r\n    registry_test.go:421: insecure registry example.com is not valid: invalid host \"example.com\"\r\n\r\n=== FAIL: registry TestAllowNondistributableArtifacts (0.00s)\r\n    registry_test.go:660: allow-nondistributable-artifacts registry example.com is not valid: invalid host \"example.com\"\r\npanic: runtime error: invalid memory address or nil pointer dereference [recovered]\r\n\tpanic: runtime error: invalid memory address or nil pointer dereference\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x949aa5]\r\n\r\ngoroutine 199 [running]:\r\ntesting.tRunner.func1.2({0x9c0260, 0xef40a0})\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1526 +0x24e\r\ntesting.tRunner.func1()\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1529 +0x39f\r\npanic({0x9c0260, 0xef40a0})\r\n\t\/usr\/local\/go\/src\/runtime\/panic.go:884 +0x213\r\ngithub.com\/docker\/docker\/registry.(*serviceConfig).allowNondistributableArtifacts(...)\r\n\t\/go\/src\/github.com\/docker\/docker\/registry\/config.go:257\r\ngithub.com\/docker\/docker\/registry.TestAllowNondistributableArtifacts(0xc00036d520)\r\n\t\/go\/src\/github.com\/docker\/docker\/registry\/registry_test.go:662 +0x705\r\ntesting.tRunner(0xc00036d520, 0xaa9370)\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1576 +0x10b\r\ncreated by testing.(*T).Run\r\n\t\/usr\/local\/go\/src\/testing\/testing.go:1629 +0x3ea\r\n\r\nDONE 2994 tests, 26 skipped, 6 failures in 312.017s\r\nmake: *** [Makefile:218: test-unit] Error 1\r\nError: Process completed with exit code 2.\r\n```","\ud83d\ude48 \ud83d\ude2c found it \ud83d\ude2c looks like I've been a victim of auto-complete in distribution; https:\/\/github.com\/moby\/moby\/blob\/5338c06c2a7b34aa5acc79076dccd8c6667cd58d\/vendor\/github.com\/docker\/distribution\/reference\/regexp_deprecated.go#L23"],"labels":["area\/distribution","status\/2-code-review"]},{"title":"libnetwork: fan out firewall reload top-down","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n- Related to #46237\r\n\r\nThe pattern of having low-level code that sets up iptables rules register reload callbacks for itself is not ideal. Callbacks can not be unregistered, so any values captured by the closures can never be GC'ed. And the callback registry is a singleton. While these issues could be overcome by iterating on the API, there are some other insurmountable issues which cannot be overcome without a complete overhaul of how the firewall reload logic is plumbed through. Imperative callback registration makes the control flow hard to reason about and the order of callbacks is implied by the order they were registered.\r\n\r\n**- What I did**\r\nMake the libnetwork Controller ultimately responsible for replaying firewall configuration when the firewall rulesets have been flushed externally. It fans out the reload signal to the drivers, which in turn fan out to the individual networks.\r\n\r\n**- How I did it**\r\nMade `(*Controller).handleFirewallReload` the only registered `iptables.OnReloaded` callback. It handles replaying firewall configuration for the controller, then calls into all the drivers which implement `FirewallReplayer` to handle replaying their own firewall configuration.\r\n\r\n**- How to verify it**\r\nTODO - UNVERIFIED\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\nN\/A\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/1-design-review","area\/networking","kind\/refactor","area\/networking\/firewalld"]},{"title":"`docker pull` pulls already present layers after `docker load`","body":"### Description\n\nUsing `docker load --input my_containers` then subsequently running `docker pull` on a container which was included in the `my_containers` file, when an update exists on the registry, causes all layers to be re-downloaded from the registry despite nearly all of them existing locally.\n\n### Reproduce\n\n1. `docker save myregistry\/mycontainer:latest -o containers.zip`\r\n2. move to new machine\r\n3. `docker load --input containers.zip`\r\n4. now tag & push the image at `myregistry\/mycontainer:latest` so it has one modified layer.\r\n5. `docker inspect myregistry\/mycontainer:latest` on the local machine vs the new machine show only one layer difference, all other layer SHA256 hashes remain the same.\r\n6. `docker pull myregistry\/mycontainer:latest` on the new machine will now pull all layers from the registry, instead of only pulling the new one.\r\n\n\n### Expected behavior\n\n6. `docker pull myregistry\/mycontainer:latest` should only pull the final, new layer, all other layers already exist on the local machine.\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.2-rd\r\n API version:       1.42 (downgraded from 1.43)\r\n Go version:        go1.20.4\r\n Git commit:        e63f5fa\r\n Built:             Fri May 26 16:43:15 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          23.0.6\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       9dbdbd4b6d7681bd18c897a6ba0376073c2a72ff\r\n  Built:            Fri May 12 13:54:36 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.2\r\n  GitCommit:        0cae528dd6cb557f7201036e9f43420650207b58\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        860f061b76bb4fc671f0f9e900f7d80ff93d4eb7\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.2-rd\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.0\r\n    Path:     C:\\Users\\MYUSER\\.docker\\cli-plugins\\docker-buildx.exe\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.19.0\r\n    Path:     C:\\Users\\MYUSER\\.docker\\cli-plugins\\docker-compose.exe\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.17.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scan.exe\r\n\r\nServer:\r\n Containers: 21\r\n  Running: 21\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 24\r\n Server Version: 23.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0cae528dd6cb557f7201036e9f43420650207b58\r\n runc version: 860f061b76bb4fc671f0f9e900f7d80ff93d4eb7\r\n init version:\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.10.16.3-microsoft-standard-WSL2\r\n Operating System: Rancher Desktop WSL Distribution\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 12.36GiB\r\n Name: XXXXXX\r\n ID: d0c71c38-fa8c-4a00-8e15-c44a236a8505\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\n```\n\n\n### Additional Info\n\n\r\n*CLIENTMACHINE*\r\n```\r\nPS C:\\dckr> docker inspect my_registry\/my_image:latest\r\n[\r\n    {\r\n        \"Id\": \"sha256:f5705335809654e419f9f35ec64f67be00dab9c55b5ea45a3645bdd4ba8b5632\",\r\n        \"RepoTags\": [\r\n            \"my_registry\/my_image:latest\"\r\n        ],\r\n        \"RepoDigests\": [\r\n            \"my_registry\/docker@sha256:b54fe073e9513a3a4076a7c159e125b78372ebe4f302ad6798ec6360da8e621a\"\r\n        ],\r\n        ...\r\n        \"RootFS\": {\r\n            \"Type\": \"layers\",\r\n            \"Layers\": [\r\n                \"sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7\",\r\n                \"sha256:537313a13d9080fc30444461b8aac5f03b9cbb0e2352b3c3098d5eca5f42688f\",\r\n                \"sha256:7ea163ba4dceada2a8b964884cb267ab52a530eac1f5336121660abaed883056\",\r\n                \"sha256:4213fd357bbe851f736d99be796eff87f5eff9fffd144edc57af5efd0e4ebdd8\",\r\n                \"sha256:e2abb3cacca0ccda48fd9cd7bb7294aba9eb1fb4b014c997dc93ebb9f28c16fb\",\r\n                \"sha256:6e16ebefff2fb103029174712a16e350858db74de3cc0090c35b7614cb4cb6fb\",\r\n                \"sha256:c363f05906daa4eb314521dece1e9caba386cdcb79adc08aefd9287cfa711d20\",\r\n                \"sha256:9df453ee9c1e8fa6f598b9f84bdc2c205cba551823a4dede9da04452e0a58fe7\",\r\n                \"sha256:bbf33adab12069e46bf733ee67c70ecdac7e5c9fd6e613054592070ab5e66104\",\r\n                \"sha256:6ff1a5c845b6db79146e7708f13f05cd18b3a1fbc852963a2f74cc793852631d\",\r\n                \"sha256:91e355326655bd151115a4631c4b4ae3755f452b90c0b71ac8ea189b3d3c2a4d\",\r\n                \"sha256:9f392f926c09e9e5cb1e1222575ad93afa4c176670cd5c90129063685418419e\",\r\n                \"sha256:ca8b8cc5822e6ee7033486bc68b09989c6ca630921e67a63b9dfee115f4dab0c\",\r\n                \"sha256:1e7bbcc2822d5e6f590a395cf9577c17a03df732f29f96f05d928fe7874d2467\"\r\n            ]\r\n        },\r\n        \"Metadata\": {\r\n            \"LastTagTime\": \"2023-10-02T09:40:22.417601035Z\"\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n\r\n\r\n*NEWMACHINE*\r\n```\r\nPS C:\\dckr> docker images\r\nREPOSITORY                                          TAG       IMAGE ID       CREATED         SIZE\r\nmy_registry\/my_image                                latest    4afcb9818f18   7 weeks ago     225MB\r\n\r\nPS C:\\dckr> docker inspect my_registry\/my_image:latest\r\n[\r\n    {\r\n        \"Id\": \"sha256:4afcb9818f18ae7a2bed28ed2639b5336f9f4db39310c4fbc2d082215e8110bf\",\r\n        \"RepoTags\": [\r\n            \"my_registry\/my_image:latest\"\r\n        ],\r\n        \"RepoDigests\": [],\r\n        ...\r\n        \"RootFS\": {\r\n            \"Type\": \"layers\",\r\n            \"Layers\": [\r\n                \"sha256:764055ebc9a7a290b64d17cf9ea550f1099c202d83795aa967428ebdf335c9f7\",\r\n                \"sha256:537313a13d9080fc30444461b8aac5f03b9cbb0e2352b3c3098d5eca5f42688f\",\r\n                \"sha256:7ea163ba4dceada2a8b964884cb267ab52a530eac1f5336121660abaed883056\",\r\n                \"sha256:4213fd357bbe851f736d99be796eff87f5eff9fffd144edc57af5efd0e4ebdd8\",\r\n                \"sha256:e2abb3cacca0ccda48fd9cd7bb7294aba9eb1fb4b014c997dc93ebb9f28c16fb\",\r\n                \"sha256:6e16ebefff2fb103029174712a16e350858db74de3cc0090c35b7614cb4cb6fb\",\r\n                \"sha256:c363f05906daa4eb314521dece1e9caba386cdcb79adc08aefd9287cfa711d20\",\r\n                \"sha256:9df453ee9c1e8fa6f598b9f84bdc2c205cba551823a4dede9da04452e0a58fe7\",\r\n                \"sha256:bbf33adab12069e46bf733ee67c70ecdac7e5c9fd6e613054592070ab5e66104\",\r\n                \"sha256:6ff1a5c845b6db79146e7708f13f05cd18b3a1fbc852963a2f74cc793852631d\",\r\n                \"sha256:91e355326655bd151115a4631c4b4ae3755f452b90c0b71ac8ea189b3d3c2a4d\",\r\n                \"sha256:9f392f926c09e9e5cb1e1222575ad93afa4c176670cd5c90129063685418419e\",\r\n                \"sha256:83b6a95cdf0dc4dec7f89e9202c19a5ddf8449e35536c970ef8e3fae15650987\"\r\n            ]\r\n        \"Metadata\": {\r\n            \"LastTagTime\": \"0001-01-01T00:00:00Z\"\r\n        }\r\n    }\r\n]\r\n\r\nC:\\dckr> docker pull my_registry\/my_image:latest\r\nlatest: Pulling from my_image\r\nb4d181a07f80: Pulling fs layer\r\nde8ecf497b75: Downloading [=>                                                 ]  95.19kB\/2.77MB\r\n707b80804672: Downloading [>                                                  ]  114.7kB\/10.06MB\r\n283715715396: Waiting\r\n8353afd48f6b: Waiting\r\n0b3a62e0d9f8: Waiting\r\n0348cf02e49a: Waiting\r\n1383a3b3225c: Waiting\r\n2e347fb21c2f: Waiting\r\ne42ed5b572d5: Waiting\r\nec233a1de041: Waiting\r\n6411108529fd: Pulling fs layer\r\n84fdf0769354: Waiting\r\ne6445b907cbb: Waiting\r\n```\r\n\r\n\r\n_Method used to create container save-file:_\r\n```\r\ndocker save \"my_registry\/my_image:latest\" -o my_containers.tar\r\n```\r\n\r\n(originally opened here https:\/\/github.com\/docker\/cli\/issues\/4607 )","comments":["This should work if you use the containerd image store integration (https:\/\/docs.docker.com\/storage\/containerd\/), but is not something we can implement for the graph-drivers.\r\n\r\nThe reason for this is that there's multiple ways to represent image-layers;\r\n\r\n- local (extracted, uncompressed) layers\r\n- layers for distribution (compressed; usually `tag+gzip`)\r\n\r\ngraph-drivers were designed to optimize for space; the local image-store used by the engine stores the raw files used in image layers. When pulling an image, image layers are downloaded (and their checksum (\"digest\") verified against the checksums in the image-manifest), after which they're extracted, and the _extracted_ tar is verified against the \"diff-id's\" listed in the image _config_. As the compressed layers are not used locally, they are discarded, but the daemon keeps metadata to associate \"compressed\" and \"extracted\" digests.\r\n\r\nWhen saving an image from the graph-driver store, a new archive is created containing the _uncompressed_ layers; saving\/loading will produce the same layers as were pulled, but the save\/load won't include information about their _compressed_ digests, because this information cannot be verified without the actual compressed artifacts (as pulled from the registry). Reconstructing the compressed layers is not possible due to compression algorithms not being 100% reproducible (they may be _most of the time_, but various factors, including CPU load, and CPU optimizations during compression can cause their checksum to differ). \r\n\r\nSo when loading an image into the image's local store, the daemon registers information about the _uncompressed_ layers, but not about their digests (as known in the registry); pulling the image therefore means the engine must download those layers, even if their extracted variant is already present.\r\n\r\nThe containerd image store is designed for production scenarios and reproducibility, but does so at the cost of disk-space (as disk space in production scenarios is considered \"cheap\"); the containerd image store preserves the downloaded (compressed) image layers _in addition_ to the extracted layers (which, if multiple snapshotters are used can be multiple copies). This design allows _both_ the \"extracted\" image (used at runtime) as well as the _compressed_ image (as distributed through registries) to be reproducible, which includes saving\/loading the image to an archive (as all downloaded layers are kept, it can just export those layers as-is, without re-compressing).\r\n\r\nGiven that the designs (as described above) are very different, it won't be possible to implement this for the graph-drivers, but while work on the containerd image store integration is still \"in progress\", you can already try it (see the https:\/\/docs.docker.com\/storage\/containerd\/ page in the docs for further details on that).","@thaJeztah Thank you for taking the time to write such a comprehensive and detailed reply to this.\r\n\r\nI'm not an expert in docker internals, so a lot of what you've written is hard to digest (excuse the pun), but I think I get the idea.\r\n\r\nFor a long time I've felt that the docker-registry was missing the uncompressed set of hashes, without these it's almost impossible to compare hashes in the client side with hashes in the registry, so proving local images match registry ones by hand is nigh-on impossible.\r\n\r\nSince compressing an image can result in varying hashes as you say, isn't there a fundamental problem with using these hashes as IDs? Wouldn't this by itself mean the uncompressed hashes should also be stored with the compressed ones, so as to detect collisions and make these performance\/storage savings?\r\n\r\nI will have a look at containerd image store integration, which sounds very promising, but on a production system I can't see it being relied upon until it's out of beta.","The other artefact I've noticed, that if the image in the registry isn't updated, when the client runs `docker-load` followed by `docker-pull`, no images are pulled, and when running `docker inspect my-repo\/my-image:latest` the local image has changed, reflecting the pulled manifest has been found on the remote server, matching the current `latest` manifest, and the `RepoTags` variable is now populated with a registry hash.\r\n\r\nSo there's clearly some logic from the registry which is proving the `docker load ; docker pull` does work when the image on the registry is unchanged.\r\n\r\nSecondly, once `docker pull` was run against the unchanged registry, updating the image in the registry by one layer causes only one layer to be pulled by `docker pull` on the client, proving that the issue can be resolved with some clever (or immutable) tag management...","Since I'm using a self-hosted private registry, I managed to solve my problem quite simply from the client side:\r\n```\r\ndocker load --input containers.tar\r\ndocker tag my-registry\/my-repo:latest my-registry\/my-repo:randomstring\r\ndocker push my-registry\/my-repo:randomstring\r\ndocker pull my-registry\/my-repo:latest\r\n# only 1 layer pulled.\r\n```\r\nAn alternative to this is I could tag the image in the registry with the version of the `container.tar` file, so the client wouldn't need push privileges.","Perhaps we are encountering the same problem https:\/\/github.com\/moby\/moby\/discussions\/46648","Just a quick update, that the method I've described above seems to work _some_ of the time, but definitely not all of the time. So there's not a decent workaround."],"labels":["status\/0-triage","kind\/enhancement","area\/images"]},{"title":"daemon\/info: add BuildKit version","body":"This is a very simple first implementation; it relies on the way that `bkversion.Version` is overwritten in an initializer, and it doesn't expose the built-in Dockerfile frontend version\/revision as that information is not currently available to BuildKit.\r\n\r\nThat being said, this is still a useful debug tool as-is given the increasing importance of the integrated BuildKit version for many users.\r\n\r\n```\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.44 (minimum version 1.12)\r\n  Go version:       go1.21.3\r\n  Git commit:       HEAD\r\n  Built:            Tue Oct 17 02:21:39 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.6\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n BuildKit:\r\n  Version:          v0.12.2\r\n  GitCommit:        6560bb937e8c\r\n```","comments":["Was there a specific reason for this PR?\r\n\r\nI'm a bit on the fence on this one; unlike the other components, BuildKit is compiled into the daemon, so (baring distro-built packages) the version will always match what's defined in the source code.\r\n\r\nAt the same time, switching builders may now become more confusing, e.g. `docker builder use ....` would switch builders, but the `docker version` output would show always show the built-in one.\r\n\r\nAnd for cases where multiple daemons \/ builders are available, the version is already included as part of `docker builder ls` \/ `docker buildx ls`, and `docker builder inspect` \/ `docker buildx inspect`;\r\n\r\n\r\n```bash\r\ndocker builder inspect\r\nName:          desktop-linux\r\nDriver:        docker\r\nLast Activity: 2023-10-16 13:54:44 +0000 UTC\r\n\r\nNodes:\r\nName:      desktop-linux\r\nEndpoint:  desktop-linux\r\nStatus:    running\r\nBuildkit:  v0.11.6+616c3f613b54\r\nPlatforms: linux\/arm64, linux\/amd64, linux\/amd64\/v2, linux\/riscv64, linux\/ppc64le, linux\/s390x, linux\/mips64le, linux\/mips64\r\nLabels:\r\n org.mobyproject.buildkit.worker.containerd.namespace: moby\r\n org.mobyproject.buildkit.worker.containerd.uuid:      5c0dab82-ebbe-4d13-912e-b1109aaecdaf\r\n org.mobyproject.buildkit.worker.executor:             containerd\r\n org.mobyproject.buildkit.worker.hostname:             docker-desktop\r\n org.mobyproject.buildkit.worker.moby.host-gateway-ip: 192.168.65.254\r\n org.mobyproject.buildkit.worker.network:              host\r\n org.mobyproject.buildkit.worker.selinux.enabled:      false\r\n org.mobyproject.buildkit.worker.snapshotter:          stargz\r\nGC Policy rule#0:\r\n All:           false\r\n Filters:       type==source.local,type==exec.cachemount,type==source.git.checkout\r\n Keep Duration: 172.8\u00b5s\r\n Keep Bytes:    2.764GiB\r\nGC Policy rule#1:\r\n All:           false\r\n Keep Duration: 5.184ms\r\n Keep Bytes:    20GiB\r\nGC Policy rule#2:\r\n All:        false\r\n Keep Bytes: 20GiB\r\nGC Policy rule#3:\r\n All:        true\r\n Keep Bytes: 20GiB\r\n```","This is mostly because I often struggle to figure out what BuildKit version is compiled into Docker Desktop with the containerd integration enabled, as we're using untagged builds for that.\r\n\r\nI'm hoping to start having a BuildKit commit available in `docker version` so when bug reports come in, we can determine what version of BuildKit was in use. Technically this can be determined from the Engine SHA (and I could write a script to hit the GitHub API to do so), but this is much more obvious.\r\n\r\nI agree that a `buildx inspect` will reveal the same information, but I do think there is some value in exposing this at the Engine level. I think that there is little risk of users being confused as to BuildKit version myself, as hopefully they're aware of any container builders they are using, but I'd defer to @crazy-max there.","Maybe this should be attached to the `Engine` struct to avoid any confusion?:\r\n\r\n```\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.44 (minimum version 1.12)\r\n  Go version:       go1.21.3\r\n  Git commit:       HEAD\r\n  Built:            Tue Oct 17 02:21:39 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n  BuildKit:         v0.12.2 (8d2625494a6a3d413e3d875a2ff7dd9b1ed1b1a9)\r\n containerd:\r\n  Version:          v1.7.6\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n","> At the same time, switching builders may now become more confusing, e.g. `docker builder use ....` would switch builders, but the `docker version` output would show always show the built-in one.\r\n\r\nI guess this is the same for `docker-init` as this can be overriden with `--init` but still shows the shipped version in `info`."],"labels":["status\/2-code-review","kind\/enhancement","area\/builder\/buildkit","area\/ux"]},{"title":"Check if firewalld is running by asking firewalld if it is running","body":"### Description\n\nThe firewalld integration for libnetwork currently probes for whether firewalld is running by testing whether the `org.fedoraproject.FirewallD1.getDefaultZone` D-Bus method can be called without error. While effective, it is technically abusing the interface to infer the state of firewalld from whether or not it is able to service D-Bus RPC calls.\r\n\r\nIn addition to its own bespoke interfaces, [firewalld exposes several properties](https:\/\/firewalld.org\/documentation\/man-pages\/firewalld.dbus.html#FirewallD1.Properties) through [the standard D-Bus property interface](https:\/\/dbus.freedesktop.org\/doc\/dbus-specification.html#standard-interfaces-properties). Notably, the `state` property signals whether firewalld is starting up, operational or failed (in an error state but still servicing D-Bus requests). We should switch to getting the `state` property, and do something sensible when the state is `INIT` or `FAILED`. Perhaps we could even subscribe to `org.freedesktop.DBus.Properties.PropertiesChanged` signals for the property and react to `INIT -> RUNNING` transitions instead of polling.","comments":[],"labels":["kind\/enhancement","area\/networking","area\/networking\/firewalld"]},{"title":"[DNM] TestLiveRestore: Wait after container removal","body":"Check if it's just some race condition between removing the container and actually decreasing the refcount.\r\n\r\nOpening a PR to have a CI run, because I can't reproduce this locally.\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/testing"]},{"title":"dockerd neglects to populate OCI Descriptor size field","body":"As background, I originally reported this issue in the buildkit repo: https:\/\/github.com\/moby\/buildkit\/issues\/4328\n\nSince that issue was closed yesterday I felt a bit crazy because on the one hand I can clearly see in my WIP registry implementation that manifests pushed to it by dockerd definitely **do not** contain the `size` field in the descriptors as [required by the OCI image spec](https:\/\/github.com\/opencontainers\/image-spec\/blob\/main\/descriptor.md?plain=1#L31-L35), yet on the other hand the issue doesn't seem reproducible pushing images to a local copy of distribution.\n\n@neersighted mentioned I should open a PR here if I can prove that the size field is missing from manifests on the wire during a `PUT \/v2\/<name>\/manifests\/<reference>` call so that's what I'm doing.\n\nHere are the details of my setup:\n\n* my WIP registry (named portfolio) running on localhost:13030\n* a local build of distribution running on localhost:5000 with the following config:\n```yaml\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: .\/var\/lib\/registry\nhttp:\n  addr: :5001\n  headers:\n    X-Content-Type-Options: [nosniff]\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\n```\n* mitmdump in reverse proxy mode listening at localhost:8080 pointed at localhost:13030 (portfolio):\n```bash\nmitmdump --mode reverse:http:\/\/127.0.0.1:13030@8080 --flow-detail 4 > portfolio.mitmproxy.flow\n```\n* mitmdump in reverse proxy mode listening at localhost:8081 pointed at localhost:5000 (distribution)\n```bash\nmitmdump --mode reverse:http:\/\/127.0.0.1:5000@8081 --flow-detail 4 > distribution.mitmproxy.flow\n```\n\nWith this setup I run the following docker build commands:\n\n```bash\n# aiming at the distribution mitmdump instance\ndocker buildx build --push .\/ -t 127.0.0.1:8081\/woof:meowmeow --no-cache\n# aiming at the portfolio mitmdump instance\ndocker buildx build --push .\/ -t 127.0.0.1:8080\/woof:meowmeow --no-cache\n```\n\n(I will upload a tarball with the resulting logs after submitting this issue since I am not using a web browser to write this description)\n\nThe relevant snippet of mitmdump output for portfolio (note the missing `size` filed in the `config` descriptor):\n```\n[08:27:34.419][127.0.0.1:39808] client connect\n[08:27:34.421][127.0.0.1:39808] server connect 127.0.0.1:13030\n127.0.0.1:39808: PUT http:\/\/127.0.0.1:13030\/v2\/woof\/manifests\/meowmeow\n    Host: 127.0.0.1:13030\n    User-Agent: docker\/24.0.6 go\/go1.21.1 git-commit\/1a7969545d kernel\/6.1.54-1-lts os\/linux arch\/amd64 UpstreamClient(Go-http-client\/1.1)\n    Content-Length: 714\n    Content-Type: application\/vnd.docker.distribution.manifest.v2+json\n    Accept-Encoding: gzip\n    Connection: close\n\n    {\n        \"config\": {\n            \"digest\": \"sha256:7bddfd52170f7ebde74e065ec731299f182d5fa332bbfbe8cd07331031b75ab9\",\n            \"mediaType\": \"application\/vnd.docker.container.image.v1+json\"\n        },\n        \"layers\": [\n            {\n                \"digest\": \"sha256:96526aa774ef0126ad0fe9e9a95764c5fc37f409ab9e97021e7b4775d82bf6fa\",\n                \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n                \"size\": 3401967\n            },\n            {\n                \"digest\": \"sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1\",\n                \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n                \"size\": 32\n            }\n        ],\n        \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\n        \"schemaVersion\": 2\n    }\n\n << 201 Created 0b\n    content-type: text\/plain; charset=utf-8\n    location: \/v2\/woof\/manifests\/meowmeow\n    docker-content-digest: sha256:5d3183be0559955ce465d519e0d6a6803249e20d1d7291febcb9b9ac3e5f1ccf\n    docker-distribution-api-version: registry\/2.0\n    content-length: 0\n    date: Fri, 13 Oct 2023 14:27:34 GMT\n\n[08:27:34.538][127.0.0.1:39808] server disconnect 127.0.0.1:13030\n[08:27:34.539][127.0.0.1:39808] client disconnect\n\n```\n\nThe relevant snippet of mitmdump output for distribution (note the presence of the `config.size` field here, in contrast with the above snippet):\n```\n[08:49:46.135][127.0.0.1:37552] client connect\n[08:49:46.136][127.0.0.1:37552] server connect 127.0.0.1:5000\n127.0.0.1:37552: PUT http:\/\/127.0.0.1:5000\/v2\/woof\/manifests\/meowmeow\n    Host: 127.0.0.1:5000\n    User-Agent: docker\/24.0.6 go\/go1.21.1 git-commit\/1a7969545d kernel\/6.1.54-1-lts os\/linux arch\/amd64 UpstreamClient(Go-http-client\/1.1)\n    Content-Length: 733\n    Content-Type: application\/vnd.docker.distribution.manifest.v2+json\n    Accept-Encoding: gzip\n    Connection: close\n\n    {\n        \"config\": {\n            \"digest\": \"sha256:e94c3024986a16ece2ea2d57034d97c91bf88662316c882f3ffdf8527010b6db\",\n            \"mediaType\": \"application\/vnd.docker.container.image.v1+json\",\n            \"size\": 816\n        },\n        \"layers\": [\n            {\n                \"digest\": \"sha256:96526aa774ef0126ad0fe9e9a95764c5fc37f409ab9e97021e7b4775d82bf6fa\",\n                \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n                \"size\": 3401967\n            },\n            {\n                \"digest\": \"sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1\",\n                \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n                \"size\": 32\n            }\n        ],\n        \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\n        \"schemaVersion\": 2\n    }\n\n << 201 Created 0b\n    Docker-Content-Digest: sha256:c68777b0ce58b075d8113abb5c9285599d6a203cbd9b9a75babfd486a07e7083\n    Docker-Distribution-Api-Version: registry\/2.0\n    Location: http:\/\/127.0.0.1:5000\/v2\/woof\/manifests\/sha256:c68777b0ce58b075d8113abb5c9285599d6a203cbd9b9a75babfd486a07e7083\n    X-Content-Type-Options: nosniff\n    Date: Fri, 13 Oct 2023 14:49:46 GMT\n    Content-Length: 0\n    Connection: close\n\n[08:49:46.151][127.0.0.1:37552] server disconnect 127.0.0.1:5000\n[08:49:46.151][127.0.0.1:37552] client disconnect\n```\n\nI believe that if I build the image without cached layers and push it first to my WIP registry I can get a manifest where the layer descriptors also don't include the `size` field, but the above logs should be enough to establish the fact that this is happening (and i'm not crazy :smile:)\n\nFor what it's worth, I would like to share my WIP registry so others can more easily reproduce this but the dev env setup is somewhat complicated by the need for a postgresql wire compatible DB and a live S3 compatible API. I'll eventually implement support for sqlite metadata storage and local file system bulk data storage but that could be a while.\n\nIn the short term I intend to work on a cloud deployment of this that I will be happy to share with maintainers who want to see the issue for themselves. (probably won't be ready until next week)\n\nI've also noticed some differences between distribution and my implementation in terms of the `\/v2\/` response (eg distribution includes an empty json in the response body, mine doesn't) and various headers. I suspect that if I align the behavior of my registry more closely to distribution this problem _might_ go away.\n\nBut I'm reporting this bug anyway because it does definitely seem like a bug somewhere in dockerd or one of the registry client libraries it might be using to exclude the descriptor size field under any circumstance.\n","comments":["Here are the mitmdump logs:\r\n[mitmdump-flows.tar.gz](https:\/\/github.com\/moby\/moby\/files\/12895063\/mitmdump-flows.tar.gz)","oh shoot, since i didn't file this issue in the web ui i neglected to include some important info:\r\n\r\n# `docker info`\r\n\r\n```\r\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.2\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.2.3\r\n    Path:     \/home\/wayne\/.docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 9\r\n  Running: 9\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 319\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 091922f03c2762540fd057fba91260237ff86acb.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.54-1-lts\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 46.29GiB\r\n Name: thing3\r\n ID: RXZW:VO3X:A36S:435C:5JOY:W4TO:S3CH:MTEE:QLQN:NJB3:CKEE:UG2W\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 96\r\n  Goroutines: 118\r\n  System Time: 2023-10-13T09:28:43.946004432-06:00\r\n  EventsListeners: 0\r\n HTTP Proxy: http:\/\/127.0.0.1:8080\r\n Username: waynr\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n# `docker version`\r\n\r\n```\r\nClient:\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.21.1\r\n Git commit:        ed223bc820\r\n Built:             Sat Sep 30 15:48:58 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.21.1\r\n  Git commit:       1a7969545d\r\n  Built:            Sat Sep 30 15:48:58 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.6\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb.m\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```","The mechanisms that generate the descriptors are different for config vs. layer blobs; see my dive here: https:\/\/github.com\/moby\/buildkit\/issues\/4328#issuecomment-1760504362\r\n\r\nIn the case of the config descriptor, the descriptor is generated in distribution\/distribution, in the client `distribution.BlobStore` implementation: https:\/\/github.com\/distribution\/distribution\/blob\/ebba01efeacc31a55640ee1efc858ea4e327c479\/registry\/client\/repository.go#L714-L733","Here's a new set of captures, but this time I made sure to clear out the local storage in each registry (I think my previous mitmdumps excluded actual upload of blobs for distribution because it had cached data from previous pushes):\r\n[captures.tar.gz](https:\/\/github.com\/moby\/moby\/files\/12895496\/captures.tar.gz)\r\n\r\nThis also includes tcpdump captures as per @neersighted's request in slack DM. The tcpdump captures were grabbed using the following command:\r\n```bash\r\n# for distribution\r\ntcpdump -i lo -XX -s 0 'tcp port 5001 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)' | tee distribution.pcap\r\n\r\n# for portfolio\r\ntcpdump -i lo -XX -s 0 'tcp port 13030 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)' | tee portfolio.pcap\r\n```","@waynr the PCAP files appear corrupt (at least according to wireshark), and the new mitmproxy flow still shows layers already present in the distribution\/distribution registry. ","@neersighted here's another attempt, but this time using `tcpdump -i lo -s 0 -w <filename> 'tcp port <port>'`. I still don't see the blob upload happening in the mitmdump output but I did clear out the `filesystem` storage driver directory and restarted the distribution server binary before collecting it.\r\n[mitmdump-flows_2.tar.gz](https:\/\/github.com\/moby\/moby\/files\/12896423\/mitmdump-flows_2.tar.gz)\r\n\r\nI did open the tcpdump captures in wireshark this time to verify that works. The tcpdump capture looks more reliable than the mitmproxy capture so I'll probably leave the latter out of future captures.","Okay one more try...one of my previous tcpdump captures was bad, this time I've checked in wireshark that both the distribution and the portfolio captures include all the expected http exchanges.\r\n[pcap-1.tar.gz](https:\/\/github.com\/moby\/moby\/files\/12896933\/pcap-1.tar.gz)\r\n","~~One other major difference I am noticing when dockerd pushes images to distribution vs portfolio is that for distribution the uploads look to be [monolithic POST uploads](https:\/\/github.com\/opencontainers\/distribution-spec\/blob\/main\/spec.md#single-post) whereas for portfolio they look to be [chunked POST-PATCH-PUT uploads](https:\/\/github.com\/opencontainers\/distribution-spec\/blob\/main\/spec.md#pushing-a-blob-in-chunks). This might be easier to use as a hint for where to start looking for what causes the difference in manifest. ie look for where the image push includes one or more PATCH requests.~~\r\n\r\nActually, forget this -- I was using mitmproxy again and compared with tcpdump it doesn't seem to show all the requests strangely. In the tcpdump capture I **do** see HTTP PATCH requests for distribution. :facepalm: ","Okay, I figured out what I need to do in my registry to get the `config.size` field from dockerd!\r\n\r\nThe problem seems to be that I was using the empty string (`\"\"`) to populate my response bodies which was causing the http framework I use to automatically add the `content-type: text\/plain; charset=utf-8` header. When I replace `\"\"` as the response body with `Json(\"\")` this causes the framework to use `content-type: application\/json`.\r\n\r\nI'm guessing that returning the wrong content type in the `HEAD \/v2\/<repository>\/blobs\/<digest>` request that immediately preceeds the `PUT \/v2\/<repository>\/manifests` request causes dockerd to follow a bad code path when generating the `config` descriptor.\r\n\r\nWhile I acknowledge that the behavior of my code was wrong (I mostly raised this issue to understand _what_ i was doing wrong), I still think it's wrong for dockerd to have a descriptor-generating code path that doesn't include a `size` field so it's probably a good idea to keep this issue open.","One more quick update!\r\n\r\nSo in my previous comment I mentioned replacing plain text body in all my registry's API endpoint responses with `Json(\"\")`. This led to the body size being calculated as 2 bytes for all requests that didn't have an explicit larger response body set.\r\n\r\nI initially assumed that just having `content-type: application\/json` in the `HEAD \/v2\/<repository>\/blobs\/<digest>` responses is what led to the manifest pushed by `PUT \/v2\/<repository>\/manifests` having a size field in its config descriptor. Then I realized something strange - the config descriptor's size field was being set to 2! I happened to know the config was actually 816 bytes since that's what the manifest being pushed to distribution showed; also, 2 bytes is a totally unrealistic size for a manifest config.\r\n\r\nI fixed the route handler in my app for `HEAD \/v2\/<repository>\/blobs\/<digest>` so that it now actually returns the size in bytes of the blob represented by `<digest>` in the `content-length` response header and the manifest received from dockerd is also now correct! (in the example docker image i was working with, 816 is the correct size of the image config).\r\n\r\nThis suggests that in dockerd's image pushing code it sets the config descriptor's size field from the `HEAD \/v2\/<repository>\/blobs\/<digest>` response's content-length header. If there is no content-length header in that response, it seems to get set to 0. I believe that because of [the `omitempty` json tag](https:\/\/github.com\/distribution\/distribution\/blob\/ebba01efeacc31a55640ee1efc858ea4e327c479\/blobs.go#L71) if the size is set to 0 it is considered \"empty\" and therefore excluded entirely from the resulting json representation.\r\n\r\nIs any of this a bug in dockerd? I don't know. I would think that it would try to calculate the size of the image config directly rather than rely on the registry response header as described above. The [OCI Distribution spec doesn't even say anything](https:\/\/github.com\/opencontainers\/distribution-spec\/blob\/main\/spec.md#checking-if-content-exists-in-the-registry) about `HEAD \/v2\/<repository>\/blobs\/<digest>` responses including a content-length header. (I plan to open a PR to update the spec)","It appears https:\/\/github.com\/docker\/for-linux\/issues\/1296 was a duplicate\/the first report of this, but the reporter chose not to open an issue once they realized that their registry was returning the 'wrong' size."],"labels":["status\/0-triage","kind\/bug"]},{"title":"c8d: include progress of \"non-blob\" content during push and pull","body":"- related: https:\/\/github.com\/moby\/moby\/issues\/41838\r\n- related https:\/\/github.com\/moby\/moby\/pull\/46412#issuecomment-1708188715\r\n- related https:\/\/github.com\/moby\/moby\/pull\/46581#discussion_r1344121805\r\n\r\nWith the containerd integration, pushing and pulling images may include more content-types than just layers;\r\n\r\n- manifest-index\r\n- image-manifest\r\n- image-config\r\n- SBOM\r\n\r\nWe currently discard progress output for these for consistency with the non-containerd-snapshotter output (https:\/\/github.com\/moby\/moby\/pull\/46412, https:\/\/github.com\/moby\/moby\/pull\/46581); most of these (JSON) types are usually small, and may not provide much \"progress\", but downloads may still stall or timeout, and we're currently discarding this information.\r\n\r\n\r\n> So, I'm considering if this is information we _would_ want to show at some point (after all, if we're pushing these, why would we not show progress?)\r\n>\r\n> That said, maybe the current presentation of these isn't ideal, if they can be mistaken for layers. Wondering if there would be a way to fix the presentation, or have additional metadata so that the client can decide whether or not to show these (without discarding the actual progress information).\r\n>\r\n>\r\n> If we _do_ decide to discard this information, then at least it may be good putting a comment in this switch block, to explain why we're skipping these (possibly with either a tracking ticket or a link to the conversation on this PR)\r\n\r\nWhile it's ok to keep the status quo and align the output with \"non-containerd-snapshotter\", we should include progress for _all_ content in push and pull operations output, and include that as part of the (re)design of these commands.\r\n","comments":["\/cc @jalonsogo"],"labels":["area\/distribution","containerd-integration","area\/ux"]},{"title":"Fix  dockercore\/engine-pull-all-test-fixture","body":"### Description\n\nThis image is used in our integration tests but it contains an error:\r\n\r\n```console\r\n$ docker buildx imagetools inspect dockercore\/engine-pull-all-test-fixture --raw | jq \r\n{\r\n  \"schemaVersion\": 2,\r\n  \"mediaType\": \"application\/vnd.docker.distribution.manifest.list.v2+json\",\r\n  \"manifests\": [\r\n    {\r\n      \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n      \"size\": 424,\r\n      \"digest\": \"sha256:1db73f3610a525e10f80c62326bfbab538d9eda3c893d3cdd4b91f65ed711998\",\r\n      \"platform\": {\r\n        \"architecture\": \"amd64\",\r\n        \"os\": \"linux\"\r\n      }\r\n    },\r\n    {\r\n      \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n      \"size\": 427,\r\n      \"digest\": \"sha256:acd186e17ca720587dc000dd8217e8a8c201f39f470bb088fde5132492676f99\",\r\n      \"platform\": {\r\n        \"architecture\": \"arm64\",\r\n        \"os\": \"linux\",\r\n        \"variant\": \"v8\"\r\n      }\r\n    },\r\n    {\r\n      \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n      \"size\": 427,\r\n      \"digest\": \"sha256:8116ed3458950def2a355933273866d586c93776cf16abb84347bfd727c9f714\",\r\n      \"platform\": {\r\n        \"architecture\": \"arm\",\r\n        \"os\": \"linux\",\r\n        \"variant\": \"v7\"\r\n      }\r\n    },\r\n    {\r\n      \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n      \"size\": 427,\r\n      \"digest\": \"sha256:c051972bddc3b34420e3792e2d0d35cbf6b7d4b8d9918b740294a8e7c3eb8a04\",\r\n      \"platform\": {\r\n        \"architecture\": \"ppc64le\",\r\n        \"os\": \"linux\"\r\n      }\r\n    },\r\n    {\r\n      \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n      \"size\": 427,\r\n      \"digest\": \"sha256:cf4d184f7ca8072d5047725af02a081b4e49b96936fd45446d8b8cee62f7eb83\",\r\n      \"platform\": {\r\n        \"architecture\": \"s390x\",\r\n        \"os\": \"linux\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe `linux\/amd64` manifest has a size of 424 but looking at the actual manifest size we get 524:\r\n\r\n```console\r\n$ docker buildx imagetools inspect dockercore\/engine-pull-all-test-fixture@sha256:1db73f3610a525e10f80c62326bfbab538d9eda3c893d3cdd4b91f65ed711998 --raw | wc -c   \r\n524\r\n```\r\n\r\nThis difference in sizes makes some of the tests fail with containerd","comments":[],"labels":["status\/0-triage"]},{"title":"libnet: Revert \"Only check if route overlaps routes with scope: LINK\"","body":"**- What I did**\r\n\r\n- Resolve moby\/moby#46615.\r\n- Revert commit ee9e52676409b2e428a816ac59f42f5ba61089dd from https:\/\/github.com\/moby\/moby\/pull\/42598\r\n- reopens https:\/\/github.com\/moby\/moby\/issues\/41525\r\n- reopens https:\/\/github.com\/moby\/moby\/issues\/33925\r\n\r\nRoute scope is used by the kernel to choose what source IP address should be used when establishing an outbound connection. As such, filtering routes based on their scope doesn't make sense. It happened to work for the original contributor only by chance.\r\n\r\nI didn't take much time to look into all the issues related to `CheckRouteOverlaps` but it'd be probably better to add an option to skip this check if the user wants it.\r\n\r\n**- How to verify it**\r\n\r\nSee the reproducer in https:\/\/github.com\/moby\/moby\/issues\/46615\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![](https:\/\/i.natgeofe.com\/n\/70f8fe9c-d289-4965-9e07-3c430fefba08\/02_animal_day_gallery_prairie_dog_2x3.jpg)\r\n","comments":["I tend to agree with you, but I still disagree on a few points and on not reverting to the original heuristics.\r\n\r\n>  The vast majority of the IPv4 address space is going to be routable (to some interface) on most systems, simply by virtue of having a default route. As pointed out in #33925 (comment) a routing table with routes for 0.0.0.0\/1 and 128.0.0.0\/1 is functionally indistinguishable from a routing table with a default route\r\n\r\nThey are functionally indistinguishable but they don't convey the same meaning as the default route is the fallback route used when the endpoint isn't aware of any specific routing rules for a given address. I highly doubt anyone or any networking software set routes for `0.0.0.0\/1` and `128.0.0.0\/1`.\r\n\r\n> Therefore the only way to be sure is to have the user to tell us which prefixes are available, by configuring the address pools.\r\n\r\nThe original reporter of #46615 doesn't mention they configured the address pools. You will tell me they should, but I believe this is where these heuristics are useful. Otherwise, I agree with you: once a user configure the address pool, it's their responsibility to make sure it fits their needs.\r\n\r\n> FindAvailableNetwork must therefore use heuristics to determine whether a network prefix is *probably* available, given incomplete information, and can never be free of false positives or false negatives.\r\n> #42598 tweaked the heuristics to be biased more towards prefixes being available when there is uncertainty.\r\n\r\nThe heuristics used by `CheckRouteOverlaps` prior to ee9e5267 were trying to avoid false negatives, and this is better IMO as it avoids shadowing an existing non-default route (or a subset of it). Moreover, the route scope doesn't indicate anything about the route itself, it only tells the kernel what source address it should pick. Said another way, #42598 is conflating the route scope with something it isn't. They could have picked any other property randomly to achieve the same result.\r\n\r\n> Someone is going to be angry whenever the daemon gets this wrong.\r\n\r\nWith ee9e5267 or not, the heuristics are still wrong for routes with scope link. And the only way to get these heuristics right is actually to get rid of them. _But_ I also think it's useful to heuristically determine whether a prefix is available on a best-effort basis (ie. by avoiding false positive).\r\n\r\nHence my proposal to add an option to enable\/disable this check when it makes sense. Thinking more about this, and based on your comment, I think it should be automatically defined when the address pool is user-defined, and still make it available to users to ease integrators' job (eg. I'm pretty sure DD suffers from this check but I need to double-check tomorrow).\r\n\r\nEDIT: Also I was considering backporting this fix to 24 since the link-scoped check **doesn't make sense**. But obviously the option I'm proposing would be made available only in a new major release.","IIRC, the issue this was trying to resolve was \"could not find _ANY_ available, non-overlapping port-range\", which resulted in the daemon failing to start (if a VPN was used), causing lots of grieve. IF we are to decide making changes (this PR, or variant thereof), should we treat that as a non-fatal error (we tried, but failed, so let's assume we never tried?)\r\n\r\nThe attempt to find a non-overlapping range was to make things easier for the user (less chance on overlapping range), but if there's a risk of invalid \/ false-positive detection, it means we're actually making things work, and denying use of ranges that actually _can_ be used (and now users have to work around invalid detection on the daemon side).\r\n\r\n\r\n\r\n","[The scope of a route says a lot about the network topology in relation to the host.](https:\/\/superuser.com\/a\/1389304) The prefixes of the link-scoped routes forms the set of prefixes which are directly reachable from the host. Filtering out prefixes which overlap the host's local subnets is an effective heuristic to avoid shadowing the LAN(s) the host is connected to. In contrast, we cannot safely assume any semantics for prefixes in wider-scoped routes because they could have come from anywhere and the user may or may not care about them, so filtering out prefixes which overlap wider-scoped route prefixes will tend to filter out more prefixes than necessary. [This is exactly the rationale used when the solution was proposed.](https:\/\/github.com\/moby\/moby\/issues\/33925#issuecomment-702470693:~:text=The%20code%20should,router%20somewhere%20remote)\r\n\r\nI'm starting to warm up to the idea of applying magic filtering to the IPAM pools iff the IPAM pools are not explicitly configured. That would align the logic for explicitly-configured pools with an explicitly-configured network prefix: trust the user and skip overlap checks. We could then make the heuristics as aggressive as we want since only users who don't bother telling the daemon what they want will be impacted. Error messaging could be improved to instruct the users to configure the daemon's pools explicitly if all the prefixes in the default pool are filtered out by our heuristics.","I'm the author of #46615 and as @akerouanton commented, we do not specify any address pools in the daemon.json. Our customers are not docker experts and the way it was behaving before was ideal for our needs. Now they cannot access to their application because they have configured routes but not the daemon.json since we have not documented it as, so far, there was no need for it.\r\nIf we could have an option to get back to the original behavior, or only apply the new one when the daemon.json is configured, that would be satisfying for us.","Admittedly my claim that link scope is only used to select source IP isn't totally true. Link scope carry _some_ information about the route but that's mostly informational and I still believe it only provides very partial information about the overall LAN topology.\r\n\r\nFor instance, here's the routing table currently set inside DD's VM:\r\n\r\n```\r\n% docker run --privileged --net=host -it alpine ip route\r\n127.0.0.0\/8 dev lo scope host \r\n172.17.0.0\/16 dev docker0 scope link  src 172.17.0.1 \r\n172.18.0.0\/16 dev br-c3269442db8e scope link  src 172.18.0.1 \r\n192.168.64.0\/24 dev eth1 scope link  src 192.168.64.134 \r\n192.168.65.0\/24 dev eth0 \r\n192.168.65.7 dev services1 \r\n```\r\n\r\nCurrently, the Engine would happily select `192.168.65.0\/24` if users specify the `default-address-pools` parameter to something that overlaps (eg. they would do that if the default value conflicts with the host's networking setup).\r\n\r\nAt this point I think we shouldn't merge this PR alone. We need a proper way to let users bypass this check if they want to, and setting `default-address-pools` should be enough. And we need a way for integrators to forcefully make it sure this check is enabled. I'll draft a PR for that.\r\n\r\nAs for backporting this revert to 24.x, I now think it's a wrong idea as we'd introduce a regression with no workaround in a minor version.","> Link scope carry _some_ information about the route but [...] I still believe it only provides very partial information about the overall LAN topology.\r\n\r\nI agree that it is only partial information. Even so, it's some of the highest-quality data about the network topology available to the daemon without user input. Perhaps in that follow-up PR we could also expose configuration knobs for systems integrators to tune the routing table scope heuristic?\r\n\r\n-----\r\n\r\n> For instance, here's the routing table currently set inside DD's VM:\r\n\r\n<details><summary>Curious, as my DD VM's route table does not have a global-scope route for any RFC 1918 prefix. I wonder why.<\/summary>\r\n<p>\r\n\r\nDocker Desktop for Mac, Version 4.23.0 (120376)\r\n\r\n```console\r\n$ docker run --rm -it --net=host alpine\r\n\/ # apk -U add iproute2\r\nfetch https:\/\/dl-cdn.alpinelinux.org\/alpine\/v3.16\/main\/x86_64\/APKINDEX.tar.gz\r\nfetch https:\/\/dl-cdn.alpinelinux.org\/alpine\/v3.16\/community\/x86_64\/APKINDEX.tar.gz\r\n(1\/12) Installing libcap (2.64-r1)\r\n(2\/12) Installing libbz2 (1.0.8-r1)\r\n(3\/12) Installing fts (1.2.7-r1)\r\n(4\/12) Installing xz-libs (5.2.5-r1)\r\n(5\/12) Installing libelf (0.186-r0)\r\n(6\/12) Installing libmnl (1.0.5-r0)\r\n(7\/12) Installing iproute2-minimal (5.17.0-r0)\r\n(8\/12) Installing libnftnl (1.2.3-r0)\r\n(9\/12) Installing iptables (1.8.8-r1)\r\n(10\/12) Installing iproute2-tc (5.17.0-r0)\r\n(11\/12) Installing iproute2-ss (5.17.0-r0)\r\n(12\/12) Installing iproute2 (5.17.0-r0)\r\nExecuting iproute2-5.17.0-r0.post-install\r\nExecuting busybox-1.35.0-r15.trigger\r\nOK: 11 MiB in 26 packages\r\n\/ # ip -d route\r\nunicast default via 192.168.65.5 dev eth0 proto boot scope global\r\nunicast 172.17.0.0\/16 dev docker0 proto kernel scope link src 172.17.0.1\r\nunicast 172.18.0.0\/16 dev br-dbf277990947 proto kernel scope link src 172.18.0.1 linkdown\r\nunicast 172.19.0.0\/16 dev br-61d6db49b86b proto kernel scope link src 172.19.0.1 linkdown\r\nunicast 172.20.0.0\/16 dev br-18b4d4f6f194 proto kernel scope link src 172.20.0.1 linkdown\r\nunicast 172.21.0.0\/16 dev br-647a1326dd2b proto kernel scope link src 172.21.0.1 linkdown\r\nunicast 172.22.0.0\/16 dev br-b5fbb3e87037 proto kernel scope link src 172.22.0.1 linkdown\r\nunicast 172.23.0.0\/16 dev br-513961aae86f proto kernel scope link src 172.23.0.1 linkdown\r\nunicast 172.25.0.0\/16 dev br-2b31c1188926 proto kernel scope link src 172.25.0.1 linkdown\r\nunicast 192.168.65.5 dev eth0 proto kernel scope link src 192.168.65.4\r\n```\r\n\r\n<\/p>\r\n<\/details> \r\n","I'm also affected by this, is there a specific reason why this isn't moving forward?"],"labels":["status\/2-code-review","area\/networking","kind\/bugfix"]},{"title":"Regression: docker network conflicts with host routes when creating a new network","body":"### Description\r\n\r\ndocker release 23 introduced a regression in its choice of the network pool which can conflict with the host existing routes. \r\nPrevious versions of docker (until release 20), would not create a network route that overlaps with the host one\r\n\r\n\r\n### Reproduce\r\n\r\n1) To start from a fresh installation:\r\n  -  All docker networks have been removed, the internal network database file `docker\/network\/files\/local-kv.db` is removed\r\n  -  docker daemon is stopped\r\n  -  docker0 bridge is removed : `ip link del docker0`\r\n2) Create a route that will conflict with docker default ones: `ip route add 172.18.13.0\/24 via xxx.xxx.xxx.xxx dev yyy`\r\n3) Start docker daemon. By default it should create the route `172.17.0.0\/16 dev docker0`\r\n4) create a docker network : docker network create net1 => This command creates a route on network 172.18.0.0\/16 which conflicts with the route from step 2. On previous versions of docker, the command creates a route on network 172.19.0.0\/16  \r\n\r\n\r\n### Expected behavior\r\n\r\nWe should get back to the former behavior of docker and not create a route that conflicts with the host one\r\n\r\n---------------------------------------------------------------------------\r\nResult on docker 20.10.14 (correct):\r\n\r\n```\r\n# ip route\r\n172.17.0.0\/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown\r\n172.18.13.0\/27 via 192.168.2.221 dev eth0\r\n**172.19.0.0\/16 dev br-c1e73b4c4b98** proto kernel scope link src 172.19.0.1 linkdown\r\n192.168.0.0\/23 via 10.4.0.104 dev eth1\r\n192.168.2.0\/23 dev eth0 proto kernel scope link src 192.168.2.13\r\n```\r\n\r\nResult on docker 24.0.6 (incorrect):\r\n\r\n```\r\n# ip route\r\n172.17.0.0\/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown\r\n**172.18.0.0\/16 dev br-5d8b3ebd612a** proto kernel scope link src 172.18.0.1 linkdown\r\n172.18.13.0\/24 via 192.168.2.221 dev eth0\r\n192.168.2.0\/24 dev eth0 proto kernel scope link src 192.168.2.231\r\n```\r\n\r\n### docker version\r\n\r\n```bash\r\n# docker version\r\nClient:\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.19.13\r\n Git commit:        24.0.6\r\n Built:             unknown-buildtime\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.19.13\r\n  Git commit:       buildroot\r\n  Built:\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:\r\n runc:\r\n  Version:          1.1.7\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version:\r\n runc version:\r\n init version: N\/A\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.130-grsec\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 1.929GiB\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nDocker debug logs on 20.10.14:\r\n\r\ndocker network create net1:\r\n\r\n```\r\nDEBU[2023-10-11T05:21:25.820794534Z] Calling HEAD \/_ping\r\nDEBU[2023-10-11T05:21:25.821691142Z] Calling POST \/v1.41\/networks\/create\r\nDEBU[2023-10-11T05:21:25.821786007Z] form data: {\"Attachable\":false,\"CheckDuplicate\":true,\"ConfigFrom\":null,\"ConfigOnly\":false,\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\":{\"Config\":[],\"Driver\":\"default\",\"Options\":{}},\"Ingress\":false,\"Internal\":false,\"Labels\":{},\"Name\":\"net1\",\"Options\":{},\"Scope\":\"\"}\r\nDEBU[2023-10-11T05:21:25.822143163Z] Allocating IPv4 pools for network net1 (077c6bdda9fc2a5f54749c4e8c7187f278f2a28b0978dfddba1937b4a6104d6d)\r\nDEBU[2023-10-11T05:21:25.822157053Z] RequestPool(LocalDefault, , , map[], false)\r\nDEBU[2023-10-11T05:21:25.822319403Z] RequestPool(LocalDefault, , , map[], false)\r\nDEBU[2023-10-11T05:21:25.822378464Z] ReleasePool(LocalDefault\/172.18.0.0\/16)\r\nDEBU[2023-10-11T05:21:25.822392003Z] RequestAddress(LocalDefault\/172.19.0.0\/16, <nil>, map[RequestAddressType:com.docker.network.gateway])\r\nDEBU[2023-10-11T05:21:25.822403238Z] Request address PoolID:172.19.0.0\/16 App: ipam\/default\/data, ID: LocalDefault\/172.19.0.0\/16, DBIndex: 0x0, Bits: 65536, Unselected: 65534, Sequence: (0x80000000, 1)->(0x0, 2046)->(0x1, 1)->end Curr:0 Serial:false PrefAddress:<nil>\r\nDEBU[2023-10-11T05:21:25.822667093Z] Did not find any interface with name br-077c6bdda9fc: Link not found\r\nDEBU[2023-10-11T05:21:25.822712613Z] Setting bridge mac address to 02:42:68:61:e3:1d\r\nDEBU[2023-10-11T05:21:25.826315409Z] Assigning address to bridge interface br-077c6bdda9fc: 172.19.0.1\/16\r\nDEBU[2023-10-11T05:21:25.830407574Z] \/usr\/sbin\/iptables, [--wait -t nat -C POSTROUTING -s 172.19.0.0\/16 ! -o br-077c6bdda9fc -j MASQUERADE]\r\nDEBU[2023-10-11T05:21:25.835525181Z] \/usr\/sbin\/iptables, [--wait -t nat -I POSTROUTING -s 172.19.0.0\/16 ! -o br-077c6bdda9fc -j MASQUERADE]\r\n```\r\n\r\nDocker debug logs on 24.0.6:\r\n\r\ndocker network create net1:\r\n\r\n```\r\nDEBU[2023-10-11T05:16:21.771289969Z] Calling HEAD \/_ping\r\nDEBU[2023-10-11T05:16:21.772811402Z] Calling POST \/v1.43\/networks\/create\r\nDEBU[2023-10-11T05:16:21.772922134Z] form data: {\"Attachable\":false,\"CheckDuplicate\":true,\"ConfigFrom\":null,\"ConfigOnly\":false,\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\":{\"Config\":[],\"Driver\":\"default\",\"Options\":{}},\"Ingress\":false,\"Internal\":false,\"Labels\":{},\"Name\":\"net1\",\"Options\":{},\"Scope\":\"\"}\r\nDEBU[2023-10-11T05:16:21.773263501Z] Allocating IPv4 pools for network net1 (c1c835401b07ddced9005834e4defbf96b0d1d13dea03787a7a11e801d28113f)\r\nDEBU[2023-10-11T05:16:21.773279797Z] RequestPool(LocalDefault, , , map[], false)\r\nDEBU[2023-10-11T05:16:21.773456142Z] RequestAddress(LocalDefault\/172.18.0.0\/16, <nil>, map[RequestAddressType:com.docker.network.gateway])\r\nDEBU[2023-10-11T05:16:21.773477392Z] Request address PoolID:172.18.0.0\/16 Bits: 65536, Unselected: 65534, Sequence: (0x80000000, 1)->(0x0, 2046)->(0x1, 1)->end Curr:0 Serial:false PrefAddress:invalid IP\r\nDEBU[2023-10-11T05:16:21.773527441Z] Did not find any interface with name br-c1c835401b07: Link not found\r\nDEBU[2023-10-11T05:16:21.773543840Z] Setting bridge mac address to 02:42:96:ef:5c:6c\r\nDEBU[2023-10-11T05:16:21.774850494Z] Assigning address to bridge interface br-c1c835401b07: 172.18.0.1\/16\r\nDEBU[2023-10-11T05:16:21.775950270Z] \/usr\/sbin\/iptables, [--wait -t nat -C POSTROUTING -s 172.18.0.0\/16 ! -o br-c1c835401b07 -j MASQUERADE]\r\nDEBU[2023-10-11T05:16:21.779158811Z] \/usr\/sbin\/iptables, [--wait -t nat -I POSTROUTING -s 172.18.0.0\/16 ! -o br-c1c835401b07 -j MASQUERADE]\r\nDEBU[2023-10-11T05:16:21.781776343Z] \/usr\/sbin\/iptables, [--wait -t nat -C DOCKER -i br-c1c835401b07 -j RETURN]\r\nDEBU[2023-10-11T05:16:21.783920686Z] \/usr\/sbin\/iptables, [--wait -t nat -I DOCKER -i br-c1c835401b07 -j RETURN]\r\n```","comments":["- sounds like related to this change (which is in 20.10.15 and up) https:\/\/github.com\/moby\/moby\/pull\/43360\r\n\r\n\/cc @akerouanton ","@dtronche  did you find a workaround? or just downgrade docker to the old version before this buildx stuff?","> @dtronche did you find a workaround? or just downgrade docker to the old version before this buildx stuff?\r\n\r\nOur platform is on a dedicated OS which we build using buildroot, so we have patched docker sources to get back to the old behavior.","I appreciate your quick reply!\r\n\r\n> so we have patched docker sources to get back to the old behavior.\r\n\r\nOkay, that's crazy. But docker is only a small indie company with measly $200M in funding so what can we do :rofl: \r\n\r\n"],"labels":["status\/0-triage","kind\/bug","area\/networking","version\/23.0","version\/24.0"]},{"title":"c8d: define exported const for dangling-image prefix","body":"> For a followup: we should find a new home for this and use the same const in the builder wrapper too\r\n\r\n_Originally posted by @rumpl in https:\/\/github.com\/moby\/moby\/pull\/46595#discussion_r1351706499_\r\n            ","comments":[],"labels":["area\/images","kind\/refactor","containerd-integration"]},{"title":"c8d: Implement max concurrency uploads\/downloads","body":"### Description\n\ndockerd has two limits for max concurrent downloads\/uploads:\r\n\r\n```\r\n--max-concurrent-downloads int            Set the max concurrent downloads (default 3)\r\n--max-concurrent-uploads int              Set the max concurrent uploads (default 5)\r\n```\r\n\r\nThese limits should be global (meaning if one pull already downloads 3 layers, a second pull should wait for them to be over). \r\n\r\nWith containerd this is not possible as is, we can define download\/upload limits but they are not global, the limits can only be defined per action.\r\n\r\nWe should, of course, implement this. We might need to wait for us to update containerd enough so that we can use the transfer service.\r\n\r\n","comments":[],"labels":["status\/0-triage","kind\/feature","containerd-integration"]},{"title":"removed\/inserted USB devices do not work stay in restart loop and are removed","body":"### Description\n\nI was directed here by https:\/\/github.com\/docker\/compose\/issues\/11075\r\n\r\n-----\r\n\r\n\r\nI have a docker compose file that runs service that read USB serial device (cheap usb to serial GPS on a sailing boat). When that device \"\/dev\/ttyUSB0\" is unplugged the container will be removed and will no recover when that USB replugged and `\/dev\/ttyUSB0` reappears. \r\n\r\n\r\nIt would be essential that this container will not be removed from running list and stays in restart loop - thus enabling recovery\/0-maintenance after reboots\/disconnects\r\n\r\ncompose file is this:\r\n```yaml\r\nversion: \"3.7\"\r\nservices:\r\n backend_nmea0183serialscrape:\r\n   # unnecessary things omitted\r\n   restart: always\r\n   devices:\r\n     - \/dev\/ttyUSB0:\/dev\/ttyUSB0:rw\r\n   device_cgroup_rules:\r\n     # https:\/\/stackoverflow.com\/a\/62758958\/2514290\r\n     - \"c 188:* rmw\"\r\n     - \"c 166:* rmw\"\r\n     - \"c 4:* rmw\"\r\n   group_add:\r\n     - dialout\r\n   volumes:\r\n     - \/run\/udev:\/run\/udev:ro\r\n```\r\n\n\n### Reproduce\n\n1. run container\r\n2. remove USB device\r\n3. expect it keep restarting in loop\r\n4. insert USB device\r\n5. expect it to recover running\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\nVersion:           24.0.2\r\nAPI version:       1.43\r\nGo version:        go1.20.4\r\nGit commit:        cb74dfc\r\nBuilt:             Thu May 25 21:51:00 2023\r\nOS\/Arch:           linux\/amd64\r\nContext:           default\r\n\r\nServer: Docker Engine - Community\r\nEngine:\r\n Version:          24.0.2\r\n API version:      1.43 (minimum version 1.12)\r\n Go version:       go1.20.4\r\n Git commit:       659604f\r\n Built:            Thu May 25 21:51:00 2023\r\n OS\/Arch:          linux\/amd64\r\n Experimental:     false\r\ncontainerd:\r\n Version:          1.6.21\r\n GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\nrunc:\r\n Version:          1.1.7\r\n GitCommit:        v1.1.7-0-g860f061\r\ndocker-init:\r\n Version:          0.19.0\r\n GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\nVersion:    24.0.2\r\nContext:    default\r\nDebug Mode: false\r\n\r\nServer:\r\nContainers: 11\r\n Running: 11\r\n Paused: 0\r\n Stopped: 0\r\nImages: 16\r\nServer Version: 24.0.2\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Using metacopy: false\r\n Native Overlay Diff: true\r\n userxattr: false\r\nLogging Driver: journald\r\nCgroup Driver: systemd\r\nCgroup Version: 2\r\nPlugins:\r\n Volume: local\r\n Network: bridge host ipvlan macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: io.containerd.runc.v2 runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\nrunc version: v1.1.7-0-g860f061\r\ninit version: de40ad0\r\nSecurity Options:\r\n apparmor\r\n seccomp\r\n  Profile: builtin\r\n cgroupns\r\nKernel Version: 5.15.0-78-generic\r\nOperating System: Ubuntu 22.04.2 LTS\r\nOSType: linux\r\nArchitecture: x86_64\r\nCPUs: 4\r\nTotal Memory: 7.604GiB\r\nName: 90231\r\nID: 331cee29-e1f8-49e0-9021-b0ba782c3214\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode: false\r\nExperimental: false\r\nInsecure Registries:\r\n 127.0.0.0\/8\r\nLive Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":[">  When that device \"\/dev\/ttyUSB0\" is unplugged the container will be removed\r\n\r\nDo you have any daemon logs from when that happens? I'm mostly curious about the part where you mention that the container is removed (I'd expect the container to still be there, but possibly as \"failing to start\")\r\n","I am probably using wrong terminology here. by \"removed\" I mean it does not show up in \"docker ps\" list as it fails to start at all (does not get even state where entry-point is executed)","This is example with some logs\r\n\r\n1. Insert USB device to my PC. This is cheap USB to UART device \/dev\/ttyUSB0\r\n```\r\n[55270.336258] usb 1-14: new full-speed USB device number 6 using xhci_hcd\r\n[55270.519244] usb 1-14: New USB device found, idVendor=10c4, idProduct=ea60, bcdDevice= 1.00\r\n[55270.519260] usb 1-14: New USB device strings: Mfr=1, Product=2, SerialNumber=3\r\n[55270.519268] usb 1-14: Product: CP2102 USB to UART Bridge Controller\r\n[55270.519275] usb 1-14: Manufacturer: Silicon Labs\r\n[55270.519281] usb 1-14: SerialNumber: 0001\r\n[55270.566086] usbcore: registered new interface driver usbserial_generic\r\n[55270.566127] usbserial: USB Serial support registered for generic\r\n[55270.571010] usbcore: registered new interface driver cp210x\r\n[55270.571053] usbserial: USB Serial support registered for cp210x\r\n[55270.571149] cp210x 1-14:1.0: cp210x converter detected\r\n[55270.573515] usb 1-14: cp210x converter now attached to ttyUSB0\r\n```\r\n\r\n2. Create docker compose file with USB device mapped to that container.\r\nIn this small example to simulate restarting the container I use ping to run 10seconds and then exit thus forcing container to restart\r\n```yaml\r\nversion: \"3.7\"\r\nservices:\r\n  usb_reader:\r\n    image: alpine:latest\r\n    command: ['ping', '-c', '10', 'google.com']\r\n    restart: always\r\n    devices:\r\n      - \/dev\/ttyUSB0:\/dev\/ttyUSB0:rw\r\n    device_cgroup_rules:\r\n      # https:\/\/stackoverflow.com\/a\/62758958\/2514290\r\n      - \"c 188:* rmw\"\r\n      - \"c 166:* rmw\"\r\n      - \"c 4:* rmw\"\r\n    group_add:\r\n      - dialout\r\n    volumes:\r\n      - \/run\/udev:\/run\/udev:ro\r\n```\r\n\r\n3. start container `sudo docker-compose up -d`\r\n4. check with `sudo docker stats` that container runs, exits and gets restarted.\r\n5. Unplug USB device from PC\r\n\r\n6. Container is removed from list and does not seems to be \"restarted\"\r\n\r\njournald has these lines as last lines\r\n```\r\nokt   19 22:55:35 aldas 8331c0336ed3[1947]: PING google.com (216.58.209.206): 56 data bytes\r\nokt   19 22:55:35 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=0 ttl=56 time=4.782 ms\r\nokt   19 22:55:36 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=1 ttl=56 time=4.599 ms\r\nokt   19 22:55:37 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=2 ttl=56 time=4.466 ms\r\nokt   19 22:55:38 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=3 ttl=56 time=4.705 ms\r\nokt   19 22:55:39 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=4 ttl=56 time=4.515 ms\r\nokt   19 22:55:40 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=5 ttl=56 time=4.181 ms\r\nokt   19 22:55:41 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=6 ttl=56 time=4.732 ms\r\nokt   19 22:55:42 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=7 ttl=56 time=4.386 ms\r\nokt   19 22:55:43 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=8 ttl=56 time=4.804 ms\r\nokt   19 22:55:44 aldas 8331c0336ed3[1947]: 64 bytes from 216.58.209.206: seq=9 ttl=56 time=4.593 ms\r\nokt   19 22:55:44 aldas 8331c0336ed3[1947]: \r\nokt   19 22:55:44 aldas 8331c0336ed3[1947]: --- google.com ping statistics ---\r\nokt   19 22:55:44 aldas 8331c0336ed3[1947]: 10 packets transmitted, 10 packets received, 0% packet loss\r\nokt   19 22:55:44 aldas 8331c0336ed3[1947]: round-trip min\/avg\/max = 4.181\/4.576\/4.804 ms\r\nokt   19 22:55:45 aldas dockerd[1947]: time=\"2023-10-19T22:55:45.001507083+03:00\" level=info msg=\"ignoring event\" container=8331c0336ed3ddc8c0d048cf14c81dc619534c74bc59ec6c02bfd1a8a9a79050 module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\nokt   19 22:56:45 aldas dockerd[1947]: time=\"2023-10-19T22:56:45.076426861+03:00\" level=error msg=\"8331c0336ed3ddc8c0d048cf14c81dc619534c74bc59ec6c02bfd1a8a9a79050 cleanup: failed to delete container from containerd: container \\\"8331c0336ed3ddc8c0d048cf14c81dc619534c74bc59ec6c02bfd1a8a9a79050\\\" in namespace \\\"moby\\\": not found\"\r\nokt   19 22:56:45 aldas dockerd[1947]: time=\"2023-10-19T22:56:45.196103564+03:00\" level=error msg=\"restartmanger wait error: error gathering device information while adding custom device \\\"\/dev\/ttyUSB0\\\": no such file or directory\"\r\n```\r\n\r\n> okt   19 22:56:45 aldas dockerd[1947]: time=\"2023-10-19T22:56:45.196103564+03:00\" level=error msg=\"restartmanger wait error: error gathering device information while adding custom device \\\"\/dev\/ttyUSB0\\\": no such file or directory\"\r\n\r\n\r\nMaybe there is a built in workaround for this situation. It seems little bit silly to build \"watchdog\" process or CRON job to check for this situation and then trigger that container to be run again.  It would be nice if engine could keep trying to recover from this situation by trying to start that container - until that device reappears\r\n\r\n\r\n","I found this https:\/\/github.com\/moby\/moby\/issues\/35359  but suggestions like mounting whole `\/dev\/bus\/usb` are no particularly safe as I would like to expose this device and not other things to that container.","@thaJeztah is current behavior working as intended?"],"labels":["status\/0-triage","kind\/bug"]},{"title":"Using Docker\/Moby Networks as a quasi DMZ","body":"### Description\n\nThe container networks for Moby\/Docker already allow a lot of isolation from other containers on the same host or the rest of the LAN and Internet via the use of --internal.\r\n\r\nHowever, one tool that is missing is the ability to isolate a container from the LAN while allowing internet access, like a DMZ'd server.  This could allow an admin to setup a container stack with:\r\nContainer <-> (\"DMZ\" Network) <-> (Reverse Proxy) #Internet accessible service \r\nor\r\nContainer <-> (\"DMZ\" Network) <-> (Reverse Proxy) <-> (Regular Docker Network)  #Internal service with locally hosted DNS\r\n\r\nTo isolate a container which needs internet access from the rest of the LAN.\r\nIt might not be a \"true\" DMZ since the traffic is getting inside the LAN in the first place, but if the routing nftables or iptables of Docker are such they may be able to prevent the container from being able to see any other LAN services while still seeing the internet and any other containers attached to the \"DMZ\" container.\r\n\r\nThe benefit of this is allowing more network isolation for containers and making DMZ-like isolation for users who would otherwise not use it much more accessible.","comments":[],"labels":["status\/0-triage","kind\/feature","area\/networking"]},{"title":"Port forwarding is not working after re-installation of docker","body":"### Description\n\nI am running a container with the nginx docker image using `docker compose`.\r\nI am using custom bridge network with port forwarding and the container is running fine. I am able to access the forwarded port from the host machine.\r\n\r\nThe problem is coming after I reinstall the docker. The container is coming up but I am unable to access the service with the forwarded port.\n\n### Reproduce\n\n1. Install docker from here https:\/\/docs.docker.com\/engine\/install\/ubuntu\/\r\n2. Start docker compose\r\n   - sudo docker compose -f nginx.yml up -d\r\n   - Do telnet (telnet <host_ip> 7777)\r\n   - This works fine\r\n\r\n3. Do the reinstall (clean and install)\r\n   - bash docker_install.sh\r\n\r\n4. Start docker compose again\r\n   - sudo docker compose -f nginx.yml up -d\r\n   - Do telnet (telnet <host_ip> 7777)\r\n   - This fails with the error `telnet: Unable to connect to remote host: No route to host`\r\n\r\n\r\nBelow is the compose yml file\r\n```\r\n# nginx.yml\r\n\r\nversion: '3.3'\r\nname: nginx\r\nservices:\r\n  nginx:\r\n    image: nginx\r\n    ports:\r\n      - 7777:80\r\n    restart: always\r\n    networks:\r\n      - nginx_network\r\nnetworks:\r\n  nginx_network:\r\n    driver: bridge\r\n    ipam:\r\n      config:\r\n        - subnet: 10.4.0.0\/16\r\n```\r\n\r\nBelow is the script to clean and install the docker\r\n```\r\n# docker_install.sh\r\n\r\nfunction install_root_docker() {\r\n    # Add Docker's official GPG key:\r\n    sudo apt-get update -y\r\n    sudo apt-get install ca-certificates curl gnupg -y\r\n    sudo install -m 0755 -d \/etc\/apt\/keyrings\r\n    curl -fsSL https:\/\/download.docker.com\/linux\/ubuntu\/gpg | sudo gpg --dearmor -o \/etc\/apt\/keyrings\/docker.gpg\r\n    sudo chmod a+r \/etc\/apt\/keyrings\/docker.gpg\r\n\r\n    # Add the repository to Apt sources:\r\n    echo \\\r\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=\/etc\/apt\/keyrings\/docker.gpg] https:\/\/download.docker.com\/linux\/ubuntu \\\r\n  \"$(. \/etc\/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\r\n    sudo tee \/etc\/apt\/sources.list.d\/docker.list > \/dev\/null\r\n    sudo apt-get update -y\r\n    sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y\r\n\r\n    sudo systemctl enable docker.service\r\n\r\n    sudo systemctl start docker.service\r\n    sudo systemctl is-active docker.service\r\n}\r\n\r\nfunction clean_root_docker() {\r\n    sudo systemctl stop docker.service docker.socket\r\n    sudo apt-get purge -y containerd.io docker-ce docker-ce-cli docker-compose-plugin docker-buildx-plugin docker-ce-rootless-extras pigz slirp4netns\r\n    sudo apt-get autoremove -y --purge containerd.io docker-ce docker-ce-cli docker-compose-plugin docker-buildx-plugin docker-ce-rootless-extras pigz slirp4netns\r\n    sudo rm -rf \/var\/lib\/docker \/etc\/docker\r\n    sudo rm -rf \/etc\/apparmor.d\/docker\r\n    sudo groupdel docker\r\n    sudo rm -rf \/var\/run\/docker.sock\r\n    sudo rm -rf ~\/.docker\r\n    sudo apt -y autoremove\r\n}\r\n\r\nclean_root_docker\r\ninstall_root_docker\r\n```\n\n### Expected behavior\n\nI should be able to access the container with the forwarded port.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:32:12 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:12 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc version: v1.1.9-0-gccaecfc\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.0-1042-gcp\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.817GiB\r\n Name: iram-ubuntu-worker\r\n ID: 7f0fe2f9-f654-460e-a82b-400e1c26f37d\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Same problem with me .\r\n\r\nI did the following : \r\n`sudo docker run -d -p 80:80 nginx:latest\r\n` this works pretty fine and i can access to nginx container via both curl , browser .\r\n`sudo docker run -d -p 8080:80 nginx:latest\r\n` when i do `curl VMipAdrr:8080` it doesn't work  , but the port is open and works pretty fine on `curl localhost:8080`\r\n```\r\nAzure Virtual machine \r\nLinux distro : Debian GNU\/Linux 11 (bullseye) x86_64 \r\nDocker version : 24.0.6\r\n```\r\n\r\nIt's not image related , the issue is still the same with Apache httpd \r\n\r\n","@IdavalapatiRamanjaneyulu the issue isn't related to Docker , it's a network security group related , in whatever cloud provider you're using , you need to add a port rule ( enable traffic on that port ) .\r\n\r\n### so yeah , it's not Docker related and my issue was solved in that way","Hi @IdavalapatiRamanjaneyulu, \r\n\r\n> * Do telnet (telnet <host_ip> 7777)\r\n> * This fails with the error telnet: Unable to connect to remote host: No route to host\r\n\r\nAre you running that command from another host than the one where Docker runs? If yes, are you sure this host has proper connectivity? `Unable to connect to remote host: No route to host` means that the host's routing table contains no entry for the IP address, hence the kernel doesn't know how to connect to the destination host. In that case, this is not an issue with Docker itself but rather a misconfiguration of your system.\r\n\r\nIf you are running the telnet command from the same host, can you copy\/paste the output of `ip route show` please and tell me what IP address you put in your telnet command? \r\n","@akerouanton I have been trying this from the same host.\r\nI also tried this with `localhost` and that also seems not working\r\n```\r\n# telnet localhost 7777\r\nTrying 127.0.0.1...\r\nConnected to localhost.\r\nEscape character is '^]'.\r\nConnection closed by foreign host.\r\n```\r\n\r\nBelow is the output of `ip route show`\r\n```\r\ndefault via 10.128.0.1 dev ens4 proto dhcp src 10.128.0.49 metric 100 \r\n10.1.0.0\/16 dev br-d87c2b8aa61e proto kernel scope link src 10.1.0.1 linkdown \r\n10.4.0.0\/30 dev br-80e26527c0be proto kernel scope link src 10.4.0.1 linkdown \r\n10.4.0.0\/30 dev br-aa170273e54a proto kernel scope link src 10.4.0.1\r\n100.128.0.1 dev ens4 proto dhcp scope link src 100.128.0.149 metric 100\r\n172.17.0.0\/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown\r\n```","Rebooting the server is working though that is not an ideal solution. Restarting docker is also not working.","Looking at your routing table, I see you have two overlapping routes:\r\n\r\n```\r\n10.4.0.0\/30 dev br-80e26527c0be proto kernel scope link src 10.4.0.1 linkdown \r\n10.4.0.0\/30 dev br-aa170273e54a proto kernel scope link src 10.4.0.1\r\n```\r\n\r\nMy hypothesis is that you created a custom network with subnet `10.4.0.0\/30` on your first install, then you stopped and reinstalled the engine. On that fresh install you re-created the same custom network with the same subnet. Is that right?","Yes, you are correct. As you can see in the yaml file network section, a custom network is defined.\r\n```\r\nnetworks:\r\n  nginx_network:\r\n    driver: bridge\r\n    ipam:\r\n      config:\r\n        - subnet: 10.4.0.0\/16\r\n```\r\n\r\nWhen I run this yaml file(compose up) and clean the docker and install it again, and re-run the same yaml file then this issue is coming.","**Note: Partial solution** is after doing `sudo docker compose -f nginx.yml down` before uninstalling the docker then it is removing the network and the issue is not coming.\r\nMy assumption is that network also will be removed if I uninstall docker engine (without doing docker compose down)."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug"]},{"title":"Add WithTLSClientConfigVerification function to enable setting TLS verification flag without env var","body":"closes #46598\r\n\r\n**- What I did**\r\n\r\nI've created a function where we can set TLS verification flag without reading it from environment variables.\r\n\r\n**- How I did it**\r\n\r\nI've added a new function called `WithTLSClientConfigVerification` to [client\/options.go](..\/blob\/master\/client\/options.go) file.\r\n\r\n**- How to verify it**\r\n\r\n- I ran a Docker daemon with tls_verify=0\r\n- I created a random ca.pem\r\n- Then I called this new function, passing the random ca.pem, with both `tlsVerify` as true and `tlsVerify` as false\r\n- I noticed I was able to connect with `tlsVerify` as true, and received a certificate error when running with `tlsVerify` as false, as expected\r\n\r\n**- Description for the changelog**\r\nAdded `WithTLSClientConfigVerification` function to client options to enable setting TLS verification flag\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/496350\/3282a427-0b45-47bf-9b87-94fefcfbc58a)\r\n\r\n","comments":["Thank you for contributing! It appears your commit message is missing a DCO sign-off,\r\ncausing the DCO check to fail.\r\n\r\nWe require all commit messages to have a `Signed-off-by` line with your name\r\nand e-mail (see [\"Sign your work\"](https:\/\/github.com\/moby\/moby\/blob\/v24.0.2\/CONTRIBUTING.md#sign-your-work)\r\nin the CONTRIBUTING.md in this repository), which looks something like:\r\n\r\n```\r\nSigned-off-by: YourFirsName YourLastName <yourname@example.org>\r\n```\r\n\r\nThere is no need to open a new pull request, but to fix this (and make CI pass),\r\nyou need to _amend_ the commit(s) in this pull request, and \"force push\" the amended\r\ncommit.\r\n\r\nUnfortunately, it's not possible to do so through GitHub's web UI, so this needs\r\nto be done through the git commandline.\r\n\r\nYou can find some instructions in the output of the DCO check (which can be found\r\nin the \"checks\" tab on this pull request), as well as in the [Moby contributing guide](https:\/\/github.com\/moby\/moby\/blob\/v24.0.2\/docs\/contributing\/set-up-git.md).\r\n\r\nSteps to do so \"roughly\" come down to:\r\n\r\n1. Set your name and e-mail in git's configuration:\r\n\r\n    ```bash\r\n    git config --global user.name \"YourFirstName YourLastName\"\r\n    git config --global user.email \"yourname@example.org\"\r\n    ```\r\n\r\n    (Make sure to use your _real_ name (**not your GitHub username\/handle**) and e-mail)\r\n\r\n\r\n2. Clone your fork locally\r\n3. Check out the branch associated with this pull request\r\n4. Sign-off and amend the existing commit(s)\r\n\r\n    ```bash\r\n    git commit --amend --no-edit --signoff\r\n    ```\r\n\r\n    If your pull request contains multiple commits, either squash the commits (if\r\n    needed) or sign-off each individual commit.\r\n\r\n5. _Force push_ your branch to GitHub (using the `--force` or [`--force-with-lease`](https:\/\/stackoverflow.com\/questions\/52823692\/git-push-force-with-lease-vs-force) flags) to update the pull request.\r\n\r\n\r\nSorry for the hassle (I wish GitHub would make this a bit easier to do), and let me know if you need help or more detailed instructions!","@vvoland and @corhere thank you for your thoughtful suggestion!\r\nI've applied the changes.\r\nPlease kindly review it again. Thanks"],"labels":["area\/api","status\/1-design-review","kind\/enhancement"]},{"title":"Create a function in the client\/options.go to enable setting TLS verification flag","body":"### Description\r\n\r\nCurrently, it's only possible to enable or disable TLS verification through environment variables on [client\/options](..\/blob\/master\/client\/options.go)\r\n\r\nCurrently, the client reads the environment variable `DOCKER_TLS_VERIFY` to decide to verify the TLS or not.\r\nTo use the client as a SDK, it would be great to have the option to pass the TLS verification flag through a function parameter.\r\n\r\n`WithTLSClientConfigFromEnv` is where the `DOCKER_TLS_VERIFY` and  `DOCKER_CERT_PATH` variable are read.\r\nThere is also the `WithTLSClientConfig` function, where you can set the certificate paths without necessarily reading from environment variables. However, it's not possible to set the TLS verify through this function","comments":[],"labels":["status\/0-triage","kind\/feature"]},{"title":"buildkit's meta resolver fails to resolve local images w\/ containerd backed storage","body":"### Description\r\n\r\nWhen buildkit tries to resolve an image config with `PreferModeLocal`, the image is not found in the containerd image store even when it does exist. As such the registry is then checked.\r\n\r\nThe exact code from the reproducer works fine with graphdrivers.\r\n\r\n### Reproduce\r\n\r\nSetup:\r\n\r\n```console\r\n$ docker pull busybox:latest\r\n$ docker tag busybox:latest notexist\r\n```\r\n\r\nbuildkit code:\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"io\"\r\n\t\"os\"\r\n\t\"os\/signal\"\r\n\r\n\t\"github.com\/containerd\/console\"\r\n\t\"github.com\/docker\/distribution\/reference\"\r\n\t\"github.com\/docker\/docker\/api\/types\"\r\n\tdocker \"github.com\/docker\/docker\/client\"\r\n\t\"github.com\/docker\/docker\/client\/buildkit\"\r\n\t\"github.com\/moby\/buildkit\/client\"\r\n\t\"github.com\/moby\/buildkit\/client\/llb\"\r\n\tgwclient \"github.com\/moby\/buildkit\/frontend\/gateway\/client\"\r\n\t\"github.com\/moby\/buildkit\/util\/progress\/progressui\"\r\n\t\"github.com\/pkg\/errors\"\r\n\t\"golang.org\/x\/sync\/errgroup\"\r\n)\r\n\r\nfunc main() {\r\n\tdc, err := docker.NewClientWithOpts(docker.FromEnv)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt)\r\n\tdefer cancel()\r\n\r\n\trdr, err := dc.ImagePull(ctx, \"busybox:latest\", types.ImagePullOptions{})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tdefer rdr.Close()\r\n\tio.Copy(os.Stderr, rdr)\r\n\r\n\tif err := dc.ImageTag(ctx, \"busybox:latest\", \"notexist:latest\"); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tc, err := client.New(ctx, \"\", buildkit.ClientOpts(dc)...)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tdefer c.Close()\r\n\r\n\tch := make(chan *client.SolveStatus)\r\n\teg, ctx := errgroup.WithContext(ctx)\r\n\r\n\teg.Go(func() error {\r\n\t\tvar c console.Console\r\n\t\tif cn, err := console.ConsoleFromFile(os.Stderr); err == nil {\r\n\t\t\tc = cn\r\n\t\t}\r\n\t\t\/\/ not using shared context to not disrupt display but let us finish reporting errors\r\n\t\t_, err = progressui.DisplaySolveStatus(ctx, c, os.Stdout, ch)\r\n\t\treturn err\r\n\t})\r\n\r\n\teg.Go(func() error {\r\n\t\t_, err := c.Build(ctx, client.SolveOpt{}, \"whatever\", func(ctx context.Context, gwc gwclient.Client) (*gwclient.Result, error) {\r\n\t\t\tnamed, err := reference.ParseNormalizedNamed(\"notexist:latest\")\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn nil, errors.Wrap(err, \"failed to parse reference\")\r\n\t\t\t}\r\n\t\t\tnamed = reference.TagNameOnly(named)\r\n\t\t\t_, _, dt, err := gwc.ResolveImageConfig(ctx, named.String(), llb.ResolveImageConfigOpt{\r\n\t\t\t\tResolveMode: llb.ResolveModePreferLocal.String(),\r\n\t\t\t})\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn nil, errors.Wrap(err, \"failed to resolve image config\")\r\n\t\t\t}\r\n\r\n\t\t\timg, err := llb.Image(\"notexist:latest\").WithImageConfig(dt)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\tdef, err := img.Marshal(ctx)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\treturn gwc.Solve(ctx, gwclient.SolveRequest{\r\n\t\t\t\tEvaluate:   true,\r\n\t\t\t\tDefinition: def.ToPB(),\r\n\t\t\t})\r\n\t\t}, ch)\r\n\t\treturn err\r\n\t})\r\n\r\n\terr = eg.Wait()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n}\r\n```\r\n\r\n### Expected behavior\r\n\r\nShould resolve from the containerd image store.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.0-beta.1-450-gfa5a0e8139.m\r\n API version:       1.44\r\n Go version:        go1.21.1\r\n Git commit:        fa5a0e8139\r\n Built:             Sun Oct  1 19:58:37 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          dev\r\n  API version:      1.44 (minimum version 1.12)\r\n  Go version:       go1.21.1\r\n  Git commit:       bc2c8279de28814369bd9db4c93e6faa957456f5\r\n  Built:            Tue Oct  3 22:00:39 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.7.6-2\r\n  GitCommit:        091922f03c2762540fd057fba91260237ff86acb\r\n runc:\r\n  Version:          1.1.9-1\r\n  GitCommit:        ccaecfcbc907d70a7aa870a6650887b901b25b82\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.0-beta.1-450-gfa5a0e8139.m\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.0-95-g13ec6359.m\r\n    Path:     \/usr\/local\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.21.0-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 17\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 15\r\n Images: 75\r\n Server Version: dev\r\n Storage Driver: overlayfs\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: local\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 091922f03c2762540fd057fba91260237ff86acb\r\n runc version: ccaecfcbc907d70a7aa870a6650887b901b25b82\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-1012-azure\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.33GiB\r\n Name: dev2\r\n ID: ccb953ef-cff6-4d73-94de-24e41bbff5c2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 38\r\n  Goroutines: 61\r\n  System Time: 2023-10-03T22:02:19.652545815Z\r\n  EventsListeners: 0\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["@rumpl noticed that changing https:\/\/github.com\/moby\/buildkit\/blob\/f94ed7cec313c0180c8af88f069fa93b204c9e15\/util\/imageutil\/config.go#L70 to use `OnlyStrict` fixes the issue (although this is not likely the \"correct\" fix).\r\n\r\nThis looks related to platform matching in buildkit.\r\nBuildkit does correctly check the local stores for image data, however it is also trying to get all other matching platforms (e.g. on \"386\" is considered match on \"amd64\" arches).\r\nSpecifically this is happening here: https:\/\/github.com\/moby\/buildkit\/blob\/f94ed7cec313c0180c8af88f069fa93b204c9e15\/util\/imageutil\/config.go#L216-L220\r\n","@tonistiigi or @dmcgowan any ideas?","What I was thinking is something along the lines of changing the handler in that 2nd link above to:\r\n\r\n\r\n```go\r\n\t\t\tif platform != nil {\r\n\t\t\t\tif len(index.Manifests) > 0 {\r\n\t\t\t\t\tsort.SliceStable(index.Manifests, func(i, j int) bool {\r\n\t\t\t\t\t\tmi := index.Manifests[i]\r\n\t\t\t\t\t\tif mi.Platform == nil {\r\n\t\t\t\t\t\t\treturn false\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\tmj := index.Manifests[j]\r\n\t\t\t\t\t\tif mj.Platform == nil {\r\n\t\t\t\t\t\t\treturn true\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t\treturn platform.Less(*mi.Platform, *mj.Platform)\r\n\t\t\t\t\t})\r\n\t\t\t\t\tdescs = append(descs, index.Manifests[0])\r\n\t\t\t\t}\r\n\t\t\t} else {\r\n\t\t\t\tdescs = append(descs, index.Manifests...)\r\n\t\t\t}\r\n```"],"labels":["kind\/bug","area\/builder\/buildkit","containerd-integration"]},{"title":"docker build iidfile can't be used with process substitution \/ fd because it tries to remove it","body":"### Description\r\n\r\ncan't use `--iidfile` with process substitution or to write to stdout\r\n\r\n```\r\n$ docker build --iidfile=\/dev\/stdout . | xargs docker inspect\r\nERROR: removing image ID file: remove \/dev\/stdout: permission denied\r\n```\r\n\r\n```\r\n$ docker build .  --iidfile=>(xargs docker inspect) \r\nERROR: removing image ID file: remove \/dev\/fd\/63: operation not permitted\r\n```\r\n\r\ncan do\r\n```\r\n$ docker build . -q | xargs docker inspect --format '{{.Size}}'\r\n116490258\r\n```\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n```\r\necho 'FROM debian' > Dockerfile\r\ndocker build . --iidfile=>(docker image inspect)\r\n```\r\n\r\n```\r\n$ docker build . --iidfile=>(docker image inspect)\r\n\"docker image inspect\" requires at least 1 argument.\r\nSee 'docker image inspect --help'.\r\n\r\nUsage:  docker image inspect [OPTIONS] IMAGE [IMAGE...]\r\n\r\nDisplay detailed information on one or more images\r\nERROR: removing image ID file: remove \/dev\/fd\/63: operation not permitted\r\n```\r\n\r\n### Expected behavior\r\n\r\nshould work with fd\r\n\r\n```\r\n$ docker build . --iidfile=>(xargs docker inspect --format '{{.Size}}' )\r\nsha256:db9749ac63b9c77f0325b0ca3358c50b28258b083f8c956f8d5ded66ba561818\r\n116490258\r\n```\r\nshould show build logs in stdout\/stderr as well as print out size to std out\r\n\r\n### docker version\r\n\r\n```bash\r\n(base) vscode \u279c \/workspaces\/test-micromamba-features\/test $ docker version\r\nClient:\r\n Version:           23.0.6+azure-2\r\n API version:       1.41 (downgraded from 1.42)\r\n Go version:        go1.19.10\r\n Git commit:        ef23cbc4315ae76c744e02d687c09548ede461bd\r\n Built:             Thu May  4 10:51:27 UTC 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.20.2\r\n  Git commit:       v20.10.23\r\n  Built:            Tue Jan  1 00:00:00 1980\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.0\r\n  GitCommit:        v1.7.0\r\n runc:\r\n  Version:          1.1.4\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n(base) vscode \u279c \/workspaces\/test-micromamba-features\/test $ docker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.2+azure-3\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.21.0-1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 42\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 39\r\n Images: 455\r\n Server Version: 20.10.23\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: journald\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: v1.7.0\r\n runc version: \r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 6.1.22\r\n Operating System: NixOS 23.05 (Stoat)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 24\r\n Total Memory: 125.7GiB\r\n Name: reese\r\n ID: OEQH:PAN5:KRDU:G7IY:UZJP:FNH6:AZ2B:L3NG:NZ4R:QRVT:WFH4:JMJ2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: true\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nsome possible fixes:\r\n\r\n1. don't delete if not regular file\r\n2. don't fail if can't delete\r\n3. always print iid to stdout (when pr was made build logs where sent to both stderr and stdout)","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Rework `TestPullFailsWithAlteredLayer` to test containerd","body":"- related to: https:\/\/github.com\/moby\/moby\/issues\/46561\r\n\r\nWe should rewrite the `TestPullFailsWithAlteredLayer` in a way that would hit the containerd path for the equivalent check.\r\n\r\n","comments":["Hey, would love to contribute to this. Can I get this issue assigned to me?","Have at it!","Hey, after reading the tests `TestPullFailsWithAlteredLayer` for schema1 and schema2, I understood that we are trying to test pull operation failure when we alter a layer blob which will alter its digest. Can I get an high level brief on what is wrong with the existing test and what needs to be changed here?"],"labels":["exp\/beginner","exp\/intermediate","area\/testing","containerd-integration"]},{"title":"c8d\/resolver: Remove http fallback transport","body":"- depends on: https:\/\/github.com\/moby\/moby\/pull\/46565\r\n\r\nThis is already handled by the RegistryHosts having a separate HTTP-only host for insecure registries.\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["The localhost case is still a bit of a challenge with containerd's configuration, if it is achievable through configuration, it is probably better, see https:\/\/github.com\/containerd\/containerd\/issues\/9157"],"labels":["status\/2-code-review","kind\/refactor","containerd-integration"]},{"title":"[24.0 backport] update to go1.21.1, default to GOTOOLCHAIN=local","body":"- backport of https:\/\/github.com\/moby\/moby\/pull\/46069\r\n- related \/ depends on https:\/\/github.com\/moby\/moby\/pull\/46559\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Needs rebase"],"labels":["status\/2-code-review","impact\/changelog","area\/packaging"]},{"title":"daemon: add fuzzer from OSS-Fuzz","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\nMoved [the daemon fuzzer](https:\/\/github.com\/google\/oss-fuzz\/blob\/master\/projects\/moby\/daemon_fuzzer.go) over from OSS-Fuzz.\r\n\r\nI also added a seed file and a dictionary. These are for OSS-Fuzz.\r\n\r\nThe fuzzer will invoke different daemon methods in an order determined by the fuzzer and with values determined by the fuzzer. It alread runs on OSS-Fuzz continuously and has been for around one year. \r\n\r\n@thaJeztah for info.\r\n\r\n**- How I did it**\r\n\r\nI refactored the fuzzer slightly for easier readability.\r\n\r\n**- How to verify it**\r\n\r\nOSS-Fuzz will build this fuzzer once it is merged. I have tested this via OSS-Fuzz only, and it runs fine on my end.","comments":["Looks like there's some build-failures on Windows; perhaps it needs a build-tag to be Linux-only \ud83e\udd14 (but haven't checked out the branch locally to check)\r\n\r\n```\r\n=== Errors\r\ndaemon\\fuzz_test.go:90:15: undefined: mount.MakePrivate\r\ndaemon\\fuzz_test.go:96:15: undefined: mount.Unmount\r\n```","> daemon\/fuzz_test.go:51:3: undefined: inittier"],"labels":["status\/2-code-review","area\/testing","status\/failing-ci","area\/daemon"]},{"title":"vendor: github.com\/moby\/buildkit master (v0.13.0-dev)","body":"- [ ] \u26a0\ufe0f need to remove https:\/\/github.com\/moby\/moby\/pull\/46748 (if https:\/\/github.com\/moby\/buildkit\/pull\/4391 is merged in BuildKit)\r\n\r\nupdating the dependency to tip of master, to get early warnings about breaking changes.\r\n\r\nsecond commit is to make it compile, but it's very much broken, as changes are needed to accommodate the changes from;\r\n\r\n- https:\/\/github.com\/moby\/buildkit\/pull\/4035\r\n- https:\/\/github.com\/moby\/buildkit\/pull\/4113\r\n\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Hmmm.. more things to fix; looks like `DisplaySolveStatus` was removed entirely in https:\/\/github.com\/moby\/buildkit\/commit\/37131781d719f0ed5a182ebf6b2f6e91288bcd28 (https:\/\/github.com\/moby\/buildkit\/pull\/4113), so we need to figure out what the replacement needs to look like.\r\n\r\n```\r\n# github.com\/docker\/docker\/integration\/build [github.com\/docker\/docker\/integration\/build.test]\r\nError: .\\build_traces_test.go:60:24: undefined: progressui.DisplaySolveStatus\r\n```\r\n","Ok, in good'ol \"code by example\" fashion; looking at that PR it needs something like this;\r\n\r\n```go\r\nd, err := progressui.NewDisplay(os.Stderr, progressui.TtyMode)\r\nif err != nil {\r\n\t\/\/ If an error occurs while attempting to create the tty display,\r\n\t\/\/ fallback to using plain mode on stdout (in contrast to stderr).\r\n\td, _ = progressui.NewDisplay(os.Stdout, progressui.PlainMode)\r\n}\r\n\/\/ not using shared context to not disrupt display but let is finish reporting errors\r\n_, err = d.UpdateFrom(context.TODO(), ch)\r\n```\r\n"],"labels":["area\/builder","status\/2-code-review","area\/builder\/buildkit"]},{"title":"builder: dockerfile COPY --from specify dir will change uid\/gid","body":"### Description\n\nI wrote two dockerfiles\uff0c\r\ndockerfile1 like this\uff1a\r\n```\r\nFrom rnd-dockerhub.huawei.com\/official\/ubuntu-arm64\r\nRUN useradd hjf\r\nRUN cd \/home\/ && mkdir test && chown hjf: test\r\n```\r\nrun `docker build -t test1 -f Dockerfile1` cmd,will construct image test1\r\ndockerfile2 like this:\r\n```\r\nFROM test1 as build\r\nFROM rnd-dockerhub.huawei.com\/official\/nginx-aarch64\r\nRUN useradd hjf\r\nCOPY --from=build \/home\/test \/home\/test\r\n```\r\nrun `docker build -t test2 -f Dockerfile2` cmd,will construct image test2\r\n\r\nfinnally docker run test2,the \/home\/test dir  permissions are as follows\uff1a\r\n```\r\nroot@f36df3977fb2:\/home# ls -al\r\ntotal 16\r\ndrwxr-xr-x. 1 root root 4096 Sep 21 12:30 .\r\ndrwxr-xr-x. 1 root root 4096 Sep 21 12:30 ..\r\ndrw-------. 2 root root 4096 Sep 21 12:30 test\r\n```\r\ntest dir permissions changed from hjf to root\r\n\n\n### Reproduce\n\n1.docker build -t test1 -f Dockerfile1 .\r\n2.docker build -t test2 -f Dockerfile2 .\r\n3.docker run  -it test1:latest bash\r\n4.docker run  -it test2:latest bash\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\n[root@localhost ~]# docker version\r\nClient:\r\n Version:           18.09.0\r\n EulerVersion:      18.09.0.400\r\n API version:       1.39\r\n Go version:        go1.17.3\r\n Git commit:        e812b33\r\n Built:             Sun Sep 17 11:16:53 2023\r\n OS\/Arch:           linux\/arm64\r\n Experimental:      false\r\n\r\nServer:\r\n Engine:\r\n  Version:          18.09.0\r\n  EulerVersion:     18.09.0.400\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.17.3\r\n  Git commit:       e812b33\r\n  Built:            Thu Sep 21 11:57:01 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\n```\n\n\n### docker info\n\n```bash\n[root@localhost ~]# docker info\r\nContainers: 49\r\n Running: 0\r\n Paused: 0\r\n Stopped: 49\r\nImages: 97\r\nServer Version: 18.09.0\r\nStorage Driver: overlay2\r\n Backing Filesystem: extfs\r\n Supports d_type: true\r\n Native Overlay Diff: true\r\nLogging Driver: json-file\r\nCgroup Driver: cgroupfs\r\nHugetlb Pagesize: 2MB, 64KB, 32MB, 1GB, 64KB, 32MB, 2MB, 1GB (default is 2MB)\r\nPlugins:\r\n Volume: local\r\n Network: bridge host macvlan null overlay\r\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nSwarm: inactive\r\nRuntimes: runc\r\nDefault Runtime: runc\r\nInit Binary: docker-init\r\ncontainerd version: ce022dde7c93458c304b4cdcf9a9b6fc7dff6778\r\nrunc version: N\/A\r\ninit version: N\/A (expected: )\r\nSecurity Options:\r\n seccomp\r\n  Profile: default\r\nKernel Version: 5.10.0\r\nOperating System: EulerOS 2.0 (SP12)\r\nOSType: linux\r\nArchitecture: aarch64\r\nCPUs: 4\r\nTotal Memory: 7.235GiB\r\nName: localhost.localdomain\r\nID: DNLX:6QK7:35N2:RAYM:I6G7:WEMI:G424:LI3W:IHEX:KZKO:CJGV:C37U\r\nDocker Root Dir: \/var\/lib\/docker\r\nDebug Mode (client): false\r\nDebug Mode (server): false\r\nRegistry: https:\/\/index.docker.io\/v1\/\r\nLabels:\r\nExperimental: false\r\nInsecure Registries:\r\n 10.175.125.206:80\r\n 127.0.0.0\/8\r\nLive Restore Enabled: true\n```\n\n\n### Additional Info\n\n_No response_","comments":["I see you're running a very old version of Docker that reached EOL nearly 5 years ago. Are you still able to reproduce your problem on a currently maintained version of Docker?\r\n\r\n\r\nThis looks like  a duplicate of;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/37123\r\n\r\nWhich was fixed in docker 20.10 through;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/38599\r\n\r\nAnd backported to docker 19.03.2 in;\r\n\r\n- https:\/\/github.com\/docker\/engine\/pull\/315","> I see you're running a very old version of Docker that reached EOL nearly 5 years ago. Are you still able to reproduce your problem on a currently maintained version of Docker?\r\n> \r\n> This looks like a duplicate of;\r\n> \r\n> * [Multi-stage COPY --from should preserve ownership\/permissions\u00a0#37123](https:\/\/github.com\/moby\/moby\/issues\/37123)\r\n> \r\n> Which was fixed in docker 20.10 through;\r\n> \r\n> * [builder: fix `COPY --from` should preserve ownership\u00a0#38599](https:\/\/github.com\/moby\/moby\/pull\/38599)\r\n> \r\n> And backported to docker 19.03.2 in;\r\n> \r\n> * [[19.03 backport] Builder: fix \"COPY --from\" to non-existing directory on Windows [ENGCORE-935]\u00a0docker\/engine#315](https:\/\/github.com\/docker\/engine\/pull\/315)\r\n\r\ni tried the laster docker version\uff0cit is still have the some question.There is no problem if --from specifies file, but if it is dir, it will change. just like the problem described earlier.\r\ntest1:\r\n```\r\nroot@28e59665afff:\/# cd \/home\/\r\nroot@28e59665afff:\/home# ls -al\r\ntotal 20\r\ndrwxr-xr-x. 1 root root 4096 Sep 21 12:30 .\r\ndrwxr-xr-x. 1 root root 4096 Sep 22 01:38 ..\r\ndrwxr-x---. 2 hjf  hjf  4096 Sep 21 12:30 test\r\n```\r\ntest2:\r\n```\r\nroot@e24869e86da3:\/# cd home\/\r\nroot@e24869e86da3:\/home# ls -al\r\ntotal 20\r\ndrwxr-xr-x. 1 root root 4096 Sep 22 01:39 .\r\ndrwxr-xr-x. 1 root root 4096 Sep 22 01:39 ..\r\ndrwxr-xr-x. 2 root root 4096 Sep 22 01:39 test\r\n```\r\ndocker version :\r\n```\r\n[root@localhost ~]# docker version \r\nClient:\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:30:04 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:30 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.3\r\n  GitCommit:        7880925980b188f4c97b462f709d0db8e8962aff\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\nMy old version of docker has also applied the patch you mentioned.","I'm able to reproduce this behavior as described above. That is, ownership transfers for non-directory files but not for directories themselves:\r\nDockerfile.1\r\n```Dockerfile\r\nFROM ubuntu:22.04\r\nRUN useradd -u 1001 hjf \r\n\r\nRUN cd \/home\/ && mkdir testdir && chown hjf: testdir \r\nRUN cd \/home\/ && touch testfile && chown hjf: testfile\r\n```\r\n\r\nDockerfile.2\r\n```Dockerfile\r\nFROM test1 as build\r\nFROM ubuntu:22.04\r\n\r\nRUN useradd -u 1001 hjf\r\nCOPY --from=build \/home\/testfile \/home\/testfile\r\nCOPY --from=build \/home\/testdir \/home\/testdir\r\n```\r\nResult:\r\n```bash\r\n> docker run --rm test2 ls -alh \/home\/\r\ntotal 12K\r\ndrwxr-xr-x 1 root root 4.0K Sep 28 17:16 .\r\ndrwxr-xr-x 1 root root 4.0K Sep 28 17:26 ..\r\ndrwxr-xr-x 2 root root 4.0K Sep 28 17:16 testdir\r\n-rw-r--r-- 1 hjf  hjf     0 Sep 28 17:16 testfile\r\n```\r\nVersion: \r\n```bash\r\n> docker version\r\nClient: Docker Engine - Community\r\n Cloud integration: v1.0.29\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:23 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:23 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```","Ah, right, I didn't consider there's an issue with copying _directories_, and it's a difficult one to fix, because doing so would break existing behavior; you can find some details in this ticket in the BuildKit repository;\r\n\r\n- https:\/\/github.com\/moby\/buildkit\/issues\/791\r\n\r\nBasically, the differences are _subtle_ but important for this specific case; this is how it _should_ have worked; and in that case, the \"source\" directory itself could be copied to the destination by specifying its parent path:\r\n\r\n```dockerfile\r\nCOPY somedir \/somewhere\/\r\n```\r\n\r\n","> Ah, right, I didn't consider there's an issue with copying _directories_, and it's a difficult one to fix, because doing so would break existing behavior; you can find some details in this ticket in the BuildKit repository;\r\n> \r\n> * [bug: COPY does not take \/. (slash-dot) into account\u00a0buildkit#791](https:\/\/github.com\/moby\/buildkit\/issues\/791)\r\n> \r\n> Basically, the differences are _subtle_ but important for this specific case; this is how it _should_ have worked; and in that case, the \"source\" directory itself could be copied to the destination by specifying its parent path:\r\n> \r\n> ```dockerfile\r\n> COPY somedir \/somewhere\/\r\n> ```\r\n\r\nThis means that if I want to copy a directory through dockerfile, I need to specify its dest_path as its upper-level directory."],"labels":["area\/builder","status\/0-triage","status\/more-info-needed","kind\/bug","version\/unsupported","version\/18.09"]},{"title":"client: make streaming functions easier to consume","body":"### Description\n\nThis is just a quick draft, related to https:\/\/github.com\/moby\/moby\/pull\/46527, but there's been other discussions in this area that I may add here. This ticket should also be added to an epic (to be created) about improving the client (or providing a better sdk \/ better tools for functionality that's currently living in docker\/cli, but should be more easy to consume).\r\n\r\n\r\nThe client.ImagePull (as well as other functions that use a streaming endpoint)\r\nfunction is very hard to consume. To use these functions, a great amount of\r\nboiler-plating code, as well as custom logic is needed, and the utilities to\r\nhelp with this (`pkg\/jsonmessage` and\/or `pkg\/progress`, which both seem to be\r\nserving similar functionality), are poorly written;\r\n\r\nLooking at this (client.ImagePull) example:\r\n\r\n- The error returned is only for the initial request (which is \"reasonable\",\r\n  and makes sense for some situations, but is definitely not intuitive).\r\n- A result of that is that _not_ handling the stream means \"cancelling\" the\r\n  pull.\r\n- To get errors that occur during the pull, it's required to process the\r\n  stream using the `jsonmessage` package. And the utilities in this package\r\n  are tightly integrated with _presenting_ the stream (i.e., error-handling\r\n  is part of \"presenting\" the stream).\r\n- Because of this coupling, it also expects things like \"do we have a terminal\r\n  attached\"? (and if so: its file-descriptor), which is irrelevant for many\r\n  scenarios where the pull is to be used for purposes other than a pull on\r\n  the command-line.\r\n- _if_ we get an error, it's a `JSONError`, which _does_ have a status-code\r\n  (although it appears to not be used) and cannot be handled by the `errdefs`\r\n  package.\r\n\r\n \r\nWe should make this better. Some options to consider:\r\n\r\n- (lower hanging fruit): add an \"synchronous\" option, and move the stream-\r\n  handling logic into the `ImagePull` function. This makes the function\r\n  handle the pull from start to finish, printing the output (if `io.writers`\r\n  are passed for this), and returns any error that occurs while pulling,\r\n  which could be either the initial error (making the request), or errors\r\n  during the pull.\r\n- It would also make sure to return errors with a correct `errdefs` type\r\n  for further processing.\r\n- For the longer term, we must rewrite the `pkg\/jsonmessage`, as well as\r\n  the `pkg\/progress` packages: remove the duplication, and make sure we\r\n  have a canonical implementation where needed.\r\n- But also separate _handling_ the stream from _presenting_ the stream;\r\n  presentation should be separate, and something the consumer should be\r\n  able to have full control over.\r\n- This would be smilar to the `auxCallback` which was added at some point,\r\n  but instead, any presentation function \/ callback should be optional\r\n  (possibly some default with printing to a vanilla io.writer: TBD).\r\n\r\n\r\n","comments":[],"labels":["area\/api","area\/cli","kind\/enhancement","kind\/refactor"]},{"title":"Flaky test: TestAuthZPluginAllowEventStream produces panic, but passes, and concurrency issues","body":"### Description\n\n- saw this on https:\/\/github.com\/moby\/moby\/pull\/46522\r\n\r\nI noticed this test showing a panic, but the test itself passing\r\nhttps:\/\/ci-next.docker.com\/public\/blue\/organizations\/jenkins\/moby\/detail\/PR-46522\/2\/pipeline\r\n\r\n\r\nThe TestAuthZPluginAllowEventStream test produces a panic during the test, but the test itself passes. There's a couple of panics listed;\r\n\r\n```\r\n[2023-09-21T07:37:39.335Z] 2023\/09\/21 07:37:39 http: panic serving 127.0.0.1:34084: runtime error: invalid memory address or nil pointer dereference\r\n[2023-09-21T07:37:40.274Z] 2023\/09\/21 07:37:40 http: panic serving 127.0.0.1:34074: could not unmarshal json for \/AuthZPlugin.AuthZRes: unexpected end of JSON input\r\n[2023-09-21T07:37:42.182Z] 2023\/09\/21 07:37:42 http: panic serving 127.0.0.1:34098: could not unmarshal json for \/AuthZPlugin.AuthZRes: unexpected end of JSON input\r\n```\r\n\r\n```\r\n[2023-09-21T07:37:38.396Z] === RUN   TestAuthZPluginAllowEventStream\r\n[2023-09-21T07:37:39.335Z] 2023\/09\/21 07:37:39 http: panic serving 127.0.0.1:34084: runtime error: invalid memory address or nil pointer dereference\r\n[2023-09-21T07:37:39.335Z] goroutine 127 [running]:\r\n[2023-09-21T07:37:39.335Z] net\/http.(*conn).serve.func1()\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1854 +0xb0\r\n[2023-09-21T07:37:39.335Z] panic({0xb157a0, 0x164c2d0})\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/runtime\/panic.go:890 +0x248\r\n[2023-09-21T07:37:39.335Z] github.com\/docker\/docker\/integration\/plugin\/authz.setupSuite.func3({0xe10a90, 0x400044d920}, 0xffff5a3ab488?)\r\n[2023-09-21T07:37:39.335Z] \t\/go\/src\/github.com\/docker\/docker\/integration\/plugin\/authz\/main_test.go:159 +0x14c\r\n[2023-09-21T07:37:39.335Z] net\/http.HandlerFunc.ServeHTTP(0x40002a8300?, {0xe10a90?, 0x400044d920?}, 0x412cf4?)\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2122 +0x38\r\n[2023-09-21T07:37:39.335Z] net\/http.(*ServeMux).ServeHTTP(0xe10f58?, {0xe10a90, 0x400044d920}, 0x40002a8300)\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2500 +0x13c\r\n[2023-09-21T07:37:39.335Z] github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*Handler).ServeHTTP(0x40001441e0, {0xe0fe60?, 0x40002750a0}, 0x40002a8000)\r\n[2023-09-21T07:37:39.335Z] \t\/go\/src\/github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/handler.go:189 +0x9ac\r\n[2023-09-21T07:37:39.335Z] net\/http.serverHandler.ServeHTTP({0xe069d0?}, {0xe0fe60, 0x40002750a0}, 0x40002a8000)\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2936 +0x2c0\r\n[2023-09-21T07:37:39.335Z] net\/http.(*conn).serve(0x4000232e10, {0xe10f58, 0x400011a6c0})\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1995 +0x518\r\n[2023-09-21T07:37:39.335Z] created by net\/http.(*Server).Serve\r\n[2023-09-21T07:37:39.335Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:3089 +0x4e8\r\n[2023-09-21T07:37:40.274Z] 2023\/09\/21 07:37:40 http: panic serving 127.0.0.1:34074: could not unmarshal json for \/AuthZPlugin.AuthZRes: unexpected end of JSON input\r\n[2023-09-21T07:37:40.274Z] goroutine 162 [running]:\r\n[2023-09-21T07:37:40.274Z] net\/http.(*conn).serve.func1()\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1854 +0xb0\r\n[2023-09-21T07:37:40.274Z] panic({0xab9e20, 0x4000124b80})\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/runtime\/panic.go:890 +0x248\r\n[2023-09-21T07:37:40.274Z] github.com\/docker\/docker\/integration\/plugin\/authz.setupSuite.func3({0xe10a90, 0x400044da40}, 0xffff5a3ab488?)\r\n[2023-09-21T07:37:40.274Z] \t\/go\/src\/github.com\/docker\/docker\/integration\/plugin\/authz\/main_test.go:149 +0x36c\r\n[2023-09-21T07:37:40.274Z] net\/http.HandlerFunc.ServeHTTP(0x40002a8800?, {0xe10a90?, 0x400044da40?}, 0x412cf4?)\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2122 +0x38\r\n[2023-09-21T07:37:40.274Z] net\/http.(*ServeMux).ServeHTTP(0xe10f58?, {0xe10a90, 0x400044da40}, 0x40002a8800)\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2500 +0x13c\r\n[2023-09-21T07:37:40.274Z] github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*Handler).ServeHTTP(0x40001441e0, {0xe0fe60?, 0x4000275180}, 0x40002a8400)\r\n[2023-09-21T07:37:40.274Z] \t\/go\/src\/github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/handler.go:189 +0x9ac\r\n[2023-09-21T07:37:40.274Z] net\/http.serverHandler.ServeHTTP({0x40002f4600?}, {0xe0fe60, 0x4000275180}, 0x40002a8400)\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2936 +0x2c0\r\n[2023-09-21T07:37:40.274Z] net\/http.(*conn).serve(0x400012f950, {0xe10f58, 0x400011a6c0})\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1995 +0x518\r\n[2023-09-21T07:37:40.274Z] created by net\/http.(*Server).Serve\r\n[2023-09-21T07:37:40.274Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:3089 +0x4e8\r\n[2023-09-21T07:37:42.182Z] 2023\/09\/21 07:37:42 http: panic serving 127.0.0.1:34098: could not unmarshal json for \/AuthZPlugin.AuthZRes: unexpected end of JSON input\r\n[2023-09-21T07:37:42.182Z] goroutine 181 [running]:\r\n[2023-09-21T07:37:42.182Z] net\/http.(*conn).serve.func1()\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1854 +0xb0\r\n[2023-09-21T07:37:42.182Z] panic({0xab9e20, 0x4000124c70})\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/runtime\/panic.go:890 +0x248\r\n[2023-09-21T07:37:42.182Z] github.com\/docker\/docker\/integration\/plugin\/authz.setupSuite.func3({0xe10a90, 0x400044db60}, 0xffff5a3ab488?)\r\n[2023-09-21T07:37:42.182Z] \t\/go\/src\/github.com\/docker\/docker\/integration\/plugin\/authz\/main_test.go:149 +0x36c\r\n[2023-09-21T07:37:42.182Z] net\/http.HandlerFunc.ServeHTTP(0x40002a8d00?, {0xe10a90?, 0x400044db60?}, 0x412cf4?)\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2122 +0x38\r\n[2023-09-21T07:37:42.182Z] net\/http.(*ServeMux).ServeHTTP(0xe10f58?, {0xe10a90, 0x400044db60}, 0x40002a8d00)\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2500 +0x13c\r\n[2023-09-21T07:37:42.182Z] github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp.(*Handler).ServeHTTP(0x40001441e0, {0xe0fe60?, 0x4000275260}, 0x40002a8900)\r\n[2023-09-21T07:37:42.182Z] \t\/go\/src\/github.com\/docker\/docker\/vendor\/go.opentelemetry.io\/contrib\/instrumentation\/net\/http\/otelhttp\/handler.go:189 +0x9ac\r\n[2023-09-21T07:37:42.182Z] net\/http.serverHandler.ServeHTTP({0x4000295350?}, {0xe0fe60, 0x4000275260}, 0x40002a8900)\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:2936 +0x2c0\r\n[2023-09-21T07:37:42.182Z] net\/http.(*conn).serve(0x40005feab0, {0xe10f58, 0x400011a6c0})\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:1995 +0x518\r\n[2023-09-21T07:37:42.182Z] created by net\/http.(*Server).Serve\r\n[2023-09-21T07:37:42.182Z] \t\/usr\/local\/go\/src\/net\/http\/server.go:3089 +0x4e8\r\n[2023-09-21T07:37:44.721Z] --- PASS: TestAuthZPluginAllowEventStream (6.05s)\r\n```\r\n\r\nThose are happening around this code; https:\/\/github.com\/moby\/moby\/blob\/3a520c1c0f0837e652347bca5d4f27ee0fd99a2f\/integration\/plugin\/authz\/main_test.go#L140-L172\r\n\r\nSome of those panics may be just \"noise\" if the test doesn't actually send a proper request (which could explain the EOF errors when unmarshaling JSON), but there's also some weird things happening in that test-suite (at a quick glance), which at least wouldn't work well concurrently;\r\n\r\nThere is a `setupTestV1` function which is used to set up the tests, and both _creates_ and _cleans up_ the `\/etc\/docker\/plugins` (hard-coded path); https:\/\/github.com\/moby\/moby\/blob\/5e7eade1f74ca8305dc3760687a44aa4b2373a3b\/integration\/plugin\/authz\/authz_plugin_test.go#L53-L71\r\n\r\nThat function is called by various tests;\r\n\r\n```bash\r\ngit grep 'setupTestV1'\r\nintegration\/plugin\/authz\/authz_plugin_test.go:func setupTestV1(t *testing.T) context.Context {\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\nintegration\/plugin\/authz\/authz_plugin_test.go:  ctx := setupTestV1(t)\r\n```\r\n\r\nSimilar to using the same `\/etc\/docker\/plugins` directory, there's also a package-level `ctrl` variable, which is initialized (and reset) in that function, which is used (and mutated) by all tests; https:\/\/github.com\/moby\/moby\/blob\/5e7eade1f74ca8305dc3760687a44aa4b2373a3b\/integration\/plugin\/authz\/authz_plugin_test.go#L39-L40\r\n\r\nEach test spins up its own daemon, so probably these part could be separated. No idea though if any of this is related; I don't see tests running in parallel, but the hard-coded `\/etc\/docker\/plugins` path on its own looks scary (if other suites may be running in parallel).\r\n","comments":[],"labels":["area\/testing","area\/plugins"]},{"title":"Flaky test: TestLiveRestore flakyness","body":"### Description\n\nThe `TestLiveRestore`  is knowns to be flaky, we should take a look at it, it sometimes fails with \r\n\r\n```console\r\n=== Failed\r\n=== FAIL: amd64.integration.daemon TestLiveRestore\/volume_references\/local_volume_with_mount_options (2.41s)\r\n    daemon_test.go:513: assertion failed: error is not nil: Error response from daemon: remove test-live-restore-volume-references-local: volume has active mounts\r\n        --- FAIL: TestLiveRestore\/volume_references\/local_volume_with_mount_options (2.41s)\r\n\r\n=== FAIL: amd64.integration.daemon TestLiveRestore\/volume_references (17.68s)\r\n    --- FAIL: TestLiveRestore\/volume_references (17.68s)\r\n\r\n=== FAIL: amd64.integration.daemon TestLiveRestore (0.00s)\r\n```\r\n\r\nExample PR where it failed with unrelated changes: https:\/\/github.com\/moby\/moby\/pull\/46507\n\n### Reproduce\n\nOpen an unrelated PR, see it fail\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nmaster\n```\n\n\n### docker info\n\n```bash\nmaster\n```\n\n\n### Additional Info\n\n_No response_","comments":["- Could be related to https:\/\/github.com\/moby\/moby\/pull\/46253"],"labels":["kind\/bug","area\/testing"]},{"title":"define \"reserved\" ambiguous image references (sha256 etc)","body":"When tagging images, we must reject some references that can be ambiguous, and can be interpreted \/ parsed as digests (e.g. `sha256:deadbeef` could be either an image-repo named `sha256` with a tag `deadbeef`, or could be a digest ( `sha256:deadbeef`)\r\n\r\n\r\nIdeally we'd take _other_ algorithms into account as well, as they _may_ occur as well.\r\n\r\nWhile the OCI spec does not define an \"image-ref\", it _does_ define a couple of algorithms that are _registered_ in the spec (sha256, sha512);\r\nhttps:\/\/github.com\/opencontainers\/image-spec\/blob\/v1.1.0-rc5\/descriptor.md#registered-algorithms, so those are good candidates to _always_ ignore (as the spec defines that the can be used).\r\n\r\nIt also describes some other (more esoteric) ones as \"optional\" https:\/\/github.com\/opencontainers\/image-spec\/blob\/v1.1.0-rc5\/descriptor.md#digests\r\n\r\nAnd potentially other algorithms (`sha384`, and `blake3` variants) as well;\r\n\r\n- https:\/\/github.com\/opencontainers\/go-digest\/blob\/ea51bea511f75cfa3ef6098cc253c5c3609b037a\/algorithm.go#L33-L35\r\n- https:\/\/github.com\/opencontainers\/go-digest\/pull\/66\r\n- https:\/\/github.com\/opencontainers\/image-spec\/issues\/819\r\n- :warning: all of which could be \"dynamically\" registered: https:\/\/github.com\/opencontainers\/go-digest\/pull\/62\r\n- :question: :warning: and _possibly_ even other ones that are implicitly registered (looking at you, `md5`)\r\n\r\nSo, I think we may need some follow-ups;\r\n\r\n- Perhaps `go-digest` could have a utility function that returns the list of registered algorithms (assuming that's the canonical list of algorithms that we accept: https:\/\/github.com\/opencontainers\/go-digest\/blob\/316f76db9b0f1b3620c3380cc34f75d262dea6db\/algorithm.go#L78-L96)\r\n- A similar utility could\/should be added to https:\/\/github.com\/distribution\/reference. Baring an OCI specification for image-references, I think we should consider that to be the canonical implementation for image references\r\n- So it should _at least_ have a utility to check for ambiguous references\r\n- And _possibly_ invalidate such references automatically\r\n- for any \"image reference spec\", it would be hard to define a \"dynamic\" list though. and unfortunately, [the `go-digest` regex is overly broad](https:\/\/github.com\/opencontainers\/go-digest\/blob\/316f76db9b0f1b3620c3380cc34f75d262dea6db\/algorithm.go#L42), so can't be used to detect whether we're dealing with a (possible) algorithm, or an image name (it looks like some context on that RegEx was removed in https:\/\/github.com\/opencontainers\/go-digest\/commit\/b9e02e015be61903bbee58e3fd349114fa28e0b4 :thinking:)\r\n- So _maybe_ such a spec must define a minimum set of algorithms to consider \"reserved\" (MUST be rejected), and possibly a recommendation on what patterns to _consider_ an algorithm (which MAY be rejected).\r\n\r\n_Originally posted by @thaJeztah in https:\/\/github.com\/moby\/moby\/pull\/46492#discussion_r1328097131_\r\n            ","comments":[],"labels":["area\/distribution","area\/images","containerd-integration"]},{"title":"Error creating overlay mount to \/var\/lib\/docker\/overlay2\/...-init\/merged: too many levels of symbolic links","body":"### Description\r\n\r\n```bash\r\ndocker ps\r\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\r\n\r\ndocker run getmeili\/meilisearch\r\ndocker: Error response from daemon: error creating overlay mount to \/var\/lib\/docker\/overlay2\/59f35ca50968fa44c283f7a613ae3b39b9468538cb34cc86ac761062641d1885-init\/merged: too many levels of symbolic links.\r\nSee 'docker run --help'.\r\n```\r\n\r\n\r\n-----------\r\n\r\n```bash\r\nscreenfetch\r\nneofetch\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     max@system\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     OS: Manjaro 23.0.1 Uranos\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Kernel: x86_64 Linux 6.5.1-1-MANJARO\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Uptime: 1h 5m\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Packages: 1583\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Shell: zsh 5.9\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Resolution: No X Server\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     DE: GNOME 44.3\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     WM: Mutter\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     WM Theme: CustomAccentColors\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     GTK Theme: Adw-dark [GTK2\/3]\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Icon Theme: Papirus-Dark-Maia\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Font: Noto Sans 11\r\n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     Disk: 25G \/ 225G (12%)\r\n                                  CPU: Intel Core i5-6500 @ 4x 3.6GHz [43.0\u00b0C]\r\n                                  GPU: Mesa Intel(R) HD Graphics 530 (SKL GT2)\r\n                                  RAM: 6370MiB \/ 15878MiB\r\n```\r\n\r\n```bash\r\nneofetch                                                                                     \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   max@system \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   ---------- \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   OS: Manjaro Linux x86_64 \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Host: HP EliteDesk 800 G2 SFF \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Kernel: 6.5.1-1-MANJARO \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Uptime: 1 hour, 5 mins \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Packages: 1583 (pacman) \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Shell: zsh 5.9 \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Resolution: 1920x1080 \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   DE: GNOME 44.4 \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   WM: Mutter \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   WM Theme: CustomAccentColors \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Theme: Adw-dark [GTK2\/3] \r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   Icons: Papirus-Dark-Maia [GTK2\/3] \r\n                               Terminal: gnome-terminal \r\n                               CPU: Intel i5-6500 (4) @ 3.600GHz \r\n                               GPU: Intel HD Graphics 530 \r\n                               Memory: 6034MiB \/ 15878MiB \r\n```\r\n\r\n\r\n### Reproduce\r\n\r\n```bash\r\ndocker run getmeili\/meilisearch\r\n```\r\n\r\n### Expected behavior\r\n\r\nWithout errors\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996600\r\n Built:             Wed Jul 26 21:44:58 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.23.0 (120376)\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:32:16 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.2\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.20.3\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-compose\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-extension\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.1.0-280-gc7fa31d4c4\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.3.13-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.713GiB\r\n Name: docker-desktop\r\n ID: 528e6332-0685-43c5-a5a0-e513dfb1a062\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["can you provide more details?\r\n\r\n- are you seeing the same problem with any image or only this specific one? (`getmeili\/meilisearch`)\r\n- were you successfully able to run containers before on this system (was this an existing install?) or was this a fresh install of Docker Desktop?\r\n- if you were able to run containers successfully before, was there anything that happened? (e.g. you ran out of disk-space, or your system crashed, or it happened after updating Docker Desktop or your distro)?\r\n\r\n\r\n","Hello, commenting here since I have the same problem and I don't want to open a new issue. The only solution apparently is to do `docker system prune` or cleaning up images in some other way, but I think there must be a real solution.\r\n\r\nIn my case the issue happens using AWS ECS, and I still cannot reproduce it consistently since I use AWS Batch and I see jobs failing randomly and I cannot SSH into the machine to get more info. The error I see is:\r\n\r\n```\r\nCannotCreateContainerError: Error response from daemon: error creating overlay mount to \/var\/lib\/docker\/overlay2\/[SOME HASH]-init\/merged: too many levels of symbolic links\r\n```\r\n\r\nThis happens very unlikely, like once each 100000 successful runs, after the first failure then a lot of successive runs fail with the same error, I suppose until AWS starts running things in another machine. \r\n\r\nInteresting things I've seen so far, based on my limited understanding of moby\/docker\/kernel\/overlayfs:\r\n\r\n- **Most people just recommend to purge\/clean docker data**, which for us is not a solution. Examples: [here](https:\/\/github.com\/docker\/for-win\/issues\/5763), [here](https:\/\/forums.docker.com\/t\/cant-pull-image-error-failed-to-register-layer\/88410), [here](https:\/\/stackoverflow.com\/questions\/61549982\/docker-v19-03-windows-10-too-many-levels-of-symbolic-links), [here](https:\/\/stackoverflow.com\/questions\/71743612\/could-not-pull-image-caused-by-failed-to-register-layer-error-creating-overl), [here](https:\/\/stackoverflow.com\/questions\/60209172\/docker-cannot-delete-intermediate-images-from-a-broken-pull)\r\n\r\n- **Maybe can be due to running out of RAM or disk. I think it is not the case** since I get many failures about \"too many levels of symbolic links\" but none because of other reasons: if the machine is on the limit of RAM\/disk usage I should also see failures because of running out of RAM\/disk (e.g. the OOM killer is never run, I don't see errors when saving results, etc.). Also, I made one machine run out of RAM and disk on purpose and the errors are different\r\n\r\n- **I think that overlay2\/overlayfs doesn\u2019t use a particularly high number of symlinks or hard links**. For example is not like each layer contains hard links to all files of the previous layers. [Here there is some info about the the internals of overlay2 in docker](https:\/\/unix.stackexchange.com\/questions\/510931\/overlay-storage-driver-internals). Just in case I checked this by counting soft links and hard links when pulling my image, and I don't see anything strange, so I don't think that the \"filesystem is running out of links\" or whatever\r\n\r\n-  **I don't think there is a problem with my image** because it works most of the time. I checked if there are symlink looks and I see loops due to \/var\/run\/speech-dispatcher, hdf5, X11, I guess this is normal. The command I used is `sudo find -L \/var\/lib\/docker -xtype l > \/dev\/null`. I guess it should happen for other docker images but I don't have proof since I use only one docker image. I also tried reducing the number of layers of my image but I saw no changes\r\n\r\n- Looking at linux source code, **the \u201ctoo many level of symbolic links\u201d is called ELOOP**, so this is another useful keyword when googling. Also, I see that ELOOP is used in some IP\/networking code, so maybe there can be networking errors returning \"too many levels of symbolic links\"? It wouldnt make sense\r\n \r\n- **It can be related to the fact that a \"strange\" disk or filesystem is used**. I found issues relating this problem to the usage of NFS, and in my case I'm using AWS EBS.\r\n\r\n- **It can be a bug in overlayfs** since I see something similar happened before and it was patched ([here](https:\/\/git.kernel.org\/pub\/scm\/linux\/kernel\/git\/torvalds\/linux.git\/commit\/?id=146d62e5a5867fbf84490d82455718bfb10fe824), [here](https:\/\/github.com\/torvalds\/linux\/commit\/0be0bfd2de9dfdd2098a9c5b14bdd8f739c9165d)), but I don't know if it is related to how moby\/docker uses overlayfs. Similarly, it can happen due to a bug in moby\/docker that causes a wrong usage of overlayfs ([overlayfs sometimes gives \"too many levels of symbolic links errors\" if you use it wrong](https:\/\/forums.developer.nvidia.com\/t\/ros-how-to-cross-compile-ros-for-drive-agx-developer-kit-with-drive-software-10-0\/111157\/21))\r\n\r\n- **Could be due a corrupted download of an image**. E.g see #42964 \r\n\r\n- **Maybe it is just bad luck**: I imagine that a super unlikely disk data corruption can produce this error, which for me it is fine, it is not Docker's fault. But the problem is that it seems to leave Docker in this broken state until the Docker data is pruned\/cleaned. In my case I'm launching thousands of docker containers per hour and I don't care if sometimes it fails since I set up 10 retries, but most of the time seems that the 10 attempts are run in the same machine so it always fails with the same error. It would be nice if Docker somehow detected a possible corruption and solved the problem automatically \r\n\r\n- I think I read everything there is on the web with the relevant keyboards but found nothing conclusive :cry: \r\n\r\nOther interesting links:\r\n\r\n- https:\/\/forums.docker.com\/t\/could-not-pull-image-caused-by-failed-to-register-layer-error-creating-overlay-mount-to-var-lib-docker-overlay2-too-many-levels-of-symbolic-links\/123219\r\n- https:\/\/forum.openmediavault.org\/index.php?thread\/42867-clean-omv6-raspi4-too-many-levels-of-symbolic-links\/\r\n- https:\/\/serverfault.com\/questions\/640895\/why-do-some-host-volumes-in-docker-containers-give-the-error-too-many-levels-of\r\n","Sorry for pinging this issue. I'm still not able to reproduce it in an isolated way. I want to ask you three quick questions because I don't know the internals of Docker\/Moby:\r\n\r\n- When are the overlay mounts created? During image download or when running a Docker container? I want to know what is Docker doing when this error happen (`error creating overlay mount to \/var\/lib\/docker\/overlay2\/[SOME HASH]-init\/merged: too many levels of symbolic links`)\r\n\r\n- Do you have any idea of why this could happen? My top theory is disk or file corruption just because of bad luck\r\n\r\n- Do you imagine any other solution apart purging all the Docker data (e.g just removing the layer with the error and doing docker pull again?)\r\n\r\nThank you!"],"labels":["area\/storage\/overlay","status\/0-triage","status\/more-info-needed","kind\/bug","version\/24.0"]},{"title":"Delete erroneous sanity check in `setupIPChains`","body":"This function is called for both IPv4 and IPv6, so this check broke `dockerd --iptables=false --ip6tables=true`.\r\n\r\n**- How to verify it**\r\n\r\n```\r\ndockerd --ipv6 --fixed-cidr-v6=fd00::\/64 --iptables=false --experimental --ip6tables\r\n```\r\n\r\n**- Description for the changelog**\r\n\r\n`dockerd --iptables=false --ip6tables=true` now works.\r\n\r\nfixes #46467 ","comments":[],"labels":["status\/2-code-review","area\/networking","area\/networking\/ipv6"]},{"title":"dockerd fails to start with `--iptables=false --ip6tables`","body":"### Description\n\nWhen started with `--iptables=false` and `--ip6tables`, `dockerd` fails with:\r\n\r\n> failed to start daemon: Error initializing network controller: error obtaining controller instance: failed to register \"bridge\" driver: cannot create new chains, EnableIPTable is disabled\r\n\n\n### Reproduce\n\n```\r\ndockerd --ipv6 --fixed-cidr-v6=fd00::\/64 --iptables=false --experimental --ip6tables\r\n```\r\n\n\n### Expected behavior\n\n`dockerd` should start, and create IPv6 rules but not IPv4 rules.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:31:44 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:44 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1159\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-26-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 23.46GiB\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.17.0.0\/16, Size: 24\r\n   Base: 172.18.0.0\/15, Size: 23\r\n   Base: 172.20.0.0\/14, Size: 22\r\n   Base: 172.24.0.0\/13, Size: 21\r\n   Base: fd01::\/64, Size: 64\n```\n\n\n### Additional Info\n\nThe following change should fix it (I'll open a pull request after I write a regression test):\r\n\r\n```diff\r\ndiff --git a\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go b\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go\r\nindex c98781f3b0..2d3ed57d9d 100644\r\n--- a\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go\r\n+++ b\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go\r\n@@ -31,11 +31,6 @@ const (\r\n )\r\n \r\n func setupIPChains(config configuration, version iptables.IPVersion) (natChain *iptables.ChainInfo, filterChain *iptables.ChainInfo, isolationChain1 *iptables.ChainInfo, isolationChain2 *iptables.ChainInfo, retErr error) {\r\n-\t\/\/ Sanity check.\r\n-\tif !config.EnableIPTables {\r\n-\t\treturn nil, nil, nil, nil, errors.New(\"cannot create new chains, EnableIPTable is disabled\")\r\n-\t}\r\n-\r\n \thairpinMode := !config.EnableUserlandProxy\r\n \r\n  \tiptable := iptables.GetIptable(version)\r\n```","comments":["Could you elaborate on why you want `iptables` disabled but `ip6tables` enabled?","I want to manually control IPv4 rules but I'm OK with Docker controlling the IPv6 rules.","> I want to manually control IPv4 rules but I'm OK with Docker controlling the IPv6 rules.\r\n\r\nCan you elaborate on _why_? Is it due to some issues you face with the iptables rules Docker creates?","Oh! Looks like I missed that a discussion was happening here as well; I left some comments related to this in https:\/\/github.com\/moby\/moby\/pull\/46468#discussion_r1324316529","I'm mostly experimenting right now.  I'm working on [adding some outgoing NAT rule unit tests](https:\/\/github.com\/rhansen\/moby\/commit\/nat-test) in preparation for [fixing](https:\/\/github.com\/rhansen\/moby\/commit\/host_ipv6) feature request #46469, and noticed this issue. I didn't expect it to be so controversial.\r\n\r\nOne reason to disable iptables (IPv4) but enable ip6tables (IPv6) is [running multiple Docker daemons](https:\/\/docs.docker.com\/engine\/reference\/commandline\/dockerd\/#run-multiple-daemons), where only one of the daemons does IPv6.","(@thaJeztah I'm moving the conversation in https:\/\/github.com\/moby\/moby\/pull\/46468#discussion_r1324316529 here because deciding the semantics of the `--iptables` flag deserves greater visibility, and in case it is decided that that PR is not the right way forward)\r\n\r\n> we need to be careful here\r\n\r\nAgreed.\r\n\r\nRegardless of the decision reached, I think the documentation for the `--iptables` flag should be updated to clarify its intended purpose.\r\n\r\n> *IF* we would working from a greenfield state\r\n\r\nIn a way we are in a greenfield state right now.  Passing `--iptables=false --ip6tables=true` is invalid due to [this check](https:\/\/github.com\/moby\/moby\/blob\/20f96354699d20eeb4674d3944486b5d73c256b2\/libnetwork\/drivers\/bridge\/setup_ip_tables_linux.go#L34-L37), so nobody uses it.  We can apply any semantics we want to that particular combination, as long as it:\r\n\r\n  * makes sense,\r\n  * doesn't change the behavior of the other combinations, and\r\n  * doesn't conflict with the flags' current documentation (which would risk surprising users).\r\n\r\nThat last bullet point is the tough one.  The current output of `--help` says:\r\n\r\n```\r\n  --ip6tables  Enable addition of ip6tables rules (experimental)\r\n  --iptables   Enable addition of iptables rules (default true)\r\n```\r\n\r\nReasonable users could interpret \"Enable addition of iptables rules\" to mean either IPv4 rules alone or both IPv4 and IPv6 rules.  (I believe most users will interpret it to mean only IPv4 rules, especially after contrasting the `--iptables` help string with the `--ip6tables` help string, and after Googling the opaque terms \"iptables rules\" and \"ip6tables rules\".)\r\n\r\n### Proposals\r\n\r\nTo ground the conversation, here are some specific proposals.  Both of these are backwards compatible.\r\n\r\nThese aren't the only options; they're just the best ones I could immediately think of.\r\n\r\n#### Option 1: `--iptables` affects both IPv4 and IPv6\r\n\r\n```\r\n  --iptables        Deprecated synonym of --netfilter\r\n  --ip6tables       Deprecated synonym of --netfilter-ipv6\r\n  --netfilter       Allow --netfilter-ipv4 and --netfilter-ipv6 to establish netfilter rules. (default true)\r\n  --netfilter-ipv4  Enable IPv4 netfilter rules.  Ignored when --netfilter=false. (default true)\r\n  --netfilter-ipv6  Enable IPv6 netfilter rules.  Ignored when --netfilter=false. (experimental, default false)\r\n```\r\n\r\nPros:\r\n  * safer: `--iptables=false` shuts off *everything*\r\n  * matches what some users expect of the `--iptables` flag (the lack of \"4\" in the flag name suggests IP in general, not IPv4 specifically)\r\n  * amenable to nftables\r\n\r\nCons:\r\n  * added cognitive load: three new flags, two deprecations, and a two-stage enable system\r\n  * mismatch between what some (most?) users expect of the `--iptables` flag vs. its actual behavior\r\n  * inconsistent with the behavior of the `iptables` command\r\n  * asymmetric behavior: `--iptables=false` overrides `--ip6tables`, but `--iptables` doesn't override `--ip6tables=false`\r\n\r\n#### Option 2: `--iptables` only affects IPv4 rules\r\n\r\n```\r\n  --ip6tables  Enable IPv6 netfilter rules. (experimental, default false)\r\n  --iptables   Enable IPv4 netfilter rules. (default true)\r\n```\r\n\r\nPros:\r\n  * simpler\r\n  * matches what some (most?) users expect of the `--iptables` flag\r\n  * consistent with the behavior of the `iptables` command\r\n  * the `--iptables` and `--ip6tables` flags are orthogonal\r\n  * already implemented in #46468 except for improvements to flag documentation\r\n\r\nCons:\r\n  * potentially less safe: users might expect `--iptables=false` to override `--ip6tables`\r\n  * mismatch between what some users expect vs. its actual behavior\r\n","I think both these flags are pretty terrible.\r\n`--iptables` was indeed added as a way to prevent docker from messing with iptables.\r\n\r\nSince `--iptables=false --ip6tables=true` is an error condition today, why not leave it an error condition and implement a better configuration option(s) which we can force users who are wanting to use both these flags into?\r\nNote: when I say \"better\" I don't mean differently named but still basically the same thing."],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/ipv6"]},{"title":"api: move version-related code to api\/types\/versions package","body":"- relates to https:\/\/github.com\/moby\/moby\/pull\/46463 (https:\/\/github.com\/moby\/moby\/pull\/46463#issuecomment-1715866997)\r\n\r\n---\r\n\r\n- implement versions.FromContext\r\n- implement versions.WithVersion to return a context with the version attached.\r\n- deprecates the APIVersionKey type in favor of WithVersion.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Ah, dang. Needs another rebase \ud83d\ude02 ","Discussing this PR with @rumpl and we both considered that `versions` (plural) is a bit of an awkward name (and not very idiomatic go) for the package.\r\n\r\nWhile this package existed for some time already, usage outside of this repository is limited (some locations in `docker\/cli`, and for some of those, we should consider moving that logic into the `client` code).\r\n\r\nAs we're moving things in this PR, we might as well rip of the bandaid and rename the package to `version` (singular); I'll have a look at doing so later.\r\n"],"labels":["area\/api","status\/2-code-review","kind\/refactor"]},{"title":"[WIP] BuildKit 0.12 network sampling","body":"- Follow-up to https:\/\/github.com\/moby\/moby\/pull\/45966","comments":["The BuildKit update was merged, so I think this one can be rebased, so that only the changes related to this will be shown in the PR"],"labels":["area\/networking","area\/builder\/buildkit"]},{"title":"`on-failure` restart policy doesn't wait 10s","body":"### Description\n\nThe Docker [docs](https:\/\/docs.docker.com\/config\/containers\/start-containers-automatically\/#restart-policy-details) states:\r\n> A restart policy only takes effect after a container starts successfully. In this case, starting successfully means that the container is up for at least 10 seconds and Docker has started monitoring it. This prevents a container which does not start at all from going into a restart loop.\r\n\r\nHowever, the `on-failure` setting does not obey this. A failed container will keep restarting even when alive for less than 10s.\r\n\r\nRelated issues:\r\n* https:\/\/github.com\/docker\/compose\/issues\/8010\r\n* https:\/\/github.com\/docker\/compose\/issues\/7619\n\n### Reproduce\n\n```\r\ndocker run -d --restart on-failure busybox \"grep\"\r\n```\n\n### Expected behavior\n\nThe container should not be restarted as it was not alive for \u2265 10 seconds.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           23.0.4\r\n API version:       1.42\r\n Go version:        go1.19.8\r\n Git commit:        f480fb1\r\n Built:             Fri Apr 14 10:32:23 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          23.0.4\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.19.8\r\n  Git commit:       cbce331\r\n  Built:            Fri Apr 14 10:32:23 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.20\r\n  GitCommit:        2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc:\r\n  Version:          1.1.5\r\n  GitCommit:        v1.1.5-0-gf19387a\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.4\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 9\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 9\r\n Images: 73\r\n Server Version: 23.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: yh6l66lukoo45yjsfwfk1fi1a\r\n  Is Manager: true\r\n  ClusterID: skb8g538hw2fiwf7bpjq4ntkl\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.2.5\r\n  Manager Addresses:\r\n   10.0.2.5:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc version: v1.1.5-0-gf19387a\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.0-1042-azure\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.822GiB\r\n Name: MyVM\r\n ID: 7SI5:5E7Z:UB7Q:4ZTH:GE4Q:EOTJ:TROW:LCRC:TCKK:AQTJ:36EK:3NSK\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Mind If I take a look at this @thaJeztah?"],"labels":["area\/runtime","status\/0-triage","kind\/bug","version\/23.0"]},{"title":"Improve UX when creating and removing containers while containerd service is not running","body":"### Description\r\n\r\nThis was after writing up some examples for a question \/ discussion, and perhaps we can improve the \"interact with containers while containerd is down\" UX a bit;\r\n\r\n- https:\/\/github.com\/moby\/moby\/discussions\/46449\r\n\r\n\r\nHere's an example where a container is started (and running), then containerd is restarted or stopped;\r\n\r\n```bash\r\n# create a container that prints the date every 2 seconds\r\ndocker run -d --rm --name mycontainer alpine sh -c 'while true; do date; sleep 2; done'\r\n\r\n# check that everything is running\r\nps auxf\r\n# ...\r\nroot       47222  0.2  4.2 1209564 42272 ?       Ssl  11:36   0:00 \/usr\/bin\/containerd\r\nroot       47370  0.0  0.9 720776  9188 ?        Sl   11:38   0:00 \/usr\/bin\/containerd-shim-runc-v2 -namespace moby -id 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 -address \/run\/containerd\/containerd.sock\r\nroot       47390  0.0  0.0   1604   936 ?        Ss   11:38   0:00  \\_ sh -c while true; do date; sleep 2; done\r\nroot       47480  0.0  0.0   1588     4 ?        S    11:39   0:00      \\_ sleep 2\r\n\r\n\r\n# stop containerd and check that it's still running\r\nsystemctl stop containerd.service\r\n\r\ndocker ps -a\r\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES\r\n4c330b522c68   alpine    \"sh -c 'while true; \u2026\"   2 minutes ago   Up 2 minutes             mycontainer\r\n\r\ndocker logs mycontainer\r\nMon Sep 11 11:28:10 UTC 2023\r\nMon Sep 11 11:28:12 UTC 2023\r\nMon Sep 11 11:28:14 UTC 2023\r\nMon Sep 11 11:28:16 UTC 2023\r\n\r\npx auxf\r\n# ...\r\nroot       47370  0.0  0.9 720776  9308 ?        Sl   11:38   0:00 \/usr\/bin\/containerd-shim-runc-v2 -namespace moby -id 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 -address \/run\/containerd\/containerd.sock\r\nroot       47390  0.0  0.0   1604   936 ?        Ss   11:38   0:00  \\_ sh -c while true; do date; sleep 2; done\r\nroot       47584  0.0  0.0   1588     4 ?        S    11:41   0:00      \\_ sleep 2\r\n```\r\n\r\nNow, while containerd is stopped, try to start a new container; this is _expected_, but perhaps we can make the error-message a bit more informative (mention \"containerd is not available\" or something);\r\n\r\n```bash\r\ndocker run -it --rm alpine\r\ndocker: Error response from daemon: connection error: desc = \"transport: Error while dialing dial unix:\/\/\/run\/containerd\/containerd.sock: timeout\": unavailable.\r\n```\r\n\r\n\r\nNow try to remove the container that's still running; this returns eventually (10 seconds?) but with an error:\r\n\r\n```bash\r\ndocker rm -fv mycontainer\r\nError response from daemon: Could not kill running container 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07, cannot remove - tried to kill container, but did not receive an exit event\r\n```\r\n\r\nNote that after this, the container's process is killed, but the shim is still running:\r\n\r\n```bash\r\npx auxf\r\n# ...\r\nroot       47370  0.0  0.9 720776  9676 ?        Sl   11:38   0:00 \/usr\/bin\/containerd-shim-runc-v2 -namespace moby -id 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 -address \/run\/containerd\/containerd.sock\r\n```\r\n\r\nAnd `docker ps` still shows the container (but logs don't get updated);\r\n\r\n```bash\r\ndocker ps -a\r\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES\r\n4c330b522c68   alpine    \"sh -c 'while true; \u2026\"   4 minutes ago   Up 4 minutes             mycontainer\r\n\r\ndocker logs mycontainer\r\nMon Sep 11 11:28:10 UTC 2023\r\n...\r\nMon Sep 11 11:31:56 UTC 2023\r\n```\r\n\r\nLogs from when containerd was stopped, and the `docker rm` was tried; the logs mention that the service was stopped, but the _shim_ process continues running (which is the expected behavior). However, because containerd is down, dockerd starts to log errors that it can't receive events;\r\n\r\n```console\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: Stopping containerd container runtime...\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:41:03.148357817Z\" level=error msg=\"Failed to get event\" error=\"rpc error: code = Unavailable desc = error reading from server: EOF\" module=libcontainerd namespace=moby\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:41:03.148458280Z\" level=info msg=\"Waiting for containerd to be ready to restart event processing\" module=libcontainerd namespace=moby\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:41:03.149260128Z\" level=error msg=\"Failed to get event\" error=\"rpc error: code = Unavailable desc = error reading from server: EOF\" module=libcontainerd namespace=plugins.moby\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:41:03.149299740Z\" level=info msg=\"Waiting for containerd to be ready to restart event processing\" module=libcontainerd namespace=plugins.moby\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: containerd.service: Deactivated successfully.\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: containerd.service: Unit process 47370 (containerd-shim) remains running after unit stopped.\r\nSep 11 11:41:03 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: Stopped containerd container runtime.\r\n```\r\n\r\nWhen trying to delete the container, the `kill` and `delete` fail;\r\n\r\n```console\r\nSep 11 11:42:47 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:42:47.851968146Z\" level=error msg=\"Container failed to exit within 10s of kill - trying direct SIGKILL\" container=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 error=\"context deadline exceeded\"\r\nSep 11 11:42:47 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: docker-4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07.scope: Deactivated successfully.\r\nSep 11 11:42:49 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T11:42:49.857466567Z\" level=error msg=\"Handler for DELETE \/v1.43\/containers\/mycontainer returned error: Could not kill running container 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07, cannot remove - tried to kill container, but did not receive an exit event\"\r\n```\r\n\r\nThe container's state is incorrect at this point, because no event was received to update its status;\r\n\r\n```bash\r\ndocker container inspect --format '{{ json .State }}' mycontainer | jq .\r\n{\r\n  \"Status\": \"running\",\r\n  \"Running\": true,\r\n  \"Paused\": false,\r\n  \"Restarting\": false,\r\n  \"OOMKilled\": false,\r\n  \"Dead\": false,\r\n  \"Pid\": 47390,\r\n  \"ExitCode\": 0,\r\n  \"Error\": \"\",\r\n  \"StartedAt\": \"2023-09-11T11:39:00.39465654Z\",\r\n  \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n}\r\n```\r\n\r\nAfter starting containerd again, the container can be removed, but again takes longer than expected;\r\n\r\n```bash\r\nsystemctl start containerd.service\r\n\r\ndocker ps -a\r\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\r\n4c330b522c68   alpine    \"sh -c 'while true; \u2026\"   24 minutes ago   Up 24 minutes             mycontainer\r\n\r\ndocker rm -fv mycontainer\r\nmycontainer\r\n```\r\n\r\nLogs from starting containerd, and removing the container contains some logs about network interfaces (perhaps something to look into?):\r\n\r\n```console\r\nSep 11 12:03:18 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: Started containerd container runtime.\r\nSep 11 12:03:18 ubuntu-s-1vcpu-1gb-ams3-01 containerd[48604]: time=\"2023-09-11T12:03:18.307703964Z\" level=info msg=\"containerd successfully booted in 0.039373s\"\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T12:03:35.171231154Z\" level=error msg=\"Container failed to exit within 10s of kill - trying direct SIGKILL\" container=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 error=\"context deadline exceeded\"\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T12:03:35.187655527Z\" level=info msg=\"ignoring event\" container=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 containerd[48604]: time=\"2023-09-11T12:03:35.188167420Z\" level=info msg=\"shim disconnected\" id=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 containerd[48604]: time=\"2023-09-11T12:03:35.188604659Z\" level=warning msg=\"cleaning up after shim disconnected\" id=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 namespace=moby\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 containerd[48604]: time=\"2023-09-11T12:03:35.188750132Z\" level=info msg=\"cleaning up dead shim\"\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 containerd[48604]: time=\"2023-09-11T12:03:35.199328471Z\" level=warning msg=\"cleanup warnings time=\\\"2023-09-11T12:03:35Z\\\" level=info msg=\\\"starting signal loop\\\" namespace=moby pid=48633 runtime=io.containerd.runc.v2\\n\"\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 systemd-networkd[47914]: veth6b09306: Lost carrier\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 kernel: [423810.371657] docker0: port 1(veth6b09306) entered disabled state\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 kernel: [423810.375041] vethbfca065: renamed from eth0\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 networkd-dispatcher[696]: WARNING:Unknown index 19 seen, reloading interface list\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 systemd-udevd[48648]: Using default interface naming scheme 'v249'.\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 systemd-networkd[47914]: veth6b09306: Link DOWN\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 kernel: [423810.397183] docker0: port 1(veth6b09306) entered disabled state\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 kernel: [423810.398437] device veth6b09306 left promiscuous mode\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 kernel: [423810.398443] docker0: port 1(veth6b09306) entered disabled state\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 networkd-dispatcher[696]: ERROR:Unknown interface index 19 seen even after reload\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 networkd-dispatcher[696]: WARNING:Unknown index 19 seen, reloading interface list\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: run-docker-netns-e93e84a80b3e.mount: Deactivated successfully.\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 systemd[1]: var-lib-docker-overlay2-bb6c4fe149ed089867eff21680939d764a30de35a8035dfaa6ee559366410e1b-merged.mount: Deactivated successfully.\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 networkd-dispatcher[696]: ERROR:Unknown interface index 19 seen even after reload\r\nSep 11 12:03:35 ubuntu-s-1vcpu-1gb-ams3-01 dockerd[29327]: time=\"2023-09-11T12:03:35.287777802Z\" level=error msg=\"error removing container\" container=4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 error=\"removal of container 4c330b522c68b8ece1947deffddb92913f6412d0a4cbacc75772922d82bb6a07 is already in progress\"\r\nSep 11 12:03:36 ubuntu-s-1vcpu-1gb-ams3-01 systemd-networkd[47914]: docker0: Lost carrier\r\n```\r\n\r\nPossible improvements here;\r\n\r\n- Should we log a more informative warning that containerd is not available to handle requests?\r\n- TBD; do we want to (by default) still continue killing the container, or should we only do so \"opt-in\"?\r\n    - The _good thing_ here is that we can still kill a container (i.e. the process), which can be useful\r\n    - The \"bad\" thing is that we don't inform why things took longer than expected (and the fact we couldn't connect with containerd)\r\n    - The \"bad\" thing is that the container's state no longer matches reality; do we have different means to get the actual state? (the shim is still running, so .. maybe?)\r\n\r\n\r\n-----\r\n\r\n\r\nThis was all on Docker 24.0.6, without containerd image-store enabled","comments":["I should also add another issue in this state; when running an interactive container, the container continues to run, but typing `exit` while containerd is not running makes it hang forever (probably dockerd waiting for exit events, which would never happen during this time).\r\n\r\nStarting the containerd service does not resolve that case (interactive shell continues hanging).","Can someone from the community take this up? For starters, it seems like an umbrella issue and we can perhaps take it one at a time. I'd like to take a shot at this.","> Can someone from the community take this up?\r\n\r\nThanks for asking! Yes, I think these are fine to work on, but (more details below) there may still be parts to \"dig\" into, to see what the best solution is as part of that :smile:!\r\n\r\nWhen I created this ticket, I mostly did so to look further into. I had written down these steps to answer that discussion I linked, but while doing so through \"hm.. perhaps there's some improvements to be made\", so I had not done any further digging yet. However, I was in a call with some maintainers Yesterday, and asked some others for a quick look \/ thoughts; let me write down some updated information that may help if people are considering to contribute (and when deciding _how_ to be st address things) :smile:\r\n\r\nFirst of all: containerd \"not running\" should be considered a **non-standard situation**; containerd may be _restarted_ (e.g. after installing a containerd update), but this would be for a _very_ brief moment: while it's _possible_ such a restart may briefly interfere with operations, this is probably not something to take into account (or \"not to spend too much time on\").\r\n\r\n\r\n> Should we log a more informative warning that containerd is not available to handle requests?\r\n\r\nThis is something that should be good to address; we already have an error-message, and that error contains useful information \"what happened\" (some connection failure), but does not have context \"what we were trying to do\" (trying to connect with containerd);\r\n\r\n    docker: Error response from daemon: connection error: desc = \"transport: Error while dialing dial unix:\/\/\/run\/containerd\/containerd.sock: timeout\": unavailable.\r\n\r\nThe first part (\"docker: Error response from daemon:\") is added by the Docker CLI, so the error the daemon produces is:\r\n\r\n    connection error: desc = \"transport: Error while dialing dial unix:\/\/\/run\/containerd\/containerd.sock: timeout\": unavailable.\r\n\r\nI think it could be useful to\r\n\r\n- :+1: wrap those errors to have some prefix, e.g. `connecting to containerd:`\r\n- :warning: but I have not looked where the error is returned, and if there's a common code-path that is hit for all possible cases, so that's something to dig into :smile:\r\n\r\n\r\n> TBD; do we want to (by default) still continue killing the container, or should we only do so \"opt-in\"?\r\n> \r\n> - The good thing here is that we can still kill a container (i.e. the process), which can be useful\r\n> - The \"bad\" thing is that we don't inform why things took longer than expected (and the fact we couldn't connect with containerd)\r\n> - The \"bad\" thing is that the container's state no longer matches reality; do we have different means to get the actual state? (the shim is still running, so .. maybe?)\r\n\r\nWe discussed this scenario;\r\n\r\n- Currently, the part that \"works\" is that the container process is _killed_. This works because the docker daemon will directly signal (`SIGTERM`), and ultimately `SIGKILL` the process.\r\n- In that scenario, the container is killed, but the `shim` stays behind (and the container state is not updated because of that)\r\n\r\nWe are considering to _NOT_ support that scenario: \r\n\r\n- produce an error (informing the user that containerd is not available \/ that we're unable to connect to containerd), instead of \"partially\" proceeding. Similar to what happens if you try to _create_ \/ _start_ a container.\r\n- ^^ HOWEVER: this needs to be looked at closely when making changes;\r\n   - :+1: such an error is \"ok\" when running an actual `docker stop` \/ `docker kill` \/ `docker rm`\r\n   - :question: but there may be other code-paths that may try to stop\/kill\/remove a container implicitly (e.g. during `docker build` with the classic builder, or other scenarios)\r\n   - :point_right: so some investigating is needed \"where\" we can put such a check, and make sure that we don't break other scenarios in unexpected ways\r\n\r\n> I should also add another issue in this state; when running an interactive container, the container continues to run, but typing exit while containerd is not running makes it hang forever (probably dockerd waiting for exit events, which would never happen during this time).\r\n\r\nThis is definitely one to dig into: \r\n\r\n- :question: what's the cause of the \"hang\"? can we handle this situation better? At least: make sure that the user is able to \"exit\" the container (and it not to hang)?\r\n- could this PR be related? :point_right: https:\/\/github.com\/moby\/moby\/pull\/41828\r\n- :question: if we _are_ able to make this work, this scenario would likely be similar to a container \"exiting\" on its own while containerd is down; can we detect these cases, and update the container state to something better (?) \"unknown\" state, or \"dead\" state?"],"labels":["area\/runtime","kind\/enhancement","containerd-integration","area\/ux","version\/24.0"]},{"title":"[WIP] api: migrate swagger to OpenAPI v3","body":"- fixes https:\/\/github.com\/moby\/moby\/issues\/45418\r\n\r\nInitial conversion was done using the conversion-tool provided by swagger. I tried that conversion tool a few years back, and it wasn't great at the time, but it improved significantly. Nonetheless, quite some manual changes were needed after converting;\r\n\r\n- The conversion tool uses a fixed order of fields, which didn't match the existing file, so made comparing impossible.\r\n- The conversion tool optimizes YAML by leaving out quotes where possible, and using single-quotes in other cases. Ths again didn't match our existing file, and made comparing impossible.\r\n\r\nSome General description of changes from v2 -> v3:\r\n\r\n- `definitions` is now `components\/schema`\r\n- `parameters` with `in: body` now get their own field: `requestBody` (which can be per-mediaType)\r\n- `parameters.type` is now `parameters.schema.type`\r\n- `produces` \/ `consumes` (mediaTypes) is now defined per request \/ response. For example, a `200` response that can return either `application\/json` or `plain\/text`, now has a `content` struct, with entries for each mediaType.\r\n- `x-nullable` is now supported by OpenAPI as `nullable`, but I kept both variants for now (to allow some tools to use the `x-` variant).\r\n\r\n(Error) responses in multiple content-types:\r\n\r\nOpenAPI v2 was limited in how to describe that an endpoint allowed for different content-types to be returned (produced). OpenAPI v3 addresses this limitation by allowing responses and requests to be defined per-media-type. Lacking more information, the conversion tool creates responses for every media-type, which includes variants for each error \/ status-code response (separate definitions for error-responses for archive\/tar, application\/json, text\/plain, etc.). In some cases it would also create _requests_ for both, documenting a JSON request as `text\/plain` (or other media-type).\r\n\r\nSome of these may have been the result or incorrect definitions in the v2 swagger (i.e., only including a `produces`, not a `consumes`), but that's to be looked at.\r\n\r\nThere's some parts to look into;\r\n\r\n- current version of the API should always return the JSON (ErrorResponse) type (API v1.23 and up, see [apiVersionSupportsJSONErrors][1]).\r\n- however, the Content-Type may not be set correctly in all cases. For example, the `GET \/images\/get` endpoint (`imageRouter.getImagesGet`), can return either \"no content type\", or `application\/x-tar`, depending on where in [the code the error happens][2].\r\n- We could assume that any error (e.g. a `500`) _could_ return a plain-text error, but that's to be looked at.\r\n\r\nFor the time being, I limited most error returns to be JSON, and kept the `text\/plain` variant for a follow-up.\r\n\r\nThere are some things remaining to be looked at;\r\n\r\n- Our swagger contains \"descriptions\" in some places where OpenAPI V3 doesn't allow them. These look to be in cases where we re-use a single definition for multiple purposes, and try to document each of them individually. These descriptions were kept as comments, with a TODO\r\n- Our go-swagger version does not support OpenAPI v3, which means that generating code from the swagger is currently broken. We need to update to a more recent version (something we had to do either way, as it was broken in many respects).\r\n\r\n[1]: https:\/\/github.com\/moby\/moby\/blob\/6ce5aa1cd5a46e02ffaa3fe0c794642b35601676\/api\/server\/errorhandler.go#L31-L34\r\n[2]: https:\/\/github.com\/moby\/moby\/blob\/6ce5aa1cd5a46e02ffaa3fe0c794642b35601676\/api\/server\/router\/image\/image_routes.go#L184-L207\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/api","status\/2-code-review"]},{"title":"Unable to create macvlan network that overlap","body":"### Description\n\nHi,\r\n\r\nMy host computer have several interfaces.\r\n\r\nI would like to create multiple macvlan network with the same subnet for each interfaces.\r\n\r\nI'm doing something technically wrong or is it a Docker limitation?\n\n### Reproduce\n\ndocker network create -d macvlan --subnet=192.168.1.1\/24 -o parent=eno1 network-1\r\ndocker network create -d macvlan --subnet=192.168.1.1\/24 -o parent=eno2 network-2\r\nError response from daemon: Pool overlaps with other one on this address space\r\n\r\n\n\n### Expected behavior\n\nThis should be acceptable\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.6\r\n API version:       1.43\r\n Go version:        go1.20.7\r\n Git commit:        ed223bc\r\n Built:             Mon Sep  4 12:31:44 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.6\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.7\r\n  Git commit:       1a79695\r\n  Built:            Mon Sep  4 12:31:44 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.6\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: 24.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-26-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.09GiB\r\n Name: hostname\r\n ID: 800cc3dc-3e9b-48d4-97b5-f4ffc0538240\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Hi @clementperon, \r\n\r\n> Error response from daemon: Pool overlaps with other one on this address space\r\n\r\nThis come from the daemon's default IPAM driver. Once a subnet is reserved for a specific network, the driver will mark this subnet as unavailable and next time a network tries to use it, it will fail.\r\n\r\n> I would like to create multiple macvlan network with the same subnet for each interfaces.\r\n\r\nCould you describe what you're trying to do? Also, how would you expect interfaces, routes, etc... to be configured?","Hi @akerouanton,\r\n\r\n> Hi @clementperon,\r\n> \r\n> > Error response from daemon: Pool overlaps with other one on this address space\r\n> \r\n> This come from the daemon's default IPAM driver. Once a subnet is reserved for a specific network, the driver will mark this subnet as unavailable and next time a network tries to use it, it will fail.\r\n\r\nYes, I understand that, but two subnet that belongs to two different interfaces should not interfer right?\r\n\r\n> \r\n> > I would like to create multiple macvlan network with the same subnet for each interfaces.\r\n> \r\n> Could you describe what you're trying to do? Also, how would you expect interfaces, routes, etc... to be configured?\r\n\r\nI'm spawing containers that are managing Devices Under Test.\r\nEach DUT is connected to a dedicated interface and have a default 192.168.1.201 IP after a factory reset.\r\n\r\nTo be able to have a replicable test, I configure my DUT to have the same Device IP and they communicate to the same Server IP.\r\nBut each container have a dedicated interface. \r\n\r\nCONTAINER1 (192.168.1.1\/24) <--ENO1--> DUT1 (192.168.1.201\/24)\r\nCONTAINER2 (192.168.1.1\/24) <--ENO2--> DUT2 (192.168.1.201\/24)\r\nCONTAINER3 (192.168.1.1\/24) <--ENO3--> DUT3 (192.168.1.201\/24)\r\n\r\nMacvlan are associated to a different interface thus it should not be an issue to have the same IP range for different macvlan.","> Macvlan are associated to a different interface thus it should not be an issue to have the same IP range for different macvlan.\r\n\r\nActually dockerd doesn't allow that because it'd be a source of connectivity issues if a container was connected to both networks.\r\n\r\nNonetheless, I thought it'd be possible to use the `null` IPAM driver to statically assign subnets and IP addresses with no validation whatsoever but it seems the macvlan \/ ipvlan drivers implicitly disallow its use. That's something we'd need to fix.\r\n\r\nFor now, unfortunately the workaround is to not use docker's networking features (ie. `--network=host`) and do it yourself.","> > Macvlan are associated to a different interface thus it should not be an issue to have the same IP range for different macvlan.\r\n> \r\n> Actually dockerd doesn't allow that because it'd be a source of connectivity issues if a container was connected to both networks.\r\n\r\nAgree, but they aren't :). So the check is only looking at the IP_Address instead of the couple (IP_Address, Interface).\r\n\r\n> \r\n> Nonetheless, I thought it'd be possible to use the `null` IPAM driver to statically assign subnets and IP addresses with no validation whatsoever but it seems the macvlan \/ ipvlan drivers implicitly disallow its use. That's something we'd need to fix.\r\n\r\nI would be very happy If I could bypass the check an assign the IP address manually. I will test it to be sure it doesn't work.\r\n\r\n> \r\n> For now, unfortunately the workaround is to not use docker's networking features (ie. `--network=host`) and do it yourself.","@akerouanton thanks for your help unfortunately setting a static ip address without the subnet \/ ip_range give me the following error:\r\n```failed to create network XXXX: Error response from daemon: ipv4 pool is empty```"],"labels":["kind\/enhancement","area\/networking","area\/networking\/d\/macvlan"]},{"title":"Conditionally disable tests when not root","body":"Root resources are not available when building moby in a Linux package build environment. This change conditionally disables them when not run as root. See discussion at https:\/\/github.com\/moby\/moby\/discussions\/46387. Not being a Go programmer, I've done my best to try and match the local style; C&C welcome.\r\n\r\n**- What I did**\r\nI have added conditional skips in unit tests which require root privileges.\r\n\r\n**- How I did it**\r\nN\/A\r\n\r\n**- How to verify it**\r\nThis is a port of a patch I made to get moby building in an RPM mock environment. We like to run as much of a projects testing as we can to build confidence that our build is correct and functional. If you run this in an RPM mock environment you'll see some of these tests skip (the mock builder is not run as root). They're run with a standard `go test` invocation. Some modules are skipped entirely, and are not modified here.\r\n\r\nI have verified that the preferred way of developing Moby is unaffected by running `make test-unit` and verifying that tests are not skipped erroneously. (DONE 3142 tests, 27 skipped in 37.667s and DONE 344 tests, 2 skipped in 243.128s).\r\n\r\n**- Description for the changelog**\r\n(not a user-facing change)\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![Image from iOS_cropped](https:\/\/github.com\/moby\/moby\/assets\/278804\/dd2ee6b8-dedc-4c81-be87-fcccb0db1519)","comments":["Looks like you missed a DCO sign-off on the second commit :)","Silly me, I've amended the commit message. Sorry about that.","> Sorry about that.\r\n\r\nNo worries, and no need to apologize _at all_ it happens all the time, but people sometimes miss it, so thought I'd leave a quick comment so that I could give CI another run \u263a\ufe0f","Doh! Looks like this needs a rebase \ud83d\ude22  while at it, I think it's fine to squash the two commits, because the second commit is touching up the first one (so we don't have to preserve that history)","Sounds good, I've just force-pushed up a squash and rebase. Thanks!"],"labels":["status\/2-code-review","area\/testing","kind\/refactor"]},{"title":"Add dependabot to update GitHub Actions","body":"Fixes #46427.\r\n\r\n**- What I did**\r\nAdded dependabot to keep GitHub Actions updated on a monthly basis. This takes advantage of dependabot's [grouped updates](https:\/\/github.blog\/2023-08-24-a-faster-way-to-manage-version-updates-with-dependabot\/), so that the project only receives a single PR each month updating all GitHub Actions at once. See [this PR on my fork](https:\/\/github.com\/pnacht\/moby\/pull\/1) updating three out-of-date Actions in moby workflows.\r\n\r\nI've selected a monthly update schedule so as to minimize the workload as much as possible. That being said, for what it's worth, the \"default\" schedule is \"weekly\". Let me know if you'd prefer I use the weekly schedule (or daily, if you're feeling wild!).\r\n\r\n**- How I did it**\r\nAdded `.github\/dependabot.yml` with a single global group for all GitHub Actions.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n![20170513_160019 (1)](https:\/\/github.com\/moby\/moby\/assets\/15221358\/dcf3f0b8-6702-4375-901f-5873660b015e)\r\n\r\n","comments":["I've added the labels and made the update daily."],"labels":["status\/2-code-review","area\/testing"]},{"title":"Add dependabot to keep workflow Actions updated","body":"### Description\n\nDependabot has recently released [grouped updates](https:\/\/github.blog\/2023-08-24-a-faster-way-to-manage-version-updates-with-dependabot\/), meaning an entire ecosystem can be updated with a single PR. I'd therefore like to suggest that Moby adopt Dependabot to keep its workflow Actions up-to-date.\r\n\r\nI believe this mitigates the concerns raised in https:\/\/github.com\/moby\/moby\/pull\/44177#issuecomment-1310746253 regarding the large volumes of PRs Dependabot ordinarily generates.\r\n\r\nI'll send a PR along with this issue implementing this change.","comments":["Does it still open pull requests against all 18.8K forks? \ud83d\ude2c (I know that was one of the other concerns)","Ah, no, it doesn't. Dependabot is disabled by default on all forks. I even had to go to the settings and enable it on my fork even though I'd just added the dependabot.yml file myself.",">  I even had to go to the settings and enable it on my fork even though I'd just added the dependabot.yml file myself.\r\n\r\nI _think_ that's only until it's merged in upstream; once it is, it's enabled.\r\n\r\ni.e.; I'm getting pull requests all the time from dependabot on my own forks \ud83d\ude1e \r\nhttps:\/\/github.com\/thaJeztah\/compose\/pulls?q=is%3Apr+author%3Aapp%2Fdependabot\r\n\r\nAnd worse; it's creating branches on _my_ fork (without the ability to deny it access to my fork, which in its own can be a security concern if dependabot would ever get compromised (which of course we hope would never happen)).\r\n\r\nAnd a quick check on other forks, I see those get them as well;\r\n\r\n- https:\/\/github.com\/ndeloof\/compose\/pulls?q=is%3Apr+author%3Aapp%2Fdependabot\r\n- https:\/\/github.com\/glours\/compose\/pulls?q=is%3Apr+author%3Aapp%2Fdependabot\r\n\r\nI _think_ they slightly improved it and don't open PRs if the fork hasn't been updated for some time, but theoretically it could mean dependabot opening pull requests (and creating branches, without consent) on 18k+ forks \ud83d\ude22 ","Sorry for the delay in replying here!\r\n\r\nSo, as of [November 2022](https:\/\/github.blog\/changelog\/2022-11-07-dependabot-pull-requests-off-by-default-for-forks\/), dependabot is no longer automatically enabled for new forks. And per the [documentation](https:\/\/docs.github.com\/en\/code-security\/dependabot\/dependabot-version-updates\/configuring-dependabot-version-updates#enabling-version-updates-on-forks), this also applies to older forks that pull a new dependabot.yml:\r\n\r\n> Version updates are not automatically enabled on forks when a dependabot.yml configuration file is present. This ensures that fork owners don't unintentionally enable version updates when they pull changes including a dependabot.yml configuration file from the original repository.\r\n\r\n---\r\n\r\n> i.e.; I'm getting pull requests all the time from dependabot on my own forks \ud83d\ude1e\r\n\r\nYou should now also be able to disable this in your fork's settings page: https:\/\/github.com\/thaJeztah\/compose\/settings\/security_analysis (there should be a \"Disable\" next to \"Dependabot version updates\").","As an example, I just forked docker\/compose myself and this is what I see in my fork's Security Analysis page:\r\n\r\n![Screenshot 2023-11-22 at 11 46 40](https:\/\/github.com\/moby\/moby\/assets\/15221358\/fa7ebb95-3b56-43ad-9ac5-29f77d1a11b9)\r\n\r\nNote that it detects the `dependabot.yml` file, but lists it as \"disabled\". In fact, all the dependabot features are disabled on my fork; I have to manually \"Enable\" them.\r\n\r\nIf my previous suggestion to try disabling it in your fork's settings doesn't work (I can't test how things were before the November 2022 change...), you could always delete and recreate your fork... though the feasibility of that option depends on how much custom code is there."],"labels":["status\/0-triage","kind\/feature"]},{"title":"daemon: convert short network ID into full name","body":"**- What I did**\r\n\r\nThe daemon already expect in many places that endpoints in `container.NetworkSettings.Networks` are keyed by network's full name, once the endpoint has been effectively created.\r\n\r\nAlso, it already handles full ID -> full name conversion but it doesn't properly handle the case where user-specified settings where stored with a partial network ID key.\r\n\r\nBecause of this missing case, if a container is created with a reference to a partial network ID, two entries for the same network are inserted in `container.NetworkSettings.Networks`.\r\n\r\nBeside bad UX (ie. duplicated entry when inspecting the container), there's another concrete issue: when such container is disconnected from a network and then restarted, the container will be unintendedly reconnected to that network.\r\n\r\n**- How to verify it**\r\n\r\nNew integration test is green, or by hand:\r\n\r\n```\r\n$ docker network create --subnet 192.168.100.0\/24 testnet1\r\n9b8a3a46b182e42523fcc625629e83ee00435a5b16016f8b4823f810e7f2a3e6\r\n\r\n$ docker run --rm --name test --network 9b8a3a46b1 --ip 192.168.100.24 -d nicolaka\/netshoot \/bin\/sleep 1m\r\n\r\n# Prior to this fix:\r\n$ docker inspect test | jq '.[] | .NetworkSettings.Networks | keys'\r\n[\r\n  \"9b8a3a46b1\",\r\n  \"testnet1\"\r\n]\r\n# And after:\r\n$ docker inspect test | jq '.[] | .NetworkSettings.Networks | keys'\r\n[\r\n  \"testnet1\"\r\n]\r\n```\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/02\/Sea_Otter_%28Enhydra_lutris%29_%2825169790524%29_crop.jpg\/800px-Sea_Otter_%28Enhydra_lutris%29_%2825169790524%29_crop.jpg\" width=75% height=75% \/>\r\n","comments":["(somewhat) related;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/32060"],"labels":["status\/2-code-review","area\/networking","process\/cherry-pick","area\/daemon","kind\/bugfix"]},{"title":"A docker image is not removed from disk after enabling userns-remap, creating an image and removing the image","body":"### Description\r\n\r\nA docker image is not removed from disk after enabling userns-remap, creating an image and removing the image\r\n\r\n### Reproduce\r\n\r\n1. Create a dockuser user account on your docker host\r\n2. Stop the docker daemon\r\n3. Configure dockerd for userns-remap\r\n```\r\n{\r\n    \"userns-remap\": \"dockuser\"\r\n}\r\n```\r\n4. Start the docker daemon\r\n5. Build a docker image\r\n6. Run a container using that docker image\r\n7. Stop the container\r\n8. Remove the container\r\n9. Remove the built docker image\r\n\r\n```\r\n[user@host ~]$ docker images -a\r\nREPOSITORY            TAG       IMAGE ID       CREATED              SIZE\r\nubuntu_systemd_test   local     0a55bc64c86d   About a minute ago   2.27GB\r\n\r\n[user@host ~]$ df -h \/var\/lib\/docker\r\nFilesystem      Size  Used Avail Use% Mounted on\r\n\/dev\/sda1        51G   46G  3.8G  93% \/\r\n\r\n[user@host ~]$ docker image rm 0a55bc64c86d\r\nUntagged: ubuntu_systemd_test:local\r\nDeleted: sha256:0a55bc64c86deb501a97534bcd5a555e4de35a31a3b5f58139c39d9623bd9d22\r\n\r\n[user@host ~]$ sync\r\n\r\n[user@host ~]$ df -h \/var\/lib\/docker\r\nFilesystem      Size  Used Avail Use% Mounted on\r\n\/dev\/sda1        51G   46G  3.8G  93% \/\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe diskspace should've been reclaimed but it wasn't. I found out that the image was written under \/var\/lib\/docker\/262144.262144\/overlay2\/ Those are the subuid and subgid for the dockuser user account.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:37:21 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:43 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: btrfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\nWARNING: bridge-nf-call-iptables is disabled\r\nWARNING: bridge-nf-call-ip6tables is disabled\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  userns\r\n  cgroupns\r\n Kernel Version: 6.4.12-100.fc37.x86_64\r\n Operating System: Fedora Linux 37 (KDE Plasma)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.731GiB\r\n Name: ac\r\n ID: 57d8675c-0448-4386-9b41-92564aee0180\r\n Docker Root Dir: \/var\/lib\/docker\/262144.262144\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["I mis-wrote saying that the image wouldn't be removed, but it's the overlay filesystem in \/var\/lib\/docker\/262144.262144\/overlay2\/ that is not removed."],"labels":["status\/0-triage","kind\/bug","area\/images","area\/security\/userns","version\/24.0"]},{"title":"Building container fails with failed to add the pair interfaces: cannot allocate memory","body":"### Description\n\nWhen building a container it sometimes fails with the error\r\n`docker: Error response from daemon: failed to create endpoint recursing_aryabhata on network bridge: failed to add the host (veth6ad97f8) <=> sandbox (veth23b66ce) pair interfaces: cannot allocate memory.`\r\n\r\nThe error only appears only around every second build, so it is not consistently reproducible.\r\nIt appeared without any changes to the system a few months ago and got worse from there on. \r\nWe updated everything on the machine (Ubuntu 20.04) and the error still happens.\r\nThere is also a related issue already open [here](https:\/\/github.com\/docker\/for-linux\/issues\/1443)\n\n### Reproduce\n\ndocker build -t my_container:tag .\n\n### Expected behavior\n\nBuild runs every time without an error.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:52:22 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:52:22 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.18.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 52\r\n Server Version: 24.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.0-1042-azure\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 7.691GiB\r\n Name: k8saz02\r\n ID: dec3a18b-adad-4bd3-8b35-dcc5d3d9782a\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 23\r\n  Goroutines: 35\r\n  System Time: 2023-09-05T09:37:02.828843632+02:00\r\n  EventsListeners: 0\r\n HTTP Proxy: http:\/\/my.proxy:80\r\n HTTPS Proxy: http:\/\/my.proxy:80\r\n No Proxy: localhost,*.out.domain\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nThe problem appeared out of nowhere. No changes to the server. Also updating packages \/ release update of the OS did not change anything. Also we have two servers with this problem.","comments":["See https:\/\/github.com\/docker\/for-linux\/issues\/1443","@bendem \r\n> See [docker\/for-linux#1443](https:\/\/github.com\/docker\/for-linux\/issues\/1443)\r\n\r\nFrom the Readme:\r\n>This is a legacy issue tracker to manage issues related with Docker Engine for Linux. To report an issue or request a new feature please refer to the upstream [Moby Project](https:\/\/github.com\/moby\/moby), or [Docker Deskop for Linux](https:\/\/github.com\/docker\/desktop-linux\/issues) in case you are running [Docker Desktop](https:\/\/www.docker.com\/products\/docker-desktop\/).\r\n\r\nThat is why I opened the issue here and linked it.","Ah, I missed you linking the existing conversation, nvm then.\r\n\r\n18 Sep 2023 17:11:53 pschoen-itsc ***@***.***>:\r\n\r\n> \r\n> See docker\/for-linux#1443[https:\/\/github.com\/docker\/for-linux\/issues\/1443]\r\n> \r\n> From the Readme:\r\n> \r\n> This is a legacy issue tracker to manage issues related with Docker Engine for Linux. To report an issue or request a new feature please refer to the upstream Moby Project[https:\/\/github.com\/moby\/moby], or Docker Deskop for Linux[https:\/\/github.com\/docker\/desktop-linux\/issues] in case you are running Docker Desktop[https:\/\/www.docker.com\/products\/docker-desktop\/].\r\n> \r\n> That is why I opened the issue here and linked it.\r\n> \r\n> \u2014\r\n> Reply to this email directly, view it on GitHub[https:\/\/github.com\/moby\/moby\/issues\/46401#issuecomment-1723656482], or unsubscribe[https:\/\/github.com\/notifications\/unsubscribe-auth\/AAUOWTOMLGQEOJSEWVPEM7TX3BQDNANCNFSM6AAAAAA4LKYU5Q].\r\n> You are receiving this because you commented.\r\n> [Tracking image][https:\/\/github.com\/notifications\/beacon\/AAUOWTKPH3EFGAMOEPBXCH3X3BQDNA5CNFSM6AAAAAA4LKYU5SWGG33NNVSW45C7OR4XAZNMJFZXG5LFINXW23LFNZ2KUY3PNVWWK3TUL5UWJTTGXTUSE.gif]\r\n> \r\n","Updating the kernel to 6.5 resolves the issue. For more details see the related issue.","After one month with kernel 6.5 the error appeared again."],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/d\/bridge","version\/24.0"]},{"title":"Docker daemon fails to start: Unable to create NAT rule:  Chain 'MASQUERADE' does not exist","body":"### Description\n\nDocker daemon fails to start.\r\n\r\nLog from `journalctl`:\r\n\r\n```sh\r\nSep 05 14:11:50 neon systemd[1]: Failed to start Docker Application Container Engine.\r\nSep 05 14:11:52 neon systemd[1]: docker.service: Scheduled restart job, restart counter is at 2.\r\nSep 05 14:11:52 neon systemd[1]: Stopped Docker Application Container Engine.\r\nSep 05 14:11:52 neon systemd[1]: Starting Docker Application Container Engine...\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.577471732+08:00\" level=info msg=\"Starting up\"\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.689040639+08:00\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.689265351+08:00\" level=info msg=\"Loading containers: start.\"\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.853667883+08:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0\/16. Daemon option --bip can be used to set a preferred IP address\"\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.942244455+08:00\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"<nil>\" module=libcontainerd namespace=moby\r\nSep 05 14:11:52 neon dockerd[2261]: time=\"2023-09-05T14:11:52.942907870+08:00\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"context canceled\" module=libcontainerd namespace=plugins.moby\r\nSep 05 14:11:52 neon dockerd[2261]: failed to start daemon: Error initializing network controller: error creating default \"bridge\" network: Failed to Setup IP tables: Unable to enable NAT rule:  (iptables failed: iptables --wait -t nat -I POSTROUTING -s 172.17.0.0\/16 ! -o docker0 -j MASQUERADE: iptables v1.8.7 (nf_tables): Chain 'MASQUERADE' does not exist\r\nSep 05 14:11:52 neon dockerd[2261]: Try `iptables -h' or 'iptables --help' for more information.\r\nSep 05 14:11:52 neon dockerd[2261]:  (exit status 2))\r\nSep 05 14:11:52 neon systemd[1]: docker.service: Main process exited, code=exited, status=1\/FAILURE\r\n```\n\n### Reproduce\n\ndocker ps\r\nsudo systemctl restart docker\n\n### Expected behavior\n\nDocker daemon should start up\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:45 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\nCannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\nERROR: Cannot connect to the Docker daemon at unix:\/\/\/var\/run\/docker.sock. Is the docker daemon running?\r\nerrors pretty printing info\n```\n\n\n### Additional Info\n\nOS info:  MX Linux 21 `Linux neon 6.4.0-2mx-ahs-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.4.4-3~mx21ahs (2023-08-10) x86_64 GNU\/Linux`\r\n\r\nOutput from `ip a`:\r\n\r\n```\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link\/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\r\n    inet 127.0.0.1\/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1\/128 scope host \r\n       valid_lft forever preferred_lft forever\r\n2: eth0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc fq_codel state DOWN group default qlen 1000\r\n    link\/ether e8:6a:64:ac:d0:81 brd ff:ff:ff:ff:ff:ff\r\n3: wlan0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\r\n    link\/ether 28:39:26:04:4f:7f brd ff:ff:ff:ff:ff:ff\r\n    inet 192.168.0.108\/24 brd 192.168.0.255 scope global dynamic noprefixroute wlan0\r\n       valid_lft 5973sec preferred_lft 5973sec\r\n    inet6 fe80::847f:3a91:749c:91a2\/64 scope link noprefixroute \r\n       valid_lft forever preferred_lft forever\r\n4: docker0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default \r\n    link\/ether 02:42:f5:b5:5d:45 brd ff:ff:ff:ff:ff:ff\r\n    inet 172.17.0.1\/16 brd 172.17.255.255 scope global docker0\r\n       valid_lft forever preferred_lft forever\r\n```\r\n\r\nI have tried the following to no avail:\r\n\r\n- Completely uninstalled and reinstalled docker\r\n- `ip link del docker0`\r\n- `sudo rm \/var\/lib\/docker\/network\/files\/local-kv.db`\r\n- `sudo  rm -rf \/var\/lib\/docker\/network`","comments":["This looks like a kernel related issue. Perhaps ip_tables wasn't build with the module.","@kamikazechaser What does `iptables --version` show you?","`iptables v1.8.7 (nf_tables)`\r\n\r\n\r\n\r\n","I just updated my ubuntu to latest packages (via cloud-init) in whole system and have same issue.\r\nBefore upgrade everything works.\r\nUbuntu 22.04.3 LTS\r\niptables v1.8.7 (nf_tables)\r\nDocker version 24.0.6, build ed223bc","@kamikazechaser Sorry, I didn't see your answer. The issue is due to `iptables-nft` being used instead of `iptables-legacy`. You need to run the following commands:\r\n\r\n```\r\n$ sudo update-alternatives --set iptables \/usr\/sbin\/iptables-legacy\r\n$ sudo update-alternatives --set ip6tables \/usr\/sbin\/ip6tables-legacy\r\n```","The issue is still there:\r\n\r\nTried switching between alternatives with: `update-alternatives --config iptables` and `update-alternatives --config ip6tables`\r\n\r\nDocker Version: 25.0.0\r\niptables v1.8.7 (legacy)\r\nKernel: Linux neon 6.5.0-5mx-ahs-amd64 SMP PREEMPT_DYNAMIC Debian 6.5.13-1~mx21ahs (2023-11-30) x86_64 GNU\/Linux\r\n"],"labels":["kind\/bug","area\/networking","version\/24.0"]},{"title":"integration\/system: TestInfoAPIWarnings: improve test, and t.Parallel","body":"- fixes https:\/\/github.com\/moby\/moby\/issues\/40156\r\n\r\n\r\n### integration\/system: TestInfoAPIWarnings: improve test, and t.Parallel\r\n\r\nThis test takes at least 15 seconds to run, because loadListeners imposes\r\na 15-second delay for insecure listeners. Rewrite the test to run with\r\nt.Parallel, use a test-table, and add additional test-cases to verify both\r\ncases where the warning should be returned, and should _not_ be returned.\r\n\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Interesting;\r\n\r\n```\r\n daemon.go:313: [dab1d02ec42cb] failed to start daemon: error initializing graphdriver: driver not supported: overlay2\r\n    info_test.go:90: [dab1d02ec42cb] failed to start daemon with arguments [--data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/dab1d02ec42cb\/root --exec-root \/tmp\/dxr\/dab1d02ec42cb --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/dab1d02ec42cb\/docker.pid --userland-proxy=true --containerd-namespace dab1d02ec42cb --containerd-plugins-namespace dab1d02ec42cbp --containerd \/var\/run\/docker\/containerd\/containerd.sock --host unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock --debug --storage-driver overlay2 -H=unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock -H=tcp:\/\/127.0.0.1:23750 --iptables=false] : [dab1d02ec42cb] daemon exited during startup: exit status 1\r\n```\r\n\r\nAlso wondering why I see the `-H=unix:\/\/\/` _twice_ in the arguments;\r\n\r\n```\r\n--host unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock --debug --storage-driver overlay2 -H=unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock -H=tcp:\/\/127.0.0.1:23750\r\n```","Ah, right, so Windows won't have SecurityOptions by default, so need to update the test for that;\r\n\r\n```\r\n=== FAIL: github.com\/docker\/docker\/integration\/system TestInfoAPI (0.02s)\r\n    info_test.go:36: assertion failed: len(info.SecurityOptions) is 0\r\n```","> Also wondering why I see the `-H=unix:\/\/\/` _twice_ in the arguments;\r\n> \r\n> ```\r\n> --host unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock --debug --storage-driver overlay2 -H=unix:\/\/\/tmp\/docker-integration\/dab1d02ec42cb.sock -H=tcp:\/\/127.0.0.1:23750\r\n> ```\r\n\r\nThis turned out to be because the test was doing too much, and manually adding the `-H=d.Sock()`, which is already done automatically by `daemon.Start -> daemon.StartWithLogFile `; https:\/\/github.com\/moby\/moby\/blob\/9c4e82435eb81c5146f9b5bb835ee259e9beb24c\/testutil\/daemon\/daemon.go#L469-L471\r\n","moving this one back to draft; I need to look at the second commit.\r\n\r\n- extracted the first commit to a separate PR: https:\/\/github.com\/moby\/moby\/pull\/46766","```\r\nStarting artifact upload\r\nFor more detailed logs during the artifact upload process, enable step-debugging: https:\/\/docs.github.com\/actions\/monitoring-and-troubleshooting-workflows\/enabling-debug-logging#enabling-step-debug-logging\r\nArtifact name is valid!\r\nError: Artifact path is not valid: \/ubuntu-20.04-rootless\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/docker.log. Contains the following character:  Colon :\r\n          \r\nInvalid characters include:  Double quote \", Colon :, Less than <, Greater than >, Vertical bar |, Asterisk *, Question mark ?, Carriage return \\r, Line feed \\n\r\n```","```\r\ndaemon.go:313: [df8bd9ce23043] failed to start daemon: error initializing graphdriver: driver not supported: overlay2\r\n```\r\n\r\n\r\n<details>\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration.system TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn (3.73s)\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:03.535513036Z\" level=info msg=\"containerd successfully booted in 0.040261s\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:03.634085520Z\" level=debug msg=\"garbage collected\" d=\"930.704\u00b5s\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.489235620Z\" level=debug msg=\"Golang's threads limit set to 49770\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.489855523Z\" level=debug msg=\"metrics API listening on \/tmp\/dxr\/d581ad911f711\/metrics.sock\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.493483541Z\" level=debug msg=\"Using default logging driver json-file\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.493768442Z\" level=debug msg=\"processing event stream\" module=libcontainerd namespace=d581ad911f711p\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.493808842Z\" level=debug msg=\"No quota support for local volumes in \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/root\/volumes: Filesystem does not support, or has not enabled quotas\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.501143078Z\" level=info msg=\"[graphdriver] trying configured driver: overlay2\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.529194816Z\" level=error msg=\"failed to mount overlay: no such file or directory\" storage-driver=overlay2\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.529248316Z\" level=debug msg=\"daemon configured with a 15 seconds minimum shutdown timeout\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.529282017Z\" level=debug msg=\"start clean shutdown of all containers with a 15 seconds timeout...\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.530930625Z\" level=debug msg=\"Cleaning up old mountid : start.\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.532777934Z\" level=debug msg=\"Cleaning up old mountid : done.\"\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.533803239Z\" level=info msg=\"stopping healthcheck following graceful shutdown\" module=libcontainerd\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.534084240Z\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"context canceled\" module=libcontainerd namespace=d581ad911f711p\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.534195241Z\" level=debug msg=\"received signal\" signal=terminated\r\n    daemon.go:313: [d581ad911f711] time=\"2023-11-03T09:35:04.534268841Z\" level=debug msg=\"sd notification\" notified=false state=\"STOPPING=1\"\r\n    daemon.go:313: [d581ad911f711] failed to start daemon: error initializing graphdriver: driver not supported: overlay2\r\n    daemon.go:313: [d581ad911f711] [rootlesskit:child ] error: command [\/usr\/local\/bin\/dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/root --exec-root \/tmp\/dxr\/d581ad911f711 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/docker.pid --userland-proxy=true --containerd-namespace d581ad911f711 --containerd-plugins-namespace d581ad911f711p --host unix:\/\/\/tmp\/docker-integration\/d581ad911f711.sock --debug --storage-driver overlay2 --host tcp:\/\/127.0.0.1:23750 --iptables=false] exited: exit status 1\r\n    daemon.go:313: [d581ad911f711] [rootlesskit:parent] error: child exited: exit status 1\r\n    info_test.go:93: [d581ad911f711] failed to start daemon with arguments [-u unprivilegeduser --preserve-env --preserve-env=PATH XDG_RUNTIME_DIR=\/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/xdgrun HOME=\/home\/unprivilegeduser -- dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/root --exec-root \/tmp\/dxr\/d581ad911f711 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn\/d581ad911f711\/docker.pid --userland-proxy=true --containerd-namespace d581ad911f711 --containerd-plugins-namespace d581ad911f711p --host unix:\/\/\/tmp\/docker-integration\/d581ad911f711.sock --debug --storage-driver overlay2 --host tcp:\/\/127.0.0.1:23750 --iptables=false] : [d581ad911f711] daemon exited during startup: exit status 1\r\n    --- FAIL: TestInfoAPIWarnings\/insecure_on_127.0.0.1:23750_should_warn (3.73s)\r\n\r\n=== FAIL: amd64.integration.system TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn (18.71s)\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:22.247001801Z\" level=info msg=\"containerd successfully booted in 0.039080s\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:22.345826916Z\" level=debug msg=\"garbage collected\" d=\"492.802\u00b5s\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.204397726Z\" level=debug msg=\"Golang's threads limit set to 49770\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.204918129Z\" level=debug msg=\"metrics API listening on \/tmp\/dxr\/df8bd9ce23043\/metrics.sock\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.208910445Z\" level=debug msg=\"Using default logging driver json-file\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.209002146Z\" level=debug msg=\"processing event stream\" module=libcontainerd namespace=df8bd9ce23043p\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.209560448Z\" level=debug msg=\"No quota support for local volumes in \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/root\/volumes: Filesystem does not support, or has not enabled quotas\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.220242993Z\" level=info msg=\"[graphdriver] trying configured driver: overlay2\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.242625587Z\" level=error msg=\"failed to mount overlay: no such file or directory\" storage-driver=overlay2\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.242666887Z\" level=debug msg=\"daemon configured with a 15 seconds minimum shutdown timeout\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.242680487Z\" level=debug msg=\"start clean shutdown of all containers with a 15 seconds timeout...\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.244190894Z\" level=debug msg=\"Cleaning up old mountid : start.\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.244803396Z\" level=debug msg=\"Cleaning up old mountid : done.\"\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.245413899Z\" level=info msg=\"stopping healthcheck following graceful shutdown\" module=libcontainerd\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.245612500Z\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"context canceled\" module=libcontainerd namespace=df8bd9ce23043p\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.245803501Z\" level=debug msg=\"received signal\" signal=terminated\r\n    daemon.go:313: [df8bd9ce23043] time=\"2023-11-03T09:35:23.245875501Z\" level=debug msg=\"sd notification\" notified=false state=\"STOPPING=1\"\r\n    daemon.go:313: [df8bd9ce23043] failed to start daemon: error initializing graphdriver: driver not supported: overlay2\r\n    daemon.go:313: [df8bd9ce23043] [rootlesskit:child ] error: command [\/usr\/local\/bin\/dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/root --exec-root \/tmp\/dxr\/df8bd9ce23043 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/docker.pid --userland-proxy=true --containerd-namespace df8bd9ce23043 --containerd-plugins-namespace df8bd9ce23043p --host unix:\/\/\/tmp\/docker-integration\/df8bd9ce23043.sock --debug --storage-driver overlay2 --host tcp:\/\/0.0.0.0:23752 --iptables=false] exited: exit status 1\r\n    daemon.go:313: [df8bd9ce23043] [rootlesskit:parent] error: child exited: exit status 1\r\n    info_test.go:93: [df8bd9ce23043] failed to start daemon with arguments [-u unprivilegeduser --preserve-env --preserve-env=PATH XDG_RUNTIME_DIR=\/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/xdgrun HOME=\/home\/unprivilegeduser -- dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/root --exec-root \/tmp\/dxr\/df8bd9ce23043 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn\/df8bd9ce23043\/docker.pid --userland-proxy=true --containerd-namespace df8bd9ce23043 --containerd-plugins-namespace df8bd9ce23043p --host unix:\/\/\/tmp\/docker-integration\/df8bd9ce23043.sock --debug --storage-driver overlay2 --host tcp:\/\/0.0.0.0:23752 --iptables=false] : [df8bd9ce23043] daemon exited during startup: exit status 1\r\n    --- FAIL: TestInfoAPIWarnings\/insecure_on_0.0.0.0:23752_should_warn (18.71s)\r\n\r\n=== FAIL: amd64.integration.system TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled (3.71s)\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:25.953057152Z\" level=info msg=\"containerd successfully booted in 0.043386s\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.051631621Z\" level=debug msg=\"garbage collected\" d=\"719.003\u00b5s\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.913846822Z\" level=debug msg=\"Golang's threads limit set to 49770\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.914266324Z\" level=debug msg=\"metrics API listening on \/tmp\/dxr\/d7c141bea7708\/metrics.sock\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.917448939Z\" level=debug msg=\"Using default logging driver json-file\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.917630340Z\" level=debug msg=\"No quota support for local volumes in \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/root\/volumes: Filesystem does not support, or has not enabled quotas\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.917974741Z\" level=debug msg=\"processing event stream\" module=libcontainerd namespace=d7c141bea7708p\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.925140975Z\" level=info msg=\"[graphdriver] trying configured driver: overlay2\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.954472715Z\" level=error msg=\"failed to mount overlay: no such file or directory\" storage-driver=overlay2\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.954554115Z\" level=debug msg=\"daemon configured with a 15 seconds minimum shutdown timeout\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.954566415Z\" level=debug msg=\"start clean shutdown of all containers with a 15 seconds timeout...\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.955799021Z\" level=debug msg=\"Cleaning up old mountid : start.\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.956325824Z\" level=debug msg=\"Cleaning up old mountid : done.\"\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.956807626Z\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"context canceled\" module=libcontainerd namespace=d7c141bea7708p\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.956819526Z\" level=info msg=\"stopping healthcheck following graceful shutdown\" module=libcontainerd\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.957034127Z\" level=debug msg=\"received signal\" signal=terminated\r\n    daemon.go:313: [d7c141bea7708] time=\"2023-11-03T09:35:26.957107327Z\" level=debug msg=\"sd notification\" notified=false state=\"STOPPING=1\"\r\n    daemon.go:313: [d7c141bea7708] failed to start daemon: error initializing graphdriver: driver not supported: overlay2\r\n    daemon.go:313: [d7c141bea7708] [rootlesskit:child ] error: command [\/usr\/local\/bin\/dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/root --exec-root \/tmp\/dxr\/d7c141bea7708 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/docker.pid --userland-proxy=true --containerd-namespace d7c141bea7708 --containerd-plugins-namespace d7c141bea7708p --host unix:\/\/\/tmp\/docker-integration\/d7c141bea7708.sock --debug --storage-driver overlay2 --host tcp:\/\/0.0.0.0:23754 --tls=false --iptables=false] exited: exit status 1\r\n    daemon.go:313: [d7c141bea7708] [rootlesskit:parent] error: child exited: exit status 1\r\n    info_test.go:93: [d7c141bea7708] failed to start daemon with arguments [-u unprivilegeduser --preserve-env --preserve-env=PATH XDG_RUNTIME_DIR=\/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/xdgrun HOME=\/home\/unprivilegeduser -- dockerd-rootless.sh --data-root \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/root --exec-root \/tmp\/dxr\/d7c141bea7708 --pidfile \/go\/src\/github.com\/docker\/docker\/bundles\/test-integration\/TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled\/d7c141bea7708\/docker.pid --userland-proxy=true --containerd-namespace d7c141bea7708 --containerd-plugins-namespace d7c141bea7708p --host unix:\/\/\/tmp\/docker-integration\/d7c141bea7708.sock --debug --storage-driver overlay2 --host tcp:\/\/0.0.0.0:23754 --tls=false --iptables=false] : [d7c141bea7708] daemon exited during startup: exit status 1\r\n    --- FAIL: TestInfoAPIWarnings\/insecure_on_0.0.0.0:23754_with_explicit_TLS_disabled (3.71s)\r\n\r\n=== FAIL: amd64.integration.system TestInfoAPIWarnings (28.60s)\r\n\r\n```\r\n\r\n\r\n<\/details>"],"labels":["status\/2-code-review","area\/testing","kind\/bugfix"]},{"title":"Default Bridge IPv6Default Gateway not present when default-address-pool is configured","body":"### Description\r\n\r\nwhen my daemon.json is configured as follows the default bridge gets and IP range and default gateway\r\n\r\n```\r\n{\r\n  \"ipv6\": true,\r\n  \"fixed-cidr-v6\": \"xxxx:xxxx:xxxxx:d1::\/64\",\r\n  \"experimental\": true,\r\n  \"ip6tables\": true,\r\n}\r\n```\r\nwhich is what i would expect, performance in the container over IPv6 is snappy (pings, apt update etc)\r\n\r\nhowever when set like this the default bridge has no IPv6 default gateway and performance really suffers (slow pings)\r\n```\r\n{\r\n  \"ipv6\": true,\r\n  \"fixed-cidr-v6\": \"xxxx:xxxx:xxxxx:d1::\/64\",\r\n  \"experimental\": true,\r\n  \"ip6tables\": true,\r\n  \"default-address-pools\": [\r\n    {\"base\" : \"172.17.0.0\/12\",\"size\" : 16},\r\n    {\"base\" : \"192.168.0.0\/16\",\"size\" : 20},\r\n    {\"base\" : \"xxxx:xxxx:xxxx:d2::\/64\", \"size\" : 80}\r\n  ]\r\n}\r\n```\r\n\r\n### Reproduce\r\n\r\nsee the differences in the configurations above \r\n\r\n### Expected behavior\r\n\r\ni would expect the default gateway to be present in both scenarios\r\n(or have it documented in the documentation why there is a difference)\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:45 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:45 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 32\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: o5gmrc4habb4zax9gew8hjfmx\r\n  Is Manager: true\r\n  ClusterID: cvuql7mtd3yh68uhg82h59emp\r\n  Managers: 3\r\n  Nodes: 3\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.1.41\r\n  Manager Addresses:\r\n   192.168.1.41:2377\r\n   192.168.1.42:2377\r\n   192.168.1.43:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.10.0-25-amd64\r\n Operating System: Debian GNU\/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.861GiB\r\n Name: Docker01\r\n ID: b8c6f15d-1c9d-4794-9931-3393db0065b3\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\nThis is a swarm, but I am creating standalone containers for IPv6 not swarm services (i know services basically don't work with IPv6)\r\n_No response_","comments":["Hi @scyto, I'm neither able to reproduce the default gateway issue nor the perf issue and I don't see anything that would explain them in your bug report. Are you able to reproduce those issues on another host \/ with another OS?\r\n\r\nAbout the default gateway issue, can you [enable daemon's debug mode](https:\/\/docs.docker.com\/config\/daemon\/logs\/#enable-debugging), restart your engine and paste here all the logs related to IPv6, IPAM and the default bridge? Can you also paste here the result of `ip addr show` please?\r\n\r\nFor the perf issue, did you try to run an iperf3 benchmark between your host and your container, and then between your container and another server?"],"labels":["status\/more-info-needed","kind\/bug","area\/networking","area\/networking\/ipv6","area\/networking\/d\/bridge","version\/24.0"]},{"title":"Docker service not coming up :: wal: max entry size limit exceeded","body":"### Description\r\n\r\nMy storage ran out and having understood that this was the root cause I cleared up space and restarted the docker.\r\n\r\nIt did not work.\r\n\r\nRan system prune and restarted server just in case but the issue persists:\r\n\r\n> docker info\r\n\r\n```\r\nroot@e2e-75-115:~# docker --version\r\nDocker version 24.0.5, build ced0996\r\nroot@e2e-75-115:~# \r\nroot@e2e-75-115:~# \r\nroot@e2e-75-115:~# docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 4\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: error\r\n  NodeID: \r\n  Error: manager stopped: can't initialize raft node: irreparable WAL error: wal: max entry size limit exceeded, recBytes: 1999, fileSize(64000000) - offset(63998680) - padBytes(1) = entryLimit(1319)\r\n  Is Manager: false\r\n  Node Address: 164.52.207.115\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-82-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 57.47GiB\r\n Name: e2e-75-115\r\n ID: OZYI:3TWW:HSJW:RT3N:BVKL:GVVP:GIRI:L3OW:CAMJ:Q5LU:52HN:U4GL\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\nI do wish to reset everything to create services again (as was suggested elsewhere for similar issue). How do I repair it?\r\n\r\nError from logs\r\n\r\n```\r\nSep  1 06:56:06 e2e-75-115 dockerd[27667]: time=\"2023-09-01T06:56:06.549157402Z\" level=error msg=\"status reporter failed to report status to agent\" error=\"context canceled\" module=node\/agent node.id=0ex8p5v6vn9warrgty7d6602u\r\nSep  1 06:56:06 e2e-75-115 dockerd[27667]: time=\"2023-09-01T06:56:06.549450950Z\" level=error msg=\"cluster exited with error: manager stopped: can't initialize raft node: irreparable WAL error: wal: max entry size limit exceeded, recBytes: 1999, fileSize(64000000) - offset(63998680) - padBytes(1) = entryLimit(1319)\"\r\nSep  1 06:56:06 e2e-75-115 dockerd[27667]: time=\"2023-09-01T06:56:06.549654210Z\" level=error msg=\"swarm component could not be started\" error=\"manager stopped: can't initialize raft node: irreparable WAL error: wal: max entry size limit exceeded, recBytes: 1999, fileSize(64000000) - offset(63998680) - padBytes(1) = entryLimit(1319)\"\r\nSep  1 06:56:06 e2e-75-115 dockerd[27667]: time=\"2023-09-01T06:56:06.549774805Z\" level=info msg=\"Daemon has completed initialization\"\r\nSep  1 06:56:06 e2e-75-115 dockerd[27667]: time=\"2023-09-01T06:56:06.605090283Z\" level=info msg=\"API listen on \/run\/docker.sock\"\r\n```\r\n\r\n### Reproduce\r\n\r\n1. Run out of storage\r\n\r\n### Expected behavior\r\n\r\nOnce storge is cleared up, it should come back after restart\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 4\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: error\r\n  NodeID: \r\n  Error: manager stopped: can't initialize raft node: irreparable WAL error: wal: max entry size limit exceeded, recBytes: 1999, fileSize(64000000) - offset(63998680) - padBytes(1) = entryLimit(1319)\r\n  Is Manager: false\r\n  Node Address: 164.52.207.115\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-82-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 12\r\n Total Memory: 57.47GiB\r\n Name: e2e-75-115\r\n ID: OZYI:3TWW:HSJW:RT3N:BVKL:GVVP:GIRI:L3OW:CAMJ:Q5LU:52HN:U4GL\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Any potential solutions or workarounds for this issue?  I'm dealing with the exact same issue.  I'm just trying to find a way to restore the manager to a working state.  Because of this error, however, the node no longer recognizes itself as a manager in the swarm.","My config is 3 node cluster with 3 swarm managers. One of my manager got root(\/) FS overflow issue and got similar problem today with same errors output in logs. Restarting docker deamon and server itself isn't fix the problem.\r\nMy solution was loging in to another swarm manager node, demote failed manager node and remove from swarm. After it, join problem node to exesting swarm again. But it's worse solution by my oppinion =(","how to solved this problem? i have met this same error","courtesy of: https:\/\/github.com\/moby\/moby\/issues\/29180#issuecomment-265219710\r\n\r\n\r\n as long as you have at least one remaining manager, i was able to solve it by:\r\n\r\n- freeing disk space first (rm logs, resizing disk)\r\n- listing docker swarm ```docker node ls```\r\n- demoting the broken node to a worker on the remaining manager ```docker node demote <id>```\r\n- leaving the swarm on the broken node ```docker swarm leave```\r\n- removing ```\/var\/lib\/docker\/swarm``` on the broken node\r\n- creating join token on the working manager ```docker swarm join-token <manager|worker>```\r\n- rejoining swarm ```docker swarm join --token xxxxx```\r\n- cleaning  up by removing the old, broken node from the swarm ```docker node rm <id>```"],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"awslogs driver for ECS - the newest log is not sent to CW if it is in EMF format","body":"### Description\n\nI'm seeing a weird behaviour of `awslogs` driver in ECS when it comes to sending logs to CloudWatch in EMF format.\r\n\r\nMy web Python app (Docker, FastAPI) writes logs to the stdout using `aws-embedded-metrics-python`. This works just fine when running locally in Python or in Docker locally. All logs are seen in stdout.\r\n\r\nHowever, in AWS ECS, if the following two logs are sent:\r\n```\r\n1. {\"datetime\": \"2023-08-31T12:27:41.576840Z\", \"level\": \"WARNING\", \"message\": \"Blahblah\", \"taskArn\": \"6900448c0b [...] blah\r\n2. {\"Environment\": \"tdudek\", \"_aws\": {\"Timestamp\": 1693484861577, \"CloudWatchMetrics\": [{\"Dimensions\": [[\"Environment\"]], \"Metric [...] # correct EMF format\r\n```\r\nonly 1. is seen in CW.\r\n\r\nIf I swap the order to the following:\r\n```\r\n1. {\"Environment\": \"tdudek\", \"_aws\": {\"Timestamp\": 1693484861577, \"CloudWatchMetrics\": [{\"Dimensions\": [[\"Environment\"]], \"Metric [...] # correct EMF format}\r\n2. {\"datetime\": \"2023-08-31T12:27:41.576840Z\", \"level\": \"WARNING\", \"message\": \"Blahblah\", \"taskArn\": \"6900448c0b [...]}\r\n```\r\nthen both 1. and 2. are seen in CW.\r\n\r\nIf I print something like this:\r\n```\r\n1. {\"datetime\": \"2023-08-31T12:27:41.576840Z\", \"level\": \"WARNING\", \"message\": \"Blahblah\", \"taskArn\": \"6900448c0b [...]}\r\n2. {\"Environment\": \"tdudek\", \"_aws\": {\"Timestamp\": 1693484861577, \"CloudWatchMetrics\": [{\"Dimensions\": [[\"Environment\"]], \"Metric [...] # correct EMF format}\r\n3. {\"datetime\": \"2023-08-31T12:27:41.576840Z\", \"level\": \"WARNING\", \"message\": \"Blahbalah\", \"taskArn\": \"6900448c0b [...]}\r\n```\r\nThen 1., 2. and 3. are sent to CW.\r\n\r\n\r\nIt is as if `awslogs` ignored\/buffered the last log if it happens to be in EMF format. Is this a documented \/ known behaviour or a bug?\r\n\n\n### Reproduce\n\nRun your container on AWS ECS and make it log ONLY the lines specified above (any other log will probably trigger the push-to-CW).\n\n### Expected behavior\n\nEvery log should be pushed to CW, EMF included.\n\n### docker version\n\n```bash\ndocker version\r\nClient:\r\n Version:           20.10.23\r\n API version:       1.41\r\n Go version:        go1.19.8\r\n Git commit:        7155243\r\n Built:             Mon May  1 21:07:11 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.23\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.19.8\r\n  Git commit:       6051f14\r\n  Built:            Wed Apr 19 00:00:00 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.19\r\n  GitCommit:        1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\ndocker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc., 0.0.0+unknown)\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 3\r\n Server Version: 20.10.23\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1e1ea6e986c6c86565bc33d52e34b81b3e2bc71f\r\n runc version: f19387a6bec4944c770f7668ab51c4348d9c2f38\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 6.1.41-63.114.amzn2023.x86_64\r\n Operating System: Amazon Linux 2023\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 905.2MiB\r\n Name: ip-10-211-1-15.eu-north-1.compute.internal\r\n ID: CO5Z:OF4D:XQ4Q:2CAU:J7RP:NEA7:3AVN:X3EF:3M7L:LRJW:Y346:LHFH\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nThis happens on AMI `ami-0b74ef3e3b43544ab` in `eu-north-1`.\r\n\r\nAlso reported it here: https:\/\/github.com\/awslabs\/aws-embedded-metrics-python\/issues\/109","comments":["\/cc @jchorl any ideas on this issue?  (asking since you had worked on https:\/\/github.com\/moby\/moby\/pull\/44087)"],"labels":["status\/0-triage","kind\/bug","version\/20.10"]},{"title":"Copy command fails when container is using a specific user","body":"### Description\r\n\r\nWhen creating a container with busybox and a specific user `docker run --rm -it --user=4 busybox`, `docker cp -a ~\/Downloads\/with <contianer>:\/home` fails with `Error response from daemon: unable to find getent command` in latest version.\r\n\r\nThe previous flow works with `20.10.25` and `23.0.6` but fails with `24.0.5`.\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n1. `docker run --rm -it --user=4 busybox`\r\n2. `docker cp -a ~\/Downloads\/with <container-id>:\/home`\r\n\r\n### Expected behavior\r\n\r\n`docker cp -a` should copy the file with the right uid\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.2-rd\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        e63f5fa\r\n Built:             Fri May 26 16:40:56 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.22.1 (118664)\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:38 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.2-rd\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.1\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     \/Users\/eddumelendez\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 46\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 5.15.49-linuxkit-pr\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.668GiB\r\n Name: docker-desktop\r\n ID: 767c69bd-3b1e-4ea6-b055-31c769b846dd\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Thanks for reporting; I'm able to reproduce this on a v24.0.5 install, both on Linux and as part of Docker Desktop;\r\n\r\n```bash\r\ndocker run -dit --rm --user=4 --name mycontainer busybox\r\n\r\necho 'hello' > hello.txt\r\n\r\ndocker cp -a .\/hello.txt mycontainer:\/home\/\r\nSuccessfully copied 2.05kB to mycontainer:\/home\/\r\nError response from daemon: unable to find getent command\r\n```\r\n\r\n`getent` is installed on the host;\r\n\r\n```bash\r\ncommand -v getent\r\n\/usr\/bin\/getent\r\n```\r\n\r\nBut it's in the same directory as `dockerd`;\r\n\r\n```bash\r\ncommand -v getent\r\n\/usr\/bin\/getent\r\n```\r\n\r\nI wonder if this could be another permutation of go1.19 and up not allowing relative paths;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46091\r\n","Nope, that's not it; tried to move the `dockerd` binary;\r\n\r\n```bash\r\nsystemctl stop docker\r\n\r\nmv \/usr\/bin\/dockerd \/usr\/local\/bin\/\r\n\r\ncommand -v dockerd\r\n\/usr\/local\/bin\/dockerd\r\n\r\n\r\ndocker run -dit --rm --user=4 --name mycontainer busybox\r\n\r\necho 'hello' > hello.txt\r\n\r\ndocker cp -a .\/hello.txt mycontainer:\/\r\nSuccessfully copied 2.05kB to mycontainer:\/\r\nError response from daemon: unable to find getent command\r\n```\r\n\r\nLogs;\r\n\r\n```\r\nERRO[2023-09-06T14:25:45.669555385Z] Handler for PUT \/v1.43\/containers\/mycontainer\/archive returned error: unable to find getent command\r\n```\r\n\r\n\r\nNote that running the `docker:dind` image (which is statically linked, and may not be using `getent`) shows a different error (possibly if a `NotFound` error is returned);\r\n\r\nRunning docker 24.0.6 docker-in-docker;\r\n\r\n```bash\r\ndocker run -dit --rm --user=4 --name mycontainer busybox\r\n\r\necho 'hello' > hello.txt\r\n\r\ndocker cp -a .\/hello.txt mycontainer:\/home\r\nSuccessfully copied 2.05kB to mycontainer:\/home\r\nError response from daemon: Could not find the file \/home in container mycontainer\r\n\r\ndocker cp -a .\/hello.txt mycontainer:\/tmp\/\r\nSuccessfully copied 2.05kB to mycontainer:\/tmp\/\r\nError response from daemon: Could not find the file \/tmp\/ in container mycontainer\r\n```\r\n","Hm... alright, so this looks to be closely related to;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/45719\r\n- which had a fix \/ workaround in https:\/\/github.com\/moby\/moby\/pull\/45720\r\n\r\nI tried to patch some of the code involved to get more details, and as part of that, I included the `PATH` and actual error that occurred. I also tried to include the output of `ls -la \/usr\/local\/bin`, which is when I ran into the same issue as linked above (`\/dev\/null` not being available);\r\n\r\n```\r\nError response from daemon: unable to find getent command: path (\/usr\/local\/cli:\/go\/bin:\/usr\/local\/go\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin): exec: \"getent\": executable file not found in $PATH: : open \/dev\/null: no such file or directory\r\n```\r\n\r\n```patch\r\ndiff --git a\/pkg\/idtools\/idtools_unix.go b\/pkg\/idtools\/idtools_unix.go\r\nindex a3a13600f7..e9b0aff20a 100644\r\n--- a\/pkg\/idtools\/idtools_unix.go\r\n+++ b\/pkg\/idtools\/idtools_unix.go\r\n@@ -19,6 +19,7 @@ import (\r\n var (\r\n        entOnce   sync.Once\r\n        getentCmd string\r\n+       getentErr error\r\n )\r\n\r\n func mkdirAs(path string, mode os.FileMode, owner Identity, mkAll, chownExisting bool) error {\r\n@@ -161,10 +162,10 @@ func getentGroup(name string) (user.Group, error) {\r\n }\r\n\r\n func callGetent(database, key string) (io.Reader, error) {\r\n-       entOnce.Do(func() { getentCmd, _ = resolveBinary(\"getent\") })\r\n+       entOnce.Do(func() { getentCmd, getentErr = resolveBinary(\"getent\") })\r\n        \/\/ if no `getent` command on host, can't do anything else\r\n-       if getentCmd == \"\" {\r\n-               return nil, fmt.Errorf(\"unable to find getent command\")\r\n+       if getentErr != nil {\r\n+               return nil, fmt.Errorf(\"unable to find getent command: %w\", getentErr)\r\n        }\r\n        command := exec.Command(getentCmd, database, key)\r\n        \/\/ we run getent within container filesystem, but without \/dev so \/dev\/null is not available for exec to mock stdin\r\ndiff --git a\/pkg\/idtools\/utils_unix.go b\/pkg\/idtools\/utils_unix.go\r\nindex 517a2f52ca..b628a8e544 100644\r\n--- a\/pkg\/idtools\/utils_unix.go\r\n+++ b\/pkg\/idtools\/utils_unix.go\r\n@@ -4,6 +4,7 @@ package idtools \/\/ import \"github.com\/docker\/docker\/pkg\/idtools\"\r\n\r\n import (\r\n        \"fmt\"\r\n+       \"os\"\r\n        \"os\/exec\"\r\n        \"path\/filepath\"\r\n )\r\n@@ -11,7 +12,9 @@ import (\r\n func resolveBinary(binname string) (string, error) {\r\n        binaryPath, err := exec.LookPath(binname)\r\n        if err != nil {\r\n-               return \"\", err\r\n+               out, err2 := exec.Command(\"ls\", \"-la \/usr\/bin\").CombinedOutput()\r\n+               return \"\", fmt.Errorf(\"path (%s): %w: %s: %v\", os.Getenv(\"PATH\"), err, out, err2)\r\n        }\r\n        resolvedPath, err := filepath.EvalSymlinks(binaryPath)\r\n        if err != nil {\r\n```\r\n\r\nSo what I suspect is happening here is that the `cp` is running in an environment where not everything is mounted \/ available, but I need to dig a bit further.\r\n","Got my debug code working with a similar workaround, and this is what I get as content of `\/usr\/bin`;\r\n\r\n```\r\ndocker cp -a .\/hello.txt mycontainer:\/home\r\nSuccessfully copied 2.05kB to mycontainer:\/home\r\nError response from daemon: unable to find getent command: path (\/usr\/local\/cli:\/go\/bin:\/usr\/local\/go\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin): exec: \"getent\": executable file not found in $PATH: total 8\r\ndrwxr-xr-x    2 root     root          4096 Jul 17 18:29 .\r\ndrwxr-xr-x    4 root     root          4096 Jul 17 18:29 ..\r\nlrwxrwxrwx    1 root     root            13 Jul 17 18:29 env -> ..\/..\/bin\/env\r\n: <nil>\r\n```\r\n\r\n\r\n","I didn't look too closely into this yet, but isn't this directly related to https:\/\/github.com\/moby\/moby\/pull\/44210 which made some changes involving performing the archive in a separate mount namespace (which shouldn't be able to access the host `getent`)?","And I just realised that `busybox` doesn't have `getent`, so the lookup failing is \"expected\" (but we should probably have a better fallback for numeric IDs). Running the same but with an ubuntu image (which does come with `getent`);\r\n\r\n```bash\r\ndocker run -dit --rm --user=4 --name mycontainer ubuntu\r\n86cf328efe33783347f4b9b05aa446a645ec3e505c50f556cbadc554c82332d8\r\n\r\ndocker cp -a .\/hello.txt mycontainer:\/home\r\nSuccessfully copied 2.05kB to mycontainer:\/home\r\n```\r\n\r\nSome things worth noting though; the lookup uses the `os.GetEnv(\"PATH\")` but runs within the container's rootfs, so it should probably use the _container's_ `PATH`, but we should also have a better fallback altogether.\r\n","Just checked and it works before https:\/\/github.com\/moby\/moby\/pull\/44210 - 2bdc7fb0a1e00439aa88438f1164677dda95737f is the first commit the issue appears.\r\n\r\ncc @corhere "],"labels":["kind\/bug","status\/confirmed","version\/24.0"]},{"title":"Swarm mode: Ports filtered between two dock on same overlay network but different hosts.","body":"### Description\r\n\r\nHi , \r\nI've setup 4 vm on vmware esxi:\r\ndmain (docker manager)\r\nd01,d02,d03 (docker worker)\r\nAll vm are ubuntu 22.04 , docker 24.05 ( I've also try with debian ).\r\nI've create the swarm and join nodes:\r\n```\r\nID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\nwlg4eo64k5j57wpr3laa6zb6d     d01        Ready     Active                          24.0.5\r\ns03apb4agqbbwss3vi5mu7nft     d02        Ready     Active                          24.0.5\r\ntu5xtll3so945mo16xnfemioe     d03        Ready     Active                          24.0.5\r\np984umcz54jt4opuij83u2ed6 *   dmain      Ready     Active         Leader           24.0.5\r\n```\r\n\r\nIf I run two dock from a service on the same node I can ping between the two AND comunicate between ports.\r\nIf I move one on a node and one on another I can ping but CAN'T comunicate between ports.\r\nThis is a test made with nl -l 1000 on a node and a nmap on other node:\r\nsame node:        1000\/tcp open\r\ndifferent node:  1000\/tcp filtered\r\n\r\nThis is the network definition:\r\n```json\r\n[\r\n    {\r\n        \"Name\": \"pippo_net\",\r\n        \"Id\": \"9cv8vw1kdqt8mwjt9ruund0pt\",\r\n        \"Created\": \"2023-08-31T15:51:46.829400641Z\",\r\n        \"Scope\": \"swarm\",\r\n        \"Driver\": \"overlay\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"10.0.5.0\/24\",\r\n                    \"Gateway\": \"10.0.5.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": true,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {\r\n            \"f5c07cecb60d5f866019c7ba37a611001fb7d43bd36e1619051acf53e68a9313\": {\r\n                \"Name\": \"pippo_agent.wlg4eo64k5j57wpr3laa6zb6d.xd6c1karhzflxdlvz2nhzgp3p\",\r\n                \"EndpointID\": \"a21f3b77466a38afaf5013765186a5ce53b3add8011c1f69cb0e87002f7facb3\",\r\n                \"MacAddress\": \"02:42:0a:00:05:0b\",\r\n                \"IPv4Address\": \"10.0.5.11\/24\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"lb-swarmpit_net\": {\r\n                \"Name\": \"pippo_net-endpoint\",\r\n                \"EndpointID\": \"bb82dd92afd381af4780bc6be832f141c87f0c8e8a07f2d6d86431dbb21090ef\",\r\n                \"MacAddress\": \"02:42:0a:00:05:0e\",\r\n                \"IPv4Address\": \"10.0.5.14\/24\",\r\n                \"IPv6Address\": \"\"\r\n            }\r\n        },\r\n        \"Options\": {\r\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4101\"\r\n        },\r\n        \"Labels\": {\r\n            \"com.docker.stack.namespace\": \"pippo\"\r\n        },\r\n        \"Peers\": [\r\n            {\r\n                \"Name\": \"f05ab9dbd658\",\r\n                \"IP\": \"10.0.30.200\"\r\n            },\r\n            {\r\n                \"Name\": \"4066916b6f5b\",\r\n                \"IP\": \"10.0.30.101\"\r\n            },\r\n            {\r\n                \"Name\": \"c87417002761\",\r\n                \"IP\": \"10.0.30.102\"\r\n            },\r\n            {\r\n                \"Name\": \"cee67ea006c2\",\r\n                \"IP\": \"10.0.30.103\"\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\nwhat am I doing wrong ?\r\n\r\n\r\n\r\n### Reproduce\r\n\r\n1) [MANAGEMENT] `docker swarm init --advertise-addr <my ip>`\r\n2) [WORKER 1,2,3] `docker swarm join......`\r\n3) `docker service create ....`\r\n4) \r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 7\r\n  Running: 6\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 9\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: p984umcz54jt4opuij83u2ed6\r\n  Is Manager: true\r\n  ClusterID: hq32icl0m5ty3xak6xua6ms3f\r\n  Managers: 1\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.30.200\r\n  Manager Addresses:\r\n   10.0.30.200:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-82-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 3.82GiB\r\n Name: dmain\r\n ID: f7db4405-183f-4a9f-8e8e-6d9165c903c0\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Hi all , \r\nI've replicate the entire environment on centos 8 and I get the same problem.\r\nI've deploy portainer for swarm mode ( using one main app and one agent on every node ) and it won't work ,\r\nafter some debugging I found that scanning the agent port always work from the same node but NON from different \r\nnode.\r\n\r\nSAME NODE:\r\n```bash\r\nroot@4611888a6705:\/# nmap 192.168.1.5\r\nStarting Nmap 7.80 ( https:\/\/nmap.org ) at 2023-09-01 09:36 UTC\r\nNmap scan report for portainer_agent.7sygohhg8u1ik9nvktd119kzj.z4ovxlunb48qta3tsvbtimwhw.portainer_agentnet (192.168.1.5)\r\nHost is up (0.0000020s latency).\r\nNot shown: 999 closed ports\r\nPORT     STATE SERVICE\r\n9001\/tcp open  tor-orport\r\nMAC Address: 02:42:C0:A8:01:05 (Unknown)\r\n```\r\n\r\nDIFFERENT NODE:\r\n```bash\r\nroot@4611888a6705:\/# nmap 192.168.1.5\r\nStarting Nmap 7.80 ( https:\/\/nmap.org ) at 2023-09-01 09:36 UTC\r\nNmap scan report for portainer_agent.7sygohhg8u1ik9nvktd119kzj.z4ovxlunb48qta3tsvbtimwhw.portainer_agentnet (192.168.1.5)\r\nHost is up (0.0000020s latency).\r\nNot shown: 999 closed ports\r\nPORT     STATE SERVICE\r\n9001\/tcp open  tor-orport\r\nMAC Address: 02:42:C0:A8:01:05 (Unknown)\r\n```\r\n\r\nNow , I've try three different environment with latest docker-ce ( ubuntu 22.04 , debian 12 , centos 8 ) and I get the \r\nsame result everywhere , is there somethink I don't understand or this is a big bug ?\r\n\r\nthanks in advance,\r\nmarco.","Ok , I found a solution by myself , using encrypted overlay network made it work.\r\nI found it very strange , is this the expected behavior ?","I'm facing the same problem. Services A and B get deployed via stack deploy. A -> Node1, B -> Node2. \r\n\r\nFrom service A i can ping service B in the overlay network but the ports are being filtered and service B can't connect to Service A port\r\n\r\n@neroita how do you managed to encrypt your overlay network?","I figured out how to encrypt the network as @neroita sugested and i can confirm that while it does solve the problem, i think it's not the expected behavior.","Yes I can confirm the bug with latest version , please can someone check it ?","The problem still persists as the network is encrypted. After some time running well, ports get filtered again.\r\n\r\n```\r\n23\/12\/04 19:02:30 INFO Utils: Successfully started service 'sparkWorker' on port 7000.\r\n23\/12\/04 19:02:30 INFO Worker: Worker decommissioning not enabled.\r\n23\/12\/04 19:02:30 INFO Worker: Starting Spark worker spark-worker-a:7000 with 2 cores, 16.0 GiB RAM\r\n23\/12\/04 19:02:30 INFO Worker: Running Spark version 3.5.0\r\n23\/12\/04 19:02:30 INFO Worker: Spark home: \/opt\/spark\r\n23\/12\/04 19:02:30 INFO ResourceUtils: ==============================================================\r\n23\/12\/04 19:02:30 INFO ResourceUtils: No custom resources configured for spark.worker.\r\n23\/12\/04 19:02:30 INFO ResourceUtils: ==============================================================\r\n23\/12\/04 19:02:30 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI\r\n23\/12\/04 19:02:30 INFO Utils: Successfully started service 'WorkerUI' on port 8081.\r\n23\/12\/04 19:02:31 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http:\/\/0.0.0.0:8081\r\n23\/12\/04 19:02:31 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/04 19:02:31 WARN Worker: Failed to connect to master spark-master:7077\r\norg.apache.spark.SparkException: Exception thrown in awaitResult:\r\n        at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\r\n        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\r\n        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\r\n        at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n        at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n        at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:313)\r\n        at java.base\/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\r\n        at java.base\/java.util.concurrent.FutureTask.run(Unknown Source)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.base\/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.base\/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Failed to connect to spark-master\/10.0.2.5:7077\r\n        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\r\n        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\r\n        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\r\n        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\r\n        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\r\n        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\r\n        ... 4 more\r\nCaused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: spark-master\/10.0.2.5:7077\r\nCaused by: java.net.ConnectException: Connection refused\r\n        at java.base\/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n        at java.base\/sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)\r\n        at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\r\n        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\r\n        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n        at java.base\/java.lang.Thread.run(Unknown Source)\r\n23\/12\/04 19:02:45 INFO Worker: Retrying connection to master (attempt # 1)\r\n23\/12\/04 19:02:45 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/04 19:02:45 INFO TransportClientFactory: Successfully created connection to spark-master\/10.0.2.5:7077 after 3 ms (0 ms spent in bootstraps)\r\n23\/12\/04 19:02:45 INFO Worker: Successfully registered with master spark:\/\/spark-master:7077\r\n23\/12\/05 13:21:49 INFO Worker: spark-master:7077 Disassociated !\r\n23\/12\/05 13:21:49 ERROR Worker: Connection to master failed! Waiting for master to reconnect...\r\n23\/12\/05 13:21:49 INFO Worker: spark-master:7077 Disassociated !\r\n23\/12\/05 13:21:49 ERROR Worker: Connection to master failed! Waiting for master to reconnect...\r\n23\/12\/05 13:21:49 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:21:49 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.\r\n23\/12\/05 13:21:49 INFO TransportClientFactory: Found inactive connection to spark-master\/10.0.2.6:7077, creating a new one.\r\n23\/12\/05 13:22:03 INFO Worker: Retrying connection to master (attempt # 1)\r\n23\/12\/05 13:22:03 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:22:17 INFO Worker: Retrying connection to master (attempt # 2)\r\n23\/12\/05 13:22:17 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:22:31 INFO Worker: Retrying connection to master (attempt # 3)\r\n23\/12\/05 13:22:31 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:22:45 INFO Worker: Retrying connection to master (attempt # 4)\r\n23\/12\/05 13:22:45 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:22:59 INFO Worker: Retrying connection to master (attempt # 5)\r\n23\/12\/05 13:22:59 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:23:13 INFO Worker: Retrying connection to master (attempt # 6)\r\n23\/12\/05 13:23:13 INFO Worker: Connecting to master spark-master:7077...\r\n23\/12\/05 13:23:49 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Connecting to spark-master\/10.0.2.6:7077 timed out (120000 ms)\r\n23\/12\/05 13:23:49 WARN Worker: Failed to connect to master spark-master:7077\r\n```\r\n\r\nA port scan shows that port 7077 is being filtered on spark-master (node 1)\r\n\r\nnmap -p 7077 spark-master\r\n```\r\nStarting Nmap 7.80 ( https:\/\/nmap.org ) at 2023-12-05 18:04 UTC\r\nNmap scan report for spark-master (10.0.2.6)\r\nHost is up (0.000044s latency).\r\nOther addresses for spark-master (not scanned): 10.0.2.5\r\nrDNS record for 10.0.2.6: cluster-spark-dapi_spark-master.yxm9ui9rauhsj228rsp8emuu7.ylbdg8vi9w8hc4evn9w28dnhk.cluster-spark-nw\r\n\r\nPORT     STATE    SERVICE\r\n7077\/tcp filtered unknown\r\n```\r\n```bash\r\n$ sudo docker network inspect cluster-spark-nw\r\n[\r\n    {\r\n        \"Name\": \"cluster-spark-nw\",\r\n        \"Id\": \"9c3feu2mjihjggxm6qujuazsk\",\r\n        \"Created\": \"2023-12-04T15:02:20.888823888-04:00\",\r\n        \"Scope\": \"swarm\",\r\n        \"Driver\": \"overlay\",\r\n        \"EnableIPv6\": false,\r\n        \"IPAM\": {\r\n            \"Driver\": \"default\",\r\n            \"Options\": null,\r\n            \"Config\": [\r\n                {\r\n                    \"Subnet\": \"10.0.2.0\/24\",\r\n                    \"Gateway\": \"10.0.2.1\"\r\n                }\r\n            ]\r\n        },\r\n        \"Internal\": false,\r\n        \"Attachable\": true,\r\n        \"Ingress\": false,\r\n        \"ConfigFrom\": {\r\n            \"Network\": \"\"\r\n        },\r\n        \"ConfigOnly\": false,\r\n        \"Containers\": {\r\n            \"97a111bd88c2e53ba6b2e7bd6db9165becf9e68672b59bf92f8a2f92b1eaf944\": {\r\n                \"Name\": \"cluster-spark-dapi_spark-master.yxm9ui9rauhsj228rsp8emuu7.ylbdg8vi9w8hc4evn9w28dnhk\",\r\n                \"EndpointID\": \"b690209f3467fe19d3967aaa9ae38a52bd23c04040fcd6cc670e3a0c0021a0fb\",\r\n                \"MacAddress\": \"XX:XX:XX:XX:XX:XX\",\r\n                \"IPv4Address\": \"10.0.2.6\/24\",\r\n                \"IPv6Address\": \"\"\r\n            },\r\n            \"lb-cluster-spark-nw\": {\r\n                \"Name\": \"cluster-spark-nw-endpoint\",\r\n                \"EndpointID\": \"bf6808d330e0080990205721117dec9a387e0b581f5717b769c4f51cb060732b\",\r\n                \"MacAddress\": \"XX:XX:XX:XX:XX:XX\",\r\n                \"IPv4Address\": \"10.0.2.7\/24\",\r\n                \"IPv6Address\": \"\"\r\n            }\r\n        },\r\n        \"Options\": {\r\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4098\",\r\n            \"encrypted\": \"true\"\r\n        },\r\n        \"Labels\": {},\r\n        \"Peers\": [\r\n            {\r\n                \"Name\": \"4d6b821c2fa5\",\r\n                \"IP\": \"10.0.17.211\"\r\n            }\r\n        ]\r\n    }\r\n]\r\n```"],"labels":["kind\/question","status\/0-triage","area\/networking","area\/swarm","area\/networking\/d\/overlay","version\/24.0"]},{"title":"libnetwork: node-discovery: fix error-handling, and some refactor","body":"### libnetwork: handleDriverTableEvent: reduce some duplication\r\n\r\nI guess the attempt here was to not duplicate code, but in doing so,\r\nthere was actually more code-duplication than necessary.\r\n\r\n\r\n### libnetwork: Controller.handleNodeTableEvent, handleEpTableEvent move vars\r\n\r\nmove variables closer to where they're used, and rename them where appropriate.\r\n\r\n### libnetwork: Controller.handleNodeTableEvent: skip networkdb.UpdateEvent\r\n\r\nController.handleNodeTableEvent was logging an error if it would handle\r\na `networkdb.UpdateEvent`, however, the function would still continue\r\nprocessing the event, and attempt to unmarshal an empty `[]byte` to a\r\n`networkdb.NodeAddr` (resulting in an `unexpected end of JSON input` error).\r\n\r\nIf it _would_ proceed (i.e., if the unmarshal would not error), it would;\r\n\r\n- call `Controller.processNodeDiscovery` (with an empty list of IPs)\r\n- which would iterate over all registered drivers, calling `Controller.pushNodeDiscovery`\r\n  for drivers that implement `discoverapi.Discover`.\r\n- ultimately ending up in `Controller.pushNodeDiscovery` returning early\r\n  because no IPs were provided.\r\n\r\nBased on the above, it looks like returning early here is the right thing\r\nto do.\r\n\r\n### libnetwork: Controller.pushNodeDiscovery: return early\r\n\r\nSkip fetching the agent and parsing its IP-address if we don't use it.\r\n\r\n### libnetwork: Controller.handleNodeTableEvent: log event-type in errors\r\n\r\nAllow discovering what kind of event was received if an error occurs.\r\n\r\n### libnetwork: Controller.pushNodeDiscovery: take single IP, ignore nil\r\n\r\nThe signature to take multiple addresses originates in [libnetwork\/188][1]\r\n(\"Libnetwork Host Discovery using Swarm Discovery pkg), which initially\r\nhad a `diff` function as part of the `hostdiscovery` package, which would\r\nproduce a list of changes in IP-addresses (added, removed).\r\n\r\nDiscovery with external k\/v stores was removed, and the `hostdiscovery`\r\npackage was removed in 00f9b23c3ae1614b648e9da1c6423c381f58153e (https:\/\/github.com\/moby\/moby\/pull\/42247), and\r\ncurrent code always pass a single IP-address (if any).\r\n\r\nThe Controller.pushNodeDiscovery method contained a check whether the\r\npassed list of node-addresses was nil, however, this was never the\r\ncase because `Controller.handleNodeTableEvent` would unconditionally\r\n[initialize a slice][2] (even if the event had a `nil` IP-address).\r\n\r\nIn case of a `nil` IP-address, this address would be;\r\n\r\n- compared with the agent's advertiseAddr to determine if this is `self`\r\n- encoded as a string, and sent as `NodeDiscoveryData.Address`. This string\r\n  would be a literal `\"<nil>\"`.\r\n\r\nWhile I don't know if there's other implementations that treat this information\r\ndifferent, looking at the [overlay implementation of `driver.DiscoverNew`][3],\r\nit would check the field to be an empty string, and otherwise produce an\r\nerror (which wouldn't apply if it contained `\"<nil>\"`), and set the\r\ndriver's advertiseAddress to this value, effectively un-setting it\r\n(`net.ParseIP()` silently ignores invalid values), or producing an error\r\n(`netip.ParseAddr()` will error).\r\n\r\nFor completeness: `driver.DiscoverDelete` has no implementation in the\r\noverlay driver, but both `driver.DiscoverNew` and `driver.DiscoverDelete`\r\nare both implemented by the `remote` driver, which sends them to the\r\nremote \"as-is\", so it will depend on the receiving end to handle.\r\n\r\nSo based on the overlay implementation, my assumption is that an empty\r\nnode-address should be considered _invalid_ (both for `DiscoverDelete`\r\nand `DiscoverNew`), so let's return early if we encounter one, but log\r\na warning to discover cases where this happened.\r\n\r\n[1]: https:\/\/github.com\/moby\/libnetwork\/commit\/ac4d7b61367e34ac341c2171d70d8b6a0a0fbecf\r\n[2]: https:\/\/github.com\/moby\/moby\/blob\/5b53ddfcdd1c0721f875d3e237078ab7de891a57\/libnetwork\/agent.go#L905\r\n[3]: https:\/\/github.com\/moby\/moby\/blob\/5b53ddfcdd1c0721f875d3e237078ab7de891a57\/libnetwork\/drivers\/overlay\/overlay.go#L92-L97\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["@corhere @dperny ptal \ud83e\udd17 ","> if there was a single exported networkdb.Event struct type with a Type driverapi.EventType field. All the conversion back-and-forth code would disappear.\r\n\r\nYes, perhaps that's something we could look into; was your intent to look at that as part of this PR, or for a follow-up?","> was your intent to look at that as part of this PR, or for a follow-up?\r\n\r\nI think it's in scope for this PR, but it's your PR so your call.","> I think it's in scope for this PR, but it's your PR so your call.\r\n\r\nAh, yes, I wanted to go look for those other changes, but didn't get further than rebasing the PR, then thought I'd push the rebase; will try to find some time and have a look again \ud83d\ude02 "],"labels":["status\/2-code-review","area\/networking","kind\/bugfix","kind\/refactor"]},{"title":"Filtering for an exact match on service name no longer works","body":"### Description\n\nI'm using the python API but this seems like something happening on the other side of the docker socket.\r\nFiltering for an existing service (named 'match_me') by it's name with `match_me` as the criteria I get the service. Filtering for the same service by it's exact name with `^match_me$` as the criteria I get nothing.\r\n\r\nAt least as of about 6 months ago I'm fairly certain I would have gotten the service. At that time I manually tested the bit of my code that takes advantage of that and I'm 100% certain it worked.\r\n\r\nThere is no problem filtering containers just services.\n\n### Reproduce\n\nThis assumes the host is part of a docker swarm.\r\n```python\r\nimport docker\r\ncli = docker.from_env()\r\ncli.services.create(\r\n        'alpine:latest', \r\n        name='match_me',\r\n        command='''\/bin\/sh -c 'while true; do sleep 600; done' '''\r\n)\r\nprint(cli.services.list(filters={'name': 'match_me'}))\r\nprint(cli.services.list(filters={'name': '^match_me$'}))\r\n```\r\n\r\nThe above prints this when run:\r\n```sh\r\n[<Service: w6p9duqis7hx>]\r\n[]\r\n```\n\n### Expected behavior\n\nThe service filters should respect regex.\r\nMore concretely the output of the script in steps to reproduce should be two identical lines (assuming there is only one service with 'match_me' in it's name).\r\n```sh\r\n[<Service: w6p9duqis7hx>]\r\n[<Service: w6p9duqis7hx>]\r\n```\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 47\r\n Server Version: 24.0.5\r\n Storage Driver: zfs\r\n  Zpool: tank\r\n  Zpool Health: ONLINE\r\n  Parent Dataset: tank\/var-lib-docker\r\n  Space Used By Parent: 15015874560\r\n  Space Available: 276434272256\r\n  Parent Quota: no\r\n  Compression: zstd\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: e256kxpwmyx613cpcjaf3y4z5\r\n  Is Manager: true\r\n  ClusterID: mrnowi9jvzyueowqd1ocr0lzk\r\n  Managers: 5\r\n  Nodes: 5\r\n  Default Address Pool: 172.19.0.0\/16  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.10.203\r\n  Manager Addresses:\r\n   192.168.10.14:2377\r\n   192.168.10.203:2377\r\n   192.168.10.54:2377\r\n   192.168.10.59:2377\r\n   192.168.10.8:2377\r\n Runtimes: io.containerd.runc.v2 nvidia runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-79-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.52GiB\r\n Name: cricket\r\n ID: POCH:ZWAM:7UPP:42AZ:A44V:UUB4:LZZZ:FZJS:GBC5:PB2G:PZ32:IPEP\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: haxwithaxe\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nMaybe I'm doing something wrong but I can't find anything. I'm writing tests so I have direct access to the service object I'm searching for from the beginning. I'm using an f-string to create the filter criteria `f'^{service.name}$'` so it isn't like there is trailing white space or typos making it not match. I take well to RTFM as long as there is a rough pointer to where or what in the FM I should be reading. Thank you for your time.","comments":["Do you know which version you were running before?","Unfortunately I don't know what version of docker I was using. I use docker-ce from the docker apt repo (both Debian and Ubuntu) and the last time I ran the relevant tests and they passed was January 31, 2023. I haven't run those tests again until a few days ago so there's not much granularity :\/ \r\nI can tell you the docker python module I was using was 6.0.1 and currently I'm using 6.1.3.","I just did some digging and based on release date it looks like version 20.10.23 is the most recent possible release that I could have been using.\r\nIf you have gotten to the point where you can't reproduce this I'd like to know so I can try to write my way around it until I stop hallucinating. :)","Just an update. This is still an issue for me in `24.0.6, build ed223bc`."],"labels":["area\/api","status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"Invalid overlay2+zfs storage drive chosen on Ubuntu install","body":"### Description\n\nAs [discussed on the Docker forums](https:\/\/forums.docker.com\/t\/unable-to-run-mv-when-building-any-image-all-mv-is-subdirectory-of-itself\/137409?u=saubin), when installing on Ubuntu 22.04 and following the official install instructions for Ubuntu, an invalid `Storage Driver` was chosen.\r\n\r\nThe output of `journalctl -e -u docker` showed no errors on boot:\r\n```\r\nAug 26 08:55:17 flatsky systemd[1]: Starting Docker Application Container Engine...\r\nAug 26 08:55:17 flatsky dockerd[5208]: time=\"2023-08-26T08:55:17.133673551-04:00\" level=info msg=\"Starting up\"\r\nAug 26 08:55:17 flatsky dockerd[5208]: time=\"2023-08-26T08:55:17.135422053-04:00\" level=info msg=\"detected 127.0.0.53 nameserver, assuming systemd-resolved, so using resolv.conf: \/run\/systemd\/resolve\/reso>\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.245966109-04:00\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.256373226-04:00\" level=info msg=\"Loading containers: start.\"\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.704598518-04:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0\/16. Daemon option --bip can be used to >\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.733618381-04:00\" level=info msg=\"Loading containers: done.\"\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.758408473-04:00\" level=info msg=\"Docker daemon\" commit=a61e2b4 graphdriver=overlay2 version=24.0.5\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.758751941-04:00\" level=info msg=\"Daemon has completed initialization\"\r\nAug 26 08:55:22 flatsky dockerd[5208]: time=\"2023-08-26T08:55:22.878648289-04:00\" level=info msg=\"API listen on \/run\/docker.sock\"\r\nAug 26 08:55:22 flatsky systemd[1]: Started Docker Application Container Engine.\r\nAug 26 09:00:01 flatsky dockerd[5208]: 2023\/08\/26 09:00:01 http2: server: error reading preface from client @: read unix \/run\/docker.sock->@: read: connection reset by peer\r\nAug 26 09:00:01 flatsky dockerd[5208]: time=\"2023-08-26T09:00:01.653909086-04:00\" level=warning msg=\"no trace recorder found, skipping\"\r\n```\n\n### Reproduce\n\nOn a Ubuntu 22.04 system using the ZFS filesystem, install following [the official guide](https:\/\/docs.docker.com\/engine\/install\/ubuntu\/). I can provide further details about my system if desired.\n\n### Expected behavior\n\nOn install, Docker should be configured with the ZFS device driver by default, since `overlayfs2` does not support the ZFS filesystem currently.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n...\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: zfs\r\n  Supports d_type: true\r\n  Using metacopy: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["Can you provide more details on the error message?  Your pasted output above is truncated.\r\n\r\n@dvdksn let's review docs here: https:\/\/docs.docker.com\/storage\/storagedriver\/select-storage-driver\/#supported-backing-filesystems\r\n\r\nThe docs describe a hard-coded list but current versions do a preliminary check to see if overlay is supported.  There may be combinations that are not supported and cannot be automatically detected.","> Can you provide more details on the error message? Your pasted output above is truncated.\r\n\r\nDo you mean the `journalctl -e -u docker` output? I didn't truncate it, but may have omitted output by accident. So that I don't omit anything important, how can you tell its truncated?","@Seanny123 Can you provide more details on the original error message? Overlay2 + zfs is not considered an invalid configuration, but there may be an interaction between overlay2 on zfs or version of zfs worth looking at there.\r\n\r\n> mv: cannot move '\/tmp\/lol.txt' to a subdirectory of itself, '\/tmp\/what.txt'\r\n\r\nIs this is occurring on multiple base images and which kernel version.","> Is this is occurring on multiple base images and which kernel version.\r\n\r\nAh, yeah, I left those details in the forum post. I'll copy them here:\r\n\r\n> Given the following trivial Docker file:\r\n> ```\r\n> FROM ubuntu:22.04\r\n> \r\n> # this works fine\r\n> RUN touch \/tmp\/lol.txt\r\n> RUN cp \/tmp\/lol.txt \/tmp\/lol2.txt\r\n> # this fails!\r\n> RUN mv \/tmp\/lol.txt \/tmp\/what.txt\r\n> ```\r\n> \r\n> I try to build the image using:\r\n> ```\r\n> docker build --no-cache --progress=plain -f Dockerfile.min -t local\/min_fail .\r\n> ```\r\n> \r\n> I get the error:\r\n> ```\r\n> #7 [4\/4] RUN mv \/tmp\/lol.txt \/tmp\/what.txt\r\n> #7 0.359 mv: cannot move '\/tmp\/lol.txt' to a subdirectory of itself, '\/tmp\/what.txt'\r\n> #7 ERROR: process \"\/bin\/sh -c mv \/tmp\/lol.txt \/tmp\/what.txt\" did not complete successfully: exit code: 1\r\n> ------\r\n>  > [4\/4] RUN mv \/tmp\/lol.txt \/tmp\/what.txt:\r\n> 0.359 mv: cannot move '\/tmp\/lol.txt' to a subdirectory of itself, '\/tmp\/what.txt'\r\n> ------\r\n> Dockerfile.min:7\r\n> --------------------\r\n>    5 |     RUN cp \/tmp\/lol.txt \/tmp\/lol2.txt\r\n>    6 |     # this fails!\r\n>    7 | >>> RUN mv \/tmp\/lol.txt \/tmp\/what.txt\r\n>    8 |     \r\n> --------------------\r\n> ERROR: failed to solve: process \"\/bin\/sh -c mv \/tmp\/lol.txt \/tmp\/what.txt\" did not complete successfully: exit code: 1\r\n> ```\r\n> \r\n> his also happens if I change the root image from Ubuntu to Fedora, so I don't think the problem is the base image:\r\n> ```\r\n> FROM fedora:38\r\n> \r\n> # this works fine\r\n> RUN touch \/tmp\/lol.txt\r\n> RUN cp \/tmp\/lol.txt \/tmp\/lol2.txt\r\n> # this still fails!\r\n> RUN mv \/tmp\/lol.txt \/tmp\/what.txt\r\n> ```\r\n> \r\n> The error also occurs if I set `DOCKER_BUILDKIT=0`.","Oops, forgot the kernel version you asked for, via `uname -a`:\r\n```\r\n6.2.0-26-generic\r\n```","This should be solved by https:\/\/github.com\/moby\/moby\/pull\/45890; the issue here is a version of OpenZFS that passes mount-time checks (for overlayfs, built in to the kernel), but that may not support all runtime, read-write operations required of the VFS implementation.","For the record, Ubuntu ships ZFS 2.1 for kernel 6.2 or older, which doesn't support the flags for `renameat2(2)`. The latest HWE kernel 6.5 ships with ZFS 2.2 which should support overlay2 reasonably well.\r\n\r\nI happen to have two Ubuntu 22.04 machines at hand and I can confirm this with experiments.\r\n\r\n### Machine 1\r\n\r\nOS: Ubuntu 22.04\r\nKernel: 6.5.0-14-generic\r\nDocker Info:\r\n\r\n```text\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n\r\nServer:\r\n ...\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: zfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n```\r\n\r\nZFS info: (`modinfo zfs | head -2`)\r\n\r\n```text\r\nfilename:       \/lib\/modules\/6.5.0-14-generic\/kernel\/zfs\/zfs.ko\r\nversion:        2.2.0-0ubuntu1~23.10\r\n```\r\n\r\nTest (exited with no error):\r\n\r\n```shell\r\n$ docker run --rm ubuntu mv \/root\/.bashrc \/root\/test\r\n$\r\n```\r\n\r\n### Machine 2\r\n\r\nOS: Ubuntu 22.04\r\nKernel: 6.2.0-39-generic\r\nDocker Info (Identical):\r\n\r\n```text\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n\r\nServer:\r\n ...\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: zfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n```\r\n\r\nZFS info:\r\n\r\n```text\r\nfilename:       \/lib\/modules\/6.2.0-39-generic\/kernel\/zfs\/zfs.ko\r\nversion:        2.1.9-2ubuntu1.1\r\n```\r\n\r\n```shell\r\n$ docker run --rm ubuntu mv \/root\/.bashrc \/root\/test\r\nmv: cannot move '\/root\/.bashrc' to a subdirectory of itself, '\/root\/test'\r\n$\r\n```"],"labels":["area\/storage\/overlay","status\/0-triage","kind\/bug","area\/docs","version\/24.0"]},{"title":"docker damon panic ","body":"### Description\r\n\r\nWhen I execute the docker command, Docker panicked.\r\n\r\n### Reproduce\r\n\r\n```bash\r\ndocker pull nginx:latest\r\n```\r\n\r\ndocker cli panic:\r\n\r\n```\r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x5609459d34c9 pc=0x5609459d34c9]\r\n```\r\n\r\n\r\nruntime stack:\r\n\r\n```\r\nruntime.throw({0x55f2902b6215?, 0x55f290b0fca0?})\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/panic.go:1047 +0x5f fp=0x7f8ed8d285e8 sp=0x7f8ed8d285b8 pc=0x55f28f81967f\r\nruntime: g 0: unexpected return pc for runtime.sigpanic called from 0x5609459d34c9\r\n\r\nstack: frame={sp:0x7f8ed8d285e8, fp:0x7f8ed8d28648} stack=[0x7f8ed85292a0,0x7f8ed8d28ea0)\r\n\r\n0x00007f8ed8d284e8:  0x000055f200000004  0x000000000000001f \r\n\r\n0x00007f8ed8d284f8:  0x00005609459d34c9  0x00005609459d34c9 \r\n\r\n0x00007f8ed8d28508:  0x0000000000000001  0x000055f28f81967f <runtime.throw+0x000000000000005f> \r\n\r\n0x00007f8ed8d28518:  0x00007f8ed8d285b8  0x000055f2902a2537 \r\n\r\n0x00007f8ed8d28528:  0x00007f8ed8d28568  0x000055f28f819a70 <runtime.fatalthrow.func1+0x0000000000000070> \r\n\r\n0x00007f8ed8d28538:  0x000000c0000076c0  0x000055f29043bce0 \r\n\r\n0x00007f8ed8d28548:  0x0000000000000001  0x00007f8ed8d285b8 \r\n\r\n0x00007f8ed8d28558:  0x000055f28f81967f <runtime.throw+0x000000000000005f>  0x000000c0000076c0 \r\n\r\n0x00007f8ed8d28568:  0x00007f8ed8d285a8  0x000055f28f8199cc <runtime.fatalthrow+0x000000000000006c> \r\n\r\n0x00007f8ed8d28578:  0x00007f8ed8d28588  0x000000c0000076c0 \r\n\r\n0x00007f8ed8d28588:  0x000055f28f819a00 <runtime.fatalthrow.func1+0x0000000000000000>  0x000000c0000076c0 \r\n\r\n0x00007f8ed8d28598:  0x000055f28f81967f <runtime.throw+0x000000000000005f>  0x00007f8ed8d285b8 \r\n\r\n0x00007f8ed8d285a8:  0x00007f8ed8d285d8  0x000055f28f81967f <runtime.throw+0x000000000000005f> \r\n\r\n0x00007f8ed8d285b8:  0x00007f8ed8d285c0  0x000055f28f8196a0 <runtime.throw.func1+0x0000000000000000> \r\n\r\n0x00007f8ed8d285c8:  0x000055f2902b6215  0x000000000000002a \r\n\r\n0x00007f8ed8d285d8:  0x00007f8ed8d28638  0x000055f28f82ff89 <runtime.sigpanic+0x00000000000003e9> \r\n\r\n0x00007f8ed8d285e8: <0x000055f2902b6215  0x000055f290b0fca0 \r\n\r\n0x00007f8ed8d285f8:  0x000000000024dd90  0x000000000024dd90 \r\n\r\n0x00007f8ed8d28608:  0x000000000000164a  0x000055f28f838ad0 <runtime.pcdatavalue+0x0000000000000050> \r\n\r\n0x00007f8ed8d28618:  0x000000c0000076c0  0x000055f29101bf00 \r\n\r\n0x00007f8ed8d28628:  0x000000000000adc3  0x000055f290b0fca0 \r\n\r\n0x00007f8ed8d28638:  0x00007f8ed8d28688 !0x00005609459d34c9 \r\n\r\n0x00007f8ed8d28648: >0x00007f8ed8d28688  0x000055f28f838ad0 <runtime.pcdatavalue+0x0000000000000050> \r\n\r\n0x00007f8ed8d28658:  0x000055f290e2dc90  0x000055f29101bf00 \r\n\r\n0x00007f8ed8d28668:  0x00007f8e0000164a  0x000055f28faf8765 <net\/http.(*Transport).dialConn.func6+0x0000000000000025> \r\n\r\n0x00007f8ed8d28678:  0x00007f8ed8d28bc8  0x00007f8ed8d28708 \r\n\r\n0x00007f8ed8d28688:  0x00007f8ed8d28d08  0x0000560945a081d9 \r\n\r\n0x00007f8ed8d28698:  0x000055f290daeed0  0x00007f8ed8d28708 \r\n\r\n0x00007f8ed8d286a8:  0x0000000000000000  0x00007f8ed8d28780 \r\n\r\n0x00007f8ed8d286b8:  0x000055f28f838009 <runtime.pcvalue+0x0000000000000209>  0x0000000000000000 \r\n\r\n0x00007f8ed8d286c8:  0x0000000000000000  0x0000000000000000 \r\n\r\n0x00007f8ed8d286d8:  0x0000000000000008  0x0000000000000000 \r\n\r\n0x00007f8ed8d286e8:  0x00000005d8d28b68  0x00007f8ed8514398 \r\n\r\n0x00007f8ed8d286f8:  0x0000000000000000  0x000055f28f84d5c0 <runtime.goexit+0x0000000000000000> \r\n\r\n0x00007f8ed8d28708:  0xd851439800000005  0x0000000000007f8e \r\n\r\n0x00007f8ed8d28718:  0x0000000000000000  0x0000000000000000 \r\n\r\n0x00007f8ed8d28728:  0x0000000000000000  0x0000000000000000 \r\n\r\n0x00007f8ed8d28738:  0x0000000000000000  0x0000000000000000 \r\n\r\nruntime.sigpanic()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/signal_unix.go:825 +0x3e9 fp=0x7f8ed8d28648 sp=0x7f8ed8d285e8 pc=0x55f28f82ff89\r\n\r\n\r\n\r\ngoroutine 1 [IO wait]:\r\n\r\nruntime.gopark(0x55f2910063f0?, 0xb?, 0x0?, 0x0?, 0x3?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc00061f070 sp=0xc00061f050 pc=0x55f28f81c3f6\r\n\r\nruntime.netpollblock(0x55f28f862865?, 0x8f7e4e4f?, 0xf2?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/netpoll.go:527 +0xf7 fp=0xc00061f0a8 sp=0xc00061f070 pc=0x55f28f814b57\r\n\r\ninternal\/poll.runtime_pollWait(0x7f8ed8514398, 0x72)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/netpoll.go:306 +0x89 fp=0xc00061f0c8 sp=0xc00061f0a8 pc=0x55f28f8479e9\r\n\r\ninternal\/poll.(*pollDesc).wait(0xc0003a7100?, 0xc000212000?, 0x0)\r\n\r\n\t\/usr\/local\/go\/src\/internal\/poll\/fd_poll_runtime.go:84 +0x32 fp=0xc00061f0f0 sp=0xc00061f0c8 pc=0x55f28f889212\r\n\r\ninternal\/poll.(*pollDesc).waitRead(...)\r\n\r\n\t\/usr\/local\/go\/src\/internal\/poll\/fd_poll_runtime.go:89\r\n\r\ninternal\/poll.(*FD).Read(0xc0003a7100, {0xc000212000, 0x1000, 0x1000})\r\n\r\n\t\/usr\/local\/go\/src\/internal\/poll\/fd_unix.go:167 +0x299 fp=0xc00061f188 sp=0xc00061f0f0 pc=0x55f28f88a5f9\r\n\r\nnet.(*netFD).Read(0xc0003a7100, {0xc000212000?, 0x6?, 0xc00061f228?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/fd_posix.go:55 +0x29 fp=0xc00061f1d0 sp=0xc00061f188 pc=0x55f28f9f93a9\r\n\r\nnet.(*conn).Read(0xc00011a930, {0xc000212000?, 0xc00061f478?, 0x55f28f8e740a?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/net.go:183 +0x45 fp=0xc00061f218 sp=0xc00061f1d0 pc=0x55f28fa0acc5\r\n\r\nnet.(*UnixConn).Read(0xc0001becc0?, {0xc000212000?, 0x87?, 0x76011999d8?})\r\n\r\n\t<autogenerated>:1 +0x29 fp=0xc00061f248 sp=0xc00061f218 pc=0x55f28fa1d109\r\n\r\nnet\/http.(*persistConn).Read(0xc0001e8b40, {0xc000212000?, 0x0?, 0x0?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:1943 +0x4e fp=0xc00061f2a8 sp=0xc00061f248 pc=0x55f28faf920e\r\n\r\nbufio.(*Reader).fill(0xc00010fbc0)\r\n\r\n\t\/usr\/local\/go\/src\/bufio\/bufio.go:106 +0xff fp=0xc00061f2e0 sp=0xc00061f2a8 pc=0x55f28f91a53f\r\n\r\nbufio.(*Reader).ReadSlice(0xc00010fbc0, 0x1?)\r\n\r\n\t\/usr\/local\/go\/src\/bufio\/bufio.go:372 +0x2f fp=0xc00061f330 sp=0xc00061f2e0 pc=0x55f28f91b12f\r\n\r\nnet\/http\/internal.readChunkLine(0x200000003?)\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/internal\/chunked.go:129 +0x25 fp=0xc00061f380 sp=0xc00061f330 pc=0x55f28fab73e5\r\n\r\nnet\/http\/internal.(*chunkedReader).beginChunk(0xc00063c060)\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/internal\/chunked.go:48 +0x28 fp=0xc00061f3b0 sp=0xc00061f380 pc=0x55f28fab6e48\r\n\r\nnet\/http\/internal.(*chunkedReader).Read(0xc00063c060, {0xc00004a602?, 0xc00061f458?, 0x55f28f88ca86?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/internal\/chunked.go:98 +0x156 fp=0xc00061f430 sp=0xc00061f3b0 pc=0x55f28fab7116\r\n\r\nnet\/http.(*body).readLocked(0xc000052040, {0xc00004a602?, 0x4?, 0x4?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transfer.go:839 +0x3c fp=0xc00061f480 sp=0xc00061f430 pc=0x55f28faee47c\r\n\r\nnet\/http.(*body).Read(0x55f28f7f7670?, {0xc00004a602?, 0xc00061f570?, 0x55f28f8964e5?})\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transfer.go:831 +0x125 fp=0xc00061f4f8 sp=0xc00061f480 pc=0x55f28faee345\r\n\r\nnet\/http.(*bodyEOFSignal).Read(0xc000052240, {0xc00004a602, 0x5fe, 0x5fe})\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:2792 +0x142 fp=0xc00061f578 sp=0xc00061f4f8 pc=0x55f28fafd902\r\n\r\nencoding\/json.(*Decoder).refill(0xc0000ee280)\r\n\r\n\t\/usr\/local\/go\/src\/encoding\/json\/stream.go:165 +0x188 fp=0xc00061f5c8 sp=0xc00061f578 pc=0x55f28f917a08\r\n\r\nencoding\/json.(*Decoder).readValue(0xc0000ee280)\r\n\r\n\t\/usr\/local\/go\/src\/encoding\/json\/stream.go:140 +0xbb fp=0xc00061f618 sp=0xc00061f5c8 pc=0x55f28f9175fb\r\n\r\nencoding\/json.(*Decoder).Decode(0xc0000ee280, {0x55f290722a60, 0xc00014fc20})\r\n\r\n\t\/usr\/local\/go\/src\/encoding\/json\/stream.go:63 +0x78 fp=0xc00061f648 sp=0xc00061f618 pc=0x55f28f917318\r\n\r\ngithub.com\/docker\/cli\/vendor\/github.com\/docker\/docker\/pkg\/jsonmessage.DisplayJSONMessagesStream({0x55f29089d880?, 0xc000052240}, {0x55f29089c480, 0xc000463c20}, 0x1, 0x1, 0x0)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/vendor\/github.com\/docker\/docker\/pkg\/jsonmessage\/jsonmessage.go:237 +0x14a fp=0xc00061f828 sp=0xc00061f648 pc=0x55f28fd29e8a\r\n\r\ngithub.com\/docker\/cli\/vendor\/github.com\/docker\/docker\/pkg\/jsonmessage.DisplayJSONMessagesToStream({0x55f29089d880, 0xc000052240}, {0x55f2908a2680, 0xc000463c20}, 0x0?)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/vendor\/github.com\/docker\/docker\/pkg\/jsonmessage\/jsonmessage.go:306 +0x8c fp=0xc00061f888 sp=0xc00061f828 pc=0x55f28fd2a26c\r\n\r\ngithub.com\/docker\/cli\/cli\/command\/image.imagePullPrivileged({0x55f2908a9150, 0xc0000460b0}, {0x55f2908b2498, 0xc0003940f0}, {{0xc000130da0, 0x1e}, 0xc0003a8930, {0x55f2908a1c90, 0xc00007c140}, 0xc0001ec720, ...}, ...)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/cli\/command\/image\/trust.go:285 +0x308 fp=0xc00061fa20 sp=0xc00061f888 pc=0x55f28fe53fc8\r\n\r\ngithub.com\/docker\/cli\/cli\/command\/image.RunPull({0x55f2908b2498, 0xc0003940f0}, {{0x7ffc281ad7b6, 0x5}, 0x0, {0x0, 0x0}, 0x0, 0x1})\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/cli\/command\/image\/pull.go:82 +0x41e fp=0xc00061fbf0 sp=0xc00061fa20 pc=0x55f28fe4f33e\r\n\r\ngithub.com\/docker\/cli\/cli\/command\/image.NewPullCommand.func1(0xc000005200?, {0xc000388ed0?, 0x1?, 0x1?})\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/cli\/command\/image\/pull.go:36 +0x70 fp=0xc00061fc40 sp=0xc00061fbf0 pc=0x55f28fe4eed0\r\n\r\ngithub.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra.(*Command).execute(0xc000005200, {0xc0002dbe50, 0x1, 0x1})\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra\/command.go:940 +0x862 fp=0xc00061fd78 sp=0xc00061fc40 pc=0x55f28fde1802\r\n\r\ngithub.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra.(*Command).ExecuteC(0xc000004300)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra\/command.go:1068 +0x3bd fp=0xc00061fe30 sp=0xc00061fd78 pc=0x55f28fde207d\r\n\r\ngithub.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra.(*Command).Execute(...)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/vendor\/github.com\/spf13\/cobra\/command.go:992\r\n\r\nmain.runDocker(0x0?)\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/cmd\/docker\/docker.go:263 +0x4b7 fp=0xc00061ff08 sp=0xc00061fe30 pc=0x55f2902783b7\r\n\r\nmain.main()\r\n\r\n\t\/go\/src\/github.com\/docker\/cli\/cmd\/docker\/docker.go:274 +0x97 fp=0xc00061ff80 sp=0xc00061ff08 pc=0x55f2902784d7\r\n\r\nruntime.main()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:250 +0x212 fp=0xc00061ffe0 sp=0xc00061ff80 pc=0x55f28f81bfb2\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc00061ffe8 sp=0xc00061ffe0 pc=0x55f28f84d5c1\r\n\r\n\r\n\r\ngoroutine 2 [force gc (idle)]:\r\n\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000074fb0 sp=0xc000074f90 pc=0x55f28f81c3f6\r\n\r\nruntime.goparkunlock(...)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:387\r\n\r\nruntime.forcegchelper()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:305 +0xb0 fp=0xc000074fe0 sp=0xc000074fb0 pc=0x55f28f81c230\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000074fe8 sp=0xc000074fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.init.6\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:293 +0x25\r\n\r\n\r\n\r\ngoroutine 3 [GC sweep wait]:\r\n\r\nruntime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000075780 sp=0xc000075760 pc=0x55f28f81c3f6\r\n\r\nruntime.goparkunlock(...)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:387\r\n\r\nruntime.bgsweep(0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgcsweep.go:319 +0xde fp=0xc0000757c8 sp=0xc000075780 pc=0x55f28f80747e\r\n\r\nruntime.gcenable.func1()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:178 +0x26 fp=0xc0000757e0 sp=0xc0000757c8 pc=0x55f28f7fc6e6\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0000757e8 sp=0xc0000757e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcenable\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:178 +0x6b\r\n\r\n\r\n\r\ngoroutine 4 [GC scavenge wait]:\r\n\r\nruntime.gopark(0xc00007e000?, 0x55f29043bc08?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000075f70 sp=0xc000075f50 pc=0x55f28f81c3f6\r\n\r\nruntime.goparkunlock(...)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:387\r\n\r\nruntime.(*scavengerState).park(0x55f29106b340)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgcscavenge.go:400 +0x53 fp=0xc000075fa0 sp=0xc000075f70 pc=0x55f28f805353\r\n\r\nruntime.bgscavenge(0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgcscavenge.go:633 +0x65 fp=0xc000075fc8 sp=0xc000075fa0 pc=0x55f28f805945\r\n\r\nruntime.gcenable.func2()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:179 +0x26 fp=0xc000075fe0 sp=0xc000075fc8 pc=0x55f28f7fc686\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000075fe8 sp=0xc000075fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcenable\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:179 +0xaa\r\n\r\n\r\n\r\ngoroutine 5 [finalizer wait]:\r\n\r\nruntime.gopark(0x1a0?, 0x55f29106bce0?, 0x0?, 0x7a?, 0xc000074770?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000074628 sp=0xc000074608 pc=0x55f28f81c3f6\r\n\r\nruntime.runfinq()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mfinal.go:193 +0x107 fp=0xc0000747e0 sp=0xc000074628 pc=0x55f28f7fb707\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0000747e8 sp=0xc0000747e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.createfing\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mfinal.go:163 +0x45\r\n\r\n\r\n\r\ngoroutine 18 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x55f28fcf5395?, 0x55f28f7e735d?, 0xc0?, 0x3?, 0xc0000767a8?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000076750 sp=0xc000076730 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc0000767e0 sp=0xc000076750 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0000767e8 sp=0xc0000767e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 34 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000070750 sp=0xc000070730 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc0000707e0 sp=0xc000070750 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0000707e8 sp=0xc0000707e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 8 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000076f50 sp=0xc000076f30 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc000076fe0 sp=0xc000076f50 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000076fe8 sp=0xc000076fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 35 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000070f50 sp=0xc000070f30 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc000070fe0 sp=0xc000070f50 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000070fe8 sp=0xc000070fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 19 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x21fec20269c37?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc0004c6750 sp=0xc0004c6730 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc0004c67e0 sp=0xc0004c6750 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0004c67e8 sp=0xc0004c67e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 9 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x21fec2026863e?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000077750 sp=0xc000077730 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc0000777e0 sp=0xc000077750 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0000777e8 sp=0xc0000777e0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 20 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x21fec20268369?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc0004c6f50 sp=0xc0004c6f30 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc0004c6fe0 sp=0xc0004c6f50 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc0004c6fe8 sp=0xc0004c6fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 10 [GC worker (idle)]:\r\n\r\nruntime.gopark(0x21fec20267c5d?, 0x0?, 0x0?, 0x0?, 0x0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000077f50 sp=0xc000077f30 pc=0x55f28f81c3f6\r\n\r\nruntime.gcBgMarkWorker()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1275 +0xf1 fp=0xc000077fe0 sp=0xc000077f50 pc=0x55f28f7fe451\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000077fe8 sp=0xc000077fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by runtime.gcBgMarkStartWorkers\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/mgc.go:1199 +0x25\r\n\r\n\r\n\r\ngoroutine 12 [select]:\r\n\r\nruntime.gopark(0xc00008bf68?, 0x4?, 0x5?, 0x0?, 0xc00008bdb0?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc00008bc00 sp=0xc00008bbe0 pc=0x55f28f81c3f6\r\n\r\nruntime.selectgo(0xc00008bf68, 0xc00008bda8, 0xc000052040?, 0x0, 0x0?, 0x1)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/select.go:327 +0x7be fp=0xc00008bd40 sp=0xc00008bc00 pc=0x55f28f82c0de\r\n\r\nnet\/http.(*persistConn).readLoop(0xc0001e8b40)\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:2227 +0xd85 fp=0xc00008bfc8 sp=0xc00008bd40 pc=0x55f28fafac05\r\n\r\nnet\/http.(*Transport).dialConn.func5()\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:1765 +0x26 fp=0xc00008bfe0 sp=0xc00008bfc8 pc=0x55f28faf87c6\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc00008bfe8 sp=0xc00008bfe0 pc=0x55f28f84d5c1\r\n\r\ncreated by net\/http.(*Transport).dialConn\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:1765 +0x16ea\r\n\r\n\r\n\r\ngoroutine 13 [select]:\r\n\r\nruntime.gopark(0xc000635f90?, 0x2?, 0xf8?, 0x5d?, 0xc000635f34?)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/proc.go:381 +0xd6 fp=0xc000635db0 sp=0xc000635d90 pc=0x55f28f81c3f6\r\n\r\nruntime.selectgo(0xc000635f90, 0xc000635f30, 0xc00013a180?, 0x0, 0x0?, 0x1)\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/select.go:327 +0x7be fp=0xc000635ef0 sp=0xc000635db0 pc=0x55f28f82c0de\r\n\r\nnet\/http.(*persistConn).writeLoop(0xc0001e8b40)\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:2410 +0xf2 fp=0xc000635fc8 sp=0xc000635ef0 pc=0x55f28fafbcf2\r\n\r\nnet\/http.(*Transport).dialConn.func6()\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:1766 +0x26 fp=0xc000635fe0 sp=0xc000635fc8 pc=0x55f28faf8766\r\n\r\nruntime.goexit()\r\n\r\n\t\/usr\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0x1 fp=0xc000635fe8 sp=0xc000635fe0 pc=0x55f28f84d5c1\r\n\r\ncreated by net\/http.(*Transport).dialConn\r\n\r\n\t\/usr\/local\/go\/src\/net\/http\/transport.go:1766 +0x173d\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.39 (downgraded from 1.43)\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:39:02 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          18.09.9\r\n  API version:      1.39 (minimum version 1.12)\r\n  Go version:       go1.11.13\r\n  Git commit:       039a7df\r\n  Built:            Wed Sep  4 16:22:32 2019\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 18.09.9\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: N\/A\r\n init version: fec3683\r\n Security Options:\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 3.10.0-1160.95.1.el7.x86_64\r\n Operating System: CentOS Linux 7 (Core)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 31.24GiB\r\n Name: localhost.domain.com\r\n ID: GPOC:XE4B:TV74:CCJU:WP22:2KDC:Y766:TSPT:7ZIW:II7J:FDEN:57AX\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Based on that output, I _think_ the panic is in the CLI, not the daemon, or did the daemon service actually crash?;\r\n\r\n\r\n```\r\ngo\/src\/github.com\/docker\/cli\/vendor\/github.com\/docker\/docker\/pkg\/jsonmessage\/jsonmessage.go:237\r\n```\r\n\r\n\r\nI should note that the daemon version you have installed is _very old_, and EOL (docker 18.09 reached EOL 5 years ago), and I would highly recommend updating to a currently supported version if possible, because it contains various vulnerabilities that are un-patched in that version.\r\n","1. The above log is from the CLI, but the error should have occurred in the daemon process because it restarted after executing the command.Dockerd pid has also changed.\r\n```\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]: \/usr\/local\/go\/src\/runtime\/select.go:327 +0x7be fp=0xco00bdaf48 sp=0xco00bdae08 pc=0\u00d755c61f791ebe\r\nAug 25 14:26:08 localhost.domain.com dockerd111232]: github.com\/docker\/docker\/pkg\/joutils.NewCancelReadcloser.func2 ()\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]:\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/pkg\/ioutils\/readers.go:121+0x3fp=0xcooobdafesp=0xco00bdaf48pc=0\u00d755c61fdc3b13\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]: runtime.goexit ()\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]: \/us\/local\/go\/src\/runtime\/asm_amd64.s:1598 +0\u00d71 fp=0xco00bdafes sp=0xco00bdafe0 pc=0\u00d755c61f7b6841\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]: created by github.com\/docker\/docker\/pkg\/outils.NewCancelReadcloser\r\nAug 25 14:26:08 localhost.domain.com dockerd[111232]:\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/pkg\/ioutils\/readers.go:119+0\u00d7318\r\nAug 25 14:26:10 localhost.domain.com systemd[1]: docker.service holdoff time over, scheduling restart.\r\nAug 25 14:26:10 localhost.domain.com systemd[1]: Stopped Docker Application Container Engine.\r\nAug 25 14:26:10 localhost.domain.com systemd[1]: Starting Docker Application Container Engine..\r\nAug 25 14:26:10 localhost.domain.com docker 111731]: time=\"2023-08-25714:26:10.569571106+08:00\" level=info msg=\"starting up\"\r\nAug 25 14:26:10 localhost.domain.com dockerd[1117311: time=\"2023-08-25T14:26:20.603059131+08:00\u201d level=info msg=\" [graphdriver] using prior storage driver: overlay2\"\r\nAug 25 14:26:10 localhost.domain.com dockerd [111731]\uff1atime=\"2023-08-25T14:26:10.605134571+08:00\" level=info msg=mLoading containers : start.\r\nAug 25 14:26:10 localhost.domain.com dockerd[111731]: time=\"2023-08-25714:26:10.726063319+08:00\" level=info msg=\"Default bridge (docker\u00ae) is assigned with an IP address 172.17.0.0\/16. Daemon option --bip can be used to set a preferred IP address\"\r\nAug 25 14:26:10 localhost.domain.com dockerd[111731]\uff1a time=\"2023-08-25T14:26:10.772257503+08:00\"\r\nlevel=info msg=\"Loading containers: done.\"\r\nAug 25 14:26:10 localhost.domain.com dockerd[111731]: time=\"2023-08-25T14:26:10.791705433+08:00\" level=info msg=\"Docker daemon\" commit=a61e2b4 graphdriver=overlay2 version=24.0.5\r\nAug 25 14:26:10 localhost.domain.com dockerd111731]: time=\"2023-08-25T14:26:10.791784204+08:00 level=info msg=\"Daemon has completed initialization\"\r\nAug 25 14:26:10 localhost.domain.com systemd[1]: Started Docker Application Container Engine.\r\nAug 25 14:26:10 localhost.domain.com dockerd[111731]: time=\"2023-08-25T14:26:10.812882049+08:00\u201d level=info msg=\"API listen on \/run\/docker.sock\"Aug 25 14:28:39 localhost.domain.com systemd [1]:docker.service: main process exited, code-killed, status=11\/SEGV\r\nAug 25 14:28:39 localhost.domain.com systemd[1]: Unit docker.service entered failed state.\r\nAug 25 14:28:39 localhost. domain.com systemd[1]: docker.service failed.\r\nAug 25 14:28:41 localhost.domain.com systemd[1]\r\ndocker.service holdoff time over, scheduling restart.\r\nAug 25 14:28:41 localhost . domain.com systemd [1]: stopped Docker Application Container Engine\r\nAug 25 14:28:41 localhost.domain.com systemd [11: starting Docker Application container Engine . .\r\nAeg 23 ta:28:41 toca host.dlormazm.com dockera ttt2039g:\u201ctime2-2023-08: 25594:28:41.6267046500s:0o' level=info msg=nstarting up~Aug 25 14:28:42 localhost.domain.com dockerd [112039]:\r\ntime=\"2023-08-25T14:28:42.007187850+08:00\" level=info msg=\" [graphdriver] using prior storage driver: overlay2\"\r\n```\r\n2. Due to the occurrence of a panic, I have downgraded the version of the daemon, but not the CLI.","Thanks!\r\n\r\nHm.. the logs aren't very informative here (other than \"it exited with a SEGFAULT\");\r\n\r\n```\r\nAug 25 14:26:10 localhost.domain.com dockerd[111731]: time=\"2023-08-25T14:26:10.812882049+08:00\u201d level=info msg=\"API listen on \/run\/docker.sock\"\r\nAug 25 14:28:39 localhost.domain.com systemd [1]:docker.service: main process exited, code-killed, status=11\/SEGV\r\nAug 25 14:28:39 localhost.domain.com systemd[1]: Unit docker.service entered failed state.\r\nAug 25 14:28:39 localhost. domain.com systemd[1]: docker.service failed.\r\n```\r\n\r\nWere you able to capture daemon logs in other places? If you're able to test more on that machine with the 24.0 version, you could also put the daemon in \"debug\" mode (`{\"debug\": true}` in the `\/etc\/docker\/daemon.json`; that file may not exist, so you'd have to create it if it doesn't, and it must be valid JSON)\r\n\r\nI do notice `runc version: N\/A` in the output of your 18.09 daemon; do you have a custom version of runc (or other runtime) installed? This _may_ be a red-herring, as we've had case where parsing the version didn't work well, so \"just checking\" here.\r\n\r\nDid you have Docker running successfully on this machine before, or was this a first-time, \"fresh\" install?\r\n\r\nWe do have a \"check-config\" script, which can provide some additional details about your Machine's configuration to check if anything is potentially missing \/ mis-configured; https:\/\/github.com\/moby\/moby\/blob\/master\/contrib\/check-config.sh","> Thanks!\r\n> \r\n> Hm.. the logs aren't very informative here (other than \"it exited with a SEGFAULT\");\r\n> \r\n> ```\r\n> Aug 25 14:26:10 localhost.domain.com dockerd[111731]: time=\"2023-08-25T14:26:10.812882049+08:00\u201d level=info msg=\"API listen on \/run\/docker.sock\"\r\n> Aug 25 14:28:39 localhost.domain.com systemd [1]:docker.service: main process exited, code-killed, status=11\/SEGV\r\n> Aug 25 14:28:39 localhost.domain.com systemd[1]: Unit docker.service entered failed state.\r\n> Aug 25 14:28:39 localhost. domain.com systemd[1]: docker.service failed.\r\n> ```\r\n> \r\n> Were you able to capture daemon logs in other places? If you're able to test more on that machine with the 24.0 version, you could also put the daemon in \"debug\" mode (`{\"debug\": true}` in the `\/etc\/docker\/daemon.json`; that file may not exist, so you'd have to create it if it doesn't, and it must be valid JSON)\r\n> \r\n> I do notice `runc version: N\/A` in the output of your 18.09 daemon; do you have a custom version of runc (or other runtime) installed? This _may_ be a red-herring, as we've had case where parsing the version didn't work well, so \"just checking\" here.\r\n> \r\n> Did you have Docker running successfully on this machine before, or was this a first-time, \"fresh\" install?\r\n> \r\n> We do have a \"check-config\" script, which can provide some additional details about your Machine's configuration to check if anything is potentially missing \/ mis-configured; https:\/\/github.com\/moby\/moby\/blob\/master\/contrib\/check-config.sh\r\n\r\nThank you for your response\uff01\r\n\r\nI used to run without any issues, but after upgrading to version 1.24, I couldn't run it anymore. Even when I reverted back to version 18.09, it still wouldn't work.\r\n\r\n`runc -version`\r\n\r\n```\r\nrunc version 1.1.8\r\ncommit: v1.1.8-0-g82f18fe\r\nspec: 1.0.2-dev\r\ngo: go1.19.11\r\nlibseccomp: 2.3.1\r\n```\r\n\r\n`check-config.sh`\r\n\r\n```\r\nwarning: \/proc\/config.gz does not exist, searching other paths for kernel config ...\r\ninfo: reading kernel config from \/boot\/config-3.10.0-1160.95.1.el7.x86_64 ...\r\n\r\nGenerally Necessary:\r\n- cgroup hierarchy: properly mounted [\/sys\/fs\/cgroup]\r\n- CONFIG_NAMESPACES: enabled\r\n- CONFIG_NET_NS: enabled\r\n- CONFIG_PID_NS: enabled\r\n- CONFIG_IPC_NS: enabled\r\n- CONFIG_UTS_NS: enabled\r\n- CONFIG_CGROUPS: enabled\r\n- CONFIG_CGROUP_CPUACCT: enabled\r\n- CONFIG_CGROUP_DEVICE: enabled\r\n- CONFIG_CGROUP_FREEZER: enabled\r\n- CONFIG_CGROUP_SCHED: enabled\r\n- CONFIG_CPUSETS: enabled\r\n- CONFIG_MEMCG: enabled\r\n- CONFIG_KEYS: enabled\r\n- CONFIG_VETH: enabled (as module)\r\n- CONFIG_BRIDGE: enabled (as module)\r\n- CONFIG_BRIDGE_NETFILTER: enabled (as module)\r\n- CONFIG_IP_NF_FILTER: enabled (as module)\r\n- CONFIG_IP_NF_TARGET_MASQUERADE: enabled (as module)\r\n- CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled (as module)\r\n- CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled (as module)\r\n- CONFIG_NETFILTER_XT_MATCH_IPVS: enabled (as module)\r\n- CONFIG_NETFILTER_XT_MARK: enabled (as module)\r\n- CONFIG_IP_NF_NAT: enabled (as module)\r\n- CONFIG_NF_NAT: enabled (as module)\r\n- CONFIG_POSIX_MQUEUE: enabled\r\n- CONFIG_DEVPTS_MULTIPLE_INSTANCES: enabled\r\n- CONFIG_NF_NAT_IPV4: enabled (as module)\r\n- CONFIG_NF_NAT_NEEDED: enabled\r\n\r\nOptional Features:\r\n- CONFIG_USER_NS: enabled\r\n  (RHEL7\/CentOS7: User namespaces disabled; add 'user_namespace.enable=1' to boot command line)\r\n- CONFIG_SECCOMP: enabled\r\n- CONFIG_SECCOMP_FILTER: enabled\r\n- CONFIG_CGROUP_PIDS: enabled\r\n- CONFIG_MEMCG_SWAP: enabled\r\n- CONFIG_MEMCG_SWAP_ENABLED: enabled\r\n    (cgroup swap accounting is currently enabled)\r\n- CONFIG_MEMCG_KMEM: enabled\r\n- CONFIG_RESOURCE_COUNTERS: missing\r\n- CONFIG_IOSCHED_CFQ: enabled\r\n- CONFIG_CFQ_GROUP_IOSCHED: enabled\r\n- CONFIG_BLK_CGROUP: enabled\r\n- CONFIG_BLK_DEV_THROTTLING: enabled\r\n- CONFIG_CGROUP_PERF: enabled\r\n- CONFIG_CGROUP_HUGETLB: enabled\r\n- CONFIG_NET_CLS_CGROUP: enabled\r\n- CONFIG_NETPRIO_CGROUP: enabled\r\n- CONFIG_CFS_BANDWIDTH: enabled\r\n- CONFIG_FAIR_GROUP_SCHED: enabled\r\n- CONFIG_IP_NF_TARGET_REDIRECT: enabled (as module)\r\n- CONFIG_IP_VS: enabled (as module)\r\n- CONFIG_IP_VS_NFCT: enabled\r\n- CONFIG_IP_VS_PROTO_TCP: enabled\r\n- CONFIG_IP_VS_PROTO_UDP: enabled\r\n- CONFIG_IP_VS_RR: enabled (as module)\r\n- CONFIG_SECURITY_SELINUX: enabled\r\n- CONFIG_SECURITY_APPARMOR: missing\r\n- CONFIG_EXT3_FS: missing\r\n- CONFIG_EXT3_FS_XATTR: missing\r\n- CONFIG_EXT3_FS_POSIX_ACL: missing\r\n- CONFIG_EXT3_FS_SECURITY: missing\r\n    (enable these ext3 configs if you are using ext3 as backing filesystem)\r\n- CONFIG_EXT4_FS: enabled (as module)\r\n- CONFIG_EXT4_FS_POSIX_ACL: enabled\r\n- CONFIG_EXT4_FS_SECURITY: enabled\r\n- Network Drivers:\r\n  - \"overlay\":\r\n    - CONFIG_VXLAN: enabled (as module)\r\n    - CONFIG_BRIDGE_VLAN_FILTERING: enabled\r\n      Optional (for encrypted networks):\r\n      - CONFIG_CRYPTO: enabled\r\n      - CONFIG_CRYPTO_AEAD: enabled\r\n      - CONFIG_CRYPTO_GCM: enabled (as module)\r\n      - CONFIG_CRYPTO_SEQIV: enabled\r\n      - CONFIG_CRYPTO_GHASH: enabled (as module)\r\n      - CONFIG_XFRM: enabled\r\n      - CONFIG_XFRM_USER: enabled\r\n      - CONFIG_XFRM_ALGO: enabled\r\n      - CONFIG_INET_ESP: enabled (as module)\r\n      - CONFIG_NETFILTER_XT_MATCH_BPF: enabled (as module)\r\n      - CONFIG_INET_XFRM_MODE_TRANSPORT: enabled (as module)\r\n  - \"ipvlan\":\r\n    - CONFIG_IPVLAN: missing\r\n  - \"macvlan\":\r\n    - CONFIG_MACVLAN: enabled (as module)\r\n    - CONFIG_DUMMY: enabled (as module)\r\n  - \"ftp,tftp client in container\":\r\n    - CONFIG_NF_NAT_FTP: enabled (as module)\r\n    - CONFIG_NF_CONNTRACK_FTP: enabled (as module)\r\n    - CONFIG_NF_NAT_TFTP: enabled (as module)\r\n    - CONFIG_NF_CONNTRACK_TFTP: enabled (as module)\r\n- Storage Drivers:\r\n  - \"btrfs\":\r\n    - CONFIG_BTRFS_FS: enabled (as module)\r\n    - CONFIG_BTRFS_FS_POSIX_ACL: enabled\r\n  - \"overlay\":\r\n    - CONFIG_OVERLAY_FS: enabled (as module)\r\n  - \"zfs\":\r\n    - \/dev\/zfs: missing\r\n    - zfs command: missing\r\n    - zpool command: missing\r\n\r\nLimits:\r\n- \/proc\/sys\/kernel\/keys\/root_maxkeys: 1000000\r\n```"],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/18.09"]},{"title":"Builder fails when building `FROM scratch` and `ARG` is the second line in Dockerfile","body":"### Description\r\n\r\nBuild is failing with the following error\r\n```\r\nfailed to get destination image \"sha256:d713c98f66880c8411b0d55a019d6aac2f45cfc57a48eae50701610d7e3ecf56\": image with reference sha256:d713c98f66880c8411b0d55a019d6aac2f45cfc57a48eae50701610d7e3ecf56 was found but does not match the specified platform: wanted linux\/arm\/v6, actual: linux\/amd64\r\n```\r\n\r\n### Reproduce\r\n\r\nGiven a Dockerfile\r\n\r\n```\r\nFROM scratch\r\n\r\nARG myArg\r\nADD files-${myArg}.tar.gz \/\r\n```\r\n\r\nStart a build with `docker build --platform=linux\/arm\/v6 --build-arg myArg=some -t myImg .`\r\n\r\nYeilds following output:\r\n```\r\ndocker build --platform=linux\/arm\/v6 --build-arg myArg=some -t myImg .\r\nSending build context to Docker daemon  29.67MB\r\nStep 1\/13 : FROM scratch\r\n ---> \r\nStep 2\/13 : ARG myArg\r\n ---> Running in 06d61fdbc7b5\r\nRemoving intermediate container 06d61fdbc7b5\r\n ---> d713c98f6688\r\nStep 3\/13 : ADD files-${myArg}.tar.gz \/\r\nfailed to get destination image \"sha256:d713c98f66880c8411b0d55a019d6aac2f45cfc57a48eae50701610d7e3ecf56\": image with reference sha256:d713c98f66880c8411b0d55a019d6aac2f45cfc57a48eae50701610d7e3ecf56 was found but does not match the specified platform: wanted linux\/arm\/v6, actual: linux\/amd64\r\n```\r\n\r\nAdding `--no-chache` does not change anything.\r\n\r\nNow, let's add some command before `ARG`\r\n\r\nDockerfile\r\n\r\n```\r\nFROM scratch\r\n\r\nADD qwe \/\r\n\r\nARG myArg\r\nADD files-${myArg}.tar.gz \/\r\n```\r\n\r\n```\r\ndocker build --platform=linux\/arm\/v6 --build-arg myArg=some -t myImg .\r\nSending build context to Docker daemon  29.67MB\r\nStep 1\/4 : FROM scratch\r\n ---> \r\nStep 2\/4 : ADD qwe \/\r\n ---> 4ed4fd43b443\r\nStep 3\/4 : ARG myArg\r\n ---> [Warning] The requested image's platform (linux\/arm\/v6) does not match the detected host platform (linux\/amd64) and no specific platform was requested\r\n ---> Running in cc1b822df5aa\r\nRemoving intermediate container cc1b822df5aa\r\n ---> 0b548c74b5cc\r\nStep 4\/4 : ADD files-${myArg}.tar.gz \/\r\n ---> 891efb6b6913\r\nSuccessfully built 891efb6b6913\r\nSuccessfully tagged myImg\r\n```\r\n\r\n### Expected behavior\r\n\r\n`docker build` should not fail\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           20.10.5+dfsg1\r\n API version:       1.41\r\n Go version:        go1.15.15\r\n Git commit:        55c4c88\r\n Built:             Mon May 30 18:34:49 2022\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.5+dfsg1\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.15.15\r\n  Git commit:       363e9a8\r\n  Built:            Mon May 30 18:34:49 2022\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.4.13~ds1\r\n  GitCommit:        1.4.13~ds1-1~deb11u4\r\n runc:\r\n  Version:          1.0.0~rc93+ds1\r\n  GitCommit:        1.0.0~rc93+ds1-5+deb11u2\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 2\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 54\r\n Server Version: 20.10.5+dfsg1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1.4.13~ds1-1~deb11u4\r\n runc version: 1.0.0~rc93+ds1-5+deb11u2\r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.10.0-23-amd64\r\n Operating System: Debian GNU\/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 7.761GiB\r\n Name: debian\r\n ID: QDFR:I4CX:AVE2:NJ5Q:LGOQ:VQCA:JGY2:7VVC:HJIW:UOJL:224W:LVV7\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: Support for cgroup v2 is experimental\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nSame issue is being described here https:\/\/github.com\/docker\/docker-py\/issues\/2990","comments":["Same issue with the following Dockerfile\r\n\r\n```\r\nFROM scratch\r\n\r\nUSER 0:0\r\nADD some-files.tar.gz \/\r\n```\r\n\r\n```\r\ndocker build --platform=linux\/arm\/v6 --no-cache -t myImg .\r\nSending build context to Docker daemon  29.67MB\r\nStep 1\/3 : FROM scratch\r\n ---> \r\nStep 2\/3 : USER 0:0\r\n ---> Running in 180ffa39adc5\r\nRemoving intermediate container 180ffa39adc5\r\n ---> f3cb51d3f4c3\r\nStep 3\/3 : ADD some-files.tar.gz \/\r\nfailed to get destination image \"sha256:f3cb51d3f4c3a7ffb1b3cacdc9c26da5b94ec002ef9871d528c31aa5e82a555b\": image with reference sha256:f3cb51d3f4c3a7ffb1b3cacdc9c26da5b94ec002ef9871d528c31aa5e82a555b was found but does not match the specified platform: wanted linux\/arm\/v6, actual: linux\/amd64\r\n```\r\n\r\nSo, it seems to me, that layers, that are not engaged in file storage operations (i.e. `ADD`) do not have `platform` properly set. Platform information provided from cli is disregarded for those layers.","This looks to be a limitation of the legacy builder. The issue here is likely that the first steps are config-only changes, which may not result in the platform being updated (and the \"scratch\" image defaults to `linux\/amd64` for legacy reasons).\r\n\r\nNote that the legacy build has been deprecated in favour of BuildKit, which is the default on current versions of Docker (I see you're using docker 20.10, which reached EOL).\r\n\r\nOn older versions, you can enable buildkit by setting the `DOCKER_BUILDKIT=1` option when running your build;\r\n\r\nTaking this Dockerfile:\r\n\r\n```dockerfile\r\n# syntax=docker\/dockerfile:1\r\n\r\nFROM scratch\r\nARG myArg\r\nADD Dockerfile \/\r\n```\r\n\r\n\r\nWith the classic builder (I use `DOCKER_BUILDKIT=0`, as I have BuildKit enabled by default) things fail indeed;\r\n\r\n```bash\r\nDOCKER_BUILDKIT=0 docker build --no-cache --platform=linux\/arm64 .\r\nSending build context to Docker daemon  2.048kB\r\nStep 1\/3 : FROM scratch\r\n --->\r\nStep 2\/3 : ARG myArg\r\n ---> Running in 11678795895d\r\nRemoving intermediate container 11678795895d\r\n ---> 147122c61635\r\nStep 3\/3 : ADD Dockerfile \/\r\nfailed to get destination image \"sha256:ac4f9b3e980c44423dc912817b84f650070642f898edee766debedbb4e5ad77f\": image with reference sha256:ac4f9b3e980c44423dc912817b84f650070642f898edee766debedbb4e5ad77f was found but does not match the specified platform: wanted linux\/arm64, actual: linux\/amd64\r\n```\r\n\r\nWith BuildKit enabled, this works;\r\n\r\n```bash\r\nDOCKER_BUILDKIT=1 docker build --no-cache --platform=linux\/arm64 .\r\n[+] Building 1.0s (9\/9) FINISHED\r\n => [internal] load build definition from Dockerfile                                                                            0.1s\r\n => => transferring dockerfile: 31B                                                                                             0.0s\r\n => [internal] load .dockerignore                                                                                               0.1s\r\n => => transferring context: 2B                                                                                                 0.0s\r\n => resolve image config for docker.io\/docker\/dockerfile:1                                                                      0.4s\r\n => CACHED docker-image:\/\/docker.io\/docker\/dockerfile:1@sha256:ac85f380a63b13dfcefa89046420e1781752bab202122f8f50032edf31be0021 0.0s\r\n => [internal] load build definition from Dockerfile                                                                            0.0s\r\n => [internal] load .dockerignore                                                                                               0.0s\r\n => [internal] load build context                                                                                               0.0s\r\n => => transferring context: 31B                                                                                                0.0s\r\n => [1\/1] ADD Dockerfile \/                                                                                                      0.0s\r\n => exporting to image                                                                                                          0.0s\r\n => => exporting layers                                                                                                         0.0s\r\n => => writing image sha256:d11d6c969734a35311f2d3b96f3296760d5bd6aa1b9fde14baad8ca2f59ba96d                                    0.0s\r\n```\r\n\r\nWe should have a look if this can be fixed, but with the legacy builder being deprecated, we need to look if this is an easy fix or would require larger changes (due to limitations of the legacy builder).\r\n\r\n","Small note; while testing this on docker 24.0, the error-message was slightly more confusing (which may be something to look into if we're looking at fixing this) (output wrapped for readability);\r\n\r\n```bash\r\nfailed to get destination image \"sha256:147122c616355cce5378c9543ad9e176e4b2d331280f172b9d1d7b082e18ddbd\":\r\nlease \"moby-image-sha256:147122c616355cce5378c9543ad9e176e4b2d331280f172b9d1d7b082e18ddbd\": not found\r\n```\r\n","\r\n```bash\r\nfailed to get destination image \"sha256:147122c616355cce5378c9543ad9e176e4b2d331280f172b9d1d7b082e18ddbd\":\r\nlease \"moby-image-sha256:147122c616355cce5378c9543ad9e176e4b2d331280f172b9d1d7b082e18ddbd\": not found\r\n```\r\nis fixed by https:\/\/github.com\/moby\/moby\/pull\/46293.","Ah! Was wondering if it was the same; thanks for checking!","Thank you for the prompt response and detailed clarification, @thaJeztah !\r\n\r\nIndeed, It does not fail with buildkit enabled on docker 20.10."],"labels":["area\/builder","kind\/bug","version\/20.10","area\/builder\/classic-builder"]},{"title":"Starting and stopping containers leads to OOM on ppc64le\/ppc64","body":"### Description\n\nRunning short lived containers in docker on Ubuntu 22.04 leads to OOM conditions. I also observe this on debian unstable on ppc64. These machines are dedicate CI machines which run jobs exclusively within transient containers.\r\n\r\nThe symptoms are increasing number of `nr_dying_descendants` in \/sys\/fs\/cgroup\/cgroup.stat, and High Percpu memory usage as reported by \/proc\/meminfo. The amount of memory is substantial, and cannot be attribute to any process when the machine is idle.\r\n\r\nEventually, CI tasks start failing randomly due to OOM.\n\n### Reproduce\n\nDockerfile:\r\n```\r\nFROM ubuntu:22.04\r\n\r\nRUN apt-get update && \\\r\n    apt-get install --yes \\\r\n          gcc gdb curl strace \\\r\n          ca-certificates netbase \\\r\n          procps lsof psmisc \\\r\n          openssh-server \\\r\n          patch bzip2 netcat && \\\r\n    wget https:\/\/go.dev\/dl\/go1.20.5.linux-ppc64le.tar.gz && \\\r\n    tar xf go1.20.5.linux-ppc64le.tar.gz\r\n\r\nRUN \/go\/bin\/go test syscall\r\n\r\nCMD [\"bash\"]\r\n```\r\n1. `Docker build -t local\/test .` \r\n2. `while [ 1 ] ; do sudo docker run -it --rm local\/test \/go\/bin\/go test syscall -test.short; cat \/sys\/fs\/cgroup\/cgroup.stat; grep Percpu \/proc\/meminfo ; done`\r\n3.  Watch Percpu and nr_dying_descendants values increase over time.\n\n### Expected behavior\n\nThe values reported should stay relatively stable over time.\n\n### docker version\n\n```bash\nClient:\r\n Version:           20.10.21\r\n API version:       1.41\r\n Go version:        go1.18.1\r\n Git commit:        20.10.21-0ubuntu1~22.04.3\r\n Built:             Thu Apr 27 05:58:14 2023\r\n OS\/Arch:           linux\/ppc64le\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.21\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.18.1\r\n  Git commit:       20.10.21-0ubuntu1~22.04.3\r\n  Built:            Thu Apr 27 05:37:25 2023\r\n  OS\/Arch:          linux\/ppc64le\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.12-0ubuntu1~22.04.3\r\n  GitCommit:        \r\n runc:\r\n  Version:          1.1.4-0ubuntu1~22.04.3\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 4\r\n Server Version: 20.10.21\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: \r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 5.15.0-48-generic\r\n Operating System: Ubuntu 22.04.2 LTS\r\n OSType: linux\r\n Architecture: ppc64le\r\n CPUs: 96\r\n Total Memory: 31.78GiB\r\n Name: ltcd97-lp1\r\n ID: FOAL:4HJM:4WJQ:5T7B:PQ74:6774:DNTC:RN47:T65T:WVL7:PWAW:GYAQ\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nThe version information was taken from the reproducer machine.\r\n\r\nOn the affected machine (ubuntu 22.04\/ppc64le). The values look like:\r\n```\r\nLinux go-le-p10-1 5.19.0-46-generic #47~22.04.1-Ubuntu SMP Wed Jun 21 15:35:52 UTC 2023 ppc64le ppc64le ppc64le GNU\/Linux\r\nnr_descendants 71\r\nnr_dying_descendants 4213\r\nPercpu:          6795264 kB\r\n```\r\n\r\nOn the affected ppc64 machine:\r\n```\r\nLinux go-be-p10-1 6.1.0-7-powerpc64 #1 SMP Debian 6.1.20-2 (2023-04-08) ppc64 GNU\/Linux\r\nnr_descendants 48\r\nnr_dying_descendants 2658\r\nPercpu:          4186112 kB\r\n```\r\n","comments":["I've not yet had time to try to reproduce, but based on the difference in Kernel version, this could be related to; \r\n\r\n- https:\/\/github.com\/containerd\/containerd\/pull\/7566 \r\n- https:\/\/github.com\/moby\/moby\/issues\/38814\r\n\r\n\r\nDo you see the same issue if you start the container with ulimits set? e.g.  `--ulimit \"nofile=1024:1048576\"` ?\r\n","I still observe `Percpu` memory usage and `nr_dying_descendants` growing when using the `--ulimit \"nofile=1024:1048576\"` option. "],"labels":["status\/0-triage","kind\/bug","version\/20.10"]},{"title":"Invalid websocket frame error, at endpoint \/containers\/<name_or_id>\/attach\/ws ( empty response )  ","body":"### Description\r\n\r\nGetting the websocket frame error, while testing out API:\r\n https:\/\/docs.docker.com\/engine\/api\/v1.43\/#tag\/Container\/operation\/ContainerAttachWebsocket\r\n\r\nScreenshot below shows result of query parameter variations, still resulting to error\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/33404820\/fadfc83b-a314-4b0e-8aff-9985ec2f5faa)\r\n### Reproduce\r\n\r\na. on browser:\r\n 1 run javascript\r\n ```\r\n url = 'ws:\/\/localhost:2375\/containers\/xtermjs-app-1\/attach\/ws?stream=1&stdin=1&stdout=1&stderr=1&logs=1';\r\n new WebSocket(url);\r\n ```\r\n2 see error: `VM1593:1 WebSocket connection to 'ws:\/\/localhost:2375\/containers\/xtermjs-app-1\/attach\/ws?stream=true&stdin=true&stdout=true&stderr=true&logs=true' failed: Invalid frame header`\r\n\r\nb on terminal:\r\n 1 run command\r\n ``` npx wscat -c  ws:\/\/localhost:2375\/v1.18\/containers\/xtermjs-app-1\/attach\/ws?stream=true%26stdout=true%26stdin=true%26logs=true```\r\n2. see error: `error: Invalid WebSocket frame: RSV1 must be clear`\r\n\r\n### Expected behavior\r\n\r\nExpected to not get any error during `new Websocket(url)` \r\n`<WebSocket>.readyState is 0` after, thus `<WebSocket>.send(<string>)` won't work and will show error `WebSocket is already in CLOSING or CLOSED state.`\r\n\r\n### docker version\r\n\r\n```bash\r\ndocker version\r\nClient:\r\n Cloud integration: v1.0.35-desktop+001\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:36:24 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Desktop 4.22.0 (117440)\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:45 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\ndocker info\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.1\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scan.exe\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe\r\n\r\nServer:\r\n Containers: 11\r\n  Running: 8\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 13\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n Kernel Version: 5.10.16.3-microsoft-standard-WSL2\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 13.59GiB\r\n Name: docker-desktop\r\n ID: 596d2bf9-8a7a-45e0-8014-6232b184d6d2\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: true\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No blkio throttle.read_bps_device support\r\nWARNING: No blkio throttle.write_bps_device support\r\nWARNING: No blkio throttle.read_iops_device support\r\nWARNING: No blkio throttle.write_iops_device support\r\nWARNING: daemon is not using the default seccomp profile\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nRunning Window 11 Single Language with WSL2\r\n\r\n","comments":[],"labels":["area\/api","status\/0-triage","kind\/bug","version\/24.0"]},{"title":"Function DeleteConntrackEntriesByPort contains an error","body":"### Description\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/libnetwork\/iptables\/conntrack.go#L57\r\nThe DeleteConntrackEntriesByPort function deletes all entries in the conntrack table with the specified port without taking into account the IP address. Thus, if there are several IP addresses on the server, records of connections to other containers with the same port but different IP addresses are deleted.\r\n\r\n### Reproduce\r\n\r\n1. docker run -p 192.168.0.1:443:443\/udp -d --name alpine1 _container_with_UDP_service_\r\n2. docker run -p 192.168.0.2:443:443\/udp -d --name alpine2 _container_with_UDP_service_\r\n3. Connect to 192.168.0.1:443 and 192.168.0.2:443\r\n4. Check conntrack table entries\r\n5. Stop and start either container\r\n6. Check conntrack table entries again, will lose connection entries for both containers\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:36:32 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:32 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.20\r\n  GitCommit:        2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc:\r\n  Version:          1.1.5\r\n  GitCommit:        v1.1.5-0-gf19387a\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 5\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 6\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc version: v1.1.5-0-gf19387a\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-372.26.1.0.1.el8_6.x86_64\r\n Operating System: Oracle Linux Server 8.6\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 48\r\n Total Memory: 31.16GiB\r\n Name: ---\r\n ID: 4fceb588-d5c1-49bb-95f5-7958b39f7dcd\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["@Amoled Indeed the code should filter on IP address(es), but I'm wondering if there's really any adverse consequences? I mean, UDP is connectionless so removing too much conntrack entries should have no impact :thinking: Can you describe how you stumbled on that?"],"labels":["status\/more-info-needed","kind\/bug","area\/networking","version\/24.0"]},{"title":"multierror: make it consistent when there's a single error","body":"Related to:\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/46188\r\n\r\n**- What I did**\r\n\r\nIf a single error is passed to `multierror.Join`, `joinError.Error()` returns the error as is. However, due to the multiline formatting applied otherwise, it's better to add a line break between the preamble and the multierror string when wrapping it, eg.:\r\n\r\n```golang\r\nfmt.Errorf(\"invalid network config:\\n%w\", multierror.Join(errs...))\r\n```\r\n\r\nThis commit removes this special case to make the format consistent no matter how many errors are joined:\r\n\r\n```\r\n# Before\r\ninvalid network config:\r\nsome error\r\n\r\n# After\r\ninvalid network config:\r\n* some error\r\n```","comments":["Bit on the fence on this one; every other error we return would be a single line;\r\n\r\n```\r\nerror while time-traveling: the flux capacitor was not found\r\n```\r\n\r\nMulti-errors are good if there's multiple reasons (in which case we can list them),  but I feel like \"alway wrapping these\" _because it happens to be a multi-error_ may be presenting the underlying implementation  to the user (we should multiple lines because it's a multi-error, not because we need to show multiple errors) \ud83e\udd14","Yeah, I agree. The best way to solve that is to change the signature of `multierror.Join` to something like: `Join(preamble string, errors ...error)`. Then, `Join` can decide whether a new line is needed based on how many non-nil errors were passed. But that would make `multierror.Join` incompatible with `errors.Join`, whereas the initial intent was to make it a drop-in replacement.\r\n\r\nAlso, this new signature would present its own challenge: what do we do with the \"preamble\"? Is it something that should be returned by `Unwrap()`? I guess no, but then we need a dedicated `Preamble() string` to expose it to those who want to walk the error chain (eg. for the error tree PR), and it makes our `joinError` depart from stdlib's `joinError` even further :grimacing: \r\n\r\nNow that being said, I now realize that the \"drop-in replacement\" promise is broken by the need to add a line break in the wrapping error to have clean formatting anyway :grimacing: ",">  The best way to solve that is to change the signature of multierror.Join to something like: `Join(preamble string, errors ...error)`.\r\n\r\nI think `fmt.Errorf` and `errors.Wrap` are your \"preamble\";\r\n\r\n\r\n```go\r\nfunc validate(f something) error {\r\n    if inValidFoo {\r\n        err = errors.Join(err, fmt.Errorf(\"invalid foo\"))\r\n    }\r\n    if inValidBar {\r\n        err = errors.Join(err, fmt.Errorf(\"invalid bar\"))\r\n    }\r\n    return err\r\n}\r\n\r\n\r\nif err := validate(foo); err != nil {\r\n    return fmt.Errorf(\"validation failed: %w\", err)\r\n}\r\n\r\n\/\/ or: errors.Wrap();\r\nif err := validate(foo); err != nil {\r\n    return errors.Wrap(err, \"validation failed\")\r\n}\r\n```\r\n\r\nIn the above; `\"validation failed: %w\"` should be read as `\"validation failed: <BECAUSE OF ONE OR MORE ERRORS>\"`, so could either be;\r\n\r\nFor a single validation failure:\r\n\r\n```console\r\nvalidation failed: invalid foo\r\n```\r\n\r\nFor a multiple validation failures:\r\n\r\n```console\r\nvalidation failed:\r\n  * invalid foo\r\n  * invalid bar\r\n```\r\n\r\nAnd the same should still apply for nested \"errors.Wrap\" or \"fmt.Errorf\"'s;\r\n\r\n```console\r\nvalidation failed:\r\n  * invalid foo: something wasn't right\r\n  * invalid bar:\r\n    * something wasn't right\r\n    * something else wasn't right\r\n```\r\n\r\nI should probably add that the last one (if `invalid bar` didn't happen) would be a regular \"wrapped\" error, because in that case none of the wrapped errors would effectively be \"multi-errors\" (all of them would be \"single-multi-errors\");\r\n\r\n```console\r\nvalidation failed: invalid foo: something wasn't right\r\n```\r\n\r\nBut if only the `invalid bar` happened, it would be something like;\r\n\r\n```console\r\nvalidation failed: invalid bar:\r\n  * something wasn't right\r\n  * something else wasn't right\r\n```\r\n\r\n\r\n\r\n\r\n","Mh I think we need a special `WrapMultiErrors` that decides whether a new line is required. It'd basically do:\r\n\r\n```golang\r\nfunc WrapMultiErrors(preamble string, errors ...error) {\r\n    err := Join(errors...)\r\n    if len(err.errors) == nil {\r\n        return nil\r\n    }\r\n    if len(err.errors) == 1 {\r\n        return fmt.Errorf(\"%s: %w\", preamble, err)\r\n    }\r\n    return fmt.Errorf(\"%s:\\n%w\", preamble, err)\r\n}\r\n```","I was thinking (but didn't try yet) that when unwrapping we can detect; we can match the interface (multi-error or not multi-error), and if it's a multi-error with only one entry, we don't need to include a newline"],"labels":["status\/2-code-review","kind\/bugfix"]},{"title":"fix \"make generated-files\" not generating and validating volume\/drivers\/volumeDriverProxy, daemon\/logger\/logPluginProxy","body":"- follow-up to https:\/\/github.com\/moby\/moby\/pull\/44697\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/13835\r\n- relates to https:\/\/github.com\/moby\/moby\/pull\/35441\r\n- addresses \/ fixes https:\/\/github.com\/moby\/moby\/issues\/40176\r\n\r\n\r\nWhile working on some changes, I noticed that a file I was editing was a generated file; https:\/\/github.com\/moby\/moby\/blob\/cf15460a3b9d23334fcc063ff1396844b16fa11d\/volume\/drivers\/proxy.go#L1\r\n\r\nHowever, the header didn't have the [correct format for generated files](https:\/\/pkg.go.dev\/cmd\/go@go1.21.0#hdr-Generate_Go_files_by_processing_source) so didn't warn me about this, so I thought: \"let me update the template to have the right comment\". To verify my changes, I ran `make generated-files`, which didn't complain, but did not update the generated file. When I tried to manually `go generate` the file, the file _did_ change, but also generated _very_ different code.\r\n\r\nSo, looking at the file's history (perhaps I used the wrong command for generating the file?), I found various manual changes were made to this file in the past, but CI never complained about this :thinking:\r\n\r\nIn short: this PR is the result of one of those \"down the rabbit-hole\" cases:\r\n\r\n- Update the template to actually generate the expected code.\r\n- Update the template to have a correctly formatted \"this is a generated file\" comment.\r\n- Fix the `make generated-files` target to also include this file (fun fact: it **was** generating the file, but not copying it out, and because of that, was not validating it for changes).\r\n- And finally: update the template with some of the changes I was about to make manually, and to make the code slightly more iodiomatic.\r\n\r\n. Looking at the history of the file, I then noticed that the file was manually updated a few times in the past.\r\n\r\n### pkg\/plugins: pluginrpc-gen: fix go generate for volume driver\r\n\r\nThe volume driver plugin generated code was manually updated in\r\nb15f8d2d4f054a87052a7065c50441f7e8479fa9, so generating the code\r\nwould no longer produce the same output.\r\n\r\nThis patch updates the generator code to allow regenerating the volume\r\ndriver plugin.\r\n\r\n- update buildImports to allow additional imports to be passed\r\n- add a conditional block to the template for volumeDriver\r\n- add a conditional block to set the right timeout, depending on what\r\n  method we're generating code for.\r\n- slightly adjust the template to make the output format match the\r\n  existing code.\r\n\r\nSome of this is a bit quirky, so we should look for a more generic option,\r\nbut I tried to preserve the ability to generate this code, so that it\r\ncan evolve if things change.\r\n\r\nTo verify the generated code:\r\n\r\n    GO111MODULE=off go install .\/pkg\/plugins\/pluginrpc-gen\r\n    GO111MODULE=off go generate .\/volume\/drivers\r\n\r\n### hack: generate-files.Dockerfile: validate volume\/drivers\r\n\r\nMaybe this was left out on purpose in 7daaa001205c259be30b479aeea28a11ec775ef5,\r\nbecause the `pluginrpc-gen` generator was broken for this code, but now\r\nthat the generator works correctly, we can enable generating this package,\r\nand validate it in CI.\r\n\r\nThe Dockerfile was actually building `pluginrpc-gen` and generating the\r\nplugin (`volume\/drivers`), but was not copying the result, and as a result\r\nalso not validating the results.\r\n\r\n### hack: generate-files.Dockerfile: simplify pluginrpc-gen install\r\n\r\npluginrpc-gen is also \"go install\"-able (if there's a go.mod), so let's\r\nuse it :)\r\n\r\n### pkg\/plugins: pluginrpc-gen: fix \"generated code\" header to be detected\r\n\r\nUpdate the comment to match the format defined by [Golang][1], otherwise\r\nit won't be detected by IDEs and linters.\r\n\r\n[1]: https:\/\/pkg.go.dev\/cmd\/go@go1.21.0#hdr-Generate_Go_files_by_processing_source\r\n\r\n\r\nWith this change, my IDE properly picks up that the file is generated:\r\n\r\n<img width=\"699\" alt=\"Screenshot 2023-08-19 at 12 01 36\" src=\"https:\/\/github.com\/moby\/moby\/assets\/1804568\/04e0f07b-8745-4b00-804a-45fe7c0d0b3e\">\r\n\r\n\r\n### pkg\/plugins: pluginrpc-gen: use struct-literals in generated code\r\n\r\nMakes the generated code slightly nicer :)\r\n\r\n\r\n### pkg\/plugins: pluginrpc-gen: use early return on error\r\n\r\nDon't assign output variables if an error occurred.\r\n\r\n### pkg\/plugins: pluginrpc-gen: assert interface\r\n\r\nMake sure the generated code matches the interface, and produce a\r\ncompile-time error otherwise.\r\n\r\n### ~pkg\/plugins: pluginrpc-gen: fix generating logger plugin~\r\n\r\n~Make the generator work for generating daemon\/logger\/logPluginProxy.~\r\n\r\n(removed this commit)\r\n\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![down-the-rabbit-hole](https:\/\/github.com\/moby\/moby\/assets\/1804568\/b40e7f46-dcd4-4255-bc34-f3d15cbb8c3f)\r\n","comments":["@cpuguy83 I dropped the last commit (for the logging-driver); let me know if the remaining bits are still problematic to keep"],"labels":["area\/logging","status\/2-code-review","area\/testing","area\/plugins","area\/volumes","kind\/bugfix"]},{"title":"Pulling an image with an unmatching host architecture doesn't always fail","body":"### Description\n\nDepending on the manifest type that a given ref returns, the `no matching manifest for ... in the manifest list entries` error may not be thrown.\n\n### Reproduce\n\n```bash\r\n$ docker pull percona:ps-8.0\r\nps-8.0: Pulling from library\/percona\r\nno matching manifest for linux\/arm64\/v8 in the manifest list entries\r\n\r\n$ docker pull percona\/percona-server:8.0\r\n8.0: Pulling from percona\/percona-server\r\n[...]\r\n```\n\n### Expected behavior\n\nFor the error to be thrown no matter if the given manifest type is a manifest list (where the expected behavior is already done) or not.\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35-desktop+001\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:32:30 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.22.0 (117440)\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:38 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2-desktop.1\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     \/Users\/gamersriseup\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 1\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 5.15.49-linuxkit-pr\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.668GiB\r\n Name: docker-desktop\r\n ID: <redacted>\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["area\/distribution","status\/0-triage","kind\/enhancement","area\/ux"]},{"title":"\"error while deleting neighbor entry\" and \"fatal task error\" on swarm manager node","body":"### Description\n\nI am seeing the following warnings and task errors in docker journalctl on swarm manager node and are unsure what exactly is wrong and what to do about it. The stream is constant, every few seconds. Cluster seems to be operating fine otherwise but I'd like to get rid of these.\r\n\r\n\r\n> Aug 16 13:26:35 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:35.186662370+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:26:35 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:35.366585939+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:26:42 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:42.366658513+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:26:42 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:42.566841260+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:26:59 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:59.167632397+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:26:59 master1.local dockerd[4019880]: time=\"2023-08-16T13:26:59.168702406+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:27:10 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:10.966982443+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:27:11 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:11.064863397+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:27:17 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:17.062481625+02:00\" level=info msg=\"ignoring event\" container=db28b5a550a8603e38db14dfaaf80c08d4ea57694137b6071385513ed2664ab3 module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\n> Aug 16 13:27:17 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:17.521924733+02:00\" level=error msg=\"fatal task error\" error=\"task: non-zero exit (2)\" module=node\/agent\/taskmanager node.id=7ubjso4b7511pqheoopz28ptf service.id=c8kwnemqbjix0tbsm2pztze7f task.id=ve84el8kc24fi6t6v3kk0tmx0\r\n> Aug 16 13:27:38 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:38.493516105+02:00\" level=info msg=\"ignoring event\" container=f1d031104a214ecfc8c8d528550707b96836ab705a404e60693a4a8b45cfbace module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\n> Aug 16 13:27:38 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:38.933000139+02:00\" level=error msg=\"fatal task error\" error=\"task: non-zero exit (2)\" module=node\/agent\/taskmanager node.id=7ubjso4b7511pqheoopz28ptf service.id=c8kwnemqbjix0tbsm2pztze7f task.id=6twfxkcctlpb8hcjwagbop9cb\r\n> Aug 16 13:27:45 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:45.571716860+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\r\n> Aug 16 13:27:45 master1.local dockerd[4019880]: time=\"2023-08-16T13:27:45.766685661+02:00\" level=warning msg=\"error while deleting neighbor entry\" error=\"no such file or directory\"\n\n### Reproduce\n\nI don't have a repro.\n\n### Expected behavior\n\nClean log without warnings and errors\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:36:32 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:32 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.12\r\n  GitCommit:        a05d175400b1145e5e6a735a6710579d181e7fb0\r\n runc:\r\n  Version:          1.1.4\r\n  GitCommit:        v1.1.4-0-g5fd4c4d\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.4\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.12.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.21.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 17\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: xfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 7ubjso4b7511pqheoopz28ptf\r\n  Is Manager: true\r\n  ClusterID: x0mc7bt7mkhe9usigjo0fgswj\r\n  Managers: 3\r\n  Nodes: 5\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4777\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: <redacted1>\r\n  Manager Addresses:\r\n   <redacted1>:2377\r\n   <redacted2>:2377\r\n   <redacted3>:2377\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: a05d175400b1145e5e6a735a6710579d181e7fb0\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.4.17-2136.313.6.el8uek.x86_64\r\n Operating System: Oracle Linux Server 8.7\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 1.807GiB\r\n Name: master1.local\r\n ID: DDME:F4LH:ALM3:CHCI:DMHD:IIEN:VXPJ:B3K4:6HMD:ZMDM:3LWU:KMRM\r\n Docker Root Dir: \/srv\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n5 node swarm, 3 managers, 2 workers","comments":["same here\uff0cworker node version 24.0.5\r\nmanage node version\uff1a20.10.8 (Leader), 20.10.8 (Reachable), 24.0.5 (Reachable)\r\nanother two workers (node version 24.0.1 and 24.0.2) have the same issue","I am also seing this error on docker 24.06 - 46 nodes, 3 managers."],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/swarm","version\/24.0"]},{"title":"libnetwork\/iptables: refactor firewalld code","body":"- relates to https:\/\/github.com\/moby\/moby\/pull\/46190 https:\/\/github.com\/moby\/moby\/pull\/46190#discussion_r1291052416\r\n- [x] depends on https:\/\/github.com\/moby\/moby\/pull\/46245\r\n- [x] depends on https:\/\/github.com\/moby\/moby\/pull\/46246\r\n- [x] depends on https:\/\/github.com\/moby\/moby\/pull\/46247\r\n- [x] depends on https:\/\/github.com\/moby\/moby\/pull\/46304\r\n\r\n### libnetwork\/iptables: un-export Conn\r\n\r\nUn-export Conn, as it's only used internally. Rename it to firewalldConnection,\r\nto make clearer what it's used for, and rename the sysconn field.\r\n\r\nlibnetwork\/iptables: firewalldInit: move error message to newConnection\r\n\r\nMove generating the error-message to where we know what the cause is,\r\ninstead of assuming we're unable to make a D-Bus connection.\r\n\r\n### libnetwork\/iptables: firewalldInit: move error message to newConnection\r\n\r\nMove generating the error-message to where we know what the cause is,\r\ninstead of assuming we're unable to make a D-Bus connection.\r\n\r\n### libnetwork\/iptables: make newConnection() slightly more idiomatic\r\n\r\nDon't consider a connection successful if firewalld is not running. If\r\nit's not running, return a suitable error message. We only use the D-Bus\r\nconnection to communicate with firewalld, so no need to return a connection\r\nif it's not running.\r\n\r\n### libnetwork\/iptables: make signalHandler a method\r\n\r\nMake it a method on the firewalldConnection, which felt more natural\r\nthan being implemented as a standalone function that depended on the\r\npackage-level variable.\r\n\r\n### libnetwork\/iptables: move setting up signals into handleSignals\r\n\r\nnewConnection was setting up these listeners, but in the event we\r\nfailed to check if firewalld was running, we would error-out, and\r\nclose the connection.\r\n\r\nLooking at b052827e025267336f0d426df44ec536745821f8, all of this code\r\nis directly related to handling signals, so let's combine this into the\r\nhandleSignals method.\r\n\r\n\r\n\r\n### libnetwork\/iptables: make setupDockerZone a method\r\n\r\nMake it a method on the firewalldConnection, which felt more natural\r\nthan being implemented as a standalone function that depended on the\r\npackage-level variable.\r\n\r\nAlso improve some error-messages to include context about the failure.\r\n\r\n\r\n### libnetwork\/iptables: make firewalldInit more atomic \r\n\r\nfirewalldInit was returning an error if we failed to set up the docker\r\nzone, but did not close the D-Bus connection. Given that we consider\r\nfirewalld to \"not be usable\" in case of an error, let's also close\r\nthe connection;\r\n\r\n    unable to initialize firewalld; using raw iptables instead\r\n\r\nAnd return the connection on success, instead of implicitly setting the\r\npackage-level `firewalld` variable.\r\n\r\n\r\n### libnetwork\/iptables: remove firewalldRunning package-var\r\n\r\nMake the firewalld status a property of firewalldConnection, so that all\r\nstatus is captured in the `firewalld` instance.\r\n\r\nfirewalldInit() already checks if firewalld is running when initializing,\r\nin which case the `firewalld` variable would be nil.\r\n\r\nWhile refactoring, also use an `atomic.Bool`; the running-status can be\r\nupdated as part of `startSignalHandler()`, which means that there's a\r\npotential concurrency-issue with other code accessing the firewalld\r\nstatus.\r\n\r\n### libnetwork\/iptables: implement addInterface\/delInterface methods\r\n\r\nMove the implementation from AddInterfaceFirewalld and DelInterfaceFirewalld\r\nto a method on firewalldConnection, but keeping the exported utility-functions.\r\n\r\n### libnetwork\/iptables: register callbacks on firewalldConnection\r\n\r\nRegister callbacks on the firewalldConnection instance instead of a global\r\nvariable. These callbacks should only be needed if firewalld is initialized\r\nand running, so associate them with the firewalldConnection that we created.\r\n\r\n\r\n### libnetwork\/iptables: Passthrough: re-use IPVersion type\r\n\r\nUse the existing IPVersion type to switch between ipv4 and ipv6. The IPV\r\ntype was only used for this function, and currently required callers to\r\nmap a IPVersion to IPV.\r\n\r\n### libnetwork\/iptables: implement passthrough as method\r\n\r\nMove the Passthrough implementation to a method on firewalldConnection,\r\nand add a check if firewalld is initialized and running.\r\n\r\n### libnetwork\/iptables: move firewalld helpers together\r\n\r\nMove the exported helpers to a separate file.\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Temporarily moved to draft; I'll open some separate PRs first for some changes, to make this PR a bit smaller.","Great rework! Hope this gets into master soon!\r\nI'll give it a try soon","@akerouanton @neersighted ptal \ud83e\udd17 ","The only callers to `iptables.OnReloaded` are the bridge driver (to replay iptables rule insertions) and this gem (to replay iptables chain creation and rule insertion): https:\/\/github.com\/moby\/moby\/blob\/cff4f20c44a3a7c882ed73934dec6a77246c6323\/libnetwork\/firewall_linux.go#L15-L31\r\nI'm going to experiment with a PR which has the firewalld connection call into the libnetwork controller, which in turn calls into the drivers to replay the insertions."],"labels":["status\/2-code-review","area\/networking","kind\/refactor","area\/networking\/firewalld"]},{"title":"Make imports to Azure\/go-ansiterm Windows-only","body":"**- What I did**\r\n\r\n- Moved integration\/internal\/termtest\/stripansi.go to integration\/internal\/termtest\/stripansi_windows.go in order to pull \"github.com\/Azure\/go-ansiterm\"  on Windows system only.\r\n\r\n- written a dummy stripansi_default.go for other platforms.\r\n\r\n**- How I did it**\r\n\r\nwith mv and kwrite.\r\n\r\n**- How to verify it**\r\n\r\nRun tests on various platforms, for which I don't have access to. But the change is trivial.\r\n\r\n\r\n**- Description for the changelog**\r\n\r\n-  Make integration\/internal\/termtest Windows-only\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\nThis is a [Lykoi cat](https:\/\/en.m.wikipedia.org\/wiki\/Lykoi):\r\n\r\n![301189136_1257635511671283_4619588125750006623_n](https:\/\/github.com\/moby\/moby\/assets\/30413512\/7b75259b-8e18-4dc0-8eaa-df56b58622f1)\r\n","comments":["The issue is:\r\n - we (Fedora) package Moby unbundled as a library to consume from other packages\r\n - we still use GOPATH, not modules yet (though the problem would probbaly occurs with Modules)\r\n - since github.com\/Azure\/go-ansiterm is not in a Windows-only file, the import path is grabbed by the dependencies generator\r\n - we don't want to package github.com\/Azure\/go-ansiterm since it is Windows only.","I'd like to have that for debian\/devuan, too."],"labels":["platform\/windows","status\/2-code-review","area\/testing","kind\/refactor"]},{"title":"daemon: assorted fixes with signal \/ kill handling","body":"### daemon: Daemon.killPossiblyDeadProcess(): fix error return\r\n\r\nThis function is meant to suppress \"no such process\" errors, but was\r\nstill returning them.\r\n\r\n### daemon: Daemon.killProcessDirectly() don't return error that will be ignored\r\n\r\nDaemon.Kill is the only consumer of this function, and will ignore \"no such\r\nprocess\" errors, so simplify the code for consumers of this function to\r\nnot have to selectively ignore errors.\r\n\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Hm.. interesting; looks like it's checking for this to be an error\r\n\r\n```\r\n=== FAIL: amd64.integration-cli TestDockerAPISuite\/TestExecStateCleanup (10.69s)\r\n    docker_api_exec_test.go:244: assertion failed: expected an error, got nil\r\n    --- FAIL: TestDockerAPISuite\/TestExecStateCleanup (10.69s)\r\n```\r\n\r\n```\r\n=== FAIL: amd64.integration-cli TestDockerCLIEventSuite\/TestEventsContainerWithMultiNetwork (1.99s)\r\n    docker_cli_events_unix_test.go:243: assertion failed: 1 (int) != 2 (int)\r\n    --- FAIL: TestDockerCLIEventSuite\/TestEventsContainerWithMultiNetwork (1.99s)\r\n```\r\n\r\n```\r\n=== FAIL: amd64.integration-cli TestDockerCLIRestartSuite\/TestRestartRunningContainer (6.31s)\r\n    docker_cli_restart_test.go:62: assertion failed: error is not nil: condition \"\"false\" == \"true\"\" not true in time (5s)\r\n    --- FAIL: TestDockerCLIRestartSuite\/TestRestartRunningContainer (6.31s)\r\n```\r\n\r\n```\r\n=== FAIL: amd64.integration-cli TestDockerExternalVolumeSuite\/TestExternalVolumeDriverUnmountOnCp (31.15s)\r\n    docker_cli_external_volume_driver_test.go:617: assertion failed: 1 (s.ec.unmounts int) != 2 (int): test\r\n        \r\n    --- FAIL: TestDockerExternalVolumeSuite\/TestExternalVolumeDriverUnmountOnCp (31.15s)\r\n```\r\n\r\n```\r\n=== FAIL: amd64.integration.container TestContainerWithAutoRemoveCanBeRestarted\/kill (10.40s)\r\n    restart_test.go:203: timeout hit after 10s: waiting for container to be one of (running), currently exited\r\n    --- FAIL: TestContainerWithAutoRemoveCanBeRestarted\/kill (10.40s)\r\n\r\n=== FAIL: amd64.integration.container TestContainerWithAutoRemoveCanBeRestarted\/stop (10.43s)\r\n    restart_test.go:203: timeout hit after 10s: waiting for container to be one of (running), currently exited\r\n    --- FAIL: TestContainerWithAutoRemoveCanBeRestarted\/stop (10.43s)\r\n```"],"labels":["status\/2-code-review","area\/daemon"]},{"title":"Choose target IP for port forwarding","body":"### Description\r\n\r\nAs far as I understand `docker run -p [2001:db8:3333:4444:5555:6666:7777:8888]:1234:5678` normally means \"listen on the host on 2001:db8:3333:4444:5555:6666:7777:8888 port 1234 and forward it to port 5678 of the container's **IPv4** address.\r\n\r\nI have a bunch of servers that don't have IPv4 connectivity, so I enabled IPv6 in `daemon.json` and now all the containers get an IPv4 and an IPv6 address.\r\n\r\nIt seems that now the above command means \"listen on the host on 2001:db8:3333:4444:5555:6666:7777:8888 port 1234 and forward it to port 5678 **of the container's IPv6 address**.\r\n\r\nThere is a surprising number of official Docker images of popular software that will only listen on IPv4.\r\n\r\nIt would be super convenient if there was a way to tell Docker to listen on the host on an IPv6 port but still forward it to the container's IPv4 address (like it already does for a container that doesn't have an IPv6 address).\r\n\r\nI can't simply disable the IPv6 address for such containers because often they need to access the internet, which they can't do without an IP address.\r\n\r\nThis feature _could_ be extended to even allow specifying a target IP for containers that have multiple IP addresses of one type but personally I don't have a need for that as I don't have any containers that have two IPv4 or two IPv6 addresses.","comments":[],"labels":["status\/0-triage","kind\/feature","area\/networking","area\/networking\/ipv6","area\/networking\/portmapping"]},{"title":"libcontainerd: consider to use task.Wait to update the container's exit code instead of task.Event","body":"### Description\n\nWithin containerd's current design, if the shim is killed before task-service.Delete API call, the callback on connect close will send 137 exit code because the callback doesn't have any context about container's exit code.\r\n\r\nhttps:\/\/github.com\/containerd\/containerd\/blob\/70a2c95ae8c02d7a4e448f0e4fb8bb0e6344b5c7\/runtime\/v2\/shim.go#L170-L184\r\n\r\nAnd the moby\/moby can't handle duplicate exit event well. Let's say that the moby receives exit code 0 at first and then the duplicate exit event with different code 137 can override 0, as the https:\/\/github.com\/containerd\/containerd\/issues\/4769 described.\r\n\r\nAnd the containerd task events are best-effort mode because there is no cache and it's handled by shim. I think it's hard to guarantee the exit status is correct. So, I suggest to use task.Wait API instead of event.\r\n\r\nREF: https:\/\/github.com\/containerd\/containerd\/pull\/8954","comments":["We discussed this in a maintainers' call earlier this week: overall consensus was that keeping events as the source of truth over using `task.Wait` is preferred; however, we **should** be able to handle (read: discard) duplicate events.\r\n\r\nIn today's containerd maintainers' call, the fact that we often re-use task IDs (and thus a restarted container might get an event intended for the predecessor under heavy load is possible) was brought up, however, given the presence of the `ExitedAt` field in the protobuf, it should be possible for us to filter events that are intended for a container created before us.\r\n\r\nIn short: we think that containerd should do what makes the most sense to fix this class of issues robustly, and we think we can adapt Moby to the change if we have to give up on having a reliable one-shot exit event.\r\n\r\ncc @dmcgowan @cpuguy83 @thaJeztah ","We should address the duplicate event handling on the Moby side before fixing on the containerd side. Any fix to this issue on the containerd side will open up the possibility of duplicate events. If containerd moves the event publishing from shim to containerd, there is still no way to ensure that any shim is not publishing that event."],"labels":["area\/runtime","status\/0-triage","kind\/feature"]},{"title":"userns: daemon: WithNamespaces lacks validation for joining multiple namespaces","body":"### Description\n\nWhile working on this code, I noticed that there's currently an issue with userns enabled. When userns is enabled, joining another container's namespace must also join its user-namespace. For example;\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/a9e8110fe35a50f50b73bb522575a656373fc21b\/daemon\/oci_linux.go#L295-L299\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/a9e8110fe35a50f50b73bb522575a656373fc21b\/daemon\/oci_linux.go#L269-L272\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/a9e8110fe35a50f50b73bb522575a656373fc21b\/daemon\/oci_linux.go#L326-L331\r\n\r\n\r\nHowever, a container can only be in a single user namespace, so if a container joins namespaces from multiple containers, latter user-namespaces overwrite former ones.\r\nhttps:\/\/github.com\/moby\/moby\/blob\/a9e8110fe35a50f50b73bb522575a656373fc21b\/daemon\/oci_linux.go#L230-L242\r\n\r\n\r\nWe should add validation for this, and handle these cases with a proper error message.\r\n","comments":[],"labels":["area\/runtime","kind\/bug","area\/security\/userns"]},{"title":"libnetwork: re-enabe TestExternalKeyWithReexec test","body":"relates to:\r\n\r\n- https:\/\/github.com\/moby\/libnetwork\/pull\/515\r\n- https:\/\/github.com\/moby\/moby\/pull\/20662\r\n- https:\/\/github.com\/moby\/libnetwork\/issues\/829\r\n- https:\/\/github.com\/moby\/libnetwork\/pull\/921\r\n\r\n\r\nSupport for \"libnetwork-setkey\" re-exec was added in libnetwork in commit [6175353964906b1279cd3477437b15cca9b1a2e0][1] (https:\/\/github.com\/moby\/libnetwork\/pull\/515) which included a test for both Sandbox.SetKey, and setting the key through the \"libnetwork-setkey\" re-exec.\r\n\r\nDuring the integration with [containerd 1.0][2], it was found that the re-exec hook used the wrong type (https:\/\/github.com\/moby\/libnetwork\/issues\/829):\r\n\r\n> In https:\/\/github.com\/docker\/libnetwork\/blob\/df3f8a10796e2173732aeafa41af45f970ced9f6\/sandbox_externalkey_unix.go#L48\r\n> the code is expecting libcontainer.State as input but data provided by\r\n> libcontainer actually has the type libcontainer\/configs.HookState.\r\n>\r\n> That means that this data doesn't contain netns path and the SetKey should not work.\r\n\r\nThis issue was fixed in [e84aad941e0577ee501de9b5ba9667e2f5daf38f][3] (https:\/\/github.com\/moby\/libnetwork\/pull\/921), but lacking a test-environment, the `TestExternalKeyWithReexec` test was removed.\r\n\r\n[1]: https:\/\/github.com\/moby\/libnetwork\/commit\/6175353964906b1279cd3477437b15cca9b1a2e0\r\n[2]: https:\/\/github.com\/moby\/moby\/pull\/20662\r\n[3]: https:\/\/github.com\/moby\/libnetwork\/commit\/e84aad941e0577ee501de9b5ba9667e2f5daf38f\r\n\r\n<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/2-code-review","area\/networking","area\/testing"]},{"title":"Paused container shows as unhealthy","body":"### Description\n\nI have a number of health-checks running against my containers. I have noticed that if I pause a container, after a while, the health-check reports the container as unhealthy, despite the exit codes of the health checks being 0:\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/14807258\/27cd0413-d67d-460c-ad20-b82c5bb8bdd2)\r\n\n\n### Reproduce\n\nStart up a container that has a health check, then pause the container.\n\n### Expected behavior\n\nWhile paused, I would expect the health of a container to be reported as healthy. The container is not in bad health, but merely paused.\r\nAlternatively, I would expect the health to be reported as Paused.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:35 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:35 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 35\r\n  Running: 33\r\n  Paused: 1\r\n  Stopped: 1\r\n Images: 44\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 nvidia runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-10-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 6\r\n Total Memory: 15.44GiB\r\n Name: Server\r\n ID: <looks a bit sensitive, am I supposed to share this bit?>\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nThe reason I have created this issue is, I pause all my containers which have volumes attached to the them, so that I can backup the containers without their volumes changing. \r\n\r\nI currently have an autoheal container which restarts all containers which are marked as unhealthy, so that they can become operational again. This is restarting the paused containers during the backup. This results in failed backups, as the files being backed up are being changed during the backup process.\r\n\r\nAs a short term solution, I can also pause the autoheal container during the backup window, but this means there could be outages on the the containers that are not paused (ones without volumes attached).","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Container using network mode host does not get its resolv.conf updated when the host's resolv.conf is updated (using systemd-resolved)","body":"### Description\r\n\r\nBecause the resolv.conf is not updated on the container, it stops having access to the internet when the host \/ device changes networks.\r\nI saw https:\/\/github.com\/docker\/for-linux\/issues\/889 which mentions that it is supposed to be updated automatically but I actually can't find where this is mentioned in https:\/\/docs.docker.com\/v17.09\/engine\/userguide\/networking\/default_network\/configure-dns\/.\r\n\r\nIs this behavior of the resolv.conf not updating with host a bug or is this something not implemented or intended behavior ?\r\n\r\n### Reproduce\r\n\r\nStart a long running container on your laptop (which is using systemd-resolved), then move to a different network with different DNS servers. Notice that the resolv.conf inside the container is now wrong.\r\n\r\n### Expected behavior\r\n\r\nresolv.conf on container should match the updated host's resolv.conf\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996600\r\n Built:             Wed Jul 26 21:44:58 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4c9c\r\n  Built:            Wed Jul 26 21:44:58 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.2\r\n  GitCommit:        0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 7\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 6\r\n Images: 14\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.4.8-zen1-1-zen\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 13.5GiB\r\n Name: Lenovo-Yoga-7\r\n ID: 3PMN:VRXJ:C3R6:RFC2:ZLXJ:OJJU:OFKE:DQLW:YBC6:YYWQ:EHPI:WDWG\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["I think I've experienced this myself when using VMware to run a VM guest running Linux as the Docker host.\r\n\r\nWhen I suspend the VM guest and later restore it, the network works fine in the guest, but not within the containers (_they could talk to each other but no longer reach external networks, such as for installing packages_). I'd resolve it by running `systemctl restart docker`.\r\n\r\nWould be great if that wasn't required.","From the description, I think this is for the \"default\" bridge network.\r\n\r\nWhen using the default bridge, the \"legacy\" networking stack (pre \"custom networks\") is used;\r\n\r\n- docker's internal DNS resolver is not used\r\n- instead, containers get a copy of the host's `resolv.conf`\r\n- as lookups happen from within the container's networking namespace, localhost DNS can't be used, which means that systemd-resolvd's IP-address (`127.0.0.53`) cannot be used, and instead, systemd-resolvd's \"upstream\" DNS servers are read from `\/run\/systemd\/resolve\/resolv.conf`, and included in the container's `resolv.conf` \r\n- if no \"non-localhost\" DNS servers are found, use a default as \"last resort\" (https:\/\/github.com\/moby\/moby\/blob\/075a2d89b96ca2c31a61ce3b05214bbe2ba49af8\/libnetwork\/resolvconf\/resolvconf.go#L76-L78)\r\n- a checksum is kept of the `resolv.conf` at time of creation, which is to allow the user to _edit_ the file (in which case, docker will no longer update it, to prevent changes made by the user from being reverted)\r\n- I _think_ when restarting the daemon, all `resolv.conf` copies of _all_ containers are re-created (skipping those that were modified by the user)\r\n\r\n\r\nThis flow originated from the very early beginnings of Docker, and was designed with the assumption that the daemon would run in a server environment (no dynamic IP and\/or networks), and before `systemd-resolvd` existed (having a `localhost` \/ `127.0.0.x` resolver was an \"exception\", not the \"rule\");\r\n\r\nIt may be clear that a lot of complexity is involved here, and quite some parts where things can go wrong (looking up systemd-resolvd's upstreams); dynamically updating the `resolv.conf` for each container _could_ be an option, but I guess the challenge would be somewhat to decide what should trigger this; alternatively, maybe we can do this on a `reload` (`systemctl reload docker.service` to trigger re-generating `resolv.conf`).\r\n\r\nThe _better_ solution would probably be to remove the legacy code-path, and always use the embedded DNS; I opened a ticket for that once;\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/22295\r\n\r\nThe reason the legacy code-path still exists was (IIRC) for a few reasons, but I _think_ most of those should no longer be a concern (and I'd love to get rid of the two distinct implementations);\r\n\r\n- The legacy links (including sharing of environment variables) were still used by quite some users, so we didn't want to break those on \"day 1\"\r\n- At the time, some tools implemented filewatchers on some of docker's internals (including the `resolv.conf` and `\/etc\/hosts`), and making the default bridge use the embedded DNS could break those tools. I'm not sure if that's something we should be really concerned about, as anything inside `\/var\/lib\/docker` is considered to be exclusively accessed by the daemon, so any tool making other assumptions is depending on \"undocumented\" behavior.\r\n- Kubernetes; at the time, kubernetes had parts in place to \"disable\" all managed networking in Docker (ISTR, they bind-mounted files _over_ the files that Docker generated, but I forgot the details); defaulting to the embedded DNS would break some scenarios there. A _lot_ has changed though, since that time, and most k8s setups would now be using containerd instead, and for those that still use the Docker Engine, perhaps this is not an issue anymore as well (but would need verifying).\r\n","LOL, and for some reason, I completely ignored the \"network host\" part; I think still related though \ud83d\ude02 ","> I completely ignored the \"network host\" part\r\n\r\nOh I did too. My experience was with the default bridge, the VM guest network changes when resumed, which requires restarting the daemon AFAIK. I'm not bothered by that much personally.\r\n\r\n---\r\n\r\n**Slightly off-topic (_still DNS + `systemd-resolved`_)**\r\n\r\nI wasn't familiar with what your response was covering _until today_, where prior to seeing your response I had [strung together some idea of why BuildKit was not behaving like users expected](https:\/\/github.com\/moby\/buildkit\/issues\/2404#issuecomment-1710060779).\r\n\r\nI tried following the PRs and related code between the projects to see what was different\/missing, and [you spotted the concern in Oct 2022](https:\/\/github.com\/moby\/buildkit\/pull\/3142#discussion_r985159898) \ud83d\ude0e \r\n\r\nI am done looking further into it, but I assume Buildx did not get around to [passing a `DNSConfig` override to BuildKit](https:\/\/github.com\/docker\/buildx\/issues\/110#issuecomment-514299102) (_assuming Buildx has more context with network config on what is more appropriate_). There's a [BuildKit issue](https:\/\/github.com\/moby\/buildkit\/issues\/3210#issuecomment-1709809490) with [PR](https:\/\/github.com\/moby\/buildkit\/pull\/3244) (going stale) which was about `resolv.conf` handling with `--network=host` \/ `networkMode` (`buildkitd.toml`).","> I tried following the PRs and related code between the projects to see what was different\/missing, and you spotted the concern in Oct 2022 \ud83d\ude0e\r\n\r\nHeh, I knew which PR you linked to without clicking the link. I opened that PR when I was somewhat cleaning up the `resolvconf` package, which had become over-engineered and complex over the years (still more to do there!).\r\n\r\n### Host `--network=host` handles DNS\r\n\r\nSo for the `--network=host` case, the situation is _somewhat_ similar to the \"default bridge\" case, but for different reasons;\r\n\r\n- In `--network=host`, the container doesn't have a networking namespace, so _\"`localhost` \"inside\" the container === `localhost` \"outside\" the container\"_ because from a networking perspective, there is no \"inside\" or \"outside\" the container they're _exactly_ the same.\r\n- And because there's no networking namespace, there's no \"embedded\" DNS that we can use in such a container\r\n- But there's also no need to have one, because we can access \"whatever\" is configured on the host directly\r\n\r\nBut: here's where the \"fun\" start, because while the \"networking\" namespace is the same, the _filesystem_ (mount namespace) is still separate, and we still need to configure the _container_ so that processes inside the container know what resolver to use;\r\n\r\n- Because the filesystem is separate, `\/etc\/resolv.conf` inside the container is a file that needs to be present _inside the container_\r\n- But its _content_ should be the same as on the host (we want to use the same resolver if we're in the same namespace)\r\n\r\nA logical approach would be to bind-mount the host's `\/etc\/resolv.conf` into the container, but that had some challenges;\r\n\r\n- `\/etc\/resolv.conf` on the host, depending on the system configuration, may be a symlink (bind-mounting that inside the container would try to resolve the symlink's target _inside_ the container)\r\n- `\/etc\/resolv.conf` on the host may be \"modified\" (the topic of this ticket); bind-mounting files uses the file's `inode`, which can be problematic because most software updating files will use a `copy file -> update copy -> (delete, and) replace original file`, in which case the container would still be holding a mount for the _deleted_ file (so the copy before updating).\r\n- and even without that issue, `\/etc\/resolv.conf` inside the container is writable, and we don't want the container to be able to modify the file on the host (which would be the case if we'd bind-mount the file from the host's `\/etc\/resolv.conf`).\r\n\r\nSo, for these reasons, we (again) need a _COPY_ of the host's `\/etc\/resolv.conf` (or whatever that's symlinked to) for each container, and make sure that\r\n\r\n- if the file on the host is modified (e.g. due to WiFi connection switching)\r\n- AND the file has not been updated by the user (inside the container)\r\n- .. that we update it, and get an updated version inside the container\r\n- (also see https:\/\/github.com\/moby\/moby\/pull\/41022, which was related to that)\r\n\r\nWhich brings us back to \"square one\" (described in my \"bridge\" comment from earlier) :joy:\r\n\r\n### Reconfiguring the \"embedded DNS\"\r\n\r\nSo this is something I need to look into, and what came up when I discussed this with @akerouanton \r\n\r\nWhile writing my earlier comment, my **assumption** was that the embedded DNS itself has no real configuration\r\n\r\n- just use a regular DNS lookup on the host, using \"whatever is configured on the host\" (`\/etc\/resolv.conf` on the host)\r\n- which can be `127.0.0.53` (if systemd-resolvd is in use)\r\n- and let `systemd-resolvd` handle the forwarding to \"upstream\" resolvers.\r\n\r\nHowever, this _MAY_ not be the case (this is something I need to look into \/ verify), and it's possible that the embedded DNS also is using more than that, and may be reading systemd-resolvd's _UPSTREAM_ DNS resolvers to configure what it should use. This would mean that dynamically switching networks would also prevent the embedded DNS from using the correct DNS. And if that's the case, that's probably something that should be fixed.\r\n","> * we don't want the container to be able to modify the file on the host (which would be the case if we'd bind-mount the file from the host's `\/etc\/resolv.conf`).\r\n\r\nDoes it need to be writeable? You could have a `:ro` bind mount? (_not that it makes much difference as you noted with the inode & symlink concerns_)\r\n\r\n","> Does it need to be writeable? \r\n\r\nFor docker itself, no. Customisations can be made through the `--dns`, `--dns-opt`, `--add-host`, `--hostname` etc options, and those are made when the container is created (so would not require the file to be writable).\r\n\r\nBut having these files (`\/etc\/hosts`, `\/etc\/resolv.conf`, `\/etc\/hostname` writable is a feature that was added at some point, so \ud83e\udd37\u200d\u2642\ufe0f ; see\r\n\r\n- https:\/\/github.com\/moby\/moby\/issues\/2267 (and various links from there)\r\n- https:\/\/github.com\/moby\/moby\/pull\/5129\r\n- https:\/\/github.com\/moby\/moby\/issues\/11950\r\n\r\n\r\nAdmitted, I think _most_ of the requests were for `\/etc\/hosts` to be writable, but there may have been some cases where either the user, or software they were running required (expected) those files to be writable.","I am thinking of trying to solve this for my setup by `127.0.0.53` to the resolv.conf within the container,\r\nif I do that, as I understand from @thaJeztah 's first comment, the file will stop getting updated,\r\nhow would I then revert it back to being managed by docker ? ie undo my changes","Don't think there's an easy way for that, although maybe it works if you change it back to the original version (and the checksum matches).\r\n\r\nBut the alternative would be to just explicitly set the container to use `--dns=127.0.0.53` because that's the fixed address for resolvd (any changes in external resolvers should be handled by \/ abstracted by resolvd itself?)","yeah but will that work for network mode host ? and in that case will the resolv.conf in container then not be a copy of the host's but rather contain just 127.0.0.53 ?","In network mode \"host\", there is no container from a networking perspective; does your resolv.conf on the host contain any other options?","the resolv.conf in host for me contains the upstream dns servers, these would not get updated when my laptop would change network,\r\nI just put the `--dns=127.0.0.53` option on the docker container and I see now that 127.0.0.53 gets set as the only entry in the resolv.conf in container, and that actually fixes my issue!\r\n\r\nI guess this issue can remain open though since the dns settings in container not updating with host, especially for network mode host containers is sort of problematic","> the resolv.conf in host for me contains the upstream dns servers, these would not get updated when my laptop would change network\r\n\r\nInteresting. I wonder why that is; to my understanding, normally it would only contain systemd-resolvd's address, because `systemd-resolvd` acts as forwarder for the \"upstream\" DNS servers.\r\n\r\nSo, in practice `\/etc\/resolv.conf` would never have to change.\r\n\r\nHere's from a test machine I have running on DigitalOcean, which has `systemd-resolved`;\r\n\r\n```bash\r\nls -la \/etc\/resolv.conf\r\nlrwxrwxrwx 1 root root 39 Nov  1  2018 \/etc\/resolv.conf -> ..\/run\/systemd\/resolve\/stub-resolv.conf\r\n\r\ncat \/etc\/resolv.conf\r\n# This file is managed by man:systemd-resolved(8). Do not edit.\r\n#\r\n# This is a dynamic resolv.conf file for connecting local clients to the\r\n# internal DNS stub resolver of systemd-resolved. This file lists all\r\n# configured search domains.\r\n#\r\n# Run \"resolvectl status\" to see details about the uplink DNS servers\r\n# currently in use.\r\n#\r\n# Third party programs must not access this file directly, but only through the\r\n# symlink at \/etc\/resolv.conf. To manage man:resolv.conf(5) in a different way,\r\n# replace this symlink by a static file or a different symlink.\r\n#\r\n# See man:systemd-resolved.service(8) for details about the supported modes of\r\n# operation for \/etc\/resolv.conf.\r\n\r\nnameserver 127.0.0.53\r\noptions edns0 trust-ad\r\n```","Wondering if something like network manager is not aware of systemd-resolvd (and that replacing the `\/etc\/resolv.conf` symlink with an actual file).\r\n\r\nI must admit that I don't run Linux locally, but I have seen reports where multiple  systems (network manager, other tools) were stepping on each-other's toes, and all trying to manage the same things.\r\n","systemd-resolved can actually run in different modes based on resolv.conf contents https:\/\/man.archlinux.org\/man\/systemd-resolved.8#\/ETC\/RESOLV.CONF\r\nthe recommended way is for resolv.conf to be a symlink to `\/run\/systemd\/resolve\/stub-resolv.conf` but resolv.conf can be maintained by something else (like NetworManager) and then systemd-resolved can act as a consumer of that file rather than managing it, and that is how I think its setup on my system.\r\nI dont remember how or why I configured it that way but its been working perfectly for me for a while and it is not really something wrong, just not the recommended way.","Ah, thanks for that link! Now that you mention it, I think I ran into that part at some point \ud83e\udd14 \r\n\r\nDefinitely something to take into account, and that may actually part of the reason why this may all work \"depending on the situation\".\r\n\r\nI'm wondering though how such a setup is \"meant\" to work, because if the recommendation (see my comment) is \"systems should always look at `\/etc\/resolv.conf` (and don't look any further)\" ... how would they ever use `systemd-resolved` ? Because in such a setup, `\/etc\/resolv.conf` doesn't even include `127.0.0.53`, so \"nothing\" would actually be using it? \ud83e\udd14 \ud83e\udd14 \ud83e\udd14 \r\n\r\nGuess I need to do more reading how that's _expected_ to work.\r\n\r\n\r\nThe \"easy\" approach is to (somehow) detect that `systemd-resolvd` is active, and then just hardcode to `127.0.0.53`, but that's making a lot of assumptions \ud83e\udd14 \r\n\r\n","> I'm wondering though how such a setup is \"meant\" to work, because if the recommendation (see my comment) is \"systems should always look at `\/etc\/resolv.conf` (and don't look any further)\" ... how would they ever use `systemd-resolved` ? Because in such a setup, `\/etc\/resolv.conf` doesn't even include `127.0.0.53`, so \"nothing\" would actually be using it? \ud83e\udd14 \ud83e\udd14 \ud83e\udd14\r\n\r\nThey can use `systemd-resolved` via a D-BUS API according to the docs. Some software will use glibc via NSS (`\/etc\/nsswitch.conf`) which can also use the `nss-resolve` module to query `systemd-resolved`.\r\n\r\nI think `nslookup` goes that route, whereas a tool like `dig` reads `\/etc\/resolv.conf` directly. If `\/etc\/resolv.conf` doesn't point to anything related to `systemd-resolved`, I still found `nslookup` was using the DNS nameserver in `\/etc\/resolv.conf` (_which `nss-resolve` I think intentionally does in that scenario_).\r\n\r\nIt may vary with other networking needs such as split-dns (_aka conditional-forwarding_), mDNS, perhaps some VPN software, but I'd focus on `\/etc\/resolv.conf` and ensuring that's supported well, then let any other scenario reveal itself via user reports \ud83d\ude05 \r\n\r\n> Guess I need to do more reading how that's expected to work.\r\n\r\nI had a shot at that if it helps. Bit verbose, so I collapsed the bulk of my response below.\r\n\r\nOnly surprise I really noticed was with the default bridge behaviour, freshly started containers were not recognizing that `\/etc\/resolv.conf` had changed (_no longer a symlink, nor `127.0.0.53` used_). To get back non-`systemd-resolved` behaviour I had to restart the Docker daemon. Seems like a bug? (`v24.0.5`).\r\n\r\n---\r\n\r\n## Notes\r\n\r\nIn my VM guest, NetworkManager is used, `systemd-resolved` is not:\r\n\r\n```console\r\n# Default config from VM install:\r\n$ cat \/etc\/resolv.conf\r\n# Generated by NetworkManager\r\nsearch localdomain\r\nnameserver 192.168.12.2\r\n\r\n$ resolvectl status\r\nFailed to get global data: Unit dbus-org.freedesktop.resolve1.service not found.\r\n\r\n$ systemctl is-active systemd-resolved\r\ninactive\r\n```\r\n\r\n### Enabling (initial setup)\r\n\r\n<details>\r\n<summary>Click to expand<\/summary>\r\n\r\n```console\r\n# Enable `systemd-resolved`:\r\n$ systemctl start systemd-resolved.service\r\n\r\n$ systemctl is-active systemd-resolved\r\nactive\r\n\r\n$ resolvectl status\r\nGlobal\r\n           Protocols: +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n    resolv.conf mode: foreign\r\n  Current DNS Server: 192.168.12.2\r\n         DNS Servers: 192.168.12.2\r\nFallback DNS Servers: 1.1.1.1#cloudflare-dns.com 9.9.9.9#dns.quad9.net 8.8.8.8#dns.google 2606:4700:4700::1111#cloudflare-dns.com\r\n                      2620:fe::9#dns.quad9.net 2001:4860:4860::8888#dns.google\r\n          DNS Domain: localdomain\r\n\r\nLink 2 (ens33)\r\n    Current Scopes: DNS LLMNR\/IPv4 LLMNR\/IPv6 mDNS\/IPv4 mDNS\/IPv6\r\n         Protocols: +DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n       DNS Servers: 192.168.12.2\r\n        DNS Domain: localdomain\r\n\r\nLink 3 (docker0)\r\n    Current Scopes: none\r\n         Protocols: -DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n\r\nLink 99 (veth733e157)\r\n    Current Scopes: LLMNR\/IPv6 mDNS\/IPv6\r\n         Protocols: -DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n\r\n$ resolvectl dns\r\nGlobal: 192.168.12.2\r\nLink 2 (ens33): 192.168.12.2\r\nLink 3 (docker0):\r\nLink 99 (veth733e157):\r\n\r\n$ resolvectl default-route\r\nGlobal\r\n           Protocols: +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n    resolv.conf mode: foreign\r\n  Current DNS Server: 192.168.12.2\r\n         DNS Servers: 192.168.12.2\r\nFallback DNS Servers: 1.1.1.1#cloudflare-dns.com 9.9.9.9#dns.quad9.net 8.8.8.8#dns.google 2606:4700:4700::1111#cloudflare-dns.com\r\n                      2620:fe::9#dns.quad9.net 2001:4860:4860::8888#dns.google\r\n          DNS Domain: localdomain\r\nLink 2 (ens33): yes\r\nLink 3 (docker0): no\r\nLink 99 (veth733e157): no\r\n\r\n# No change here yet of course (even if restarting NetworkManager service):\r\n$ cat \/etc\/resolv.conf\r\n# Generated by NetworkManager\r\nsearch localdomain\r\nnameserver 192.168.12.2\r\n```\r\n\r\n<\/details>\r\n\r\n### `systemd-resolved` - Foreign mode (no symlink)\r\n\r\n<details>\r\n<summary>Click to expand<\/summary>\r\n\r\nThe above only enabled `systemd-resolved` to run in **`foreign` mode** (_no symlink used_), which is [described in docs](https:\/\/man.archlinux.org\/man\/systemd-resolved.8#\/ETC\/RESOLV.CONF):\r\n\r\n> _Alternatively, `\/etc\/resolv.conf` may be managed by other packages, in which case `systemd-resolved` will read it for DNS configuration data. In this mode of operation `systemd-resolved` is consumer rather than provider of this configuration file._\r\n> \r\n> _Note that the selected mode of operation for this file is detected fully automatically, depending on whether `\/etc\/resolv.conf` is a symlink to `\/run\/systemd\/resolve\/resolv.conf` or lists `127.0.0.53` as DNS server._\r\n\r\nSo naturally no difference for most software (_NSS \/ glibc will use `nss-resolve` via `\/etc\/nsswitch.conf`, and that'll look at `\/etc\/resolv.conf` AFAIK_) - I think with Alpine \/ musl that's a bit different as I don't recall that going through NSS.\r\n\r\nQuerying with DNS lookups we get the expected `192.168.12.2` nameserver used:\r\n\r\n```console\r\n# Reads `\/etc\/resolv.conf` directly:\r\n$ dig docker.com | grep SERVER\r\n;; SERVER: 192.168.12.2#53(192.168.12.2) (UDP)\r\n\r\n# Copy `\/etc\/resolv.conf` into container:\r\n$ docker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER      \r\ndocker.com.     A       IN      5s      141.193.213.20  192.168.12.2:53\r\ndocker.com.     A       IN      5s      141.193.213.21  192.168.12.2:53\r\n```\r\n\r\nYou can change which DNS servers to query in this case for `systemd-resolved` (_which will recognize these, but most software AFAIK is unaffected unless using D-BUS API `systemd-resolved` has?_).\r\n\r\n`resolvectl query docker.com` would respect the configuration update (_although it's not necessarily obvious in this case, but pointing to a local DNS resolver with a different response would help verify_).\r\n\r\nBut since software is using `\/etc\/resolv.conf` like `dig` (_or Docker, since it's not a symlink, nor any `127.0.0.53` entry present_), this won't make much difference and queries would still go through `192.168.12.2` (_VMware is managing the VM guest DNS_). It may work a bit differently with more specialized networking setups \ud83e\udd37\u200d\u2642\ufe0f \r\n\r\n```bash\r\n# Changing DNS servers to use:\r\n\r\n# Global DNS via override in `\/etc\/systemd\/resolved.conf.d\/`:\r\nsudo mkdir \/etc\/systemd\/resolved.conf.d\/\r\necho -e '[Resolve]\\nDNS = 1.1.1.1 1.0.0.1' | sudo tee \/etc\/systemd\/resolved.conf.d\/global-dns.conf\r\n# Restart required:\r\nsudo systemctl restart systemd-resolved\r\n\r\n# Temporary DNS override per interface (eg: ens33):\r\nsudo resolvectl dns ens33 1.1.1.1\r\n```\r\n\r\n<\/details>\r\n\r\n### `systemd-resolved` - Stub mode (symlink)\r\n\r\nThis is the most common way to use `systemd-resolved` out of the 4 supported modes?\r\n\r\n```console\r\n# Switch to symlink with stub:\r\n$ sudo ln -sf \/run\/systemd\/resolve\/stub-resolv.conf \/etc\/resolv.conf\r\n$ sudo systemctl restart systemd-resolved\r\n$ sudo systemctl restart NetworkManager\r\n\r\n# Testing:\r\n$ dig docker.com | grep SERVER\r\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\r\n\r\n# Wrong?:\r\n$ docker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER \r\ndocker.com.     A       IN      175s    141.193.213.21  8.8.8.8:53\r\ndocker.com.     A       IN      175s    141.193.213.20  8.8.8.8:53\r\ndocker.com.     A       IN      223s    141.193.213.21  8.8.4.4:53\r\ndocker.com.     A       IN      223s    141.193.213.20  8.8.4.4:53\r\n\r\n# Correct:\r\n$ docker run --rm -it --network host ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER    \r\ndocker.com.     A       IN      5s      141.193.213.20  127.0.0.53:53\r\ndocker.com.     A       IN      5s      141.193.213.21  127.0.0.53:53\r\n\r\n# Correct:\r\n$ docker network create custom\r\n$ docker run --rm -it --network custom ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER    \r\ndocker.com.     A       IN      172s    141.193.213.21  127.0.0.11:53\r\ndocker.com.     A       IN      172s    141.193.213.20  127.0.0.11:53\r\n```\r\n\r\n## Default bridge `\/etc\/resolv.conf` bug\r\n\r\nNotably the default bridge `docker0` seemed odd. Those are the default fallback DNS servers to use by the docker daemon, but these were not the content from the symlinked `\/etc\/resolv.conf` (`127.0.0.1`), or the `\/run\/systemd\/resolve\/resolv.conf` (_which would have been expected? [1](https:\/\/github.com\/moby\/moby\/blob\/075a2d89b96ca2c31a61ce3b05214bbe2ba49af8\/daemon\/container_operations_unix.go#L412-L427) => [2](https:\/\/github.com\/moby\/moby\/blob\/075a2d89b96ca2c31a61ce3b05214bbe2ba49af8\/libnetwork\/resolvconf\/resolvconf.go#L36-L39)_):\r\n\r\n```console\r\n# NOTE: Large comment blocks removed from outputs to reduce noise\r\n\r\n$ cat \/etc\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/stub-resolv.conf managed by man:systemd-resolved(8).\r\nnameserver 127.0.0.53\r\noptions edns0 trust-ad\r\nsearch localdomain\r\n\r\n$ cat \/run\/systemd\/resolve\/stub-resolv.conf\r\n# This is \/run\/systemd\/resolve\/stub-resolv.conf managed by man:systemd-resolved(8).\r\nnameserver 127.0.0.53\r\noptions edns0 trust-ad\r\nsearch localdomain\r\n\r\n# Default upstream DNS + `global-dns.conf` override:\r\n$ cat \/run\/systemd\/resolve\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/resolv.conf managed by man:systemd-resolved(8).\r\nnameserver 1.1.1.1\r\nnameserver 1.0.0.1\r\nnameserver 192.168.12.2\r\nsearch localdomain\r\n\r\n\r\n# Docker content copied the host `\/etc\/resolv.conf`, but modified the `nameserver` entries:\r\n$ docker run --rm -it alpine cat \/etc\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/stub-resolv.conf managed by man:systemd-resolved(8).\r\noptions edns0 trust-ad\r\nsearch localdomain\r\nnameserver 8.8.8.8\r\nnameserver 8.8.4.4\r\n```\r\n\r\n### Workaround\r\n\r\nThis wasn't difficult to resolve, but I didn't realize that the daemon needed to be restarted for it to ensure containers source the `\/run\/systemd\/resolve\/resolv.conf` content..\r\n\r\n<details>\r\n<summary>Click to expand<\/summary>\r\n\r\n```\r\n# The fix:\r\nsudo systemctl restart docker\r\n\r\n# Correct:\r\ndocker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER      \r\ndocker.com.     A       IN      250s    141.193.213.20  1.1.1.1:53     \r\ndocker.com.     A       IN      250s    141.193.213.21  1.1.1.1:53     \r\ndocker.com.     A       IN      251s    141.193.213.20  1.0.0.1:53     \r\ndocker.com.     A       IN      251s    141.193.213.21  1.0.0.1:53     \r\ndocker.com.     A       IN      5s      141.193.213.20  192.168.12.2:53\r\ndocker.com.     A       IN      5s      141.193.213.21  192.168.12.2:53\r\n\r\n# Correct (Comment now the expected source, nameservers now correct):\r\n$ docker run --rm -it alpine cat \/etc\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/resolv.conf managed by man:systemd-resolved(8).\r\nnameserver 1.1.1.1\r\nnameserver 1.0.0.1\r\nnameserver 192.168.12.2\r\nsearch localdomain\r\n```\r\n\r\nReproduction:\r\n\r\n```console\r\n# Back to static \/etc\/resolv.conf:\r\n$ sudo rm \/etc\/resolv.conf\r\n# NetworkManager will now recreate `\/etc\/resolv.conf`:\r\n$ sudo systemctl restart NetworkManager\r\n\r\n$ cat \/etc\/resolv.conf\r\n# Generated by NetworkManager\r\nsearch localdomain\r\nnameserver 192.168.12.2\r\n\r\n# Correct:\r\n$ docker run --rm -it --network host ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER      \r\ndocker.com.     A       IN      5s      141.193.213.21  192.168.12.2:53\r\n\r\n# Correct:\r\n$ docker run --rm -it --network custom ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER    \r\ndocker.com.     A       IN      5s      141.193.213.21  127.0.0.11:53\r\n\r\n# Outdated:\r\n$ docker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER      \r\ndocker.com.     A       IN      73s     141.193.213.21  1.1.1.1:53     \r\ndocker.com.     A       IN      73s     141.193.213.20  1.1.1.1:53     \r\ndocker.com.     A       IN      72s     141.193.213.21  1.0.0.1:53     \r\ndocker.com.     A       IN      72s     141.193.213.20  1.0.0.1:53     \r\ndocker.com.     A       IN      5s      141.193.213.21  192.168.12.2:53\r\ndocker.com.     A       IN      5s      141.193.213.20  192.168.12.2:53\r\n\r\n\r\n# The Cloudflare entries from `global-dns.conf` override should not be there, nor the comment.\r\n# Container is sourcing the systemd-resolved config, but \/etc\/resolv.conf no longer has 127.0.0.53:\r\n$ docker run --rm -it alpine cat \/etc\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/resolv.conf managed by man:systemd-resolved(8).\r\nnameserver 1.1.1.1\r\nnameserver 1.0.0.1\r\nnameserver 192.168.12.2\r\nsearch localdomain\r\n```\r\n\r\nNow with updates:\r\n\r\n```console\r\n# Back to systemd-resolved with stub symlink:\r\n$ sudo ln -sf \/run\/systemd\/resolve\/stub-resolv.conf \/etc\/resolv.conf\r\n\r\n# Removing custom config:\r\n$ sudo rm \/etc\/systemd\/resolved.conf.d\/global-dns.conf\r\n$ sudo systemctl restart systemd-resolved\r\n\r\n# Restart NM so it doesn't think it manages \/etc\/resolv.conf anymore:\r\n# No need to restart Docker daemon, it's still going to think we're using systemd-resolved anyway:\r\n$ sudo systemctl restart NetworkManager\r\n\r\n$ resolvectl dns\r\nGlobal:\r\nLink 2 (ens33): 192.168.12.2\r\nLink 3 (docker0):\r\nLink 134 (br-3f6e4658ec31):\r\n\r\n$ resolvectl default-route\r\nGlobal\r\n           Protocols: +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n    resolv.conf mode: stub\r\nFallback DNS Servers: 1.1.1.1#cloudflare-dns.com 9.9.9.9#dns.quad9.net 8.8.8.8#dns.google 2606:4700:4700::1111#cloudflare-dns.com\r\n                      2620:fe::9#dns.quad9.net 2001:4860:4860::8888#dns.google\r\nLink 2 (ens33): yes\r\nLink 3 (docker0): no\r\nLink 134 (br-3f6e4658ec31): no\r\n\r\n$ resolvectl status\r\nGlobal\r\n           Protocols: +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n    resolv.conf mode: stub\r\nFallback DNS Servers: 1.1.1.1#cloudflare-dns.com 9.9.9.9#dns.quad9.net 8.8.8.8#dns.google 2606:4700:4700::1111#cloudflare-dns.com\r\n                      2620:fe::9#dns.quad9.net 2001:4860:4860::8888#dns.google\r\n\r\nLink 2 (ens33)\r\n    Current Scopes: DNS LLMNR\/IPv4 LLMNR\/IPv6 mDNS\/IPv4 mDNS\/IPv6\r\n         Protocols: +DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\nCurrent DNS Server: 192.168.12.2\r\n       DNS Servers: 192.168.12.2\r\n        DNS Domain: localdomain\r\n\r\nLink 3 (docker0)\r\n    Current Scopes: none\r\n         Protocols: -DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n\r\nLink 134 (br-3f6e4658ec31)\r\n    Current Scopes: none\r\n         Protocols: -DefaultRoute +LLMNR +mDNS -DNSOverTLS DNSSEC=no\/unsupported\r\n\r\n\r\n# Correct (_at least Docker is copying a freshly updated symlink file_):\r\n$ docker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER      \r\ndocker.com.     A       IN      5s      141.193.213.20  192.168.12.2:53\r\ndocker.com.     A       IN      5s      141.193.213.21  192.168.12.2:53\r\n\r\n# Temporarily modify the default-route (ens33) to use a different DNS service,\r\n# Docker default bridge should use this one instead:\r\n$ resolvectl dns ens33 1.1.1.1\r\n\r\n# Config is updated as we requested, that should reflect in a Docker container:\r\n$ cat \/run\/systemd\/resolve\/resolv.conf\r\nnameserver 1.1.1.1\r\nsearch localdomain\r\n\r\n# Correct:\r\n$ docker run --rm -it ghcr.io\/mr-karan\/doggo docker.com\r\nNAME            TYPE    CLASS   TTL     ADDRESS         NAMESERVER \r\ndocker.com.     A       IN      33s     141.193.213.21  1.1.1.1:53\r\ndocker.com.     A       IN      33s     141.193.213.20  1.1.1.1:53\r\n```\r\n\r\nObviously, this won't cover containers already running. and that isn't feasible for reasons already given in prior comments.\r\n\r\n<\/details>\r\n\r\n---\r\n\r\n## Helpful resources\r\n\r\n<details>\r\n<summary>Click to expand<\/summary>\r\n\r\nArchWiki is usually a pretty good resource on system configuration. This is just a quick link + image dump if helpful for reference.\r\n\r\n### DNS (`systemd-resolved`)\r\n\r\nhttps:\/\/wiki.archlinux.org\/title\/Systemd-resolved#DNS\r\n\r\n<details>\r\n<summary>Click to show images<\/summary>\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/6da04c36-859e-4d79-95b0-c171f172812d)\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/e31caa86-37d6-4e85-942b-da7d43abec55)\r\n\r\n<\/details>\r\n\r\n### NetworkManager\r\n\r\nThis is common to have involved in the mix (_on a linux desktop at least_): https:\/\/wiki.archlinux.org\/title\/NetworkManager#DNS_caching_and_conditional_forwarding\r\n\r\n<details>\r\n<summary>Click to show images<\/summary>\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/dac856cc-fdca-4d21-9ed8-07495937ddfa)\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/fc63d3ac-ae76-4ffc-9cda-3a5466a80800)\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/a579000b-40c0-4dde-94d4-ffdc2440ff73)\r\n\r\n<\/details>\r\n\r\n### NSS and glibc (_shouldn't be a concern I think?_)\r\n\r\nhttps:\/\/wiki.archlinux.org\/title\/Domain_name_resolution\r\n\r\n<details>\r\n<summary>Click to show images<\/summary>\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/44a24a6b-8fae-4e57-8cfb-bcb724b1c15b)\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/5098581\/fe619064-b50f-4c83-a3d1-ff7b0633c3ed)\r\n\r\n<\/details>\r\n\r\n<\/details>\r\n","> `resolv.conf` can be maintained by something else (like NetworManager) and then `systemd-resolved` can act as a consumer of that file rather than managing it, and that is how I think its setup on my system.\r\n\r\n> I dont remember how or why I configured it that way but its been working perfectly for me for a while and it is not really something wrong, just not the recommended way.\r\n\r\nMy system was using NetworkManager, but `systemd-resolved` was not active. I've since enabled it, and this doesn't seem to influence `\/etc\/resolv.conf` in anyway which Docker cares about. I get the impression you'd have this problem without `systemd-resolved` involved too.\r\n\r\nWhen the DNS changes and NetworkManager updates that in `\/etc\/resolv.conf`, Docker with `--network=host` still keeps the old version that the container was started with. I don't think Docker presently supports carrying that update over (_I misunderstood and thought that was supported if the container copy was known not to be modified, that it'd be detected and synced_).\r\n\r\nThe fix for you is to symlink `\/etc\/resolv.conf` to the `systemd-resolved` stub resolver config:\r\n\r\n```bash\r\n# Configure \/etc\/resolv.conf to symlink to the systemd-resolved stub config:\r\n$ sudo ln -sf \/run\/systemd\/resolve\/stub-resolv.conf \/etc\/resolv.conf\r\n# Might be required:\r\n$ sudo systemctl restart systemd-resolved\r\n\r\n# Restart NM so it doesn't think it manages \/etc\/resolv.conf anymore:\r\n$ sudo systemctl restart NetworkManager\r\n# Restart the Docker daemon, it presumably optimizes by caching the \/etc\/resolv.conf check\r\n$ sudo systemctl restart docker\r\n```\r\n\r\n---\r\n\r\nPerhaps this could be covered better in the [Docker docs Networking DNS section](https:\/\/docs.docker.com\/network\/#dns-services)?","> I get the impression you'd have this problem without systemd-resolved involved too.\r\n\r\nI'm getting the same behavior testing on a host with nothing of the fancy stuff like `systemd-resolved` or `NetworkManager` enabled, just updating `\/etc\/resolv.conf` by hand...\r\n\r\nBy my tests, if I spun up a container on the `bridge` or `host` network, then change the host `\/etc\/resolv.conf` to something else, it will just update on the container if I restart the container, restarting the daemon doesn't have any effect, we can easily verify this with the following commands\r\n\r\n<details>\r\n  <summary>test with `host` network - `bridge` has the same effect<\/summary>\r\n\r\n    $  echo 'nameserver 8.8.8.8' | sudo tee \/etc\/resolv.conf\r\n    nameserver 8.8.8.8\r\n    $  docker run -d --name before-change-host-ns --network host wbitt\/network-multitool\r\n    a67ca8cfc2482e49c7040a0b9621c5cd27ed253336892509db83ace9ef63562e\r\n    $  docker exec before-change-host-ns cat \/etc\/resolv.conf\r\n    nameserver 8.8.8.8\r\n    $  echo 'nameserver 8.8.4.4' | sudo tee \/etc\/resolv.conf\r\n    nameserver 8.8.4.4 # DNS changed at host level\r\n    $  docker exec before-change-host-ns cat \/etc\/resolv.conf\r\n    nameserver 8.8.8.8 # DNS still the same on container\r\n    $  sudo systemctl restart docker\r\n    $  docker exec before-change-host-ns cat \/etc\/resolv.conf\r\n    nameserver 8.8.8.8 # don't have any effect if restarting only\r\n    $  docker restart before-change-host-ns\r\n    before-change-host-ns\r\n    $  docker exec before-change-host-ns cat \/etc\/resolv.conf\r\n    nameserver 8.8.4.4\r\n    $  docker stop before-change-host-ns\r\n    before-change-host-ns\r\n    $  docker rm before-change-host-ns\r\n    before-change-host-ns\r\n<\/details>\r\n\r\nas [Docker docs Networking DNS section](https:\/\/docs.docker.com\/network\/#dns-services) for the `bridge` network states:\r\n\r\n> By default, containers inherit the DNS settings of the host, as defined in the \/etc\/resolv.conf configuration file. Containers that attach to the default bridge network receive a copy of this file.\r\n\r\n\r\nand a [previous comment](https:\/\/github.com\/moby\/moby\/issues\/46199#issuecomment-1710567879) regarding `host` network \r\n\r\n> So, for these reasons, we (again) need a COPY of the host's \/etc\/resolv.conf (or whatever that's symlinked to) for each container, and make sure that\r\n\r\n\r\nso, we should assume that this is the expected behavior and changes made to `\/etc\/resolv.conf` on the host, a container restart is needed for those using `host` or `bridge` networks? as both need to COPY the file again :thinking: \r\n"],"labels":["kind\/enhancement","area\/networking","area\/networking\/d\/bridge","area\/networking\/dns","version\/24.0"]},{"title":"Mixing up x86_64 with i386\/x86","body":"### Description\n\nWhen using `docker run` or `docker build` with `--platform=linux\/i386` followed by another such command but with `--platform=linux\/x86_64`, the second command reuses the `i386` base image rather switching to an `x86_64` base image. `uname -m` still reports `x86_64` which mismatches package managers' repository configurations leading to some rather bizarre install errors.\n\n### Reproduce\n\n```fish\r\n> docker pull --platform=linux\/i386 alpine:3.18\r\n3.18: Pulling from library\/alpine\r\nDigest: sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a\r\nStatus: Image is up to date for alpine:3.18\r\ndocker.io\/library\/alpine:3.18\r\n\r\n> docker run --platform=linux\/x86_64 alpine:3.18 cat \/etc\/apk\/arch\r\nx86  # <- Should have used an x86_64 image\r\n```\r\n\n\n### Expected behavior\n\nDocker should see that the current `x86` Alpine image is not an `x86_64` image and pull the correct image. This is how Docker handles seemingly any other variant of the above (where the `x86` and `x86_64` are swapped or replaced with any other architectures) correctly.\r\n\r\n```fish\r\n> docker pull --platform=linux\/x86_64 alpine:3.18\r\n3.18: Pulling from library\/alpine\r\nDigest: sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a\r\nStatus: Downloaded newer image for alpine:3.18\r\ndocker.io\/library\/alpine:3.18\r\n\r\n> docker run --platform=linux\/i386 alpine:3.18 cat \/etc\/apk\/arch\r\nUnable to find image 'alpine:3.18' locally\r\n3.18: Pulling from library\/alpine\r\nDigest: sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a\r\nStatus: Downloaded newer image for alpine:3.18\r\nx86\r\n```\r\n\n\n### docker version\n\n```bash\nClient:\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfcd85\r\n Built:             Mon May 29 15:50:06 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f9ee\r\n  Built:            Mon May 29 15:50:06 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.2\r\n  GitCommit:        0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n\r\nServer:\r\n Containers: 20\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 18\r\n Images: 57\r\n Server Version: 24.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: true\r\n  Native Overlay Diff: false\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.4.3-1-MANJARO\r\n Operating System: Manjaro Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 7.494GiB\r\n Name: manjaro-2212\r\n ID: HBWP:AGRX:MHCB:XEVM:37M2:Z4PF:37RF:GDXA:7KP4:DERV:2JNI:XGKR\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"Can't use swap memory on docker - Rasberry pi os","body":"### Description\n\nHello, I have a problem with docker : every time I launch docker I have this message :\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\nI searched in internet but found nothing to help me with it because most of it are made for ubuntu and ask for \/etc\/default\/grub file but it doesn't exist on my OS. Please help me I have this problem since 1 month and I didn't \n\n### Reproduce\n\nadmin@XXX : ~$ docker info\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 9\r\n Images: 6\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.42-v8+\r\n Operating System: Debian GNU\/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 3.705GiB\r\n Name: TechCraft\r\n ID: 548e5130-6d81-4263-abdb-de39f706ec29\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\r\n\n\n### Expected behavior\n\nNormally I will not have the two last lines\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:38 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:38 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 10\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 9\r\n Images: 6\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.42-v8+\r\n Operating System: Debian GNU\/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 3.705GiB\r\n Name: TechCraft\r\n ID: 548e5130-6d81-4263-abdb-de39f706ec29\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: No memory limit support\r\nWARNING: No swap limit support\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug"]},{"title":"swarm:Two nodes have the same node ID","body":"### Description\r\n\r\nIn a cluster environment, Docker19 is upgraded to 23.0.3,Two nodes have the same node ID\r\n\r\n### Reproduce\r\n\r\n```[root@X]# ip a s eth0\r\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc htb state UP group default qlen 1000\r\n    link\/ether 52:54:00:fd:aa:47 brd ff:ff:ff:ff:ff:ff\r\n    inet 9.88.11.147\/24 brd 9.88.11.255 scope global dynamic eth0\r\n       valid_lft 71317sec preferred_lft 71317sec\r\n    inet6 fe80::5054:ff:fefd:aa47\/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n[root@X]# docker node ls\r\nID                            HOSTNAME         STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\n4rwvq8ysxkc8g90vds6zb49eb *   X                Ready     Active         Reachable        23.0.3\r\ngkq1ds6yplhxwyqcl3p3d1gbr     X                Ready     Active         Leader           23.0.3\r\nj6lju21jatgb9suqx57fv2u9m     X                Down      Active         Reachable        23.0.3\r\nu3x3c5ty50uuoxlwrvlrvuzro     X                Ready     Active                          23.0.3\r\n```\r\n\r\n```\r\n[root@X]# ip a s eth0\r\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc htb state UP group default qlen 1000\r\n    link\/ether 52:54:00:fd:aa:48 brd ff:ff:ff:ff:ff:ff\r\n    inet 9.88.11.148\/24 brd 9.88.11.255 scope global dynamic eth0\r\n       valid_lft 67024sec preferred_lft 67024sec\r\n    inet6 fe80::5054:ff:fefd:aa48\/64 scope link \r\n       valid_lft forever preferred_lft forever\r\n[root@X]# docker node ls\r\nID                            HOSTNAME         STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\n4rwvq8ysxkc8g90vds6zb49eb *   X                Ready     Active         Reachable        23.0.3\r\ngkq1ds6yplhxwyqcl3p3d1gbr     X                Ready     Active         Leader           23.0.3\r\nj6lju21jatgb9suqx57fv2u9m     X                Down      Active         Reachable        23.0.3\r\nu3x3c5ty50uuoxlwrvlrvuzro     X                Ready     Active                          23.0.3\r\n```\r\n\r\n```\r\n[root@X]# docker node inspect 4rwvq8ysxkc8g90vds6zb49eb |tail -n 10\r\n        \"Status\": {\r\n            \"State\": \"ready\",\r\n            \"Addr\": \"9.88.11.147\"\r\n        },\r\n        \"ManagerStatus\": {\r\n            \"Reachability\": \"reachable\",\r\n            \"Addr\": \"9.88.11.147:2377\"\r\n        }\r\n    }\r\n]\r\n[root@X]# docker node inspect 4rwvq8ysxkc8g90vds6zb49eb |tail -n 10\r\n        \"Status\": {\r\n            \"State\": \"ready\",\r\n            \"Addr\": \"9.88.11.148\"\r\n        },\r\n        \"ManagerStatus\": {\r\n            \"Reachability\": \"reachable\",\r\n            \"Addr\": \"9.88.11.147:2377\"\r\n        }\r\n    }\r\n]\r\n\r\n\r\n\r\n[root@X]# docker node inspect j6lju21jatgb9suqx57fv2u9m |tail -n 10\r\n            \"State\": \"down\",\r\n            \"Message\": \"heartbeat failure\",\r\n            \"Addr\": \"9.88.11.148\"\r\n        },\r\n        \"ManagerStatus\": {\r\n            \"Reachability\": \"reachable\",\r\n            \"Addr\": \"9.88.11.148:2377\"\r\n        }\r\n    }\r\n]\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\n_No response_\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: X\r\n Version:           23.0.3\r\n API version:       1.42 (downgraded from 1.43)\r\n Go version:        go1.19.6\r\n Git commit:        \r\n Built:             Mon Aug  7 16:46:00 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: X\r\n Engine:\r\n  Version:          23.0.3\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.19.6\r\n  Git commit:       \r\n  Built:            Mon Aug  7 16:46:00 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.16\r\n  GitCommit:        .m\r\n runc:\r\n  Version:          1.1.3\r\n  GitCommit:        03ac17e4797bea32893c0a26c1996f8b2697a1af\r\n docker-init:\r\n  Version:          0.13.0\r\n  GitCommit:\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: X\r\n Version:    23.0.3\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 175\r\n  Running: 30\r\n  Paused: 0\r\n  Stopped: 145\r\n Images: 37\r\n Server Version: 23.0.3\r\n Storage Driver: devicemapper\r\n  Pool Name: docker-253:0-269385-pool\r\n  Pool Blocksize: 65.54kB\r\n  Base Device Size: 32.21GB\r\n  Backing Filesystem: ext4\r\n  Udev Sync Supported: true\r\n  Data file: \/dev\/X\/data\r\n  Metadata file: \/dev\/X\/metadata\r\n  Data Space Used: 3.829GB\r\n  Data Space Total: 42.95GB\r\n  Data Space Available: 39.12GB\r\n  Metadata Space Used: 27.72MB\r\n  Metadata Space Total: 2.147GB\r\n  Metadata Space Available: 2.12GB\r\n  Thin Pool Minimum Free Space: 4.295GB\r\n  Deferred Removal Enabled: true\r\n  Deferred Deletion Enabled: true\r\n  Deferred Deleted Device Count: 0\r\n  Library Version: 1.02.150 (2018-08-01)\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: 4rwvq8ysxkc8g90vds6zb49eb\r\n  Is Manager: true\r\n  ClusterID: slfteid5ex7q30h82ziu4372q\r\n  Managers: 3\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 15 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 100 years\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 9.88.11.147\r\n  Manager Addresses:\r\n   9.88.11.147:2377\r\n   9.88.11.148:2377\r\n   9.88.11.149:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: .m\r\n runc version: 03ac17e4797bea32893c0a26c1996f8b2697a1af\r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: X\r\n Operating System: X2.0 (SP8)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.118GiB\r\n Name: X\r\n ID: ZQ6O:7PAW:NXMR:O46C:E2WY:Z32O:LHOM:SBJD:Q4E7:3BZS:4K6T:CW6H\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["cc @dperny ","You've obfuscated important information like the kernel uname and containerd version in `docker info` -- can you please provide the unredacted output?","Posting this here again, as #46462 is closed:\r\n\r\nI intermittently had this issue as well on a swarm cluster on Hetzner Cloud. Only ever happened once so far. I originally suspected the upgrade scripts I used to be the problem because I upgraded the nodes too fast in succession so that it might cause problems with raft. But I am not sure if that is the case - as I don't know enough about the internals here.","I have seen this same issue multiple times on several different clusters. It is not repeatable 100% and I do not know what is the trigger condition. It happens perhaps 10-20% of updates to a cluster or even a single-digit %.  \r\nSeveral clusters running identical versions (both Docker, OS) could be updated just fine until one of the clusters has this issue. And updates might work just fine on all clusters for months until the exact same thing happens again.\r\n\r\nFirst time probably around version 20.10 but even that is only because I do not have data for older versions. Latest occurrence was 25.0.1 while updating to 25.0.2.\r\n\r\nAlways the same thing:\r\n\r\n1. Node is drained.\r\n2. Upgrade packages, restart system.\r\n3. Activate node.\r\n\r\nAnd then in `docker node ls` one manager node will start to flip-flop every few seconds between it's \"real\" hostname and duplicating the hostname of another manager.  \r\nThe 2 nodes are distinguishable by their node ID only that does not change - all other information like hostname & version of the \"problem\" node will take over the information of another node. And a few seconds later the problem node shows again with its \"real\" information (hostname, version, etc.). And repeat that forever.  \r\nUnfortunately the OP has clumsily \"redacted\" the hostnames in the `docker node ls` listing so it could be not said if this is the exact same thing. IMHO the *key* information of what is happening was lost there. As it is clear the 2 listings *do* show different node IDs and no duplicates; I believe he was trying to say they have duplicated *hostnames*.  \r\nRestarting swarm on the \"problem\" node does not help.\r\n\r\nMore version information since it was asked but not answered, from the latest occurrence:\r\n\r\nEL7, 3.10.0-1160.105.1.el7.x86_64 - but has happened on 2-3 years old kernel as well, and on versions between.\r\n\r\n```console\r\ncontainerd.io-1.6.28-3.1.el7.x86_64\r\ndocker-ce-25.0.2-1.el7.x86_64\r\n```\r\nAnd on 1 manager node:\r\n```console\r\ncontainerd.io-1.6.27-3.1.el7.x86_64\r\ndocker-ce-25.0.1-1.el7.x86_64\r\n```\r\n\r\nBut as I listed above it has happened several times over 2-3 years so on many different versions.","It has not happened for a while on our clusters, but I think I remember that it even had a duplicated ID in my case. Could be bad memory, though.\r\n\r\nI think this will be really hard to reproduce, so maybe some guidance from one of the maintainers would be great. Would be awesome if we could reproduce this with an automated VM setup, e.g. with Vagrant.","> It has not happened for a while on our clusters, but I think I remember that it even had a duplicated ID in my case. Could be bad memory, though.\r\n\r\nPossibly but I cannot remember. And for the latest 25.0.1->25.0.2 problem the results are below. For me the IDs in `docker node ls` stay correct (??) and it is all the other information that keeps flip-flopping between the true information and information of another node. Example:\r\n```console\r\n# docker node ls ; sleep 5 ; docker node ls\r\nID        HOSTNAME                 STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\nxx...     man01                    Ready     Active         Reachable        25.0.1\r\nm0...     man01                    Down      Active         Reachable        25.0.1\r\nij... *   man02                    Ready     Active         Leader           25.0.2\r\nzh...     wor01                    Ready     Active                          25.0.1\r\nco...     wor02                    Ready     Active                          25.0.2\r\n...(more workers snip)\r\nID        HOSTNAME                 STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\nm0...     man01                    Down      Active         Reachable        25.0.1\r\nij... *   man02                    Ready     Active         Leader           25.0.2\r\nxx...     man03                    Ready     Active         Reachable        25.0.2\r\nzh...     wor01                    Ready     Active                          25.0.1\r\nco...     wor02                    Ready     Active                          25.0.2\r\n...(more workers snip)\r\n``` \r\n\r\nLook at the IDs and compare the HOSTNAME & VERSION for them between the 2 listings. You see that the first listing shows the \"wrong\" data where one node with ID `xx...` has taken over the data of another node `m0...`.  \r\nAlso the manager `m0...` is listed as \"Down\" but still \"Reachable\" - even though it should be working I do not think anything was done to this node in days.  \r\nThis started after node `xx...`\/`man03` was upgraded. just a normal drain + update&reboot + activate same as every time nodes are updated&restarted.\r\n\r\n> I think this will be really hard to reproduce, so maybe some guidance from one of the maintainers would be great. Would be awesome if we could reproduce this with an automated VM setup, e.g. with Vagrant.\r\n\r\nPossibly. But if I just calculate the possibility of seeing this based on the update+restart cycles done over the various clusters I'm getting a number in the magnitude 5-20% so not hugely improbable and indeed I've seen it several times.","Can you check the output of all nodes from the view of all manager nodes? I remember that part of the cluster was seeing it one way, the other part of the cluster another.","> Can you check the output of all nodes from the view of all manager nodes? I remember that part of the cluster was seeing it one way, the other part of the cluster another.\r\n\r\nUnfortunately I cannot."],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/23.0"]},{"title":"\"Filesystem layer verification failed for digest\" for Ubuntu installation of Docker Engine","body":"### Description\n\nI'm running docker on Ubuntu 22.04 (CLI Engine, not Docker-Desktop) and I keep getting layer verification error.\r\nI've tested this command multiple times. This issue occurs on different layers (although some tend to have more occurrences - seems that bigger layers have less chance of success) and once in a while everything works. But on other images (i.e. `mcr.microsoft.com\/mssql\/server:2019-CU18-ubuntu-20.04`) I wasn't able to successfully finish the pull.\r\n\r\n\r\nIt's now a fresh installation. I've completely removed Docker along with its configs and reinstalled it all. `\/var\/lib\/docker` has been removed, so were `\/etc\/docker\/daemon.json` and `~\/.docker\/config.json`. I don't use proxy, the internet connection is stable, and this is not a VM - it's a native Ubuntu installation. I've tried docker-desktop and there everything is fine, so it's not a repository issue nor a connection one. Those pulls are not from any private repository, nor are those images built by me.\r\n\r\n\r\nThe following two commands were run immediately one after another:\r\n\r\n```\r\nmjozwikowski:~$ docker -D pull postgres:latest\r\nlatest: Pulling from library\/postgres\r\n648e0aadf75a: Downloading [==========================>                        ]  15.44MB\/29.12MB\r\nf715c8c55756: Download complete \r\nb11a1dc32c8c: Downloading  2.693MB\r\nf29e8ba9d17c: Verifying Checksum \r\n78af88a8afb0: Waiting \r\nb74279c188d9: Waiting \r\n6e3e5bf64fd2: Waiting \r\nb62a2c2d2ce5: Waiting \r\neba91ca3c7a3: Waiting \r\nd4a24cdf2433: Waiting \r\nb20f8a8dfd5c: Waiting \r\ne0731dd084c3: Waiting \r\n0361da6a228e: Waiting \r\nfilesystem layer verification failed for digest sha256:f29e8ba9d17cfa147141648b72ff8ab49a86234dfe1194f6220690939f1daa3c\r\n\r\nmjozwikowski:~$ docker -D pull postgres:latest\r\nlatest: Pulling from library\/postgres\r\n648e0aadf75a: Pull complete \r\nf715c8c55756: Pull complete \r\nb11a1dc32c8c: Pull complete \r\nf29e8ba9d17c: Pull complete \r\n78af88a8afb0: Pull complete \r\nb74279c188d9: Pull complete \r\n6e3e5bf64fd2: Pull complete \r\nb62a2c2d2ce5: Pull complete \r\neba91ca3c7a3: Verifying Checksum \r\nd4a24cdf2433: Download complete \r\nb20f8a8dfd5c: Download complete \r\ne0731dd084c3: Download complete \r\n0361da6a228e: Download complete \r\nfilesystem layer verification failed for digest sha256:eba91ca3c7a37844775569d1771c8acfab80b32d9c24f4a0b5b998d91911d747\r\n```\n\n### Reproduce\n\n1. docker -D pull postgres:latest\n\n### Expected behavior\n\n`docker pull` should complete without any issues\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:18 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 1\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.2.0-26-generic\r\n Operating System: Ubuntu 22.04.3 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 42.93GiB\r\n Name: Company\r\n ID: 8c0696ec-411e-479d-91b8-82be613dda2f\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n```\r\nmjozwikowski:~$ sudo service docker restart\r\nmjozwikowski:~$ docker pull postgres\r\nUsing default tag: latest\r\nlatest: Pulling from library\/postgres\r\n648e0aadf75a: Pull complete \r\nf715c8c55756: Pull complete \r\nb11a1dc32c8c: Pull complete \r\nf29e8ba9d17c: Pull complete \r\n78af88a8afb0: Pull complete \r\nb74279c188d9: Pull complete \r\n6e3e5bf64fd2: Pull complete \r\nb62a2c2d2ce5: Pull complete \r\neba91ca3c7a3: Verifying Checksum \r\nd4a24cdf2433: Download complete \r\nb20f8a8dfd5c: Download complete \r\ne0731dd084c3: Download complete \r\n0361da6a228e: Download complete \r\nfilesystem layer verification failed for digest sha256:eba91ca3c7a37844775569d1771c8acfab80b32d9c24f4a0b5b998d91911d747\r\n```\r\n\r\nResulted in following entries in `journalctl`\r\n```\r\nsie 06 11:33:01 sudo[10242]: mjozwikowski : TTY=pts\/0 ; PWD=\/home\/mjozwikowski ; USER=root ; COMMAND=\/usr\/sbin\/service docker restart\r\nsie 06 11:33:01 dockerd[9480]: time=\"2023-08-06T11:33:01.889463320+02:00\" level=info msg=\"Processing signal 'terminated'\"\r\nsie 06 11:33:01 dockerd[9480]: time=\"2023-08-06T11:33:01.890225881+02:00\" level=info msg=\"stopping event stream following graceful shutdown\" error=\"<nil>\" module=libcontainerd namespace=moby\r\nsie 06 11:33:01 dockerd[9480]: time=\"2023-08-06T11:33:01.890540576+02:00\" level=info msg=\"Daemon shutdown complete\"\r\nsie 06 11:33:01 systemd[1]: docker.service: Deactivated successfully.\r\nsie 06 11:33:01 systemd[1]: docker.service: Consumed 5.232s CPU time.\r\nsie 06 11:33:01 dockerd[10248]: time=\"2023-08-06T11:33:01.990678853+02:00\" level=info msg=\"Starting up\"\r\nsie 06 11:33:01 dockerd[10248]: time=\"2023-08-06T11:33:01.991800333+02:00\" level=info msg=\"detected 127.0.0.53 nameserver, assuming systemd-resolved, so using resolv.conf: \/run\/systemd\/resolve\/resolv.conf\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.042684542+02:00\" level=info msg=\"[graphdriver] using prior storage driver: overlay2\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.043220655+02:00\" level=info msg=\"Loading containers: start.\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.709110191+02:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0\/16. Daemon option --bip can be used to set a preferred IP address\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.800643704+02:00\" level=info msg=\"Loading containers: done.\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.806029624+02:00\" level=info msg=\"Docker daemon\" commit=a61e2b4 graphdriver=overlay2 version=24.0.5\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.806068207+02:00\" level=info msg=\"Daemon has completed initialization\"\r\nsie 06 11:33:02 dockerd[10248]: time=\"2023-08-06T11:33:02.819571260+02:00\" level=info msg=\"API listen on \/run\/docker.sock\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.791852149+02:00\" level=error msg=\"filesystem layer verification failed for digest sha256:eba91ca3c7a37844775569d1771c8acfab80b32d9c24f4a0b5b998d91911d747\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.791895641+02:00\" level=error msg=\"Download failed after 1 attempts: filesystem layer verification failed for digest sha256:eba91ca3c7a37844775569d1771c8acfab80b32d9c24f4a0b5b998d91911d747\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.808820762+02:00\" level=info msg=\"Attempting next endpoint for pull after error: filesystem layer verification failed for digest sha256:eba91ca3c7a37844775569d1771c8acfab80b32d9c24f4a0b5b998d91911d747\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.810248872+02:00\" level=info msg=\"Layer sha256:e918add37af5039e3aa5c6fc505fbc0d7f3084997afdafe8cfa5cdc60794371f cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.811304838+02:00\" level=info msg=\"Layer sha256:63d0f06fce28434b01c759f220f44e6d18f3820db10e724f1100293d38698df0 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870937723+02:00\" level=info msg=\"Layer sha256:22de904444fbf7bb0f4598f30014ab426ed2e4038bb8eae707c5a503c33a4a72 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870958051+02:00\" level=info msg=\"Layer sha256:a067415fc97316aa484b04343164efd15f2de48522fa5ccf8ae33cfe7b91b2d4 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870962900+02:00\" level=info msg=\"Layer sha256:a3e038b9513aad18e436f7ed38244eabdb2eb15655212a48428dcf96af4aecf6 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870966437+02:00\" level=info msg=\"Layer sha256:1ae5138983888dd7346b42ca998c986e8f75113599395b89f2294652f9fb7e76 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870970124+02:00\" level=info msg=\"Layer sha256:d33250e6749e53e7280f627466421a672cfa59f10a66cd654830240d1387b816 cleaned up\"\r\nsie 06 11:33:18 dockerd[10248]: time=\"2023-08-06T11:33:18.870973831+02:00\" level=info msg=\"Layer sha256:c6e34807c2d51444c41c15f4fda65847faa2f43c9b4b976a2f6f476eca7429ce cleaned up\"\r\n```","comments":["This error would occur when validating the checksum of the extracted files doesn't match the expected checksum \ud83e\udd14. No direct clues, other than actual disk issues. \r\n\r\n- Is there anything special with `\/var\/lib\/docker`, e.g., could it be a separate mount for storage or something along those lines?\r\n- Is there any-virus \/ malware software running on the machine? I have seen some reports where such software was quarantining files, causing validation to fail.\r\n\r\n\r\n","No. `\/var\/lib` is part of `\/` which takes the whole `\/dev\/nvme0n1p5` formatted to `Ext4 (version 1.0)`.\r\n```\r\nmjozwikowski:~$ df -ha\r\ndf: \/run\/user\/1000\/doc: Operation not permitted\r\nFilesystem                   Size  Used Avail Use% Mounted on\r\nsysfs                           0     0     0    - \/sys\r\nproc                            0     0     0    - \/proc\r\nudev                          22G     0   22G   0% \/dev\r\ndevpts                          0     0     0    - \/dev\/pts\r\ntmpfs                        4,3G  3,0M  4,3G   1% \/run\r\n\/dev\/nvme0n1p5               359G  144G  197G  43% \/\r\nsecurityfs                      0     0     0    - \/sys\/kernel\/security\r\ntmpfs                         22G   51M   22G   1% \/dev\/shm\r\ntmpfs                        5,0M  4,0K  5,0M   1% \/run\/lock\r\ncgroup2                         0     0     0    - \/sys\/fs\/cgroup\r\npstore                          0     0     0    - \/sys\/fs\/pstore\r\nefivarfs                        0     0     0    - \/sys\/firmware\/efi\/efivars\r\nbpf                             0     0     0    - \/sys\/fs\/bpf\r\nsystemd-1                       -     -     -    - \/proc\/sys\/fs\/binfmt_misc\r\nmqueue                          0     0     0    - \/dev\/mqueue\r\nhugetlbfs                       0     0     0    - \/dev\/hugepages\r\ndebugfs                         0     0     0    - \/sys\/kernel\/debug\r\ntracefs                         0     0     0    - \/sys\/kernel\/tracing\r\nfusectl                         0     0     0    - \/sys\/fs\/fuse\/connections\r\nconfigfs                        0     0     0    - \/sys\/kernel\/config\r\nramfs                           0     0     0    - \/run\/credentials\/systemd-sysusers.service\r\n\/dev\/loop0                   128K  128K     0 100% \/snap\/bare\/5\r\n\/dev\/loop1                   119M  119M     0 100% \/snap\/core\/15419\r\n\/dev\/loop2                   119M  119M     0 100% \/snap\/core\/15511\r\n\/dev\/loop3                    56M   56M     0 100% \/snap\/core18\/2751\r\n\/dev\/loop4                    56M   56M     0 100% \/snap\/core18\/2785\r\n\/dev\/loop5                    64M   64M     0 100% \/snap\/core20\/1950\r\n\/dev\/loop7                    74M   74M     0 100% \/snap\/core22\/817\r\n\/dev\/loop6                    64M   64M     0 100% \/snap\/core20\/1974\r\n\/dev\/loop8                    74M   74M     0 100% \/snap\/core22\/858\r\n\/dev\/loop10                  238M  238M     0 100% \/snap\/firefox\/2971\r\n\/dev\/loop11                  219M  219M     0 100% \/snap\/gnome-3-34-1804\/90\r\n\/dev\/loop12                  219M  219M     0 100% \/snap\/gnome-3-34-1804\/93\r\n\/dev\/loop13                  350M  350M     0 100% \/snap\/gnome-3-38-2004\/140\r\n\/dev\/loop14                  350M  350M     0 100% \/snap\/gnome-3-38-2004\/143\r\n\/dev\/loop15                  467M  467M     0 100% \/snap\/gnome-42-2204\/111\r\n\/dev\/loop16                  486M  486M     0 100% \/snap\/gnome-42-2204\/120\r\n\/dev\/loop17                   82M   82M     0 100% \/snap\/gtk-common-themes\/1534\r\n\/dev\/loop18                   92M   92M     0 100% \/snap\/gtk-common-themes\/1535\r\n\/dev\/loop19                  9,8M  9,8M     0 100% \/snap\/htop\/3735\r\n\/dev\/loop20                  9,8M  9,8M     0 100% \/snap\/htop\/3758\r\n\/dev\/loop21                   38M   38M     0 100% \/snap\/hunspell-dictionaries-1-7-2004\/2\r\n\/dev\/loop22                  437M  437M     0 100% \/snap\/kde-frameworks-5-96-qt-5-15-5-core20\/7\r\n\/dev\/loop23                  261M  261M     0 100% \/snap\/kde-frameworks-5-core18\/32\r\n\/dev\/loop24                  290M  290M     0 100% \/snap\/kde-frameworks-5-core18\/35\r\n\/dev\/loop25                  449M  449M     0 100% \/snap\/kf5-5-104-qt-5-15-8-core22\/7\r\n\/dev\/loop26                  449M  449M     0 100% \/snap\/kf5-5-104-qt-5-15-8-core22\/9\r\n\/dev\/loop27                  253M  253M     0 100% \/snap\/krita\/85\r\n\/dev\/loop28                  253M  253M     0 100% \/snap\/krita\/90\r\n\/dev\/loop29                  113M  113M     0 100% \/snap\/slack\/82\r\n\/dev\/loop30                  114M  114M     0 100% \/snap\/slack\/83\r\n\/dev\/loop31                   46M   46M     0 100% \/snap\/snap-store\/638\r\n\/dev\/loop32                   13M   13M     0 100% \/snap\/snap-store\/959\r\n\/dev\/loop33                   54M   54M     0 100% \/snap\/snapd\/19361\r\n\/dev\/loop34                   54M   54M     0 100% \/snap\/snapd\/19457\r\n\/dev\/loop35                  512K  512K     0 100% \/snap\/snapd-desktop-integration\/57\r\n\/dev\/loop36                  512K  512K     0 100% \/snap\/snapd-desktop-integration\/83\r\n\/dev\/loop37                   66M   66M     0 100% \/snap\/sublime-text\/118\r\n\/dev\/loop38                   64M   64M     0 100% \/snap\/sublime-text\/122\r\n\/dev\/nvme0n1p5               359G  144G  197G  43% \/var\/snap\/firefox\/common\/host-hunspell\r\n\/dev\/loop39                  254M  254M     0 100% \/snap\/subsync\/11\r\n\/dev\/nvme0n1p1               256M   72M  185M  29% \/boot\/efi\r\ntmpfs                         22G     0   22G   0% \/run\/qemu\r\nbinfmt_misc                     0     0     0    - \/proc\/sys\/fs\/binfmt_misc\r\nsunrpc                          0     0     0    - \/run\/rpc_pipefs\r\ntmpfs                        4,3G  3,0M  4,3G   1% \/run\/snapd\/ns\r\nnsfs                            0     0     0    - \/run\/snapd\/ns\/snapd-desktop-integration.mnt\r\ntmpfs                        4,3G  136K  4,3G   1% \/run\/user\/1000\r\n\/home\/mjozwikowski\/.Private  359G  144G  197G  43% \/home\/mjozwikowski\r\nnsfs                            0     0     0    - \/run\/snapd\/ns\/snap-store.mnt\r\n\/dev\/loop41                  238M  238M     0 100% \/snap\/firefox\/2987\r\nnsfs                            0     0     0    - \/run\/snapd\/ns\/firefox.mnt\r\n```\r\nAnd the whole drive seems to be in good condition:\r\n```\r\nmjozwikowski:~$ sudo nvme smart-log \/dev\/nvme0\r\nSmart Log for NVME device:nvme0 namespace-id:ffffffff\r\ncritical_warning\t\t\t: 0\r\ntemperature\t\t\t\t: 24 C (297 Kelvin)\r\navailable_spare\t\t\t\t: 100%\r\navailable_spare_threshold\t\t: 10%\r\npercentage_used\t\t\t\t: 3%\r\nendurance group critical warning summary: 0\r\ndata_units_read\t\t\t\t: 18\u202f308\u202f944\r\ndata_units_written\t\t\t: 25\u202f745\u202f218\r\nhost_read_commands\t\t\t: 264\u202f878\u202f067\r\nhost_write_commands\t\t\t: 429\u202f459\u202f124\r\ncontroller_busy_time\t\t\t: 4\u202f131\r\npower_cycles\t\t\t\t: 1\u202f547\r\npower_on_hours\t\t\t\t: 3\u202f531\r\nunsafe_shutdowns\t\t\t: 27\r\nmedia_errors\t\t\t\t: 0\r\nnum_err_log_entries\t\t\t: 0\r\nWarning Temperature Time\t\t: 0\r\nCritical Composite Temperature Time\t: 0\r\nThermal Management T1 Trans Count\t: 0\r\nThermal Management T2 Trans Count\t: 0\r\nThermal Management T1 Total Time\t: 0\r\nThermal Management T2 Total Time\t: 0\r\n```\r\n\r\nNo, I don't have any antivirus\/antimalware software installed. Unless something is one and I'm not aware...\r\nAt least I didn't until today. Installed clamav, and did a full scan - nothing found.\r\n\r\nWould there be a way to enable some more detailed logs? I'd gladly send you all the details I could get but I don't know what exactly to do."],"labels":["status\/0-triage","status\/more-info-needed","kind\/bug","version\/24.0"]},{"title":"Containers unreachable when they are connected by multiple networks and some of them have icc=false","body":"### Description\n\nWhen two containers *a* and *b* are attached to more than one network, for example an uplink network having `com.docker.network.bridge.enable_icc=false` and another internal network, then *a* can sometimes not reach *b* when the internal DNS server returns the IP address of *b* on the uplink network. However it is still technically possible for *a* to reach *b* by using a different IP address.\n\n### Reproduce\n\nRun `docker-compose up` with the following file:\r\n\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n  pinga:\r\n   image: arunvelsriram\/utils\r\n   networks:\r\n     - interconnect\r\n     - uplink\r\n   command: [\"bash\", \"-c\", \"sleep 2; ping pingb\"]\r\n\r\n  pingb:\r\n    image: arunvelsriram\/utils\r\n    networks:\r\n     - uplink\r\n     - interconnect\r\n    command: [\"bash\", \"-c\", \"ping 8.8.8.8; sleep 60\"]\r\n\r\n\r\nnetworks:\r\n  uplink:\r\n    driver: bridge\r\n    driver_opts:\r\n      com.docker.network.bridge.enable_icc: \"false\"\r\n      com.docker.network.bridge.enable_ip_masquerade: \"true\"\r\n  interconnect:\r\n    driver: bridge\r\n    driver_opts:\r\n      com.docker.network.bridge.enable_icc: \"true\"\r\n      com.docker.network.bridge.enable_ip_masquerade: \"false\"\r\n    internal: true\r\n```\r\n\r\nWe can now see that *b* is able to ping *8.8.8.8*, but *a* can't ping *b*.\n\n### Expected behavior\n\nI expect that I see *a* being able to ping *b* and *b* being able to ping *8.8.8.8*. However *a* can't ping *b* since the DNS server seems to return the IP address of *b* on the *uplink* network that doesn't supper *inter container connectivity*.\n\n### docker version\n\n```bash\nClient:\r\n Version:           20.10.25+dfsg1\r\n API version:       1.41\r\n Go version:        go1.20.6\r\n Git commit:        b82b9f3\r\n Built:             Mon Jul 17 08:24:02 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer:\r\n Engine:\r\n  Version:          20.10.25+dfsg1\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       5df983c\r\n  Built:            Mon Jul 17 08:24:02 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     true\r\n containerd:\r\n  Version:          1.6.20~ds1\r\n  GitCommit:        1.6.20~ds1-1+b1\r\n runc:\r\n  Version:          1.1.5+ds1\r\n  GitCommit:        1.1.5+ds1-1+b1\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 5\r\n Images: 96\r\n Server Version: 20.10.25+dfsg1\r\n Storage Driver: btrfs\r\n  Build Version: Btrfs v6.3.2\r\n  Library Version: 102\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1.6.20~ds1-1+b1\r\n runc version: 1.1.5+ds1-1+b1\r\n init version: \r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n  cgroupns\r\n Kernel Version: 6.4.0-1-amd64\r\n Operating System: Debian GNU\/Linux trixie\/sid\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 8\r\n Total Memory: 15.27GiB\r\n Name: yogi\r\n ID: G26P:K3SN:CG3S:F3QF:EWDO:HUV2:RSEB:RHG7:KVH2:CYKY:36PH:IOY4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: etews\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: true\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 10.10.0.0\/16, Size: 24\n```\n\n\n### Additional Info\n\n_No response_","comments":["Can you try with the latest Moby release, rather than the 20.10 Debian-built packages?  See installation information here: https:\/\/docs.docker.com\/engine\/install\/debian\/","I'm able to reproduce this issue with moby master.\r\n\r\n```\r\n$ sudo iptables-tracer -netns=\/var\/run\/docker\/netns\/d81724ff1856 -family ipv4 -filter=icmp\r\nIN=br-f3152cc5a549 OUT= SRC=172.25.0.2 DST=172.25.0.3 LEN=84 TOS=00 TTL=64 ID=28889 PROTO=ICMPv4 TYPE\/CODE=EchoRequest CSUM=e6ae \r\n\traw PREROUTING NFMARK=0x0 \r\n\t\tDEFAULT POLICY\r\n\t\t=> ACCEPT\r\n\tmangle PREROUTING NFMARK=0x0 \r\n\t\tDEFAULT POLICY\r\n\t\t=> ACCEPT\r\n\tnat PREROUTING NFMARK=0x0 \r\n\t\tDEFAULT POLICY\r\n\t\t=> ACCEPT\r\n\tmangle FORWARD NFMARK=0x0 OUT=br-f3152cc5a549 (changed by last rule)\r\n\t\tDEFAULT POLICY\r\n\t\t=> ACCEPT\r\n\tfilter FORWARD NFMARK=0x0 \r\n\t\tMATCH RULE (#1): -j DOCKER-USER\r\n\t\t=> DOCKER-USER\r\n\tfilter DOCKER-USER NFMARK=0x0 \r\n\t\t=> RETURN\r\n\tfilter FORWARD NFMARK=0x0 \r\n\t\tMATCH RULE (#2): -j DOCKER-ISOLATION-STAGE-1\r\n\t\t=> DOCKER-ISOLATION-STAGE-1\r\n\tfilter DOCKER-ISOLATION-STAGE-1 NFMARK=0x0 \r\n\t\t=> RETURN\r\n\tfilter FORWARD NFMARK=0x0 \r\n\t\tMATCH RULE (#4): -o br-f3152cc5a549 -j DOCKER\r\n\t\t=> DOCKER\r\n\tfilter DOCKER NFMARK=0x0 \r\n\t\t=> RETURN\r\n\tfilter FORWARD NFMARK=0x0 \r\n\t\tMATCH RULE (#31): -i br-f3152cc5a549 -o br-f3152cc5a549 -j DROP\r\n\t\t=> DROP\r\n```\r\n\r\nThe input interface `br-f3152cc5a549` is the bridge created for the `uplink` network:\r\n\r\n```\r\n$ docker network ls\r\nNETWORK ID     NAME                       DRIVER    SCOPE\r\n7b772858ca20   issue-46162_interconnect   bridge    local\r\nf3152cc5a549   issue-46162_uplink         bridge    local\r\n```\r\n\r\nResolving `pingb` from `pinga` shows that the embedded DNS resolver returns the IP address of `pingb` on the uplink network:\r\n\r\n```\r\n$ docker exec issue-46162-pinga-1 dig +short pingb\r\n172.25.0.3\r\n$ docker network inspect --format='{{ (index .IPAM.Config 0).Subnet }}' issue-46162_uplink\r\n172.25.0.0\/16\r\n```\r\n\r\nI think the embedded DNS resolver should skip networks which have `icc=false`.","Thank you for reproducing my bug report!\r\n\r\n> I think the embedded DNS resolver should skip networks which have icc=false.\r\n\r\nThat sounds like a good solution, or at least skip it when there are other networks available that support icc.\r\n\r\nFor the meantime, is there maybe a good workaround? Maybe something like `ping pingb.interconnect` or so, but I don't know how I can reliably determine the hostname of *pingb* on *interconnect* when the network was created with *docker-compose*. As far as I know, the *FQDN* of *pingb* then depends on the directory name of the directory the compose-file is located in.","Yeah, you can use the container's FQDN as a workaround. To do this, you can use the env var `${COMPOSE_PROJECT_NAME}` in your `docker-compose.yml` (see [here](https:\/\/docs.docker.com\/compose\/environment-variables\/envvars\/#compose_project_name)):\r\n\r\n```\r\nservices:\r\n  pinga:\r\n   command: [\"bash\", \"-c\", \"sleep 2; ping pingb.${COMPOSE_PROJECT_NAME}_interconnect\"]\r\n```"],"labels":["kind\/bug","area\/networking","version\/20.10","area\/networking\/dns","version\/24.0"]},{"title":"WIP: libnetwork: remove unused networkdb table types","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["status\/2-code-review","area\/networking","kind\/refactor"]},{"title":"Failure to create correct firewall rules under Debian 12 (\"Bookworm\")","body":"### Description\r\n\r\nDocker does create the `DOCKER-USER` chain along with a jump rule at the top of the `FORWARD` chain to the aforementioned chain. This does work as expected under Debian 11 and under Debian 12 as long as the `DOCKER-USER` chain does not yet exist when Docker is being started.\r\n\r\nHowever, if the `DOCKER-USER` chain already does exist, e.g. because it has been manually created, the Docker services produces an error message during startup, indicating that the chain already exists.\r\n\r\n`level=warning msg=\"Failed to create DOCKER-USER IPV4 chain\" error=\"iptables failed: iptables -t filter -N DOCKER-USER: iptables: Chain already exists.\\n (exit status 1)\"`\r\n\r\nMore importantly, this leads to the knock-on effect that no jump rule at the top of the `FORWARD` chain is created, leading to a situation where the `DOCKER-USER` chain is effectively being ignored.\r\n\r\nUsing the same Docker version and configuration, this issue does not occur under Debian 11 even if the `DOCKER-USER` chain already exists, it only occurs under Debian 12 if the chain exists already.\r\n\r\n---\r\n\r\nThe most likely explanation seems to be different behaviour by at least one of the packages on the OS side to which Docker reacts slightly differently, and does not create the jump rule.\r\n\r\n### Reproduce\r\n\r\n1.  iptables -t filter -N DOCKER-USER\r\n2.  systemctl start docker.service\r\n3. nft list ruleset\r\n\r\n-> jump rule in `FORWARD` chain is missing\r\n\r\n\r\n### Expected behavior\r\n\r\nJump rule to `DOCKER-USER` at the top of the `FORWARD` chain exists.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:35 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:35 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/local\/lib\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 5\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 2\r\n Images: 8\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\r\n runc version: v1.1.8-0-g82f18fe\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-10-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 10\r\n Total Memory: 31.34GiB\r\n Name: <MY DOMAIN>\r\n ID: <MY ID>\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nDebian 12 versions:\r\n\r\n* debian_version: 12.1\r\n* iptables v1.8.9 (nf_tables)\r\n* nftables v1.0.6 (Lester Gooch # 5)\r\n\r\nDebian 11 versions:\r\n\r\n* debian_version: 11.7\r\n* iptables v1.8.7 (nf_tables)\r\n* nftables v0.9.8 (E.D.S.)\r\n","comments":["Hi @this-user,\r\n\r\nSince you're mentioning `iptables-nft` in the Additional Info section, could you try to switch to `iptables-legacy` with the following commands:\r\n\r\n```\r\n# Select the legacy option each time\r\n$ update-alternatives --config iptables\r\n$ update-alternatives --config ip6tables\r\n# Then:\r\n$ sudo systemctl restart docker.service\r\n```\r\n\r\nIf that doesn't work, could you please enable debug log (see https:\/\/docs.docker.com\/config\/daemon\/logs\/#enable-debugging), restart your daemon once more (the `kill -SIGHUP` mentioned in the docs won't help here) and then copy\/paste or upload a text file of what's logged by the daemon at startup?","I have tried switching the iptables backend. This does solve the issue of Docker producing an error during startup, even if a `DOCKER-USER` chain exists in the iptables backend. Unfortunately, this doesn't really help with doing what I am actually trying to do with that chain.\r\n\r\nWhat's more, Debian 11 has always been using the `iptables-nft` backend instead of the legacy one, and that has never been a problem. So the issue has to be that either the Docker build for Debian 12 is slightly different, even though it is the exact same version, or there are differences in the OS packages. But the difference in version between Debian 11 and 12 of the `iptables` and `nftables` packages are not huge.\r\n\r\nI have created the log, it is attached below. The key aspect there is the error message regarding the `DOCKER-USER` chain that already exists, which does not happen on Debian 11.\r\n\r\n`level=warning msg=\"Failed to create DOCKER-USER IPV4 chain\" error=\"iptables failed: iptables -t filter -N DOCKER-USER: iptables: Chain already exists.\\n (exit status 1)\"`\r\n\r\n[docker.log](https:\/\/github.com\/moby\/moby\/files\/12243830\/docker.log)\r\n\r\n---\r\n\r\nEDIT: There is also another issue early on where the daemon cannot determine whether iptables supports xlock. This seems to be an issue with the 1.8.9 version of that packages. Maybe this is related, and is what trips up Docker.\r\n\r\nhttps:\/\/bugzilla.netfilter.org\/show_bug.cgi?id=1632","I think I understand most of the issue. It's not the existence of the `DOCKER-USER` chain, it's the fact that I have a meta rule in that chain already. That rule causes an error with the `iptables` command in version 1.8.9 (and probably 1.8.8 too), 1.8.7 (which is what Deb 11 uses) is fine.\r\n\r\nThis error then trips up the startup of the Docker daemon, causing the first issue with being unable to determine the xlock capabilities. This, in turn, seems to change how Docker handles the rest of the FW initialisation, which then causes an error when it tries to create the already existing `DOCKER-USER` chain. What I do not fully understand is how this second issue connects to the first one, as the `iptables -t filter N DOCKER-USER` command does not produce an error even if the meta rule already exists in that chain. So it has to be something that is caused by the first issue.\r\n\r\nBelow is a log of starting the Docker daemon on Debian 12 with an existing but empty `DOCKER-USER` chain, which works as expected.\r\n\r\n[docker_2.log](https:\/\/github.com\/moby\/moby\/files\/12244208\/docker_2.log)\r\n\r\nUnfortunately, there seems to be no good way of fixing this, as Debian 12 only supports iptables 1.8.9, and there is no newer upstream version yet.\r\n","I'm confirming hitting the exact same issue.\r\nTook a while to understand what was happening and finally hit this bug report.\r\nThe problem occurs only a machine with Debian 12. Same log entry:\r\n```\r\n`level=warning msg=\"Failed to create DOCKER-USER IPV4 chain\" error=\"iptables failed: iptables -t filter -N DOCKER-USER: iptables: Chain already exists.\\n (exit status 1)`\r\n```\r\nfor the same reason: I have an customized DOCKER-USER chain configured prior to having Docker generate its rules.\r\nThe outcome is as described above, we miss the jump DOCKER-USER in the FORWARD chain, leading to containers connected to the bridge network not being able to reach anything outside. Containers connected to the host network are working as expected.\r\n\r\nI'll try to either drop my customised content or insert manually the missing command somewhere:\r\n```\r\niptables -I FORWARD -j DOCKER-USER\r\n```","It's a problem with the `iptables` package that was introduced in version 1.8.9. I can't remember the specific commit, but it had something to do with meta rules, I think. Anyway, the best solution for the time being on Debian 12 seems to be to build the 1.8.8 version of the packages `iptables`, `libip4tc2`, `libip6tc2`,  and `libxtables12` yourself, install them, and block updates. Hopefully, upstream will eventually get around to fixing this. But even then, it might take Debian some time to update the package, too.\r\n\r\nEDIT: They do have a [Bugzilla issue](https:\/\/bugzilla.netfilter.org\/show_bug.cgi?id=1659) open for this, but unfortunately, this is one of those annoying projects that make it really difficult to reach them.","To save others from following the link; it's closed as WONTFIX;\r\n\r\n> Compatibility between iptables-nft and nftables can't be \"fixed\", many\r\n> expressions in nftables rules can't be translated into iptables syntax as it\r\n> simply lacks the necessary capabilities.\r\n> \r\n> The specific problem illustrated here (setting packet mark) is fixed by commit\r\n> 7304f1982d619 (\"nft-ruleparse: parse meta mark set as MARK target\"), enabling\r\n> iptables-nft to correctly parse the meta mark statement.\r\n> \r\n> Improving the iptables-nft parser to understand more native expressions is a\r\n> task actively being worked on, but mixing iptables-nft and nftables will always\r\n> remain problematic and a good way to shoot one's own foot!","The linked Bugzilla issue mentions this bug is triggered by mixing both nftables and iptables-nft rules. I believe you need to do some deliberate action to end up mixing both, and given that there're already a bunch of resources warning that anyone trying to do that would end up in troubled waters, I consider there's not much we can do here. We might consider adding a warning into our docs if we find more users are affected by this issue or alike.\r\n\r\nSo let me close this one, but feel free to continue the discussion if you find something interesting or if I said something wrong.","I've been trying to catch up on fixing something I didn't setup -- so could you please clarify the \"expected\" way of doing things here?\r\nI'm not a fan of either solution but it seems Netfilter has decided that nftables is supposed to be way forward and deprecates iptables -- and as far as I understand so far, the only way of having Docker collaborates is one of the iptables\/nft adapters. \r\nIs the conclusion that we have to choose between either Docker, or nftables here?","The real problem is that Docker does not currently support nftables directly and instead relies on the nftables-iptables compatibility layer on systems that are using nftables when it is managing the firewall rules. This is what necessitates the workaround with the meta mark rule in the first place. Unfortunately, that workaround is now broken when using version 1.8.9 of the iptables packages. The only other option except for using version 1.8.8 would be to disable Docker's handling of FW rules and manage them manually.\r\n\r\nBut the real solution would be for Docker to implement direct support of nftables. Then we would not need the workaround in the first place, and we would not run into this bug. Given that nftables is effectively the designated successor of iptables, it is to be expected that more distributions will make the switch eventually which is a good reason why Docker should be able to work with it.","> Is the conclusion that we have to choose between either Docker, or nftables here?\r\n\r\nMore or less, yes. Or rather, you have to choose between `iptables-nft` and `iptables-legacy`, and `nft` is ruled out.\r\n\r\nTo be more precise, Docker is only compatible with `iptables-legacy` and `iptables-nft`. If you create some rules with `nft`, those might not be parsed correctly by `iptables-nft` and that leads to some variant of the original error message reported here. As noted in Phil Sutter's answer, mixing both `iptables-nft` and `nft` is highly discouraged. So you effectively have no other choice than using either `iptables-nft` or `iptables-legacy` _system-wide_ when Docker is installed.\r\n\r\n> Improving the iptables-nft parser to understand more native expressions is a\r\n> task actively being worked on, but mixing iptables-nft and nftables will always\r\n> remain problematic and a good way to shoot one's own foot!\r\n\r\nWe're aware we need to tackle this issue (see #26824), but we have some important overhauling to go through first as the way iptables rules are managed right now is quite messy. That's an unfortunate situation, but we can't do better right now.\r\n\r\n> This is what necessitates the workaround with the meta mark rule in the first place.\r\n\r\n@this-user Can you expand on that? I don't get why this is needed.\r\n\r\nLooking at https:\/\/wiki.debian.org\/nftables, I see:\r\n\r\n> the iptables utility may not be installed in a system by default.\r\n\r\nI guess the corollary is `nft` might be installed by default on those systems. Looking at our [installation guide for Debian](https:\/\/docs.docker.com\/engine\/install\/debian\/), I don't see any warning about that. That makes me change my mind, and I now think it's worth mentioning it in our docs. Let me reopen this issue and friendly ping @dvdksn.","@akerouanton \r\n\r\n> Can you expand on that? I don't get why this is needed.\r\n\r\nIf you install Docker per its documentation on a system with nftables like the most recent versions of Debian, the containers simply will not be able to create any outgoing connections, because their traffic gets stuck in the nftables `inet filter forward` chain, unless you set its policy to `ACCEPT`, which you probably shouldn't. The reason for that is  that Docker cannot touch that chain as it is using the iptables legacy mode and can therefore only modify the iptables FORWARD chain.\r\n\r\nThus, you can either disable Docker's management of FW rules altogether and do it manually, or you can do a workaround like adding one rule to the DOCKER-USER chain to set a mark that identifies Docker traffic, and then add a second rule in the nftables forward chain to accept traffic with that flag. Then it works as expected.\r\n\r\nIn any case, the bottom line is that traffic forwarding from containers does not currently work out of the box if the system is using nftables with anything but an accept policy for its forward chain if Docker is managing the firewall rules. That is arguably something that ought to be fixed on the Docker side.\r\n","> > Is the conclusion that we have to choose between either Docker, or nftables here?\r\n> \r\n> More or less, yes. Or rather, you have to choose between `iptables-nft` and `iptables-legacy`, and `nft` is ruled out.\r\n\r\nAlright, thanks for taking the time to reply.\r\n\r\n> We're aware we need to tackle this issue (see #26824), but we have some important overhauling to go through first as the way iptables rules are managed right now is quite messy. That's an unfortunate situation, but we can't do better right now.\r\n\r\nNo problem, I know this is not trivial; the summary is that we are all in a bad spot but it's not really the fault of anyone, just the general entropy of the universe :-)\r\nAnyway, I got a working workaround, and other options are mentioned in this thread so people can refer to that;\r\n\r\nLastly, I can understand you don't want to keep this ticket lingering around, I'd imagine it could possibly be marked as dup of a more general ticket related to the Docker\/Nft upcoming support? something like that.\r\n","I also have the problem that my own nftables rules (\/etc\/nftables.conf) are not loaded correctly on restart. \r\n\r\n```\r\n# journalctl | grep \"Failed to create DOCKER-USER\"\r\nDec 27 01:05:46 leon dockerd[837]: time=\"2023-12-27T01:05:46.780516480+01:00\" level=warning msg=\"Failed to create DOCKER-USER IPV4 chain\" error=\"iptables failed: iptables -t filter -N DOCKER-USER: iptables: Chain already exists.\\n (exit status 1)\"\r\n```\r\nMy solution is to create my own chain FORWARD-CB.\r\nAlthough I create the DOCKER-USER chain and fill it with my own rule, docker does not interfere with it. \r\n\r\n```\r\nadd chain ip  filter DOCKER-USER\r\nadd chain ip6 filter DOCKER-USER\r\ninsert rule ip  filter DOCKER-USER counter jump FORWARD-CB\r\ninsert rule ip6 filter DOCKER-USER counter jump FORWARD-CB\r\n```\r\n\r\nThe flow is as follows: \r\n\r\nchain FORWARD -> jump DOCKER-USER -> \r\nchain DOCKER-USER -> jump FORWARD-CB -> \r\nchain FORWARD-CB -> return (DOCKER-USER) -> return (FORWARD)\r\n\r\n\r\n\r\n\r\n\r\n\r\nmy \/etc\/nftabes.conf\r\n```\r\n#!\/usr\/sbin\/nft -f\r\n\r\ndefine IF-INET = enp3s0\r\n\r\nadd table ip  filter\r\nadd table ip6 filter\r\nadd chain ip  filter INPUT   { type filter hook input   priority 0; policy accept; }\r\nadd chain ip6 filter INPUT   { type filter hook input   priority 0; policy accept; }\r\nadd chain ip  filter OUTPUT  { type filter hook output  priority 0; policy accept; }\r\nadd chain ip6 filter OUTPUT  { type filter hook output  priority 0; policy accept; }\r\nadd chain ip  filter FORWARD { type filter hook forward priority 0; policy accept; }\r\nadd chain ip6 filter FORWARD { type filter hook forward priority 0; policy accept; }\r\nadd chain ip  filter DOCKER-USER\r\nadd chain ip6 filter DOCKER-USER\r\nadd chain ip  filter FORWARD-CB\r\nadd chain ip6 filter FORWARD-CB\r\n\r\nflush chain ip  filter INPUT\r\nflush chain ip6 filter INPUT\r\nflush chain ip  filter OUTPUT\r\nflush chain ip6 filter OUTPUT\r\nflush chain ip  filter DOCKER-USER\r\nflush chain ip6 filter DOCKER-USER\r\nflush chain ip  filter FORWARD-CB\r\nflush chain ip6 filter FORWARD-CB\r\n\r\n\r\ninsert rule ip  filter DOCKER-USER counter jump FORWARD-CB\r\ninsert rule ip6 filter DOCKER-USER counter jump FORWARD-CB\r\n\r\n# these rules are set automatically by docker\r\n#insert rule ip  filter DOCKER-USER counter return\r\n#insert rule ip6 filter DOCKER-USER counter return\r\n\r\n#################################################################################################################################\r\n# INPUT\r\n#################################################################################################################################\r\n\r\n# already established connections\r\nadd rule ip  filter INPUT iifname $IF-INET ct state related,established counter accept\r\nadd rule ip6 filter INPUT iifname $IF-INET ct state related,established counter accept\r\n\r\n# icmp\r\nadd rule ip  filter INPUT iifname $IF-INET icmp   type echo-request counter accept\r\nadd rule ip6 filter INPUT iifname $IF-INET icmpv6 type echo-request counter accept\r\nadd rule ip6 filter INPUT iifname $IF-INET icmpv6 type { nd-neighbor-advert, nd-neighbor-solicit, nd-router-advert} counter accept\r\n\r\n# broadcast,multicast\r\nadd rule ip  filter INPUT iifname $IF-INET pkttype { broadcast,multicast} counter accept\r\nadd rule ip6 filter INPUT iifname $IF-INET pkttype { broadcast,multicast} counter accept\r\n\r\n# ssh\r\nadd rule ip  filter INPUT iifname $IF-INET ct state new tcp dport 22 counter accept comment \"ssh\"\r\nadd rule ip6 filter INPUT iifname $IF-INET ct state new tcp dport 22 counter accept comment \"ssh\"\r\n\r\n# logging\r\nadd rule ip  filter INPUT iifname $IF-INET counter log prefix \"nft-drop-INPUT: \" flags tcp options flags ip options\r\nadd rule ip6 filter INPUT iifname $IF-INET counter log prefix \"nft-drop-INPUT: \" flags tcp options flags ip options\r\n\r\n# drop all\r\nadd rule ip  filter INPUT iifname $IF-INET counter drop comment \"drop-all\"\r\nadd rule ip6 filter INPUT iifname $IF-INET counter drop comment \"drop-all\"\r\n\r\n\r\n#################################################################################################################################\r\n# FORWARD-CB (in DOCKER-USER chain)\r\n#################################################################################################################################\r\n\r\n# already established connections\r\nadd rule ip  filter FORWARD-CB iifname $IF-INET ct state related,established counter accept\r\nadd rule ip6 filter FORWARD-CB iifname $IF-INET ct state related,established counter accept\r\n\r\n# proxy-web (docker container)\r\nadd rule ip  filter FORWARD-CB iifname $IF-INET ct state new meta l4proto { tcp, udp } th dport { 80, 443 } counter accept comment \"proxy-web\"\r\nadd rule ip6 filter FORWARD-CB iifname $IF-INET ct state new meta l4proto { tcp, udp } th dport { 80, 443 } counter accept comment \"proxy-web\"\r\n\r\n# logging\r\nadd rule ip  filter FORWARD-CB iifname $IF-INET counter log prefix \"nft-drop-DOCKER-USER: \" flags tcp options flags ip options\r\nadd rule ip6 filter FORWARD-CB iifname $IF-INET counter log prefix \"nft-drop-DOCKER-USER: \" flags tcp options flags ip options\r\n\r\n# drop all\r\nadd rule ip  filter FORWARD-CB iifname $IF-INET counter drop comment \"drop-all\"\r\nadd rule ip6 filter FORWARD-CB iifname $IF-INET counter drop comment \"drop-all\"\r\n\r\n# return to FORWARD chain\r\nadd rule ip  filter FORWARD-CB counter return comment \"return to DOCKER-USER chain\"\r\nadd rule ip6 filter FORWARD-CB counter return comment \"return to DOCKER-USER chain\"\r\n```"],"labels":["status\/0-triage","kind\/bug","area\/networking","version\/24.0"]},{"title":"ERROR: failed to build resolver: passthrough: received empty target in Build()","body":"### Description\n\nsince I updated docker I get the above error whenever I try to run `docker build`\n\n### Reproduce\n\ncreate an arbitrary Dockerfile\r\nrun `docker build .`\r\nERROR: failed to build resolver: passthrough: received empty target in Build()\r\n \n\n### Expected behavior\n\nIt should build the image\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.4\r\n API version:       1.43\r\n Go version:        go1.20.5\r\n Git commit:        3713ee1\r\n Built:             Fri Jul  7 14:50:57 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.4\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.5\r\n  Git commit:       4ffc614\r\n  Built:            Fri Jul  7 14:50:57 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.4\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.19.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 47\r\n  Running: 6\r\n  Paused: 0\r\n  Stopped: 41\r\n Images: 14\r\n Server Version: 24.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 5.15.0-78-generic\r\n Operating System: Ubuntu 20.04.6 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 31.27GiB\r\n Name: russell-workstation\r\n ID: 14e2a789-c6d2-4a7e-92e2-293b93e0abe5\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nrunning other commands (docker run etc) works\r\n\r\nalso running docker build with sudo works. ","comments":["- Do you have more details? Does this happen with any `docker build` or only specific ones? (can you provide a minimal example Dockerfile that allows reproducing the issue?)\r\n- What version of Docker were you running before upgrading?\r\n- If you were running Docker 20.10 or older before, were you using BuildKit before updating?\r\n- Are you using as `# syntax=docker\/dockerfile:1` directive in the Dockerfile you're building?\r\n\r\nThis message is produced by GRPC; \r\n\r\n```bash\r\ngit grep 'received empty target'\r\nvendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go:             return nil, errors.New(\"passthrough: received empty target in Build()\")\r\n```\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/7baab8bd6c0674475f108f4cd1158d9ea30a62cb\/vendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go#L34-L36\r\n\r\nBut that error is not in docker 24.0.4 (the version you're running); https:\/\/github.com\/moby\/moby\/blob\/v24.0.4\/vendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go\r\n\r\nI also checked if that error possibly was moved from somewhere else, but it doesn't appear in the code at v24.0.4;\r\n\r\n```bash\r\ngit checkout v24.0.4\r\nHEAD is now at 4ffc61430b Merge pull request #45903 from thaJeztah\/24.0_backport_fix_volume_npe\r\n\r\ngit grep 'received empty target'\r\n# (no results)\r\n```\r\n\r\nHowever, I do see it was updated in `buildx` (which runs `docker build` on the cli side), https:\/\/github.com\/docker\/buildx\/blob\/9872040b6626fb7d87ef7296fd5b832e8cc2ad17\/vendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go#L34C1-L36\r\n\r\nAnd that line of code was introduced in https:\/\/github.com\/docker\/buildx\/commit\/9541457c5429a67b434c8d65c3cb44fa58c70834 (https:\/\/github.com\/docker\/buildx\/pull\/1703), which is part of `buildx` v0.11.0-rc1 and up.\r\n\r\nIt also appears in the Dockerfile parser, which would be used if you have a `# syntax=...` directive in your Dockerfile: https:\/\/github.com\/moby\/buildkit\/blob\/18fc875d9bfd6e065cd8211abc639434ba65aa56\/vendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go#L35\r\n\r\n```bash\r\ngit checkout dockerfile\/1.6.0\r\nHEAD is now at 18fc875d9 Merge pull request #4014 from tonistiigi\/fix-policy-rule-order\r\n\r\ngit grep 'received empty target'\r\nvendor\/google.golang.org\/grpc\/internal\/resolver\/passthrough\/passthrough.go:             return nil, errors.New(\"passthrough: received empty target in Build()\")\r\n```\r\n\r\nThat was introduced in https:\/\/github.com\/moby\/buildkit\/pull\/3636, which is part of `dockerfile\/1.6.0-rc1` and up. If you are using a `# syntax=` directive in your Dockerfile, does the problem go away if you use the previous (1.6) syntax?\r\n\r\n```dockerfile\r\n# syntax=docker\/dockerfile:1.5\r\n```\r\n","Hi! Thanks for the quick reply. :wave: \r\n\r\nIt happens on all docker build commands. Literally, any Dockerfile will recreate the problem. \r\n\r\nI don't know what my previous version was I'm afraid. Also, I don't know if I was using buildkit before the upgrade. \r\n\r\nUnfortunately, adding `# syntax=docker\/dockerfile:1` to the top of the file doesn't fix it.\r\n\r\n\r\n\r\n","Hm.. curious.\r\n\r\nI see you mentioned:\r\n\r\n> also running docker build with sudo works.\r\n\r\nI wonder if running with sudo makes it use a different version of `buildx` as CLI plugins can be installed in different paths, with their own priority, and some of those paths are based on the user's home directory and `$PATH` (which may be different if you use `sudo`).\r\n\r\nIf you run `sudo docker info` does the \"Plugins\" section of the output show a different path for the `buildx` plugin?\r\n\r\n```\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.19.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n```\r\n\r\nAnother option is that the error comes from something else, which _could be_ a credentials helper (and possibly that helper is not called when using `sudo`.\r\n\r\n\r\n","FWIW as there were some breaking changes in Go1.20.6 (I thought.. perhaps those _could_ be related) and assuming you installed buildx from our `.deb` packages at download.docker.com, I also double-checked if buildx v0.11.1 was built with Go1.20.5 (buildx v0.11.2 has fixes to work with the latest go version);\r\n\r\n\r\nBuildx v0.11.2 is built with go1.20.6 (as expected);\r\n\r\n```bash\r\ndocker run -it --rm -v \/usr\/libexec\/docker\/cli-plugins\/:\/plugins golang sh -c '\/plugins\/docker-buildx version && go version \/plugins\/docker-buildx'\r\ngithub.com\/docker\/buildx v0.11.2 9872040\r\n\/plugins\/docker-buildx: go1.20.6\r\n```\r\n\r\nDowngrading to buildx v0.11.1 (the version you have installed):\r\n\r\n```bash\r\napt-get install docker-buildx-plugin=0.11.1-1~ubuntu.20.04~focal\r\n\r\ndocker run -it --rm -v \/usr\/libexec\/docker\/cli-plugins\/:\/plugins golang sh -c '\/plugins\/docker-buildx version && go version \/plugins\/docker-buildx'\r\ngithub.com\/docker\/buildx v0.11.1 b4df085\r\n\/plugins\/docker-buildx: go1.20.5\r\n```\r\n\r\nSo Buildx v0.11.1 is built with go1.20.5, so the Golang changes should not be related.","@russell-wave Please run the same command again with `GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=100` and report back the extra logs. Set `--progress=plain` to make sure no logs get missing because of the interactive tty.\r\n\r\nAlso, please provide info on how you installed this Docker version (and Buildx in case you installed it separately).","The code-path I found that hit it was\r\n\r\n- commands\/runBuild() (if experimental is enabled (`isExperimental()`)): https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/commands\/build.go#L269-L270\r\n- controller\/NewController() (if `opts.Detach` == \"remote\") -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/controller\/controller.go#L15-L29\r\n- controller\/remote\/NewRemoteBuildxController() -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/controller\/remote\/controller.go#L66-L94\r\n- controller\/remote\/newBuildxClientAndCheck() -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/controller\/remote\/controller.go#L264-L265\r\n- controller\/remote\/client.NewClient() -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/controller\/remote\/client.go#L39\r\n- grpc.DialContext() -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/vendor\/google.golang.org\/grpc\/clientconn.go#L296\r\n- grpc.newCCResolverWrapper -> https:\/\/github.com\/docker\/buildx\/blob\/v0.11.1\/vendor\/google.golang.org\/grpc\/resolver_conn_wrapper.go#L72\r\n\r\nSo from that, I wonder if the `sudo` environment would have experimental _disabled_ and the non-sudo environment has it enabled.","Here is the output with GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=100 \r\n\r\n```\r\n\u279c docker build --progress=plain .\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] Channel created\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] original dial target is: \"\"\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] parsed dial target is: {Scheme: Authority: URL:{Scheme: Opaque: User: Host: Path: RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] fallback to scheme \"passthrough\"\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] parsed dial target is: {Scheme:passthrough Authority: URL:{Scheme:passthrough Opaque: User: Host: Path:\/ RawPath: OmitHost:false ForceQuery:false RawQuery: Fragment: RawFragment:}}\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] Channel authority set to \"\"\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] Channel Connectivity change to SHUTDOWN\r\n2023\/08\/07 10:04:59 INFO: [core] [Channel #1] Channel deleted\r\nERROR: failed to build resolver: passthrough: received empty target in Build()\r\n```\r\n\r\nI installed docker like this:\r\n\r\n```                    \"wget -qO- https:\/\/get.docker.com\/gpg | sudo apt-key add -\",\r\n                    \"curl -fsSL https:\/\/get.docker.com -o \/tmp\/get-docker.sh\",\r\n                    \"sudo sh \/tmp\/get-docker.sh\",\r\n```\r\n\r\n","I was having a similar problem, getting the exact same error message:\r\n\r\n```\r\nERROR: failed to build resolver: passthrough: received empty target in Build()\r\n```\r\n\r\nAfter a lot of experimenting with `sudo` and environment variables I found that `OTEL_EXPORTER_OTLP_ENDPOINT` being set was the culprit (_in my case_). Unsetting it got rid of the error and `docker build` started working.\r\n","@biffen unsetting that fixes it for me too. Thanks! ","> After a lot of experimenting with sudo and environment variables I found that OTEL_EXPORTER_OTLP_ENDPOINT being set was the culprit (in my case). Unsetting it got rid of the error and docker build started working.\r\n\r\nHmm.. interesting, thanks for finding that! I'll check with the team working on buildx if they have an idea.","What was the `OTEL_EXPORTER_OTLP_ENDPOINT` set to? You don't need to provide full URL, but what was the format and was there an OTEL collector running at that endpoint?","> What was the `OTEL_EXPORTER_OTLP_ENDPOINT` set to? You don't need to provide full URL, but what was the format and was there an OTEL collector running at that endpoint?\r\n\r\nIn my case it was set to `localhost:4317` and no, there was nothing listening there.","> What was the OTEL_EXPORTER_OTLP_ENDPOINT set to? You don't need to provide full URL, but what was the format and was there an OTEL collector running at that endpoint?\r\n\r\nin my case it was `api.honeycomb.io:443`  and AFIK there is a collector there. ","Glad to have found this! In my case it was the following OTEL env variables that triggered the issue:\r\n\r\n```sh\r\n> env | grep OTEL_\r\nOTEL_METRICS_EXPORTER=otlp\r\nOTEL_TRACES_EXPORTER=otlp\r\n```","I've started seeing this error in our GitHub Actions workflows when running docker compose. It looks like the version of docker compose installed on the GitHub hosted runners was recently updated ([see here](https:\/\/github.com\/actions\/runner-images\/commit\/bf202afb1e9165f015d61c603af273de35e50067#diff-66aec6097318276b09842a3ba2caf3037afbd8dadca2dfcdf76631100613ea69R78)) with a change that [upgrades its buildx dependency to v0.12.0](https:\/\/docs.docker.com\/compose\/release-notes\/#2233).\r\n\r\nI'm assuming this is the cause, though it seems like the previous docker compose version on the runner ([v2.23.0](https:\/\/docs.docker.com\/compose\/release-notes\/#2230)), should have been using [at least buildx v0.11.2](https:\/\/docs.docker.com\/compose\/release-notes\/#2201), and that should already have had this bug in it...","\/cc @milas @glours ^^","I am having a quite similar issue.\r\n\r\nCurrently running **Docker Desktop v.4.26.1 (131620)** on Apple Silicon M2 Pro on MacOS: 14.2.1\r\n\r\nWhile I can run `docker build` directly using a Dockerfile, if I try to use `docker compose build` to build the same Dockerfile, I get the same `failed to build resolver: passthrough: received empty target in Build()` error\r\n\r\nThe **docker compose** file is as simple as it gets:\r\n```yaml\r\nservices:\r\n  web:\r\n    build:\r\n      dockerfile: .\/build\/Dockerfile-dev\r\n```\r\n\r\nThe funny thing is that if I zero the OTEL variable the command works ` OTEL_EXPORTER_OTLP_ENDPOINT=\"\" docker compose build `\r\n\r\nPlease note that I do not have the `OTEL_EXPORTER_OTLP_ENDPOINT` set in my environment by default\r\n\r\nHere is the output of the `docker info`\r\n```\r\nClient:\r\n Version:    24.0.7\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.12.0-desktop.2\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.23.3-desktop.2\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-compose\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.21\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-extension\r\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\r\n    Version:  0.1\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-feedback\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.10\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-scan\r\n  scout: Docker Scout (Docker Inc.)\r\n    Version:  v1.2.0\r\n    Path:     \/Users\/mitsos\/.docker\/cli-plugins\/docker-scout\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 0\r\n Server Version: 24.0.7\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f\r\n runc version: v1.1.10-0-g18a0cb0\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 6.5.11-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 10\r\n Total Memory: 7.663GiB\r\n Name: docker-desktop\r\n ID: a51f8f0f-fbb4-4731-b306-ec9c1f07b237\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  hubproxy.docker.internal:5555\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n\r\nWARNING: daemon is not using the default seccomp profile\r\n```","> The funny thing is that if I zero the OTEL variable the command works OTEL_EXPORTER_OTLP_ENDPOINT=\"\" docker compose build\r\n>\r\n> Please note that I do not have the OTEL_EXPORTER_OTLP_ENDPOINT set in my environment by default\r\n\r\nI think it's possible that docker compose sets these, based on config in the docker context (I know the compose team has been working on wiring up OTEL when used with Docker Desktop).","> > The funny thing is that if I zero the OTEL variable the command works OTEL_EXPORTER_OTLP_ENDPOINT=\"\" docker compose build\r\n> > Please note that I do not have the OTEL_EXPORTER_OTLP_ENDPOINT set in my environment by default\r\n> \r\n> I think it's possible that docker compose sets these, based on config in the docker context (I know the compose team has been working on wiring up OTEL when used with Docker Desktop).\r\n\r\nHmmm I see. But if docker compose sets them, shouldn't the default value work out of the box? What is it stopping it?","Yes, I assume the default should work. But perhaps my assumption (compose setting a value) is incorrect; I asked in our internal slack if the compose team could have a peek if it's possible related (or not). ","**UPDATE**\r\nI wanted to give an update regarding my issue so that you're not looking into something that isn't worth it.\r\n\r\nI was doing all my tests in a specific repo folder and I was explicitly providing different kinds of environment files in the docker compose configuration.\r\n\r\nHowever, I also had a `.env` env that contained config to be picked up from an other plugin I had and this had `OTEL_EXPORTER_OTLP_ENDPOINT` in it.... I had simply forgotten the `docker compose` automatically picks up `.env` files...\r\n\r\nAs soon as I removed it, everything worked as expected\r\n\r\nSorry for the trouble!","Thanks for the update! That's good to know.\r\n\r\n(still curious about the exact root-cause of the issue, and if we could have a more graceful way to handle it, but at least we know it's not due to compose setting a non-functional default) ","We have the exact same problem running `docker compose build` with the `OTEL_EXPORTER_OTLP_ENDPOINT` in a .env file.\r\n\r\nThe bug comes with [this v2.23.3 release](https:\/\/github.com\/docker\/compose\/releases\/tag\/v2.23.3) which does upgrade buildx to v0.12\r\n\r\nHowever using buildx directly (`docker buildx build --build-arg OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=collector-gateway:4317 .`) with version v0.12.0 or v0.12.1 works fine.\r\n\r\nSo the workaround is to downgrade to v2.23.2 ","I ran into the same error message. Adding the protocol `otlp:\/\/` to the OTEL_EXPORTER_OTLP_TRACES_ENDPOINT env var solved it for me.","> I ran into the same error message. Adding the protocol `otlp:\/\/` to the OTEL_EXPORTER_OTLP_TRACES_ENDPOINT env var solved it for me.\r\n\r\nIs this even documented anywhere? This is the fix! Thanks!\r\n\r\n"],"labels":["area\/builder","status\/0-triage","status\/more-info-needed","kind\/bug","area\/builder\/buildkit","version\/24.0","area\/metrics\/otel"]},{"title":"c8d: cannot use images with multiple \"namespaces\"","body":"### Description\n\nwith containerd integration enabled, docker does not handle images with multiple \"namespaces\" (`namespace\/namespace\/image:tag` or `registry.example.com\/namespace\/namespace\/image:tag`). While nested namespaces are not supported by Docker Hub, they are allowed in the OCI specs, so we should support them.\n\n### Reproduce\n\nTag an image with multiple namespaces (note that I haven't tried pulling one);\r\n\r\n\r\n```bash\r\ndocker pull busybox\r\ndocker tag busybox base-linux\/arm64\/v8\r\n\r\ndocker image ls\r\nREPOSITORY            TAG       IMAGE ID       CREATED          SIZE\r\nbase-linux\/arm64\/v8   latest    3fbc63216742   2 minutes ago    6.09MB\r\nbusybox               latest    3fbc63216742   2 days ago       6.09MB\r\n```\r\n\r\nWhich _seemed_ to work, but when actually trying to use the image, it failed:\r\n\r\n\r\n```bash\r\ndocker run --rm base-linux\/arm64\/v8 echo hello\r\nUnable to find image 'base-linux\/arm64\/v8:latest' locally\r\ndocker: Error response from daemon: failed to resolve reference \"docker.io\/base-linux\/arm64\/v8:latest\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed.\r\nSee 'docker run --help'.\r\n```\r\n\n\n### Expected behavior\n\n\r\nWithout containerd integration, this works:\r\n\r\n```bash\r\ndocker run --rm base-linux\/arm64\/v8 echo hello\r\nhello\r\n```\r\n\n\n### docker version\n\n```bash\nClient:\r\n Cloud integration: v1.0.35+desktop.2\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:32:30 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.23.0 (116967)\r\n Engine:\r\n  Version:          24.0\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       HEAD\r\n  Built:            Fri Jul 21 18:11:43 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Version:    24.0.5\r\n Context:    desktop-linux\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2+desktop.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-compose\r\n  desktop: Commands to interact with Docker Desktop (Docker Inc.)\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-desktop\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-scout\r\n  shell: Open a browser shell on the Docker Host. (thaJeztah)\r\n    Version:  v0.0.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-shell\r\n\r\nServer:\r\n Containers: 8\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 8\r\n Images: 11\r\n Server Version: 24.0\r\n Storage Driver: stargz\r\n  driver-type: io.containerd.snapshotter.v1\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: unconfined\r\n  cgroupns\r\n Kernel Version: 5.15.49-linuxkit\r\n Operating System: Docker Desktop\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 5\r\n Total Memory: 7.668GiB\r\n Name: docker-desktop\r\n ID: 1d410d6c-593f-4f8d-ba03-1fe940b5daf4\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 62\r\n  Goroutines: 107\r\n  System Time: 2023-07-31T09:22:11.600269719Z\r\n  EventsListeners: 29\r\n HTTP Proxy: http.docker.internal:3128\r\n HTTPS Proxy: http.docker.internal:3128\r\n No Proxy: hubproxy.docker.internal\r\n Experimental: false\r\n Insecure Registries:\r\n  host.docker.internal:5002\r\n  hubproxy.docker.internal:5555\r\n  host.docker.internal:5001\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Moby\n```\n\n\n### Additional Info\n\n_No response_","comments":[],"labels":["kind\/bug","area\/images","containerd-integration"]},{"title":"Wire contexts through a couple of subsystems","body":"This plumbs through a context within `plugin\/` and `libcontainerd\/`.\r\nIn some cases this touched some higher-level things.\r\n\r\nNote: I did not change `pkg\/plugins` or `pkg\/plugingetter` because this is a *much* larger change (deceptively so) which won't even compile until some swarmkit changes are made (fun times here: https:\/\/github.com\/cpuguy83\/docker\/commit\/9c25233a2f24c6b0b84790af628044fe6edd1aa8).\r\n\r\nBasically taking a bottom-up approach here so the underlying subsystems are ready to take a context when ready.","comments":["oh, scratch that; looks like a build is failing;\r\n\r\n\r\n```\r\n8 65.69 # github.com\/docker\/docker\/plugin\r\n#38 65.69 plugin\/manager.go:54:29: too many arguments in call to pm.restore\r\n#38 65.69 \thave (context.Context, *\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller)\r\n#38 65.69 \twant (*\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller)\r\n#38 65.69 plugin\/manager.go:150:24: too many arguments in call to pm.enable\r\n#38 65.69 \thave (context.Context, *\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller, bool)\r\n#38 65.69 \twant (*\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller, bool)\r\n#38 65.69 plugin\/manager.go:244:36: too many arguments in call to pm.enable\r\n#38 65.69 \thave (context.Context, *\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller, bool)\r\n#38 65.69 \twant (*\"github.com\/docker\/docker\/plugin\/v2\".Plugin, *controller, bool)\r\n#38 74.47 # github.com\/docker\/docker\/libcontainerd\/local\r\n#38 74.47 libcontainerd\/local\/local_windows.go:170:70: not enough arguments in call to c.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:506:89: not enough arguments in call to ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:662:89: not enough arguments in call to t.ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:670:90: not enough arguments in call to t.ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:794:86: not enough arguments in call to t.ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:835:87: not enough arguments in call to t.ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 libcontainerd\/local\/local_windows.go:1174:84: not enough arguments in call to p.ctr.client.backend.ProcessEvent\r\n#38 74.47 \thave (string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 74.47 \twant (context.Context, string, \"github.com\/docker\/docker\/libcontainerd\/types\".EventType, \"github.com\/docker\/docker\/libcontainerd\/types\".EventInfo)\r\n#38 75.05 # github.com\/docker\/docker\/libcontainerd\/remote\r\n#38 75.05 libcontainerd\/remote\/client.go:223:52: too many arguments in call to newFIFOSet\r\n#38 75.05 \thave (context.Context, string, string, bool, bool)\r\n#38 75.05 \twant (string, string, bool, bool)\r\n#38 75.05 libcontainerd\/remote\/client.go:276:87: too many arguments in call to newFIFOSet\r\n#38 75.05 \thave (context.Context, string, string, bool, bool)\r\n#38 75.05 \twant (string, string, bool, bool)\r\n```","Ah the \"other\" platform \ud83d\ude04 . Should be good now.","Missed 2 \ud83d\ude05\r\n\r\n```\r\nlibcontainerd\/remote\/client.go:223:47: not enough arguments in call to newFIFOSet\r\n136.3 \thave (string, string, bool, bool)\r\n136.3 \twant (context.Context, string, string, bool, bool)\r\n136.3 libcontainerd\/remote\/client.go:276:82: not enough arguments in call to newFIFOSet\r\n136.3 \thave (string, string, bool, bool)\r\n136.3 \twant (context.Context, string, string, bool, bool)","WAT? Compiles here... \ud83e\udd14 ","Ok, now it's good realzies this time.","I tried restarting a few times, but this specific test keeps failing, and I wonder if it's related (although I have no direct clue how)\r\n\r\n```\r\n=== Failed\r\n=== FAIL: amd64.integration-cli TestDockerAPISuite\/TestExecStateCleanup (5.49s)\r\n    docker_api_exec_test.go:234: timeout hit after 5s: first check never completed\r\n    --- FAIL: TestDockerAPISuite\/TestExecStateCleanup (5.49s)\r\n\r\n=== FAIL: amd64.integration-cli TestDockerAPISuite (99.08s)\r\n```","Looks like I missed a timeout context masking a parent context, so the `process.Delete` was failing due the context being timed out.\r\nFixed that... will see what the tests say.","Saw the diff (and the typo fix \ud83d\ude02), thanks! \ud83e\udd1e "],"labels":["status\/2-code-review","kind\/refactor"]},{"title":"c8d: docker build shows \"naming to moby-dangling@sha256....\" when not tagging image","body":"### Description\n\nI saw this some time back, but forgot \"where\", and I just can an example and now now where I saw it :smile:\r\n\r\nWhen running a `docker build`, but _not_ setting a tag (no `-t` \/ `--tag`), the build progress shows `moby-dangling@sha256...`  in the output.\r\n\r\nFor example:\r\n\r\n```bash\r\ndocker build -<<'EOF'\r\n# syntax=docker\/dockerfile:1\r\nFROM busybox\r\nRUN echo hello\r\nEOF\r\n```\r\n\r\nShows:\r\n\r\n```\r\n[+] Building 4.3s (8\/8) FINISHED                                                                                  docker:desktop-linux\r\n => [internal] load build definition from Dockerfile                                                                              0.0s\r\n => => transferring dockerfile: 94B                                                                                               0.0s\r\n => [internal] load .dockerignore                                                                                                 0.0s\r\n => => transferring context: 2B                                                                                                   0.0s\r\n => resolve image config for docker.io\/docker\/dockerfile:1                                                                        2.1s\r\n => CACHED docker-image:\/\/docker.io\/docker\/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f4d6a7782a409b14   0.0s\r\n => => resolve docker.io\/docker\/dockerfile:1@sha256:39b85bbfa7536a5feceb7372a0817649ecb2724562a38360f4d6a7782a409b14              0.0s\r\n => [internal] load metadata for docker.io\/library\/busybox:latest                                                                 1.1s\r\n => CACHED [1\/2] FROM docker.io\/library\/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79           0.0s\r\n => => resolve docker.io\/library\/busybox@sha256:3fbc632167424a6d997e74f52b878d7cc478225cffac6bc977eedfe51c7f4e79                  0.0s\r\n => [2\/2] RUN echo hello                                                                                                          0.7s\r\n => exporting to image                                                                                                            0.1s\r\n => => exporting layers                                                                                                           0.1s\r\n => => exporting manifest sha256:31e23409c78fa53607729b968c6f1af93b4dfb65fc15d64d08b1a589eda8adb0                                 0.0s\r\n => => exporting config sha256:fd125d213f1db457d9491daa6cdc418624df26828ebef79ceca4656417c28446                                   0.0s\r\n => => exporting attestation manifest sha256:479e4952365bbf05e9373609dd7e36ca1426924171ab0962bc12f981b9742b73                     0.0s\r\n => => exporting manifest list sha256:c87125196876d6dc33c62d0ba365889634ec41643aea68cf94f567026da17c81                            0.0s\r\n => => naming to moby-dangling@sha256:c87125196876d6dc33c62d0ba365889634ec41643aea68cf94f567026da17c81                            0.0s\r\n => => unpacking to moby-dangling@sha256:c87125196876d6dc33c62d0ba365889634ec41643aea68cf94f567026da17c81                         0.0s\r\n```\r\n\r\nNotice the `naming to` in the output;\r\n\r\n```\r\n=> => naming to moby-dangling@sha256:c87125196876d6dc33c62d0ba365889634ec41643aea68cf94f567026da17c81                            0.0s\r\n```\r\n\r\nWe should look if we can clean up the output to strip the \"naming to\" and removing the `moby-dangling`\r\n\r\n\r\nThis was on:\r\n\r\n```\r\nClient:\r\n Cloud integration: v1.0.35-desktop+001\r\n Version:           24.0.4\r\n API version:       1.43\r\n Go version:        go1.20.5\r\n Git commit:        3713ee1\r\n Built:             Fri Jul  7 14:47:27 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           desktop-linux\r\n\r\nServer: Docker Desktop 4.22.0 (116548)\r\n Engine:\r\n  Version:          24.0.4-3-gecd494abf3\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.5\r\n  Git commit:       ecd494abf3ca1c0c7ae7c568375a4494b8a47319\r\n  Built:            Fri Jul  7 21:18:08 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n","comments":["@rumpl @vvoland found it! Now I know where I saw that `moby-dangling` in the output \ud83d\ude02 \r\n\r\n\/cc @crazy-max ","found another spot where it's printing the `dangling@`; `docker system prune`;\r\n\r\n```bash\r\ndocker system prune -af\r\nDeleted Images:\r\nuntagged: moby-dangling@sha256:5729952b5308ec006804f695f97bbed6137317cde0caab66464e2dd013ee2313\r\nuntagged: docker.io\/library\/busybox:latest\r\ndeleted: sha256:5cd228af7cde277502487da780b34ba111b8fcdcf37ca518d68c5ba565002b36\r\ndeleted: sha256:ee899917ce6be185380c8404efb61aa683b649ab2d6a81857887fd746404edbf\r\ndeleted: sha256:064a9f60d69ca91b86fbc49a700c3e8971d66939a6832d95afe082722af637cc\r\nuntagged: docker.io\/library\/docker-dev:latest\r\nuntagged: docker.io\/library\/fedora:38\r\ndeleted: sha256:61f921e0c7b51e162e6f94b14ef4e6b0d38eac5987286fe4f52a2c1158cc2399\r\ndeleted: sha256:99f96b7b103c71d35b63c39bae15e93085988de8eb155ec9e22f6b5f670faaf8\r\ndeleted: sha256:f838063d816c428b612b4e599ea70d35d87ef6335d0a5bf98b9043bff3528dd6\r\ndeleted: sha256:9523322a2d97b6a26a31c5c19b62d9ec9e3ea7e7ed808d90b225b9c6081ffb13\r\nuntagged: docker.io\/library\/fedora:39\r\ndeleted: sha256:0e841162744bd97cb5b9574561daa8f1bee3e67e28105f2fd6d510590572c203\r\ndeleted: sha256:6158f211a72bc4b23ec2136503ea8a794ccf86bd7455e82fd69d21344aed889a\r\ndeleted: sha256:d1d788e380d64d5474dc19c2564294b89c13ead665dd0fadc776d94dd297a417\r\ndeleted: sha256:c4d503eb71ba388e12ec632128efcb358f839de1dc70d09135ad00d2cf19d598\r\nuntagged: docker.io\/library\/golang:1.20\r\ndeleted: sha256:2408498f932649aaa1f12490e5c8ebd9b249022800a61e99c3328376cb628159\r\ndeleted: sha256:dce79c6c996f5c674112a744c1c3fb3d8ab6c66175d235ca8ef2351af411bf4a\r\ndeleted: sha256:c3ae0d214b5bc5230a50d2f3e4f86a97a6173e543d99aa4a08d99ab6e2b0a9b4\r\ndeleted: sha256:a014e5e7d08c37cf1703b97e701ccdc850e4a18d0ee679f03aa875dcd520aa85\r\ndeleted: sha256:715cea74ecbb15cb82efef1e77dd60c31d90b01d1286d6f39b4562afaebe75f3\r\ndeleted: sha256:003f1109a21287fa17dc866e87e8c6685113960cbb0379fee8f42b83de63c647\r\ndeleted: sha256:6529d3f2eda2bb51de0720b46e2e8359ed7a45efa1298a067b519e1cf2c094c0\r\ndeleted: sha256:39c67f667447d946168b5e8569dd4f15e7a7f3020af4b773f8a41a1f808ce32a\r\ndeleted: sha256:d54b6b539d36e6cffd916b6b866d2b01fdf0ef5943e78224723a29f0064cd3e5\r\nuntagged: docker.io\/library\/localimage:latest\r\n```\r\n"],"labels":["area\/builder","kind\/enhancement","area\/builder\/buildkit","containerd-integration","area\/ux"]},{"title":"logger\/fluentd: remove deprecated fluentd-async-connect option","body":"**- What I did**\r\n\r\nThis option was marked as deprecated in cc1f3c750 (released in v20.10). The option `fluentd-async`, introduced in the same commit, should be used instead.\r\n\r\nThere's no target for removal defined in our docs (cf. https:\/\/docs.docker.com\/engine\/deprecated\/#fluentd-async-connect-log-opt), but let's see what other maintainers think about dropping it in v25.0 or v26.0.\r\n\r\n**- Description for the changelog**\r\n\r\n* The Fluent logger option `fluentd-async-connect option` has been deprecated in v20.10 and is now removed.\r\n","comments":[],"labels":["area\/logging","status\/2-code-review","impact\/changelog"]},{"title":"api: Endpoints can now return a tree of errors","body":"**- What I did**\r\n\r\nThere're cases like input validation where it makes sense to gather multiple errors and return them all to the client.\r\n\r\nPrior to this PR, the API would only return a string `message` field. As a consequence, it requires those errors to be coalesced into a single string using a specific format, whether it's just a newline separator or some more complicated format.\r\n\r\nThis PR adds a new `errors` field containing a list of `ErrorResponse`.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![](https:\/\/media.npr.org\/assets\/img\/2017\/04\/25\/istock-115796521-fcf434f36d3d0865301cdcb9c996cfd80578ca99-s1100-c50.jpg)\r\n","comments":["So, the initial formatting problem isn't gone:\r\n\r\n```\r\nError response from daemon: invalid endpoint config for network testnet: invalid IPv4 address: foobar\r\ninvalid IPv6 address: foobar\r\ninvalid link-local IP address yolo\r\nuser specified IP address is supported only when connecting to networks with user configured subnets\r\n```\r\n\r\nThe original intent of nicely formatting the top-level message was to provide a backward-compatible way of supporting `errors.Join`. If we get that right, then newer versions of the CLI don't even need to consume the error tree; they can just stick with the formatted `message` field. However, serving error trees might still be of interest for API consumers that build a GUI, or otherwise need something different than what we'd provide in the formatted `message` field.\r\n\r\nAs we don't have a clear view of how this error tree should be consumed, there're disagreement about what errors should be skipped or not (see [here](https:\/\/github.com\/moby\/moby\/pull\/46099#discussion_r1287723755) and [here](https:\/\/github.com\/moby\/moby\/pull\/46099#discussion_r1287721501)).\r\n\r\nAlso, the formatting issue resurface at the logging level. We didn't discuss it so far, but we have the exact same problem, although less important. We'd have to create our own formatter to process errors set in [`logrus.Fields`](https:\/\/pkg.go.dev\/github.com\/sirupsen\/logrus#Fields) and reformat errors as we desire.\r\n\r\nAs an alternative, I'm proposing #46188. It's a temporary drop-in replacement for `errors.Join`; it lives in an internal package, has the same semantics as `errors.Join`, and only provide better formatting with no API changes required.\r\n"],"labels":["area\/api","status\/2-code-review","impact\/api"]},{"title":"Setting BIP and default-address-pools does not calculate networks correctly","body":"### Description\r\n\r\nGiven two non overlapping networks for the bip (172.30.0.1\/24) and for the default-address-pools (172.30.128.0\/17) with this daemon.json\r\n\r\n```\r\n{\r\n    \"bip\": \"172.30.0.1\/24\",\r\n    \"default-address-pools\": [\r\n        { \"base\":\"172.30.128.0\/17\",\"size\":24 }\r\n    ],\r\n    \"ipv6\": false\r\n}\r\n```\r\n\r\nresults in the error message\r\n\r\n`failed to start daemon: Error initializing network controller: error creating default \"bridge\" network: Pool overlaps with other one on this address space`\r\n\r\nwhen running `dockerd --debug`\r\n\r\n### Reproduce\r\n\r\n1. define bip = 172.30.0.1\/24 and default-address-pools = 172.30.128.0\/17 in daemon.json\r\n2. start docker\r\n\r\n### Expected behavior\r\n\r\nDocker is running\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:36:32 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:32 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 4\r\n Images: 5\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 4.18.0-477.15.1.el8_8.x86_64\r\n Operating System: Rocky Linux 8.8 (Green Obsidian)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 2\r\n Total Memory: 7.502GiB\r\n Name: vxxxxx\r\n ID: Pxxxx\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.30.0.0\/16, Size: 24\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["status\/0-triage","kind\/bug","area\/networking","area\/networking\/ipam","version\/24.0"]},{"title":"AppArmor profile is not loaded","body":"### Description\r\n\r\nWhen explicitly applying an AppArmor profile, processes run as unconfined.\r\n\r\n### Reproduce\r\n\r\n```\r\ncat > \/etc\/apparmor.d\/containers\/docker-empty <<EOF\r\n#include <tunables\/global>\r\n\r\nprofile docker-empty flags=(attach_disconnected,mediate_deleted) {\r\n  #include <abstractions\/base>\r\n  file,\r\n  deny \/etc\/** wl,\r\n\r\n  capability chown,\r\n  capability dac_override,\r\n  capability setuid,\r\n  capability setgid,\r\n  capability net_bind_service,\r\n}\r\nEOF\r\n\r\napparmor_parser -r -W \/etc\/apparmor.d\/containers\/docker-empty\r\n\r\ndocker run --security-opt \"apparmor=docker-empty\" --rm -it debian:12 bash -c \"sleep infinity\"\r\n\r\n# returns \"unconfined\"\r\ncat \/proc\/$(pgrep sleep)\/attr\/current \r\n```\r\n\r\n### Expected behavior\r\n\r\nProcesses should run with the AppArmor profile `docker-empty`.\r\nThis works correctly with `podman`.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.5\r\n API version:       1.43\r\n Go version:        go1.20.6\r\n Git commit:        ced0996\r\n Built:             Fri Jul 21 20:35:41 2023\r\n OS\/Arch:           linux\/arm64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:41 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.5\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 6\r\n  Running: 6\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 6\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: runc io.containerd.runc.v2\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: \r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-10-arm64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 4\r\n Total Memory: 7.567GiB\r\n Name: w1\r\n ID: \r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n`docker inspect <container id>`:\r\n\r\n```\r\n...\r\n\"AppArmorProfile\": \"docker-empty\",\r\n...\r\n  \"SecurityOpt\": [\r\n                \"apparmor=docker-empty\"\r\n            ],\r\n...\r\n```","comments":["Possibly related: https:\/\/github.com\/moby\/moby\/issues\/44984"],"labels":["area\/security\/apparmor","status\/0-triage","kind\/bug","version\/24.0"]},{"title":"Handle relative paths in VOLUME directives","body":"### Description\r\n\r\n- Related to https:\/\/github.com\/containerd\/containerd\/issues\/5547\r\n- Related to https:\/\/github.com\/opencontainers\/runc\/issues\/3944\r\n\r\nIn runc 1.0, runc started erroring out if a mount destination was a relative path.\r\nThis was changed to be just a warning here: https:\/\/github.com\/opencontainers\/runc\/pull\/3004\r\nAn issue was created to re-add the error: https:\/\/github.com\/opencontainers\/runc\/issues\/3020\r\nAs part of id-mapped mounts support, this was added back here: https:\/\/github.com\/opencontainers\/runc\/pull\/3717\r\n\r\nWe need to handle this case because we don't want to break old images or builds.\r\n\r\n### Reproduce\r\n\r\nN\/A\r\n\r\n### Expected behavior\r\n\r\nN\/A\r\n\r\n### docker version\r\n\r\n```bash\r\nN\/A\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nN\/A\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["So for `docker run` at least it looks like we're producing an error;\r\n\r\n```bash\r\ndocker run -it --rm --workdir=\/hello -v foovolume:world alpine\r\ndocker: Error response from daemon: invalid volume specification: 'foovolume:world': invalid mount config for type \"volume\": invalid mount path: 'world' mount path must be absolute.\r\n```\r\n\r\n```bash\r\ndocker run -it --rm --workdir=\/hello -v foovolume:.\/world alpine\r\ndocker: Error response from daemon: invalid volume specification: 'foovolume:.\/world': invalid mount config for type \"volume\": invalid mount path: '.\/world' mount path must be absolute.\r\n```\r\n\r\nFor `docker build` with BuildKit, it looks like we're silently allowing the relative path;\r\n\r\n```bash\r\ndocker build -t foo -<<'EOF'\r\nFROM alpine\r\nWORKDIR \/hello\r\nVOLUME .\/world\r\nVOLUME world2\r\nEOF\r\n```\r\n\r\nSo the image's config has the relative path:\r\n\r\n```bash\r\ndocker image inspect --format '{{ json .Config.Volumes}}' foo\r\n{\".\/world\":{},\"world2\":{}}\r\n```\r\n\r\nWhich gets fixed \"somewhere\" when running a container from it.\r\n\r\n```bash\r\ndocker run --rm foo sh -c 'mount | grep world'\r\n\/dev\/vda1 on \/world2 type ext4 (rw,relatime,data=ordered)\r\n\/dev\/vda1 on \/world type ext4 (rw,relatime,data=ordered)\r\n```\r\n"],"labels":["area\/runtime","kind\/bug"]},{"title":"Inconsistent behaviour of docker pull rate limit","body":"### Description\r\n\r\nAs i understand it, the rate limit on anonymous pulls does not apply to open source project images.\r\nI have verified this using the docker hub api and querying rate limit before and after pulling an alpine linux image, i.e. no change.\r\n\r\nHowever when I conduct the same experiment from within a container, running on the same host and \u2018billing\u2019 to the same IP, the rate limit is decremented. If I continue to pull more OS images, I can drive the limit to zero and I get \u201ctoomanyrequests: You have reached your pull rate limit\u2026\u201d\r\n\r\nReturning to the host, the rate limit also shows as zero, but I can pull the same open source image successfully as expected.\r\n\r\nWhy does the \u2018Open Source\u2019 nature of the pulled image not recognized from within the container?\r\n\r\nDocker version 23.0.5, build bc4487a on both host and within container\r\n\r\nOS: Debian bullseye\r\n\r\nThe container is using the docker socket from the host.\r\n\r\n### Reproduce\r\n\r\n```\r\nlimit () {     local TOKEN=\"\";     local LIMIT_DATA=\"\";     local LIMIT=\"\";     local LIMIT_SOURCE=\"\"     TOKEN=$(curl \"https:\/\/auth.docker.io\/token?service=registry.docker.io&scope=repository:ratelimitpreview\/test:pull\" 2> \/dev\/null | jq -r .token);     LIMIT_DATA=\"$(curl --head -H \"Authorization: Bearer $TOKEN\" https:\/\/registry-1.docker.io\/v2\/ratelimitpreview\/test\/manifests\/latest 2> \/dev\/null | grep ratelimit)\";     LIMIT=$(echo \"$LIMIT_DATA\" | grep ratelimit-remaining | sed \"s#ratelimit-remaining: ##\" | cut -d ';' -f 1);     LIMIT_SOURCE=$(echo \"$LIMIT_DATA\" | grep docker-ratelimit-source | sed \"s#docker-ratelimit-source: ##\" | cut -d ';' -f 1);     echo \"$LIMIT from $LIMIT_SOURCE\"; }\r\n```\r\n\r\n# Do not login,  use anonymous pulls ...\r\nlimit\r\ndocker pull alpine\r\nlimit\r\n\r\nIF run under debian bullseye on host, limit does not decrement.\r\nIf run under debian bullseye docker container limit does decrement.\r\nBoth are billed to the same ip address.\r\n\r\nIf a large enough number of images from within the container the limit drops to zero.\r\n\r\nThe next pull of an opensource qualified image will succeed from host, but fail from container.\r\n\r\n### Expected behavior\r\n\r\nthe docker pull limit does not decrement when run on a qualified open source image, not matter if the command is run inside of a container or not.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           23.0.5\r\n API version:       1.42\r\n Go version:        go1.19.8\r\n Git commit:        bc4487a\r\n Built:             Wed Apr 26 16:17:45 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          23.0.5\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.19.8\r\n  Git commit:       94d3ad6\r\n  Built:            Wed Apr 26 16:17:45 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.20\r\n  GitCommit:        2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc:\r\n  Version:          1.1.5\r\n  GitCommit:        v1.1.5-0-gf19387a\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.4\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.17.3\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.23.0\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-scan\r\n\r\nServer:\r\n Containers: 48\r\n  Running: 41\r\n  Paused: 0\r\n  Stopped: 7\r\n Images: 130\r\n Server Version: 23.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 2806fc1057397dbaeefbea0e4e17bddfbd388f38\r\n runc version: v1.1.5-0-gf19387a\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.10.0-21-amd64\r\n Operating System: Debian GNU\/Linux 11 (bullseye)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 72\r\n Total Memory: 125.8GiB\r\n Name: yow2-wrcp2-lx\r\n ID: fb802abb-6ed1-4880-a801-f587d994ff9f\r\n Docker Root Dir: \/localdisk\/var_lib_docker\r\n Debug Mode: false\r\n Username: slittlewrs\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["> Returning to the host, the rate limit also shows as zero, but I can pull the same open source image successfully as expected.\r\n\r\n\r\nIs your host a local machine or GitHub actions? GitHub actions sets up authentication with a token as default, which makes it not have rate-limits.","It was a local machine, and a container running on the same machine.  both relied on anonymous pulls.  Both are 'charged' to the same IP."],"labels":["status\/0-triage","kind\/bug"]},{"title":"Docker run command is taking around 10 min to return container id only on RHEL9.1","body":"### Description\r\n\r\nHere are our system info\r\nPlatform RHEL9.1\r\nDocker version: \r\n```\r\nClient: Docker Engine - Community\r\n Version:           23.0.1\r\n API version:       1.42\r\n Go version:        go1.19.5\r\n Git commit:        a5ee5b1\r\n Built:             Thu Feb  9 19:49:35 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n```\r\n\r\nThis issue is happening Intermittantly\r\n\r\nWhenever we try to run docker run commands for images which are already downloaded\/pulled the run command just hangs and returns containerid only after 8-10 min. \r\n\r\nwhen i turned on the debug logging for docker daemon it seems there is a 7 min difference between these two log lines \r\n\r\n```\r\nJul 26 17:20:11  dockerd[2213991]: time=\"2023-07-26T17:20:11.278168573Z\" level=debug msg=\"form data: {\\\"AttachStderr\\\":false\r\n```\r\n\r\n```\r\nJul 26 17:27:02 dockerd[2213991]: time=\"2023-07-26T17:27:02.286866485Z\" level=debug msg=\"container mounted via layerStore: &{\/home\/docker\/overlay2\/***\/merged 0x55620155de60 0x55620155de60}\" container=**\r\n```\r\n\r\n### Reproduce\r\n\r\ndocker run -d <image id>\r\n\r\n### Expected behavior\r\n\r\ndocker run should return container id immediately\r\n\r\n### docker version\r\n\r\n```bash\r\n[root@atm-10-168-1-110-gcp ~]# docker version\r\nClient: Docker Engine - Community\r\n Version:           23.0.1\r\n API version:       1.42\r\n Go version:        go1.19.5\r\n Git commit:        a5ee5b1\r\n Built:             Thu Feb  9 19:49:35 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          23.0.1\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.19.5\r\n  Git commit:       bc3805a\r\n  Built:            Thu Feb  9 19:46:32 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.18\r\n  GitCommit:        2456e983eb9e37e47538f59ea18f2043c9a73640\r\n runc:\r\n  Version:          1.1.4\r\n  GitCommit:        v1.1.4-0-g5fd4c4d\r\n docker-init:\r\n  Version:          0.19.0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\n[root@atm-10-168-1-110-gcp ~]# docker info\r\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 2\r\n  Paused: 0\r\n  Stopped: 1\r\n Images: 30\r\n Server Version: 23.0.1\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 2456e983eb9e37e47538f59ea18f2043c9a73640\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.14.0-162.6.1.el9_1.x86_64\r\n Operating System: Red Hat Enterprise Linux 9.1 (Plow)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 125.3GiB\r\n Name: *****\r\n ID: fbe5c1ff-0218-4d7c-9a3a-64aa9c2d7ec7\r\n Docker Root Dir: \/home\/docker\r\n Debug Mode: true\r\n  File Descriptors: 36\r\n  Goroutines: 43\r\n  System Time: 2023-07-27T10:53:09.93405536Z\r\n  EventsListeners: 1\r\n HTTP Proxy: *\r\n HTTPS Proxy: *\r\n No Proxy: sfdc.net,salesforce.com,gcr.io,googleapis.com,localhost\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Default Address Pools:\r\n   Base: 172.16.0.0\/22, Size: 26\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nDocker run command hangs and returns container id only after 10 min","comments":["You're on a pretty old containerd version; can you try with 1.6.21? ","@neersighted Thanks for response i tried with containerd version 1.6.21 and docker ce version  - the behaviour is still same - taking 7 min to return the container id - The delay is during the mounting of container to layer store  - Its a big image of oracle (9gb) can that cause this delay ?\r\nClient: Docker Engine - Community\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:53:24 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:51:50 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8","Observation: when we tested the same image on centos7 - we are not seeing this time delay for docker run command\r\nIts happening only on RHEL9.1","Can you dump a stack trace? More details at https:\/\/docs.docker.com\/config\/daemon\/logs\/#force-a-stack-trace-to-be-logged","[goroutine-stacks-2023-07-28T161356Z.log.zip](https:\/\/github.com\/moby\/moby\/files\/12196407\/goroutine-stacks-2023-07-28T161356Z.log.zip)\r\n@bsousaa  ","Last chunk of that dump shows something around unmounting;\r\n\r\n```\r\ngoroutine 19534 [syscall, 3 minutes]:\r\nsyscall.Syscall(0x62?, 0xc000d8b400?, 0xc000c96070?, 0x63?)\r\n\t\/usr\/local\/go\/src\/syscall\/syscall_linux.go:69 +0x27\r\ngithub.com\/docker\/docker\/vendor\/golang.org\/x\/sys\/unix.Unmount({0xc000c96070?, 0xc000c96070?}, 0x45?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/golang.org\/x\/sys\/unix\/zsyscall_linux.go:1701 +0x7d\r\ngithub.com\/docker\/docker\/daemon\/graphdriver\/overlay2.(*Driver).Put(0xc0009a8380, {0xc0013fd5e0, 0x45})\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/daemon\/graphdriver\/overlay2\/overlay.go:633 +0x2eb\r\ngithub.com\/docker\/docker\/layer.(*layerStore).initMount(0xc0000c6280, {0xc0010975c0?, 0xc0013fd590?}, {0xc00023dcc0, 0x40}, {0x0, 0x0}, 0xc000cc9440, 0x0)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/layer\/layer_store.go:686 +0x198\r\ngithub.com\/docker\/docker\/layer.(*layerStore).CreateRWLayer(0xc0000c6280, {0xc001097180, 0x40}, {0xc0013fd590, 0x47}, 0x556669b9ca20?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/layer\/layer_store.go:536 +0x4a9\r\ngithub.com\/docker\/docker\/daemon\/images.(*ImageService).CreateLayer(0xc000c9c2c0, 0xc00061a500, 0xc000cc9440)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/daemon\/images\/service.go:135 +0x13e\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).create(0xc0006286c0, {0x55666a887d10, 0xc000ad2bd0}, {{{0xc001096263, 0x9}, 0xc000618780, 0xc000dccd80, 0xc000014238, 0x0, 0x0}, ...})\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/daemon\/create.go:191 +0x5f0\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).containerCreate(0xc0006286c0, {0x55666a887d10, 0xc000ad2bd0}, {{{0xc001096263, 0x9}, 0xc000618780, 0xc000dccd80, 0xc000014238, 0x0, 0x0}, ...})\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/daemon\/create.go:102 +0x685\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).ContainerCreate(0xc0012f2460?, {0x55666a887d10?, 0xc000ad2bd0?}, {{0xc001096263, 0x9}, 0xc000618780, 0xc000dccd80, 0xc000014238, 0x0, 0x0})\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/daemon\/create.go:45 +0xe5\r\ngithub.com\/docker\/docker\/api\/server\/router\/container.(*containerRouter).postContainersCreate(0xc0010ad180, {0x55666a887d10, 0xc000ad2bd0}, {0x55666a8864d0, 0xc0006362a0}, 0xc000136e00, 0xc0005e0580?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/api\/server\/router\/container\/container_routes.go:591 +0x8d6\r\ngithub.com\/docker\/docker\/api\/server\/middleware.ExperimentalMiddleware.WrapHandler.func1({0x55666a887d10, 0xc000ad2bd0}, {0x55666a8864d0?, 0xc0006362a0?}, 0x55666a370840?, 0xc0012f34b0?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/api\/server\/middleware\/experimental.go:26 +0x15b\r\ngithub.com\/docker\/docker\/api\/server\/middleware.VersionMiddleware.WrapHandler.func1({0x55666a887d10, 0xc000ad2570}, {0x55666a8864d0, 0xc0006362a0}, 0xc000128040?, 0xc00113e208?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/api\/server\/middleware\/version.go:62 +0x4d7\r\ngithub.com\/docker\/docker\/pkg\/authorization.(*Middleware).WrapHandler.func1({0x55666a887d10, 0xc000ad2570}, {0x55666a8864d0?, 0xc0006362a0?}, 0xc000136e00, 0x1?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/pkg\/authorization\/middleware.go:59 +0x649\r\ngithub.com\/docker\/docker\/api\/server\/middleware.DebugRequestMiddleware.func1({0x55666a887d10, 0xc000ad2570}, {0x55666a8864d0, 0xc0006362a0}, 0xc000136e00, 0xc0012f2270?)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/api\/server\/middleware\/debug.go:53 +0x5c5\r\ngithub.com\/docker\/docker\/api\/server.(*Server).makeHTTPHandler.func1({0x55666a8864d0, 0xc0006362a0}, 0xc000136d00)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/api\/server\/server.go:53 +0x1ce\r\nnet\/http.HandlerFunc.ServeHTTP(0xc000136b00?, {0x55666a8864d0?, 0xc0006362a0?}, 0xc000d979e8?)\r\n\t\/usr\/local\/go\/src\/net\/http\/server.go:2122 +0x2f\r\ngithub.com\/docker\/docker\/vendor\/github.com\/gorilla\/mux.(*Router).ServeHTTP(0xc000001980, {0x55666a8864d0, 0xc0006362a0}, 0xc000136a00)\r\n\t\/root\/rpmbuild\/BUILD\/src\/engine\/.gopath\/src\/github.com\/docker\/docker\/vendor\/github.com\/gorilla\/mux\/mux.go:210 +0x1cf\r\nnet\/http.serverHandler.ServeHTTP({0x55666a876ba0?}, {0x55666a8864d0, 0xc0006362a0}, 0xc000136a00)\r\n\t\/usr\/local\/go\/src\/net\/http\/server.go:2936 +0x316\r\nnet\/http.(*conn).serve(0xc000d00120, {0x55666a887d10, 0xc000a2d3b0})\r\n\t\/usr\/local\/go\/src\/net\/http\/server.go:1995 +0x612\r\ncreated by net\/http.(*Server).Serve\r\n\t\/usr\/local\/go\/src\/net\/http\/server.go:3089 +0x5ed\r\n```\r\n\r\nFollowing the last few bits of that (note: dump above is in reverse order);\r\n\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/daemon\/create.go#L45\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/daemon\/create.go#L102\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/daemon\/create.go#L191\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/daemon\/images\/service.go#L135\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/layer\/layer_store.go#L536\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/layer\/layer_store.go#L686\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/daemon\/graphdriver\/overlay2\/overlay.go#L633\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/vendor\/golang.org\/x\/sys\/unix\/zsyscall_linux.go#L1701\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/vendor\/golang.org\/x\/sys\/unix\/syscall_linux.go#L69\r\n\r\nThat last line is part of this function:\r\nhttps:\/\/github.com\/moby\/moby\/blob\/659604f9ee60f147020bdd444b26e4b5c636dc28\/vendor\/golang.org\/x\/sys\/unix\/syscall_linux.go#L65-L75\r\n","@thaJeztah what is it trying to unmount ? is it unable to mount the container ?","hi @thaJeztah could you please clarify ?"],"labels":["status\/0-triage","kind\/bug","version\/23.0"]},{"title":"ci: remove check for cgroupv2 in integration-cli","body":"**- What I did**\r\n\r\nRemoved the check for cgroup v2 for the integration-cli tests. Draft for now, I want to see what fails.\r\n\r\n**- How I did it**\r\n\r\n**- How to verify it**\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["@AkihiroSuda do you recall any reason we were excluding these on cgroupv2 or was it just \"cgroup v2 still being a work in progress\" at the time? (we were discussing that and weren't sure)","> @AkihiroSuda do you recall any reason we were excluding these on cgroupv2 or was it just \"cgroup v2 still being a work in progress\" at the time? (we were discussing that and weren't sure)\r\n\r\nhttps:\/\/github.com\/moby\/moby\/pull\/41065\/files#r435472539","Ah, nice, so it really was \"too much changes to make individual tests to be skipped\".\r\n\r\n\r\nLooks like we did a good job then, fixing them all without knowing \ud83e\uddc1","@rumpl could you address @AkihiroSuda's comment?","Right, so we can't really remove the `TEST_SKIP_INTEGRATION_CLI` because it's used to split integration and integration-cli tests, I did however add a matrix for the integration-cli tests (I added ubuntu 22.04) and indeed there are real failures, I'l take a look, see what can be done."],"labels":["status\/2-code-review","area\/testing","area\/cgroup2"]},{"title":"chore: remove daemon.reduceContainers and containerReducer","body":"Looks like this functionality is no longer used, so we can probably include all of this in `Daemon.Containers`;\r\n\r\n> I see some cleaning opportunities up in the future \ud83e\ude84 . Looks like `daemon.reduceContainers` is only called in one place. And that always calls it with `daemon.refreshImage` as argument, so we can probably get rid of all that indirection.\r\n\r\n\r\n_Originally posted by @thaJeztah in https:\/\/github.com\/moby\/moby\/pull\/46081#discussion_r1275093140_\r\n            ","comments":["Hey @thaJeztah , looks like no one started working on this. Can you assign this to me please?","@vchiranjeeviak github doesn't allow me to assign, but let me add a label \"status\/claimed\" on it! Thanks in advance for working on this; let me know if you need more input on what needs to be done (admitted, I noticed we could probably get rid of it, but didn't make a full pass over all the code affected).","Thanks @thaJeztah . I m starting my open-source journey with this. I hope this is not something very urgent. I can definitely use your mentorship whenever possible.","The function `daemon.reduceContainers` is only called inside `daemon.Containers` and that is the only line of code inside `daemon.Containers`. So, if we can eliminate the `daemon.reduceContainers` and add that code inside `daemon.Containers`, it avoids one extra function call. The `daemon.refreshImage` is also called only inside `daemon.reduceContainers`, but this function has it's own functionality and looks better to keep it separated that way.","Also, the test file `list_test.go` which tests the list feature only involves `Daemon.Containers` and not any other functions. So, I don't think any changes to the tests are required in this case. Can I go ahead and raise PR for this approach? @thaJeztah ","Thank you for your interest in contributing to Moby!\r\n\r\nYes, your approach seems to be aligned with what we have in mind!\r\nEliminating `daemon.reduceContainers` and inlining it into `daemon.Containers` definitely makes sense there.\r\nHaving done that, the `containerReducer` will no longer be used by anything, so in result we get rid of the whole \"reducer\" concept, so we should probably also rename the `reduce*` functions into something more descriptive (although I must admit, I don't have a good candidate for a name yet \ud83d\ude05).\r\n`refreshImage` definitely deserves to stay in its own function.\r\n\r\n> Can I go ahead and raise PR for this approach? \r\n\r\nAlways feel free to open a PR! Even if you're unsure if your changes are correct, having a draft PR is always a good starting point for a discussion.","Hey, please check the changes in PR and let me know if you see any issues with the code or any improvements can be done. Thanks!"],"labels":["exp\/beginner","exp\/intermediate","status\/claimed","area\/daemon","kind\/refactor"]},{"title":"Docker daemon can't kill an unhealthy container and start a new one","body":"### Description\r\n\r\nHello there,\r\n\r\nI have several 3-node Swarm clusters running in different regions. Most of them work on machines with Ubuntu Bionic and Docker 20.10.12. A few weeks ago I created one more cluster with Ubuntu Jammy and Docker 24.0.4, because I want to test the upgrade from the old Ubuntu to the new one. I use exactly the same Docker images on it as on the rest of the clusters.\r\n\r\nMost of the services work without any problem, only one of them breaks down after a few hours of operation, for example:\r\n```\r\nubuntu@my-swarm-node-1:~$ docker node ls\r\nID                            HOSTNAME          STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\r\n8wbwbatp5ojc0wb9ldjgt8rrf *   my-swarm-node-1   Ready     Active         Leader           24.0.4\r\nt28vtvaby72edvv0vka4cqmtx     my-swarm-node-2   Ready     Active         Reachable        24.0.4\r\n7hy719eg484smgx4ww3zy1cp7     my-swarm-node-3   Ready     Active         Reachable        24.0.4\r\nubuntu@my-swarm-node-1:~$\r\nubuntu@my-swarm-node-2:~$ docker ps |grep unhealthy\r\nb5715b8730e9   my-api-gateway:5.0.1      \"docker-entrypoint.sh\"   10 hours ago   Up 10 hours (unhealthy)             my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\r\nubuntu@my-swarm-node-2:~$\r\nubuntu@my-swarm-node-2:~$ sudo grep b5715b8730e9 \/var\/log\/syslog\r\nJul 24 09:49:26 my-swarm-node-2 dockerd[1056697]: time=\"2023-07-24T09:49:26.674053385Z\" level=info msg=\"Configured log driver does not support reads, enabling local file cache for container logs\" container=b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86 driver=gelf\r\nJul 24 09:49:26 my-swarm-node-2 containerd[832]: time=\"2023-07-24T09:49:26.681052173Z\" level=info msg=\"starting signal loop\" namespace=moby path=\/run\/containerd\/io.containerd.runtime.v2.task\/moby\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86 pid=1061772 runtime=io.containerd.runc.v2\r\nJul 24 09:49:26 my-swarm-node-2 systemd[1]: Started libcontainer container b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86.\r\nJul 24 15:17:33 my-swarm-node-2 dockerd[1056697]: time=\"2023-07-24T15:17:33.016085570Z\" level=info msg=\"Container failed to exit within 10s of signal 15 - using the force\" container=b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\r\nJul 24 15:17:33 my-swarm-node-2 systemd[1]: docker-b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86.scope: Deactivated successfully.\r\nJul 24 15:17:33 my-swarm-node-2 systemd[1]: docker-b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86.scope: Consumed 1min 47.315s CPU time.\r\nJul 24 15:17:43 my-swarm-node-2 dockerd[1056697]: time=\"2023-07-24T15:17:43.032267014Z\" level=error msg=\"Container failed to exit within 10s of kill - trying direct SIGKILL\" container=b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86 error=\"context deadline exceeded\"\r\nubuntu@my-swarm-node-2:~$\r\n```\r\nAs you can see below I'm not able to check the failed container, because the Docker daemon becomes unresponsive:\r\n```\r\nubuntu@my-swarm-node-2:~$ docker logs -n 100 b5715b8730e9\r\n^C\r\nubuntu@my-swarm-node-2:~$\r\nubuntu@my-swarm-node-2:~$ docker inspect b5715b8730e9\r\n^C\r\nubuntu@my-swarm-node-2:~$\r\n```\r\nI can't kill the container either:\r\n```\r\nubuntu@my-swarm-node-2:~$ docker kill b5715b8730e9\r\n^C\r\nubuntu@my-swarm-node-2:~$\r\n```\r\nIt seems that container process doesn't exist, but the Docker daemon still keeps its internal data:\r\n```\r\nroot@my-swarm-node-2:\/home\/ubuntu# find \/var\/lib\/docker\/containers\/b5715b8730e9*\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/hosts\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/resolv.conf.hash\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/container-cached.log\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/hostconfig.json\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/config.v2.json\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/resolv.conf\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/mounts\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/checkpoints\r\n\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/hostname\r\nroot@my-swarm-node-2:\/home\/ubuntu#\r\n```\r\nI noticed the Docker daemon still tries to get the network stats, but it can't do it, so the number of connections to `\/run\/docker.sock` socket still increases:\r\n```\r\nubuntu@my-swarm-node-2:\/home\/ubuntu# netstat |grep docker.sock |wc -l\r\n1823\r\nubuntu@my-swarm-node-2:\/home\/ubuntu#\r\n```\r\nThe only way to fix the issue is to stop and start the Docker daemon. But that only helps for a few hours.\r\n\r\nThe problem sometimes occurs on `my-swarm-node-2` instance, but sometimes it happens also on `my-swarm-node-1` and `my-swarm-node-3` ones.\r\n\r\nI checked the old Docker 20.10.13 as well, but that didn't help either. There is also an error there. So now I'm not sure if the issue is related to Docker or Ubuntu Jammy.\r\n\r\nHow can I investigate the issue? As you can see above the Docker daemon is not too verbose. I also can't see any related system events during this time.\r\n\r\nI'll be thankful for any hints :)\r\n\r\n### Reproduce\r\n\r\nHere I need to deploy my Swarm stack from scratch and wait for about 5-6 hours.\r\n\r\n### Expected behavior\r\n\r\nDocker should remove an unhealthy container, cleanup all associated data and launch a new container.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:           24.0.4\r\n API version:       1.43\r\n Go version:        go1.20.5\r\n Git commit:        3713ee1\r\n Built:             Fri Jul  7 14:50:55 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.4\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.5\r\n  Git commit:       4ffc614\r\n  Built:            Fri Jul  7 14:50:55 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient: Docker Engine - Community\r\n Version:    24.0.4\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 11\r\n  Running: 8\r\n  Paused: 0\r\n  Stopped: 3\r\n Images: 16\r\n Server Version: 24.0.4\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: gelf\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: t28vtvaby72edvv0vka4cqmtx\r\n  Is Manager: true\r\n  ClusterID: 2jujnedl6u4hkfz1xjmkkltb6\r\n  Managers: 3\r\n  Nodes: 3\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 172.31.42.234\r\n  Manager Addresses:\r\n   172.31.0.43:2377\r\n   172.31.23.159:2377\r\n   172.31.42.234:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-76-generic\r\n Operating System: Ubuntu 22.04.2 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 62.79GiB\r\n Name: my-swarm-node-2\r\n ID: 7b2258dd-3d3d-4ead-96b2-be5bf8080ce3\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\nA piece of the goroutine dump can be found at https:\/\/gist.github.com\/ptecza\/f47688cd357083ae350586bdf2e2e280","comments":["BTW, the old Docker 20.10.13 was reporting that container was failing the health-check, for example:\r\n```\r\nroot@my-swarm-node-1:\/home\/ubuntu# grep bf1ae0a8ebd3 \/var\/log\/syslog\r\nJul 21 12:38:46 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T12:38:46.026372588Z\" level=info msg=\"Configured log driver does not support reads, enabling local file cache for container logs\" container=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae driver=gelf\r\nJul 21 12:38:46 my-swarm-node-1 containerd[834]: time=\"2023-07-21T12:38:46.034895847Z\" level=info msg=\"starting signal loop\" namespace=moby path=\/run\/containerd\/io.containerd.runtime.v2.task\/moby\/bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae pid=3893481 runtime=io.containerd.runc.v2\r\nJul 21 12:38:46 my-swarm-node-1 systemd[1]: Started libcontainer container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae.\r\nJul 21 17:25:29 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:25:29.824466896Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:25:54 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:25:54.833463175Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:26:19 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:26:19.841862398Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:26:44 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:26:44.849601790Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:27:09 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:27:09.857627483Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:27:34 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:27:34.865193054Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:27:59 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:27:59.872366792Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:28:24 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:28:24.883499044Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:28:49 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:28:49.892677136Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:29:14 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:29:14.899047856Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:29:39 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:29:39.905731139Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:30:04 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:30:04.912773816Z\" level=warning msg=\"Health check for container bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error: context deadline exceeded\"\r\nJul 21 17:30:16 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:30:16.936292134Z\" level=info msg=\"Container failed to exit within 10s of signal 15 - using the force\" container=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae\r\nJul 21 17:30:16 my-swarm-node-1 systemd[1]: docker-bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae.scope: Deactivated successfully.\r\nJul 21 17:30:16 my-swarm-node-1 systemd[1]: docker-bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae.scope: Consumed 1min 34.955s CPU time.\r\nJul 21 17:30:18 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:30:18.972938929Z\" level=info msg=\"ignoring event\" container=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae module=libcontainerd namespace=moby topic=\/tasks\/delete type=\"*events.TaskDelete\"\r\nJul 21 17:30:18 my-swarm-node-1 containerd[834]: time=\"2023-07-21T17:30:18.972999489Z\" level=info msg=\"shim disconnected\" id=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae\r\nJul 21 17:30:18 my-swarm-node-1 containerd[834]: time=\"2023-07-21T17:30:18.973991641Z\" level=warning msg=\"cleaning up after shim disconnected\" id=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae namespace=moby\r\nJul 21 17:30:26 my-swarm-node-1 dockerd[3335926]: time=\"2023-07-21T17:30:26.951785900Z\" level=error msg=\"Container failed to exit within 10 seconds of kill - trying direct SIGKILL\" container=bf1ae0a8ebd3e9064f199458921b06437ca269cd85c6428a92c7409fd39b9eae error=\"context deadline exceeded\"\r\nroot@my-swarm-node-1:\/home\/ubuntu#\r\n```\r\nThe latest version of Docker doesn't do it. Why? Should I change the log level?","After you restarted the daemon, what's the status that's shown for the container (in the containers inspect). Or if you are in that state, what do the `hostconfig.json` and `config.v2.json` files of the container show?\r\n\r\nI'm curious if the daemon doesn't update the container's state (which should be \"stopped\" or \"exited\" after it's killed), and therefore attempts to access it.\r\n\r\nIs there anything you can share about the container and\/or healthcheck that's running? (also details such as if the container is bind-mounting things or something special could always be useful).\r\n","Oh! I see you attached a goroutine dump as well; haven't found time yet to look at that \ud83d\ude05 ","Hello @thaJeztah \r\n\r\nThanks a lot for your quick response! :)\r\n\r\nI haven't restarted the Docker daemon recently, so it still shows me the failed container:\r\n```\r\nroot@my-swarm-node-2:\/home\/ubuntu# docker ps |grep unhealthy\r\nb5715b8730e9   my-api-gateway:5.0.1      \"docker-entrypoint.sh\"   2 days ago   Up 2 days (unhealthy)             my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\r\nroot@my-swarm-node-2:\/home\/ubuntu#\r\n```\r\nBelow there is a content you asked for:\r\n```\r\nroot@my-swarm-node-2:\/home\/ubuntu# ls -l \/var\/lib\/docker\/containers\/b5715b8730e9*\r\ntotal 76\r\ndrwx------ 2 root root  4096 Jul 24 09:49 checkpoints\r\n-rw------- 1 root root  7898 Jul 24 15:17 config.v2.json\r\n-rw-r----- 1 root root 35476 Jul 24 09:55 container-cached.log\r\n-rw------- 1 root root  2609 Jul 24 15:17 hostconfig.json\r\n-rw-r--r-- 1 root root    19 Jul 24 09:49 hostname\r\n-rw-r--r-- 1 root root   181 Jul 24 09:49 hosts\r\ndrwx--x--- 2 root root  4096 Jul 24 09:49 mounts\r\n-rw-r--r-- 1 root root    87 Jul 24 09:49 resolv.conf\r\n-rw-r--r-- 1 root root    71 Jul 24 09:49 resolv.conf.hash\r\nroot@my-swarm-node-2:\/home\/ubuntu#\r\nroot@my-swarm-node-2:\/home\/ubuntu# cat \/var\/lib\/docker\/containers\/b5715b8730e9*\/hostconfig.json |jq -r\r\n{\r\n  \"Binds\": null,\r\n  \"ContainerIDFile\": \"\",\r\n  \"LogConfig\": {\r\n    \"Type\": \"gelf\",\r\n    \"Config\": {\r\n      \"gelf-address\": \"tcp:\/\/logstash.service.my.consul:12201\",\r\n      \"labels\": \"service,my_service,swarm_stack,swarm_service\"\r\n    }\r\n  },\r\n  \"NetworkMode\": \"default\",\r\n  \"PortBindings\": {},\r\n  \"RestartPolicy\": {\r\n    \"Name\": \"\",\r\n    \"MaximumRetryCount\": 0\r\n  },\r\n  \"AutoRemove\": false,\r\n  \"VolumeDriver\": \"\",\r\n  \"VolumesFrom\": null,\r\n  \"ConsoleSize\": [\r\n    0,\r\n    0\r\n  ],\r\n  \"CapAdd\": null,\r\n  \"CapDrop\": null,\r\n  \"CgroupnsMode\": \"private\",\r\n  \"Dns\": null,\r\n  \"DnsOptions\": null,\r\n  \"DnsSearch\": null,\r\n  \"ExtraHosts\": null,\r\n  \"GroupAdd\": null,\r\n  \"IpcMode\": \"private\",\r\n  \"Cgroup\": \"\",\r\n  \"Links\": null,\r\n  \"OomScoreAdj\": 0,\r\n  \"PidMode\": \"\",\r\n  \"Privileged\": false,\r\n  \"PublishAllPorts\": false,\r\n  \"ReadonlyRootfs\": false,\r\n  \"SecurityOpt\": null,\r\n  \"UTSMode\": \"\",\r\n  \"UsernsMode\": \"\",\r\n  \"ShmSize\": 67108864,\r\n  \"Runtime\": \"runc\",\r\n  \"Isolation\": \"default\",\r\n  \"CpuShares\": 0,\r\n  \"Memory\": 536870912,\r\n  \"NanoCpus\": 1000000000,\r\n  \"CgroupParent\": \"\",\r\n  \"BlkioWeight\": 0,\r\n  \"BlkioWeightDevice\": null,\r\n  \"BlkioDeviceReadBps\": null,\r\n  \"BlkioDeviceWriteBps\": null,\r\n  \"BlkioDeviceReadIOps\": null,\r\n  \"BlkioDeviceWriteIOps\": null,\r\n  \"CpuPeriod\": 0,\r\n  \"CpuQuota\": 0,\r\n  \"CpuRealtimePeriod\": 0,\r\n  \"CpuRealtimeRuntime\": 0,\r\n  \"CpusetCpus\": \"\",\r\n  \"CpusetMems\": \"\",\r\n  \"Devices\": null,\r\n  \"DeviceCgroupRules\": null,\r\n  \"DeviceRequests\": null,\r\n  \"MemoryReservation\": 0,\r\n  \"MemorySwap\": 1073741824,\r\n  \"MemorySwappiness\": null,\r\n  \"OomKillDisable\": null,\r\n  \"PidsLimit\": null,\r\n  \"Ulimits\": [],\r\n  \"CpuCount\": 0,\r\n  \"CpuPercent\": 0,\r\n  \"IOMaximumIOps\": 0,\r\n  \"IOMaximumBandwidth\": 0,\r\n  \"Mounts\": [\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/region.env\",\r\n      \"Target\": \"\/region.env\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/keycloak.env\",\r\n      \"Target\": \"\/keycloak.env\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway-containerpilot.json5\",\r\n      \"Target\": \"\/etc\/containerpilot.json5\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway.json\",\r\n      \"Target\": \"\/my-api-gateway.json\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/logrotate\",\r\n      \"Target\": \"\/root\/logrotate\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-my\",\r\n      \"Target\": \"\/root\/cron-my\"\r\n    },\r\n    {\r\n      \"Type\": \"bind\",\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-crontab\",\r\n      \"Target\": \"\/root\/cron-crontab\"\r\n    }\r\n  ],\r\n  \"MaskedPaths\": [\r\n    \"\/proc\/asound\",\r\n    \"\/proc\/acpi\",\r\n    \"\/proc\/kcore\",\r\n    \"\/proc\/keys\",\r\n    \"\/proc\/latency_stats\",\r\n    \"\/proc\/timer_list\",\r\n    \"\/proc\/timer_stats\",\r\n    \"\/proc\/sched_debug\",\r\n    \"\/proc\/scsi\",\r\n    \"\/sys\/firmware\"\r\n  ],\r\n  \"ReadonlyPaths\": [\r\n    \"\/proc\/bus\",\r\n    \"\/proc\/fs\",\r\n    \"\/proc\/irq\",\r\n    \"\/proc\/sys\",\r\n    \"\/proc\/sysrq-trigger\"\r\n  ]\r\n}\r\nroot@my-swarm-node-2:\/home\/ubuntu#\r\nroot@my-swarm-node-2:\/home\/ubuntu# cat \/var\/lib\/docker\/containers\/b5715b8730e9*\/config.v2.json |jq -r\r\n{\r\n  \"StreamConfig\": {},\r\n  \"State\": {\r\n    \"Running\": true,\r\n    \"Paused\": false,\r\n    \"Restarting\": false,\r\n    \"OOMKilled\": false,\r\n    \"RemovalInProgress\": false,\r\n    \"Dead\": false,\r\n    \"Pid\": 1061798,\r\n    \"ExitCode\": 0,\r\n    \"Error\": \"\",\r\n    \"StartedAt\": \"2023-07-24T09:49:27.044834116Z\",\r\n    \"FinishedAt\": \"0001-01-01T00:00:00Z\",\r\n    \"Health\": {\r\n      \"Status\": \"unhealthy\",\r\n      \"FailingStreak\": 12,\r\n      \"Log\": [\r\n        {\r\n          \"Start\": \"2023-07-24T15:15:30.630761565Z\",\r\n          \"End\": \"2023-07-24T15:15:40.696624729Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:15:55.705057663Z\",\r\n          \"End\": \"2023-07-24T15:16:05.76224049Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:16:20.771568439Z\",\r\n          \"End\": \"2023-07-24T15:16:30.829649436Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:16:45.839295206Z\",\r\n          \"End\": \"2023-07-24T15:16:55.902240173Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:17:10.910921222Z\",\r\n          \"End\": \"2023-07-24T15:17:20.986432895Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  \"ID\": \"b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\",\r\n  \"Created\": \"2023-07-24T09:49:26.457059856Z\",\r\n  \"Managed\": true,\r\n  \"Path\": \"docker-entrypoint.sh\",\r\n  \"Args\": [],\r\n  \"Config\": {\r\n    \"Hostname\": \"my-api-gateway-2\",\r\n    \"Domainname\": \"\",\r\n    \"User\": \"\",\r\n    \"AttachStdin\": false,\r\n    \"AttachStdout\": false,\r\n    \"AttachStderr\": false,\r\n    \"Tty\": false,\r\n    \"OpenStdin\": false,\r\n    \"StdinOnce\": false,\r\n    \"Env\": [\r\n      \"MY_SERVICE=my-api-gateway\",\r\n      \"CONSUL_HTTP_ADDR=http:\/\/172.17.0.1:8500\",\r\n      \"REPLICA_NUM=2\",\r\n      \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\",\r\n      \"CONTAINER=my-api-gateway\"\r\n    ],\r\n    \"Cmd\": [\r\n      \"docker-entrypoint.sh\"\r\n    ],\r\n    \"Healthcheck\": {\r\n      \"Test\": [\r\n        \"CMD-SHELL\",\r\n        \"\/usr\/local\/bin\/health-check-my-api-gateway.sh || exit 1\"\r\n      ],\r\n      \"Interval\": 15000000000,\r\n      \"Timeout\": 10000000000,\r\n      \"StartPeriod\": 40000000000,\r\n      \"Retries\": 12\r\n    },\r\n    \"Image\": \"my-api-gateway:5.0.1\",\r\n    \"Volumes\": null,\r\n    \"WorkingDir\": \"\/\",\r\n    \"Entrypoint\": null,\r\n    \"OnBuild\": null,\r\n    \"Labels\": {\r\n      \"my_service\": \"my-api-gateway\",\r\n      \"com.docker.stack.namespace\": \"my-platform\",\r\n      \"com.docker.swarm.node.id\": \"t28vtvaby72edvv0vka4cqmtx\",\r\n      \"com.docker.swarm.service.id\": \"rf2eo0jmycm8frrkti4g2kca2\",\r\n      \"com.docker.swarm.service.name\": \"my-platform_my-api-gateway\",\r\n      \"com.docker.swarm.task\": \"\",\r\n      \"com.docker.swarm.task.id\": \"ja6uydyxp88eqpjlicy1a5brx\",\r\n      \"com.docker.swarm.task.name\": \"my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\",\r\n      \"service\": \"swarm\",\r\n      \"swarm_service\": \"my-platform_my-api-gateway\",\r\n      \"swarm_stack\": \"my-platform\"\r\n    }\r\n  },\r\n  \"Image\": \"sha256:59145fffb1f1768dfd5248cea62dd2315f5d5dadd74f9932f7ebb4e49571a2a9\",\r\n  \"ImageManifest\": null,\r\n  \"NetworkSettings\": {\r\n    \"Bridge\": \"\",\r\n    \"SandboxID\": \"fb2ebe2e48be1858d715249d1696aaed1aec8c46ae67978f853db36d40758936\",\r\n    \"HairpinMode\": false,\r\n    \"LinkLocalIPv6Address\": \"\",\r\n    \"LinkLocalIPv6PrefixLen\": 0,\r\n    \"Networks\": {\r\n      \"ingress\": {\r\n        \"IPAMConfig\": {\r\n          \"IPv4Address\": \"10.0.0.13\"\r\n        },\r\n        \"Links\": null,\r\n        \"Aliases\": [\r\n          \"b5715b8730e9\",\r\n          \"my-api-gateway-2\"\r\n        ],\r\n        \"NetworkID\": \"i7k77izvi13b6xd3795nf3bff\",\r\n        \"EndpointID\": \"f70bd8205fd5e545f7b410828ce963af7aae0f250b85677237608a12f2206b60\",\r\n        \"Gateway\": \"\",\r\n        \"IPAddress\": \"10.0.0.13\",\r\n        \"IPPrefixLen\": 24,\r\n        \"IPv6Gateway\": \"\",\r\n        \"GlobalIPv6Address\": \"\",\r\n        \"GlobalIPv6PrefixLen\": 0,\r\n        \"MacAddress\": \"02:42:0a:00:00:0d\",\r\n        \"DriverOpts\": null,\r\n        \"IPAMOperational\": false\r\n      },\r\n      \"platform_mgmt_net\": {\r\n        \"IPAMConfig\": {\r\n          \"IPv4Address\": \"10.10.10.24\"\r\n        },\r\n        \"Links\": null,\r\n        \"Aliases\": [\r\n          \"b5715b8730e9\",\r\n          \"my-api-gateway-2\"\r\n        ],\r\n        \"NetworkID\": \"wrq3f15nh4f4t5tly0b9kz2di\",\r\n        \"EndpointID\": \"379f26fcb71bdd389bee21195fb82376a0d10edba1189379ed9bbe5fd1429170\",\r\n        \"Gateway\": \"\",\r\n        \"IPAddress\": \"10.10.10.24\",\r\n        \"IPPrefixLen\": 24,\r\n        \"IPv6Gateway\": \"\",\r\n        \"GlobalIPv6Address\": \"\",\r\n        \"GlobalIPv6PrefixLen\": 0,\r\n        \"MacAddress\": \"02:42:0a:0a:0a:18\",\r\n        \"DriverOpts\": null,\r\n        \"IPAMOperational\": false\r\n      }\r\n    },\r\n    \"Service\": {\r\n      \"ID\": \"rf2eo0jmycm8frrkti4g2kca2\",\r\n      \"Name\": \"my-platform_my-api-gateway\",\r\n      \"Aliases\": {\r\n        \"wrq3f15nh4f4t5tly0b9kz2di\": [\r\n          \"my-api-gateway.server\",\r\n          \"my-api-gateway\"\r\n        ]\r\n      },\r\n      \"VirtualAddresses\": {\r\n        \"i7k77izvi13b6xd3795nf3bff\": {\r\n          \"IPv4\": \"10.0.0.10\",\r\n          \"IPv6\": \"\"\r\n        },\r\n        \"wrq3f15nh4f4t5tly0b9kz2di\": {\r\n          \"IPv4\": \"10.10.10.21\",\r\n          \"IPv6\": \"\"\r\n        }\r\n      },\r\n      \"ExposedPorts\": [\r\n        {\r\n          \"Name\": \"\",\r\n          \"Protocol\": 0,\r\n          \"TargetPort\": 80,\r\n          \"PublishedPort\": 82\r\n        },\r\n        {\r\n          \"Name\": \"\",\r\n          \"Protocol\": 0,\r\n          \"TargetPort\": 83,\r\n          \"PublishedPort\": 83\r\n        }\r\n      ]\r\n    },\r\n    \"Ports\": {},\r\n    \"SandboxKey\": \"\/var\/run\/docker\/netns\/fb2ebe2e48be\",\r\n    \"SecondaryIPAddresses\": null,\r\n    \"SecondaryIPv6Addresses\": null,\r\n    \"IsAnonymousEndpoint\": false,\r\n    \"HasSwarmEndpoint\": false\r\n  },\r\n  \"LogPath\": \"\",\r\n  \"Name\": \"\/my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\",\r\n  \"Driver\": \"overlay2\",\r\n  \"OS\": \"linux\",\r\n  \"RestartCount\": 0,\r\n  \"HasBeenStartedBefore\": true,\r\n  \"HasBeenManuallyStopped\": true,\r\n  \"MountPoints\": {\r\n    \"\/my-api-gateway.json\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway.json\",\r\n      \"Destination\": \"\/my-api-gateway.json\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway.json\",\r\n        \"Target\": \"\/my-api-gateway.json\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/etc\/containerpilot.json5\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway-containerpilot.json5\",\r\n      \"Destination\": \"\/etc\/containerpilot.json5\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/my-api-gateway-containerpilot.json5\",\r\n        \"Target\": \"\/etc\/containerpilot.json5\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/keycloak.env\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/keycloak.env\",\r\n      \"Destination\": \"\/keycloak.env\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/keycloak.env\",\r\n        \"Target\": \"\/keycloak.env\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/region.env\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/region.env\",\r\n      \"Destination\": \"\/region.env\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/region.env\",\r\n        \"Target\": \"\/region.env\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/root\/cron-my\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-my\",\r\n      \"Destination\": \"\/root\/cron-my\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-my\",\r\n        \"Target\": \"\/root\/cron-my\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/root\/cron-crontab\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-crontab\",\r\n      \"Destination\": \"\/root\/cron-crontab\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/cron-crontab\",\r\n        \"Target\": \"\/root\/cron-crontab\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    },\r\n    \"\/root\/logrotate\": {\r\n      \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/logrotate\",\r\n      \"Destination\": \"\/root\/logrotate\",\r\n      \"RW\": true,\r\n      \"Name\": \"\",\r\n      \"Driver\": \"\",\r\n      \"Type\": \"bind\",\r\n      \"Propagation\": \"rprivate\",\r\n      \"Spec\": {\r\n        \"Type\": \"bind\",\r\n        \"Source\": \"\/home\/ubuntu\/git\/my\/config-files\/outputs\/my-platform\/logrotate\",\r\n        \"Target\": \"\/root\/logrotate\"\r\n      },\r\n      \"SkipMountpointCreation\": true\r\n    }\r\n  },\r\n  \"SecretReferences\": [],\r\n  \"ConfigReferences\": null,\r\n  \"MountLabel\": \"\",\r\n  \"ProcessLabel\": \"\",\r\n  \"AppArmorProfile\": \"docker-default\",\r\n  \"SeccompProfile\": \"\",\r\n  \"NoNewPrivileges\": false,\r\n  \"HostnamePath\": \"\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/hostname\",\r\n  \"HostsPath\": \"\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/hosts\",\r\n  \"ShmPath\": \"\",\r\n  \"ResolvConfPath\": \"\/var\/lib\/docker\/containers\/b5715b8730e97a8741c982f3bcf864046b31d46df6d64c5f4fb48a5d6aa95a86\/resolv.conf\",\r\n  \"LocalLogCacheMeta\": {\r\n    \"HaveNotifyEnabled\": true\r\n  }\r\n}\r\nroot@my-swarm-node-2:\/home\/ubuntu#\r\n```\r\nThe internal health-check script is very simple. It checks if 2 local ports are available:\r\n```\r\nmy-api-gateway-3@us-east-1a:\/# cat \/usr\/local\/bin\/health-check-my-api-gateway.sh\r\n#!\/bin\/bash\r\n\r\nHOST=localhost\r\nPORTS=\"80 82\"\r\nTIMEOUT=10\r\nERR_FILE=\/tmp\/health-check-my-api-gateway.err\r\n\r\nfor PORT in $PORTS; do\r\n  STATUS=$(curl -s -S -m $TIMEOUT -w \"%{http_code}\" \\\r\n           -o \/dev\/null --stderr $ERR_FILE \\\r\n           http:\/\/$HOST:$PORT\/)\r\n  echo \"PORT $PORT CODE: $STATUS\"\r\n  if [ $STATUS -ne 200 ] && [ $STATUS -ne 307 ]; then\r\n    echo -n \"PORT $PORT ERR: \"\r\n    cat $ERR_FILE\r\n    exit $PORT\r\n  fi\r\ndone\r\n\r\nexit 0\r\n\r\nmy-api-gateway-3@us-east-1a:\/#\r\n```","No problem! :) If I can deliver you any helpful data, then just please let me know.","Thanks! So at least looking at this part of that information;\r\n\r\n```json\r\n{\r\n  \"State\": {\r\n    \"Running\": true,\r\n    \"Paused\": false,\r\n    \"Restarting\": false,\r\n    \"OOMKilled\": false,\r\n    \"RemovalInProgress\": false,\r\n    \"Dead\": false,\r\n    \"Pid\": 1061798,\r\n    \"ExitCode\": 0,\r\n    \"Error\": \"\",\r\n    \"StartedAt\": \"2023-07-24T09:49:27.044834116Z\",\r\n    \"FinishedAt\": \"0001-01-01T00:00:00Z\",\r\n    \"Health\": {\r\n      \"Status\": \"unhealthy\",\r\n      \"FailingStreak\": 12,\r\n      \"Log\": [\r\n        {\r\n          \"Start\": \"2023-07-24T15:15:30.630761565Z\",\r\n          \"End\": \"2023-07-24T15:15:40.696624729Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:15:55.705057663Z\",\r\n          \"End\": \"2023-07-24T15:16:05.76224049Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:16:20.771568439Z\",\r\n          \"End\": \"2023-07-24T15:16:30.829649436Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:16:45.839295206Z\",\r\n          \"End\": \"2023-07-24T15:16:55.902240173Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        },\r\n        {\r\n          \"Start\": \"2023-07-24T15:17:10.910921222Z\",\r\n          \"End\": \"2023-07-24T15:17:20.986432895Z\",\r\n          \"ExitCode\": -1,\r\n          \"Output\": \"Health check exceeded timeout (10s)\"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\r\nI see that;\r\n\r\n- health checks failed at least 12 times in a row\r\n- the container _was_ marked as \"unhealthy\"\r\n- but the state (on-disk) for the container shows that it's considered \"running\"\r\n \r\n \r\nWe'd have to do more digging probably to be sure that's not just a red herring, as the daemon also keeps track of state in-memory, but it _could_ indicate that the daemon successfully killed the container (which _technically_ it should it it was `SIGKILL`'ed), but did not update the state after that.","Thank you!\r\n\r\nThe first conclusion is related to following piece of Compose file for my Swarm stack:\r\n```\r\n  my-api-gateway:\r\n    hostname: my-api-gateway-{{.Task.Slot}}\r\n    image: my-api-gateway:5.0.1\r\n    ports:\r\n      - 82:80\r\n      - 83:83\r\n    [...]\r\n    healthcheck:\r\n      test: \/usr\/local\/bin\/health-check-my-api-gateway.sh || exit 1\r\n      interval: 15s\r\n      retries: 12\r\n      start_period: 40s\r\n      timeout: 10s\r\n```\r\nYep, the container process has been killed:\r\n```\r\nroot@my-swarm-node-2:\/home\/ubuntu# docker ps \r\nCONTAINER ID   IMAGE                                                             COMMAND                  CREATED      STATUS                  PORTS     NAMES\r\n908fe0131895   my-api:5.0.6                 \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-api.2.e55wmbwick5dfqpz9cx9weh6n\r\n20402609dafd   my-api-worker:5.0.5          \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-api-worker.3.xhu5n0tij99oxyo7dpaqqc7ri\r\n8dc0c0883e85   my-ui:5.0.7                  \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-ui.2.hs3qo4935lpkjr5olyk4qs762\r\nd5521c1b01bc   my-healthchecks:5.0.1        \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-healthchecks.3.xykuht1emu7u0z76nc5jjj7e1\r\nb5715b8730e9   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   2 days ago   Up 2 days (unhealthy)             my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\r\n15f626bb2cb1   my-internal-watchdog:5.0.1   \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-internal-watchdog.3.pk24crylkstpiwkwqkzps3cbv\r\n1b4819706d88   my-admin-api:5.0.1           \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-admin-api.2.x3pwqnxdli0kk5qez7b2wcdlk\r\ncd2d50e565cc   my-locker:5.0.1              \"docker-entrypoint.sh\"   2 days ago   Up 2 days (healthy)               my-platform_my-locker.2.f6lobwwcffjkt45l97l8bwnte\r\nroot@my-swarm-node-2:\/home\/ubuntu# \r\nroot@my-swarm-node-2:\/home\/ubuntu# docker ps |grep my- |wc -l\r\n8\r\nroot@my-swarm-node-2:\/home\/ubuntu# \r\nroot@my-swarm-node-2:\/home\/ubuntu# ps aux |grep docker-entrypoint.sh |grep -v grep\r\nroot     1060655  0.0  0.0   3868  3020 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1061211  0.0  0.0   3868  2968 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1061521  0.0  0.0   3868  3072 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1062737  0.0  0.0   3868  3052 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1116111  0.0  0.0   3868  2932 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1117019  0.0  0.0   3868  2852 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot     1118276  0.0  0.0   3868  3008 ?        Ss   Jul24   0:00 \/bin\/bash \/usr\/local\/bin\/docker-entrypoint.sh\r\nroot@my-swarm-node-2:\/home\/ubuntu# \r\nroot@my-swarm-node-2:\/home\/ubuntu# ps aux |grep docker-entrypoint.sh |grep -v grep |awk '{ print $2 }' |while read PID; do cat \/proc\/$PID\/environ |tr '\\0' '\\n' |grep ^MY_SERVICE= ; done |sort\r\nMY_SERVICE=my-admin-api\r\nMY_SERVICE=my-api\r\nMY_SERVICE=my-api-worker\r\nMY_SERVICE=my-healthchecks\r\nMY_SERVICE=my-internal-watchdog\r\nMY_SERVICE=my-locker\r\nMY_SERVICE=my-ui\r\nroot@my-swarm-node-2:\/home\/ubuntu# \r\nroot@my-swarm-node-2:\/home\/ubuntu# ps aux |grep docker-entrypoint.sh |grep -v grep |awk '{ print $2 }' |while read PID; do cat \/proc\/$PID\/environ |tr '\\0' '\\n' |grep ^MY_SERVICE= ; done |sort |wc -l\r\n7\r\nroot@my-swarm-node-2:\/home\/ubuntu# \r\n```\r\nHow can I help now?","Hello!\r\n\r\n@thaJeztah Any idea what can be wrong?","@ptecza I'm trying to look into this a bit \u2013 for clarity, you mentioned this happens on both 20.10.13 and 24.0.4, but only when running on Ubuntu Jammy and not Bionic, correct? \r\n\r\nI'm can't replicate this (on Ubuntu 22.10), without manually forcing Docker into inconsistent states. Could you share some more information about the image\/workflow you're running? It's possible that this is due to different kernel versions between those Ubuntu versions, but since I can't replicate it on a current version I'm unable to check.\r\n\r\nIt might also be useful if you could run docker in debug mode, and collect those logs \u2013 either running dockerd with `dockerd -D` or adding `\"debug\": true` to the daemon.json file.","Hi @laurazard !\r\n\r\nThanks a lot for showing interest! :)\r\n\r\nYesterday I launched a new 3-node Swarm cluster in the same cloud region, VPC and subnet. Now it's based on Ubuntu Bionic and Docker 20.10.12. I deployed there exactly the same Swarm stack with the same Docker images and... it seems that it's affected as well :( I'm very confused at this moment, because this software combo works perfectly to me in other cloud regions.\r\n\r\nAll my own Docker images are built using Debian Buster image as a base. There are several processes running in the container I have trouble with, so it's a bit like a small VM :) I run my Swarm stack using `docker stack deploy` command.\r\n\r\nOf course I can enable the debug mode, but I'm not sure how Docker will be verbose. The root volume at my Swarm nodes is not too big...","@laurazard I've restarted the Docker daemon at Ubuntu Jammy nodes and started collecting the debug logs.\r\n\r\nBTW, as you can see below, only one Swarm node had a problem with the reported container:\r\n```\r\nubuntu@my-swarm-node-1:~$ docker ps |grep my-api-gateway\r\nCONTAINER ID   IMAGE                                                        COMMAND                  CREATED      STATUS                PORTS     NAMES\r\n2bd425e02785   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   8 days ago   Up 8 days (healthy)             my-platform_my-api-gateway.3.gkynl567t6qamehwdqz9hwlyq\r\nubuntu@my-swarm-node-1:~$\r\n\r\nubuntu@my-swarm-node-2:~$ docker ps |grep my-api-gateway\r\nCONTAINER ID   IMAGE                                                             COMMAND                  CREATED      STATUS                  PORTS     NAMES\r\nb5715b8730e9   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   8 days ago   Up 8 days (unhealthy)             my-platform_my-api-gateway.2.ja6uydyxp88eqpjlicy1a5brx\r\nubuntu@my-swarm-node-2:~$\r\n\r\nubuntu@my-swarm-node-3:~$ docker ps |grep my-api-gateway\r\nCONTAINER ID   IMAGE                                                             COMMAND                  CREATED      STATUS                PORTS     NAMES\r\n0454a5fa8afe   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   8 days ago   Up 8 days (healthy)             my-platform_my-api-gateway.1.nlaob6jw536zjjqgdoi8n1m6g\r\nubuntu@my-swarm-node-3:~$\r\n```","@laurazard I have one unhealthy Docker container again :)\r\n```\r\nubuntu@my-swarm-node-1:~$ date\r\nWed Aug  2 05:33:36 UTC 2023\r\nubuntu@my-swarm-node-1:~$\r\nubuntu@my-swarm-node-1:~$ docker ps |grep my-api-gateway\r\n9bc92affb8d3   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   16 hours ago   Up 16 hours (healthy)             my-platform_my-api-gateway.1.spm0oowfd5r7l0ojam1kitkdc\r\nubuntu@my-swarm-node-1:~$\r\n\r\nubuntu@my-swarm-node-2:~$ docker ps |grep my-api-gateway\r\nb809982755c0   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   16 hours ago   Up 16 hours (unhealthy)             my-platform_my-api-gateway.2.s9ezecshff2zozy2wlwfotewz\r\nubuntu@my-swarm-node-2:~$\r\n\r\nubuntu@my-swarm-node-3:~$ docker ps |grep my-api-gateway\r\n0edd84de9269   my-api-gateway:5.0.1         \"docker-entrypoint.sh\"   16 hours ago   Up 16 hours (healthy)             my-platform_my-api-gateway.3.4j66ybkl5e8i3tah7a7kxz5bw\r\nubuntu@my-swarm-node-3:~$\r\n```\r\nI've uploaded the debugging details at https:\/\/gist.github.com\/ptecza\/5c29d0faf64b8bf0d4b0e415136bae8d\r\n\r\nIt includes a following files:\r\n1. `gistfile1.txt`:\r\n```\r\nubuntu@my-swarm-node-2:~$ sudo grep b809982755c0 \/var\/log\/syslog\r\n```\r\n2. `gistfile2.txt`:\r\n```\r\nubuntu@my-swarm-node-2:~$ sudo grep \"^Aug  1 20:\" \/var\/log\/syslog \r\n```\r\n3. `gistfile3.txt`:\r\n```\r\nubuntu@my-swarm-node-2:~$ sudo grep \"^Aug  1 21:\" \/var\/log\/syslog \r\n```\r\n4. `gistfile4.txt`:\r\n```\r\nubuntu@my-swarm-node-2:~$ curl -s --unix-socket \/var\/run\/docker.sock http:\/\/.\/debug\/pprof\/goroutine\\?debug\\=2\r\n```\r\nPlease let me know if I can deliver you something else.","Hiya! Sorry for taking a while to reply, have been looking into other things and missed the pings \ud83d\ude05.\r\n\r\nFrom looking at the logs you shared (thanks! \u2764\ufe0f), the breakdown of the situation seems to be:\r\n- the healthcheck test you have configured, at some point, starts taking too long, doesn't finish running, and gets killed (this happens for `n` retries, then the container gets moved to an unhealthy state)\r\n- the daemon attempts to kill the container, which is now in an unhealthy state \u2013 however, from the logs, the attempt to kill the container hangs, and subsequent calls fail.\r\n\r\nThis is supported by the following logs in your goroutine dump:\r\n<details>\r\n\r\n- there are `3049` goroutines stuck locking on\r\nhttps:\/\/github.com\/moby\/moby\/blob\/b32c7594a53ff2cc2751c2ae2340736a46d72340\/container\/state.go#L244-L249\r\n```\r\ngoroutine 1210007 [sync.Mutex.Lock, 492 minutes]:\r\nsync.runtime_SemacquireMutex(0xc007e44d70?, 0x50?, 0x7ffffffe7fa343e8?)\r\n\t\/usr\/local\/go\/src\/runtime\/sema.go:77 +0x26\r\nsync.(*Mutex).lockSlow(0xc0098263c0)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:171 +0x165\r\nsync.(*Mutex).Lock(...)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:90\r\ngithub.com\/docker\/docker\/container.(*State).IsRunning(0xc000fe2480?)\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/state.go:245 +0x36\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).ContainerStats(0xc000fe2480, {0x5557d11f8180, 0xc00986aa20}, {0xc008f85826?, 0xc007e453e8?}, 0xc00986aab0)\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/stats.go:39 +0x111\r\ngithub.com\/docker\/docker\/api\/server\/router\/container.(*containerRouter).getContainersStats(0xc008a5c980, {0x5557d11f8180, 0xc00986aa20}, {0x5557d11f6940, 0xc002e6da40}, 0xc000f66200, 0xc007e8de40?)\r\n\t\/go\/src\/github.com\/docker\/docker\/api\/server\/router\/container\/container_routes.go:125 +0x2ed\r\n```\r\nwhich is expected, and why you can't check\/kill\/do much with the container.\r\n\r\n- there are also `8` goroutines stuck locking on\r\nhttps:\/\/github.com\/moby\/moby\/blob\/b32c7594a53ff2cc2751c2ae2340736a46d72340\/container\/state.go#L215-L227\r\n```\r\ngoroutine 7229 [select, 994 minutes]:\r\ngithub.com\/docker\/docker\/container.(*State).Wait.func1()\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/state.go:216 +0xbd\r\ncreated by github.com\/docker\/docker\/container.(*State).Wait\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/state.go:215 +0x3ca\r\n```\r\n\r\n- finally, there is `1` goroutine, hanging on\r\nhttps:\/\/github.com\/moby\/moby\/blob\/b32c7594a53ff2cc2751c2ae2340736a46d72340\/container\/state.go#L252-L257\r\n```\r\ngoroutine 8084 [sync.Mutex.Lock, 508 minutes]:\r\nsync.runtime_SemacquireMutex(0xc007f690d0?, 0xbc?, 0xc0010cfa40?)\r\n\t\/usr\/local\/go\/src\/runtime\/sema.go:77 +0x26\r\nsync.(*Mutex).lockSlow(0xc0098263c0)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:171 +0x165\r\nsync.(*Mutex).Lock(...)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:90\r\ngithub.com\/docker\/docker\/container.(*State).GetPID(0x5557cec7e3e5?)\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/state.go:253 +0x36\r\ngithub.com\/docker\/docker\/daemon.killProcessDirectly(0xc000614780)\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/container_operations_unix.go:339 +0x3d\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).Kill(0xc0008c5f80?, 0xc000614780)\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/kill.go:164 +0x2ba\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).containerStop(0xc000fe2480, {0xc0080aa880?, 0x3c?}, 0xc000614780, {{0x0?, 0x3?}, 0x0?})\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/stop.go:108 +0x512\r\n```\r\nwhich doesn't tell us much, except confirm that the issue is indeed that we're stuck trying to kill the container, and that's why everything else is hanging.\r\n\r\n<\/details>\r\n\r\nLooking further through the logs, we eventually find the root cause of why we're hanging trying to get kill the container:\r\n```\r\ngoroutine 1174391 [sync.Mutex.Lock, 508 minutes]:\r\nsync.runtime_SemacquireMutex(0x5557d0292f56?, 0xa0?, 0xa3?)\r\n\t\/usr\/local\/go\/src\/runtime\/sema.go:77 +0x26\r\nsync.(*Mutex).lockSlow(0xc00a2a1800)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:171 +0x165\r\nsync.(*Mutex).Lock(...)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:90\r\ngithub.com\/docker\/docker\/pkg\/broadcaster.(*Unbuffered).Clean(0xc00a2a1800)\r\n\t\/go\/src\/github.com\/docker\/docker\/pkg\/broadcaster\/unbuffered.go:42 +0x3a\r\ngithub.com\/docker\/docker\/container\/stream.(*Config).CloseStreams(0xc009e59a40)\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/stream\/streams.go:102 +0x105\r\ngithub.com\/docker\/docker\/container.(*rio).Close(0xc00900a558)\r\n\t\/go\/src\/github.com\/docker\/docker\/container\/container.go:806 +0x37\r\ngithub.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd.(*task).Delete(0xc00107b540, {0x5557d11f8148, 0xc0097176e0}, {0x0, 0x0, 0x5557d265df40?})\r\n\t\/go\/src\/github.com\/docker\/docker\/vendor\/github.com\/containerd\/containerd\/task.go:328 +0x2ab\r\ngithub.com\/docker\/docker\/libcontainerd\/remote.(*task).Delete(0x5557d11f8110?, {0x5557d11f8148?, 0xc0097176e0?})\r\n\t\/go\/src\/github.com\/docker\/docker\/libcontainerd\/remote\/client.go:372 +0x32\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).handleContainerExit(0xc000fe2480, 0xc000614780, 0xc0058d93b0)\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/monitor.go:39 +0x13a\r\ngithub.com\/docker\/docker\/daemon.(*Daemon).ProcessEvent(0xc000fe2480, {0xc009553e40, 0x40}, {0x5557d076f6ac, 0x4}, {{0xc009553e40, 0x40}, {0xc009553e80, 0x40}, 0x2b632b, ...})\r\n\t\/go\/src\/github.com\/docker\/docker\/daemon\/monitor.go:166 +0x6cc\r\n```\r\n\r\nWhile trying to process the container's exit event, which locks out the container's state (and hence everything else), we're locked while closing it's IO: https:\/\/github.com\/moby\/moby\/blob\/89b542b421f439b3c703098f7f1c29f661e430bb\/pkg\/broadcaster\/unbuffered.go#L9-L12\r\n\r\nthat's stuck on `Write()`:\r\n```\r\ngoroutine 6081 [sync.Cond.Wait, 494 minutes]:\r\nsync.runtime_notifyListWait(0xc008958910, 0x62)\r\n\t\/usr\/local\/go\/src\/runtime\/sema.go:527 +0x14c\r\nsync.(*Cond).Wait(0x5557cec7e2e5?)\r\n\t\/usr\/local\/go\/src\/sync\/cond.go:70 +0x8c\r\ngithub.com\/docker\/docker\/pkg\/ioutils.(*BytesPipe).Write(0xc008e75680, {0xc003fc6000?, 0x5557ceb8f565?, 0x0?})\r\n\t\/go\/src\/github.com\/docker\/docker\/pkg\/ioutils\/bytespipe.go:92 +0x5fe\r\n```\r\nhttps:\/\/github.com\/moby\/moby\/blob\/89b542b421f439b3c703098f7f1c29f661e430bb\/pkg\/broadcaster\/unbuffered.go#L23-L31\r\n\r\nwhich locks `Clean()` out of working:\r\n```\r\ngoroutine 1174391 [sync.Mutex.Lock, 508 minutes]:\r\nsync.runtime_SemacquireMutex(0x5557d0292f56?, 0xa0?, 0xa3?)\r\n\t\/usr\/local\/go\/src\/runtime\/sema.go:77 +0x26\r\nsync.(*Mutex).lockSlow(0xc00a2a1800)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:171 +0x165\r\nsync.(*Mutex).Lock(...)\r\n\t\/usr\/local\/go\/src\/sync\/mutex.go:90\r\ngithub.com\/docker\/docker\/pkg\/broadcaster.(*Unbuffered).Clean(0xc00a2a1800)\r\n\t\/go\/src\/github.com\/docker\/docker\/pkg\/broadcaster\/unbuffered.go:42 +0x3a\r\n```\r\nhttps:\/\/github.com\/moby\/moby\/blob\/89b542b421f439b3c703098f7f1c29f661e430bb\/pkg\/broadcaster\/unbuffered.go#L41-L42\r\n\r\npresumably, due to a bad attached writer hanging.\r\n\r\nI also found https:\/\/github.com\/moby\/moby\/issues\/42705 \u2013 which is a report for the same issue: one of the writers for that `Unbuffered` gets broken, locking forever on `Write`, which then locks everything else out (in this case it's the Loki logging driver getting restarted that causes it).\r\n\r\nSpeculating here, but from your logs @ptecza, the cause might be the `gelf` driver misbehaving.\r\n\r\n------------\r\n\r\nupdate: I just noticed we already also had this other report \u2013 https:\/\/github.com\/moby\/moby\/issues\/41827, **AND** a PR to fix it: https:\/\/github.com\/moby\/moby\/pull\/41828. I'll try to take a look at it\/ping the author and see if we can get it reviewed\/merged.\r\n","Hello @laurazard ! Many thanks for your thorough analysis! No problem, I know that you have many other tasks. I couldn't reply earlier because I was on vacation last week :)\r\n\r\nBut how to explain that the same images work fine to me with the same version of Docker in different cloud regions? Could my problem be caused by a network or hardware malfunction?\r\n\r\nCurrently I kill the affected containers every 2 hours using Cron, but I could try to disable GELF log driver to verify your speculating. What do you think?","Hello @laurazard ! Any news? :)","@laurazard is on leave for some weeks, but I see there was progress on https:\/\/github.com\/moby\/moby\/pull\/41828. I'll check if others are available to help review that PR.\r\n","@thaJeztah Great, thank you! :)"],"labels":["status\/0-triage","kind\/bug","area\/swarm","version\/24.0"]},{"title":"c8d: broken \/ missing image can render \"docker ps\" broken","body":"### Description\n\nI noticed I was no longer on our internal nightly channel of Docker Desktop, and switched from Docker Desktop 4.21 to 4.22 (nightly), and found `docker ps -a` to be broken.\r\n\r\nIt looks like an image was possibly garbage collected?\r\n\r\n```bash\r\ndocker ps -a\r\nError response from daemon: failed to read config content: content digest sha256:35d589f20301ca0a99f301c3c92567d50fb0aea3a95294b630f8ae13e49b98d1: not found\r\n```\r\n\r\nAfter I noticed that, I tried running a new container, and that worked. So `docker ps` works, but `docker ps -a` is broken (due to the missing image)\r\n\r\n```bash\r\ndocker ps\r\nCONTAINER ID   IMAGE                          COMMAND                  CREATED          STATUS          PORTS                     NAMES\r\n013c96c9148c   thajeztah\/dockershell:latest   \"\/usr\/bin\/gotty --ti\u2026\"   12 minutes ago   Up 12 minutes   0.0.0.0:32768->8080\/tcp   magical_moser\r\n```\r\n\r\n```bash\r\ndocker ps -a\r\nError response from daemon: failed to read config content: content digest sha256:35d589f20301ca0a99f301c3c92567d50fb0aea3a95294b630f8ae13e49b98d1: not found\r\n```\r\n\r\nListing images appears to work as  usual;\r\n\r\n```bash\r\ndocker image ls\r\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\r\nhello-world   latest    9c7a54a9a43c   2 months ago    13.3kB\r\ngolang        rc        76b61db4adea   17 months ago   964MB\r\n\r\ndocker image ls -a\r\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\r\nhello-world   latest    9c7a54a9a43c   2 months ago    13.3kB\r\ngolang        rc        76b61db4adea   17 months ago   964MB\r\n```\r\n\r\nThat error is produced by `readConfig` https:\/\/github.com\/moby\/moby\/blob\/536e3692c631b4a8c8c2b82ddd17dc000f7fd3af\/daemon\/containerd\/image_list.go#L476-L487\r\n\r\nWhich is called by \r\n\r\n- GetImage,\r\n- ImageHistory\r\n- setupLabelFilter (which is called by setupFilters and ImagesPrune and Images (docker image ls))\r\n\r\nOf course the underlying issue should be looked at (if not fixed already), but I'm wondering if we should have some fallback for this situation (\"we're missing this image; container is broken\"). Either mark the container `dead` (which I think we use for broken containers to be garbage collected on daemon restart), and\/or show the container but with something like `<missing>` for the image, so that at least I can _remove_ the container manually.\r\n\r\n\r\nDocker Version and Info\r\n\r\n```bash\r\nClient:\r\n Cloud integration: v1.0.35-desktop+001\r\n Version:           24.0.4\r\n API version:       1.43\r\n Go version:        go1.20.5\r\n Git commit:        3713ee1\r\n Built:             Fri Jul  7 14:47:27 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           remote2\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.5\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       a61e2b4\r\n  Built:            Fri Jul 21 20:35:18 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.4\r\n Context:    remote2\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.11.2+desktop.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.20.2-desktop.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-compose\r\n  desktop: Commands to interact with Docker Desktop (Docker Inc.)\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-desktop\r\n  dev: Docker Dev Environments (Docker Inc.)\r\n    Version:  v0.1.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-dev\r\n  extension: Manages Docker extensions (Docker Inc.)\r\n    Version:  v0.2.20\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-extension\r\n  init: Creates Docker-related starter files for your project (Docker Inc.)\r\n    Version:  v0.1.0-beta.6\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-init\r\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\r\n    Version:  0.6.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-sbom\r\n  scan: Docker Scan (Docker Inc.)\r\n    Version:  v0.26.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-scan\r\n  scout: Command line tool for Docker Scout (Docker Inc.)\r\n    Version:  0.20.0\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-scout\r\n  shell: Open a browser shell on the Docker Host. (thaJeztah)\r\n    Version:  v0.0.1\r\n    Path:     \/Users\/thajeztah\/.docker\/cli-plugins\/docker-shell\r\n\r\nServer:\r\n Containers: 0\r\n  Running: 0\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: 24.0.5\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 5.15.0-67-generic\r\n Operating System: Ubuntu 22.04.2 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 969.4MiB\r\n Name: ubuntu-s-1vcpu-1gb-ams3-01\r\n ID: 8dfbceba-458c-4793-b866-fffcd1214b2f\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: true\r\n  File Descriptors: 24\r\n  Goroutines: 36\r\n  System Time: 2023-07-26T08:59:46.607419832Z\r\n  EventsListeners: 0\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n","comments":["Chatted on Slack, and @vvoland had hit this situation as well, and may have a partial fix for this","Unfortunately, `docker system prune` etc also won't work as a workaround;\r\n\r\n```bash\r\ndocker container prune\r\nWARNING! This will remove all stopped containers.\r\nAre you sure you want to continue? [y\/N] y\r\nError response from daemon: snapshot 013c96c9148c4b8349e5d8cc176c743d50e20021f34b4841a78c5a4cea803684 does not exist: not found\r\n\r\nocker system prune -a\r\nWARNING! This will remove:\r\n  - all stopped containers\r\n  - all networks not used by at least one container\r\n  - all images without at least one container associated to them\r\n  - all build cache\r\n\r\nAre you sure you want to continue? [y\/N] y\r\nError response from daemon: snapshot 013c96c9148c4b8349e5d8cc176c743d50e20021f34b4841a78c5a4cea803684 does not exist: not found\r\n```","Just ran into another permutation of this issue;\r\n\r\n```bash\r\ndocker run -it --rm busybox\r\nUnable to find image 'busybox:latest' locally\r\n3fbc63216742: Already exists\r\n1fa89c01cd04: Already exists\r\nfc9db2894f4e: Already exists\r\ndocker: Error response from daemon: failed to read config content: content digest sha256:3010a01e6ddbec8b36101553aa0fb12bc24c076beb64bd4035cccd06bf58af68: not found.\r\nSee 'docker run --help'.\r\n```\r\n\r\nThis was on a \"nightly\" build of Desktop (117955) (but unfortunately that version doesn't show the engine's commit it was built from);\r\n\r\n```bash\r\nServer: Docker Desktop 4.23.0 (117955)\r\n Engine:\r\n  Version:          24.0\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.6\r\n  Git commit:       HEAD\r\n  Built:            Fri Jul 21 18:11:43 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.22\r\n  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        v1.1.8-0-g82f18fe\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```"],"labels":["kind\/bug","area\/images","containerd-integration"]},{"title":"`docker save` uses local time zone for `Created` field","body":"### Description\n\n`docker save` uses local time zone for `Created` field. This affects the layer IDs used in the tar directories as well.\r\n\r\n`time.Unix` creates a local time object and should be set to UTC. https:\/\/github.com\/moby\/moby\/blob\/7e4ffa3fa935856409685cb9711139b5903a2b4c\/image\/tarexport\/save.go#L382C8-L382C8 \n\n### Reproduce\n\n1. Set timezone to e.g. UTC-5\r\n2. `docker save <image> -o <image>.tar`\r\n3. Look in `<image>\/<layerID>\/json`\r\n4. Note `Created` field timestamp has a timezone\n\n### Expected behavior\n\nTimestamp should use UTC for reproducibility.\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           23.0.5\r\n API version:       1.42\r\n Go version:        go1.20.3\r\n Git commit:        bc4487a59e\r\n Built:             Wed Apr 26 14:46:06 2023\r\n OS\/Arch:           darwin\/arm64\r\n Context:           colima\r\n\r\nServer:\r\n Engine:\r\n  Version:          23.0.6\r\n  API version:      1.42 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       9dbdbd4b6d7681bd18c897a6ba0376073c2a72ff\r\n  Built:            Fri May 12 13:54:36 2023\r\n  OS\/Arch:          linux\/arm64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.0\r\n  GitCommit:        1fbd70374134b891f97ce19c70b6e50c7b9f4e0d\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        860f061b76bb4fc671f0f9e900f7d80ff93d4eb7\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    colima\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.4\r\n    Path:     \/Users\/jdygert\/.docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.17.3\r\n    Path:     \/Users\/jdygert\/.docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 1\r\n  Running: 1\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 2\r\n Server Version: 23.0.6\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 1fbd70374134b891f97ce19c70b6e50c7b9f4e0d\r\n runc version: 860f061b76bb4fc671f0f9e900f7d80ff93d4eb7\r\n init version: \r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n Kernel Version: 6.1.29-0-virt\r\n Operating System: Alpine Linux v3.18\r\n OSType: linux\r\n Architecture: aarch64\r\n CPUs: 8\r\n Total Memory: 15.59GiB\r\n Name: colima\r\n ID: 65daa0a6-7abd-4f73-ae32-34e68d956f39\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["- hm.. reminds me of https:\/\/github.com\/moby\/moby\/pull\/41905 (which I realise now was never merged \ud83d\ude05)\r\n\r\nAre you interested in contributing and opening a PR?"],"labels":["status\/0-triage","kind\/bug","area\/images"]},{"title":"Memory leak of docked process","body":"### Description\n\nI have a swarm cluster with 4 nodes. I found a memory leak in the dockerd process on the leader node. Upon inspecting it with pprof\/heap, I found that it is related to the logBroker.  During this period, 1 node went offline for a certain time and then came back online. Here is a screenshot for reference:\r\n\r\n![image](https:\/\/github.com\/moby\/moby\/assets\/8296653\/7e2c138e-3a45-4eb2-8711-1aedc9e45c63)\r\n\r\n![1690189042340_09AE19D6-B692-4a23-B2E3-1D236EA8FFA4](https:\/\/github.com\/moby\/moby\/assets\/8296653\/c13ff204-6cc6-4b9a-a584-5e1ed9b39977)\r\n\r\n![pprof003](https:\/\/github.com\/moby\/moby\/assets\/8296653\/2b9130dc-f8d2-4f59-b2cc-ca05cc7b2ee5)\r\n![pprof005](https:\/\/github.com\/moby\/moby\/assets\/8296653\/c8aef441-054e-4858-bc80-b7ae8ec575c4)\r\n\n\n### Reproduce\n\nI don't know how to reproduce\n\n### Expected behavior\n\n_No response_\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           20.10.22\r\n API version:       1.41\r\n Go version:        go1.18.9\r\n Git commit:        3a2c30b\r\n Built:             Thu Dec 15 22:28:08 2022\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n Experimental:      true\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          20.10.22\r\n  API version:      1.41 (minimum version 1.12)\r\n  Go version:       go1.18.9\r\n  Git commit:       42c8b31\r\n  Built:            Thu Dec 15 22:25:58 2022\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.14\r\n  GitCommit:        9ba4b250366a5ddde94bb7c9d1def331423aa323\r\n runc:\r\n  Version:          1.1.4\r\n  GitCommit:        v1.1.4-0-g5fd4c4d\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient:\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  app: Docker App (Docker Inc., v0.9.1-beta3)\r\n  buildx: Docker Buildx (Docker Inc., v0.9.1-docker)\r\n  scan: Docker Scan (Docker Inc., v0.23.0)\r\n\r\nServer:\r\n Containers: 106\r\n  Running: 51\r\n  Paused: 0\r\n  Stopped: 55\r\n Images: 32\r\n Server Version: 20.10.22\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: cgroupfs\r\n Cgroup Version: 1\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: qwl0incqpqa0v6blrfuell12y\r\n  Is Manager: true\r\n  ClusterID: 0hfq3e0stka9rz1pcngj6x8sp\r\n  Managers: 1\r\n  Nodes: 4\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 20 years\r\n   Force Rotate: 1\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.2.8.131\r\n  Manager Addresses:\r\n   192.2.8.131:2377\r\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 9ba4b250366a5ddde94bb7c9d1def331423aa323\r\n runc version: v1.1.4-0-g5fd4c4d\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: default\r\n Kernel Version: 5.4.0-148-generic\r\n Operating System: Ubuntu 20.04.5 LTS\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 16\r\n Total Memory: 15.47GiB\r\n Name: node131\r\n ID: 3QEH:KQMB:AGJB:3S7Q:JPV4:BSH5:73FO:WAGF:EJPN:6BWB:TI22:WUEO\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Registry: https:\/\/index.docker.io\/v1\/\r\n Labels:\r\n Experimental: false\n```\n\n\n### Additional Info\n\n_No response_","comments":["`LogBroker` is a Swarmkit interface; cc @dperny "],"labels":["area\/logging","status\/0-triage","kind\/bug","area\/swarm","version\/20.10"]},{"title":" Fix for creation of windows source directory in case it didn't exist","body":"<!--\r\nPlease make sure you've read and understood our contributing guidelines;\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/CONTRIBUTING.md\r\n\r\n** Make sure all your commits include a signature generated with `git commit -s` **\r\n\r\nFor additional information on our contributing process, read our contributing\r\nguide https:\/\/docs.docker.com\/opensource\/code\/\r\n\r\nIf this is a bug fix, make sure your description includes \"fixes #xxxx\", or\r\n\"closes #xxxx\"\r\n\r\nPlease provide the following information:\r\n-->\r\n\r\n**- What I did**\r\nWindows 1803 introduced a new bind filter which is more robust and also mentioned in #37474. The code to not allow Windows source directory for older Windows makes sense. Now, we do have a [check](https:\/\/github.com\/moby\/moby\/blob\/96b473a0bdc3f4c1cd0324f3ff044b04ab349df8\/daemon\/daemon_windows.go#L190-L191) for Windows 1809\/RS5. So we could align the behavior with Linux.\r\n\r\nFixes #44799\r\n\r\nThe issue is reported on 20.10. For backport to 20.10, it won't be a clean backport as the creation would need to be put behind an OS version check.\r\n\r\n**- How I did it**\r\nCreating a directory when it doesn't exist in `windows_parser.go` because the computation for mount type is done in `windowsDetectMountType` function in the same file.\r\n\r\n**- How to verify it**\r\n- `docker run --rm -it -v C:\\hello:C:\\world mcr.microsoft.com\/windows\/servercore:ltsc2019 powershell`\r\n- `docker run --isolation=hyperv  --rm -it -v C:\\hello:C:\\world mcr.microsoft.com\/windows\/servercore:ltsc2019 powershell`\r\n\r\n**- Description for the changelog**\r\nFix to create a source directory for bind mount on Windows if it didn't exist to align the behavior with Linux and docs.\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":["Looks like build is failing because the SDDL definition is windows-only, but this file is not a windows-only file;\r\n\r\n```\r\nvolume\/mounts\/windows_parser.go:251:59: undefined: system.SddlAdministratorsLocalSystem\r\n```\r\n\r\n\r\nI guess there's two options for that;\r\n\r\n- add a thin wrapper for `system.MkdirAllWithACL`\r\n- move the `SddlAdministratorsLocalSystem` const to a non-platform specific file (but make sure the doc of the const mentions that it has no meaning on non-Windows\r\n\r\nDoes this case need the `SddlAdministratorsLocalSystem` ACL set? (I _guess_ it's the equivalent to \"create with `root` ownership on Linux, so I _guess_ it does?)\r\n","> Does this case need the SddlAdministratorsLocalSystem ACL set? (I guess it's the equivalent to \"create with root ownership on Linux, so I guess it does?)\r\n\r\nYes, it is required, I moved it to `filesys.go`"],"labels":["platform\/windows","status\/2-code-review","area\/volumes"]},{"title":"api\/types\/network: split EndpointSettings config and operational data","body":"### daemon: Daemon.releaseNetwork move nil-check to cleanOperationalData\r\n\r\nnetwork.EndpointSettings is a wrapper around types.EndpointSettings,\r\nand the fields being cleared are all in that struct, so if it's nil,\r\nthere's nothing to do.\r\n\r\nMaking the nil-check a responsibility of the caller didn't make sense,\r\nso let's move it inside that function.\r\n\r\n### api\/types\/network: split EndpointSettings config and operational data\r\n\r\nEndpointSettings holds both \"Config\" data (definition) and runtime (\"operational\")\r\ndata. Fields for the latter one are set\/reset at runtime. Resetting is handled\r\nby the `cleanOperationalData` function, but it does so for each field individually.\r\n\r\nThis handling is slightly brittle, as the `cleanOperationalData` must be\r\nupdated when adding new \"operational\" data, which can easily be overlooked.\r\n\r\nThis patch updates the EndpointSettings type to split the \"config\" and\r\n\"operational\" data into separate structs. The new `EndpointOperationalData`\r\nstruct is embedded in the existing type, so the net-result should be the\r\nsame for most cases (except for constructing struct-literals).\r\n\r\nResetting the operational data now allows for the `EndpointOperationalData`\r\nto be set to an empty struct (resetting its values to their defaults \/ empty).\r\n\r\n**- Description for the changelog**\r\n<!--\r\nWrite a short (one line) summary that describes the changes in this\r\npull request for inclusion in the changelog:\r\n-->\r\n\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n","comments":[],"labels":["area\/api","status\/2-code-review","area\/networking","kind\/refactor"]},{"title":"PortStatus not present in swagger.yaml","body":"### Description\r\n\r\nIn source code `TaskStatus` contains `PortStatus` but `api\/swagger.yaml` doesn't.\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/api\/swagger.yaml#L3983C7\r\nhttps:\/\/github.com\/moby\/moby\/blob\/master\/api\/types\/swarm\/task.go#L197\r\n\r\n### Reproduce\r\n\r\n-\r\n\r\n### Expected behavior\r\n\r\n`swagger.yaml` should contain `PortStatus`\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfcd85\r\n Built:             Mon May 29 15:50:06 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer:\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f9ee\r\n  Built:            Mon May 29 15:50:06 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          v1.7.2\r\n  GitCommit:        0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc:\r\n  Version:          1.1.8\r\n  GitCommit:        \r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  0.11.0\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  2.20.2\r\n    Path:     \/usr\/lib\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 4\r\n  Running: 4\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 46\r\n Server Version: 24.0.2\r\n Storage Driver: btrfs\r\n  Btrfs: \r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: d9y196uwpsfu83dub7wj9he9i\r\n  Is Manager: true\r\n  ClusterID: jfpx5gqcuq13tq1ky5uv4mgmz\r\n  Managers: 1\r\n  Nodes: 1\r\n  Default Address Pool: 10.0.0.0\/8  \r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 10.0.1.12\r\n  Manager Addresses:\r\n   10.0.1.12:2377\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 0cae528dd6cb557f7201036e9f43420650207b58.m\r\n runc version: \r\n init version: de40ad0\r\n Security Options:\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.4.4-arch1-1\r\n Operating System: Arch Linux\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 32\r\n Total Memory: 125.7GiB\r\n Name: hyper-arch\r\n ID: D43P:D2BV:GL3V:3PQR:S2OM:VY2Y:7PIJ:7XAV:MFAK:L2YA:EVTJ:4YLC\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Username: borodmorod\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":["Hi @BorMor,\r\n\r\nI hope you're doing well. I've been exploring the project and came across the issue you've raised. I'm interested in contributing to this issue, and I'd like to discuss it further with you.","Looks like this field was added in https:\/\/github.com\/moby\/moby\/commit\/14ac9f60d0174256e0713701ebffaf5ca827da71, which was part of this PR;\r\n\r\n- https:\/\/github.com\/moby\/moby\/pull\/27917\r\n\r\n"],"labels":["area\/api","status\/0-triage","kind\/bug","exp\/beginner","area\/docs"]},{"title":"(Re)create endpoints in a stable order","body":"Related to:\r\n\r\n- #20179\r\n- #45906 \r\n- moby\/libnetwork#2093\r\n- https:\/\/github.com\/moby\/moby\/issues\/20067\r\n\r\n**- What I did**\r\n\r\n* 1st commit: Add a new `Priority` field to `EndpointSettings` ;\r\n* 2nd commit:\r\n  * Use this new field to make sure the calls to `daemon.connectToNetwork` are done in descending order ;\r\n  * Also, use it to make sure the endpoints join the sandbox in the same order (that's what guarantees interface names are stable over restarts) ;\r\n* 3rd commit: On `NetworkConnect`, the `Priority` value is automatically computed based on the lowest `Priority` minus one ;\r\n\r\nThe choice to use descending over ascending order has been made to match what the Compose spec defines.\r\n\r\nThis change brings interesting advantages for end users:\r\n\r\n* Since the interface name\/order is now stable, users can bind a specific interface without needing an external script to resolve what interface should be used ;\r\n* The DNS resolver will always iterate on a container's networks in the exact same order, hence DNS resolution for a given name present on multiple networks a container is connected to will always resolve to the same thing ;\r\n\r\n**- How to verify it**\r\n\r\n* CI should be green.\r\n\r\n**- Description for the changelog**\r\n\r\nInterfaces are always created in the same order and their name is stable over restarts.\r\n\r\n**- A picture of a cute animal (not mandatory but encouraged)**\r\n\r\n![](https:\/\/www.caniprof.com\/wp-content\/uploads\/2019\/02\/eurasier-4.jpg)\r\n\r\n","comments":["So, one of the failing test is checking if the default gateway and \"anonymous\" port mappings change whenever the container connects to a network which comes before other networks the container is connected to in the lexicographic order:\r\n\r\n```\r\n$ docker network create testnet1\r\n$ docker network create aaaa\r\n$ docker run -d --name test -p 9000:90 -p 70 busybox top\r\n\r\n$ docker port test\r\n70\/tcp -> 0.0.0.0:32774\r\n70\/tcp -> [::]:32774\r\n90\/tcp -> 0.0.0.0:9000\r\n90\/tcp -> [::]:9000\r\n$ docker exec test ip -o route show      \r\ndefault via 172.18.0.1 dev eth0 \r\n172.18.0.0\/16 dev eth0 scope link  src 172.18.0.2 \r\n\r\n$ docker network connect testnet1 test\r\n$ docker port test                       \r\n70\/tcp -> 0.0.0.0:32774\r\n70\/tcp -> [::]:32774\r\n90\/tcp -> 0.0.0.0:9000\r\n90\/tcp -> [::]:9000\r\n$ docker exec test ip -o route show      \r\ndefault via 172.18.0.1 dev eth0 \r\n172.18.0.0\/16 dev eth0 scope link  src 172.18.0.2 \r\n172.19.0.0\/16 dev eth3 scope link  src 172.19.0.2 \r\n\r\n# Here we go:\r\n$ docker network connect aaaa test \r\n$ docker port test                 \r\n70\/tcp -> 0.0.0.0:32775\r\n70\/tcp -> [::]:32775\r\n90\/tcp -> 0.0.0.0:9000\r\n90\/tcp -> [::]:9000\r\n$ docker exec test ip -o route show\r\ndefault via 172.21.0.1 dev eth4 \r\n172.18.0.0\/16 dev eth0 scope link  src 172.18.0.2 \r\n172.19.0.0\/16 dev eth3 scope link  src 172.19.0.2 \r\n172.21.0.0\/16 dev eth4 scope link  src 172.21.0.2 \r\n```\r\n\r\nAnd I'm pretty sure some people rely on this to set their default gateway. I need to check if that's still documented (it was at some point) and what to do next.","Is there still work in progress here? Or something missing?"],"labels":["area\/api","status\/2-code-review","kind\/enhancement","area\/networking","impact\/api","impact\/changelog","area\/daemon"]},{"title":"Inconsistent behaviour of masquerade \/ proxy","body":"### Description\n\nDepending on user \/ OS setup, masquerade and the userland proxy seems to have inconsistent behaviour.\r\nIn particular in some cases, getting an HTTP client remote address work fine, but in others it returns the gateway IP\n\n### Reproduce\n\n1. perform the basic Ubuntu + Docker setup\r\n2. run any http server as a container\r\n3. check the log for client remote IP -> it will be the right IP\r\n\r\nNow I have so far noted two instances where it didn't work: \r\n\r\n- using Docker Desktop on Windows\r\n- in some cases when network is IPV6\r\n\n\n### Expected behavior\n\nHaving a clearer view on how to diagnose this issue and what are the steps to always getting the right IP back in every situation. This is a pretty bad behaviour as it will break any container with an HTTP server doing any kind of rate limiting (every requests come from the same IP)\n\n### docker version\n\n```bash\nClient: Docker Engine - Community\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:52:07 2023\r\n OS\/Arch:           linux\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f\r\n  Built:            Thu May 25 21:52:07 2023\r\n  OS\/Arch:          linux\/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.21\r\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc:\r\n  Version:          1.1.7\r\n  GitCommit:        v1.1.7-0-g860f061\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\n```\n\n\n### docker info\n\n```bash\nClient: Docker Engine - Community\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n Plugins:\r\n  buildx: Docker Buildx (Docker Inc.)\r\n    Version:  v0.10.5\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-buildx\r\n  compose: Docker Compose (Docker Inc.)\r\n    Version:  v2.18.1\r\n    Path:     \/usr\/libexec\/docker\/cli-plugins\/docker-compose\r\n\r\nServer:\r\n Containers: 3\r\n  Running: 3\r\n  Paused: 0\r\n  Stopped: 0\r\n Images: 18\r\n Server Version: 24.0.2\r\n Storage Driver: overlay2\r\n  Backing Filesystem: extfs\r\n  Supports d_type: true\r\n  Using metacopy: false\r\n  Native Overlay Diff: true\r\n  userxattr: false\r\n Logging Driver: json-file\r\n Cgroup Driver: systemd\r\n Cgroup Version: 2\r\n Plugins:\r\n  Volume: local\r\n  Network: bridge host ipvlan macvlan null overlay\r\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n Swarm: inactive\r\n Runtimes: io.containerd.runc.v2 runc\r\n Default Runtime: runc\r\n Init Binary: docker-init\r\n containerd version: 3dce8eb055cbb6872793272b4f20ed16117344f8\r\n runc version: v1.1.7-0-g860f061\r\n init version: de40ad0\r\n Security Options:\r\n  apparmor\r\n  seccomp\r\n   Profile: builtin\r\n  cgroupns\r\n Kernel Version: 6.1.0-10-amd64\r\n Operating System: Debian GNU\/Linux 12 (bookworm)\r\n OSType: linux\r\n Architecture: x86_64\r\n CPUs: 1\r\n Total Memory: 961MiB\r\n Name: debian-s-1vcpu-1gb-intel-lon1-01\r\n ID: f7cae198-a223-4747-8676-3a6203517c77\r\n Docker Root Dir: \/var\/lib\/docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\n```\n\n\n### Additional Info\n\nBehaviours of masquerade and the userland proxy are not so well documented and some resources on the web seems outdated which makes it hard to troubleshoot issues with it. Any hint on how to make this ticket a little bit more relevant would be appreciated too","comments":["Here are two `docker inspect` of two containers, one has the proper IP one has gateway IP when reading the HTTP incoming address. They both use Vanilla docker daemon, the working one is on Linux, the broken one is on WSL. As far as I can tell nothing in ther description hint any flaw in the network config but for the sake of completeness\r\n\r\nThe broken one:\r\n\r\n```json\r\n[\r\n    {\r\n        \"Id\": \"ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0\",\r\n        \"Created\": \"2023-07-19T12:17:54.330530418Z\",\r\n        \"Path\": \".\/cosmos\",\r\n        \"Args\": [],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 123323,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2023-07-19T12:17:55.927016763Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:0b7dd7be3adb5a0b03c970ba424da6140ac1425e2328b6cf84eec218f762fa9f\",\r\n        \"ResolvConfPath\": \"\/var\/lib\/docker\/containers\/ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/lib\/docker\/containers\/ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0\/hostname\",\r\n        \"HostsPath\": \"\/var\/lib\/docker\/containers\/ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0\/hosts\",\r\n        \"LogPath\": \"\/var\/lib\/docker\/containers\/ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0\/ba5afc62c50b4ee11b45d4882e4ebe4233f62f52fc5e0754c320edf8388fceb0-json.log\",\r\n        \"Name\": \"\/cosmos-server\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": [\r\n                \"\/var\/run\/docker.sock:\/var\/run\/docker.sock\",\r\n                \"\/:\/mnt\/host\",\r\n                \"\/var\/lib\/cosmos:\/config\"\r\n            ],\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"default\",\r\n            \"PortBindings\": {\r\n                \"443\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"443\"\r\n                    }\r\n                ],\r\n                \"80\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"80\"\r\n                    }\r\n                ]\r\n            },\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"always\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"ConsoleSize\": [\r\n                30,\r\n                120\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"host\",\r\n            \"Dns\": [],\r\n            \"DnsOptions\": [],\r\n            \"DnsSearch\": [],\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": true,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": [\r\n                \"label=disable\"\r\n            ],\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": [],\r\n            \"BlkioDeviceReadBps\": [],\r\n            \"BlkioDeviceWriteBps\": [],\r\n            \"BlkioDeviceReadIOps\": [],\r\n            \"BlkioDeviceWriteIOps\": [],\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": [],\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": false,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": null,\r\n            \"ReadonlyPaths\": null\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/lib\/docker\/overlay2\/e1ba140799a8197281ba7d5eb37d8979ad96258e240d043528ea2d16bd08046e-init\/diff:\/var\/lib\/docker\/overlay2\/2bff27ffe484dd92c505b184847512820d9c8f095d7c7c67c7d0b9e90da4d898\/diff:\/var\/lib\/docker\/overlay2\/2be2a537e5983c7d3f7a2d4e69774435f2c9c52bf619189f2b20b161df070c6d\/diff:\/var\/lib\/docker\/overlay2\/1212ce9f9b8b5e43024e689ec3b897ba3dd8083fc967e3d88271cbbe2d6af8bc\/diff:\/var\/lib\/docker\/overlay2\/fb32c01c3a334c98da507e078e2a419c09a608bf857c29d480b68f5a36f6e683\/diff:\/var\/lib\/docker\/overlay2\/bd60527a78a498b8725924391b05264c44c0e3e6f591b5ad75229418b60c2890\/diff:\/var\/lib\/docker\/overlay2\/80142e35806ec4cc6871fef7359fe053a08399f9dae98da2be66fc859d6245ab\/diff:\/var\/lib\/docker\/overlay2\/4beef0204045824585bb8713de655fc27dba617c4987c9891530161ba21f16fe\/diff:\/var\/lib\/docker\/overlay2\/3970f65a9e421771b01b811be85aa7dca64d338d78479e022fc329681e3c1a3d\/diff:\/var\/lib\/docker\/overlay2\/456363ce67775b281f1bde637eae70edc89ba8fc56cac029392c2b262d97c1b5\/diff\",\r\n                \"MergedDir\": \"\/var\/lib\/docker\/overlay2\/e1ba140799a8197281ba7d5eb37d8979ad96258e240d043528ea2d16bd08046e\/merged\",\r\n                \"UpperDir\": \"\/var\/lib\/docker\/overlay2\/e1ba140799a8197281ba7d5eb37d8979ad96258e240d043528ea2d16bd08046e\/diff\",\r\n                \"WorkDir\": \"\/var\/lib\/docker\/overlay2\/e1ba140799a8197281ba7d5eb37d8979ad96258e240d043528ea2d16bd08046e\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/var\/run\/docker.sock\",\r\n                \"Destination\": \"\/var\/run\/docker.sock\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            },\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/\",\r\n                \"Destination\": \"\/mnt\/host\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rslave\"\r\n            },\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/var\/lib\/cosmos\",\r\n                \"Destination\": \"\/config\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            }\r\n        ],\r\n        \"Config\": {\r\n            \"Hostname\": \"cosmos-server\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"ExposedPorts\": {\r\n                \"443\/tcp\": {},\r\n                \"80\/tcp\": {}\r\n            },\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n            ],\r\n            \"Cmd\": [\r\n                \".\/cosmos\"\r\n            ],\r\n            \"Image\": \"azukaar\/cosmos-server:latest\",\r\n            \"Volumes\": {\r\n                \"\/config\": {}\r\n            },\r\n            \"WorkingDir\": \"\/app\",\r\n            \"Entrypoint\": null,\r\n            \"OnBuild\": null,\r\n            \"Labels\": {}\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"1325b1e887dd3883de0811e0ab67c801a8c4a3db84e1b2a8d10d2469808a2301\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {\r\n                \"443\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"443\"\r\n                    }\r\n                ],\r\n                \"80\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"80\"\r\n                    }\r\n                ]\r\n            },\r\n            \"SandboxKey\": \"\/var\/run\/docker\/netns\/1325b1e887dd\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"bb4bb9a9af7ec4425d70c1482c5dddfdb9ec02177f57a7311c4fd186ada53662\",\r\n            \"Gateway\": \"172.17.0.1\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"172.17.0.2\",\r\n            \"IPPrefixLen\": 16,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"02:42:ac:11:00:02\",\r\n            \"Networks\": {\r\n                \"bridge\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": null,\r\n                    \"NetworkID\": \"2a5b53ca77d1e31bda76b1e8e45fb99ce37cbcc5a7c9f9bde933bdfed3214f09\",\r\n                    \"EndpointID\": \"bb4bb9a9af7ec4425d70c1482c5dddfdb9ec02177f57a7311c4fd186ada53662\",\r\n                    \"Gateway\": \"172.17.0.1\",\r\n                    \"IPAddress\": \"172.17.0.2\",\r\n                    \"IPPrefixLen\": 16,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-A46avkguL\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"acd2f253408ddb2222f170286fa2ac52a829b2901797085229beb4314ab5411f\",\r\n                    \"EndpointID\": \"226500c6096cfe65cf76ffe32a919501c4f0e0b90e467580617e773079083e55\",\r\n                    \"Gateway\": \"100.0.0.25\",\r\n                    \"IPAddress\": \"100.0.0.27\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:1b\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-N9C594YPJ\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"e5787cb44bbcc7d327bd86822b28650a3343d138c5d0e9ea1785b64c3270d94a\",\r\n                    \"EndpointID\": \"e6cf166425a994cebd2ce4faac16dd8ff50f555e8786e3a483071c8eefa831a1\",\r\n                    \"Gateway\": \"100.0.0.17\",\r\n                    \"IPAddress\": \"100.0.0.19\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:13\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-ONU3S6KIM\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"add73329129fe4ea49e53062ab17844d1494cd2f47ed1c76718c249dbd700d31\",\r\n                    \"EndpointID\": \"9d30aef34d100cb38b18f736f542e25aefcec39ceac68e432a976edf6944a8ec\",\r\n                    \"Gateway\": \"100.0.0.73\",\r\n                    \"IPAddress\": \"100.0.0.75\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:4b\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-RZs2WKRfP\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"e3c9bb42a2b5e5c2b8aaaa6ecf87b9c98c695c235678d868e17c61cddbb5d5de\",\r\n                    \"EndpointID\": \"360dae3cf92e4a714a07cff1f0eef3e88e4447448091117121577de5c5359bcc\",\r\n                    \"Gateway\": \"100.0.0.33\",\r\n                    \"IPAddress\": \"100.0.0.35\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:23\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-XenIF09oc\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"a74a5193be43c0f17ea95b73352fcfe016d258a4f39c05e8c7da8f549953ffea\",\r\n                    \"EndpointID\": \"8fe4702ae9b2fe4f65339305693e374d54d01d3c3e79dc1a8469c81b6a6de2c8\",\r\n                    \"Gateway\": \"100.0.0.1\",\r\n                    \"IPAddress\": \"100.0.0.3\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:03\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-jS276P0g4\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"ab36b878f8c543f227f40a5412283c3d3976512be13d36db7ec85ff551987023\",\r\n                    \"EndpointID\": \"f004e30f3436a437cdffe9a4cec0915e534ba015873a0f4e90b7923d0a9efc5b\",\r\n                    \"Gateway\": \"100.0.0.89\",\r\n                    \"IPAddress\": \"100.0.0.91\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:5b\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-vsO6hcgcm\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ba5afc62c50b\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"7fc825d9f65820955b22d0fde8b0682c47f36e2b065c42c289b7926747251a5d\",\r\n                    \"EndpointID\": \"955ff1ce0c2eac2ed22fe61972a4bb71013aac3bc99beeddcabcc93c6e257911\",\r\n                    \"Gateway\": \"100.0.0.97\",\r\n                    \"IPAddress\": \"100.0.0.99\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:63\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n\r\n```\r\n\r\nAnd the working one:\r\n\r\n```json\r\n[\r\n    {\r\n        \"Id\": \"ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4\",\r\n        \"Created\": \"2023-07-19T11:14:35.87963535Z\",\r\n        \"Path\": \".\/cosmos\",\r\n        \"Args\": [],\r\n        \"State\": {\r\n            \"Status\": \"running\",\r\n            \"Running\": true,\r\n            \"Paused\": false,\r\n            \"Restarting\": false,\r\n            \"OOMKilled\": false,\r\n            \"Dead\": false,\r\n            \"Pid\": 32537,\r\n            \"ExitCode\": 0,\r\n            \"Error\": \"\",\r\n            \"StartedAt\": \"2023-07-19T11:14:36.368661779Z\",\r\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n        },\r\n        \"Image\": \"sha256:0b7dd7be3adb5a0b03c970ba424da6140ac1425e2328b6cf84eec218f762fa9f\",\r\n        \"ResolvConfPath\": \"\/var\/lib\/docker\/containers\/ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4\/resolv.conf\",\r\n        \"HostnamePath\": \"\/var\/lib\/docker\/containers\/ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4\/hostname\",\r\n        \"HostsPath\": \"\/var\/lib\/docker\/containers\/ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4\/hosts\",\r\n        \"LogPath\": \"\/var\/lib\/docker\/containers\/ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4\/ff4c529d2430cb930cdad7966debaf6b2e6601e65f1ef4ce29a589afef994fe4-json.log\",\r\n        \"Name\": \"\/cosmos-server\",\r\n        \"RestartCount\": 0,\r\n        \"Driver\": \"overlay2\",\r\n        \"Platform\": \"linux\",\r\n        \"MountLabel\": \"\",\r\n        \"ProcessLabel\": \"\",\r\n        \"AppArmorProfile\": \"unconfined\",\r\n        \"ExecIDs\": null,\r\n        \"HostConfig\": {\r\n            \"Binds\": [\r\n                \"\/var\/run\/docker.sock:\/var\/run\/docker.sock\",\r\n                \"\/:\/mnt\/host\",\r\n                \"\/var\/lib\/cosmos:\/config\"\r\n            ],\r\n            \"ContainerIDFile\": \"\",\r\n            \"LogConfig\": {\r\n                \"Type\": \"json-file\",\r\n                \"Config\": {}\r\n            },\r\n            \"NetworkMode\": \"default\",\r\n            \"PortBindings\": {\r\n                \"443\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"443\"\r\n                    }\r\n                ],\r\n                \"80\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"\",\r\n                        \"HostPort\": \"80\"\r\n                    }\r\n                ]\r\n            },\r\n            \"RestartPolicy\": {\r\n                \"Name\": \"always\",\r\n                \"MaximumRetryCount\": 0\r\n            },\r\n            \"AutoRemove\": false,\r\n            \"VolumeDriver\": \"\",\r\n            \"VolumesFrom\": null,\r\n            \"ConsoleSize\": [\r\n                57,\r\n                211\r\n            ],\r\n            \"CapAdd\": null,\r\n            \"CapDrop\": null,\r\n            \"CgroupnsMode\": \"private\",\r\n            \"Dns\": [],\r\n            \"DnsOptions\": [],\r\n            \"DnsSearch\": [],\r\n            \"ExtraHosts\": null,\r\n            \"GroupAdd\": null,\r\n            \"IpcMode\": \"private\",\r\n            \"Cgroup\": \"\",\r\n            \"Links\": null,\r\n            \"OomScoreAdj\": 0,\r\n            \"PidMode\": \"\",\r\n            \"Privileged\": true,\r\n            \"PublishAllPorts\": false,\r\n            \"ReadonlyRootfs\": false,\r\n            \"SecurityOpt\": [\r\n                \"label=disable\"\r\n            ],\r\n            \"UTSMode\": \"\",\r\n            \"UsernsMode\": \"\",\r\n            \"ShmSize\": 67108864,\r\n            \"Runtime\": \"runc\",\r\n            \"Isolation\": \"\",\r\n            \"CpuShares\": 0,\r\n            \"Memory\": 0,\r\n            \"NanoCpus\": 0,\r\n            \"CgroupParent\": \"\",\r\n            \"BlkioWeight\": 0,\r\n            \"BlkioWeightDevice\": [],\r\n            \"BlkioDeviceReadBps\": [],\r\n            \"BlkioDeviceWriteBps\": [],\r\n            \"BlkioDeviceReadIOps\": [],\r\n            \"BlkioDeviceWriteIOps\": [],\r\n            \"CpuPeriod\": 0,\r\n            \"CpuQuota\": 0,\r\n            \"CpuRealtimePeriod\": 0,\r\n            \"CpuRealtimeRuntime\": 0,\r\n            \"CpusetCpus\": \"\",\r\n            \"CpusetMems\": \"\",\r\n            \"Devices\": [],\r\n            \"DeviceCgroupRules\": null,\r\n            \"DeviceRequests\": null,\r\n            \"MemoryReservation\": 0,\r\n            \"MemorySwap\": 0,\r\n            \"MemorySwappiness\": null,\r\n            \"OomKillDisable\": null,\r\n            \"PidsLimit\": null,\r\n            \"Ulimits\": null,\r\n            \"CpuCount\": 0,\r\n            \"CpuPercent\": 0,\r\n            \"IOMaximumIOps\": 0,\r\n            \"IOMaximumBandwidth\": 0,\r\n            \"MaskedPaths\": null,\r\n            \"ReadonlyPaths\": null\r\n        },\r\n        \"GraphDriver\": {\r\n            \"Data\": {\r\n                \"LowerDir\": \"\/var\/lib\/docker\/overlay2\/8e70d126dab2179edb921f79752e2a05b15db8ecefd639c8a7a3a496fecfd790-init\/diff:\/var\/lib\/docker\/overlay2\/dffd5cab86bb1d25133ec87c8c44b5c15198d220dae090d144096a4cab25393f\/diff:\/var\/lib\/docker\/overlay2\/993441cf88cf85d6dacb2e6a6f03cc0d880cb81680b30163f8f17473e4378662\/diff:\/var\/lib\/docker\/overlay2\/07a2f4fa40be6d0299fe6fab62c5b8c1894d57cff560bae8975ba45fdc14619c\/diff:\/var\/lib\/docker\/overlay2\/c89886c1a12a409fa55ca491f5e9a674d34d28e8e679eccb090e44ddcd6bcdb6\/diff:\/var\/lib\/docker\/overlay2\/a5ca0541cf2523519aedd9febd685c2336db798c6e64821cf1517ad51880535b\/diff:\/var\/lib\/docker\/overlay2\/5cc46029dcc51a2dbd0492ed0c657ef16019bb1327c8873a0cc692d8f35b0d1a\/diff:\/var\/lib\/docker\/overlay2\/a8cb6072d58a39e5dd765a459ded396c3468588d6d4504ac7cb7aed4a45d2118\/diff:\/var\/lib\/docker\/overlay2\/68e37a041a8f651178adcb0ad25e75dfcbcde924dd00dc8a974ee70783ffdb67\/diff:\/var\/lib\/docker\/overlay2\/a41089183a51cc4f13308cf51c13fc07daf0644dc8d8540a4e9f9d21d124eca7\/diff\",\r\n                \"MergedDir\": \"\/var\/lib\/docker\/overlay2\/8e70d126dab2179edb921f79752e2a05b15db8ecefd639c8a7a3a496fecfd790\/merged\",\r\n                \"UpperDir\": \"\/var\/lib\/docker\/overlay2\/8e70d126dab2179edb921f79752e2a05b15db8ecefd639c8a7a3a496fecfd790\/diff\",\r\n                \"WorkDir\": \"\/var\/lib\/docker\/overlay2\/8e70d126dab2179edb921f79752e2a05b15db8ecefd639c8a7a3a496fecfd790\/work\"\r\n            },\r\n            \"Name\": \"overlay2\"\r\n        },\r\n        \"Mounts\": [\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/var\/run\/docker.sock\",\r\n                \"Destination\": \"\/var\/run\/docker.sock\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            },\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/\",\r\n                \"Destination\": \"\/mnt\/host\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rslave\"\r\n            },\r\n            {\r\n                \"Type\": \"bind\",\r\n                \"Source\": \"\/var\/lib\/cosmos\",\r\n                \"Destination\": \"\/config\",\r\n                \"Mode\": \"\",\r\n                \"RW\": true,\r\n                \"Propagation\": \"rprivate\"\r\n            }\r\n        ],\r\n        \"Config\": {\r\n            \"Hostname\": \"cosmos-server\",\r\n            \"Domainname\": \"\",\r\n            \"User\": \"\",\r\n            \"AttachStdin\": false,\r\n            \"AttachStdout\": false,\r\n            \"AttachStderr\": false,\r\n            \"ExposedPorts\": {\r\n                \"443\/tcp\": {},\r\n                \"80\/tcp\": {}\r\n            },\r\n            \"Tty\": false,\r\n            \"OpenStdin\": false,\r\n            \"StdinOnce\": false,\r\n            \"Env\": [\r\n                \"PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\"\r\n            ],\r\n            \"Cmd\": [\r\n                \".\/cosmos\"\r\n            ],\r\n            \"Image\": \"azukaar\/cosmos-server:latest\",\r\n            \"Volumes\": {\r\n                \"\/config\": {}\r\n            },\r\n            \"WorkingDir\": \"\/app\",\r\n            \"Entrypoint\": null,\r\n            \"OnBuild\": null,\r\n            \"Labels\": {}\r\n        },\r\n        \"NetworkSettings\": {\r\n            \"Bridge\": \"\",\r\n            \"SandboxID\": \"9f737cffd217731378165c922cd4f19b7efae3ddf6aa01354feab63845a7394e\",\r\n            \"HairpinMode\": false,\r\n            \"LinkLocalIPv6Address\": \"\",\r\n            \"LinkLocalIPv6PrefixLen\": 0,\r\n            \"Ports\": {\r\n                \"443\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"443\"\r\n                    },\r\n                    {\r\n                        \"HostIp\": \"::\",\r\n                        \"HostPort\": \"443\"\r\n                    }\r\n                ],\r\n                \"80\/tcp\": [\r\n                    {\r\n                        \"HostIp\": \"0.0.0.0\",\r\n                        \"HostPort\": \"80\"\r\n                    },\r\n                    {\r\n                        \"HostIp\": \"::\",\r\n                        \"HostPort\": \"80\"\r\n                    }\r\n                ]\r\n            },\r\n            \"SandboxKey\": \"\/var\/run\/docker\/netns\/9f737cffd217\",\r\n            \"SecondaryIPAddresses\": null,\r\n            \"SecondaryIPv6Addresses\": null,\r\n            \"EndpointID\": \"21c651a842031fd4980faaa4eac6c3a8fff07909bd286d0e727a5bb53a7ae4d1\",\r\n            \"Gateway\": \"172.17.0.1\",\r\n            \"GlobalIPv6Address\": \"\",\r\n            \"GlobalIPv6PrefixLen\": 0,\r\n            \"IPAddress\": \"172.17.0.2\",\r\n            \"IPPrefixLen\": 16,\r\n            \"IPv6Gateway\": \"\",\r\n            \"MacAddress\": \"02:42:ac:11:00:02\",\r\n            \"Networks\": {\r\n                \"bridge\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": null,\r\n                    \"NetworkID\": \"7a3dcbf30020f2fcdd7d1690e5ce1a5eb9eb881f907aca7981e7d2c95887b463\",\r\n                    \"EndpointID\": \"21c651a842031fd4980faaa4eac6c3a8fff07909bd286d0e727a5bb53a7ae4d1\",\r\n                    \"Gateway\": \"172.17.0.1\",\r\n                    \"IPAddress\": \"172.17.0.2\",\r\n                    \"IPPrefixLen\": 16,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-SJzWq8MEb\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ff4c529d2430\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"dad60e87267b511f38a2076f16274c4b3c4f9ed3864943cee4b468f3952b931b\",\r\n                    \"EndpointID\": \"6a23d186ef07d6b919d5cbb66c38428c0d8ac5dbbf75e1d547db0fd07d4ea086\",\r\n                    \"Gateway\": \"100.0.0.1\",\r\n                    \"IPAddress\": \"100.0.0.3\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:03\",\r\n                    \"DriverOpts\": null\r\n                },\r\n                \"cosmos-network-XbW3AsmAs\": {\r\n                    \"IPAMConfig\": null,\r\n                    \"Links\": null,\r\n                    \"Aliases\": [\r\n                        \"ff4c529d2430\",\r\n                        \"cosmos-server\"\r\n                    ],\r\n                    \"NetworkID\": \"edc480aa2a43c791741a762a29295f546504ae8afe7b4fdc58df546225f095ed\",\r\n                    \"EndpointID\": \"7d3f3c0bbb65385870f2a409ae9138cf7e8b3c12c86a8c085086355e183761ee\",\r\n                    \"Gateway\": \"100.0.0.9\",\r\n                    \"IPAddress\": \"100.0.0.11\",\r\n                    \"IPPrefixLen\": 29,\r\n                    \"IPv6Gateway\": \"\",\r\n                    \"GlobalIPv6Address\": \"\",\r\n                    \"GlobalIPv6PrefixLen\": 0,\r\n                    \"MacAddress\": \"02:42:64:00:00:0b\",\r\n                    \"DriverOpts\": null\r\n                }\r\n            }\r\n        }\r\n    }\r\n]\r\n```","I've actually documented `userland-proxy` pretty well, and I've found how to get it more consistent in both states with some additional iptables rules. I have a little bit left as WIP which I lost a portion of (_for more niche scenarios_) due to PC dying of a power cut.\r\n\r\nThe main inconsistencies I raised separate issues for and provided the iptables rules to resolve them. I don't think they've received any engagement to resolve though. There is focus on an upcoming replacement with a IPVS network driver that will avoid having to mess with iptables entirely.\r\n\r\n---\r\n\r\nFor your IPv6 experience, make sure you've enabled `experimental: true` and `ip6tables: true`. Assuming you're using an IPv6 ULA subnet to match the IPv4 NAT experience. That should be consistent there.\r\n\r\nYou'll not be able to access localhost over IPv6 (`[::1]`) with `userland-proxy: false`, there is no `docker-proxy` process involved, and it only works for IPv4 proxying to `dockerd` because of a kernel tunable `route_localhost` (only for IPv4).\r\n\r\nFor the localhost to work with `userland-proxy: true` it relies on iptables rules to masquerade and make `127.0.0.0\/8` an exception IIRC. It'll always be the docker network gateway IP, instead use public interface or direct container IP\/alias if this matters for you. For external clients it shouldn't be an issue.\r\n\r\nI can't comment much on Windows and macOS which AFAIK rely on a VM layer to run docker inside (and lacks IPv6?). I do recall that these don't use `userland-proxy` via the Docker Desktop app and have a different approach instead.","Thanks for the answer, unfortunately does not make my life any easier :D \r\nIs there an approach that could simplify this issue?\r\nEg. aside from localhost issue, is there any reason you wouldnt want to disable userland-proxy if you have a masquerade issue? Will it fix the issue all\/most of the time?\r\nI also noticed it was possible to publish ports in \"host\" mode in docker-compose is that something that would solve the issue, does the container still live within the docker network layer in that case? (as in, my issue with host containers is that you cannot connect to user defined networks anymore)\r\nthanks for your help!","> I also noticed it was possible to publish ports in \"host\" mode in docker-compose is that something that would solve the issue, does the container still live within the docker network layer in that case? (as in, my issue with host containers is that you cannot connect to user defined networks anymore)\r\n\r\nHost mode typically isn't meant to support port publishing, I've heard of users using compose to publish ports in host mode specifically though but I'm not familiar with how that works myself.\r\n\r\nYou can have host mode network defined apparently and add that and user-defined networks: https:\/\/docs.docker.com\/compose\/compose-file\/06-networks\/#host-or-none\r\n\r\n> Eg. aside from localhost issue, is there any reason you wouldnt want to disable userland-proxy if you have a masquerade issue? Will it fix the issue all\/most of the time?\r\n\r\n`userland-proxy` mostly only matters for local connections, primarily those with localhost. Really depends what you're doing though as both `false` and `true` settings presently have the gateway IP issue in different scenarios.\r\n\r\nHere is what I salvaged from the last of my WIP notes:\r\n\r\n---\r\n\r\n## `userland-proxy` table for preserving client IP vs replacing with gateway IP\r\n\r\n| `userland-proxy` | Remote (H)                       | Host (L) <sup>2<\/sup> | Host (H)                       | Host (C) | Container (H)        | Container (C)  | Self (H) | Self (C)   | \r\n| :----------------------- | :----------------------------: | :-------------------------------: | :----------------------------: | :----------: | :-----------------------: | :-------------------: | :---------: | :---------: |\r\n| `true`                     | \u2714\ufe0f <sup>1a<\/sup>     | \u274c                                    | \u2714\ufe0f <sup>1a<\/sup>     | \u274c          | \u274c <sup>4<\/sup> | \u2714\ufe0f                     | \u274c         | \u2714\ufe0f          |\r\n| `false`                    | \u2714\ufe0f <sup>1b<\/sup> | \u274c                                    | \u274c <sup>1b, 3<\/sup> | \u274c          | \u2714\ufe0f <sup>5<\/sup> | \u2714\ufe0f                    | \u274c         | \u2714\ufe0f          |\r\n\r\n1. If the _Host IP_ is IPv6 and `ip6tables: false` (default):\r\n   a. Source Address becomes the Gateway IP\r\n   b. Connection fails (_hangs if the container was assigned an IPv6 address_)\r\n2. `localhost` \/ `127.0.0.1` as the Source Address would be ambiguous between host and container (_the Docker Gateway IP helps disambiguate that?_)\r\n3. Gateway IP (_[Resolvable](https:\/\/github.com\/moby\/moby\/issues\/45629)_)\r\n4. Gateway IP (_[Resolvable](https:\/\/github.com\/moby\/moby\/issues\/38784#issuecomment-1565219612)_)\r\n5. Connection hangs trying to connect across separate docker networks (_[Resolvable](https:\/\/github.com\/moby\/moby\/issues\/38784#issuecomment-1565219612)_)\r\n\r\n### Legend\r\n\r\n`curl` request (_with source IP originating from the **Remote**, **Host**, a separate **Container**, or the same container **Self**_) to the target container (eg: `traefik\/whoami`) via _destination IP_:\r\n- **L** => `localhost` (`127.0.0.1` \/ `[::1]`, indirect)\r\n- **H** => Host IP (indirect)\r\n- **C** => Container IP (direct)\r\n\r\n`RemoteAddr` (_Connection source address_) is:\r\n  - \u2714\ufe0f => Preserved.\r\n  - \u274c => Replaced with the Docker Gateway IP associated to the target container.\r\n    - If the docker gateway lacks an IPv6 address, then IPv6 connections receive an IPv4 gateway address instead.","If you'd like more insights, I've been documenting them here: https:\/\/github.com\/docker\/docs\/issues\/17312\r\n\r\nThere's a lot of hidden\/collapsed information in the comments there. I still need to do a bit more revision\/organization of what is already there to make it more digestable, and that's what the table above was part of.\r\n\r\nSome of the :x: above doesn't have a resolvable `iptables` rule shared, but I believe I figured them out... I just can't remember how now and doubt I'll have time again to invest in that unfortunately so hopefully the future IPVS network driver will be a better choice going forward.\r\n\r\nYou'll find the issue I linked provides the most comprehensive documentation on `userland-proxy` than you will find anywhere else, I assure you of that :sweat_smile: \r\n\r\n---\r\n\r\nYou'll also find this long-standing tracking issue for getting `userland-proxy` disabled by default in future release, and removed completely once the feature has sufficient parity (_can't get IPv6 localhost though_): https:\/\/github.com\/moby\/moby\/issues\/14856#issuecomment-1541139817\r\n\r\nThat highlights\/tracks all known issues regarding the feature, I've looked into many of them and identified if they're still a blocker or provided `iptables` rules to resolve them. Just needs the Docker devs to tackle the actual work beyond that :muscle: "],"labels":["status\/0-triage","kind\/bug","area\/networking"]},{"title":"c8d: Cleanup leases on startup","body":"### Description\n\nSome of the operations with containerd could leak a lease if the daemon is forcefully killed while working. Add a check at startup that lists and removes all the dangling leases\r\n\r\nSee [this discussion](https:\/\/github.com\/moby\/moby\/pull\/45963#discussion_r1266610759)","comments":[],"labels":["status\/0-triage","kind\/feature","containerd-integration"]},{"title":"Swarm: Internal DNS favors one replica IP over the other","body":"### Description\r\n\r\nI have a central logger service which receives log messages from other app services over ZeroMQ. The MQ is setup to listen on tcp:\/\/log_svc:20000, and other services connect to it over TCP with dns name of the log_svc.\r\nIt all starts evenly distributed, one message is routed to log_svc on node1, the other on node2, but after some time, the Swarm DNS resolution starts favoring node1 over the other, or vice versa.\r\nThis is a Windows app, and nodes are Windows Server 2022, as are the images.\r\n\r\n```\r\n# this is log_svc deploy section\r\n\r\n    deploy:\r\n#      endpoint_mode: dnsrr\r\n#      mode: global\r\n      replicas: 2\r\n      restart_policy:\r\n        condition: any\r\n      placement:\r\n        max_replicas_per_node: 1\r\n        constraints:\r\n#          - node.role==worker\r\n          - node.labels.os == windows\r\n    networks:\r\n      - stack1_net\r\n      - stack2_net\r\n```\r\n\r\n### Reproduce\r\n\r\n1. docker stack deploy -c C:\\docker\\swarm\\others\\docker-compose-others.yml --with-registry-auth others_stack\r\n2. docker stack deploy -c C:\\docker\\swarm\\log_svc\\docker-compose-log_svc.yml --with-registry-auth log_svc_stack\r\n3. wait 5-6 hours\r\n4. send messages\r\n\r\n### Expected behavior\r\n\r\nThe messages are evenly distributed to both log_svc replicas.\r\n\r\n### docker version\r\n\r\n```bash\r\nClient:\r\n Version:           24.0.2\r\n API version:       1.43\r\n Go version:        go1.20.4\r\n Git commit:        cb74dfc\r\n Built:             Thu May 25 21:53:15 2023\r\n OS\/Arch:           windows\/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.2\r\n  API version:      1.43 (minimum version 1.24)\r\n  Go version:       go1.20.4\r\n  Git commit:       659604f9\r\n  Built:            Thu May 25 21:52:13 2023\r\n  OS\/Arch:          windows\/amd64\r\n  Experimental:     false\r\n```\r\n\r\n\r\n### docker info\r\n\r\n```bash\r\nClient:\r\n Version:    24.0.2\r\n Context:    default\r\n Debug Mode: false\r\n\r\nServer:\r\n Containers: 42\r\n  Running: 10\r\n  Paused: 0\r\n  Stopped: 32\r\n Images: 35\r\n Server Version: 24.0.2\r\n Storage Driver: windowsfilter\r\n  Windows:\r\n Logging Driver: json-file\r\n Plugins:\r\n  Volume: local\r\n  Network: ics internal l2bridge l2tunnel nat null overlay private transparent\r\n  Log: awslogs etwlogs fluentd gcplogs gelf json-file local logentries splunk syslog\r\n Swarm: active\r\n  NodeID: xtl2iowds1kdki7tlb2ju08sa\r\n  Is Manager: true\r\n  ClusterID: t562c4arasjgtunv7xfo0lnu5\r\n  Managers: 1\r\n  Nodes: 6\r\n  Default Address Pool: 10.0.0.0\/8\r\n  SubnetSize: 24\r\n  Data Path Port: 4789\r\n  Orchestration:\r\n   Task History Retention Limit: 5\r\n  Raft:\r\n   Snapshot Interval: 10000\r\n   Number of Old Snapshots to Retain: 0\r\n   Heartbeat Tick: 1\r\n   Election Tick: 10\r\n  Dispatcher:\r\n   Heartbeat Period: 5 seconds\r\n  CA Configuration:\r\n   Expiry Duration: 3 months\r\n   Force Rotate: 0\r\n  Autolock Managers: false\r\n  Root Rotation In Progress: false\r\n  Node Address: 192.168.0.151\r\n  Manager Addresses:\r\n   192.168.0.151:2377\r\n Default Isolation: process\r\n Kernel Version: 10.0 20348 (20348.1.amd64fre.fe_release.210507-1500)\r\n Operating System: Microsoft Windows Server Version 21H2 (OS Build 20348.1850)\r\n OSType: windows\r\n Architecture: x86_64\r\n CPUs: 4\r\n Total Memory: 3.418GiB\r\n Name: node-a\r\n ID: 8eb9ad1d-3c88-459b-a012-bfdd2aca0d78\r\n Docker Root Dir: C:\\ProgramData\\docker\r\n Debug Mode: false\r\n Experimental: false\r\n Insecure Registries:\r\n  our-repo.example\r\n  127.0.0.0\/8\r\n Live Restore Enabled: false\r\n Product License: Community Engine\r\n```\r\n\r\n\r\n### Additional Info\r\n\r\n_No response_","comments":[],"labels":["platform\/windows","status\/0-triage","status\/more-info-needed","kind\/bug","area\/networking","area\/networking\/dns"]}]
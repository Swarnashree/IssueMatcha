[{"title":"Deprecate & remove Kubelet RunOnce mode","body":"This was raised back in 2017 (https:\/\/github.com\/kubernetes\/kubernetes\/issues\/47184), but I think it's time to revisit it.\r\n\r\nI'm not sure if anyone is using it, but runonce mode doesn't support many newer pod features (init containers), and the pod lifecycle for runonce mode is even less well defined than normal. The mode appears to be entirely untested in CI, except for a few unit tests.\r\n\r\nFurthermore, I believe podman addresses the same use case in a more well-supported way: https:\/\/docs.podman.io\/en\/latest\/markdown\/podman-kube.1.html\r\n\r\nI propose we mark runonce mode as deprecated in k8s v1.31, slated for removal in v1.33\r\n\r\n\/sig node\r\n\/milestone v1.31\r\n\/kind deprecation","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","@haircommander do you think it makes sense to recommend users migrate to podman for this use case? Are there any notable differences?","Do we need a KEP for this?"],"labels":["sig\/node","kind\/deprecation","needs-triage"]},{"title":"[WIP] [POC] Watchcache readiness","body":"xref https:\/\/github.com\/kubernetes\/enhancements\/pull\/4557\r\n\r\n\/sig api-machinery\r\n\/kind feature\r\n\/priority important-longerm\r\n\r\n```release-note\r\nNONE\r\n```","comments":["@wojtek-t: The label(s) `priority\/important-longerm` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124029):\n\n>xref https:\/\/github.com\/kubernetes\/enhancements\/pull\/4557\r\n>\r\n>\/sig api-machinery\r\n>\/kind feature\r\n>\/priority important-longerm\r\n>\r\n>```release-note\r\n>NONE\r\n>```\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124029#\" title=\"Author self-approved\">wojtek-t<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)~~ [wojtek-t]\n- ~~[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)~~ [wojtek-t]\n- ~~[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)~~ [wojtek-t]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","@wojtek-t: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-integration | 82fceac4e6fb869bbb311dba38e4ebba31b75090 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124029\/pull-kubernetes-integration\/1771148159418896384) | true | `\/test pull-kubernetes-integration`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=124029). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Awojtek-t). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/apiserver","sig\/api-machinery","size\/L","kind\/feature","release-note-none","approved","cncf-cla: yes","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"e2e\/storage: speed up kubectl commands","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nSpeed up stopping by not waiting for Node not ready, `systemctl` will ensure kubelet process stopped before return. This should save 40s per case.\r\n\r\nSince stop command does not wait for not ready, start command needs to wait for the next heartbeat to ensure we are checking status from new process.\r\n\r\nimplement restart by stop then start, to get heartbeat time when kubelet is down. And we do not need to sleep 30s now. The sleep is moved to callers, since they still need them to ensure the volume does not disappear.\r\n\r\nDropped support for non-systemd system.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\n\r\n#### Special notes for your reviewer:\r\n\r\nI think the non-systemd setup never worked before. Do a non-systemd system have `Main PID` in its output?\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @huww98. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124028#\" title=\"Author self-approved\">huww98<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [pohly](https:\/\/github.com\/pohly) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)**\n- **[test\/e2e\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"pohly\"]} -->"],"labels":["area\/test","kind\/cleanup","sig\/storage","size\/L","release-note-none","cncf-cla: yes","sig\/testing","needs-ok-to-test","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"etcd: update to v3.5.13","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind cleanup\r\n#### What this PR does \/ why we need it:\r\n\r\n\/hold\r\nwaiting for:\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124026\r\nTODO promite image\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\netcd: Update to v3.5.13\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @liangyuanpeng. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124027#\" title=\"Author self-approved\">liangyuanpeng<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dims](https:\/\/github.com\/dims) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[build\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/OWNERS)**\n- **[cluster\/gce\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/OWNERS)**\n- **[cmd\/kubeadm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubeadm\/OWNERS)**\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[test\/utils\/image\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/utils\/image\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dims\"]} -->"],"labels":["area\/test","kind\/cleanup","area\/provider\/gcp","sig\/api-machinery","sig\/cluster-lifecycle","release-note","size\/M","area\/kubeadm","cncf-cla: yes","sig\/testing","needs-ok-to-test","do-not-merge\/hold","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"etcd: build etcd image of v3.5.13","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nHave two major fix for watch:\r\n- https:\/\/github.com\/etcd-io\/etcd\/pull\/17566\r\n- https:\/\/github.com\/etcd-io\/etcd\/pull\/17612\r\n\r\nfull detail: https:\/\/github.com\/etcd-io\/etcd\/compare\/v3.5.12...release-3.5\r\nTODO update ^^^ to https:\/\/github.com\/etcd-io\/etcd\/compare\/v3.5.12...v3.5.13\r\n\r\n\/hold\r\nWaiting for:\r\n- https:\/\/github.com\/etcd-io\/etcd\/issues\/17633\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nBuild etcd image v3.5.13\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nnone\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @liangyuanpeng. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124026#\" title=\"Author self-approved\">liangyuanpeng<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dims](https:\/\/github.com\/dims) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[build\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/OWNERS)**\n- **[cluster\/images\/etcd\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/images\/etcd\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dims\"]} -->"],"labels":["kind\/cleanup","sig\/api-machinery","release-note","size\/S","area\/release-eng","cncf-cla: yes","needs-ok-to-test","do-not-merge\/hold","needs-priority","needs-triage"]},{"title":"Continue streaming kubelet logs when runtime is unavailable","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\nContainer runtimes are able to run existing containers even when their main CRI server is not available for any reason. The call to the container status RPC happens quite frequently during log parsing, means that a single CRI interruption will also abort streaming the logs.\r\n\r\nWe now check that specific use case and continue following the log streaming if the CRI is unavailable. We still abort the streaming accordingly if the CRI comes back and the container status reports that the workload has exited.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nRefers to https:\/\/github.com\/cri-o\/cri-o\/issues\/7826\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nContinue streaming kubelet logs when the CRI server of the runtime is unavailable.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","PTAL @kubernetes\/sig-node-pr-reviews ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124025#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mrunalp](https:\/\/github.com\/mrunalp) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mrunalp\"]} -->","Is this patch just related to kubelet logs?","> Is this patch just related to kubelet logs?\r\n\r\nI researched this. The code that @saschagrunert added is only used in https:\/\/github.com\/saschagrunert\/kubernetes\/blob\/7ea3d0245a63fbbba698f1cb939831fe8143db3e\/pkg\/kubelet\/kuberuntime\/logs\/logs.go#L442\r\n\r\nSo this change is actually only limited to logs.\r\n\r\n"],"labels":["area\/kubelet","sig\/node","release-note","size\/XS","kind\/feature","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"DRA: E2E: add test case for structured parameters + deallocation","body":"### What would you like to be added?\n\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/95a6f2e4dcc2801612933707b05d31609744ada7\/test\/e2e\/dra\/dra.go#L666 covers the [PostFilter](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/9c50b2503b7190dbbf7e745103bc58574b3da590\/pkg\/scheduler\/framework\/plugins\/dynamicresources\/dynamicresources.go#L1198) code path in the scheduler.\r\n\r\nWe need a corresponding E2E test for structured parameters. How to trigger it in a realistic way is a bit open. It probably can occur in combination with some other PreBind plugin or a write error during PreBind (both not easy to simulate). Perhaps we simply need to set up a claim in the right starting state.\n\n### Why is this needed?\n\nUnit tests alone are not sufficient.","comments":["\/sig node\r\n\/triage accepted\r\n\/priority backlog"],"labels":["priority\/backlog","sig\/node","kind\/feature","triage\/accepted"]},{"title":"Promote some InTreePluginXXXUnregister to GA","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\/sig storage\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThe following plugins in the `Special notes for your reviewer` section already completed the migration task and its in-tree code was removed as part of the migration process. So, the corresponding feature gate should be removed, but it has not been removed yet.\r\n\r\nWhatever the value of the feature gate is, it won't affect the code's behavior. Even if the feature gate is false, we will still get the right value from csinode's annotations. The PR updates the value to true and promotes them to GA. And those feature gates will be removed in v1.32.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\nKEPs:\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1487\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1490\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1885\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1488\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1489\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1491\r\n\r\n\/cc @leakingtapan @andyzhangx  @davidz627 @adisky @dims @divyenpatel @xing-yang\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nPromote the following feature-gates to GA:\r\n- InTreePluginAWSUnregister\r\n- InTreePluginAzureDiskUnregister\r\n- InTreePluginAzureFileUnregister\r\n- InTreePluginGCEUnregister\r\n- InTreePluginOpenStackUnregister\r\n- InTreePluginvSphereUnregister\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["@carlory: GitHub didn't allow me to request PR reviews from the following users: davidz627.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) and repo collaborators can review this PR, and authors cannot review their own PRs.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124023):\n\n><!--  Thanks for sending a pull request!  Here are some tips for you:\r\n>\r\n>1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n>2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\n>https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n>3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n>4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n>5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n>-->\r\n>\r\n>#### What type of PR is this?\r\n>\r\n>\/kind cleanup\r\n>\/sig storage\r\n>\r\n>#### What this PR does \/ why we need it:\r\n>\r\n>The following plugins in the `Special notes for your reviewer` section already completed the migration task and its in-tree code was removed as part of the migration process. So, the corresponding feature-gate should be removed as well, but it is not removed yet.\r\n>\r\n>Whatever the value of the feature-gate is, it won't have any effect on the behavior of the code. Even if the feature-gate is set to false, we will still get the right value from csinode's annotations. The PR is to promote their to GA. And those feature-gates will be removed in v1.32.\r\n>\r\n>#### Which issue(s) this PR fixes:\r\n><!--\r\n>*Automatically closes linked issue when PR is merged.\r\n>Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n>_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n>-->\r\n>Fixes #\r\n>\r\n>#### Special notes for your reviewer:\r\n>\r\n>KEPs:\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1487\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1490\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1885\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1488\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1489\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/1491\r\n>\r\n>\/cc @leakingtapan @andyzhangx  @davidz627 @adisky @dims @divyenpatel @xing-yang\r\n>\r\n>#### Does this PR introduce a user-facing change?\r\n><!--\r\n>If no, just write \"NONE\" in the release-note block below.\r\n>If yes, a release note is required:\r\n>Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n>\r\n>For more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n>-->\r\n>```release-note\r\n>Promote the following feature-gates to GA:\r\n>- InTreePluginAWSUnregister\r\n>- InTreePluginAzureDiskUnregister\r\n>- InTreePluginAzureFileUnregister\r\n>- InTreePluginGCEUnregister\r\n>- InTreePluginOpenStackUnregister\r\n>- InTreePluginvSphereUnregister\r\n>```\r\n>\r\n>#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n>\r\n><!--\r\n>This section can be blank if this pull request does not require a release note.\r\n>\r\n>When adding links which point to resources within git repositories, like\r\n>KEPs or supporting documentation, please reference a specific commit and avoid\r\n>linking directly to the master branch. This ensures that links reference a\r\n>specific point in time, rather than a document that may change over time.\r\n>\r\n>See here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n>\r\n>Please use the following format for linking documentation:\r\n>- [KEP]: <link>\r\n>- [Usage]: <link>\r\n>- [Other doc]: <link>\r\n>-->\r\n>```docs\r\n>\r\n>```\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124023#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"thockin\"]} -->","@carlory: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-gce-cos-alpha-features | abc3f23964687780bbfb0b5c38593bf2dc2f095e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124023\/pull-kubernetes-e2e-gce-cos-alpha-features\/1771082185344815104) | false | `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\npull-kubernetes-unit | abc3f23964687780bbfb0b5c38593bf2dc2f095e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124023\/pull-kubernetes-unit\/1771082185835548672) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=124023). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Acarlory). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["kind\/cleanup","sig\/storage","release-note","size\/L","sig\/apps","cncf-cla: yes","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"apiserver time skew can lead serviceaccount token in pod keep in invalid state","body":"### What happened?\n\n1. The system time in control-plane node was skew to future somehow.\r\n2. Alerts fired and SRE team address the time manually.\r\n3. There are a lots of pods keep crashing because of invalid token:\r\n  a. Tokens are not valid yet, because they are issued at 'future' time.\r\n  b. Kubelet will not routate those tokens becase they are not expired.\r\n\r\nThe only way to fix this condition is to delete the pod manually, or do nothing and wait until the token be valid.\n\n### What did you expect to happen?\n\nkubelet can fix this condition automatically if the token is issued from the future.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. create a new kind cluster;\r\n1. docker exec into controlplane node, use `date -s` to change the time to 2 days later.\r\n2. wait a few minutes;\r\n3. change the time back;\r\n4. you can find a lots of pods crash because token are not valid yet.\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/4017672\/5778391a-f1ab-4772-bd71-54a9671f5835)\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n reficul@ReficuldeMacBook-Pro \ue0b0 ~ \ue0b0 kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.2\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nnone\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\nDarwin ReficuldeMacBook-Pro.local 23.0.0 Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:34 PDT 2023; root:xnu-10002.1.13~1\/RELEASE_ARM64_T8103 arm64\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n reficul@ReficuldeMacBook-Pro \ue0b0 ~ \ue0b0 kind version\r\nkind v0.22.0 go1.21.7 darwin\/arm64\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nroot@new-control-plane:\/# crictl version\r\nVersion:  0.1.0\r\nRuntimeName:  containerd\r\nRuntimeVersion:  v1.7.13\r\nRuntimeApiVersion:  v1\r\n\r\nroot@new-control-plane:\/# crictl info\r\n{\r\n  \"status\": {\r\n    \"conditions\": [\r\n      {\r\n        \"type\": \"RuntimeReady\",\r\n        \"status\": true,\r\n        \"reason\": \"\",\r\n        \"message\": \"\"\r\n      },\r\n      {\r\n        \"type\": \"NetworkReady\",\r\n        \"status\": true,\r\n        \"reason\": \"\",\r\n        \"message\": \"\"\r\n      }\r\n    ]\r\n  },\r\n  \"cniconfig\": {\r\n    \"PluginDirs\": [\r\n      \"\/opt\/cni\/bin\"\r\n    ],\r\n    \"PluginConfDir\": \"\/etc\/cni\/net.d\",\r\n    \"PluginMaxConfNum\": 1,\r\n    \"Prefix\": \"eth\",\r\n    \"Networks\": [\r\n      {\r\n        \"Config\": {\r\n          \"Name\": \"cni-loopback\",\r\n          \"CNIVersion\": \"0.3.1\",\r\n          \"Plugins\": [\r\n            {\r\n              \"Network\": {\r\n                \"type\": \"loopback\",\r\n                \"ipam\": {},\r\n                \"dns\": {}\r\n              },\r\n              \"Source\": \"{\\\"type\\\":\\\"loopback\\\"}\"\r\n            }\r\n          ],\r\n          \"Source\": \"{\\n\\\"cniVersion\\\": \\\"0.3.1\\\",\\n\\\"name\\\": \\\"cni-loopback\\\",\\n\\\"plugins\\\": [{\\n  \\\"type\\\": \\\"loopback\\\"\\n}]\\n}\"\r\n        },\r\n        \"IFName\": \"lo\"\r\n      },\r\n      {\r\n        \"Config\": {\r\n          \"Name\": \"kindnet\",\r\n          \"CNIVersion\": \"0.3.1\",\r\n          \"Plugins\": [\r\n            {\r\n              \"Network\": {\r\n                \"type\": \"ptp\",\r\n                \"ipam\": {\r\n                  \"type\": \"host-local\"\r\n                },\r\n                \"dns\": {}\r\n              },\r\n              \"Source\": \"{\\\"ipMasq\\\":false,\\\"ipam\\\":{\\\"dataDir\\\":\\\"\/run\/cni-ipam-state\\\",\\\"ranges\\\":[[{\\\"subnet\\\":\\\"10.244.0.0\/24\\\"}]],\\\"routes\\\":[{\\\"dst\\\":\\\"0.0.0.0\/0\\\"}],\\\"type\\\":\\\"host-local\\\"},\\\"mtu\\\":1500,\\\"type\\\":\\\"ptp\\\"}\"\r\n            },\r\n            {\r\n              \"Network\": {\r\n                \"type\": \"portmap\",\r\n                \"capabilities\": {\r\n                  \"portMappings\": true\r\n                },\r\n                \"ipam\": {},\r\n                \"dns\": {}\r\n              },\r\n              \"Source\": \"{\\\"capabilities\\\":{\\\"portMappings\\\":true},\\\"type\\\":\\\"portmap\\\"}\"\r\n            }\r\n          ],\r\n          \"Source\": \"\\n{\\n\\t\\\"cniVersion\\\": \\\"0.3.1\\\",\\n\\t\\\"name\\\": \\\"kindnet\\\",\\n\\t\\\"plugins\\\": [\\n\\t{\\n\\t\\t\\\"type\\\": \\\"ptp\\\",\\n\\t\\t\\\"ipMasq\\\": false,\\n\\t\\t\\\"ipam\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"host-local\\\",\\n\\t\\t\\t\\\"dataDir\\\": \\\"\/run\/cni-ipam-state\\\",\\n\\t\\t\\t\\\"routes\\\": [\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t{ \\\"dst\\\": \\\"0.0.0.0\/0\\\" }\\n\\t\\t\\t],\\n\\t\\t\\t\\\"ranges\\\": [\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t[ { \\\"subnet\\\": \\\"10.244.0.0\/24\\\" } ]\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t,\\n\\t\\t\\\"mtu\\\": 1500\\n\\t\\t\\n\\t},\\n\\t{\\n\\t\\t\\\"type\\\": \\\"portmap\\\",\\n\\t\\t\\\"capabilities\\\": {\\n\\t\\t\\t\\\"portMappings\\\": true\\n\\t\\t}\\n\\t}\\n\\t]\\n}\\n\"\r\n        },\r\n        \"IFName\": \"eth0\"\r\n      }\r\n    ]\r\n  },\r\n  \"config\": {\r\n    \"containerd\": {\r\n      \"snapshotter\": \"overlayfs\",\r\n      \"defaultRuntimeName\": \"runc\",\r\n      \"defaultRuntime\": {\r\n        \"runtimeType\": \"\",\r\n        \"runtimePath\": \"\",\r\n        \"runtimeEngine\": \"\",\r\n        \"PodAnnotations\": null,\r\n        \"ContainerAnnotations\": null,\r\n        \"runtimeRoot\": \"\",\r\n        \"options\": null,\r\n        \"privileged_without_host_devices\": false,\r\n        \"privileged_without_host_devices_all_devices_allowed\": false,\r\n        \"baseRuntimeSpec\": \"\",\r\n        \"cniConfDir\": \"\",\r\n        \"cniMaxConfNum\": 0,\r\n        \"snapshotter\": \"\",\r\n        \"sandboxMode\": \"\"\r\n      },\r\n      \"untrustedWorkloadRuntime\": {\r\n        \"runtimeType\": \"\",\r\n        \"runtimePath\": \"\",\r\n        \"runtimeEngine\": \"\",\r\n        \"PodAnnotations\": null,\r\n        \"ContainerAnnotations\": null,\r\n        \"runtimeRoot\": \"\",\r\n        \"options\": null,\r\n        \"privileged_without_host_devices\": false,\r\n        \"privileged_without_host_devices_all_devices_allowed\": false,\r\n        \"baseRuntimeSpec\": \"\",\r\n        \"cniConfDir\": \"\",\r\n        \"cniMaxConfNum\": 0,\r\n        \"snapshotter\": \"\",\r\n        \"sandboxMode\": \"\"\r\n      },\r\n      \"runtimes\": {\r\n        \"runc\": {\r\n          \"runtimeType\": \"io.containerd.runc.v2\",\r\n          \"runtimePath\": \"\",\r\n          \"runtimeEngine\": \"\",\r\n          \"PodAnnotations\": null,\r\n          \"ContainerAnnotations\": null,\r\n          \"runtimeRoot\": \"\",\r\n          \"options\": {\r\n            \"SystemdCgroup\": true\r\n          },\r\n          \"privileged_without_host_devices\": false,\r\n          \"privileged_without_host_devices_all_devices_allowed\": false,\r\n          \"baseRuntimeSpec\": \"\/etc\/containerd\/cri-base.json\",\r\n          \"cniConfDir\": \"\",\r\n          \"cniMaxConfNum\": 0,\r\n          \"snapshotter\": \"\",\r\n          \"sandboxMode\": \"podsandbox\"\r\n        },\r\n        \"test-handler\": {\r\n          \"runtimeType\": \"io.containerd.runc.v2\",\r\n          \"runtimePath\": \"\",\r\n          \"runtimeEngine\": \"\",\r\n          \"PodAnnotations\": null,\r\n          \"ContainerAnnotations\": null,\r\n          \"runtimeRoot\": \"\",\r\n          \"options\": {\r\n            \"SystemdCgroup\": true\r\n          },\r\n          \"privileged_without_host_devices\": false,\r\n          \"privileged_without_host_devices_all_devices_allowed\": false,\r\n          \"baseRuntimeSpec\": \"\/etc\/containerd\/cri-base.json\",\r\n          \"cniConfDir\": \"\",\r\n          \"cniMaxConfNum\": 0,\r\n          \"snapshotter\": \"\",\r\n          \"sandboxMode\": \"podsandbox\"\r\n        }\r\n      },\r\n      \"noPivot\": false,\r\n      \"disableSnapshotAnnotations\": true,\r\n      \"discardUnpackedLayers\": true,\r\n      \"ignoreBlockIONotEnabledErrors\": false,\r\n      \"ignoreRdtNotEnabledErrors\": false\r\n    },\r\n    \"cni\": {\r\n      \"binDir\": \"\/opt\/cni\/bin\",\r\n      \"confDir\": \"\/etc\/cni\/net.d\",\r\n      \"maxConfNum\": 1,\r\n      \"setupSerially\": false,\r\n      \"confTemplate\": \"\",\r\n      \"ipPref\": \"\"\r\n    },\r\n    \"registry\": {\r\n      \"configPath\": \"\",\r\n      \"mirrors\": null,\r\n      \"configs\": null,\r\n      \"auths\": null,\r\n      \"headers\": null\r\n    },\r\n    \"imageDecryption\": {\r\n      \"keyModel\": \"node\"\r\n    },\r\n    \"disableTCPService\": true,\r\n    \"streamServerAddress\": \"127.0.0.1\",\r\n    \"streamServerPort\": \"0\",\r\n    \"streamIdleTimeout\": \"4h0m0s\",\r\n    \"enableSelinux\": false,\r\n    \"selinuxCategoryRange\": 1024,\r\n    \"sandboxImage\": \"registry.k8s.io\/pause:3.7\",\r\n    \"statsCollectPeriod\": 10,\r\n    \"systemdCgroup\": false,\r\n    \"enableTLSStreaming\": false,\r\n    \"x509KeyPairStreaming\": {\r\n      \"tlsCertFile\": \"\",\r\n      \"tlsKeyFile\": \"\"\r\n    },\r\n    \"maxContainerLogSize\": 16384,\r\n    \"disableCgroup\": false,\r\n    \"disableApparmor\": false,\r\n    \"restrictOOMScoreAdj\": false,\r\n    \"maxConcurrentDownloads\": 3,\r\n    \"disableProcMount\": false,\r\n    \"unsetSeccompProfile\": \"\",\r\n    \"tolerateMissingHugetlbController\": true,\r\n    \"disableHugetlbController\": true,\r\n    \"device_ownership_from_security_context\": false,\r\n    \"ignoreImageDefinedVolumes\": false,\r\n    \"netnsMountsUnderStateDir\": false,\r\n    \"enableUnprivilegedPorts\": false,\r\n    \"enableUnprivilegedICMP\": false,\r\n    \"enableCDI\": false,\r\n    \"cdiSpecDirs\": [\r\n      \"\/etc\/cdi\",\r\n      \"\/var\/run\/cdi\"\r\n    ],\r\n    \"imagePullProgressTimeout\": \"5m0s\",\r\n    \"drainExecSyncIOTimeout\": \"0s\",\r\n    \"containerdRootDir\": \"\/var\/lib\/containerd\",\r\n    \"containerdEndpoint\": \"\/run\/containerd\/containerd.sock\",\r\n    \"rootDir\": \"\/var\/lib\/containerd\/io.containerd.grpc.v1.cri\",\r\n    \"stateDir\": \"\/run\/containerd\/io.containerd.grpc.v1.cri\"\r\n  },\r\n  \"golang\": \"go1.20.13\",\r\n  \"lastCNILoadStatus\": \"OK\",\r\n  \"lastCNILoadStatus.default\": \"OK\"\r\n}\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig nodes\r\n\r\nWDYT this issue a valid bug? @pacoxu ","@xuzhenglun: The label(s) `sig\/nodes` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/124022#issuecomment-2014301734):\n\n>\/sig nodes\r\n>\r\n>WDYT this issue a valid bug? @pacoxu \n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node"],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"kube-apiserver with --authorization-mode=AlwaysAllow leads to continous restart of kube-apiserver","body":"### What happened?\n\nWhen I configure kube-apiserver with --authorization-mode=AlwaysAllow in a solely test environment to become familiar with this authorization method, every 3-5 minutes kube-apiserver restarts. There is nothing special in the kube-apiserver log file. But according the kubelet there are log entries shown below:\r\n\r\nNearly at the beginning of the provided logfile kubelet complains with \r\n**\"... Failed to make webhook authenticator request ...\"**\r\n\r\nIs there something I have to configure in kubelet too, to get a stable kube-apiserver running with the mentioned --authorization-mode. I believe kubelet is not able to connect to the kube-apiserver and restarts the kube-apiserver (see one of the last lines in the log file)\r\n\r\n```\r\n#> journalctl -f -u kubelet\r\n...\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.468255  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?resourceVersion=0&timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.468646  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.469004  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.469290  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.469572  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.469596  238260 kubelet_node_status.go:531] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.764019  238260 controller.go:195] \"Failed to update lease\" err=\"Put \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.764407  238260 controller.go:195] \"Failed to update lease\" err=\"Put \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.764808  238260 controller.go:195] \"Failed to update lease\" err=\"Put \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.765141  238260 controller.go:195] \"Failed to update lease\" err=\"Put \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.765456  238260 controller.go:195] \"Failed to update lease\" err=\"Put \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: I0321 16:40:06.765486  238260 controller.go:115] \"failed to update lease using latest lease, fallback to ensure lease\" err=\"failed 5 attempts to update lease\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.765774  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"200ms\"\r\nMar 21 16:40:06 k8sm201 kubelet[238260]: E0321 16:40:06.967100  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"400ms\"\r\nMar 21 16:40:07 k8sm201 kubelet[238260]: E0321 16:40:07.368943  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"800ms\"\r\nMar 21 16:40:08 k8sm201 kubelet[238260]: E0321 16:40:08.170052  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"1.6s\"\r\nMar 21 16:40:08 k8sm201 kubelet[238260]: E0321 16:40:08.338287  238260 webhook.go:154] Failed to make webhook authenticator request: Post \"https:\/\/192.168.X.YYY:6443\/apis\/authentication.k8s.io\/v1\/tokenreviews\": dial tcp 192.168.X.YYY:6443: connect: connection refused\r\nMar 21 16:40:08 k8sm201 kubelet[238260]: E0321 16:40:08.338908  238260 server.go:310] \"Unable to authenticate the request due to an error\" err=\"Post \\\"https:\/\/192.168.X.YYY:6443\/apis\/authentication.k8s.io\/v1\/tokenreviews\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:09 k8sm201 kubelet[238260]: E0321 16:40:09.773594  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"3.2s\"\r\nMar 21 16:40:12 k8sm201 kubelet[238260]: E0321 16:40:12.975411  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"6.4s\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.575260  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.795231  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?resourceVersion=0&timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.797352  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.798147  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.798944  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.799674  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:16 k8sm201 kubelet[238260]: E0321 16:40:16.799930  238260 kubelet_node_status.go:531] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.210044  238260 scope.go:117] \"RemoveContainer\" containerID=\"4cb85360e4d37566d3f5f2e95003f1dc8fd7133721380dfe53b212edc5f810d2\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.210984  238260 scope.go:117] \"RemoveContainer\" containerID=\"182b8866bce4b99a5c97d68c84d3320a5340841136d6f5c1e8043fa9aae03e36\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: E0321 16:40:18.211320  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.211856  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: E0321 16:40:18.212259  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-scheduler\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-k8sm201_kube-system(cddc27a161818172b68ad74b0a07a53a)\\\"\" pod=\"kube-system\/kube-scheduler-k8sm201\" podUID=\"cddc27a161818172b68ad74b0a07a53a\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.216452  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.216803  238260 scope.go:117] \"RemoveContainer\" containerID=\"baa998504b5bb0e22eedeb8e87a73fffc0880a42e8eac5cb5f510be6f39c2c3e\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.217193  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\" pod=\"kube-system\/kube-controller-manager-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-controller-manager-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: E0321 16:40:18.217211  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: E0321 16:40:18.218219  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8sm201_kube-system(782b86b2f576a6884e76e5b0aa14c85f)\\\"\" pod=\"kube-system\/kube-controller-manager-k8sm201\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\"\r\nMar 21 16:40:18 k8sm201 kubelet[238260]: I0321 16:40:18.263702  238260 scope.go:117] \"RemoveContainer\" containerID=\"c85c19ded21c3e9054ab36b4f738758ea774edf1aedad20f49343e62c526ca50\"\r\nMar 21 16:40:19 k8sm201 kubelet[238260]: E0321 16:40:19.377014  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"7s\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: I0321 16:40:20.124919  238260 scope.go:117] \"RemoveContainer\" containerID=\"182b8866bce4b99a5c97d68c84d3320a5340841136d6f5c1e8043fa9aae03e36\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: E0321 16:40:20.125577  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: E0321 16:40:20.126316  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-scheduler\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-k8sm201_kube-system(cddc27a161818172b68ad74b0a07a53a)\\\"\" pod=\"kube-system\/kube-scheduler-k8sm201\" podUID=\"cddc27a161818172b68ad74b0a07a53a\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: I0321 16:40:20.126370  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: I0321 16:40:20.127413  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\" pod=\"kube-system\/kube-controller-manager-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-controller-manager-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: I0321 16:40:20.227995  238260 scope.go:117] \"RemoveContainer\" containerID=\"182b8866bce4b99a5c97d68c84d3320a5340841136d6f5c1e8043fa9aae03e36\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: E0321 16:40:20.228245  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:20 k8sm201 kubelet[238260]: E0321 16:40:20.229368  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-scheduler\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-k8sm201_kube-system(cddc27a161818172b68ad74b0a07a53a)\\\"\" pod=\"kube-system\/kube-scheduler-k8sm201\" podUID=\"cddc27a161818172b68ad74b0a07a53a\"\r\nMar 21 16:40:21 k8sm201 kubelet[238260]: I0321 16:40:21.575964  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:21 k8sm201 kubelet[238260]: I0321 16:40:21.576582  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\" pod=\"kube-system\/kube-controller-manager-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-controller-manager-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:21 k8sm201 kubelet[238260]: E0321 16:40:21.579004  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:22 k8sm201 kubelet[238260]: E0321 16:40:22.577020  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:23 k8sm201 kubelet[238260]: E0321 16:40:23.328387  238260 webhook.go:154] Failed to make webhook authenticator request: Post \"https:\/\/192.168.X.YYY:6443\/apis\/authentication.k8s.io\/v1\/tokenreviews\": dial tcp 192.168.X.YYY:6443: connect: connection refused\r\nMar 21 16:40:23 k8sm201 kubelet[238260]: E0321 16:40:23.329299  238260 server.go:310] \"Unable to authenticate the request due to an error\" err=\"Post \\\"https:\/\/192.168.X.YYY:6443\/apis\/authentication.k8s.io\/v1\/tokenreviews\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:24 k8sm201 kubelet[238260]: I0321 16:40:24.876156  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:24 k8sm201 kubelet[238260]: I0321 16:40:24.878539  238260 scope.go:117] \"RemoveContainer\" containerID=\"baa998504b5bb0e22eedeb8e87a73fffc0880a42e8eac5cb5f510be6f39c2c3e\"\r\nMar 21 16:40:24 k8sm201 kubelet[238260]: I0321 16:40:24.878978  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\" pod=\"kube-system\/kube-controller-manager-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-controller-manager-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:24 k8sm201 kubelet[238260]: E0321 16:40:24.879469  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:24 k8sm201 kubelet[238260]: E0321 16:40:24.882127  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-controller-manager\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8sm201_kube-system(782b86b2f576a6884e76e5b0aa14c85f)\\\"\" pod=\"kube-system\/kube-controller-manager-k8sm201\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.378428  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"7s\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.859533  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?resourceVersion=0&timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.861421  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.861735  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.862028  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.862294  238260 kubelet_node_status.go:544] \"Error updating node status, will retry\" err=\"error getting node \\\"k8sm201\\\": Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/nodes\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:26 k8sm201 kubelet[238260]: E0321 16:40:26.862316  238260 kubelet_node_status.go:531] \"Unable to update node status\" err=\"update node status exceeds retry count\"\r\nMar 21 16:40:29 k8sm201 kubelet[238260]: E0321 16:40:29.578444  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:31 k8sm201 kubelet[238260]: I0321 16:40:31.575157  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"cddc27a161818172b68ad74b0a07a53a\" pod=\"kube-system\/kube-scheduler-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-scheduler-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:31 k8sm201 kubelet[238260]: I0321 16:40:31.577234  238260 status_manager.go:853] \"Failed to get status for pod\" podUID=\"782b86b2f576a6884e76e5b0aa14c85f\" pod=\"kube-system\/kube-controller-manager-k8sm201\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/api\/v1\/namespaces\/kube-system\/pods\/kube-controller-manager-k8sm201\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\"\r\nMar 21 16:40:31 k8sm201 kubelet[238260]: I0321 16:40:31.578785  238260 scope.go:117] \"RemoveContainer\" containerID=\"182b8866bce4b99a5c97d68c84d3320a5340841136d6f5c1e8043fa9aae03e36\"\r\nMar 21 16:40:31 k8sm201 kubelet[238260]: E0321 16:40:31.579514  238260 dns.go:153] \"Nameserver limits exceeded\" err=\"Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 192.168.1.154 192.168.1.1 192.168.1.154\"\r\nMar 21 16:40:31 k8sm201 kubelet[238260]: E0321 16:40:31.581012  238260 pod_workers.go:1298] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-scheduler\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-k8sm201_kube-system(cddc27a161818172b68ad74b0a07a53a)\\\"\" pod=\"kube-system\/kube-scheduler-k8sm201\" podUID=\"cddc27a161818172b68ad74b0a07a53a\"\r\nMar 21 16:40:33 k8sm201 kubelet[238260]: E0321 16:40:33.381343  238260 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https:\/\/192.168.X.YYY:6443\/apis\/coordination.k8s.io\/v1\/namespaces\/kube-node-lease\/leases\/k8sm201?timeout=10s\\\": dial tcp 192.168.X.YYY:6443: connect: connection refused\" interval=\"7s\r\n```\r\n\r\n\r\nI should mention that there is nothing running in the k8s cluster. It's a fresh installed k8s with kubeadm.\r\n\n\n### What did you expect to happen?\n\nstable kube-apiserver with --authorization-mode=AlwaysAllow\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nIn kube-apiserver yaml file\r\n\r\nchange fom\r\n- --authorization-mode=Node,RBAC\r\nto\r\n- --authorization-mode=Node,AlwaysAllow\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n\r\n### On-prem\r\n\r\n$ kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.2\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n\r\nPRETTY_NAME=\"Ubuntu 22.04.2 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.2 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n$ uname -a\r\nLinux k8sm201 5.15.0-69-generic #76-Ubuntu SMP Fri Mar 17 17:19:29 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig auth"],"labels":["kind\/bug","sig\/auth","needs-triage"]},{"title":"Performance tracking\/improvements across Kubernetes release ","body":"Hi Team,\r\n\r\nI did some search the Kubernetes performance documentation. https:\/\/kubernetes.io\/blog\/2015\/09\/kubernetes-performance-measurements-and\/\r\n\r\nDo we have a tracking of the Kubernetes performance across releases?\r\n","comments":["There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `\/sig <group-name>`\n- `\/wg <group-name>`\n- `\/committee <group-name>`\n\nPlease see the [group list](https:\/\/git.k8s.io\/community\/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["needs-sig","needs-triage"]},{"title":"fix test flake caused by not waiting for CRD schema update","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\/kind failing-test\r\n\/kind flake\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nFixes a flake in some test code. Flake was caused by our check to make sure CRD schema was updated. We would check to see a returned error had special string, but the error message would unconditionally contain the string since we had used within the send value, so we never would actually wait....I'm surprised it did not flake more than it had.\r\n\r\nFIxed by no longer sending the special string in with the object, and instead encoding the special string as a custom CEL error message. This is a better way of checking that the new schema is being updated.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123854\r\n\r\n#### Special notes for your reviewer:\r\n\r\n\/assign @Jefftree \r\n\/cc @pacoxu \r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124019#\" title=\"Author self-approved\">alexzielenski<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","\/triage accepted\r\n\/assign @jpbetz \r\n\/lgtm","LGTM label has been added.  <details>Git tree hash: d0560d45fd95567094cc79060062fc5a795680af<\/details>"],"labels":["kind\/bug","lgtm","sig\/api-machinery","size\/S","kind\/flake","release-note-none","cncf-cla: yes","kind\/failing-test","needs-priority","triage\/accepted"]},{"title":"PodEvict API does not trigger Pod deletion callback when DeleteOptions is set to metav1.NewDeleteOptions(0)","body":"### What happened?\r\n\r\n\u5f53\u6211\u4f7f\u7528client-go\u7ec4\u4ef6\u7684Evict API\u65f6\uff0c\u53d1\u73b0\u5f53\u8bbe\u7f6e\u4e86 DeleteOptions \u4e3a metav1.NewDeleteOptions(0) \u65f6\uff0c\u65e0\u6cd5\u89e6\u53d1mutating webhook\u7684Pod \u5220\u9664\u56de\u8c03, \u5f53\u6211\u4e0d\u8bbe\u7f6e DeleteOptions \u65f6\uff0c\u53ef\u4ee5\u6b63\u5e38\u89e6\u53d1\r\n\r\n\u4ee3\u7801\u5982\u4e0b:\r\n```golang\r\nif err := cli.CoreV1().Pods(\"default\").Evict(context.Background(), &policy.Eviction{\r\n\t\tTypeMeta: metav1.TypeMeta{\r\n\t\t\tKind:       \"Pod\",\r\n\t\t\tAPIVersion: \"v1\",\r\n\t\t},\r\n\t\tObjectMeta: metav1.ObjectMeta{\r\n\t\t\tName:      \"pod-xxx\",\r\n\t\t\tNamespace: \"default\",\r\n\t\t},\r\n\t\tDeleteOptions: metav1.NewDeleteOptions(0),\r\n\t}); err != nil {\r\n\t\tglog.Fatalf(err.Error())\r\n\t}\r\n```\r\n\r\nmutatingwebhookconfiguration:\r\n```yaml\r\napiVersion: admissionregistration.k8s.io\/v1\r\nkind: MutatingWebhookConfiguration\r\nmetadata:\r\n  name: my-webhook\r\nwebhooks:\r\n- admissionReviewVersions:\r\n  - v1beta1\r\n  clientConfig:\r\n    caBundle: xxxxx\r\n    service:\r\n      name: my-webhook-svc\r\n      namespace: kube-system\r\n      path: \/delete\/pod\r\n      port: 443\r\n  failurePolicy: Ignore\r\n  matchPolicy: Exact\r\n  name: delete-pod-webhook.test.org\r\n  reinvocationPolicy: Never\r\n  rules:\r\n  - apiGroups:\r\n    - \"\"\r\n    apiVersions:\r\n    - v1\r\n    operations:\r\n    - DELETE\r\n    resources:\r\n    - pods\r\n    scope: '*'\r\n  sideEffects: Unknown\r\n  timeoutSeconds: 30\r\n```\r\n\r\n### What did you expect to happen?\r\n\r\n\u6211\u671f\u671bEvict API\u5728\u8bbe\u7f6e\u4e86 DeleteOptions \u4e3a metav1.NewDeleteOptions(0) \u65f6\uff0c\u80fd\u591f\u6b63\u5e38\u89e6\u53d1mutatingwebhook\u7684Pod\u5220\u9664\u56de\u8c03\u3002\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. \u4f7f\u7528 k8s.io\/client-go v0.21.2 \u521b\u5efa\u4e00\u4e2a\u5ba2\u6237\u7aef\u3002\r\n2. \u4f7f\u7528 Evict API \u5e76\u8bbe\u7f6e DeleteOptions \u4e3a metav1.NewDeleteOptions(0)\u3002\r\n3. \u89c2\u5bdf\u5230mutatingwebhook\u7684Pod\u5220\u9664\u56de\u8c03\u6ca1\u6709\u88ab\u89e6\u53d1\u3002\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ cluster version\r\n#  v1.20.6\r\n\r\n$ client-go version\r\n#  v0.21.2\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nTencent Kubernetes Engine (TKE)\r\n<\/details>\r\n\r\n### OS version\r\n\r\n_No response_\r\n\r\n### Install tools\r\n\r\n_No response_\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n_No response_\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n_No response_","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","> $ cluster version\r\n> #  v1.20.6\r\n\r\nkubernetes v1.20 is not a support version, and a better place to ask would be on the support channels. please see:\r\nhttps:\/\/git.k8s.io\/kubernetes\/SUPPORT.md\r\n\r\n\/kind support\r\n\r\n","\/sig api-machinery"],"labels":["kind\/bug","kind\/support","sig\/api-machinery","needs-triage"]},{"title":"kube-controller-manager removes deprecated command flags","body":"\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n`--volume-host-cidr-denylist and --volume-host-allow-local-loopback` was deprecated in v1.28. https:\/\/github.com\/kubernetes\/kubernetes\/pull\/118128\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nkube-controller-manager removes deprecated command flags: --volume-host-cidr-denylist and --volume-host-allow-local-loopback\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/cc @danwinship","\/retest","Seems to be gce specific tests that are failing. Does something need to be changed there?\r\n\/triage accepted","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124017#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cheftako](https:\/\/github.com\/cheftako) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cluster\/gce\/gci\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/gci\/OWNERS)**\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cheftako\"]} -->","> Does something need to be changed there?\r\n\r\nFixed in https:\/\/github.com\/kubernetes\/kubernetes\/compare\/51c92cf815ef18ba136d00af80d09306780d0a30..b3b7a1dace199bf258191d9099e94f6e07d08f84\r\n","\/retest"],"labels":["kind\/cleanup","area\/provider\/gcp","sig\/storage","sig\/api-machinery","release-note","size\/S","sig\/apps","cncf-cla: yes","sig\/cloud-provider","needs-priority","triage\/accepted"]},{"title":"node should not be ready when klog flush deamon in kubelet is block in fsync","body":"### What happened?\n\n1. some issues cause a part of  fsync syscalls to persist in blocking\r\n2. klog flush deamon hold the lock, call `flushAll` and block in fsync syscall \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/a309fadbac3339bc8db9ae0a928a33b8e81ef10f\/vendor\/k8s.io\/klog\/v2\/klog.go#L1223-L1228\r\n\r\n4. other goroutines in kubelet will be block when print log\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/a309fadbac3339bc8db9ae0a928a33b8e81ef10f\/vendor\/k8s.io\/klog\/v2\/klog.go#L885-L894\r\n\r\n5. kubelet can renew node lease normally because this goroutine do not print log\r\n6. node controller think this node is ready\n\n### What did you expect to happen?\n\nnode should be not ready\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nmaybe use ptrace to block fsync?\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n1.26.0\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\nnone\r\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node"],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"Visit ephemeral containers when calculating fs user","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\/sig storage\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nbased on https:\/\/github.com\/kubernetes\/kubernetes\/pull\/108800, I add a unit test.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/108799\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124015#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"thockin\"]} -->","\/cc @verb @msau42 @xing-yang "],"labels":["kind\/cleanup","sig\/storage","size\/M","release-note-none","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"Admit failed, intercepting Pod Failed, bug fix.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes # https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123980\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: davshen \/ name: david_shen  (08286fb285156dd8b632430df9d74c1e6016a448)<\/li><\/ul>","Welcome @davshen! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @davshen. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124014#\" title=\"Author self-approved\">davshen<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->"],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/S","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage"]},{"title":"Add histogram exemplar support","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124013#\" title=\"Author self-approved\">richabanker<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https:\/\/github.com\/apelisse), [logicalhan](https:\/\/github.com\/logicalhan) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/handlers\/fieldmanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/handlers\/fieldmanager\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/metrics\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/metrics\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"apelisse\",\"logicalhan\"]} -->"],"labels":["area\/apiserver","sig\/api-machinery","size\/L","cncf-cla: yes","sig\/instrumentation","sig\/architecture","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"[WIP] Coordinated Leader Election","body":"\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nImplementation of corodinated leader election\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nCoordinated Leader Election \r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\r\n\/triage accepted\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124012#\" title=\"Author self-approved\">Jefftree<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n- **[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)**\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[plugin\/pkg\/auth\/authorizer\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/plugin\/pkg\/auth\/authorizer\/OWNERS)**\n- **[staging\/publishing\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/publishing\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"thockin\"]} -->","@Jefftree: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify-lint | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-verify-lint\/1770971418347966464) | true | `\/test pull-kubernetes-verify-lint`\npull-kubernetes-unit | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-unit\/1770971414157856768) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-e2e-gce | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-e2e-gce\/1770971412677267456) | true | `\/test pull-kubernetes-e2e-gce`\npull-kubernetes-integration | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-integration\/1770971412878594048) | true | `\/test pull-kubernetes-integration`\npull-kubernetes-e2e-gce-cos-alpha-features | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-e2e-gce-cos-alpha-features\/1770971412807290880) | false | `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\npull-kubernetes-local-e2e | 73484beb9a6266a564f5fc31ca619f0d8693598f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124012\/pull-kubernetes-local-e2e\/1770971417530077184) | false | `\/test pull-kubernetes-local-e2e`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=124012). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AJefftree). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/apiserver","sig\/api-machinery","release-note","size\/XXL","kind\/api-change","kind\/feature","area\/release-eng","sig\/auth","sig\/apps","cncf-cla: yes","sig\/release","do-not-merge\/work-in-progress","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"skip initContainer NUMA calc if initContainer is admited","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nWhen Kubelet restart, it will recalc Admit for every pod in node, in some case, memory manager will return false even if initContainer is already allocated with resource;\r\nThis PR is to skip InitContainer topology calculate if initContainer is already started.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123971 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @AllenXu93. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124005#\" title=\"Author self-approved\">AllenXu93<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [random-liu](https:\/\/github.com\/random-liu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"random-liu\"]} -->"],"labels":["area\/kubelet","sig\/node","size\/S","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"Fixed flake test due to data race in while accessing dummyStorage in cacher-test","body":"\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind flake\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nAdding locks on dummyStorage to prevent data race. Ref : #123991\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123991\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```","comments":["Hi @srivastav-abhishek. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124004#\" title=\"Author self-approved\">srivastav-abhishek<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [wojtek-t](https:\/\/github.com\/wojtek-t) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"wojtek-t\"]} -->","\/ok-to-test\r\n\/triage accepted"],"labels":["area\/apiserver","sig\/api-machinery","size\/XS","kind\/flake","release-note-none","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"kube-scheduler remove non-csi volumelimit plugins","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nSee https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123970\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nkube-scheduler removed the following plugins: \r\n- AzureDiskLimits \r\n- CinderLimits\r\n- EBSLimits\r\n- GCEPDLimits\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/124003#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/scheduler\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->","\/cc @jsafrane @msau42 ","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","\/test pull-kubernetes-e2e-gce\r\n\/test pull-kubernetes-integration","\/test pull-kubernetes-integration","@carlory: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-integration | 7a01d65563e82c27ac8516ae1e644c5353975f32 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/124003\/pull-kubernetes-integration\/1771181739385819136) | true | `\/test pull-kubernetes-integration`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=124003). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Acarlory). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","kind\/cleanup","sig\/scheduling","sig\/storage","release-note","size\/XXL","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/testing","needs-priority","needs-triage"]},{"title":"Kubernetes Skips Init Containers Beyond First During Some Pod Restarts","body":"### What happened?\r\n\r\nInit containers beyond the first are ignored during certain pod restarts.  In particular, when the pod sandbox suddenly temporarily disappears (possibly due to a node restart) the automatic restart sequence of the pod seems to skip init containers after the first.\r\n\r\n### What did you expect to happen?\r\n\r\nFrom the [Kubernetes docs](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/pods\/init-containers\/#detailed-behavior):\r\n\r\n\"If the Pod [restarts](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/pods\/init-containers\/#pod-restart-reasons), or is restarted, all init containers must execute again.\"\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nBelow `repro.yaml` is a trimmed-down deployment:\r\ntwo init containers and a primary container, all running bash.\r\nPrimary just sleeps, init containers each echo and exit.\r\n\r\nApply the yaml, let it get running, and then, eg, forcibly kill its pod sandbox (variety of methods, depending on CRI, host environment, etc..):\r\n\r\nA specific repro that assumes a linux host, containerd:\r\n\r\n```bash\r\nkubectl create namespace delme\r\nkubectl apply -f repro.yaml\r\nkubectl get pods -n delme  # confirm the deployment started normally\r\n\r\n# simulate a node\/sandbox-level disruption by killing pod's slice (probably a better way to do this..?)\r\n# run next command on **node host**:\r\nsudo systemctl stop --force \\\r\n \"$(ps -o cgroup --pid \"$(pgrep --full 'pgrepid:PkIOeQU4J6zI')\" |\r\n sed -ne '2s\/.*\\(kubepods-besteffort-pod.*\\.slice\\).*\/\\1\/p')\"\r\n\r\nkubectl describe pods -n delme\r\n```\r\n\r\nThe restart count of the second init container is 0, while main container and first init container are 1.  Can repeat the stop, and the restart count of the first init container will increment, staying in sync with the main container and the overall pod restart counter, but the second init container stays at 0.  Start\/finish time stamps in describe, and results of `kubectl logs`, are also consistent with the second init container not being re-run.\r\n\r\nAnd here's `repro.yaml`:\r\n\r\n```yaml\r\napiVersion: apps\/v1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    app: multi-init-container-bug-repro\r\n  name: multi-init-container-bug-repro\r\n  namespace: delme\r\nspec:\r\n  progressDeadlineSeconds: 600\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: multi-init-container-bug-repro\r\n  strategy:\r\n    type: Recreate\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: multi-init-container-bug-repro\r\n    spec:\r\n      automountServiceAccountToken: false\r\n      containers:\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - sleep infinity && echo 'pgrepid:PkIOeQU4J6zI'\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: main\r\n      initContainers:\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - \"echo 'init-container: First!'\"\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: init-first\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - \"echo 'init-container: Second!'\"\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: init-second\r\n      restartPolicy: Always\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n## Practical Impact:\r\n\r\nDeployments that use multiple init containers (which includes some popular ones, eg gitlab via helm) can get stuck during their auto restart after a transient node outage (requiring manual intervention to get unstuck).  In production, this is hopefully a rare (but potentially unpleasant) event.  It is fairly common and annoying in certain kinds of development\/test configurations if eg intentional host node restarts are common.\r\n\r\n## Workarounds:\r\n\r\nDeleting the pod through `kubectl delete pod`, or `kubectl rollout restart`ing its eg deployment, causes init containers to be re-run when the pod is restarted (or more precisely, when the fresh replacement pod is started).\r\n\r\n\r\n## Additional Notes\r\n\r\n- [This issue](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/47599) might be related.\r\n- In the repro below, if instead of killing the whole pod sandbox, just the main container is killed, then `kubectl get pods` again reports a restart, but `describe` now shows *both* init containers are skipped.  This might be intended behavior? It is less-obviously-bad than arbitrarily skipping just some init containers, as happens in the main repro path.. but if the restart count in the output of `kubectl get pods` is to be interpreted as the \"number of pod restarts\", it seems like this path also contradicts the cited docs.\r\n\r\n### Kubernetes version\r\n\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.1\r\n\r\n### Cloud provider\r\n\r\nn\/a (repros on bare-metal `kubeadm`)\r\n\r\n### OS version\r\n\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n\r\n### Install tools\r\n\r\nkubeadm\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\ncontainerd github.com\/containerd\/containerd v1.7.5 fe457eb99ac0e27b3ce638175ef8e68a7d2bc373\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","Hi Mike.\r\n\r\nI modified the init container commands to use the workaround script and added a shared emptyDir volume for the completion markers. This configuration adds an emptyDir volume named init-completion-markers, which is mounted into both init containers and the main container at \/tmp\/init-markers. Each init container now checks for the presence of its specific marker file before executing its main command. If the marker is not present, it executes the command and creates the marker. This makes sure that on pod restart, if the init container was previously completed successfully, it will not re-execute its payload, effectively working around the issue.\r\n\r\nTry this revised version of \"repro.yaml\":\r\n\r\n```\r\n`apiVersion: apps\/v1\r\nkind: Deployment\r\nmetadata:\r\n  labels:\r\n    app: multi-init-container-bug-repro\r\n  name: multi-init-container-bug-repro\r\n  namespace: delme\r\nspec:\r\n  progressDeadlineSeconds: 600\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: multi-init-container-bug-repro\r\n  strategy:\r\n    type: Recreate\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: multi-init-container-bug-repro\r\n    spec:\r\n      automountServiceAccountToken: false\r\n      containers:\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - sleep infinity && echo 'pgrepid:PkIOeQU4J6zI'\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: main\r\n        volumeMounts:\r\n        - name: init-completion-markers\r\n          mountPath: \/tmp\/init-markers\r\n      initContainers:\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - >\r\n          if [ ! -f \/tmp\/init-markers\/init-first ]; then\r\n            echo 'init-container: First!';\r\n            touch \/tmp\/init-markers\/init-first;\r\n          fi\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: init-first\r\n        volumeMounts:\r\n        - name: init-completion-markers\r\n          mountPath: \/tmp\/init-markers\r\n      - command:\r\n        - \/bin\/bash\r\n        - -c\r\n        - >\r\n          if [ ! -f \/tmp\/init-markers\/init-second ]; then\r\n            echo 'init-container: Second!';\r\n            touch \/tmp\/init-markers\/init-second;\r\n          fi\r\n        image: debian:stable\r\n        imagePullPolicy: IfNotPresent\r\n        name: init-second\r\n        volumeMounts:\r\n        - name: init-completion-markers\r\n          mountPath: \/tmp\/init-markers\r\n      restartPolicy: Always\r\n      volumes:\r\n      - name: init-completion-markers\r\n        emptyDir: {}\r\n`\r\n```"],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"Storageversion API for CRDs (using workqueue)","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR is based off of https:\/\/github.com\/kubernetes\/kubernetes\/pull\/120582. \r\n\r\nThis is an attempt to drive the StorageVersion API graduation toward beta by addressing requirements\/concerns for the same.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/enhancements\/issues\/2339\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-api-machinery\/2339-storageversion-api-for-ha-api-servers\r\n```","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123999#\" title=\"Author self-approved\">richabanker<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storageversion\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storageversion\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/triage accepted","\/retest-required","@richabanker: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-kind-ipv6 | 922852c0e8fc7e67bb299fea6844052b855c0dab | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123999\/pull-kubernetes-e2e-kind-ipv6\/1771413370642108416) | true | `\/test pull-kubernetes-e2e-kind-ipv6`\npull-kubernetes-unit | 922852c0e8fc7e67bb299fea6844052b855c0dab | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123999\/pull-kubernetes-unit\/1771413370939904000) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123999). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Arichabanker). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","area\/apiserver","sig\/api-machinery","size\/XXL","kind\/feature","cncf-cla: yes","sig\/testing","do-not-merge\/release-note-label-needed","needs-priority","triage\/accepted"]},{"title":"Relax WatchSemanticsTest to make it faster","body":"Ref https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850\r\n\r\n\/kind flake\r\n\/sig api-machinery\r\n\/priority important-soon\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n\/assign @liggitt @serathius ","comments":["\/triage accepted","@liggitt - PTAL","\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: 8cbf70223c5761fea0e895ce2ec8391edc7d7ecf<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123997#issuecomment-2013437945\" title=\"Approved\">liggitt<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123997#\" title=\"Author self-approved\">wojtek-t<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)~~ [liggitt,wojtek-t]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123997) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123997) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123997) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123997) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","Just fixed gofmt","\/lgtm","LGTM label has been added.  <details>Git tree hash: 6d8a3ebf7b36b247a6ad732a6979cef2a0b5508e<\/details>"],"labels":["priority\/important-soon","area\/apiserver","lgtm","sig\/api-machinery","size\/XS","kind\/flake","release-note-none","approved","cncf-cla: yes","triage\/accepted"]},{"title":"[WIP] Test generating deepcopy for structs containing generics","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\nAdds a test for https:\/\/github.com\/kubernetes\/gengo\/pull\/255\r\n\r\n#### Which issue(s) this PR fixes:\r\nN\/A\r\n\r\n#### Special notes for your reviewer:\r\nThis test is currently WIP due to the aforementioned `gengo` feature being unreleased.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @TheSpiritXIII! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @TheSpiritXIII. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123996#\" title=\"Author self-approved\">TheSpiritXIII<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [dims](https:\/\/github.com\/dims), [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n- **[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)**\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)**\n- **[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)**\n- **[staging\/src\/k8s.io\/csi-translation-lib\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/csi-translation-lib\/OWNERS)**\n- **[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)**\n- **[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n- **[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)**\n- **[staging\/src\/k8s.io\/mount-utils\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/mount-utils\/OWNERS)**\n- **[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"dims\",\"thockin\"]} -->"],"labels":["sig\/network","area\/kubelet","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","size\/L","kind\/feature","release-note-none","sig\/auth","sig\/cli","cncf-cla: yes","sig\/instrumentation","needs-ok-to-test","sig\/architecture","do-not-merge\/work-in-progress","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"WIP: Manage container termination in a non-blocking way","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #121398 \r\n\r\nThis manages container termination in a non-blocking way.\r\nxref:\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/121412\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/4438\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/753\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\r\n\/uncc all","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123995#\" title=\"Author self-approved\">gjkim42<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mrunalp](https:\/\/github.com\/mrunalp) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mrunalp\"]} -->","@gjkim42: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-e2e-containerd | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-node-e2e-containerd\/1770482158427705344) | true | `\/test pull-kubernetes-node-e2e-containerd`\npull-kubernetes-e2e-kind | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-e2e-kind\/1770482153377763328) | true | `\/test pull-kubernetes-e2e-kind`\npull-kubernetes-e2e-kind-ipv6 | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-e2e-kind-ipv6\/1770482154229207040) | true | `\/test pull-kubernetes-e2e-kind-ipv6`\npull-kubernetes-unit | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-unit\/1770482157584650240) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-verify | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-verify\/1770482162680729600) | true | `\/test pull-kubernetes-verify`\npull-kubernetes-e2e-gce | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-e2e-gce\/1770482146671071232) | true | `\/test pull-kubernetes-e2e-gce`\npull-kubernetes-e2e-gce-cos-alpha-features | 533cb7fd7e1293e4575cfe65005bd3ea8f54669b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123995\/pull-kubernetes-e2e-gce-cos-alpha-features\/1770482150865375232) | false | `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123995). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Agjkim42). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/kubelet","sig\/node","size\/L","kind\/feature","cncf-cla: yes","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"strictARP Configuration in kube-proxy (IPVS Mode) Does Not Revert sysctl Parameters on Setting to false","body":"### What happened?\n\nWhen `strictARP` is set to `true` in kube-proxy's configuration for IPVS mode, kube-proxy modifies certain `sysctl` parameters as expected. However, if the `strictARP` is later changed to `false`, the modified `sysctl` parameters are not reverted to their original values.\r\n\r\nUpon inspecting the source code, I noticed that there is logic to apply changes when `strictARP` is `true`, but there appears to be no corresponding logic to revert these changes when `strictARP` is set to `false`.\n\n### What did you expect to happen?\n\nrevert\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nmaster branch\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig Network","@lizhipeng629","well, I think this should be a point that can be improved. This sysctl is a host-level configuration, and we should avoid it affecting the entire host.\r\n\r\n\/cc @uablrek @aojea ","IMO `kube-proxy` shouldn't tamper with sysctl at all, and never when it isn't ansolutely necessary. Someone may want the sysctl set for other reasons, and `kube-proxy` should not reset it. ","When kube-proxy is in ipvs mode and using metallb as a load balancing service, it should be required to configure `strictARP`. see https:\/\/metallb.universe.tf\/installation\/#preparation.\r\n\r\nIf strictARP is set to false or kube-proxy mode switches to iptables mode, should we restore the sysctl? I think yes, since we no longer need to configure it and we don't want it to affect the rest of the host's progress.","You can't say \"restore\", since you don't know if it was `kube-proxy` that set it in the first place. It may be set by a user, e.g. in `\/etc\/sysctl.conf`, because they need it.","> You can't say \"restore\"\r\n\r\nsorry for my bad words. I would say that if the user needs it then they would set `strictARP ` to true and then kube-proxy sets the sysctl. If the user still needs it, I don't think they would want to change `strictARP` to false?\r\n\r\nIf they really don't need it anymore, currently, they need to set the sysctl manually for each node, so it would be nice if we could set `strictARP` to false and then kube-proxy could help us set the sysctl.","Yes, this is precisely why the issue arose. I assumed that kube-proxy would revert any changes made upon setting it origin(default) value. Prior to reviewing the code, I was unaware of the things kube-proxy would do to my nodes under such circumstances. It seemed logical to me that it would undo any modifications to my nodes upon being set to false.\r\n\r\n> > You can't say \"restore\"\r\n> \r\n> sorry for my bad words. I would say that if the user needs it then they would set `strictARP ` to true and then kube-proxy sets the sysctl. If the user still needs it, I don't think they would want to change `strictARP` to false?\r\n> \r\n> If they really don't need it anymore, currently, they need to set the sysctl manually for each node, so it would be nice if we could set `strictARP` to false and then kube-proxy could help us set the sysctl.\r\n\r\n"],"labels":["kind\/bug","sig\/network","needs-triage"]},{"title":"[FLAKING] Test TestWaitUntilWatchCacheFreshAndForceAllEvents is flaking with data race","body":"### Which jobs are flaking?\n\nInternal CI for ppc64le arch :\r\n\r\nhttps:\/\/prow.ppc64le-cloud.cis.ibm.net\/view\/gs\/ppc64le-kubernetes\/logs\/postsubmit-master-golang-kubernetes-unit-test-ppc64le\/1769786143454269440\n\n### Which tests are flaking?\n\nk8s.io\/apiserver\/pkg\/storage: cacher \r\n\r\n```\r\n{Failed;Failed;Failed;Failed;  === RUN   TestWaitUntilWatchCacheFreshAndForceAllEvents\/allowWatchBookmarks=true,_sendInitialEvents=true,_RV=unset\r\n=== PAUSE TestWaitUntilWatchCacheFreshAndForceAllEvents\/allowWatchBookmarks=true,_sendInitialEvents=true,_RV=unset\r\n=== CONT  TestWaitUntilWatchCacheFreshAndForceAllEvents\/allowWatchBookmarks=true,_sendInitialEvents=true,_RV=unset\r\n    testing.go:1398: race detected during execution of test\r\n```\n\n### Since when has it been flaking?\n\nFailure is observed on Mar 18th\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/ibm-k8s-unit-tests-ppc64le#periodic-k8s-unit-tests-ppc64le\n\n### Reason for failure (if possible)\n\n_No response_\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This test is flaking with data race as ```dummyStorage``` is getting accessed for field ```requestWatchProgressCounter``` without obtaining locks in ```staging\/src\/k8s.io\/apiserver\/pkg\/storage\/cacher\/cacher_whitebox_test.go : TestWaitUntilWatchCacheFreshAndForceAllEvents ```\r\n\r\nI am thinking to raise a PR to add necessary locks for different ```scenarios```\r\n\r\n\/cc @mkumatag  @Rajalakshmi-Girish ","\/assign"],"labels":["sig\/node","kind\/flake","needs-triage"]},{"title":"storage\/cacher: move black box tests to a separate pkg to reduce running time","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nxref: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850#issuecomment-2006077401\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\nwhile reviewing please make sure i didn't accidentally remove any test. \r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123990#\" title=\"Author self-approved\">p0lyn0mial<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [wojtek-t](https:\/\/github.com\/wojtek-t) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"wojtek-t\"]} -->","I ack the need to shorten the run time. I really really dislike losing history of the tests. Is there any way to fix the timeout and keep the source history?","cc @wojtek-t @serathius ","> I ack the need to shorten the run time. I really really dislike losing history of the tests. Is there any way to fix the timeout and keep the source history?\r\n\r\nIf we are okay with moving the tests to a new pkg I could experiment a bit with git to see if there is a way of preserving history.","After moving the tests to a new pkg ([ref](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123990\/pull-kubernetes-unit\/1770052157001699328)):\r\n\r\n```\r\nk8s.io\/apiserver\/pkg\/storage: cacher | 1m38s\r\n\r\nk8s.io\/apiserver\/pkg\/storage\/cacher: testing | 3m23s\r\n\r\n```\r\n\r\nI'll try to run some of the test in Parallel to further reduce running time.","we need to recover master health right away without a major test refactor ... @serathius, can you undo the double-run of TestWatchSemantics?","> we need to recover master health right away without a major test refactor ... @serathius, can you undo the double-run of TestWatchSemantics?\r\n\r\nok, sounds good.","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/triage accepted"],"labels":["kind\/cleanup","area\/apiserver","sig\/api-machinery","needs-rebase","size\/XL","release-note-none","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"GKE schedules pods that prevent scale down","body":"### What happened?\n\nOriginal issue was closed for no reason: #69696 \r\n\r\nGKE still schedules all kinds of pods that prevent scaledown\n\n### What did you expect to happen?\n\nPods not to prevent scaledown\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun a cluster and try to scaledown after scaleup\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nsince 2018 until today\n\n### Cloud provider\n\nGoogle\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig cloud-provider","\/sig gcp","@pranav-pandey0804: The label(s) `sig\/gcp` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123989#issuecomment-2006639373):\n\n>\/sig gcp\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/area provider\/gcp"],"labels":["kind\/bug","area\/provider\/gcp","sig\/cloud-provider","needs-triage"]},{"title":"code-generator: use cases.Title instead of strings.Title","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nstrings.Title is deprecated; this converts the few uses in code-generator to cases.Title, relying on language.Und to match the previous behaviour.\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123988#\" title=\"Author self-approved\">skitt<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sttts\"]} -->","\/cc @thockin ","\/triage accepted"],"labels":["kind\/cleanup","sig\/api-machinery","size\/S","release-note-none","cncf-cla: yes","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"Discussion Feature: Ability to use downward API in node\/pod Affinity\/antiAffinity","body":"Problem:\r\n\r\nwe have platform where batch jobs of various size will be submitted by users. Thus we need to dynamically request\/select instances for pods based on resources for each pod=step in batch jobs. We are provisioning instances with karpenter, so we have several NodeClasses\/PRovisioners (which contain available types of instances, which are initializied based on pod resource requirements), our problem is that those pods often will be placed on instances which have more resources than it was requested, because pods e.g. from previous steps asked for big machine, they are finished, but big machine is still there (and thus we are paing for it) and k8s schedules them there because they are free, because it takes some time before it is garbage collected. What we would like to have is referencing resources from pod spec in node affinity rules so that  we can e.g write:\r\n```\r\naffinity:\r\n  nodeAffinity:\r\n    requiredDuringSchedulingIgnoredDuringExecution:\r\n    - nodeSelectorTerms:\r\n        matchExpressions:\r\n        - key: karpenter.aws.io\/instance-cpu\r\n          operator: In\r\n          values:\r\n          - valueFrom:\r\n              fieldRef:\r\n                fieldPath: spec.resource.requests.cpu\r\n```\r\n\r\nThere can be probably also other applications of this: https:\/\/stackoverflow.com\/questions\/48223562\/how-to-create-statefulset-with-nodeaffinity","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig scheduling","\/cc"],"labels":["sig\/scheduling","needs-triage"]},{"title":"Analyzing and Addressing Unforeseen Performance Issues in a Large-Scale Kubernetes Cluster","body":"### What happened?\n\nHi everyone,\r\n\r\nI hope this message finds you well. I'm reaching out to share and seek advice on a performance issue we've encountered in our production Kubernetes (k8s) cluster. Our setup includes 300 nodes and supports 8,000 Pods, and we've recently started experiencing some concerns.\r\n\r\nThe issue began with a Pod we deployed, designed to periodically scrape Pod information by making an API call to `\/api\/v1\/pods?fieldSelector=spec.nodeName%3D<node-name>` on the k8s API server. This Pod updates Pod information from its storage while periodically initiating these API calls.\r\n\r\nAs illustrated in Figure 1 below, there's been a noticeable increase in the request rate from an average of 2.75 to 4 requests per second. During this period, we observed significant delays in the k8s API servers when processing LIST verb API calls, as highlighted in Figure 2. Concurrently, there was a dramatic increase in the memory usage of the API servers, depicted in Figure 3. Additionally, the gRPC traffic from etcd to the k8s API servers also saw an increase, as shown in Figure 4.\r\n\r\nHowever, what puzzles me is that the workload generated by this Pod appears to be too small to cause such a significant impact. It only contributes to 1.5 requests per second per API server, amounting to a total of 4.5 requests per second.\r\n\r\nI'm eager to hear your thoughts, insights, or any advice you might have on addressing this issue. \r\n\r\n**[Fig. 1: requests per sec from k8s API servers]**\r\n![k8s api \uc758 list verb\uc758 total request](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/4262433\/d7eb114e-8823-4f36-8a56-c0092fad6291)\r\n\r\n**[Fig. 2: 90%-tile Latency of Each Verb from k8s API server]**\r\n![k8s api \uc758 verb \ubcc4 tail latency](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/4262433\/15810101-6817-45fc-b15a-f0e23e1a8b2e)\r\n\r\n**[Fig. 3: k8s API servers' memory usage]**\r\n![k8s apiserver \uba54\ubaa8\ub9ac footprint](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/4262433\/7f53ad44-6e7b-41e6-8569-bd98f6dfe44f)\r\n\r\n\r\n**[Fig. 4: outboud gRPC traffic from etcd]**\r\n![etcd Client Traffic Out (\ud574\ub2f9 \uae30\uac04\uc5d0 \uc99d\uac00\ud588\uc74c)](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/4262433\/b0027b3e-4d4a-4137-87d2-be99440a86bc)\r\n\n\n### What did you expect to happen?\n\nI expected that the workload would not impact the performance of the k8s API servers.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI think you should be able to replicate this issue by setting up a cluster with over 8,000 Pods and periodically fetching the Pods' information through API calls.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.8\", GitCommit:\"7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T14:10:06Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.6\", GitCommit:\"d921bc6d1810da51177fbd0ed61dc811c5228097\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T17:44:26Z\", GoVersion:\"go1.16.9\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nWe have set up our cluster on-premises.\r\n\r\n```console\r\n$ cat \/proc\/cpuinfo\r\n...\r\nprocessor\t: 111\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 106\r\nmodel name\t: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\r\nstepping\t: 6\r\nmicrocode\t: 0xd0003a5\r\ncpu MHz\t\t: 3400.000\r\n\r\n$ cat \/proc\/meminfo \r\nMemTotal:       528038752 kB\r\nMemFree:        284106660 kB\r\nMemAvailable:   464637832 kB\r\nBuffers:         5636388 kB\r\nCached:         168626216 kB\r\nSwapCached:            0 kB\r\nActive:         66852988 kB\r\nInactive:       144133368 kB\r\nActive(anon):   38236244 kB\r\nInactive(anon):   101448 kB\r\nActive(file):   28616744 kB\r\nInactive(file): 144031920 kB\r\nUnevictable:     9536672 kB\r\nMlocked:         9536672 kB\r\nSwapTotal:             0 kB\r\nSwapFree:              0 kB\r\nDirty:            116556 kB\r\nWriteback:             0 kB\r\nAnonPages:      39823220 kB\r\nMapped:          4326780 kB\r\nShmem:           1242292 kB\r\nKReclaimable:   10923516 kB\r\nSlab:           13771744 kB\r\nSReclaimable:   10923516 kB\r\nSUnreclaim:      2848228 kB\r\nKernelStack:       77232 kB\r\nPageTables:       242660 kB\r\nNFS_Unstable:          0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:    261131696 kB\r\nCommitted_AS:   153182256 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:      779924 kB\r\nVmallocChunk:          0 kB\r\nPercpu:           456704 kB\r\nHardwareCorrupted:     0 kB\r\nAnonHugePages:   4622336 kB\r\nShmemHugePages:        0 kB\r\nShmemPmdMapped:        0 kB\r\nFileHugePages:         0 kB\r\nFilePmdMapped:         0 kB\r\nCmaTotal:              0 kB\r\nCmaFree:               0 kB\r\nHugePages_Total:    2820\r\nHugePages_Free:        0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\nHugetlb:         5775360 kB\r\nDirectMap4k:     7577012 kB\r\nDirectMap2M:    355958784 kB\r\nDirectMap1G:    175112192 kB\r\n```\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.4 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.4 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n\r\n$ uname -a\r\nLinux n6-main-master01 5.4.0-113-generic #127-Ubuntu SMP Wed May 18 14:30:56 UTC 2022 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nkubespray v2.19\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd v1.5.8\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nCNI=calico\r\nCSI=weka-fs-plugin\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig api-machinery\r\n\/sig etcd\r\n\/sig scalability","> The memory has increased significantly. I feel that there is more than one client on your side that initiated the List request (please correct me if I am wrong).\r\n\r\nIn addition, I have a few suggestions here, I hope they can help you:\r\n\r\n1. We can obtain the Pods in the cluster through Informer. We can think that Informer.Lister contains the information we need to obtain. (Perhaps we want to perform Server Side Filter during List). Example here: https:\/\/github.com\/kubernetes\/client-go\/blob\/master\/examples\/workqueue\/main.go\r\n\r\n2. We try our best to use APIServer\u2019s WatchCache to prevent kube-apiserver from forwarding requests to ETCD. The rules are as follows: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/1f3c65c5f4c79a57427297d99985ab1493db7147\/staging\/src\/k8s.io\/apiserver\/pkg \/storage\/cacher\/cacher.go#L763","Thank you @sxllwx for your suggestions. We will take a look at them :D"],"labels":["kind\/bug","sig\/scalability","sig\/api-machinery","needs-triage","sig\/etcd"]},{"title":"Remove cloud provider dependency from volume host and volume controllers","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThe cloud-vendor in-tree volume plugins have completed csi migration, see https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/#types-of-persistent-volumes.\r\n\r\nThe portworxVolume does not depend on the cloud provider, so removing related codes from volume controllers in the controller manager and volume host in Kubelet is safe.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\nKEP: https:\/\/github.com\/elmiko\/kubernetes-enhancements\/blob\/master\/keps\/sig-cloud-provider\/2395-removing-in-tree-cloud-providers\/README.md\r\n\r\nThe DisableCloudProviders and DisableKubeletCloudCredentialProvider feature-gates only impact functionality tied to the --cloud-provider flag, specifically in-tree volume plugins are not covered. \r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123984#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cheftako](https:\/\/github.com\/cheftako), [saad-ali](https:\/\/github.com\/saad-ali), [yujuhong](https:\/\/github.com\/yujuhong) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cheftako\",\"saad-ali\",\"yujuhong\"]} -->","\/sig cloud-provider","\/cc @jsafrane @dims ","\/triage accepted\r\n\/priority important-longterm"],"labels":["area\/test","area\/kubelet","kind\/cleanup","sig\/storage","sig\/node","sig\/api-machinery","size\/L","kind\/feature","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","priority\/important-longterm","sig\/cloud-provider","triage\/accepted"]},{"title":"Fix unexpected change of container ready state when kubelet restart","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\npod config readinessprobe:\r\n1. if ready state is true and kubelet restart, pod temporarily report containerNotReady\uff0cthis may lead to service not available\r\n2. if ready state is false and kubelet restart, pod temporarily report containerReady\uff0cthis may also lead to service not available\r\n\r\nthus, this PR keep the container ready state when kubelet restart.\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #100277 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @LastNight1997. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123982#\" title=\"Author self-approved\">LastNight1997<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->"],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/L","release-note-none","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage"]},{"title":"[e2e] Update traffic distribution test","body":"Skip traffic distribution test if cluster has < 3 zones.\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nSkip traffic distribution test if cluster has < 3 zones.\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["@lzhecheng: The label(s) `kind\/testing` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123981):\n\n>Skip traffic distribution test if cluster has < 3 zones.\r\n>\r\n>\r\n>\r\n>#### What type of PR is this?\r\n>\/kind testing\r\n>\r\n>\r\n>#### What this PR does \/ why we need it:\r\n>Skip traffic distribution test if cluster has < 3 zones.\r\n>#### Which issue(s) this PR fixes:\r\n>\r\n>Fixes #\r\n>\r\n>#### Special notes for your reviewer:\r\n>\r\n>#### Does this PR introduce a user-facing change?\r\n>\r\n>```release-note\r\n>NONE\r\n>```\r\n>\r\n>#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n>\r\n>\r\n>```docs\r\n>\r\n>```\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123981#\" title=\"Author self-approved\">lzhecheng<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [andrewsykim](https:\/\/github.com\/andrewsykim) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)**\n- **[test\/e2e\/network\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/network\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"andrewsykim\"]} -->","\/test pull-kubernetes-integration\r\n\/kind cleanup","\/retest","\/retest","@lzhecheng: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-ubuntu-gce-network-policies | d93ae7bf35f49beed7d3cda78edd3f7e9c122b4f | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123981\/pull-kubernetes-e2e-ubuntu-gce-network-policies\/1770265952705843200) | false | `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123981). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Alzhecheng). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","sig\/network","kind\/cleanup","size\/S","release-note-none","cncf-cla: yes","sig\/testing","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"restarting a kubelet should never affect the running workload","body":"### What happened?\n\n1.Node label changed.\r\nNode label changed is because the operation and maintenance engineer organized the node label, such as hpc=true, and removed this label. When a pod is compatible with this label, restarting the node kubelet will cause the pod to be rebuilt. This actually shouldn\u2019t be the case. It will affect the normal operation of the business.\r\n2.https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123971\r\n3.https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123816\r\nAfter the above three scenarios occur, as long as the kubelet is restarted, the pod will be evicted. Obviously, this is not as expected. It is also an undesirable result for online business.\n\n### What did you expect to happen?\n\nRestarting a kubelet should not cause any disruption to the running workload (which likely will mean skip admission of running pods, but let's not run ahead of ourselves) and backlink to this issue is probably the best way forward still.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nBefore restarting the kubelet, fully check the scenarios mentioned above.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```\r\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:47:36Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:42:04Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig node\r\n","https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123859","I think when the admit fails, the pod creation time and current status should be judged. If the pod is currently in the running state, and the pod creation time is earlier than the kubelet startup time, skip rejecting the pod.","\/triage accepted\r\n\/priority important-longterm\r\n\r\nWe are aware of cases on which a kubelet restart perturbs the workload, and it shouldn't. We will collect the cases on this issue.\r\nI'm torn between `backlog` and `important-longterm`. Setting the latter because otherwise this issue will likely slip through cracks.","> I think when the admit fails, the pod creation time and current status should be judged. If the pod is currently in the running state, and the pod creation time is earlier than the kubelet startup time, skip rejecting the pod.\r\n\r\nI tend to agree but with more emphasis on the information from the runtime. We probably need to integrate the reported runtime state into the pod creation loop, which is likely a nontrivial effort.","\/cc @bobbypage @smarterclayton ","https:\/\/github.com\/kubernetes\/kubernetes\/issues\/118559 Here is another case about \"Running pods with devices are terminated if kubelet is restarted\"\r\nThe way he modified it was https:\/\/github.com\/kubernetes\/kubernetes\/pull\/119432","How long does it take for kubelet to restart?\r\n\r\nMy test is normal.\r\nIf the kubelet restart takes too long, it will cause the pod to be evicted. The pod fixed on this machine will be restarted."],"labels":["kind\/bug","sig\/node","priority\/important-longterm","triage\/accepted"]},{"title":"KEP-4540: Add CPUManager policy option to restrict reservedSystemCPUs to system daemons and interrupt processing","body":"#### What type of PR is this?\r\n\/kind feature\r\n\/sig node\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThis PR implements https:\/\/github.com\/kubernetes\/enhancements\/pull\/4541\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nEnhancement https:\/\/github.com\/kubernetes\/enhancements\/issues\/4540\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\nAdd a new `strict-cpu-reservation` policy option to cpu manager `static` policy. When enabled reserved CPU cores for system usage will be strictly used for system daemons and interrupt processing no longer available for burstable and best-effort workloads.\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/pull\/4541","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @jingczhang! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @jingczhang. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123979#\" title=\"Author self-approved\">jingczhang<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [derekwaynecarr](https:\/\/github.com\/derekwaynecarr) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cm\/cpumanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/cpumanager\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"derekwaynecarr\"]} -->","\/sig node\r\n\/ok-to-test\r\n\r\nnote the parent KEP is not approved yet","@jingczhang: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-linter-hints | d584fcb4b8ac2cf0749ae7062b1471014c35ee4a | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123979\/pull-kubernetes-linter-hints\/1770708324820455424) | false | `\/test pull-kubernetes-linter-hints`\npull-kubernetes-verify-lint | d584fcb4b8ac2cf0749ae7062b1471014c35ee4a | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123979\/pull-kubernetes-verify-lint\/1770708323977400320) | true | `\/test pull-kubernetes-verify-lint`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123979). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Ajingczhang). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/kubelet","sig\/node","size\/M","kind\/feature","cncf-cla: yes","do-not-merge\/release-note-label-needed","ok-to-test","needs-priority","needs-triage"]},{"title":"Get node from local cache instead of kube-apiserver cache for kubelet status updates","body":"This allows to avoid majority of  GET Node calls to kube-apiserver coming from kubelet - this call is happening every 10s by default, so we save #nodes\/10 QPSes, which can be very substantial with large clusters.\r\n\r\n\/kind cleanup\r\n\/priority important-longterm\r\n\/sig-node\r\n\r\n```release-note\r\nNONE\r\n```","comments":["\/triage accepted\r\n\/lgtm\r\n\/hold\r\n@wojtek-t please unhold when needed, I just prevent auto merging","LGTM label has been added.  <details>Git tree hash: e39ee25dc4a9724d326dad68ba29a1508ac4a08c<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123977#pullrequestreview-1945376035\" title=\"Approved\">matthyx<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123977#\" title=\"Author self-approved\">wojtek-t<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)~~ [wojtek-t]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["area\/kubelet","kind\/cleanup","lgtm","sig\/node","size\/M","release-note-none","approved","cncf-cla: yes","priority\/important-longterm","do-not-merge\/hold","triage\/accepted"]},{"title":"[wip] featuregate future?","body":"\/cc @thockin ","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123976#\" title=\"Author self-approved\">deads2k<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[LICENSES\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/LICENSES\/OWNERS)**\n- ~~[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)~~ [deads2k]\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [deads2k]\n- ~~[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)~~ [deads2k]\n- ~~[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)~~ [deads2k]\n- **[staging\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/OWNERS)**\n- ~~[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)~~ [deads2k]\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n- ~~[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)~~ [deads2k]\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->"],"labels":["area\/test","sig\/network","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","size\/XXL","sig\/auth","sig\/cli","cncf-cla: yes","sig\/instrumentation","sig\/testing","sig\/architecture","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage","do-not-merge\/needs-kind"]},{"title":"Fix StatefulSetMinReadySeconds healthy concept","body":"#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does\r\nBy the Kubernetes docs, a healthy pod is a pod that is ready to accept requests. But, this definition could be a bit misleading when we introduce MinReadySeconds. With the \"MinReadySeconds\" introduction, we should only consider a pod healthy when it is available (after \"MinReadySeconds\" is over).\r\n\r\n#### Why we need it\r\nIt fixes some reported bugs with the StatefulSetMinReadySeconds feature. It's also necessary for promoting StatefulSet MaxUnavailable feature gate from alpha to beta.\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #112307, fixes #123918 and fixes #119234\r\n\r\n#### Special notes for your reviewer:\r\nRelates to https:\/\/github.com\/kubernetes\/enhancements\/pull\/4474\r\n\r\nAddresses https:\/\/github.com\/kubernetes\/enhancements\/pull\/4474#discussion_r1478474701\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nStatefulSet now respects MinReadySeconds.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/pull\/4474\r\n\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @leomichalski. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123975#\" title=\"Author self-approved\">leomichalski<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controller\/statefulset\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/statefulset\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","\/ok-to-test","\/test pull-kubernetes-unit\r\n","The test that failed is not related to this PR. 7 hours ago, it also failed for the same reason for another PR.\r\n\r\nLogs for this test:\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123975\/pull-kubernetes-unit\/1769782408061652992#1:build-log.txt%3A2680\r\n\r\nLogs for this test for another PR (#123916):\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123916\/pull-kubernetes-unit\/1769802627165458432#1:build-log.txt%3A2696\r\n\r\n","\/cc @knelasevero ","The tests that failed are flaky, as documented here: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850\r\n\r\nIt includes TestWatchStreamSeparation and Test_newReadyCancelPending","\/retest"],"labels":["kind\/bug","release-note","size\/S","sig\/apps","cncf-cla: yes","do-not-merge\/work-in-progress","ok-to-test","needs-priority","needs-triage"]},{"title":"client-go\/features\/testing: intro SetFeatureGatesDuringTest","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nIt will allow writing unit tests that need to change the default value of a feature gate, for example `clientfeaturestesting.SetFeatureGatesDuringTest(t, clientfeatures.FeatureGates(), clientfeatures.WatchListClient, false)`\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123974#\" title=\"Author self-approved\">p0lyn0mial<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","\/triage accepted"],"labels":["sig\/api-machinery","size\/M","kind\/feature","release-note-none","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"Remove k8s.io\/apiserver ability to bind insecure ports","body":"The project does not recommend using insecure ports.  Even unauthenticated TLS is an improvement since it provides confidentiality. If you relied upon this, please update to secure serving options.\r\n\r\n\/kind cleanup\r\n\/priority important-soon\r\n\/milestone v1.31\r\n\/sig auth\r\n\r\n```release-note\r\nNONE\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123972#\" title=\"Author self-approved\">deads2k<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)~~ [deads2k]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/triage accepted","\/lgtm","LGTM label has been added.  <details>Git tree hash: bb02a92aa4ab1faf23b72f17381f4542b941fea3<\/details>"],"labels":["priority\/important-soon","kind\/cleanup","area\/apiserver","lgtm","sig\/api-machinery","size\/L","release-note-none","sig\/auth","approved","cncf-cla: yes","triage\/accepted"]},{"title":"pod with initcontainer failed with UnexpectedAdmissionError when restart kubelet","body":"### What happened?\n\nMemory manager have bug, restart kubelet (do not delete any checkpoint file ), pod with init-container will failed.\n\n### What did you expect to happen?\n\nPod A and Pod B is still running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI have a node had 256Gi and 2 NUMA node with 128Gi for one NUMA;\r\n1. Create Pod A, with init-container and container both 100Gi\r\n2. Create Pod B, with init-container and container both 100Gi \r\n3. Restart kubelet.\r\n4. Both Pod A and Pod B failed with UnexpectedAdmissionError\n\n### Anything else we need to know?\n\nIn `--topology-policy=single-numa-node` mode.\r\nWhen memory manager caculate memory for topology, it will caculate init-container firstly, when comes to app container, it will reuse the init-container's memory as possible,  and reduce init-container's memory in state, it's as expected;\r\n\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/9df98f3d393e204dd8db02322057ac2e7f3f6c28\/pkg\/kubelet\/cm\/memorymanager\/policy_static.go#L192\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/9df98f3d393e204dd8db02322057ac2e7f3f6c28\/pkg\/kubelet\/cm\/memorymanager\/policy_static.go#L956-L965\r\n\r\nBut when restart kubelet, memory manager will re calculate memory for init-container,  if now node has not enough memory for init-container, the whole pod will be failed because of Admit failed;\r\n\r\n<img width=\"1060\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/151703335\/d455fd2e-b7a6-4661-825a-ae661d96d84a\">\r\n\r\nAfter restart kubelet:\r\n<img width=\"1420\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/151703335\/cd1f196d-b3c7-418f-b670-90bb51051321\">\r\n\r\n\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:47:36Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:42:04Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig node","I think we need to fix this bug by change logic:  if init-container is already terminated, skip memory \/ cpu topology calculate;\r\nAnd we still should not make any running pod failed after restart kubelet ;","Why memory not enough, when kubelet restart? it calc main-container(100G) first? and left 28Gi for init-container?","> Why memory not enough, when kubelet restart? it calc main-container(100G) first? and left 28Gi for init-container?\r\n\r\nWhen kubelet restart, it will calc all the pods on node once again,  and containers in pod's order is calc init-container then app-container; \r\nBecause init-container's memory in checkpoint is considerd to reuse by main-container, so in checkpoint(\/var\/lib\/kubelet\/memory_manager_state) file, init-container will be removed. ","dose this exists in master branch?","\/triage accepted\r\n\/priority backlog\r\n\r\nWe seen similar issues in the past. I'll have a look and crosslink them. I think the issue is legit and there's something to fix here. Setting priority low pending further investigation"],"labels":["kind\/bug","priority\/backlog","sig\/node","triage\/accepted"]},{"title":"remove VolumePluginWithAttachLimits interface","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nno in-tree volume plugin implements VolumePluginWithAttachLimits.\r\n\r\nthe following plugins were removed from Kubelet:\r\n\r\n- awsElasticBlockStore\r\n- azureDisk\r\n- cinder\r\n- gcePersistentDisk\r\n\r\nFYI https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/#types-of-persistent-volumes\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nKubelet stoped to report volume limit information to node status since v1.27. \r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123970#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [saad-ali](https:\/\/github.com\/saad-ali), [yujuhong](https:\/\/github.com\/yujuhong) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"saad-ali\",\"yujuhong\"]} -->","\/retest","\/test pull-kubernetes-e2e-gce-csi-serial","\/triage accepted\r\n\/priority important-longterm"],"labels":["area\/kubelet","kind\/cleanup","sig\/storage","sig\/node","release-note","size\/L","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"cleanup: delete rand.Seed(time.Now().UnixNano()) and using global number generator.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind cleanup\r\n#### What this PR does \/ why we need it:\r\n\r\nAfter go 1.20, the\u00a0math\/rand\u00a0package now automatically seeds the global random number generator.\r\n\r\nso this PR is working for delete `rand.Seed(time.Now().UnixNano())` and using global number generator.\r\n\r\n see https:\/\/tip.golang.org\/doc\/go1.20\r\n \r\n There are still codes like `rand.Seed(12345)` using fixed values in the code to create predictable results, and also need to update. But it's probably worth treating them with caution, so this PR doesn't update them and I can continue working on them in other PRs.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @liangyuanpeng. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/ok-to-test","\/triage accepted\r\n\/retest","\/priority important-longterm","\/lgtm\r\n\/assign @liggitt","LGTM label has been added.  <details>Git tree hash: 67f17dc424848e925032896df31026b2605f4816<\/details>","\/approve","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123969#\" title=\"Author self-approved\">liangyuanpeng<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123969#issuecomment-2011161539\" title=\"Approved\">liggitt<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/component-base\/cli\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/cli\/OWNERS)~~ [liggitt]\n- ~~[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)~~ [liggitt]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["area\/test","kind\/cleanup","lgtm","sig\/node","sig\/api-machinery","size\/S","release-note-none","approved","sig\/cli","cncf-cla: yes","sig\/testing","priority\/important-longterm","sig\/architecture","ok-to-test","triage\/accepted"]},{"title":"Commands and output are logged when attached to a container via kubectl","body":"When attached to a container, all commands and output executed during an attached session will be visible via pods\/log that could be scraped.\r\n\r\n**Repro steps:**\r\nRun a pod and directly attach to its shell with the following command:\r\n> kubectl run -it my-pod \u2014image=busybox \u2014 sh\r\n\r\nOnce the pod is started and attached, anything that is typed in the terminal is not only visible in the Kubernetes console (for example in Openshift pod logs tab), but also scraped by the log-scrapers like fluentbit and are forwarded to ELK\/EFK\/etc.\r\nMany SREs, infra engineers, troubleshooters run pods like this to troubleshoot and spot problems, and many times credentials and other sensitive information is used during troubleshooting. \r\n\r\ne.g.\r\n> curl -u username:mypass http:\/\/example.com\/ \r\nwill log username and password on the console and eventually into logs\r\n\r\n**Note**: when exec into a container, that session is never logged\r\n\r\n**Proposal:**\r\nAdd a warning to remind users that `all commands and output executed when attached to a container will be visible via pods\/log` to help users who might not expect this behavior\r\n\r\nInitially reported by @yosuf","comments":["\/sig auth\r\n\/sig node\r\n\/triage accepted","I think also\r\n\/sig security","@ritazh \r\n\r\nSo we just need to issue a warning when kubectl attach, right?\r\n\r\nlike\r\n\r\n```shell\r\n$ kubectl run -it mypod --image=busybox -- sh\r\nall commands and output executed when attached to a container will be visible via pods\/log\r\n\/ #\r\n```\r\n\r\n","> @ritazh\r\n> \r\n> So we just need to issue a warning when kubectl attach, right?\r\n> \r\n> like\r\n> \r\n> ```shell\r\n> $ kubectl run -it mypod --image=busybox -- sh\r\n> all commands and output executed when attached to a container will be visible via pods\/log\r\n> \/ #\r\n> ```\r\n\r\nYes that would be a good start.","> > @ritazh\r\n> > So we just need to issue a warning when kubectl attach, right?\r\n> > like\r\n> > ```shell\r\n> > $ kubectl run -it mypod --image=busybox -- sh\r\n> > all commands and output executed when attached to a container will be visible via pods\/log\r\n> > \/ #\r\n> > ```\r\n> \r\n> Yes that would be a good start.\r\n\r\nBefore making any PRs or changes, we should discuss this at a SIG Auth \/ node \/ etc meeting."],"labels":["sig\/node","sig\/auth","sig\/security","triage\/accepted"]},{"title":"add cross field errors","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nAdds errors of the form \"X: must not be set when Y is set\" or \"X: may only be set when Y is Z\", relieving reliance on field.Invalid().\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nThis is an ongoing effort of #121432 \r\n\r\n#### Special notes for your reviewer:\r\nAs alluded to in the original issue, the affected errors will need to be addressed piecemeal, and may lead to the discovery of further errors that may be added. To avoid bloat, I've started with validation and will include more in later PRs that do not involve reviewing the error functions as well. \r\nThere are a few very bespoke errors still in there that may prove out to warrant a dedicated error type based on usage elsewhere. Someone with deeper knowledge may already be able to speak to this and make recommendations, otherwise they will have to be revised in the future.\r\n\r\n\r\nIf I've left any required information out of this PR or have anything wrong, please let me know!\r\n\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: Gladdstone \/ (6056146d072d94bec50ee72996dca1a187df9163)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @Gladdstone! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @Gladdstone. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123966#\" title=\"Author self-approved\">Gladdstone<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/util\/validation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/validation\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169)."],"labels":["kind\/cleanup","size\/L","kind\/api-change","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage","do-not-merge\/needs-sig"]},{"title":"Update loadbalancer policy changes.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @princepereira. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123965#\" title=\"Author self-approved\">princepereira<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [feiskyer](https:\/\/github.com\/feiskyer), [soltysh](https:\/\/github.com\/soltysh) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/pkg\/features\/OWNERS)**\n- **[pkg\/proxy\/winkernel\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/pkg\/proxy\/winkernel\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"feiskyer\",\"soltysh\"]} -->"],"labels":["sig\/network","size\/XL","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","do-not-merge\/cherry-pick-not-approved","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"CVE-2023-5043 and CVE-2023-5044 missing from official list of vulnerabilities","body":"Per https:\/\/github.com\/kubernetes\/website\/issues\/45576, the official CVE feed at https:\/\/kubernetes.io\/docs\/reference\/issues-security\/official-cve-feed\/ doesn't have entries for:\r\n- [CVE-2023-5043](https:\/\/www.cvedetails.com\/cve\/CVE-2023-5043\/)\r\n- [CVE-2023-5044](https:\/\/www.cvedetails.com\/cve\/CVE-2023-5044\/)\r\n\r\nI am not sure if we want to narrow the scope of the feed, fix the missing issues, or change our processes to ensure all announced vulnerabilities show up in the feed.\r\n\r\nHowever, this issue is about taking a step to add those entries into the upstream feed. Doing that should close issue https:\/\/github.com\/kubernetes\/website\/issues\/45576.\r\n\r\n\/sig security\r\n\/committee security-response","comments":["This issue is currently awaiting triage.\n\nSIG Docs takes a lead on issue triage for this website, but any Kubernetes member can accept issues by applying the `triage\/accepted` label.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/transfer kubernetes","Seems the issues were created in their respective repos. But are missing from the official k8s CVE feed probably because there were no corresponding issues created in k\/k with the official-cve-feed label. \r\nhttps:\/\/github.com\/kubernetes\/ingress-nginx\/issues\/10571\r\nhttps:\/\/github.com\/kubernetes\/ingress-nginx\/issues\/10572\r\n\r\nWe have had to create issues in both the sub project and k\/k in the past. e.g. https:\/\/github.com\/kubernetes\/kubernetes\/issues\/118419\r\n\r\n@cjcullen "],"labels":["committee\/security-response","sig\/security","needs-triage"]},{"title":"[KEP2400]: Add swap to kubectl describe node's output","body":"#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR provides a way of easily inspecting the amount of swap memory that's available on nodes.\r\nThis has been an ongoing request from the community for a long time now. Although this data already exists as part of `\/stats\/summary` and `\/metrics\/resource` (see https:\/\/github.com\/kubernetes\/kubernetes\/pull\/118865), using `kubectl describe node` is much simpler and straight forward.\r\n\r\n##### Example output:\r\n```bash\r\n> kubectl describe node | grep Capacity -A15\r\nCapacity:\r\n  cpu:                40\r\n  ephemeral-storage:  1293695812Ki\r\n  hugepages-1Gi:      0\r\n  hugepages-2Mi:      0\r\n  memory:             394492620Ki\r\n  pods:               110\r\n  swap:               41943032Ki\r\nAllocatable:\r\n  cpu:                40\r\n  ephemeral-storage:  1293695812Ki\r\n  hugepages-1Gi:      0\r\n  hugepages-2Mi:      0\r\n  memory:             394492620Ki\r\n  pods:               110\r\n  swap:               41943032Ki\r\n```\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #123962\r\n\r\n#### Special notes for your reviewer:\r\nI've added swap as a `ResourceName`. However, it's important to note that it **cannot** be added to the pod spec.\r\n\r\nAnother alternative is to use \"swap\" as a string and cast it to `ResourceName`, but then we'd need to keep it as a constant string somewhere and remember it is actually used as a ResourceName.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nUpdated `kubectl describe node` to report the available amount of swap for Linux nodes.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-node\/2400-node-swap\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123963#\" title=\"Author self-approved\">iholder101<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [random-liu](https:\/\/github.com\/random-liu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"random-liu\"]} -->","\/sig node\r\n\/cc @kannon92 ","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","\/retest ","(nit)\r\nI recommend not putting backticks in commit messages (and, therefore, also not using them in PR titles)","Changelog suggestion\r\n```diff\r\n-It is now possible to get the amount of available swap on a node via `kubectl describe node`.\r\n+Updated `kubectl describe node` to report the available amount of swap.\r\n```\r\n\r\nIf this only applies to Linux nodes, consider stating that as well.","We could use `kubenetes.io\/linux-swap` as a resource to report - see https:\/\/k8s.io\/docs\/tasks\/administer-cluster\/extended-resource-node\/ for more about these.","Hey @sftim! Thanks for having a look!\r\n\r\n> I recommend not putting backticks in commit messages (and, therefore, also not using them in PR titles)\r\n\r\nDone :+1: \r\n\r\n> Changelog suggestion\r\n> If this only applies to Linux nodes, consider stating that as well.\r\n\r\nChanged to:\r\n```\r\nUpdated `kubectl describe node` to report the available amount of swap for Linux nodes.\r\n```\r\nWDYT?\r\n\r\n> We could use kubenetes.io\/linux-swap as a resource to report - see [k8s.io\/docs\/tasks\/administer-cluster\/extended-resource-node](https:\/\/k8s.io\/docs\/tasks\/administer-cluster\/extended-resource-node\/) for more about these.\r\n\r\nSo IIUC the purpose of extended resources is to add a resource that k8s does not understand and that the admin needs to manage. However, since swap is a resource that's managed by kubelet and that Kubernetes does understand I thought that an extended resources won't be a perfect fit here.\r\n\r\nAm I missing anything?","I can see how out of `swap` and `kubernetes.io\/swap`, neither feels like the obvious right choice. I don't mind `swap`; I wanted to point out that we have an alternative if we want it.","\/cc\r\n\r\nI think we should target this for 1.31. ","Unit tests pass locally, but failed on CI as one test failed on timeout. retrying.\r\n\/test pull-kubernetes-unit ","Swap would only be evaluated at kubelet start. Isn't the intention to have more swap added to the node after the kubelet starts?","I am somewhat worried about exposing this via publicly available APIs would lead the impression that scheduler takes into account swap - which it doesn't.","We added this based on feedback during KEP review process. https:\/\/github.com\/kubernetes\/enhancements\/pull\/4401#discussion_r1479124963\r\n\r\nThe other issue is that users may not use swap as swap will be only used in cases of cgroup pressure or memory pressure. ","@rphillips \r\n\r\n> Swap would only be evaluated at kubelet start. Isn't the intention to have more swap added to the node after the kubelet starts?\r\n\r\nWhile I agree it would be better if kubelet would be able to dynamically update the node's capacity, the same problem exists for other resources as well.\r\n\r\nFor example, it is possible to change the number of huge pages during runtime and the node's capabilities would not reflect that change.\r\n\r\nHere's a quick experiment I did. First let's check the node's 2Mi hugepages count:\r\n```bash\r\n> k describe node node01 | grep hugepages-2Mi\r\n  hugepages-2Mi:                  128Mi\r\n```\r\n\r\nOn the node:\r\n```bash\r\n[node01 ~]$ sysctl vm.nr_hugepages\r\nvm.nr_hugepages = 64\r\n```\r\n\r\nThat makes sense, since 62*2Mi == 128Mi.\r\nNow let's change the number of huge pages on the node:\r\n```bash\r\n[node01 ~]$ sysctl -w vm.nr_hugepages=128\r\nvm.nr_hugepages = 128\r\n[node01 ~]$ sysctl vm.nr_hugepages\r\nvm.nr_hugepages = 128\r\n```\r\n\r\nBut the node's capacity does not change:\r\n```bash\r\n> k describe node node01 | grep hugepages-2Mi\r\n  hugepages-2Mi:                  128Mi\r\n```\r\n\r\nSo to conclude, I think that while this is an unfortunate limitation, it is consistent with how Capacity reporting works with other resources.","> I am somewhat worried about exposing this via publicly available APIs would lead the impression that scheduler takes into account swap - which it doesn't.\r\n\r\nI understand where you're coming from.\r\nHowever on the other hand, as opposed to other ResourceNames (memory, hugepages, etc) there is no way to explicitly ask for these resources. IOW, if a pod does not ask for swap, it's sensible to assume that swap capacity won't be a factor in its scheduling decisions.\r\n\r\nAnother point is that in the future we might want to add some API to explicitly ask for swap. If we'd choose to go down this path it might require that the scheduler would be aware of swap."],"labels":["area\/kubelet","area\/kubectl","sig\/node","release-note","size\/L","kind\/api-change","kind\/feature","sig\/apps","sig\/cli","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"[KEP-2400] Add ability to get node's swap capacity with `kubectl describe node`","body":"### What would you like to be added?\n\nAdd ability to get node's swap capacity with `kubectl describe node`\n\n### Why is this needed?\n\nKEP https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-node\/2400-node-swap","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node"],"labels":["sig\/node","kind\/feature","needs-triage"]},{"title":"rewrite verify publishing bot to golang","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nResurrect this [old PR](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/118727) to fix https:\/\/github.com\/kubernetes\/publishing-bot\/issues\/345\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123961#\" title=\"Author self-approved\">SD-13<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sataqiu](https:\/\/github.com\/sataqiu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sataqiu\"]} -->"],"labels":["size\/L","cncf-cla: yes","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-sig","do-not-merge\/needs-kind"]},{"title":"nfs umount gets stuck when pod is destroyed","body":"### What happened?\n\nwhen deleting a statefulset, the pod gets stuck in \"terminating\" state. The node where the pod is scheduled, develops a high iowait time. If another pod on that node fails to terminate then iowait increases with the multiple stuck nfs mounts. no way to clear the iowait, have to reboot the node.  `kubectl delete pods podname --force -n namespace` does remove the pod k8s, but does not fix the stuck nfs mount. If I dont't force delete the pod, the pod can't be restarted.\n\n### What did you expect to happen?\n\nwhen pod is deleted the node should be able to umount the nfs mount.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI can't reproduce the error. I have many statefulsets that are created and deleted with a random pod which gets stuck.\n\n### Anything else we need to know?\n\nI am mounting a nfs volume from an external nfs server. I am not using a nfs provisioner.\r\n\r\n```\r\nspec:\r\n  volumes:\r\n    - name: data\r\n      nfs:\r\n        server: nfs1.storage.server.com\r\n        path: \/home\/brad\r\n```\r\n\r\n\r\nnfs client config on node\r\n```\r\n[ NFSMount_Global_Options ]\r\n        #rsize=1048576\r\n        #wsize=1048576\r\n        soft\r\n        timeo=50\r\n        nfsvers=4.1\r\n        retry=5\r\n```\r\n\r\nnfs server export\r\n```\r\n\/home  10.10.30.0\/24(rw,sync,no_wdelay,no_root_squash,no_subtree_check,insecure)\r\n```\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n#Client Version: v1.28.7\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.2\r\n\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nbare metal\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n$ uname -a\r\nLinux adm-wk1 5.15.0-1051-kvm #56-Ubuntu SMP Thu Feb 8 23:30:16 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nkubeadm\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd 1.7.2\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\ncni - flannel\r\ncsi - none\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig storage"],"labels":["kind\/bug","sig\/storage","needs-triage"]},{"title":"Debian Bookworm installation not stable","body":"### What happened?\r\n\r\nUpon Installing Kubernetes from scratch the cluster is briefly reachable via kubectl announcing that there are not pods in the default namespace as is to be expected upon executing `kubectl get pods` .\r\nAfter some time passes  the installation itself will not be reachable any longer.\r\n\r\nWhat is special about this is that it is a single node, but used to work before version 1.25 (It is a toysetup and since I missed the window for the shift(why are only the latest in the repository anyways and not all the latest supported versions(Debian 11 will not be happy with that given that they ship containerd 1.4.))\r\n\r\nThis might be related to https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123837 .\r\n\r\n\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nThe cluster, like all the installations before is stable and can receive pods upon untainting master to schedule pods.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n```bash\r\nsystemctl mask swap.target \r\nmodprobe br_netfilter \r\necho \"net.bridge.bridge-nf-call-iptables = 1\" >> \/etc\/sysctl.conf \r\nsysctl -p \/etc\/sysctl.conf \r\n\r\napt-get update && \r\napt-get install -y curl gnupg-agent fail2ban &&\r\ncurl -fsSL https:\/\/pkgs.k8s.io\/core:\/stable:\/v1.29\/deb\/Release.key | gpg --dearmor -o \/etc\/apt\/keyrings\/kubernetes-apt-keyring.gpg &&\r\necho 'deb [signed-by=\/etc\/apt\/keyrings\/kubernetes-apt-keyring.gpg] https:\/\/pkgs.k8s.io\/core:\/stable:\/v1.29\/deb\/ \/' | sudo tee \/etc\/apt\/sources.list.d\/kubernetes.list && \r\ncurl https:\/\/baltocdn.com\/helm\/signing.asc | sudo apt-key add -  &&\r\necho \"deb https:\/\/baltocdn.com\/helm\/stable\/debian\/ all main\" | sudo tee \/etc\/apt\/sources.list.d\/helm-stable-debian.list &&\r\napt-get update && \r\napt-get install -y apt-transport-https ca-certificates curl && \r\napt install -y kubeadm kubectl containerd kubelet kubernetes-cni helm\r\n\r\ncat << EOF | sudo tee \/etc\/sysctl.d\/99-kubernetes-cri.conf\r\nnet.bridge.bridge-nf-call-iptables = 1\r\nnet.ipv4.ip_forward = 1\r\nnet.bridge.bridge-nf-call-ip6tables = 1\r\nEOF\r\n\r\n\r\n## this is optional, I ran it once or twice trying to debug, but the results were the same\r\n# mkdir -p \/etc\/containerd\r\n# containerd config default | sudo tee \/etc\/containerd\/config.toml\r\n# systemctl restart containerd\r\n\r\n\r\n\r\nsysctl --system && \r\necho 1 > \/proc\/sys\/net\/ipv4\/ip_forward &&\r\nsystemctl reboot \r\n\r\nkubeadm init --pod-network-cidr=10.217.0.0\/16   && \r\nmkdir $HOME\/.kube &&\r\ncat \/etc\/kubernetes\/admin.conf > $HOME\/.kube\/config &&\r\nkubectl taint node singleNode node-role.kubernetes.io\/control-plane:NoSchedule- \r\n```\r\n\r\n### Anything else we need to know?\r\n\r\nKeep in mind, this is a single node only(didn't give me any grief before though).\r\nThe installation is fresh so just the installation and the the installation of kubernetes. \r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# 1.29.3\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nfresh install of debian 12\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\nkubeadm\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ncontainerd 1.6\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `\/sig <group-name>`\n- `\/wg <group-name>`\n- `\/committee <group-name>`\n\nPlease see the [group list](https:\/\/git.k8s.io\/community\/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Whilst we're likely to work on this issue and spend at least some time looking into what'd help towards a fix, I'm not sure that Kubernetes + containerd + Debian is a _supported_ combination.\r\n\r\nSo far as I know, we - Kubernetes - don't publish a matrix of combinations of operating system, container runtime, network plugin, OCI-level runtime, Kubernetes version, CPU architecture showing which of those are officially supported. Formally, it would be a matrix of all zeroes (no official support).\r\n\r\nIf you want a set of tested packages for Debian, you may be able to get those from Debian itself (we also don't guarantee that you can).\r\n\r\nSo, there's no guarantee or commitment for it to work. Even so, those steps to reproduce are helpful and we may get the capacity to find out what's not working there. If you can tell us about the most recent Kubernetes release where the equivalent steps succeed, that's useful extra context.","Hello!\r\nThanks for the prompt reply.\r\n\r\nThe last version I know of where it worked was 1.25.x . other versions might also work leading up to 1.29, but I have not had any interaction with those. Hard to try out the other versions as the old repo is offline and the new one is only holding the latest version.\r\n\r\ncontainerd is listed as an alternative: https:\/\/kubernetes.io\/docs\/setup\/production-environment\/container-runtimes\/ to the now removed docker-shim.\r\nAlso, I created the node via kubeadm. So Debian only provided the tools which could've been gotten via means described on the k8s homepage as well. I doubt the repo-managers made some heavy modifications there if any at all.\r\nDebian is also listed as being supported https:\/\/kubernetes.io\/docs\/setup\/production-environment\/tools\/kubeadm\/create-cluster-kubeadm\/ as it is deb-based.\r\n\r\nSo from what I am seeing, unless I misjudged something, Debian + containerd + k8s-tooling should be a working combination .\r\n\r\n\/\/Edit: an educated guess: newer versions of k8s require containerd 1.6, maybe the ones requiring 1.4 will work, but then there is also the ticket with the potential iptables-issue. don't know if that is related to containerd."],"labels":["kind\/bug","needs-sig","needs-triage"]},{"title":"fix a deprecated explain in comment","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\nThis comment make me confused for a long time. In fact, Resync in Informer has not send list to apiserver now but only when ListAndWatch fail.\r\nWe should fix it for other developers.\r\n\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNot\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @terloo. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123958#\" title=\"Author self-approved\">terloo<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/triage accepted\r\n\/assign @alexzielenski "],"labels":["kind\/cleanup","sig\/api-machinery","release-note","size\/XS","cncf-cla: yes","needs-ok-to-test","needs-priority","triage\/accepted"]},{"title":"Do not bind webhook port if webhooks are not present","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nDon't bind to the webhook port if there are no webhooks. Since we don't start the server if there are no webhooks, there is no need to bind.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #120043 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @kmala. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123957#\" title=\"Author self-approved\">kmala<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [andrewsykim](https:\/\/github.com\/andrewsykim) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"andrewsykim\"]} -->"],"labels":["area\/cloudprovider","size\/L","release-note-none","cncf-cla: yes","needs-ok-to-test","sig\/cloud-provider","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"Both Endpoints and EndpointSlices not working with Service abstraction for external IPs","body":"### What happened?\r\n\r\nAs recommend in https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#services-without-selectors I did try to abstract access to an externally hosted mariaDB. However, both the recommend approach and the legacy approach with Endpoints did not work as expected. There is a slight difference however, in the EndpointSlice case the service description showed `Endpoints: <none>` while in the endpoints case it was empty, i.e. `Endpoints: `.\r\n\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nThe external database should be reachable through the service abstraction.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nIt is a clean K3s with --disable traefik, nginx ingress as well as cert-manager installed via helm, plus GitLab Agent for Kubernetes also using Helm.\r\n\r\nThe Service:\r\n\r\n``` yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: mariadb\r\nspec:\r\n  ports:\r\n    - protocol: TCP\r\n      port: 3306\r\n      targetPort: 3306\r\n```\r\n\r\nI've tried both, EndpointSlice:\r\n\r\n``` yaml\r\napiVersion: discovery.k8s.io\/v1\r\nkind: EndpointSlice\r\nmetadata:\r\n  name: mariadb-1\r\n  labels:\r\n    kubernetes.io\/service-name: mariadb\r\naddressType: IPv4\r\nports:\r\n  - name: mysql\r\n    port: 3306\r\n    protocol: TCP\r\nendpoints:\r\n  - addresses:\r\n    - 10.0.0.5\r\n```\r\n\r\n, and Endpoints:\r\n\r\n``` yaml\r\napiVersion: v1\r\nkind: Endpoints\r\nmetadata:\r\n  name: mariadb\r\nsubsets:\r\n  - addresses:\r\n      - ip: 10.0.0.5\r\n    ports:\r\n      - name: mysql\r\n        port: 3306\r\n```\r\n\r\nBoth were successfully deployed in the same namespace as the service and linked to the service.\r\n\r\n## case: EndpointSlices\r\n\r\n```\r\nkubectl get endpointslice -l kubernetes.io\/service-name=mariadb\r\n\r\nNAME        ADDRESSTYPE   PORTS   ENDPOINTS   AGE\r\nmariadb-1   IPv4          3306    10.0.0.5    3m29s\r\n```\r\n```\r\nkubectl describe service mariadb\r\n\r\nName:              mariadb\r\nNamespace:         keldysh-projects-workshop\r\nLabels:            app.kubernetes.io\/managed-by=Helm\r\nAnnotations:       meta.helm.sh\/release-name: service\r\n                   meta.helm.sh\/release-namespace: keldysh-projects-workshop\r\nSelector:          <none>\r\nType:              ClusterIP\r\nIP Family Policy:  SingleStack\r\nIP Families:       IPv4\r\nIP:                10.43.126.23\r\nIPs:               10.43.126.23\r\nPort:              <unset>  3306\/TCP\r\nTargetPort:        3306\/TCP\r\nEndpoints:         <none>\r\nSession Affinity:  None\r\nEvents:            <none>\r\n```\r\n\r\n## case: Endpoints\r\n\r\n```\r\nkubectl describe endpoints mariadb\r\n\r\nName:         mariadb\r\nNamespace:    keldysh-projects-workshop\r\nLabels:       app.kubernetes.io\/managed-by=Helm\r\nAnnotations:  meta.helm.sh\/release-name: service\r\n              meta.helm.sh\/release-namespace: keldysh-projects-workshop\r\nSubsets:\r\n  Addresses:          10.0.0.5\r\n  NotReadyAddresses:  <none>\r\n  Ports:\r\n    Name   Port  Protocol\r\n    ----   ----  --------\r\n    mysql  3306  TCP\r\n\r\nEvents:  <none>\r\n```\r\n```\r\nkubectl describe service mariadb\r\n\r\nName:              mariadb\r\nNamespace:         keldysh-projects-workshop\r\nLabels:            app.kubernetes.io\/managed-by=Helm\r\nAnnotations:       meta.helm.sh\/release-name: service\r\n                   meta.helm.sh\/release-namespace: keldysh-projects-workshop\r\nSelector:          <none>\r\nType:              ClusterIP\r\nIP Family Policy:  SingleStack\r\nIP Families:       IPv4\r\nIP:                10.43.126.23\r\nIPs:               10.43.126.23\r\nPort:              <unset>  3306\/TCP\r\nTargetPort:        3306\/TCP\r\nEndpoints:         \r\nSession Affinity:  None\r\nEvents:            <none>\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\nDNS is working in principle:\r\n\r\n```\r\nkubectl exec -i -t dnsutils -- nslookup mariadb\r\n\r\nServer:         10.43.0.10\r\nAddress:        10.43.0.10#53\r\n\r\nName:   mariadb.keldysh-projects-workshop.svc.cluster.local\r\nAddress: 10.43.19.67\r\n```\r\n\r\n\r\nI already submitted a bug report in the K3s repository. They however claim to not change the core kubernetes functionality:\r\nhttps:\/\/github.com\/k3s-io\/k3s\/issues\/9725\r\nhttps:\/\/github.com\/k3s-io\/k3s\/discussions\/9726\r\n\r\n### Kubernetes version\r\n\r\n\r\nK3s Version:\r\nk3s version v1.28.7+k3s1 (https:\/\/github.com\/k3s-io\/k3s\/commit\/051b14b248655896fdfd7ba6c93db6182cde7431)\r\ngo version go1.21.7\r\n\r\n\r\n### Cloud provider\r\n\r\nSelf hosted K3s on Hetzner Cloud Servers\r\n\r\n### OS version\r\n\r\nLinux kubernetes-test-server 5.15.0-91-generic https:\/\/github.com\/k3s-io\/k3s\/issues\/101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n","comments":["Can you explain exactly what you expect to happen? is not very clear from your description,\r\n\r\ndo you expect that a pod that tries to connect to the Service ClusterIP `10.43.0.10` ends connected to the IP `10.0.0.5` ?\r\n\r\n~Have you verified all connectivity works without services too?~\r\n\r\n@PMacho assuming you are using kube-proxy, can you please paste an `iptables-save` from one of the worker nodes ?","@aojea \r\n\r\n\r\nThe documentation of \"Services without Selectors\" https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#service-no-selector-access explicitly states \r\n> Accessing a Service without a selector works the same as if it had a selector. In the [example](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#services-without-selectors) for a Service without a selector, traffic is routed to one of the two endpoints defined in the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.\r\n\r\nWhere the \"example\" defines:\r\n\r\n``` yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-service\r\nspec:\r\n  ports:\r\n    - protocol: TCP\r\n      port: 80\r\n      targetPort: 9376\r\n```\r\n\r\nand \r\n``` yaml\r\napiVersion: discovery.k8s.io\/v1\r\nkind: EndpointSlice\r\nmetadata:\r\n  name: my-service-1 # by convention, use the name of the Service\r\n                     # as a prefix for the name of the EndpointSlice\r\n  labels:\r\n    # You should set the \"kubernetes.io\/service-name\" label.\r\n    # Set its value to match the name of the Service\r\n    kubernetes.io\/service-name: my-service\r\naddressType: IPv4\r\nports:\r\n  - name: '' # empty because port 9376 is not assigned as a well-known\r\n             # port (by IANA)\r\n    appProtocol: http\r\n    protocol: TCP\r\n    port: 9376\r\nendpoints:\r\n  - addresses:\r\n      - \"10.4.5.6\"\r\n  - addresses:\r\n      - \"10.1.2.3\"\r\n```\r\n\r\nTherefore, in my configuration I expect `r2dbc:mariadb:\/\/mariadb` to be routed to `10.0.0.5`, instead of complaining `Connection refused: mariadb.debug-endpoints.svc.cluster.local\/10.43.202.125:3306`.\r\n\r\nFor the record, defining the service to be headless, i.e. `spec.clusterIP: \"None\"`, does route to `10.0.0.5`. However, https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#headless-services states: \r\n> Sometimes you don't need load-balancing and a single Service IP.\r\nConsidering that the Database may very well be a cluster, this does not necessarily apply.\r\n\r\n\r\nThe requested output of `iptables save`, with the above configuration deployed to namespace `debug-endpoints`:\r\n[iptables-save.txt](https:\/\/github.com\/kubernetes\/kubernetes\/files\/14624940\/iptables-save.txt)","interesting, it seems you are using kube-proxy and kube-proxy does not process those endpoints\r\n\r\n> -A KUBE-SERVICES -d 10.43.202.125\/32 -p tcp -m comment --comment \"debug-endpoints\/mariadb has no endpoints\" -m tcp --dport 3306 -j REJECT --reject-with icmp-port-unreachable\r\n\r\nI'm in my way to kubecon so no much time until the week after, but this is a good issue for networking contributors to help, it seems pretty simple to reproduce as you are already providing the steps ... once reproduced is a matter of understanding why the kube-proxy does not see those EndpointSlices\r\n\r\n@aroradaman @uablrek you are quite familiar with the proxy code, maybe you can take a look?\r\n\/cc @danwinship ","\/sig network\r\n\/triage accepted","> interesting, it seems you are using kube-proxy and kube-proxy does not process those endpoints\r\n> \r\n> > -A KUBE-SERVICES -d 10.43.202.125\/32 -p tcp -m comment --comment \"debug-endpoints\/mariadb has no endpoints\" -m tcp --dport 3306 -j REJECT --reject-with icmp-port-unreachable\r\n> \r\n> I'm in my way to kubecon so no much time until the week after, but this is a good issue for networking contributors to help, it seems pretty simple to reproduce as you are already providing the steps ... once reproduced is a matter of understanding why the kube-proxy does not see those EndpointSlices\r\n\r\nThank you for the positive validation. I wish you a great time in Paris.","\/assign","@PMacho I see you are defining ports with a name in the endpointslice but without a name in the service which is breaking our internal mapping of service to endpoints. \r\n\r\nMaking both the ports either named or un-named should solve the issue. \r\n\r\nquick fix:\r\n```\r\nkubectl patch service mariadb --type='json' -p='[{\"op\": \"add\", \"path\": \"\/spec\/ports\/0\/name\", \"value\": \"mysql\"}]'\r\n```\r\n\r\n","Maybe the docs could use a tweak to recommend setting a name for port 80?","> Maybe the docs could use a tweak to recommend setting a name for port 80?\r\n\r\nyes, will update the doc.","@aroradaman \r\n\r\nYour quick fix indeed did the job. It will certainly be useful for other users to update the docs.\r\n\r\nFor the record, I do not know if this is intended but to my understanding the description of the service does not reflect its behavior since `Endpoints` are still `<none>` (even though the routing actually works now):\r\n\r\n``` bash\r\nkubectl describe service mariadb -n debug-endpoints               \r\nName:              mariadb\r\nNamespace:         debug-endpoints\r\nLabels:            app.kubernetes.io\/managed-by=Helm\r\nAnnotations:       meta.helm.sh\/release-name: kubernetes-debug-service\r\n                   meta.helm.sh\/release-namespace: debug-endpoints\r\nSelector:          <none>\r\nType:              ClusterIP\r\nIP Family Policy:  SingleStack\r\nIP Families:       IPv4\r\nIP:                10.43.202.125\r\nIPs:               10.43.202.125\r\nPort:              mysql  3306\/TCP\r\nTargetPort:        3306\/TCP\r\nEndpoints:         <none>\r\nSession Affinity:  None\r\nEvents:            <none>\r\n```","If you manually create EndpointSlices, the associated Endpoints for the same Service doesn't get autopopulated (and that's where `kubectl` is looking).","If you create endpoint instead of endpointslice then you can see the desired endpoints in kubectl describe output.\r\nAnd endpointslice mirror controller will create one endpointslice for your endpoint. ","> If you manually create EndpointSlices, the associated Endpoints for the same Service doesn't get autopopulated (and that's where `kubectl` is looking).\r\n\r\noh, that's no good. It should look at the EndpointSlices. Hey @aroradaman wanna fix that too? :slightly_smiling_face: ","> > If you manually create EndpointSlices, the associated Endpoints for the same Service doesn't get autopopulated (and that's where `kubectl` is looking).\r\n> \r\n> oh, that's no good. It should look at the EndpointSlices. Hey @aroradaman wanna fix that too? \ud83d\ude42\r\n\r\nsure, will look into this."],"labels":["kind\/bug","sig\/network","triage\/accepted"]},{"title":"Figure out what to do with long running serial tests","body":"As we were investigating https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123589, we discovered that there are quite a few jobs that take up much more than the average to run.\r\n\r\nI created https:\/\/docs.google.com\/spreadsheets\/d\/1V8ezpBUo9xoZcaDrsuSwsK1baQXLioawutp5Y_-hWjs\/edit?usp=sharing to write down which tests are taking up the most time.\r\n\r\nFrom this doc, I see a few action items.\r\n\r\n- [ ] CpuManager\r\n  - CPU Manager [Serial] [Feature:CPUManager] With kubeconfig updated with static CPU Manager policy run the CPU Manager tests should assign CPUs as expected based on the Pod spec takes 150 s\r\n  - [sig-node] CPU Manager Metrics [Serial] [Feature:CPUManager] when querying \/metrics should report zero pinning counters after a fresh restart [sig-node, Serial, Feature:CPUManager] takes 450 s\r\n      - This test is partially skipped due to some hardware limitations\r\n      - Turns out there is an existing job for this one.\r\n- [ ] TopologyManager\r\n  -  [sig-node] Topology Manager [Serial] [Feature:TopologyManager] With kubeconfig updated to static CPU Manager policy run the Topology Manager tests run Topology Manager policy test suite [sig-node, Serial, Feature:TopologyManager] takes around 1000 s\r\n  - Existing job exists for this one also.\r\n- [ ] MemoryManager\r\n  - [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] when querying \/metrics should not report any pinning failures when the memorymanager allocation is expected to succeed [sig-node, Serial, Feature:MemoryManager] takes 1000 s\r\n  - [sig-node] Memory Manager [Disruptive] [Serial] [Feature:MemoryManager] with static policy when guaranteed pod has init and app containers should succeed to start the pod [sig-node, Disruptive, Serial, Feature:MemoryManager] takes 1012 s\r\n  - existing job exists for this one also.\r\n- [ ] PodResources\r\n  -  A lot of these tests take a long time \r\n- [ ] DevicePlugin\r\n  - Same here. \r\n- [x] SystemNodeCriticalTest\r\n  - [sig-node] SystemNodeCriticalPod [Slow] [Serial] [Disruptive] [NodeFeature:SystemNodeCriticalPod] when create a system-node-critical pod  should not be evicted upon DiskPressure [sig-node, Slow, Serial, Disruptive, NodeFeature:SystemNodeCriticalPod]\r\n  - Opened up https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123950 to move this to eviction tests   ","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig testing\r\n\/sig node","Most of the manager jobs are actually in their own test lane..\r\n\r\n- https:\/\/testgrid.k8s.io\/sig-node-kubelet#kubelet-serial-gce-e2e-cpu-manager\r\n- https:\/\/testgrid.k8s.io\/sig-node-kubelet#kubelet-serial-gce-e2e-memory-manager\r\n- https:\/\/testgrid.k8s.io\/sig-node-kubelet#kubelet-serial-gce-e2e-topology-manager\r\n\r\nOne thing we could do is filter out these tests out of the general serial job as we are already covering them. And these test has a beefer image so we are able to run most of them..","Opened up https:\/\/github.com\/kubernetes\/test-infra\/pull\/32271 to move managers out of this general serial job. We have these tests covered already in kubelet. We don't have a corresponding crio job but I don't think these features are dependent on container runtime."],"labels":["sig\/node","sig\/testing","needs-triage"]},{"title":"pkg\/kubelet\/kuberuntime: Add userns tests for NamespacesForPod","body":"As requested by @tallclair here:\r\n\thttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/123909\/files#r1523599275\r\n\r\ncc @giuseppe @AkihiroSuda @tallclair @mrunalp \r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nCover more code with tests\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123952#\" title=\"Author self-approved\">rata<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->","\/priority important-soon\r\n\/triage accepted\r\n\r\nMarking it as important soon as it was called out during the kep freeze. I think this change should be safe to include during test-freeze as it only impacts unit tests.","\/retest","\/retest","LGTM label has been added.  <details>Git tree hash: 530cb660e1327de9c3a1bc0e1dfe97e8e2b3b331<\/details>","\/assign @mrunalp @dchen1107"],"labels":["priority\/important-soon","area\/kubelet","kind\/cleanup","lgtm","sig\/node","size\/L","release-note-none","cncf-cla: yes","triage\/accepted"]},{"title":"Failure pod-resize-scheduler-tests in ci-kubernetes-e2e-ec2-alpha-features","body":"### Failure cluster [885be54a24e8bdd18cfb](https:\/\/go.k8s.io\/triage#885be54a24e8bdd18cfb)\r\n\r\n##### Error text:\r\n```\r\n[FAILED] Timed out after 300.001s.\r\nExpected Pod to be in <v1.PodPhase>: \"Running\"\r\n```\r\n#### Recent failures:\r\n[3\/15\/2024, 8:07:58 AM ci-kubernetes-e2e-ec2-alpha-features](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ec2-alpha-features\/1768534311335628800)\r\n[3\/15\/2024, 2:06:56 AM ci-kubernetes-e2e-ec2-alpha-features](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ec2-alpha-features\/1768443461721133056)\r\n[3\/14\/2024, 8:06:52 PM ci-kubernetes-e2e-ec2-alpha-features](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ec2-alpha-features\/1768352846178160640)\r\n[3\/14\/2024, 2:06:15 PM ci-kubernetes-e2e-ec2-alpha-features](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ec2-alpha-features\/1768262098967597056)\r\n[3\/14\/2024, 8:06:15 AM ci-kubernetes-e2e-ec2-alpha-features](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ec2-alpha-features\/1768171500910678016)\r\n\r\n\r\n\/kind failing-test\r\n\r\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @dims "],"labels":["sig\/node","kind\/failing-test","needs-triage"]},{"title":"move system node critical test to eviction test lane","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nMove system_critical test to eviction as it is testing disk pressure.\r\n\r\nIt is a long test so trying to move it out of general serial test.\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-ec2-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-cloud-provider-loadbalancer`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-containerd-flaky`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-conformance-fedora-serial`\n* `\/test pull-kubernetes-node-swap-conformance-ubuntu-serial`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123950#issuecomment-1999803065):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123950#\" title=\"Author self-approved\">kannon92<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sergeykanzhelev](https:\/\/github.com\/sergeykanzhelev) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sergeykanzhelev\"]} -->","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction\r\n\/test pull-kubernetes-node-kubelet-serial-containerd\r\n","\/cc @bart0sh","\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1","\/test pull-kubernetes-unit","\/assign @mrunalp @dims \r\n\r\nOpening this up so we can make some room. This test can take anywhere up to 10-30 minutes in CI. It is testing eviction for a static pod. Since our eviction tests are really set up for this already, I think moving this to eviction makes sense.\r\n\r\nI can confirm that this test is covered in the eviction job while it is dropped in the serial jobs now.","\/triage accepted\r\n\/priority important-longterm","@kannon92: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-kubelet-serial-containerd | 0bdc4c3911288938ccfdd82bb23154e71d08faa2 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123950\/pull-kubernetes-node-kubelet-serial-containerd\/1768647583682531328) | false | `\/test pull-kubernetes-node-kubelet-serial-containerd`\npull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction | 0bdc4c3911288938ccfdd82bb23154e71d08faa2 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123950\/pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction\/1768647583737057280) | false | `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\npull-kubernetes-node-kubelet-serial-crio-cgroupv1 | 0bdc4c3911288938ccfdd82bb23154e71d08faa2 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123950\/pull-kubernetes-node-kubelet-serial-crio-cgroupv1\/1768671241448722432) | false | `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123950). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Akannon92). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","@kannon92 In my last serial job run this test took 10 minutes: https:\/\/storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/120459\/pull-kubernetes-node-kubelet-serial-crio-cgroupv1\/1769356436250300416\/build-log.txt\r\n\r\n\/hold\r\nI'd propose to wait until https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123589 is fixed.\r\nIts last run with almost everything enabled [took 2hours](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123589#issuecomment-2002597106)","IMO I still think I'd prefer to move this test to eviction anyway. It reuses the eviction manager code and even tests DiskPressure.\r\n\r\nLogically I would prefer it in the eviction tests rather. ","ok, unholding\r\n\/unhold\r\n\/lgtm\r\n","LGTM label has been added.  <details>Git tree hash: 5853175f2d8bc94b9aa2696a7add87351cf87cea<\/details>"],"labels":["area\/test","kind\/cleanup","lgtm","sig\/node","size\/XS","release-note-none","cncf-cla: yes","sig\/testing","priority\/important-longterm","triage\/accepted"]},{"title":"image_gc_test: Increase ImageMaximumGCAge to 3 minutes","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\/kind failing-test\r\n\/kind flake\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nImageGarbageCollect Serial with existing ImageMaximumGCAge set to 1 minute fail because KubeletRestart takes to much time resulting to GC unused images prematurely.\r\n\r\nThis commit tries to fix this by increasing ImageMaximumGCAge to 3 minutes, thus giving it sufficient time for the test to pass.\r\n\r\n3 minutes was selected to cover the worst case scenario for waitForKubeletToStart timeouts\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/fa07055b1f35a5c316e95c7e620c28ed44d890d5\/test\/e2e_node\/util.go#L240-L252\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\n[Failed run](https:\/\/storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/123386\/pull-kubernetes-node-kubelet-serial-crio-cgroupv1\/1765800552832176128\/build-log.txt)\r\n\r\n\r\n```\r\nSTEP: Destroying namespace \"image-garbage-collect-test-9548\" for this suite. @ 03\/07\/24 20:22:56.18\r\n\u2022 [FAILED] [1408.810 seconds]\r\n[sig-node] ImageGarbageCollect [Serial] [NodeFeature:GarbageCollect] when ImageMaximumGCAge is set [It] should not GC unused images prematurely [sig-node, Serial, NodeFeature:GarbageCollect]\r\nk8s.io\/kubernetes\/test\/e2e_node\/image_gc_test.go:85\r\n\r\n  [FAILED] Failed after 0.001s.\r\n  Expected\r\n      <int>: 1\r\n  to equal\r\n      <int>: 19\r\n  In [It] at: k8s.io\/kubernetes\/test\/e2e_node\/image_gc_test.go:106 @ 03\/07\/24 20:13:44.367\r\n```\r\n\r\n#### Special notes for your reviewer:\r\n\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @esotsal. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123949#\" title=\"Author self-approved\">esotsal<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sjenning](https:\/\/github.com\/sjenning) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sjenning\"]} -->","\/ok-to-test","\/retest","@esotsal Could this slowness be caused by https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123589 ?\r\nI never saw `ImageGarbageCollect` test failing after fixing the slowness in both containerd and crio serial jobs.","> @esotsal Could this slowness be caused by #123589 ? I never saw `ImageGarbageCollect` test failing after fixing the slowness in both containerd and crio serial jobs.\r\n\r\nPerhaps,  1 clusters of 2 failures out of 136336 builds from 3\/6\/2024, 1:00:11 AM to 3\/20\/2024, 1:05:15 PM according to [k8s-triage history](https:\/\/storage.googleapis.com\/k8s-triage\/index.html?sig=node&text=when%20ImageMaximumGCAge%20is%20set%20should%20not%20GC%20unused%20images%20prematurely)","I'm not sure this very rare flake worth fixing. However, as the change is tiny, I'm ok with this if other reviewers think it makes sense. @kannon92 WDYT?"],"labels":["area\/test","kind\/cleanup","sig\/node","size\/XS","kind\/flake","release-note-none","cncf-cla: yes","sig\/testing","kind\/failing-test","ok-to-test","needs-priority","needs-triage"]},{"title":"Internal dns (svc.cluster.local) doesn't work after applying ipvs mode.","body":"### What happened?\r\n\r\nHello, i am new to k8s. First, i apologize for my English skill. :-(\r\n\r\nI have trouble with accessing to service object from pods in another namespaces after **applying ipvs mode**. My nginx pod logs like below.\r\n\r\n```bash\r\nhost not found in upstream \"<svc-name>.<namespace-name>.svc.cluster.local:<port>\"\r\n```\r\nHowever, It looks possible that accessing specific service objects from outside of cluster. (e.g. accessing nginx `NodePort` service object, accessing `ClusterIP` service object via portforwarding)\r\n\r\n\r\nAnd then, i have installed `ipvsadm`, i loaded some kind of modules to activating ipvs mode.\r\n```bash\r\nuser@randomhost:~$ lsmod | grep ip_vs\r\nip_vs_sh               12288  0\r\nip_vs_wrr              12288  0\r\nip_vs_rr               12288  49\r\nip_vs_lc               12288  0\r\nip_vs                 221184  57 ip_vs_rr,ip_vs_sh,ip_vs_wrr,ip_vs_lc\r\nnf_conntrack          200704  6 xt_conntrack,nf_nat,xt_nat,nf_conntrack_netlink,xt_MASQUERADE,ip_vs\r\nnf_defrag_ipv6         24576  2 nf_conntrack,ip_vs\r\nlibcrc32c              12288  4 nf_conntrack,nf_nat,nf_tables,ip_vs\r\n```\r\n\r\n```bash\r\nuser@randomhost:~$ sudo ipvsadm -ln\r\nIP Virtual Server version 1.2.1 (size=4096)\r\nProt LocalAddress:Port Scheduler Flags\r\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\r\nTCP  10.10.50.6:30010 rr\r\n  -> 10.244.240.231:80            Masq    1      0          0         \r\nTCP  10.10.50.6:30020 rr\r\nTCP  10.10.50.6:30040 rr\r\n  -> 10.244.240.210:8080          Masq    1      0          0         \r\nTCP  10.10.50.6:30100 rr\r\n  -> 10.244.240.203:8080          Masq    1      0          0         \r\nTCP  10.10.50.6:31580 rr\r\n  -> 10.244.240.217:9090          Masq    1      0          0         \r\nTCP  10.96.0.1:443 rr\r\n  -> 10.10.50.6:6443              Masq    1      2          0         \r\nTCP  10.96.0.10:53 rr\r\n  -> 10.244.169.6:53              Masq    1      0          0         \r\n  -> 10.244.240.215:53            Masq    1      0          0         \r\nTCP  10.96.0.10:9153 rr\r\n  -> 10.244.169.6:9153            Masq    1      0          0         \r\n  -> 10.244.240.215:9153          Masq    1      0          0         \r\nTCP  10.96.66.35:8080 rr\r\n  -> 10.244.240.199:8080          Masq    1      0          0         \r\nTCP  10.96.77.112:80 rr\r\n  -> 10.244.240.217:9090          Masq    1      0          0         \r\nTCP  10.96.197.131:8080 rr\r\n  -> 10.244.240.255:8080          Masq    1      0          0         \r\nTCP  10.98.190.64:9400 rr\r\n  -> 10.244.240.205:9400          Masq    1      0          0         \r\nTCP  10.98.211.200:9091 rr\r\n  -> 10.244.240.246:9091          Masq    1      0          0         \r\nTCP  10.99.124.7:8080 rr\r\n  -> 10.244.240.213:8080          Masq    1      0          0         \r\nTCP  10.100.12.75:9093 rr\r\n  -> 10.244.240.220:9093          Masq    1      0          0         \r\nTCP  10.100.42.9:8080 rr\r\n  -> 10.244.240.225:8080          Masq    1      0          0         \r\nTCP  10.100.182.211:6379 rr\r\n  -> 10.244.240.230:6379          Masq    1      0          0         \r\nTCP  10.100.213.41:9100 rr\r\n  -> 10.10.50.5:9100              Masq    1      0          0         \r\n  -> 10.10.50.6:9100              Masq    1      0          0         \r\nTCP  10.100.216.98:8080 rr\r\n  -> 10.244.240.198:8080          Masq    1      0          0         \r\nTCP  10.100.223.0:8080 rr\r\n  -> 10.244.240.209:8080          Masq    1      0          0         \r\nTCP  10.101.100.218:8080 rr\r\n  -> 10.244.240.238:8080          Masq    1      0          0         \r\nTCP  10.101.139.28:9001 rr\r\n  -> 10.244.240.235:9001          Masq    1      0          0         \r\nTCP  10.101.215.114:80 rr\r\n  -> 10.244.240.193:8080          Masq    1      0          0         \r\nTCP  10.101.215.114:443 rr\r\n  -> 10.244.240.193:8080          Masq    1      0          0         \r\nTCP  10.102.197.12:9114 rr\r\nTCP  10.103.254.124:8081 rr\r\n  -> 10.244.240.229:8081          Masq    1      0          0         \r\nTCP  10.104.27.215:8080 rr\r\n  -> 10.244.240.203:8080          Masq    1      0          0         \r\nTCP  10.104.234.123:8080 rr\r\n  -> 10.244.169.4:8080            Masq    1      0          0         \r\nTCP  10.105.53.9:8080 rr\r\n  -> 10.244.240.222:8080          Masq    1      0          0         \r\n  -> 10.244.240.223:8080          Masq    1      0          0         \r\n  -> 10.244.240.224:8080          Masq    1      0          0         \r\n  -> 10.244.240.228:8080          Masq    1      0          0         \r\n  -> 10.244.240.234:8080          Masq    1      0          0         \r\n  -> 10.244.240.237:8080          Masq    1      0          0         \r\n  -> 10.244.240.240:8080          Masq    1      0          0         \r\n  -> 10.244.240.241:8080          Masq    1      0          0         \r\nTCP  10.106.23.135:80 rr\r\n  -> 10.244.240.231:80            Masq    1      0          0         \r\nTCP  10.107.149.189:8080 rr\r\n  -> 10.244.240.194:8080          Masq    1      0          0         \r\nTCP  10.109.119.195:5556 rr\r\n  -> 10.244.240.254:5556          Masq    1      0          0         \r\nTCP  10.109.119.195:5557 rr\r\n  -> 10.244.240.254:5557          Masq    1      0          0         \r\nTCP  10.109.161.105:8080 rr\r\n  -> 10.244.240.210:8080          Masq    1      0          0         \r\nTCP  10.110.137.110:80 rr\r\nTCP  10.110.152.189:8080 rr\r\n  -> 10.244.169.3:8080            Masq    1      0          0         \r\nTCP  10.110.204.48:7000 rr\r\n  -> 10.244.240.216:7000          Masq    1      0          0         \r\nTCP  10.111.130.154:80 rr\r\n  -> 10.244.240.253:3000          Masq    1      0          0         \r\nTCP  10.244.0.0:30010 rr\r\n  -> 10.244.240.231:80            Masq    1      0          0         \r\nTCP  10.244.0.0:30020 rr\r\nTCP  10.244.0.0:30040 rr\r\n  -> 10.244.240.210:8080          Masq    1      0          0         \r\nTCP  10.244.0.0:30100 rr\r\n  -> 10.244.240.203:8080          Masq    1      0          0         \r\nTCP  10.244.0.0:31580 rr\r\n  -> 10.244.240.217:9090          Masq    1      0          0         \r\nTCP  10.244.169.0:30010 rr\r\n  -> 10.244.240.231:80            Masq    1      0          0         \r\nTCP  10.244.169.0:30020 rr\r\nTCP  10.244.169.0:30040 rr\r\n  -> 10.244.240.210:8080          Masq    1      0          0         \r\nTCP  10.244.169.0:30100 rr\r\n  -> 10.244.240.203:8080          Masq    1      0          0         \r\nTCP  10.244.169.0:31580 rr\r\n  -> 10.244.240.217:9090          Masq    1      0          0         \r\nUDP  10.96.0.10:53 rr\r\n  -> 10.244.169.6:53              Masq    1      0          0         \r\n  -> 10.244.240.215:53            Masq    1      0          0\r\n```\r\n\r\nIs there something i omitted?\r\nI appreciate that read this issue.\r\n\r\n### What did you expect to happen?\r\n\r\nI want to access service object in another namespace via (`svc.cluster.local`) from another pod.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. initialize k8s cluster via `kubeadm` (version : 1.28.x)\r\n2. install calico CNI (I installed flannel CNI at first time and then migrate to calico CNI)\r\n3. Edit configmap `kube-proxy`\r\n```yaml\r\nmode: \"ipvs\"\r\n```\r\n4. Delete `kube-proxy` pods and waiting for reproduce.\r\n\r\n### Anything else we need to know?\r\n\r\n#### Configmap for kube-proxy\r\n```bash\r\nuser@randomhost:~$ kubectl describe configmap kube-proxy -n kube-system\r\n\r\napiVersion: kubeproxy.config.k8s.io\/v1alpha1\r\nbindAddress: 0.0.0.0\r\nbindAddressHardFail: false\r\nclientConnection:\r\n  acceptContentTypes: \"\"\r\n  burst: 0\r\n  contentType: \"\"\r\n  kubeconfig: \/var\/lib\/kube-proxy\/kubeconfig.conf\r\n  qps: 0\r\nclusterCIDR: 10.244.0.0\/16\r\nconfigSyncPeriod: 0s\r\nconntrack:\r\n  maxPerCore: null\r\n  min: null\r\n  tcpCloseWaitTimeout: null\r\n  tcpEstablishedTimeout: null\r\ndetectLocal:\r\n  bridgeInterface: \"\"\r\n  interfaceNamePrefix: \"\"\r\ndetectLocalMode: \"\"\r\nenableProfiling: false\r\nhealthzBindAddress: \"\"\r\nhostnameOverride: \"\"\r\niptables:\r\n  localhostNodePorts: null\r\n  masqueradeAll: false\r\n  masqueradeBit: null\r\n  minSyncPeriod: 0s\r\n  syncPeriod: 0s\r\nipvs:\r\n  excludeCIDRs: null\r\n  minSyncPeriod: 0s\r\n  scheduler: \"\"\r\n  strictARP: false\r\n  syncPeriod: 0s\r\n  tcpFinTimeout: 0s\r\n  tcpTimeout: 0s\r\n  udpTimeout: 0s\r\nkind: KubeProxyConfiguration\r\nlogging:\r\n  flushFrequency: 0\r\n  options:\r\n    json:\r\n      infoBufferSize: \"0\"\r\n  verbosity: 0\r\nmetricsBindAddress: \"\"\r\nmode: \"\"\r\nnodePortAddresses: null\r\noomScoreAdj: null\r\nportRange: \"\"\r\nshowHiddenMetricsForVersion: \"\"\r\nwinkernel:\r\n  enableDSR: false\r\n  forwardHealthCheckVip: false\r\n  networkName: \"\"\r\n  rootHnsEndpointName: \"\"\r\n  sourceVip: \"\"\r\nkubeconfig.conf:\r\n```\r\n#### `\/etc\/resolve.conf`\r\n```bash\r\nuser@randomhost:~$ cat \/etc\/resolv.conf\r\n# This is \/run\/systemd\/resolve\/stub-resolv.conf managed by man:systemd-resolved(8).\r\n# Do not edit.\r\n#\r\n# This file might be symlinked as \/etc\/resolv.conf. If you're looking at\r\n# \/etc\/resolv.conf and seeing this text, you have followed the symlink.\r\n#\r\n# This is a dynamic resolv.conf file for connecting local clients to the\r\n# internal DNS stub resolver of systemd-resolved. This file lists all\r\n# configured search domains.\r\n#\r\n# Run \"resolvectl status\" to see details about the uplink DNS servers\r\n# currently in use.\r\n#\r\n# Third party programs should typically not access this file directly, but only\r\n# through the symlink at \/etc\/resolv.conf. To manage man:resolv.conf(5) in a\r\n# different way, replace this symlink by a static file or a different symlink.\r\n#\r\n# See man:systemd-resolved.service(8) for details about the supported modes of\r\n# operation for \/etc\/resolv.conf.\r\n\r\nnameserver 127.0.0.53\r\noptions edns0 trust-ad\r\nsearch .\r\nsearch 8.8.4.4\r\nnameserver 8.8.8.8\r\n```\r\n\r\n#### Pod log for kube-proxy \r\n```bash\r\nuser@randomhost:~$ kubectl logs -f kube-proxy-ptwtv -n kube-system\r\nI0315 01:08:48.212186       1 node.go:141] Successfully retrieved node IP: 10.10.50.6\r\nI0315 01:08:48.213268       1 conntrack.go:52] \"Setting nf_conntrack_max\" nfConntrackMax=131072\r\nI0315 01:08:48.223455       1 server.go:632] \"kube-proxy running in dual-stack mode\" primary ipFamily=\"IPv4\"\r\nI0315 01:08:48.229596       1 server_others.go:218] \"Using ipvs Proxier\"\r\nI0315 01:08:48.229722       1 server_others.go:421] \"Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family\" ipFamily=\"IPv6\"\r\nI0315 01:08:48.229743       1 server_others.go:438] \"Defaulting to no-op detect-local\"\r\nI0315 01:08:48.229928       1 ipset.go:116] \"Ipset name truncated\" ipSetName=\"KUBE-6-LOAD-BALANCER-SOURCE-CIDR\" truncatedName=\"KUBE-6-LOAD-BALANCER-SOURCE-CID\"\r\nI0315 01:08:48.229955       1 ipset.go:116] \"Ipset name truncated\" ipSetName=\"KUBE-6-NODE-PORT-LOCAL-SCTP-HASH\" truncatedName=\"KUBE-6-NODE-PORT-LOCAL-SCTP-HAS\"\r\nI0315 01:08:48.229988       1 server.go:846] \"Version info\" version=\"v1.28.6\"\r\nI0315 01:08:48.229998       1 server.go:848] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nI0315 01:08:48.230266       1 config.go:315] \"Starting node config controller\"\r\nI0315 01:08:48.230292       1 shared_informer.go:311] Waiting for caches to sync for node config\r\nI0315 01:08:48.230403       1 config.go:188] \"Starting service config controller\"\r\nI0315 01:08:48.230411       1 shared_informer.go:311] Waiting for caches to sync for service config\r\nI0315 01:08:48.230420       1 config.go:97] \"Starting endpoint slice config controller\"\r\nI0315 01:08:48.230421       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config\r\nI0315 01:08:48.331128       1 shared_informer.go:318] Caches are synced for endpoint slice config\r\nI0315 01:08:48.331140       1 shared_informer.go:318] Caches are synced for node config\r\nI0315 01:08:48.331172       1 shared_informer.go:318] Caches are synced for service config\r\n```\r\n\r\n#### iptables\r\n```bash\r\nuser@randomhost:~$ sudo iptables --list\r\nChain INPUT (policy ACCEPT)\r\ntarget     prot opt source               destination         \r\ncali-INPUT  all  --  anywhere             anywhere             \/* cali:Cz_u1IQiXIMmKD4c *\/\r\nKUBE-IPVS-FILTER  all  --  anywhere             anywhere             \/* kubernetes ipvs access filter *\/\r\nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             \/* kube-proxy firewall rules *\/\r\nKUBE-NODE-PORT  all  --  anywhere             anywhere             \/* kubernetes health check rules *\/\r\nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes load balancer firewall *\/\r\nKUBE-NODEPORTS  all  --  anywhere             anywhere             \/* kubernetes health check service ports *\/\r\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes externally-visible service portals *\/\r\nKUBE-FIREWALL  all  --  anywhere             anywhere            \r\n\r\nChain FORWARD (policy ACCEPT)\r\ntarget     prot opt source               destination         \r\ncali-FORWARD  all  --  anywhere             anywhere             \/* cali:wUHhoiAYhphO9Mso *\/\r\nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             \/* kube-proxy firewall rules *\/\r\nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes load balancer firewall *\/\r\nKUBE-FORWARD  all  --  anywhere             anywhere             \/* kubernetes forwarding rules *\/\r\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes service portals *\/\r\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes externally-visible service portals *\/\r\nFLANNEL-FWD  all  --  anywhere             anywhere             \/* flanneld forward *\/\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:S93hcgKJrXEqnTfs *\/ \/* Policy explicitly accepted packet. *\/ mark match 0x10000\/0x10000\r\nMARK       all  --  anywhere             anywhere             \/* cali:mp77cMpurHhyjLrM *\/ MARK or 0x10000\r\n\r\nChain OUTPUT (policy ACCEPT)\r\ntarget     prot opt source               destination         \r\ncali-OUTPUT  all  --  anywhere             anywhere             \/* cali:tVnHkvAo15HuiPy0 *\/\r\nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes load balancer firewall *\/\r\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW \/* kubernetes service portals *\/\r\nKUBE-FIREWALL  all  --  anywhere             anywhere            \r\n\r\nChain FLANNEL-FWD (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  randomhost\/16          anywhere             \/* flanneld forward *\/\r\nACCEPT     all  --  anywhere             randomhost\/16          \/* flanneld forward *\/\r\n\r\nChain KUBE-EXTERNAL-SERVICES (2 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-FIREWALL (2 references)\r\ntarget     prot opt source               destination         \r\nDROP       all  -- !localhost\/8          localhost\/8          \/* block incoming localnet connections *\/ ! ctstate RELATED,ESTABLISHED,DNAT\r\n\r\nChain KUBE-FORWARD (1 references)\r\ntarget     prot opt source               destination         \r\nDROP       all  --  anywhere             anywhere             ctstate INVALID\r\nACCEPT     all  --  anywhere             anywhere             \/* kubernetes forwarding rules *\/ mark match 0x4000\/0x4000\r\nACCEPT     all  --  anywhere             anywhere             \/* kubernetes forwarding conntrack rule *\/ ctstate RELATED,ESTABLISHED\r\n\r\nChain KUBE-IPVS-FILTER (1 references)\r\ntarget     prot opt source               destination         \r\nRETURN     all  --  anywhere             anywhere             match-set KUBE-LOAD-BALANCER dst,dst\r\nRETURN     all  --  anywhere             anywhere             match-set KUBE-CLUSTER-IP dst,dst\r\nRETURN     all  --  anywhere             anywhere             match-set KUBE-EXTERNAL-IP dst,dst\r\nRETURN     all  --  anywhere             anywhere             match-set KUBE-EXTERNAL-IP-LOCAL dst,dst\r\nRETURN     all  --  anywhere             anywhere             match-set KUBE-HEALTH-CHECK-NODE-PORT dst\r\nREJECT     all  --  anywhere             anywhere             ctstate NEW match-set KUBE-IPVS-IPS dst reject-with icmp-port-unreachable\r\n\r\nChain KUBE-KUBELET-CANARY (0 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-NODE-PORT (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* Kubernetes health check node port *\/ match-set KUBE-HEALTH-CHECK-NODE-PORT dst\r\n\r\nChain KUBE-NODEPORTS (1 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-PROXY-CANARY (0 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-PROXY-FIREWALL (5 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-SERVICES (2 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain KUBE-SOURCE-RANGES-FIREWALL (0 references)\r\ntarget     prot opt source               destination         \r\nDROP       all  --  anywhere             anywhere            \r\n\r\nChain cali-FORWARD (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:vjrMJCRpqwy5oRoX *\/ MARK and 0xfff1ffff\r\ncali-from-hep-forward  all  --  anywhere             anywhere             \/* cali:A_sPAO0mcxbT9mOV *\/ mark match 0x0\/0x10000\r\ncali-from-wl-dispatch  all  --  anywhere             anywhere             \/* cali:8ZoYfO5HKXWbB3pk *\/\r\ncali-to-wl-dispatch  all  --  anywhere             anywhere             \/* cali:jdEuaPBe14V2hutn *\/\r\ncali-to-hep-forward  all  --  anywhere             anywhere             \/* cali:12bc6HljsMKsmfr- *\/\r\ncali-cidr-block  all  --  anywhere             anywhere             \/* cali:NOSxoaGx8OIstr1z *\/\r\n\r\nChain cali-INPUT (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     ipencap--  anywhere             anywhere             \/* cali:PajejrV4aFdkZojI *\/ \/* Allow IPIP packets from Calico hosts *\/ match-set cali40all-hosts-net src ADDRTYPE match dst-type LOCAL\r\nDROP       ipencap--  anywhere             anywhere             \/* cali:_wjq-Yrma8Ly1Svo *\/ \/* Drop IPIP packets from non-Calico hosts *\/\r\nMARK       all  --  anywhere             anywhere             \/* cali:ss8lEMQsXi-s6qYT *\/ MARK and 0xfffff\r\ncali-forward-check  all  --  anywhere             anywhere             \/* cali:PgIW-V0nEjwPhF_8 *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:QMJlDwlS0OjHyfMN *\/ mark match ! 0x0\/0xfff00000\r\ncali-wl-to-host  all  --  anywhere             anywhere            [goto]  \/* cali:nDRe73txrna-aZjG *\/\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:iX2AYvqGXaVqwkro *\/ mark match 0x10000\/0x10000\r\nMARK       all  --  anywhere             anywhere             \/* cali:bhpnxD5IRtBP8KW0 *\/ MARK and 0xfff0ffff\r\ncali-from-host-endpoint  all  --  anywhere             anywhere             \/* cali:H5_bccAbHV0sooVy *\/\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:inBL01YlfurT0dbI *\/ \/* Host endpoint policy accepted packet. *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-OUTPUT (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:Mq1_rAdXXH3YkrzW *\/ mark match 0x10000\/0x10000\r\ncali-forward-endpoint-mark  all  --  anywhere             anywhere            [goto]  \/* cali:5Z67OUUpTOM7Xa1a *\/ mark match ! 0x0\/0xfff00000\r\nRETURN     all  --  anywhere             anywhere             \/* cali:M2Wf0OehNdig8MHR *\/\r\nACCEPT     ipencap--  anywhere             anywhere             \/* cali:AJBkLho_0Qd8LNr3 *\/ \/* Allow IPIP packets to other Calico hosts *\/ match-set cali40all-hosts-net dst ADDRTYPE match src-type LOCAL\r\nMARK       all  --  anywhere             anywhere             \/* cali:iz2RWXlXJDUfsLpe *\/ MARK and 0xfff0ffff\r\ncali-to-host-endpoint  all  --  anywhere             anywhere             \/* cali:xQqLi8S0sxbiyvjR *\/ ! ctstate DNAT\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:aSnsxZdmhxm_ilRZ *\/ \/* Host endpoint policy accepted packet. *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-cidr-block (1 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain cali-forward-check (1 references)\r\ntarget     prot opt source               destination         \r\nRETURN     all  --  anywhere             anywhere             \/* cali:Pbldlb4FaULvpdD8 *\/ ctstate RELATED,ESTABLISHED\r\ncali-set-endpoint-mark  tcp  --  anywhere             anywhere            [goto]  \/* cali:ZD-6UxuUtGW-xtzg *\/ \/* To kubernetes NodePort service *\/ multiport dports 30000:32767 match-set cali40this-host dst\r\ncali-set-endpoint-mark  udp  --  anywhere             anywhere            [goto]  \/* cali:CbPfUajQ2bFVnDq4 *\/ \/* To kubernetes NodePort service *\/ multiport dports 30000:32767 match-set cali40this-host dst\r\ncali-set-endpoint-mark  all  --  anywhere             anywhere             \/* cali:jmhU0ODogX-Zfe5g *\/ \/* To kubernetes service *\/ ! match-set cali40this-host dst\r\n\r\nChain cali-forward-endpoint-mark (1 references)\r\ntarget     prot opt source               destination         \r\ncali-from-endpoint-mark  all  --  anywhere             anywhere             \/* cali:O0SmFDrnm7KggWqW *\/ mark match ! 0x100000\/0xfff00000\r\ncali-to-wl-dispatch  all  --  anywhere             anywhere             \/* cali:aFl0WFKRxDqj8oA6 *\/\r\ncali-to-hep-forward  all  --  anywhere             anywhere             \/* cali:AZKVrO3i_8cLai5f *\/\r\nMARK       all  --  anywhere             anywhere             \/* cali:96HaP1sFtb-NYoYA *\/ MARK and 0xfffff\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:VxO6hyNWz62YEtul *\/ \/* Policy explicitly accepted packet. *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-from-endpoint-mark (1 references)\r\ntarget     prot opt source               destination         \r\ncali-fw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  \/* cali:_g0DAJdwj0zFTZDd *\/ mark match 0x40300000\/0xfff00000\r\ncali-fw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  \/* cali:_xvgcxf_OYnxRPpL *\/ mark match 0x93400000\/0xfff00000\r\ncali-fw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  \/* cali:vobAwpu2w-OpfxcW *\/ mark match 0xf9e00000\/0xfff00000\r\ncali-fw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  \/* cali:2NCz6Z8qUvBSq-1n *\/ mark match 0x92300000\/0xfff00000\r\nDROP       all  --  anywhere             anywhere             \/* cali:Iami5Z3O-giBayN1 *\/ \/* Unknown interface *\/\r\n\r\nChain cali-from-hep-forward (1 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain cali-from-host-endpoint (1 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain cali-from-wl-dispatch (2 references)\r\ntarget     prot opt source               destination         \r\ncali-fw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  \/* cali:K9tdwcxEej5TWJwt *\/\r\ncali-fw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  \/* cali:tCrG_oPsjs7oDjiR *\/\r\ncali-fw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  \/* cali:ty0aZN9DLespD-He *\/\r\ncali-fw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  \/* cali:RuLvWlq3xyYNFBBq *\/\r\nDROP       all  --  anywhere             anywhere             \/* cali:TzHmjW1BcVePGwUs *\/ \/* Unknown interface *\/\r\n\r\nChain cali-fw-cali1a60675da04 (2 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:ZGWFfGodtixX8XmQ *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:NsuY-Vd-csUklFg6 *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:HEkNokvoMUqhs_mz *\/ MARK and 0xfffeffff\r\nDROP       udp  --  anywhere             anywhere             \/* cali:yT2TFLX6lbgFvt06 *\/ \/* Drop VXLAN encapped packets originating in workloads *\/ multiport dports 4789\r\nDROP       ipencap--  anywhere             anywhere             \/* cali:JeohvZMA2MUMtka- *\/ \/* Drop IPinIP encapped packets originating in workloads *\/\r\ncali-pro-kns.kube-system  all  --  anywhere             anywhere             \/* cali:Ry3YsktIkN_WGshL *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:j42-VxrHhDCsCHKq *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pro-_u2Tn2rSoAPffvE7JO6  all  --  anywhere             anywhere             \/* cali:DZeztPbl9GuWcSnS *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:QlAcy0LXTxrSFoIZ *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:020SLlST78qhqnSb *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-fw-cali61bf3705192 (2 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:HukEhemyC3jUEB2h *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:nS7m2w5F8T8HFKDB *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:SyHL_U3-NYfJnzHU *\/ MARK and 0xfffeffff\r\nDROP       udp  --  anywhere             anywhere             \/* cali:7W0TplglhR5tCDL8 *\/ \/* Drop VXLAN encapped packets originating in workloads *\/ multiport dports 4789\r\nDROP       ipencap--  anywhere             anywhere             \/* cali:-qjAC-m_cklBJkjl *\/ \/* Drop IPinIP encapped packets originating in workloads *\/\r\ncali-pro-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:6Dm_rdQkT2d-xn0M *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:xNfbxZAEEE0ZBeFn *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pro-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             \/* cali:umAELb2j4mLttFOx *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:FiF3VTCFM8GBIctG *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:9kcFLKXYuJ3kjgED *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-fw-cali80d4429314e (2 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:g7SK49eF3d8GQTzM *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:VWWFvGl1dJh0Cblf *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:QXXXDB_vuQ4LM5oU *\/ MARK and 0xfffeffff\r\nDROP       udp  --  anywhere             anywhere             \/* cali:47ZKBD6pl4tSINwl *\/ \/* Drop VXLAN encapped packets originating in workloads *\/ multiport dports 4789\r\nDROP       ipencap--  anywhere             anywhere             \/* cali:bBEbmNiq-HYa4oz8 *\/ \/* Drop IPinIP encapped packets originating in workloads *\/\r\ncali-pro-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:V-1mwp2arUxEn_dH *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:mCCRhepT2W1NMr-Z *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pro-_7Z-cbWmLXlJI5GWsIJ  all  --  anywhere             anywhere             \/* cali:DhDPbLnz1bOTKjAj *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:-tfrAm7GnlHHz19t *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:wFkb_R24Tl5nk2uM *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-fw-caliaff77ba4713 (2 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:zAhgVOdw1_ZwY_qX *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:E_elvvjGDtCyrp2s *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:wYbvB5dlGZFEmI7z *\/ MARK and 0xfffeffff\r\nDROP       udp  --  anywhere             anywhere             \/* cali:Sv2vOgdI-3JOJHV0 *\/ \/* Drop VXLAN encapped packets originating in workloads *\/ multiport dports 4789\r\nDROP       ipencap--  anywhere             anywhere             \/* cali:Ini0PSE7tGhtZvB1 *\/ \/* Drop IPinIP encapped packets originating in workloads *\/\r\ncali-pro-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:4yzbR1i3_3i5zBej *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:seHIJ440rgN8m2Gl *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pro-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             \/* cali:G06ddVJcUNN-QcI_ *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:0oAp0x-lyWfDbm7q *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:E-X7DQX5M64GSk2C *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-pri-_4AJPMUgjnkp6PvIZs_ (2 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:6-P0HapqloRNafEU *\/ \/* Profile ksa.gpu-operator.node-feature-discovery ingress *\/\r\n\r\nChain cali-pri-_7Z-cbWmLXlJI5GWsIJ (1 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:33eST6J3mye-oils *\/ \/* Profile ksa.gpu-operator.gpu-operator ingress *\/\r\n\r\nChain cali-pri-_u2Tn2rSoAPffvE7JO6 (1 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:WqgznqAQ-uYV0oBx *\/ \/* Profile ksa.kube-system.coredns ingress *\/\r\n\r\nChain cali-pri-kns.gpu-operator (3 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:MVC_LdvqXMM61ZiS *\/ \/* Profile kns.gpu-operator ingress *\/ MARK or 0x10000\r\nRETURN     all  --  anywhere             anywhere             \/* cali:s0wCV1amxXQsLjX6 *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-pri-kns.kube-system (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:J1TyxtHWd0qaBGK- *\/ \/* Profile kns.kube-system ingress *\/ MARK or 0x10000\r\nRETURN     all  --  anywhere             anywhere             \/* cali:QIB6k7eEKdIg73Jp *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-pro-_4AJPMUgjnkp6PvIZs_ (2 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:67xwX_Dluk58rujA *\/ \/* Profile ksa.gpu-operator.node-feature-discovery egress *\/\r\n\r\nChain cali-pro-_7Z-cbWmLXlJI5GWsIJ (1 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:Jt8hHFPFkxNxFHV9 *\/ \/* Profile ksa.gpu-operator.gpu-operator egress *\/\r\n\r\nChain cali-pro-_u2Tn2rSoAPffvE7JO6 (1 references)\r\ntarget     prot opt source               destination         \r\n           all  --  anywhere             anywhere             \/* cali:0-_UPh39dt5XfhmJ *\/ \/* Profile ksa.kube-system.coredns egress *\/\r\n\r\nChain cali-pro-kns.gpu-operator (3 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:dowjNH-r-IuDxByL *\/ \/* Profile kns.gpu-operator egress *\/ MARK or 0x10000\r\nRETURN     all  --  anywhere             anywhere             \/* cali:9xl88GIXoFBqCna9 *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-pro-kns.kube-system (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:tgOR2S8DVHZW3F1M *\/ \/* Profile kns.kube-system egress *\/ MARK or 0x10000\r\nRETURN     all  --  anywhere             anywhere             \/* cali:HVEEtYPJsiGRXCIt *\/ mark match 0x10000\/0x10000\r\n\r\nChain cali-set-endpoint-mark (3 references)\r\ntarget     prot opt source               destination         \r\ncali-sm-cali1a60675da04  all  --  anywhere             anywhere            [goto]  \/* cali:-46PjhDiNsRWf1FU *\/\r\ncali-sm-cali61bf3705192  all  --  anywhere             anywhere            [goto]  \/* cali:q_vQECIkADutQIWL *\/\r\ncali-sm-cali80d4429314e  all  --  anywhere             anywhere            [goto]  \/* cali:kXqypj13p8lEFDTx *\/\r\ncali-sm-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  \/* cali:KkkLwmGp8wgvlkny *\/\r\nDROP       all  --  anywhere             anywhere             \/* cali:9Lj2OU8ZON1EJ4XC *\/ \/* Unknown endpoint *\/\r\nMARK       all  --  anywhere             anywhere             \/* cali:3mMep0QEGmc9Qt7q *\/ \/* Non-Cali endpoint mark *\/ MARK xset 0x100000\/0xfff00000\r\n\r\nChain cali-sm-cali1a60675da04 (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:cFx4mA6whaURTNOC *\/ MARK xset 0x40300000\/0xfff00000\r\n\r\nChain cali-sm-cali61bf3705192 (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:SQRm6KIr7h0YoMIZ *\/ MARK xset 0x93400000\/0xfff00000\r\n\r\nChain cali-sm-cali80d4429314e (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:msddQTu3_4-8yx_x *\/ MARK xset 0xf9e00000\/0xfff00000\r\n\r\nChain cali-sm-caliaff77ba4713 (1 references)\r\ntarget     prot opt source               destination         \r\nMARK       all  --  anywhere             anywhere             \/* cali:RZKZ_MnNUqUKKGiD *\/ MARK xset 0x92300000\/0xfff00000\r\n\r\nChain cali-to-hep-forward (2 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain cali-to-host-endpoint (1 references)\r\ntarget     prot opt source               destination         \r\n\r\nChain cali-to-wl-dispatch (2 references)\r\ntarget     prot opt source               destination         \r\ncali-tw-cali1a60675da04  all  --  anywhere             anywhere            [goto]  \/* cali:zIuiWorw_-gqLEXO *\/\r\ncali-tw-cali61bf3705192  all  --  anywhere             anywhere            [goto]  \/* cali:xag75YL5akcG0XLF *\/\r\ncali-tw-cali80d4429314e  all  --  anywhere             anywhere            [goto]  \/* cali:gYjFDNf-SltrRZB7 *\/\r\ncali-tw-caliaff77ba4713  all  --  anywhere             anywhere            [goto]  \/* cali:FdkGAd3ATwS-vVVD *\/\r\nDROP       all  --  anywhere             anywhere             \/* cali:fqHovg-WPs4e6zTK *\/ \/* Unknown interface *\/\r\n\r\nChain cali-tw-cali1a60675da04 (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:w1Cm2ULOSvjTHudq *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:9umnlXX6GO6Vc4_C *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:4hLjdY3hkfGFs4uD *\/ MARK and 0xfffeffff\r\ncali-pri-kns.kube-system  all  --  anywhere             anywhere             \/* cali:gLTSlA3UVavlYqRu *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:1pYcsK43THPrR8Qt *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pri-_u2Tn2rSoAPffvE7JO6  all  --  anywhere             anywhere             \/* cali:tA0_fCHeaUVu5_kN *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:5PoHpiQMvOhdA4jr *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:6P9MHq2lZbGcJMUs *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-tw-cali61bf3705192 (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:4TsaIXYZcZWbiXXW *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:xlvqDm2nH2zq4V8p *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:GU1FuBxFrFydlHq3 *\/ MARK and 0xfffeffff\r\ncali-pri-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:1uJveElD9OXuShKZ *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:l928XWHGLf6ksfAV *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pri-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             \/* cali:n-nBk2VPtnM1xYwe *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:wkuEQFdIIQ77Wq7W *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:ElgyHAWClRlWkpqG *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-tw-cali80d4429314e (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:H1jNyDPmKvDDn5Vk *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:NcCXk1FMcyKvN-0s *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:JlnCwI9rZZNdivZU *\/ MARK and 0xfffeffff\r\ncali-pri-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:GVRxY9RD3CcrgUk3 *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:10QG2G9XDcx2Bwfi *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pri-_7Z-cbWmLXlJI5GWsIJ  all  --  anywhere             anywhere             \/* cali:JOmVuKKbAS2BFWnc *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:pcYu_mJoNuj2B6Cj *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:AxCTJffEY1zGhysc *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-tw-caliaff77ba4713 (1 references)\r\ntarget     prot opt source               destination         \r\nACCEPT     all  --  anywhere             anywhere             \/* cali:Wm5cRViQaTB0AM6_ *\/ ctstate RELATED,ESTABLISHED\r\nDROP       all  --  anywhere             anywhere             \/* cali:3jOmjG1mkPWiBb57 *\/ ctstate INVALID\r\nMARK       all  --  anywhere             anywhere             \/* cali:1AAP5imvUYhZQLJE *\/ MARK and 0xfffeffff\r\ncali-pri-kns.gpu-operator  all  --  anywhere             anywhere             \/* cali:PfCoNSFnCX-GKCGz *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:A9mVsPhWlgdu61dl *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\ncali-pri-_4AJPMUgjnkp6PvIZs_  all  --  anywhere             anywhere             \/* cali:fOMutePJ56ab3bD_ *\/\r\nRETURN     all  --  anywhere             anywhere             \/* cali:jNAhD8xZ9K3Laqwi *\/ \/* Return if profile accepted *\/ mark match 0x10000\/0x10000\r\nDROP       all  --  anywhere             anywhere             \/* cali:nWbl-mTELMxfckFL *\/ \/* Drop if no profiles matched *\/\r\n\r\nChain cali-wl-to-host (1 references)\r\ntarget     prot opt source               destination         \r\ncali-from-wl-dispatch  all  --  anywhere             anywhere             \/* cali:Ee9Sbo10IpVujdIY *\/\r\nACCEPT     all  --  anywhere             anywhere             \/* cali:nSZbcOoG1xPONxb8 *\/ \/* Configured DefaultEndpointToHostAction *\/\r\n```\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\nuser@randomhost:~$ kubectl version\r\nClient Version: v1.28.4\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.6\r\n```\r\nk8s version : `1.28.3`, `1.28.4`\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nOnpremise\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```bash\r\nuser@randomhost:~$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\nkubeadm\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\ncontainerd\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\ncalico\r\n<\/details>","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig network"],"labels":["kind\/bug","sig\/network","needs-triage"]},{"title":"[DO NOT MERGE]Update govulncheck version to 1.0.4","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123947#\" title=\"Author self-approved\">ArkaSaha30<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","@ArkaSaha30: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify | 79e10502530e7f566999aab2dcce8426edef51f6 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123947\/pull-kubernetes-verify\/1768541105265053696) | true | `\/test pull-kubernetes-verify`\npull-kubernetes-unit | 79e10502530e7f566999aab2dcce8426edef51f6 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123947\/pull-kubernetes-unit\/1768541103293730816) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123947). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AArkaSaha30). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["size\/XS","cncf-cla: yes","do-not-merge\/release-note-label-needed","needs-priority","needs-triage","do-not-merge\/needs-sig","do-not-merge\/needs-kind"]},{"title":"[Flaking Test] ci-kubernetes-integration-master TestSelectableFields","body":"### Which jobs are flaking?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1768366435643428864\n\n### Which tests are flaking?\n\nk8s.io\/apiextensions-apiserver\/test: integration\r\n\r\nTestSelectableFields\n\n### Since when has it been flaking?\n\n15\/3\/2024\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#integration-master\n\n### Reason for failure (if possible)\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1768366435643428864#1:build-log.txt%3A27112\r\n\r\n```\r\npanic: conversion webhook for tests.example.com\/v1, Kind=Shirt failed: Post \"https:\/\/127.0.0.1:45105\/convert?timeout=30s\": EOF\r\ngoroutine 57030 [running]:\r\nk8s.io\/apiserver\/pkg\/storage\/etcd3.decodeObj({0x2cd4dc0?, 0xc0005d7360?}, {0x2ce45c0, 0x43049a0}, {0xc000727680?, 0x1e0?, 0xc000951f30?}, 0x503c)\r\n\t\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/watcher.go:612 +0x1c6\r\nk8s.io\/apiserver\/pkg\/storage\/etcd3.(*watchChan).prepareObjs(0xc0007ede10, 0xc0012ddef0)\r\n\t\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/watcher.go:580 +0xe5\r\nk8s.io\/apiserver\/pkg\/storage\/etcd3.(*watchChan).transform(0xc0007ede10, 0xc0012ddef0)\r\n\t\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/watcher.go:469 +0x3b\r\nk8s.io\/apiserver\/pkg\/storage\/etcd3.(*watchChan).processEvent(0xc0007ede10, 0x2c7d7b3a22737574?)\r\n\t\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/watcher.go:434 +0x125\r\ncreated by k8s.io\/apiserver\/pkg\/storage\/etcd3.(*watchChan).run in goroutine 57028\r\n\t\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/watcher.go:234 +0x129\r\n```\n\n### Anything else we need to know?\n\nTest was introduced in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122717\n\n### Relevant SIG(s)\n\n\/sig api-machinery","comments":["\/assign @jpbetz \r\n\/triage accepted"],"labels":["sig\/api-machinery","kind\/flake","triage\/accepted"]},{"title":"add apiserver_client_certificate_expiration_seconds_bucket log","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nThere is no way now we can figure out which client's certificate is going to expire\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/89611\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n\r\n```\r\n\r\n","comments":["Hi @denglouping. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123945#\" title=\"Author self-approved\">denglouping<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mikedanese](https:\/\/github.com\/mikedanese) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/authentication\/request\/x509\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/authentication\/request\/x509\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mikedanese\"]} -->","\/assign @liggitt \r\n\/triage accepted\r\n\r\nAssigned as part of the sig auth triage meeting"],"labels":["area\/apiserver","sig\/api-machinery","size\/XS","kind\/feature","release-note-none","sig\/auth","cncf-cla: yes","needs-ok-to-test","needs-priority","triage\/accepted"]},{"title":"Resolved compatibility issue between Kubelet PLEG and inplace VPA","body":"Make In-place VPA feature work with PLEG relist. Use auxiliary runtime pod status and PLEG cache pod status to distinguish the resize pod and make sure it generate correct PLEG event and come into the event channel.\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\nPleg doesn't handle resized pod well. See https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123940 for more details\r\n\r\nThis is part of https:\/\/github.com\/kubernetes\/enhancements\/pull\/4433\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes \r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123940\r\n- https:\/\/docs.google.com\/document\/d\/1V3DLh3pH3CD-xhhJvAnOq_oWgPyjO-vj6wY6qdew9H0\/edit#heading=h.5gfeh6i9awgb\r\n\r\n#### Special notes for your reviewer:\r\n\r\nI have not added tests yet. I did some manual e2e tests. If the idea looks good to you, I will spend some time improving the test coverage.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/pull\/4433\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123941#\" title=\"Author self-approved\">Jeffwan<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","\/cc @smarterclayton @bobbypage @liggitt  @kubernetes\/sig-node-pr-reviews ","@Jeffwan: GitHub didn't allow me to request PR reviews from the following users: kubernetes\/sig-node-pr-reviews.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) and repo collaborators can review this PR, and authors cannot review their own PRs.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123941#issuecomment-1998461251):\n\n>\/cc @smarterclayton @bobbypage @liggitt  @kubernetes\/sig-node-pr-reviews \n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","@Jeffwan: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-linter-hints | e58fbfe7d5810d800ac6ee678728d03da24e6d42 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123941\/pull-kubernetes-linter-hints\/1768430238888366080) | false | `\/test pull-kubernetes-linter-hints`\npull-kubernetes-verify-lint | e58fbfe7d5810d800ac6ee678728d03da24e6d42 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123941\/pull-kubernetes-verify-lint\/1768430238049505280) | true | `\/test pull-kubernetes-verify-lint`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123941). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AJeffwan). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/M","release-note-none","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"PLEG doesn't work well with alpha feature InPlacePodVerticalScaling","body":"### What happened?\r\n\r\n#### Background\r\n\r\nThis is a follow up issue from https:\/\/github.com\/kubernetes\/kubernetes\/pull\/120432\/files#r1489932247\r\n\r\nOriginally, we fix the InPlacePodVerticalScaling performance issue by fetching the runtime status in single sync loop which is not elegant. Later, I follow @smarterclayton's suggestion to leverage PLEG to emit events to fix it. \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/89f03e3988a4e7fed90ffce22f355ff248520ad2\/pkg\/kubelet\/kubelet.go#L1988-L1995\r\n(InPlacePodVerticalScaling puts the resizing pod into PLEG cache for further reconcilation)\r\n\r\nI notice there're two problems.\r\n\r\n#### 1. There's no way to generate a PodLifecycleEvent event for resized pod.\r\n\r\na.  oldPod and newPod are exact same in in-place update scenarios. It can not distinguish the pod that has been resized.  Since they are same, no events would be generated.\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/89f03e3988a4e7fed90ffce22f355ff248520ad2\/pkg\/kubelet\/pleg\/generic.go#L252-L257C14\r\n\r\nb. The running pod PLEG cache are not being used at all. \r\n`pleg.updateCache()` will call `runtime.GetPodStatus` underneath. In VPA case, the latest CRI container status will be fetched. That means the cache store a new container status. However, it is not being used in the `Relist` flow.  I think even we use it in`pleg.Relist()`, there's no way to distinguish the resized pod based on existing fields. Please check attached code snippets of their data structures.\r\n\r\n\r\n#### 2. plegContainerUnknown(ContainerChanged) is not correctly handled for resized container\r\n\r\nSeems `plegContainerUnknown(ContainerChanged)` is the best state for the resized container.  However, this is not correctly handle and it was not successfully passed in the event channel. \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/89f03e3988a4e7fed90ffce22f355ff248520ad2\/pkg\/kubelet\/pleg\/generic.go#L311-L312\r\n\r\n#### Proposal\r\nI would suggest to remove the `kl.pleg.UpdateCache(&runningPod, pod.UID);` logic in kubelet syncPod loop since the pod would be fetched in `pleg.Relist()` and properly handle the `ContainerChanged` event.\r\n\r\nI cut a new PR to fix this issue https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123941, please check whether that makes sense?\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nI hope the resized container could be picked up by PLEG Relist logic (interval is 1 s) and trigger the status update and no need to wait for kubelet's next reconcile loop  (interval is 1 minute)\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. Enable the InPlacePodVerticalScaling feature gate and start the cluster\r\n2. Create a pod with 1c.\r\n3. Bump to 2c.\r\n4. Check pod container status.  `ResourceAllocated` would be updated but status won't be updated until next kubelet recocnile loop, which means normally you will wait ~1 mins.\r\n\r\n\r\n### Anything else we need to know?\r\n\r\nrelist pod status. The status is captured after the pod cpu was update from 1 to 2\r\n\r\n\r\nold pod\r\n```\r\n{\r\n  \"ID\": \"4e2b3d81-129f-40e4-a579-631397aa718c\",\r\n  \"Name\": \"tomcat\",\r\n  \"Namespace\": \"default\",\r\n  \"CreatedAt\": 1710219686362299600,\r\n  \"Containers\": [\r\n    {\r\n      \"ID\": \"containerd:\/\/a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17\",\r\n      \"Name\": \"tomcat\",\r\n      \"Image\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageID\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageRef\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageRuntimeHandler\": \"\",\r\n      \"Hash\": 2397588892,\r\n      \"HashWithoutResources\": 3106650780,\r\n      \"State\": \"running\"\r\n    }\r\n  ],\r\n  \"Sandboxes\": [\r\n    {\r\n      \"ID\": \"containerd:\/\/6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a\",\r\n      \"Name\": \"\",\r\n      \"Image\": \"\",\r\n      \"ImageID\": \"\",\r\n      \"ImageRef\": \"\",\r\n      \"ImageRuntimeHandler\": \"\",\r\n      \"Hash\": 0,\r\n      \"HashWithoutResources\": 0,\r\n      \"State\": \"running\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nnewpod\r\n```\r\n{\r\n  \"ID\": \"4e2b3d81-129f-40e4-a579-631397aa718c\",\r\n  \"Name\": \"tomcat\",\r\n  \"Namespace\": \"default\",\r\n  \"CreatedAt\": 1710219686362299600,\r\n  \"Containers\": [\r\n    {\r\n      \"ID\": \"containerd:\/\/a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17\",\r\n      \"Name\": \"tomcat\",\r\n      \"Image\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageID\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageRef\": \"sha256:ef6a7c98d192507d6066dcf24e44bec66d07ec9cf7c55d8d3d1ea0a24660bdef\",\r\n      \"ImageRuntimeHandler\": \"\",\r\n      \"Hash\": 2397588892,\r\n      \"HashWithoutResources\": 3106650780,\r\n      \"State\": \"running\"\r\n    }\r\n  ],\r\n  \"Sandboxes\": [\r\n    {\r\n      \"ID\": \"containerd:\/\/6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a\",\r\n      \"Name\": \"\",\r\n      \"Image\": \"\",\r\n      \"ImageID\": \"\",\r\n      \"ImageRef\": \"\",\r\n      \"ImageRuntimeHandler\": \"\",\r\n      \"Hash\": 0,\r\n      \"HashWithoutResources\": 0,\r\n      \"State\": \"running\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\npleg cached pod - from https:\/\/github.com\/kubernetes\/kubernetes\/blob\/89f03e3988a4e7fed90ffce22f355ff248520ad2\/pkg\/kubelet\/kubelet.go#L1988-L1995\r\n\r\n```\r\n{\r\n  \"ID\": \"4e2b3d81-129f-40e4-a579-631397aa718c\",\r\n  \"Name\": \"tomcat\",\r\n  \"Namespace\": \"default\",\r\n  \"IPs\": [\r\n    \"10.88.0.63\",\r\n    \"2001:db8:4860::3f\"\r\n  ],\r\n  \"ContainerStatuses\": [\r\n    {\r\n      \"ID\": \"containerd:\/\/a06de85dbb87c2c0632df34b60ff9323ea29b9be362080ae75b7fdf0526e9c17\",\r\n      \"Name\": \"tomcat\",\r\n      \"State\": \"running\",\r\n      \"CreatedAt\": \"2024-03-12T05:01:27.032584855Z\",\r\n      \"StartedAt\": \"2024-03-12T05:01:27.083899049Z\",\r\n      \"FinishedAt\": \"0001-01-01T00:00:00Z\",\r\n      \"ExitCode\": 0,\r\n      \"Image\": \"docker.io\/library\/tomcat:8.0\",\r\n      \"ImageID\": \"docker.io\/library\/tomcat@sha256:8ecb10948deb32c34aeadf7bf95d12a93fbd3527911fa629c1a3e7823b89ce6f\",\r\n      \"ImageRef\": \"docker.io\/library\/tomcat@sha256:8ecb10948deb32c34aeadf7bf95d12a93fbd3527911fa629c1a3e7823b89ce6f\",\r\n      \"ImageRuntimeHandler\": \"\",\r\n      \"Hash\": 2397588892,\r\n      \"HashWithoutResources\": 3106650780,\r\n      \"RestartCount\": 0,\r\n      \"Reason\": \"\",\r\n      \"Message\": \"\",\r\n      \"Resources\": {\r\n        \"CPURequest\": \"2\",\r\n        \"CPULimit\": \"2\",\r\n        \"MemoryRequest\": null,\r\n        \"MemoryLimit\": null\r\n      }\r\n    }\r\n  ],\r\n  \"SandboxStatuses\": [\r\n    {\r\n      \"id\": \"6bda38b639c84b369a169b94f8bf820bda49ab8f98a2ab365acc52883cebb25a\",\r\n      \"metadata\": {\r\n        \"name\": \"tomcat\",\r\n        \"uid\": \"4e2b3d81-129f-40e4-a579-631397aa718c\",\r\n        \"namespace\": \"default\"\r\n      },\r\n      \"created_at\": 1710219686362299600,\r\n      \"network\": {\r\n        \"ip\": \"10.88.0.63\",\r\n        \"additional_ips\": [\r\n          {\r\n            \"ip\": \"2001:db8:4860::3f\"\r\n          }\r\n        ]\r\n      },\r\n      \"linux\": {\r\n        \"namespaces\": {\r\n          \"options\": {\r\n            \"pid\": 1\r\n          }\r\n        }\r\n      },\r\n      \"labels\": {\r\n        \"io.kubernetes.pod.name\": \"tomcat\",\r\n        \"io.kubernetes.pod.namespace\": \"default\",\r\n        \"io.kubernetes.pod.uid\": \"4e2b3d81-129f-40e4-a579-631397aa718c\"\r\n      },\r\n      \"annotations\": {\r\n        \"kubectl.kubernetes.io\/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"tomcat\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"image\\\":\\\"tomcat:8.0\\\",\\\"imagePullPolicy\\\":\\\"Always\\\",\\\"name\\\":\\\"tomcat\\\",\\\"ports\\\":[{\\\"containerPort\\\":7500}],\\\"resizePolicy\\\":[{\\\"resourceName\\\":\\\"cpu\\\",\\\"restartPolicy\\\":\\\"NotRequired\\\"}],\\\"resources\\\":{\\\"limits\\\":{\\\"cpu\\\":1},\\\"requests\\\":{\\\"cpu\\\":1}}}]}}\\n\",\r\n        \"kubernetes.io\/config.seen\": \"2024-03-12T05:01:26.046824676Z\",\r\n        \"kubernetes.io\/config.source\": \"api\"\r\n      }\r\n    }\r\n  ],\r\n  \"TimeStamp\": \"0001-01-01T00:00:00Z\"\r\n}\r\n```\r\n\r\n\r\n\r\n### Kubernetes version\r\n\r\nmaster version\r\n\r\n\r\n### Cloud provider\r\n\r\nCommon problem so it applies to any cloud providers.\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\n"],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"DRA: test for structured parameters","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516 got merged with some tests still missing due to time pressure. This PR adds those tests. One bug was found while doing so.\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Unit testing depends on correct handling of `ResourceClaimParameters` and `ResourceClassParameters` by the fake client, which is currently broken because of a [wrong guess of the plural form](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123936). The [right solution](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/119924) is still pending. Note sure yet how to handle this. Merge without the `dynamicresources_test.go` changes?\r\n","\/cc","> The [right solution](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/119924) is still pending. Note sure yet how to handle this. Merge without the `dynamicresources_test.go` changes?\r\n\r\nI have [a workaround](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123936#issuecomment-1998082422). I'm currently waiting for https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123932 to merge, then I will rebase.\r\n\r\n\r\n","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123938#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [huang-wei](https:\/\/github.com\/huang-wei) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/controller\/resourceclaim\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/resourceclaim\/OWNERS)~~ [pohly]\n- **[pkg\/scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/scheduler\/OWNERS)**\n- ~~[test\/e2e\/dra\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/dra\/OWNERS)~~ [pohly]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"huang-wei\"]} -->","Rebased, PR should be okay now.\r\n\r\n\/test pull-kubernetes-node-e2e-containerd-1-7-dra\r\n\r\n[Looks](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123938\/pull-kubernetes-node-e2e-containerd-1-7-dra\/1768364746291023872) like a Flake.\r\n\r\n\/cc @bart0sh ","\/remove-sig api-machinery","PR updated.","\/retest\r\n","\/kind bug\r\n\r\nThere is a (minor) fix included here, see first commit."],"labels":["kind\/bug","area\/test","area\/kubelet","kind\/cleanup","sig\/scheduling","sig\/node","size\/XXL","kind\/api-change","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","area\/code-generation","needs-priority","needs-triage"]},{"title":"use metav1.InitialEventsAnnotationKey const","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nuse `metav1. InitialEventsAnnotationKey` introduced in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123708\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/assign @wojtek-t ","\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: c580d167505e4fa25144eaa18be6566285ea0ac7<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123937#\" title=\"Author self-approved\">p0lyn0mial<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123937#issuecomment-1997832768\" title=\"Approved\">wojtek-t<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)~~ [wojtek-t]\n- ~~[staging\/src\/k8s.io\/client-go\/tools\/cache\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/tools\/cache\/OWNERS)~~ [wojtek-t]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/triage accepted"],"labels":["kind\/cleanup","area\/apiserver","lgtm","sig\/api-machinery","size\/M","release-note-none","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"Code generator is no longer usable via `go run`","body":"### What happened?\n\nThe code generator was usable via `go run` up until v1.30.0-alpha.3:\r\n\r\n```console\r\n$ go version\r\ngo version go1.22.1 linux\/amd64\r\n```\r\n\r\n```console\r\n$ go run k8s.io\/code-generator\/cmd\/client-gen@v0.30.0-alpha.3 --help\r\nUsage of \/tmp\/go-build2845286753\/b001\/exe\/client-gen:\r\n[...]\r\n```\r\n\r\nSince v1.30.0-beta.0:\r\n\r\n```console\r\n$ go run k8s.io\/code-generator\/cmd\/client-gen@v0.30.0-beta.0 --help\r\ngo: k8s.io\/code-generator\/cmd\/client-gen@v0.30.0-beta.0 (in k8s.io\/code-generator@v0.30.0-beta.0):\r\n        The go.mod file for the module providing named packages contains one or\r\n        more replace directives. It must not contain directives that would cause\r\n        it to be interpreted differently than if it were the main module.\r\n```\n\n### What did you expect to happen?\n\nThe code generator works via `go run`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee above.\n\n### Anything else we need to know?\n\nThis may not be a supported way to use the code generator at all, but it worked quite well until now, and I thought I'd bring this to attention. IIUC the offending replace directive came in as part of the workspace support (#123529) and I'm not sure there's anything that can be done about it.\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# v1.30.0-beta.0\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\nN\/A\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n$ cat \/etc\/os-release\r\nBUG_REPORT_URL=\"https:\/\/github.com\/NixOS\/nixpkgs\/issues\"\r\nBUILD_ID=\"23.11.20240311.ddcd759\"\r\nDOCUMENTATION_URL=\"https:\/\/nixos.org\/learn.html\"\r\nHOME_URL=\"https:\/\/nixos.org\/\"\r\nID=nixos\r\nLOGO=\"nix-snowflake\"\r\nNAME=NixOS\r\nPRETTY_NAME=\"NixOS 23.11 (Tapir)\"\r\nSUPPORT_END=\"2024-06-30\"\r\nSUPPORT_URL=\"https:\/\/nixos.org\/community.html\"\r\nVERSION=\"23.11 (Tapir)\"\r\nVERSION_CODENAME=tapir\r\nVERSION_ID=\"23.11\"\r\n$ uname -srvmo\r\nLinux 6.6.21 #1-NixOS SMP PREEMPT_DYNAMIC Wed Mar  6 14:48:45 UTC 2024 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\nN\/A\n\n### Container runtime (CRI) and version (if applicable)\n\nN\/A\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nN\/A","comments":["\/sig codegen","@twz123: The label(s) `sig\/codegen` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123933#issuecomment-1997491196):\n\n>\/sig codegen\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig api-machinery","\/assign @thockin\r\nLooks like a side effect of the workspace support. Could you take a look?\r\n\/triage accepted","Ugh.  The workspaces work means that code-generator has a dep (`require`) on apimachinery (https:\/\/github.com\/kubernetes\/code-generator\/blob\/master\/go.mod#L15) and the tooling we use to publish adds that as a `replace` (https:\/\/github.com\/kubernetes\/code-generator\/blob\/master\/go.mod#L40).\r\n\r\nIt seems redundant to me  - the require and replace are the same version.\r\n\r\n@liggitt you know the go deps stuff better than me - couldn't we elide that `replace` when publishing?\r\n\r\n","makes sense to me, was proposed in https:\/\/github.com\/kubernetes\/publishing-bot\/issues\/214 https:\/\/github.com\/kubernetes\/publishing-bot\/pull\/207 but never done","and actually, we ensure there are no replace directives that modify versions any more, so we should be able to drop all replace directives when publishing tags now","opened https:\/\/github.com\/kubernetes\/publishing-bot\/pull\/415 to resolve this at publish time but I'm not sure how to test publishing-bot changes","This should be fix on the next tag!"],"labels":["kind\/bug","sig\/api-machinery","triage\/accepted"]},{"title":"readiness prober timeout do not run as expected","body":"### What happened?\r\n\r\n1. We meet the issue that the readiness probe timeout do not run as expected.  It run as 2mins + timeout setting in actual \r\n\r\n2. According  to the below official kubernets doc, it define that the timeoutSeconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.  After I read this infromation from the doc , i think it means if the prober command running time exceeded the timeout period, the kubelet will stop this prober action and start the next prober, butl it not in actual.  The next prober action  will run after the last prober  action completed or last prober action lasted (2mins + timeout setting).\r\n\r\nhttps:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/#define-readiness-probes\r\n\r\n3. I also check the code, and find that it deifine timeous =timeout setting  + default timeout (2 minutes) as timeout to leave some time for  the runtime to do cleanup uless timeout setting is 0s. I think 2 minutes is used for the containerd runtime to prepare necessary action. But the doc description make the customer confusion easily if not add  some details\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/d1a2a134c532109540025c990697a6900c2e62fc\/pkg\/kubelet\/cri\/remote\/remote_runtime.go#L469\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/3a53ef8c-e01a-40c9-8582-65c56feabb88)\r\n\r\n\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nCould you plz modify the deafult timeout in code  or add description in  the officail doc ?\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. create a configmap  like below. it show the two commd: 1. create a file named with the current time; 2.  sleep 300s\r\n```\r\napiVersion: v1\r\ndata:\r\n  container-readiness-probe-handler.sh: |-\r\n    #!\/bin\/sh\r\n    touch \/tmp\/$(date +'%Y-%m-%d-%H-%M-%S')\r\n    sleep 300\r\nkind: ConfigMap\r\nmetadata:\r\n  name: tesh\r\n  namespace: default\r\n```\r\n\r\n\r\n2. create a pod with the nginx image and mount the above configmap as the volume  to the \/script path.  Setting the timeoutSeconds as 8s\r\n```\r\n containers:\r\n  - image: nginx:latest\r\n    imagePullPolicy: Always\r\n    name: nginx-test\r\n    readinessProbe:\r\n      exec:\r\n        command:\r\n        - \/bin\/sh\r\n        - \/script\/container-readiness-probe-handler.sh\r\n      failureThreshold: 7\r\n      initialDelaySeconds: 6\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 8\r\n    resources:\r\n      requests:\r\n        cpu: 250m\r\n        memory: 512Mi\r\n    - mountPath: \/script\r\n      name: volume-1710223576413\r\n  volumes:\r\n  - configMap:\r\n      defaultMode: 493\r\n      name: tesh\r\n    name: volume-1710223576413\r\n```\r\n    \r\n    \r\n   3.  After the pod running , will find that the new file will be created every (2mins+8s\uff09=128s in \/tmp path\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/d426d039-81d8-4396-8255-2aca74ed2a0c)\r\n\r\n   4.  strace the cri which pull the containerd and find that  it will trigger the prober action every  (2mins+8s\uff09\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/6cbc9a21-6e3e-40d2-bf30-70dbe5aea9de)\r\n\r\n5. if the  timeout period setting < prober command runiig time< 2 mins.  The new readiness prober will run after last prober command completed, not timeout  period\r\n\r\n\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.3-aliyun.1\r\n\r\n\r\n\r\n### Cloud provider\r\n\r\nAlibaba Cloud\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Alibaba Cloud Linux (Aliyun Linux)\"\r\nVERSION=\"2.1903 LTS (Hunting Beagle)\"\r\nID=\"alinux\"\r\nID_LIKE=\"rhel fedora centos anolis\"\r\nVERSION_ID=\"2.1903\"\r\nPRETTY_NAME=\"Alibaba Cloud Linux (Aliyun Linux) 2.1903 LTS (Hunting Beagle)\"\r\nANSI_COLOR=\"0;31\"\r\nHOME_URL=\"https:\/\/www.aliyun.com\/\"\r\n\r\n$ uname -a\r\nLinux iZj6c9edi9gpcd5qesfwxvZ 4.19.91-25.6.al7.x86_64 #1 SMP Thu Feb 10 19:15:17 CST 2022 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\n","The timeout code you linked to is the timeout on the CRI call the to the runtime, not the actual timeout on the probe exec call. That should be the timeout specified in the ExecSync request here: \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/d1a2a134c532109540025c990697a6900c2e62fc\/pkg\/kubelet\/cri\/remote\/remote_runtime.go#L487-L493\r\n\r\nIf the longer timeout is being hit, that likely indicates a problem with the runtime, e.g. it timed out creating the exec process, or making the request. What version of containerd are you running?\r\n\r\nI wasn't able to reproduce this with a quick check:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name:   probe-timeout\r\nspec:\r\n  volumes:\r\n    - name: probe-log\r\n      emptyDir: {}\r\n  containers:\r\n    - name:  alpine\r\n      image: alpine:latest\r\n      args: [sh, -c, 'while true; do sleep 100000; done']\r\n      readinessProbe:      \r\n        failureThreshold: 3\r\n        periodSeconds: 10\r\n        successThreshold: 1\r\n        timeoutSeconds: 5\r\n        exec:\r\n          command: ['sh', '-c', 'date >> \/probe\/readiness-log.txt && sleep 300']\r\n      volumeMounts:\r\n        - mountPath: \/probe\r\n          name: probe-log\r\n```\r\nOutput\r\n```\r\n$ kubectl exec -t probe-timeout -- tail -f \/probe\/readiness-log.txt\r\nThu Mar 14 21:00:19 UTC 2024\r\nThu Mar 14 21:00:29 UTC 2024\r\nThu Mar 14 21:00:34 UTC 2024\r\nThu Mar 14 21:00:39 UTC 2024\r\nThu Mar 14 21:00:49 UTC 2024\r\nThu Mar 14 21:00:59 UTC 2024\r\nThu Mar 14 21:01:09 UTC 2024\r\nThu Mar 14 21:01:19 UTC 2024\r\nThu Mar 14 21:01:29 UTC 2024\r\nThu Mar 14 21:01:39 UTC 2024\r\nThu Mar 14 21:01:49 UTC 2024\r\nThu Mar 14 21:01:54 UTC 2024\r\nThu Mar 14 21:01:59 UTC 2024\r\nThu Mar 14 21:02:09 UTC 2024\r\n```\r\n\r\nNote that the interval is sometimes 5s, and sometimes 10s. The 10s interval comes from the probe period, but the 5s is probably from retries on probe error. I'm not sure why it's inconsistent.\r\n\r\nIn either case, this sounds like it's probably a containerd issue, not a Kubernetes issue.","@tallclair  \r\ncontainerd containerd.io 1.6.20 405c12edca9fce99095c9827fca59ce0d6da128e\r\n\r\n I think this is due to the image . If you use the nginx instead of the alpine, you could reproduce the issue . I also make a strace for the busybox and nginx image . And found that the the sleep 300 is not triggered  in busybox or alpine image , so it will trigger the readiness prober every timeout period.\r\n1.  from the strace, you coud  find that when cri exec the command ,it only run the date command without following sleep command. So the prober oocur every timeout period. only touch command  process without the prober process\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/4e544f31-d6b2-4051-9bc4-119511dc6c9c)\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/9cd97aec-6e88-4087-b8cb-5af41df98560)\r\n\r\n2. But when use the nginx image, the sleep 300 is run after the date command . from the pstree , can check the pid is triggered\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/87382c62-5311-488e-8647-0c928fb9cf69)\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/88937681\/7998f168-3796-488b-b66c-ff18189ac29f)\r\n\r\n","Ah, interesting. I swapped the alpine image for nginx, and sure enough it replicated:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name:   probe-timeout\r\nspec:\r\n  volumes:\r\n    - name: probe-log\r\n      emptyDir: {}\r\n  containers:\r\n    - name:  test\r\n      image: nginx\r\n      args: [sh, -c, 'while true; do sleep 100000; done']\r\n      readinessProbe:\r\n        timeoutSeconds: 5\r\n        exec:\r\n          command: ['sh', '-c', 'date >> \/probe\/readiness-log.txt && sleep 45']\r\n      volumeMounts:\r\n        - mountPath: \/probe\r\n          name: probe-log\r\n```\r\n\r\nOutput:\r\n```\r\n$ kubectl exec -t probe-timeout -- tail -f \/probe\/readiness-log.txt\r\nTue Mar 19 23:35:13 UTC 2024\r\nTue Mar 19 23:35:58 UTC 2024\r\nTue Mar 19 23:36:43 UTC 2024\r\nTue Mar 19 23:37:28 UTC 2024\r\nTue Mar 19 23:38:13 UTC 2024\r\nTue Mar 19 23:38:58 UTC 2024\r\nTue Mar 19 23:39:43 UTC 2024\r\nTue Mar 19 23:40:28 UTC 2024\r\n```\r\n\r\nProbe taking the full 45 seconds, rather than the desired 5.\r\n\r\nThis looks like a containerd issue, not a Kubelet issue. I found https:\/\/github.com\/containerd\/containerd\/issues\/9568 reporting the same issue."],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"Move DockerHung test in the end","body":"#### What type of PR is this?\r\n\r\n\/kind failing-test\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nChecking the logs from https:\/\/gcsweb.k8s.io\/gcs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768108836683517952\/artifacts\/bootstrap-e2e-minion-group-sgqx\/ i noticed that container is destroyed \r\n\r\nI see also from kernel logs that docker is blocked for 120second ( around the same time the container is destroyed )\r\n\r\nCould it be that Docker Hung should be moved to be last in the test to avoid deleting a container? \r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/159060890\/e7714e63-9d6d-4741-85e5-8d3f0af55647)\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\nAttempt to fix https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123928\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n\r\n```\r\n","comments":["Hi @esotsal. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123930#\" title=\"Author self-approved\">esotsal<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [tallclair](https:\/\/github.com\/tallclair) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e\/node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"tallclair\"]} -->","\/triage accepted\r\n\/priority important-longterm\r\n\/ok-to-test","\/cc @kannon92 @SergeyKanzhelev ","\/retest"],"labels":["area\/test","sig\/node","size\/M","release-note-none","cncf-cla: yes","sig\/testing","priority\/important-longterm","kind\/failing-test","ok-to-test","triage\/accepted"]},{"title":"build\/rsyncd: replace newlines with spaces when generating ALLOW_HOST list","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nWhen the ALLOW_LIST variable is generated, depending on the network configuration of the container runtime being used, there may be multiple addresses\/hosts returned.\r\n\r\nThis PR replaces newlines with spaces, leading to my build working \ud83c\udf89 \r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes #99025 (I think..)\r\n\r\n#### Special notes for your reviewer:\r\n\r\nAs noted in the linked issue, building with anything other than Docker isn't expected to be supported\/work.\r\n\r\nThat said, this one line change *does* enable it to work in a wider range of circumstances\/situations, and as far as I can tell will have no adverse effects on other existing users.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n\/cc @BenTheElder ","comments":["\/retest","We may be better off doing this to both ALLOW as well as ALLOW_HOST but I'm not too familiar with the precise difference\/expectations enough to know whether it makes sense: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/build-image\/rsyncd.sh#L75\n","We have a tracking issue somewhere to drop the rsync entirely as the original reason for adding it was docket for Mac performance with mounts, that has since improved and we've stopped advertising remote docket support in the docs O(years) ago, I've mostly eliminated our dependency on the rsync but we haven't actually dropped it yet.\r\n\r\nwe've said that remote builds should be done over eg ssh with the source local to the build and we're hoping to just use a bind mount","The rsync \/ data \/ build container hack is adding a lot of overhead and complexity to the build.","> we've said that remote builds should be done over eg ssh with the source local to the build and we're hoping to just use a bind mount\n\n'local' here is sort of relative to the networking setup though - the docker socket is listening on an address other than localhost, but the daemon is still 'local' (supports bind mounts etc)","> We may be better off doing this to both ALLOW as well as ALLOW_HOST but I'm not too familiar with the precise difference\/expectations enough to know whether it makes sense:\r\n\r\nI don't think it matters, because ALLOW should be inside the container, which should just be on the docker bridge and not have multiple interfaces, and if somehow it has multiple then you're almost certainly doing something unsupported that will break.\r\n\r\nThis change seems reasonable in the meantime.\r\n\r\n\/sig release\r\n\/triage accepted\r\n\/lgtm\r\n\/approve\r\n\/priority backlog","LGTM label has been added.  <details>Git tree hash: ca9735ca9901af5f47ac56dfc06fa262821d6b18<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123929#issuecomment-1998360432\" title=\"Approved\">BenTheElder<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123929#\" title=\"Author self-approved\">munnerz<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[build\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/OWNERS)~~ [BenTheElder]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["kind\/bug","priority\/backlog","lgtm","size\/XS","release-note-none","approved","cncf-cla: yes","sig\/release","triage\/accepted"]},{"title":"Failure cluster [fb7fd2c4...] in ci-kubernetes-e2e-gci-gce-alpha-enabled-default","body":"### Failure cluster [fb7fd2c4a1d5d868bc9f](https:\/\/go.k8s.io\/triage#fb7fd2c4a1d5d868bc9f)\r\n\r\n##### Error text:\r\n```\r\n[FAILED] an error on the server (\"Internal Error: failed to list pod stats: rpc error: code = Unknown desc = failed to decode sandbox container metrics for sandbox \\\"0171ff000af5ac84916cd7efd64b2fb10927406a38fb3fb088ac1852311ab8ea\\\": failed to get pod sandbox stats since sandbox container \\\"0171ff000af5ac84916cd7efd64b2fb10927406a38fb3fb088ac1852311ab8ea\\\" is not in ready state\") has prevented the request from succeeding (get nodes bootstrap-e2e-minion-group-3clh:10250)\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/node\/node_problem_detector.go:380 @ 02\/29\/24 00:30:22.402\r\n\r\n```\r\n#### Recent failures:\r\n[3\/14\/2024, 8:00:12 AM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768170244460777472)\r\n[3\/14\/2024, 7:20:11 AM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768160175937556480)\r\n[3\/14\/2024, 6:40:11 AM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768150109595373568)\r\n[3\/14\/2024, 5:57:11 AM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768139287661907968)\r\n[3\/14\/2024, 5:18:12 AM ci-kubernetes-e2e-gci-gce-alpha-enabled-default](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768129474517274624)\r\n\r\n\r\n\/kind failing-test\r\n<!-- If this is a flake, please add: \/kind flake -->\r\n\r\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","needs containerd 1.7.14 in the COS image - https:\/\/github.com\/containerd\/containerd\/releases\/tag\/v1.7.14\r\n\r\nspecifically https:\/\/github.com\/containerd\/containerd\/pull\/9905","Thanks, I missed that. \r\n\r\nChecking the logs from https:\/\/gcsweb.k8s.io\/gcs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-alpha-enabled-default\/1768108836683517952\/artifacts\/bootstrap-e2e-minion-group-sgqx\/ i noticed that container is destroyed \r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/159060890\/0e77af77-2bac-4e31-a234-e12e80a4a9c2)\r\n\r\nI see also from kernel logs that docker is blocked for 120second ( around the same time the container is destroyed )\r\n\r\nCould it be that Docker Hung should be moved to be last in the test to avoid deleting a container?  https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123930"],"labels":["sig\/node","kind\/failing-test","needs-triage"]},{"title":"apiserver\/storage\/cacher: decrease running time of TestCacheWatcherDrainingNoBookmarkAfterResourceVersionReceived","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nbefore:\r\n```\r\ngo test -v -race -count 1 -run ^TestCacheWatcherDrainingNoBookmarkAfterResourceVersionReceived$ ok  \tk8s.io\/apiserver\/pkg\/storage\/cacher\t3.792s\r\n```\r\nafter:\r\n```\r\ngo test -v -race -count 1 -run ^TestCacheWatcherDrainingNoBookmarkAfterResourceVersionReceived$ ok  \tk8s.io\/apiserver\/pkg\/storage\/cacher\t1.783s\r\n```\r\n\r\nxref: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850\r\nxref: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123685\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/assign @wojtek-t ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123927#\" title=\"Author self-approved\">p0lyn0mial<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [wojtek-t](https:\/\/github.com\/wojtek-t). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"wojtek-t\"]} -->","one alternative is to split some functionalities in different modules, maybe that is simpler and faster, just a FYI, I'm not really following this series","> one alternative is to split some functionalities in different modules, maybe that is simpler and faster, just a FYI, I'm not really following this series\r\n\r\nThis is a low level test that needs access to the cacher. Tests that treat the cacher as a black box could be moved to a separate package but before doing that we should try analyse them and see why they take long time to execute.\r\n\r\nThe other idea I had was to execute long running tests (> `1s`) in parallel (`t.Parallel`).","Also with the recent improvements we should be below the threshold, currently the package fail on https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123926","\/assign @pacoxu\r\nFor context. Thank you.\r\n\/triage accepted"],"labels":["kind\/cleanup","area\/apiserver","sig\/api-machinery","size\/S","release-note-none","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"[Flaking Test] [sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should ...","body":"\r\nActually, there are 2 flaking test \r\n- [ ]  [sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should be able to create and delete a critical pod\r\n - [ ] [sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should add DisruptionTarget condition to the preempted pod [NodeFeature:PodDisruptionConditions]\r\n\r\n\r\nBoth are flaking in https:\/\/testgrid.k8s.io\/sig-node-release-blocking#node-kubelet-serial-containerd.\r\n\r\nSome old issues that related are\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/109296\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103690\r\n\r\n### Failure cluster [380df937894e999e459f](https:\/\/go.k8s.io\/triage#380df937894e999e459f)\r\n\r\n##### Error text:\r\n\r\nE2eNode Suite [It] [sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should be able to create and delete a critical pod\r\n```\r\n[FAILED] Told to stop trying after 2.004s.\r\nThe phase of Pod static-critical-pod is Failed which is unexpected.\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/framework\/pod\/pod_client.go:106 @ 03\/05\/24 09:14:59.881\r\n\r\n```\r\n\r\n\r\nE2eNode Suite [It] [sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should add DisruptionTarget condition to the preempted pod [NodeFeature:PodDisruptionConditions]\r\n\r\n```\r\n        status:\r\n          message: 'Pod was rejected: Unexpected error while attempting to recover from admission\r\n            failure: preemption: error finding a set of pods to preempt: no set of running\r\n            pods found to reclaim resources: [(res: memory, q: 15728640), ]'\r\n          phase: Failed\r\n          reason: UnexpectedAdmissionError\r\n          startTime: \"2024-03-09T11:55:48Z\"\r\n```\r\n\r\n#### Recent failures:\r\n[2024\/3\/14 03:01:15 ci-cgroupv2-containerd-node-arm64-al2023-e2e-serial-ec2-eks](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-arm64-al2023-e2e-serial-ec2-eks\/1767989046358839296)\r\n[2024\/3\/10 20:55:56 ci-cgroupv2-containerd-node-e2e-serial-ec2](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-e2e-serial-ec2\/1766809953827295232)\r\n[2024\/3\/10 20:32:43 ci-kubernetes-node-swap-fedora-serial](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-swap-fedora-serial\/1766804165771661312)\r\n[2024\/3\/10 02:54:57 ci-cgroupv2-containerd-node-arm64-e2e-serial-ec2](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-arm64-e2e-serial-ec2\/1766537909143343104)\r\n[2024\/3\/8 21:49:09 ci-kubernetes-node-kubelet-serial-containerd](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-kubelet-serial-containerd\/1766098621788327936)\r\n\r\n\r\n\r\n\r\n\/kind flake\r\n\/sig node","comments":["\/cc @saschagrunert @ruiwen-zhao @ffromani \r\nwho was handling the previous failures\/flakes.","\/triage accepted\r\n\r\nI think this flake is pretty old: `[sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should be able to create and delete a critical pod`\r\n\r\nI'm not sure if the second one (`[sig-node] CriticalPod [Serial] [Disruptive] [NodeFeature:CriticalPod] when we need to admit a critical pod should add DisruptionTarget condition to the preempted pod [NodeFeature:PodDisruptionConditions]`) is related to that.\r\n\r\nI may find some time to look into that in the next weeks :+1: "],"labels":["sig\/node","kind\/flake","triage\/accepted"]},{"title":"Provide Support  to HPA to autoscale the target resource by label as well ","body":"### What would you like to be added?\r\n\r\nCurrently in HPA,  we define target resource  in below format. \r\n\r\n```\r\nspec:\r\n  scaleTargetRef:\r\n    apiVersion: apps\/v1\r\n    kind: Deployment\r\n    name: my-app-deployment\r\n\r\n```\r\nit basically scales the resource matching above apiVersion, kind and name. \r\n\r\nAlong with above feature, if we add label. it will scale up all the resources matching that label for given kind and apiVersion\r\n\r\nExample:\r\n```\r\nspec:\r\n  scaleTargetRef:\r\n    apiVersion: apps\/v1\r\n    kind: Deployment\r\n    label:\r\n      app.kubernetes.io\/name:  my-app\r\n```\r\n\r\n\r\n\r\n\r\n### Why is this needed?\r\n\r\nWe had a requirement to scale up the multiple resources to the same number of replicas during high CPU usage. \r\n\r\nBut if we use HPA for each deployment, the replica count is not matching with other resources. \r\n\r\nLet say, we have 3 deployments\r\n1. Deployment-1\r\n2. Deployment-2\r\n3. Deployment-3\r\n\r\nDuring peak usage,  if we have requirement scale all 3 deployments by same number of replicas, we can have common label to scale all 3 deployment,. ","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig autoscaling\r\n\r\n\r\n\r\n[autoscaling](https:\/\/github.com\/kubernetes\/kubernetes\/labels\/sig%2Fautoscaling)","\/wg autoscaling","@shashikumar09: The label(s) `wg\/autoscaling` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123923#issuecomment-1996609545):\n\n>\/wg autoscaling\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/wg batch","If the end goal is to scale deployments with the **_same_** number of replicas under certain **_common_** conditions verified by the HPA, wouldn't it make more sense to just have the workloads of the 3 deployments bundled in the same deployment ?"],"labels":["sig\/autoscaling","kind\/feature","needs-triage","wg\/batch"]},{"title":"[Flaking Test] [sig-node] [GracefulNodeShutdownBasedOnPodPriority] when gracefully shutting down with Pod priority should be able to gracefully shutdown pods with various grace periods ","body":"### Which jobs are flaking?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-kubelet-serial-containerd\/1767914121111539712\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-kubelet-serial-containerd\/1767792820610928640\n\n### Which tests are flaking?\n\n- [sig-node] GracefulNodeShutdown [Serial] [NodeFeature:GracefulNodeShutdown] [NodeFeature:GracefulNodeShutdownBasedOnPodPriority] when gracefully shutting down with Pod priority should be able to gracefully shutdown pods with various grace periods \n\n### Since when has it been flaking?\n\nN\/A\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-node-release-blocking#node-kubelet-serial-containerd\n\n### Reason for failure (if possible)\n\n```\r\n{ failed [FAILED] Error creating Pod: pods \"period-c-5-be638242-92ae-4bc4-a16c-4245428b9465\" is forbidden: no PriorityClass with name custom-class-c was found\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/framework\/pod\/pod_client.go:99 @ 03\/13\/24 07:40:07.519\r\n}\r\n```\n\n### Anything else we need to know?\n\n\/cc @bobbypage @wzshiming \n\n### Relevant SIG(s)\n\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/node","kind\/flake","needs-triage"]},{"title":"kubelet: emit OptionalRuntimeConditionUnsatisfied events","body":"\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind feature (?)\r\n\r\n#### What this PR does \/ why we need it:\r\nPropagate conditions like `ContainerdHasNoDeprecationWarnings=false` to users.\r\n\r\nAlternative to:\r\n- #123155\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123148\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThis should be cherry-picked to existing releases\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nkubelet: emit OptionalRuntimeConditionUnsatisfied events\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["@AkihiroSuda: The label(s) `kind\/(?)` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123920):\n\n>\r\n>\r\n>\r\n>\r\n>#### What type of PR is this?\r\n>\r\n>\r\n>\/kind feature (?)\r\n>\r\n>#### What this PR does \/ why we need it:\r\n>Propagate conditions like `ContainerdHasNoDeprecationWarnings=false` to users.\r\n>\r\n>Alternative to:\r\n>- #123155\r\n>\r\n>#### Which issue(s) this PR fixes:\r\n>\r\n>Fixes #123148\r\n>\r\n>#### Special notes for your reviewer:\r\n>\r\n>#### Does this PR introduce a user-facing change?\r\n>\r\n>```release-note\r\n>kubelet: emit OptionalRuntimeConditionUnsatisfied events\r\n>```\r\n>\r\n>#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n>\r\n>\r\n>```docs\r\n>\r\n>```\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123920#\" title=\"Author self-approved\">AkihiroSuda<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [yujuhong](https:\/\/github.com\/yujuhong) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"yujuhong\"]} -->","An example event (containerd v1.7.14)\r\n\r\n```\r\n$ kubectl describe node\r\n[...]\r\n  Warning  OptionalRuntimeConditionUnsatisfied  2m55s (x53 over 7m15s)  kubelet          ContainerdHasNoDeprecationWarnings is unsatisfied: ContainerdHasDeprecationWarnings: \"{\\\"io.containerd.deprecation\/pull-schema-1-image\\\":\\\"Schema 1 images are deprecated since containerd v1.7 and removed in containerd v2.0. Since containerd v1.7.8, schema 1 images are identified by the \\\\\\\"io.containerd.image\/converted-docker-schema1\\\\\\\" label.\\\"}\"\r\n```\r\n","\/triage accepted\r\n\/priority important-longterm","Changelog suggestion\r\n```diff\r\n-kubelet: emit OptionalRuntimeConditionUnsatisfied events\r\n+kubelet: Added event reporting about optional runtime conditions.\r\n+The kubelet now emits an Event with the `OptionalRuntimeConditionUnsatisfied` when\r\n+an optional condition is not satisfied (for the first time).\r\n```"],"labels":["area\/kubelet","sig\/node","release-note","size\/S","kind\/feature","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"Annotate APF Wait Queue Latnecy  at the request level","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR annotates the APIServer audit request with APF wait queue latency.\r\nExisting latency annotations in audit logs do not include an annotation for the time a request spent in an APF queue.\r\nThis will help in further drill down of the latencies experienced at individual request level when debugging latency related issues after the fact.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\nN\/A ( I can create one if needed)\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nAdd apiserver.latency.k8s.io\/apf-queue-wait annotation to the audit log to record the time spent waiting in apf queue\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123919#\" title=\"Author self-approved\">hakuna-matatah<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https:\/\/github.com\/apelisse) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"apelisse\"]} -->","\/cc @wojtek-t","sig\/api-machinery","\/retest","\/cc @dims ","\/retest","Unit tests has been failing already for other reasons being discussed [here](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850)","\/assign @liggitt \r\nFor APF\r\nThank you.\r\n\/triage accepted","\/retest ","@hakuna-matatah: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | 9a0d922a296e837b344d5538c0829d9e222c2ef6 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123919\/pull-kubernetes-unit\/1769773544838795264) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123919). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Ahakuna-matatah). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/apiserver","sig\/api-machinery","release-note","size\/XS","kind\/feature","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"StatefulSet with podManagementPolicy=OrderedReady and minReadySeconds does not scale down correctly","body":"### What happened?\r\n\r\n1. create the OrderedReady statefulset\r\n\r\n```\r\napiVersion: apps\/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  name: nginx-roll\r\nspec:\r\n  replicas: 2\r\n  minReadySeconds: 30\r\n  podManagementPolicy: OrderedReady\r\n  updateStrategy:\r\n    type: RollingUpdate\r\n  selector:\r\n    matchLabels:\r\n      app: nginx-roll\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx-roll\r\n    spec:\r\n      containers:\r\n      - name: nginx\r\n        image: ghcr.io\/nginxinc\/nginx-unprivileged:latest\r\n        ports:\r\n        - containerPort: 80\r\n          name: web\r\n```\r\n\r\n\r\n2. wait for the 2nd pod to become ready, but not available (can also happen when the pod has lost availability)\r\n3. scale the statefulset to 1 replica\r\n \r\n```\r\nkubectl scale statefulset nginx-roll --replicas=1\r\n```\r\n\r\n4. the 2nd pod should start terminating immediately but instead it hangs until the KCM is fully resynced\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nthe 2nd pod should start terminating immediately\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nsee `What happened?` \r\n\r\n### Anything else we need to know?\r\n\r\nwe can see in the logs that the syncs are happening but the firstUnhealthyPod variable is not resolved, so the progress is stalled\r\n\r\n```\r\nstateful_set_control.go:509] \"StatefulSet is waiting for Pod to be Available prior to scale down\" statefulSet=\"test\/nginx-roll\" pod=\"\"\r\n```\r\n\r\nWe even get  the availability check scheduled and called, but because a prior scheduling already ocurred, the new check is thrown away (should not happen, but a similar issue was reported in https:\/\/github.com\/kubernetes\/kubernetes\/issues\/119352 and a fix is blocked on https:\/\/github.com\/kubernetes\/kubernetes\/pull\/112328)\r\n\r\n```\r\nstateful_set.go:243] \"StatefulSet will be enqueued after minReadySeconds for availability check\" statefulSet=\"test\/nginx-roll\" minReadySeconds=30\r\n```\r\n\r\nthe sync will be called too soon and resolves again in \r\n```\r\nstateful_set_control.go:509] \"StatefulSet is waiting for Pod to be Available prior to scale down\" statefulSet=\"test\/nginx-roll\" pod=\"\"\r\n```\r\n\r\nThe next sync will ocurr when KCM is fully resynced which can be a long time (depending on KCM resync period)\r\n\r\n\r\n\r\nstatefulset controller should scale down the first condemned pod as soon as it can, but keep the predecessor pods running available\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.0-rc.1.3813+a0beecc776d492-dirty\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\nNA\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig apps\r\n\/triage accepted\r\n\/priority important-longterm"],"labels":["kind\/bug","sig\/apps","priority\/important-longterm","triage\/accepted"]},{"title":"Update kube-dns and nodelocaldns to 1.23.0","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThe PR bumps `kube-dns` and `nodelocaldns` to the latest version `1.23.0`. The new version of kube-dns includes dnsmasq v2.90, which (among other things)\r\n * fixes the long-standing memory leak for SRV records with zero TTL, and\r\n * introduces the flag --max-tcp-connections to adjust the previously hardcoded limit on the number of concurrent TCP connections.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nN\/A\r\n\r\n#### Special notes for your reviewer:\r\n\r\n`kube-dns 1.23.0` upgrades `dnsmasq` to `v2.90`.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @aojea ","\/lgtm\r\n\/approve\r\n\r\n@DamianSawicki please remove the release note, this changes are only used by some CI jobs, are not exposed to the users","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123917#issuecomment-1996006228\" title=\"Approved\">aojea<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123917#\" title=\"Author self-approved\">DamianSawicki<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [bowei](https:\/\/github.com\/bowei) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cluster\/addons\/dns\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/addons\/dns\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"bowei\"]} -->","LGTM label has been added.  <details>Git tree hash: 3a4670c14deef3c6fa592c95f693c5490e462fa3<\/details>","> \/lgtm \/approve\r\n> \r\n> @DamianSawicki please remove the release note, this changes are only used by some CI jobs, are not exposed to the users\r\n\r\nThanks for reviewing! I removed the release note as instructed and moved the relevant info to _What this PR does \/ why we need it_.\r\n","Thanks, this can wait for next release as is an internal CI details\r\n\/milestone v1.31"],"labels":["kind\/cleanup","lgtm","area\/provider\/gcp","size\/S","release-note-none","cncf-cla: yes","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"Update GroupPolicy to disable Windows Automatic Updates","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\/sig windows\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nI found that the Windows Update service started even though we disable it. I found out that Windows has a default Group Policy that schedules automatic updates via Windows Update. This re-enables the service again even though it is disabled. In this PR, we disable this policy.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @AnishShah. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/cc @mauriciopoppe","Nice! What command did you run to verify that the policy was disabled? ","\/assign @ibabou ","\/ok-to-test","The command to check that it was set is:\r\n\r\n```\r\nPS C:\\Users\\mauriciopoppe> Get-ItemProperty HKLM:\\Software\\Policies\\Microsoft\\Windows\\WindowsUpdate\\AU\r\n\r\n\r\n(default)            :\r\nAUOptions            : 3\r\nScheduledInstallDay  : 0\r\nScheduledInstallTime : 3\r\nNoAutoUpdate         : 1\r\nPSPath               : Microsoft.PowerShell.Core\\Registry::HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Windows\\Windo\r\n                       wsUpdate\\AU\r\nPSParentPath         : Microsoft.PowerShell.Core\\Registry::HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Windows\\Windo\r\n                       wsUpdate\r\nPSChildName          : AU\r\nPSDrive              : HKLM\r\nPSProvider           : Microsoft.PowerShell.Core\\Registry\r\n\r\n```\r\n\r\nMy dev instance hasn't restarted in around 16 days:\r\n\r\n```\r\nPS C:\\Users\\mauriciopoppe> Get-Uptime\r\n\r\nComputername   Days   Hours   Minutes   Seconds\r\n------------   ----   -----   -------   -------\r\nlocalhost      16     22      6         20\r\n```\r\n\r\nI don't have a baseline though i.e. I didn't see that without this fix the VM was auto updated.","\/retest","LGTM label has been added.  <details>Git tree hash: ac827bfe3345a1d48183f2121ac0c2e7dbd2c302<\/details>","\/approve\r\n\r\nBut we can wait until the code freeze is lifted.","\/cc @marosset ","I don't see any Windows testing being run in the CI jobs linked here. How do I run the Windows test? cc: @SergeyKanzhelev , @marosset ","I see periodic tests instead of presubmit tests, unfortunately it looks like the tests have been broken for a really long time ([testgrid](https:\/\/testgrid.k8s.io\/sig-windows-gce#gce-windows-2022-containerd-master), [ProwJob source](https:\/\/github.com\/kubernetes\/test-infra\/blob\/master\/config\/jobs\/kubernetes\/sig-windows\/windows-gce.yaml)) ","Unless we're actually running windows tests using these scripts (I'm not sure we are), I'm -1 on continuing to change them. If these are no longer used, let's remove them instead.","> Unless we're actually running windows tests using these scripts (I'm not sure we are), I'm -1 on continuing to change them. If these are no longer used, let's remove them instead.\r\n\r\n+1, @marosset can you please comment on this?","\/lgtm","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123916#\" title=\"Author self-approved\">AnishShah<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123916#pullrequestreview-1940064034\" title=\"Approved\">ibabou<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123916#issuecomment-1995942254\" title=\"Approved\">yujuhong<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[cluster\/gce\/windows\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/windows\/OWNERS)~~ [ibabou,yujuhong]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","LGTM label has been added.  <details>Git tree hash: 4c6bfbe0fdc107117c22ba976ec6387e8ad32d1f<\/details>","> > Unless we're actually running windows tests using these scripts (I'm not sure we are), I'm -1 on continuing to change them. If these are no longer used, let's remove them instead.\r\n> \r\n> +1, @marosset can you please comment on this?\r\n\r\n\/hold for an answer to this question","\/assign @marosset ","New changes are detected. LGTM label has been removed.","\/retest","@AnishShah: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | 33f7f19c35c59d48f315abe2cf147f29ff1e73fb | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123916\/pull-kubernetes-unit\/1769802627165458432) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123916). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AAnishShah). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","These scripts were used by the google jobs. We don't use them for sig-windows release jobs or PR jobs any longer.  There might be a few GCE Windows jobs still running but not sure what the state or those are, @ibabou are you still monitoring and maintaining them?\r\n\r\nOtherwise the changes LGTM, We disable automated Updates on Azure jobs as well.","Looks like there are two sets of tests for `sig-windows`: [GCE Jobs](https:\/\/github.com\/kubernetes\/test-infra\/blob\/546c34e1df034ee3683ee7584368e4ad3bbf4dff\/config\/jobs\/kubernetes\/sig-windows\/windows-gce.yaml),  [Azure Jobs](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/config\/jobs\/kubernetes-sigs\/sig-windows).\r\n\r\nBased on the [testgrid](https:\/\/testgrid.k8s.io\/sig-windows), it looks like the GCE Jobs have been failing for quite a while and not maintained whereas Azure jobs are being used for presubmits and release process. Should we remove these GCE jobs and Powershell bootstrap scripts?","Can we say when did AU started getting turned by default? Was this always the case, or was this introduced by a new version of Windows (or something else)?"],"labels":["kind\/bug","area\/provider\/gcp","size\/XS","release-note-none","approved","sig\/windows","cncf-cla: yes","do-not-merge\/hold","sig\/cloud-provider","ok-to-test","needs-priority","needs-triage"]},{"title":"Fix StatefulSet MaxUnavailable with MinReadySeconds, and add necessary e2e tests","body":"#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\nThis is necessary to promote this feature from alpha to beta. StatefulSet MaxUnavailable needs to honor MinReadySeconds, not ignore it. \r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #112307\r\n\r\n#### Special notes for your reviewer:\r\nRelates to https:\/\/github.com\/kubernetes\/enhancements\/pull\/4474\r\n\r\nAddresses https:\/\/github.com\/kubernetes\/enhancements\/pull\/4474#discussion_r1478474701\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nStatefulSet MaxUnavailable now properly honors MinReadySeconds.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/pull\/4474\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: bersalazar \/ name: Bernardo Salazar  (4ce2b964129ec9466573762404afc982222c16b8)<\/li><li>:white_check_mark: login: leomichalski \/ name: Leonardo  (bfce32798dbaef181c3952a919026d15ace0d112)<\/li><\/ul>","Welcome @leomichalski! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @leomichalski. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123915#\" title=\"Author self-approved\">leomichalski<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [kow3ns](https:\/\/github.com\/kow3ns), [pohly](https:\/\/github.com\/pohly) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controller\/statefulset\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/statefulset\/OWNERS)**\n- **[test\/e2e\/apps\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/apps\/OWNERS)**\n- **[test\/e2e\/feature\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/feature\/OWNERS)**\n- **[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"kow3ns\",\"pohly\"]} -->","\/cc @knelasevero\r\n\/cc @bersalazar\r\n\/cc @yuxiang-he\r\n","@leomichalski: GitHub didn't allow me to request PR reviews from the following users: bersalazar, yuxiang-he.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) and repo collaborators can review this PR, and authors cannot review their own PRs.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123915#issuecomment-1994609361):\n\n>\/cc @knelasevero\r\n>\/cc @bersalazar\r\n>\/cc @yuxiang-he\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/ok-to-test","Here are the results of manual testing. The next comment contains instructions on how to reproduce the test results.\r\n\r\nTest logs without the fix (1 test fails and 2 pass, as they should)\r\n\r\n```\r\n  I0313 11:05:41.146320 119445 test_context.go:561] The --provider flag is not set. Continuing as if --provider=skeleton had been used.\r\n=== RUN   TestE2E\r\n  I0313 11:05:41.146442  119445 e2e.go:109] Starting e2e run \"9655e93e-fd18-440b-afad-54c7b0368250\" on Ginkgo node 1\r\nRunning Suite: Kubernetes e2e suite - \/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\r\n======================================================================================\r\nRandom Seed: 1710338740 - will randomize all specs\r\n\r\nWill run 3 of 7197 specs\r\n------------------------------\r\n[ReportBeforeSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e_test.go:157\r\n[ReportBeforeSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n[SynchronizedBeforeSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e.go:69\r\n  I0313 11:05:41.286008 119445 util.go:506] >>> kubeConfig: \/home\/lab\/.kube\/config\r\n  I0313 11:05:41.287699 119445 helper.go:48] Waiting up to 30m0s for all (but 0) nodes to be schedulable\r\n  I0313 11:05:41.304627 119445 e2e.go:142] Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start\r\n  I0313 11:05:41.306530 119445 e2e.go:153] 1 \/ 1 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)\r\n  I0313 11:05:41.306559 119445 e2e.go:153] 1 \/ 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)\r\n  I0313 11:05:41.306568 119445 e2e.go:245] e2e test version: v0.0.0-master+$Format:%H$\r\n  I0313 11:05:41.307072 119445 e2e.go:254] kube-apiserver version: v1.27.3\r\n  I0313 11:05:41.307120 119445 util.go:506] >>> kubeConfig: \/home\/lab\/.kube\/config\r\n  I0313 11:05:41.309017 119445 e2e.go:383] Cluster IP family: ipv4\r\n[SynchronizedBeforeSuite] PASSED [0.023 seconds]\r\n------------------------------\r\nSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should update successfully with OrderedReady PodManagementPolicy [sig-apps]\r\n\r\n(...)\r\n\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should honor MinReadySeconds [sig-apps]\r\n(...)\r\n\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should update successfully with Parallel PodManagementPolicy [sig-apps]\r\n(...)\r\n\r\n------------------------------\r\n[SynchronizedAfterSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e.go:80\r\n  I0313 11:09:02.622261 119445 suites.go:34] Running AfterSuite actions on node 1\r\n[SynchronizedAfterSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n[ReportAfterSuite] Kubernetes e2e suite report\r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e_test.go:161\r\n[ReportAfterSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n\r\nSummarizing 1 Failure:\r\n  [FAIL] [sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature [It] should honor MinReadySeconds [sig-apps]\r\n  \/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/apps\/statefulset.go:1737\r\n\r\nRan 3 of 7197 Specs in 201.336 seconds\r\nFAIL! -- 2 Passed | 1 Failed | 0 Pending | 7194 Skipped\r\n--- FAIL: TestE2E (201.48s)\r\nFAIL\r\n```\r\n\r\n---\r\n---\r\n---\r\n\r\nTest logs with the fix (all tests pass)\r\n\r\n```\r\n  I0313 11:37:50.426229 158105 test_context.go:561] The --provider flag is not set. Continuing as if --provider=skeleton had been used.\r\n=== RUN   TestE2E\r\n  I0313 11:37:50.426346  158105 e2e.go:109] Starting e2e run \"9e852e18-c8e8-46e7-a983-d4eecc53ef50\" on Ginkgo node 1\r\nRunning Suite: Kubernetes e2e suite - \/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\r\n======================================================================================\r\nRandom Seed: 1710340670 - will randomize all specs\r\n\r\nWill run 3 of 7197 specs\r\n------------------------------\r\n[ReportBeforeSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e_test.go:157\r\n[ReportBeforeSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n[SynchronizedBeforeSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e.go:69\r\n  I0313 11:37:50.571733 158105 util.go:506] >>> kubeConfig: \/home\/lab\/.kube\/config\r\n  I0313 11:37:50.572734 158105 helper.go:48] Waiting up to 30m0s for all (but 0) nodes to be schedulable\r\n  I0313 11:37:50.586563 158105 e2e.go:142] Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start\r\n  I0313 11:37:50.589395 158105 e2e.go:153] 1 \/ 1 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)\r\n  I0313 11:37:50.589449 158105 e2e.go:153] 1 \/ 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)\r\n  I0313 11:37:50.589464 158105 e2e.go:245] e2e test version: v0.0.0-master+$Format:%H$\r\n  I0313 11:37:50.590674 158105 e2e.go:254] kube-apiserver version: v1.27.3\r\n  I0313 11:37:50.590785 158105 util.go:506] >>> kubeConfig: \/home\/lab\/.kube\/config\r\n  I0313 11:37:50.593755 158105 e2e.go:383] Cluster IP family: ipv4\r\n[SynchronizedBeforeSuite] PASSED [0.022 seconds]\r\n------------------------------\r\nSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should honor MinReadySeconds [sig-apps]\r\n(...)\r\n\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should update successfully with Parallel PodManagementPolicy [sig-apps]\r\n(...)\r\n\r\n------------------------------\r\n[sig-apps] Rolling update strategy with MaxUnavailable Rolling update with the MaxUnavailable feature should update successfully with OrderedReady PodManagementPolicy [sig-apps]\r\n(...)\r\n\r\n------------------------------\r\n[SynchronizedAfterSuite] \r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e.go:80\r\n  I0313 11:42:31.762832 158105 suites.go:34] Running AfterSuite actions on node 1\r\n[SynchronizedAfterSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n[ReportAfterSuite] Kubernetes e2e suite report\r\n\/home\/lab\/Desktop\/pareamento\/kubernetes\/test\/e2e\/e2e_test.go:161\r\n[ReportAfterSuite] PASSED [0.000 seconds]\r\n------------------------------\r\n\r\nRan 3 of 7197 Specs in 281.191 seconds\r\nSUCCESS! -- 3 Passed | 0 Failed | 0 Pending | 7194 Skipped\r\nPASS | FOCUSED\r\n```\r\n","Instructions to reproduce the tests.\r\n\r\n\r\n#### Setup\r\n\r\nSetup a cluster with \"MaxUnavailableStatefulSet\" feature gate enabled. The following Kind config may be used.\r\n\r\n```yaml\r\nkind: Cluster\r\napiVersion: kind.x-k8s.io\/v1alpha4\r\nfeatureGates:\r\n  MaxUnavailableStatefulSet: true\r\n```\r\n\r\nChange this line (https:\/\/github.com\/leomichalski\/kubernetes\/blob\/bfce32798dbaef181c3952a919026d15ace0d112\/test\/e2e\/apps\/statefulset.go#L1674) temporarily so only the right e2e test runs.\r\n\r\n```go\r\n\/\/ from\r\nvar _ = SIGDescribe(\"Rolling update strategy with MaxUnavailable\", feature.MaxUnavailableStatefulSet, func() {\r\n\r\n\r\n\/\/ to\r\nvar _ = SIGDescribe(\"Rolling update strategy with MaxUnavailable\", ginkgo.Focus, func() {\r\n```\r\n\r\n#### Run the test without the fix\r\n\r\nRun the test.\r\n\r\n```bash\r\ngo test -v .\/test\/e2e -ginkgo.v -kubeconfig=\/substitute\/with\/path\/to\/kubeconfig\r\n```\r\n\r\n#### Run the test with the fix\r\n\r\nDo the fix.\r\n\r\n```bash\r\n# Clone this branch.\r\ngit clone https:\/\/github.com\/leomichalski\/kubernetes -b fix\/minreadyseconds-maxunavailable\r\n\r\n# Build the kube controller manager.\r\nmake quick-release-images DBG=1\r\n\r\n# Load the new image to the Docker registry\r\ndocker load -i _output\/release-images\/amd64\/kube-controller-manager.tar\r\n\r\n# Load Docker image to Kind substituting \"SUBSTITUTE_WITH_TAG\"\r\nkind load docker-image registry.k8s.io\/kube-controller-manager-amd64:SUBSTITUTE_WITH_TAG\r\n\r\n# Edit, with your preferred editor, the `\/etc\/kubernetes\/manifests\/kube-controller-manager.yaml` file in the Kind docker container so it uses the new Docker image.\r\n\r\n# If necessary, delete the old kube controller manager pod so it starts a new one using the new Docker image.\r\n```\r\n\r\nRun the test.\r\n\r\n```bash\r\ngo test -v .\/test\/e2e -ginkgo.v -kubeconfig=\/substitute\/with\/path\/to\/kubeconfig\r\n```\r\n\r\n","\/hold\r\n\r\nIt's better to fix the StatefulSetMinReadySeconds beta feature before working on the StatefulSetMaxUnavailable alpha feature. Also because, as discussed here https:\/\/github.com\/kubernetes\/enhancements\/pull\/4474#discussion_r1478474701 , StatefulSetMinReadySeconds bugs are blocking StatefulSetMaxUnavailable graduation to beta.","I opened another PR just to fix the StatefulSetMinReadySeconds beta feature: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123975"],"labels":["kind\/bug","area\/test","release-note","size\/L","sig\/apps","cncf-cla: yes","sig\/testing","do-not-merge\/hold","ok-to-test","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"Spike: Investigate why cadvisor_stats_provider is reporting 0 process_count","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-ec2-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-cloud-provider-loadbalancer`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-containerd-flaky`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-providerless`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123914#issuecomment-1994475861):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-crio-cgrpv2-e2e\r\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123914#\" title=\"Author self-approved\">kannon92<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cblecker](https:\/\/github.com\/cblecker), [derekwaynecarr](https:\/\/github.com\/derekwaynecarr) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cblecker\",\"derekwaynecarr\"]} -->","\/test pull-kubernetes-node-crio-cgrpv2-e2e","@kannon92: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-dependencies | b5f46124b60203c1b0835c36b3972c5574dba37e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123914\/pull-kubernetes-dependencies\/1768008136725106688) | true | `\/test pull-kubernetes-dependencies`\npull-kubernetes-unit | b5f46124b60203c1b0835c36b3972c5574dba37e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123914\/pull-kubernetes-unit\/1768008152663461888) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123914). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Akannon92). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/test pull-kubernetes-node-crio-cgrpv2-e2e","\/test pull-kubernetes-node-crio-cgrpv2-e2e\r\n\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2","\/triage accepted\r\n\/priority important-soon","\/lgtm","LGTM label has been added.  <details>Git tree hash: bdd163e8678abfbaf84a10e48b192931a841c7fe<\/details>","\/kind failing-test\r\n\/hold\r\n\r\ncadvisor change needs to merge upstream."],"labels":["area\/test","priority\/important-soon","area\/kubelet","lgtm","sig\/node","size\/L","release-note-none","cncf-cla: yes","sig\/testing","kind\/failing-test","do-not-merge\/hold","triage\/accepted"]},{"title":"`kubectl exec -i` echoes passwords","body":"### What happened?\r\n\r\n`kubectl exec -i` sets up a terminal that prevents disabling echoing. So when an application ask for a password, it is visible in the terminal.\r\n\r\n```\r\n$ kubectl exec -i pod\/test -- passwd\r\nNew password: mypassword\r\nBAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word\r\nRetype new password: mypassword\r\nChanging password for user root.\r\npasswd: all authentication tokens updated successfully.\r\n```\r\n\r\nIt may seem that adding the `-t` is the way to help out here, but `-t` merges `stdout` and `stderr` together [on purpose](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/38998#issuecomment-268129969), unfortunately. So when you try to redirect the output of such command into a variable (or `stdout` to a file), you will never see the \"Password: \" promt because it gets written into `stdout` because of how  `kubectl exec -it` sets up the terminal using [raw option](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/a147693deb2e7f040cf367aae4a7ae5d1cb3e7aa\/staging\/src\/k8s.io\/kubectl\/pkg\/cmd\/exec\/exec.go#L295).\r\n\r\n```\r\n$ STDOUT_RESULT=\"$( kubectl exec -i pod\/test -- passwd )\"\r\nNew password: mypassword\r\nBAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word\r\nRetype new password: mypassword\r\n$ echo \"${STDOUT_RESULT}\"\r\nChanging password for user root.\r\npasswd: all authentication tokens updated successfully.\r\n```\r\n```\r\n$ STDOUT_RESULT=\"$( kubectl exec -it pod\/test -- passwd )\"\r\n# (Without any prompt, I have blindly typed two times `mypassword` into the terminal.)\r\n$\u00a0echo \"${STDOUT_RESULT}\"\r\nChanging password for user root.\r\nNew password: \r\nBAD PASSWORD: The password fails the dictionary check - it is based on a dictionary word\r\nRetype new password: \r\npasswd: all authentication tokens updated successfully.\r\n```\r\n\r\n\/sig cli\r\n\r\n### What did you expect to happen?\r\n\r\n`kubectl exec -i` should set up terminal in a way that allows disabling echo. Alternatively, `-t` should distinguish between `stdout` and `stderr`.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. `kubectl run test --image=registry.access.redhat.com\/ubi9\/ubi:9.3-1610@sha256:66233eebd72bb5baa25190d4f55e1dc3fff3a9b77186c1f91a0abdb274452072 -- sleep infinity`\r\n2. `kubectl exec -i pod\/test -- passwd`\r\n3. see the password echoed\r\n4. STDOUT_RESULT=\"$( kubectl exec -it pod\/test -- passwd )\"\r\n5. observe the missing password prompt\r\n\r\n### Anything else we need to know?\r\n\r\nI have used `passwd` as an example of a tool that ask for password on `stderr` and is available to everyone to reproduce this. It doesn't return much useful value on `stdout`, so here is a real word example of a `cqlsh` query that needs to ask for a password first:\r\n\r\n*(password echoed)*\r\n```\r\nTOKEN=\"$( kubectl exec -i \"service\/scylla-client\" -c scylla -- cqlsh --user cassandra -e \"SELECT salted_hash from system_auth.roles WHERE role = 'cassandra'\" )\"\r\n\/opt\/scylladb\/python3\/lib64\/python3.11\/getpass.py:91: GetPassWarning: Can not control echo on the terminal.\r\nWarning: Password input may be echoed.\r\nPassword: cassandra\r\n```\r\n\r\n*(no output but silently expects the password)*\r\n```\r\nTOKEN=\"$( kubectl exec -it \"service\/scylla-client\" -c scylla -- cqlsh --user cassandra -e \"SELECT salted_hash from system_auth.roles WHERE role = 'cassandra'\" )\"\r\n```\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nkubeadm version\r\nkubeadm version: &version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.28.2\", GitCommit:\"89a4ea3e1e4ddd7f7572286090359983e0387b2f\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T09:34:32Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n$ uname -a\r\nLinux ubuntu-2204 5.15.0-100-generic #110-Ubuntu SMP Wed Feb 7 13:27:48 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n$ crictl version\r\nVersion:  0.1.0\r\nRuntimeName:  cri-o\r\nRuntimeVersion:  1.24.6\r\nRuntimeApiVersion:  v1\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n$ kubectl version\r\nClient Version: v1.30.0-beta.0.7+c9384550cbf771\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.3\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","# Issue #123913\r\n\r\n#### You might have assumed the `-i` flag in `kubectl exec -i` would disable echo, allowing you to type passwords without them being displayed on the screen.Echoing is disabled by default with `-i`, but typed characters are visible in terminal history.\r\n#### The `-t` flag allocates a pseudo-tty (terminal) for the container. This does not distinguish between standard output and standard error. Both streams are combined and displayed on the terminal.\r\n\r\n## Try these approach\r\n\r\n- #### Environment Variables: Store passwords securely as environment variables within the container image. Access them using the designated environment variable name inside the container.\r\n\r\n- #### Kubernetes Secrets: Utilize `kubectl create secret` to store passwords securely. These secrets are encrypted and accessible to containers through volume mounts or environment variables. [Kubernetes Documentation (secrets)](https:\/\/kubernetes.io\/docs\/concepts\/configuration\/secret\/)","> Environment Variables: Store passwords securely as environment variables within the container image. Access them using the designated environment variable name inside the container.\r\n\r\n> Utilize kubectl create secret to store passwords securely.\r\n\r\nI can't see how that helps for the case of `kubectl exec`. What you are pointing to is how a Pod should be setup for the database (daemon) itself. The issue here is using the database CLI by a particular user, his password shouldn't be exposed to everyone through environment variable or a secret. (Plus the password hashes are stored within the database.)"],"labels":["kind\/bug","sig\/cli","needs-triage"]},{"title":"PDB views pods in Terminating state as available","body":"### What happened?\n\n1. I have a statefulset with 2 pods. The grace period for these pods is long, so the terminating state can endure over hours, but that's not important.\r\n2. I have a basic PDB of minAvailable: 1\r\n3. I have a service that distributes across both pods.\r\n4. I rollout restart the sts, which first restarts pod-1, then tries to restart pod-0.\r\n5. While pod-0 is still in status \"Terminating\", I issue a second rollout restart to the sts.\r\n6. K8s restarts pod-1 again, while pod-0 is still Terminating from the first restart.\r\n7. My service is effectively down, as my service returns 503 Service Unavailable, because all pods are Terminating.\n\n### What did you expect to happen?\n\n6. K8s waits for pod-0 to be restarted and ready again before continuing to restart pod-1\r\n7. My service stays available, as there is always at least one pod that is ready AND Running, which is what I expect from the PDB\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI currently don't have much time to build a minimal sample.\r\nI hope steps 1-4 are precise enough. In order to force the terminating state to take a while, a lifeCycle.preStop command can be specified for the pod container used to delay the container teardown.\n\n### Anything else we need to know?\n\nI don't want to allow my service to route traffic to terminating pods, though I know that's an option.\r\nI want the PDB to view terminating pods as unavailable, just like the default behavior of a service.\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.3\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nAzure Kubernetes Service\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux (In WSL):\r\n$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n$ uname -a\r\nLinux DESKTOP-J27URRA 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n# On Windows (Host for WSL):\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\nBuildNumber  Caption                          OSArchitecture  Version\r\n22631        Microsoft Windows 11 Enterprise  64-bit          10.0.22631\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","As PDB is part of the policy api, I assume the policy working group is the correct destination for this.\r\n\r\n\/wg policy","cc @atiratree \r\n\r\nAs there was a feature around terminating in deployments. Curious if this is related.","I think this not related, but a bug in statefulset.\r\n\r\nWe are tracking these two bugs already:\r\n\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/119234\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123918\r\n\r\n@InDieTasten can you please post a minimum YAML example of your statefulset?","I'll try to cook something up, but I have limited time. It might take a while."],"labels":["kind\/bug","wg\/policy","needs-triage"]},{"title":"Fix printers tests - remove dependency on leap years","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind bug\r\n\/kind failing-test\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nTest that were using AddDate(+y, 0, 0) and then time.Sub were sensitive to a specific date of their execution.\r\n\r\nAn example is a test with AddDate(-5, 0, 0) when executed on 28th of February 2024 and when executed on 1st of March 2024.\r\n\r\nThe difference seen by Sub will be 5y1d in the first case and 5y2d in the second case, because 29th of Feb 2024 is a leap year as well as 29th of Feb 2020 that falls within the 5 year difference.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @MarSik. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/ok-to-test","the fix looks legit, but which test is failing or flaking?","@ffromani I see git was smart and actually handled my conflict for me - the failing test was fixed by https:\/\/github.com\/kubernetes\/kubernetes\/commit\/e9b30a0d29b894eb6e2233e063da3600b8fd2825 \r\n\r\nThe other cases where it will break eventually (in few years :) were left intact by that commit though.","It seems to me the issue is fixedby https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123603 . This is a further improvement over that it seems?","Yeah, I missed the initial patch and git resolved the conflicts for me. https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123603 did not actually explain what happened either, but well... ","\/triage accepted\r\n\/priority backlog\r\n\r\nthanks, labelling as test improvement so","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123910#\" title=\"Author self-approved\">MarSik<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123910#pullrequestreview-1952194934\" title=\"LGTM\">swatisehgal<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [ardaguclu](https:\/\/github.com\/ardaguclu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/printers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/printers\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"ardaguclu\"]} -->","LGTM label has been added.  <details>Git tree hash: 4b4064fb826549ffa7f42ddbac02c9fac8dd9a8f<\/details>"],"labels":["priority\/backlog","lgtm","size\/S","release-note-none","cncf-cla: yes","ok-to-test","triage\/accepted","do-not-merge\/needs-sig","do-not-merge\/needs-kind"]},{"title":"oomkiller_linux_test: fix warnings","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\/kind failing-test\r\n\/kind flake\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nChecking https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv1-containerd-node-arm64-e2e-serial-ec2-eks\/1767568635812909056 , theory to be tested is that perhaps test fails due to warning message before the failed test\r\n\r\n`W0312 16:26:24.831637    4727 warnings.go:70] would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"oomkill-nodeallocatable-container\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"oomkill-nodeallocatable-container\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"oomkill-nodeallocatable-container\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"oomkill-nodeallocatable-container\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\r\n`\r\nThis commit set in PodSpec according to satisfy above recommendations\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nAttempt to fix failing OOMKiller for pod using more memory than node allocatable [LinuxOnly] [Serial] failing ( Failed waiting for pod to terminate, nodeallocatable-oomkiller-test-831\/oomkill-nodeallocatable-pod )\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @esotsal. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","i also see the same failure here - https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cos-cgroupv1-containerd-node-e2e-serial\/1767357743963836416\r\n\r\n```\r\nE2eNode Suite: [It] [sig-node] OOMKiller for pod using more memory than node allocatable [LinuxOnly] [Serial] single process container without resource limits The containers terminated by OOM killer should have the reason set to OOMKilled \r\n```","\/ok-to-test","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test ci-cgroupv1-containerd-node-arm64-e2e-serial-ec2-eks","@esotsal: The specified target(s) for `\/test` were not found.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-ec2-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-cloud-provider-loadbalancer`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-containerd-flaky`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123908#issuecomment-1994188187):\n\n>\/test ci-cgroupv1-containerd-node-arm64-e2e-serial-ec2-eks\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2","@dims 2\/2 success of pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","@dims @mrunalp @SergeyKanzhelev Could we see about getting this into 1.30? ","The test code has been updated to reflect what we emit as warning from the apiserver. we'd expect an end user to do the same thing! only change is in test code and it is very localized changed. Happy to land thanks. thanks @esotsal \r\n\r\n\/approve\r\n\/lgtm\r\n\/milestone v1.30\r\n\r\n\/hold for a little bit if @SergeyKanzhelev or others want to chime in.","LGTM label has been added.  <details>Git tree hash: a710c155ebc105db5e533edde93136d528d84483<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123908#issuecomment-1995462792\" title=\"LGTM\">dims<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123908#\" title=\"Author self-approved\">esotsal<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123908#pullrequestreview-1933329530\" title=\"Approved\">saschagrunert<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)~~ [dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/retest-required","\/retest"," ~~I've added for testing pull-kubernetes-node-kubelet-serial-crio-cgroupv1 and \r\npull-kubernetes-node-kubelet-serial-crio-cgroupv2 , by mistake, i thought this PR would help it but it is not related with this PR, sorry for that.~~ Checking the logs, same issue was found there, thus i've updated the commit to handle also impacted pods. , ( Didn't fix it  made things worse, lifecycle tests failed, restored container lifecycle test file)\r\n\r\n`  W0314 14:12:32.732612    2992 warnings.go:70] would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\r\n`\r\n`  W0314 12:25:24.937831    2992 warnings.go:70] would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (containers \"init-1\", \"restartable-init-2\", \"init-3\", \"regular-1\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \"init-1\", \"restartable-init-2\", \"init-3\", \"regular-1\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or containers \"init-1\", \"restartable-init-2\", \"init-3\", \"regular-1\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers \"init-1\", \"restartable-init-2\", \"init-3\", \"regular-1\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\r\n`\r\n`  W0314 14:12:32.732612    2992 warnings.go:70] would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers \"init-1\", \"init-2\", \"init-3\", \"regular-1\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\r\n`","New changes are detected. LGTM label has been removed.","\/retest","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/retest","\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2","\/retest","@esotsal: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-kubelet-serial-crio-cgroupv2 | 5f44a611df90a4661c51293b342739c410812e91 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123908\/pull-kubernetes-node-kubelet-serial-crio-cgroupv2\/1769016690781196288) | false | `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\npull-kubernetes-node-kubelet-serial-crio-cgroupv1 | 5f44a611df90a4661c51293b342739c410812e91 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123908\/pull-kubernetes-node-kubelet-serial-crio-cgroupv1\/1769016690722476032) | false | `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123908). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aesotsal). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial","\/cc @kannon92 @dims @SergeyKanzhelev "],"labels":["area\/test","kind\/cleanup","sig\/node","size\/M","kind\/flake","release-note-none","approved","cncf-cla: yes","sig\/testing","kind\/failing-test","do-not-merge\/hold","ok-to-test","needs-priority","needs-triage"]},{"title":"Field selector for Services based on ClusterIP and Type","body":"Implement a field selector for services that allows users to filter by the clusterIP field and the Type.\r\nUse this new field to filter server side headless Services on Kubelet to improve the scalability on environments with large number of Headless Services.\r\n\r\n\r\n\r\n\/kind feature\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n\r\nFixes #122394,#77662\r\n\r\n```release-note\r\nServices implement a field selector for the ClusterIP and Type fields.\r\nKubelet uses the fieldselector on Services to avoid watching for Headless Services and reduce the memory consumption.\r\n```\r\n\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123905#\" title=\"Author self-approved\">aojea<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- ~~[pkg\/registry\/core\/service\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/registry\/core\/service\/OWNERS)~~ [aojea]\n- ~~[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)~~ [aojea]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"thockin\"]} -->","This should go in 1.31","\/assign @liggitt @thockin \r\n\r\nqueued for next release added regression tests to avoid the same issue that got it reverted in the first place","\/triage accepted","Thanks @aojea for implementing this long requested feature. I owe you a drink!","Will we consider skip watch headless services on kube-proxy in 1.31 ? @aojea ","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","> Will we consider skip watch headless services on kube-proxy in 1.31 ? @aojea\r\n\r\nyes"],"labels":["area\/test","area\/kubelet","sig\/node","release-note","needs-rebase","size\/XL","kind\/feature","sig\/apps","cncf-cla: yes","sig\/testing","needs-priority","triage\/accepted"]},{"title":"WIP: DRA structured parameters: scheduler fixes","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThis fixes two issues in code added for structured parameters in 1.30. The typo is harmless. The other might affect real drivers once they start to use structured parameters, depending on how they use it.\r\n\r\n#### Special notes for your reviewer:\r\n\r\nNo release note because the goal is to fix the code before it ever gets released with the problem.\r\n\r\nFound while writing unit tests... those will follow in a separate PR, including coverage for for both cases.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n\/assign @klueska \r\n\/cc @alculquicondor ","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123903#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [ahg-g](https:\/\/github.com\/ahg-g) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/scheduler\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"ahg-g\"]} -->","@pohly: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-e2e-containerd-1-7-dra | 5392e30df4923a1b06be92d82dd83c76475ec593 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123903\/pull-kubernetes-node-e2e-containerd-1-7-dra\/1767606688430428160) | false | `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123903). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Apohly). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/assign @klueska \r\n\/triage accepted"],"labels":["kind\/bug","sig\/scheduling","sig\/node","size\/XS","release-note-none","cncf-cla: yes","do-not-merge\/work-in-progress","needs-priority","triage\/accepted"]},{"title":"Update aggregated discovery fixture to v2","body":"\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nGenerate aggregated discovery fixture for v2. \r\n\r\nI'm not sure if it's okay to directly remove the file and opted to generate both for the time being. We will still be supporting the beta types for a couple of releases and I'm not sure if anyone currently depends on the files.\r\n\r\nWondering if you have any thoughts on this @liggitt\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\r\n\/triage accepted\r\n\/assign @liggitt","comments":["\/assign @liggitt\r\nThank you.\r\n\/triage accepted","diff is only the API version:\r\n\r\n```diff\r\n$ diff api\/discovery\/aggregated_{v2beta1,v2}.json\r\n2c2\r\n<   \"apiVersion\": \"apidiscovery.k8s.io\/v2beta1\",\r\n---\r\n>   \"apiVersion\": \"apidiscovery.k8s.io\/v2\",\r\n```\r\n\r\n\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: b02811b4b180d790900f9faed5d6838e08ae38ec<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123902#\" title=\"Author self-approved\">Jefftree<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123902#issuecomment-1992640931\" title=\"Approved\">liggitt<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)~~ [liggitt]\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [liggitt]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/sig api-machinery"],"labels":["kind\/cleanup","lgtm","sig\/api-machinery","size\/XXL","release-note-none","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"[ja]fix typo in \"kubectl config delete-context -h\"","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nFix Japanese translation issue in kubectl\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123899\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @rmiki! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @rmiki. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123900#\" title=\"Author self-approved\">rmiki<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [eddiezane](https:\/\/github.com\/eddiezane) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/pkg\/util\/i18n\/translations\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/pkg\/util\/i18n\/translations\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"eddiezane\"]} -->","\/assign @soltysh\r\nFor CLI \r\n\/triage accepted"],"labels":["kind\/bug","area\/kubectl","size\/XS","sig\/cli","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","triage\/accepted"]},{"title":"[ja] translation issue in \"kubectl config delete-context -h\"","body":"### what happend\r\n\r\nThere is a minor translation issue about `kubectl config delete-context -h` in Japanese.\r\nOutput of `delete-cluster` and `delete-context` are completely same.\r\n\r\n```\r\n$ echo $LANG\r\nja_JP.UTF-8\r\n\r\n$ kubectl config get-context -h\r\nModify kubeconfig files using subcommands like \"kubectl config set current-context my-context\".\r\n\r\n The loading order follows these rules:\r\n\r\n  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes\r\nplace.\r\n  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for\r\nyour system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When\r\na value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the\r\nlast file in the list.\r\n  3.  Otherwise, ${HOME}\/.kube\/config is used and no merging takes place.\r\n\r\nAvailable Commands:\r\n  current-context   Display the current-context\r\n  delete-cluster    \u6307\u5b9a\u3057\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092kubeconfig\u304b\u3089\u524a\u9664\u3059\u308b\r\n  delete-context    \u6307\u5b9a\u3057\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092kubeconfig\u304b\u3089\u524a\u9664\u3059\u308b\r\n  delete-user       Delete the specified user from the kubeconfig\r\n  get-clusters      kubeconfig\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8868\u793a\u3059\u308b\r\n  get-contexts      1\u3064\u307e\u305f\u306f\u8907\u6570\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8a18\u8ff0\u3059\u308b\r\n  get-users         Display users defined in the kubeconfig\r\n  rename-context    Rename a context from the kubeconfig file\r\n  set               Set an individual value in a kubeconfig file\r\n  set-cluster       Set a cluster entry in kubeconfig\r\n  set-context       Set a context entry in kubeconfig\r\n  set-credentials   Set a user entry in kubeconfig\r\n  unset             Unset an individual value in a kubeconfig file\r\n  use-context       Set the current-context in a kubeconfig file\r\n  view              \u30de\u30fc\u30b8\u3055\u308c\u305fkubeconfig\u306e\u8a2d\u5b9a\u307e\u305f\u306f\u6307\u5b9a\u3055\u308c\u305fkubeconfig\u3092\u8868\u793a\u3059\u308b\r\n\r\nUsage:\r\n  kubectl config SUBCOMMAND [options]\r\n\r\nUse \"kubectl config <command> --help\" for more information about a given command.\r\nUse \"kubectl options\" for a list of global command-line options (applies to all commands).\r\n```\r\n\r\n### what did you expect to happen\r\n\r\n`\u30af\u30e9\u30b9\u30bf\u30fc` means cluster in japanese, so it must be\r\n\r\n```\r\ndelete-cluster    \u6307\u5b9a\u3057\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u3092kubeconfig\u304b\u3089\u524a\u9664\u3059\u308b\r\n```","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign","\/sig docs"],"labels":["sig\/docs","needs-triage"]},{"title":"kubelet: Make probe to be on for time drift","body":"The `initialDelaySeconds` holds liveness or readiness probes for specified seconds after container has started. To achieve this, kubelet relies on `StartedAt` time of container which never change during its lifecycle. But in case of time drift scenario (example: During maintenance, NVRAM is flashed, looks like it resets the BIOS settings and RTC is also affected), The `StartedAt` time may contain future time which makes probe to be disabled until system reaches `StartedAt` time. Hence this commit handles this scenario and makes probe to be still initiated for those cases.\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @pperiyasamy. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123898#\" title=\"Author self-approved\">pperiyasamy<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [tallclair](https:\/\/github.com\/tallclair) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/prober\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/prober\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"tallclair\"]} -->","\/release-note-none\r\n\/ok-to-test","\/retest-required","\/remove-sig api-machinery","@jiahuif: Those labels are not set on the issue: `sig\/api-machinery`\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123898#issuecomment-1992491725):\n\n>\/remove-sig api-machinery\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Does this PR need a changelog entry?","\/triage accepted\r\n\/priority important-longterm\r\n\/cc"],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/M","release-note-none","cncf-cla: yes","priority\/important-longterm","ok-to-test","triage\/accepted"]},{"title":"Update cni-plugins to v1.4.1","body":"\r\n\r\n#### What type of PR is this?\r\n\r\n\r\n\/kind cleanup\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nUpdated cni-plugins to the latest release v1.4.1.\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nNone\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nUpdated cni-plugins to v1.4.1.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Looks like golang and image bumps mostly - https:\/\/github.com\/containernetworking\/plugins\/compare\/v1.4.0...v1.4.1\r\n\r\n\/approve\r\n\/lgtm\r\n\r\nplease add milestone if you need this for v1.30","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123894#issuecomment-1991594510\" title=\"LGTM\">dims<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123894#\" title=\"Author self-approved\">saschagrunert<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[build\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/OWNERS)~~ [dims]\n- ~~[cluster\/gce\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/OWNERS)~~ [dims]\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [dims]\n- ~~[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)~~ [dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","LGTM label has been added.  <details>Git tree hash: 8430caef298c7b82c63f1f50d7241a3b1c01b5bc<\/details>","> please add milestone if you need this for v1.30\r\n\r\nI would not include it. What does @kubernetes\/release-engineering thinks?","@saschagrunert This is a patch bump so I think it's safe to include it in v1.30.1","> @saschagrunert This is a patch bump so I think it's safe to include it in v1.30.1\r\n\r\nDo you mind to clarify: v1.30.1 as cherry-pick or v1.30.0 as merge now?","@saschagrunert I mean as a cherry-pick after v1.30.0 is out","\/restest","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123894) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","\/triage accepted","The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass.\n\nThis bot retests PRs for certain kubernetes repos according to the following rules:\n- The PR does have any `do-not-merge\/*` labels\n- The PR does not have the `needs-ok-to-test` label\n- The PR is mergeable (does not have a `needs-rebase` label)\n- The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels)\n- The PR is failing tests required for merge\n\nYou can:\n- Review the [full test history](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123894) for this PR\n- Prevent this bot from retesting with `\/lgtm cancel` or `\/hold`\n- Help make our tests less flaky by following our [Flaky Tests Guide][1]\n\n\/retest\n\n[1]: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md","\/test pull-kubernetes-local-e2e","@saschagrunert: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-local-e2e | a35b75ee570043ce79f16c9b6c6ae0ebb4446b69 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123894\/pull-kubernetes-local-e2e\/1767816277411237888) | false | `\/test pull-kubernetes-local-e2e`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123894). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Asaschagrunert). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","I think this job fails every time.","@saschagrunert yes, please ignore the job\r\n\r\n\/skip"],"labels":["area\/test","kind\/cleanup","lgtm","area\/provider\/gcp","sig\/node","release-note","size\/S","approved","cncf-cla: yes","sig\/testing","sig\/cloud-provider","needs-priority","triage\/accepted"]},{"title":"kube_codegen: expose applyconfig-openapi-schema flag for client generatio","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR exposes `--applyconfig-openapi-schema` flag which allows overriding the openapi-schema in applyconfiguration-gen.\r\n\r\n#### Special notes for your reviewer:\r\n\/assign @thockin \r\n\/cc @JoelSpeed \r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n","comments":["\/lgtm\r\n\/approve\r\n\r\n\/hold for one question, clear if good.","LGTM label has been added.  <details>Git tree hash: 96fde40029d7cdf61b5858f81d2df4f206c157b5<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123893#\" title=\"Author self-approved\">soltysh<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123893#issuecomment-1992111340\" title=\"Approved\">thockin<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)~~ [thockin]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/triage accepted"],"labels":["lgtm","sig\/api-machinery","size\/XS","kind\/feature","release-note-none","approved","cncf-cla: yes","do-not-merge\/hold","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"DRA: CDI: maximum annotation length","body":"### What would you like to be added?\r\n\r\nThis code checks that \"plugin name\" (= DRA driver name) + \"CDI device ID\" with \"_\" as separator is at most `maxNameLen` = 63:\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/3ec6a387955b1240ad6d795663513f1ee12ceaec\/pkg\/kubelet\/cm\/util\/cdi\/cdi.go#L104-L108\r\n\r\n\r\nCan that happen for valid driver and CDI device IDs? Do CDI driver authors have to avoid this? If yes, we need to document it.\r\n\r\n\r\n\/cc @klueska @bart0sh \r\n\/sig node\r\n\r\n\r\n### Why is this needed?\r\n\r\nAvoid surprises...\r\n\r\n","comments":["\/triage accepted\r\n\/priority important-longterm\r\n\r\nnot sure between important-soon or important-longterm, taking a conservative approach for starters"],"labels":["sig\/node","kind\/feature","priority\/important-longterm","triage\/accepted"]},{"title":"Shorten waits in TestWatchStreamSeparation","body":"\/kind cleanup\r\n\r\nRef https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\nBefore\r\n```\r\nkubernetes $ go test .\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/cacher --run TestWatchStreamSeparation --count 1\r\nok      k8s.io\/apiserver\/pkg\/storage\/cacher     13.804s\r\n```\r\n\r\nAfter \r\n```\r\ngo test .\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/cacher --run TestWatchStreamSeparation --count 1\r\nok      k8s.io\/apiserver\/pkg\/storage\/cacher     7.783s\r\n```\r\n\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123888#\" title=\"Author self-approved\">serathius<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [wojtek-t](https:\/\/github.com\/wojtek-t) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"wojtek-t\"]} -->","xref: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123685","\/triage accepted"],"labels":["kind\/cleanup","area\/apiserver","sig\/api-machinery","size\/XS","release-note-none","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"Handle containerd \"CRIU not found\" error message","body":"#### What type of PR is this?\r\n\/kind bug\r\n\/kind failing-test\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nDuring the PR to get \"Forensic Container Checkpointing\" enabled in containerd the decision was made to not correctly report if containerd cannot find the CRIU binary. The reason was that the e2e_node checkpoint test did not understand the error message.\r\n\r\nThe e2e_node checkpoint test is skipped if the container runtime (CRI-O or containerd) does not enable checkpoint support of if checkpoint support is not implemented.\r\n\r\nThis commit adds another reason to skip a check. If the underlying OS which is used to test \"Forensic Container Checkpointing\" in combination with containerd or CRI-O is missing the CRIU binary.\r\n\r\nThis was encountered on Google's Container-Optimized OS (COS) based tests where CRIU was not installed.\r\n\r\nWith this change merged it is possible for containerd to return the correct error message without breaking Kubernetes e2e tests.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123886#\" title=\"Author self-approved\">adrianreber<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @adrianreber. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","Corresponding containerd changes: https:\/\/github.com\/containerd\/containerd\/pull\/9960","\/ok-to-test","\/assign @mrunalp \r\n\r\n","\/test pull-kubernetes-e2e-gce","here's what i think ...\r\n\r\n- We cannot assume `criu` will be in every environment (end user)\r\n- We cannot assume `criu` will be in every testing environment\r\n- e2e_node.test can check if `criu` is available before running a test that needs it (as the e2e_node.test is running on the same node with the containerd\/crio\/kubelet etc) - for a positive test - things that need CRIU work correctly\r\n- We should make sure we have tests that fail \"the right way\" when `criu` is absent if we don't have already in our test suite - for a negative test (when the test env does not have `criu`)","> here's what i think ...\r\n> \r\n>     * We cannot assume `criu` will be in every environment (end user)\r\n> \r\n>     * We cannot assume `criu` will be in every testing environment\r\n> \r\n>     * e2e_node.test can check if `criu` is available before running a test that needs it (as the e2e_node.test is running on the same node with the containerd\/crio\/kubelet etc) - for a positive test - things that need CRIU work correctly\r\n\r\nI can do the same check I do in containerd and CRI-O also in e2e_node before running the test and skip it then.\r\n\r\n>     * We should make sure we have tests that fail \"the right way\" when `criu` is absent if we don't have already in our test suite - for a negative test (when the test env does not have `criu`)\r\n\r\nFail \"the right way\". Is that skip or is there something better than skip? @kannon92 and I were already talking about how to fail better in a previous PR.\r\n\r\n","> Fail \"the right way\". Is that skip or is there something better than skip? @kannon92 and I were already talking about how to fail better in a previous PR.\r\n\r\nI don't like skip because we aren't really testing anything. You can always assert on the case you expect in the test rather than skipping. It may be a bit painful over time as if the runtime changes any error message or whatever, we would want to adjust the tests. But I think that is worth it over a skip and we never look at the test.","\/lgtm\r\n\/assign @SergeyKanzhelev @mrunalp ","LGTM label has been added.  <details>Git tree hash: 538f5bfa9e1c499dd01b79ccf3141945b3ebabe9<\/details>","> Fail \"the right way\". Is that skip or is there something better than skip?\r\n\r\nwas mostly thinking about the folks who are trying to use tools to trigger a checkpoint (via k8s) and they get a proper message that it is not supported or has failed..."],"labels":["kind\/bug","area\/test","lgtm","sig\/node","size\/S","release-note-none","cncf-cla: yes","sig\/testing","kind\/failing-test","ok-to-test","needs-priority","needs-triage"]},{"title":"HPA doesn't work because HPA reads cronjobs resources with a \"completed\" status.","body":"### What happened?\n\n\r\nHPA Target is unknown state.\r\n```\r\n\u276f k get hpa\r\nNAME             REFERENCE                   TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\r\n{{name}}   Deployment\/{{deployment}}   <unknown>\/50%   1         10        1          105m\r\n\r\n```\r\n\r\nBut, deployment and pod resource settings are okay.\r\n```\r\nkubectl get -o json deployment {{deployment}} | jq '.spec.template.spec.containers[].resources'\r\n{\r\n  \"limits\": {\r\n    \"cpu\": \"1\",\r\n    \"memory\": \"1500Mi\"\r\n  },\r\n  \"requests\": {\r\n    \"cpu\": \"500m\",\r\n    \"memory\": \"1000Mi\"\r\n  }\r\n}\r\n```\r\n\r\nmetric-server  logs are find.\r\n\r\nhpa settings are okay.\r\n\r\nhpa.yaml\r\n```\r\napiVersion: v1\r\nitems:\r\n- apiVersion: autoscaling\/v2\r\n  kind: HorizontalPodAutoscaler\r\n  metadata:\r\n    labels:\r\n      hpa.test: hpa\r\n    name: {{name}}\r\n    namespace: {{namespace}}\r\n  spec:\r\n    maxReplicas: 10\r\n    metrics:\r\n    - resource:\r\n        name: cpu\r\n        target:\r\n          averageUtilization: 50\r\n          type: Utilization\r\n      type: Resource\r\n    minReplicas: 1\r\n    scaleTargetRef:\r\n      apiVersion: apps\/v1\r\n      kind: Deployment\r\n      name: {{deployment name}}\r\n\r\n```\r\n( cronjobs not have  `hpa.test: hpa` labels but deployment have `hpa.test: hpa` label) \r\n\r\n\r\nHPA make some logs\r\n\r\n```\r\n k describe hpa\r\n\r\n...\r\n  Type     Reason                   Age                     From                       Message\r\n  ----     ------                   ----                    ----                       -------\r\n  Warning  FailedGetResourceMetric  49m (x13 over 101m)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}} \r\n  Warning  FailedGetResourceMetric  39m (x72 over 110m)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}} \r\n  Warning  FailedGetResourceMetric  34m (x7 over 99m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}}of Pod {{cronjob pod name}} \r\n  Warning  FailedGetResourceMetric  29m (x2 over 82m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}}of Pod {{cronjob pod name}} \r\n  Warning  FailedGetResourceMetric  19m (x4 over 63m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}}  \r\n  Warning  FailedGetResourceMetric  14m (x5 over 85m)       horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container {{cronjob container}} of Pod {{cronjob pod name}}  \r\n  Warning  FailedGetResourceMetric  4m50s (x167 over 106m)  horizontal-pod-autoscaler  (combined from similar\r\n\r\n```\r\n\r\nThere are many pod which state are \"Completed\"\r\n\r\n```\r\n\u276f k get po\r\nNAME                                                              READY   STATUS      RESTARTS      AGE\r\n{{deployment name}}                                1\/1     Running     0             19m\r\n{{cronjob pod }}   0\/1     Completed   4             7h43m\r\n{{cronjob pod }} 0\/1     Completed   3             5h43m\r\n{{cronjob pod }}  0\/1     Completed   3             163m\r\n{{cronjob pod }}          0\/1     Completed   4             6h53m\r\n\r\n```\r\n\r\n\r\nSo, I think hpa read metric not only deployment also cronjob\n\n### What did you expect to happen?\n\nI expect hpa to express an exact number\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\r\nDeploy the deployment and cronjob together and put the cronjob in completed status.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nv1.28.5 \r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nAWS EKS\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/area autoscaler","@AmarNathChary: The label(s) `area\/autoscaler` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123885#issuecomment-1990988009):\n\n>\/area autoscaler\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig autoscaling"],"labels":["kind\/bug","sig\/autoscaling","needs-triage"]},{"title":"[Failing Test] ci-kubernetes-kubemark-gce-scale-scheduler | kubemark-5000-scheduler","body":"### Which jobs are failing?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-kubemark-gce-scale-scheduler\/1766766416834334720\n\n### Which tests are failing?\n\n- kubetest.Kubemark \r\n- kubetest.Kubemark Up\n\n### Since when has it been failing?\n\n3 failures in a row since 03-06\r\nlastpass was @ 03-04\r\nbefore that 3 failures.\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-scalability-kubemark#kubemark-5000-scheduler\n\n### Reason for failure (if possible)\n\n```\r\nWARNING: Could not store access token in cache: database is locked\r\nERROR: gcloud crashed (OperationalError): database is locked\r\n\r\nIf you would like to report this issue, please run the following command:\r\n  gcloud feedback\r\n\r\nTo check gcloud for common problems, please run the following command:\r\n  gcloud info --run-diagnostics\r\nTraceback (most recent call last):\r\n  File \"\/google-cloud-sdk\/lib\/gcloud.py\", line 189, in <module>\r\n    main()\r\n  File \"\/google-cloud-sdk\/lib\/gcloud.py\", line 185, in main\r\n    sys.exit(gcloud_main.main())\r\n             ^^^^^^^^^^^^^^^^^^\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/command_lib\/crash_handling.py\", line 199, in Wrapper\r\n    target_function(*args, **kwargs)\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/gcloud_main.py\", line 186, in main\r\n    gcloud_cli.Execute()\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/cli.py\", line 1024, in Execute\r\n    self._HandleAllErrors(exc, command_path_string, specified_arg_names)\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/cli.py\", line 1061, in _HandleAllErrors\r\nERROR: gcloud crashed (OperationalError): database is locked\r\n    exceptions.HandleError(exc, command_path_string, self.__known_error_handler)\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/exceptions.py\", line 557, in HandleError\r\n    core_exceptions.reraise(exc)\r\n  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/core\/exceptions.py\", line 146, in reraise\r\n    six.reraise(type(exc_value), exc_value, tb)\r\n  File \"\/google-cloud-sdk\/lib\/third_party\/six\/__init__.py\", line 719, in reraise\r\n...\r\nsqlite3.OperationalError: database is locked\r\nWARNING: Could not store access token in cache: database is locked\r\nWARNING: Could not store access token in cache: database is locked\r\n```\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig scalibility scheduling","comments":["@pacoxu: The label(s) `sig\/scalibility` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123884):\n\n>### Which jobs are failing?\n>\n>https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-kubemark-gce-scale-scheduler\/1766766416834334720\n>\n>### Which tests are failing?\n>\n>- kubetest.Kubemark \r\n>- kubetest.Kubemark Up\n>\n>### Since when has it been failing?\n>\n>3 failures in a row since 03-06\r\n>lastpass was @ 03-04\r\n>before that 3 failures.\n>\n>### Testgrid link\n>\n>https:\/\/testgrid.k8s.io\/sig-scalability-kubemark#kubemark-5000-scheduler\n>\n>### Reason for failure (if possible)\n>\n>```\r\n>WARNING: Could not store access token in cache: database is locked\r\n>ERROR: gcloud crashed (OperationalError): database is locked\r\n>\r\n>If you would like to report this issue, please run the following command:\r\n>  gcloud feedback\r\n>\r\n>To check gcloud for common problems, please run the following command:\r\n>  gcloud info --run-diagnostics\r\n>Traceback (most recent call last):\r\n>  File \"\/google-cloud-sdk\/lib\/gcloud.py\", line 189, in <module>\r\n>    main()\r\n>  File \"\/google-cloud-sdk\/lib\/gcloud.py\", line 185, in main\r\n>    sys.exit(gcloud_main.main())\r\n>             ^^^^^^^^^^^^^^^^^^\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/command_lib\/crash_handling.py\", line 199, in Wrapper\r\n>    target_function(*args, **kwargs)\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/gcloud_main.py\", line 186, in main\r\n>    gcloud_cli.Execute()\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/cli.py\", line 1024, in Execute\r\n>    self._HandleAllErrors(exc, command_path_string, specified_arg_names)\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/cli.py\", line 1061, in _HandleAllErrors\r\n>ERROR: gcloud crashed (OperationalError): database is locked\r\n>    exceptions.HandleError(exc, command_path_string, self.__known_error_handler)\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/calliope\/exceptions.py\", line 557, in HandleError\r\n>    core_exceptions.reraise(exc)\r\n>  File \"\/google-cloud-sdk\/lib\/googlecloudsdk\/core\/exceptions.py\", line 146, in reraise\r\n>    six.reraise(type(exc_value), exc_value, tb)\r\n>  File \"\/google-cloud-sdk\/lib\/third_party\/six\/__init__.py\", line 719, in reraise\r\n>...\r\n>sqlite3.OperationalError: database is locked\r\n>WARNING: Could not store access token in cache: database is locked\r\n>WARNING: Could not store access token in cache: database is locked\r\n>```\r\n>\n>\n>### Anything else we need to know?\n>\n>_No response_\n>\n>### Relevant SIG(s)\n>\n>\/sig scalibility scheduling\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig scalability"],"labels":["sig\/scalability","sig\/scheduling","kind\/failing-test","needs-triage"]},{"title":"[WIP] Bump cel to 0.18","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNONE\r\n```\r\n\r\n\/cc @jiahuif","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123883#\" title=\"Author self-approved\">wzshiming<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [andrewsykim](https:\/\/github.com\/andrewsykim), [liggitt](https:\/\/github.com\/liggitt), [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[LICENSES\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/LICENSES\/OWNERS)**\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)**\n- **[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)**\n- **[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n- **[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"andrewsykim\",\"liggitt\",\"sttts\"]} -->","@wzshiming: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | 3213565c0fbf1b425d0fae0252106549b49da454 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123883\/pull-kubernetes-unit\/1767457715652661248) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-e2e-gce | 3213565c0fbf1b425d0fae0252106549b49da454 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123883\/pull-kubernetes-e2e-gce\/1767457703921192960) | true | `\/test pull-kubernetes-e2e-gce`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123883). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Awzshiming). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["sig\/network","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/node","sig\/api-machinery","size\/L","kind\/feature","release-note-none","sig\/auth","sig\/cli","cncf-cla: yes","sig\/architecture","do-not-merge\/work-in-progress","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"[Flaking Test] [kubetest2 ec2] [sig-network] Services should preserve source pod IP for traffic thru service cluster IP [LinuxOnly]","body":"### Which jobs are failing?\r\n\r\n\r\nci-kubernetes-e2e-ubuntu-ec2-containerd\r\n\r\nhttps:\/\/storage.googleapis.com\/k8s-triage\/index.html?job=ci-kubernetes-e2e-ubuntu-ec2-containerd&test=Services%20should%20preserve%20source%20pod%20IP%20for%20traffic%20thru%20service%20cluster%20IP\r\n\r\n### Which tests are failing?\r\n\r\nKubernetes e2e suite [It] [sig-network] Services should preserve source pod IP for traffic thru service cluster IP [LinuxOnly]\r\n\r\n### Since when has it been failing?\r\n\r\nN\/A\r\n\r\n### Testgrid link\r\n\r\n**These CIs failed for only this test** (flake rate is very high, almost a failing CI)\r\n- https:\/\/testgrid.k8s.io\/amazon-ec2#ec2-ubuntu-master-containerd ()\r\n- https:\/\/testgrid.k8s.io\/amazon-ec2#ci-kubernetes-e2e-ec2-eks-al2023\r\n\r\nThese CIs failed for other test cases as well\r\n- https:\/\/testgrid.k8s.io\/amazon-ec2#ci-kubernetes-e2e-ec2-alpha-enabled-default\r\n- https:\/\/testgrid.k8s.io\/amazon-ec2#ec2-arm64-ubuntu-master-containerd\r\n\r\n\r\n### Reason for failure (if possible)\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ubuntu-ec2-containerd\/1767320129487507456\r\n```\r\n{ failed [FAILED] Expected\r\n    <string>: 172.31.44.204\r\nto equal\r\n    <string>: 172.31.42.198\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/network\/service.go:1087 @ 03\/11\/24 23:04:55.518\r\n}\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Relevant SIG(s)\r\n\r\n\/sig network","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","note this is a `kubetest2 ec2` specific bug. i'll probably have to dig in at some point"],"labels":["sig\/network","kind\/failing-test","needs-triage"]},{"title":"[Failing Test] e2e-ci-kubernetes-e2e-[cos-gce|al2023-aws]-disruptive-canary","body":"### Which jobs are failing?\r\n\r\ne2e-ci-kubernetes-e2e-cos-gce-disruptive-canary\r\nhttps:\/\/prow.k8s.io\/job-history\/gs\/kubernetes-jenkins\/logs\/e2e-ci-kubernetes-e2e-cos-gce-disruptive-canary\r\n\r\n### Which tests are failing?\r\n\r\n- [ ] Etcd failure\r\n  -  [sig-api-machinery] Etcd failure [Disruptive] should recover from SIGKILL\r\n  -  [sig-api-machinery] Etcd failure [Disruptive] should recover from network partition with master\r\n- [ ] DaemonRestart\r\n  -  [sig-apps] DaemonRestart [Disruptive] Controller Manager should not create\/delete replicas across restart\r\n  -  [sig-apps] DaemonRestart [Disruptive] Kubelet should not restart containers across restart\r\n  -  [sig-apps] DaemonRestart [Disruptive] Scheduler should continue assigning pods to nodes across restart\r\n- [ ]  [sig-cloud-provider-gcp] [Disruptive] NodeLease NodeLease deletion node lease should be deleted when corresponding node is deleted\r\n- [ ] GenericPersistentVolume\r\n   -  [sig-storage] GenericPersistentVolume [Disruptive] When kubelet restarts Should test that a file written to the mount before kubelet restart is readable after restart.Changes\r\n   -  [sig-storage] GenericPersistentVolume [Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns.Changes\r\n  -  [sig-storage] GenericPersistentVolume [Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is force deleted while the kubelet is down unmounts when the kubelet returns.Changes\r\n\r\n### Since when has it been failing?\r\n\r\nstarted failing at the beginning of the test-grid\r\nearlier than 2023-12-13\r\n\r\n### Testgrid link\r\n\r\n- https:\/\/testgrid.k8s.io\/sig-cluster-lifecycle-kops#ci-kubernetes-e2e-cos-gce-disruptive-canary\r\n- https:\/\/testgrid.k8s.io\/sig-cluster-lifecycle-kops#ci-kubernetes-e2e-al2023-aws-disruptive-canary\r\n\r\n\r\n### Reason for failure (if possible)\r\n\r\n```\r\n{ failed [FAILED] Daemon kube-scheduler on node 34.102.107.245 did not respond with a 200 via curl -sk -o \/dev\/null -I -w \"%{http_code}\" https:\/\/localhost:10259\/healthz within 10m0s: error getting SSH client to prow@34.102.107.245:22: timed out waiting for the condition\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/apps\/daemon_restart.go:132 @ 03\/11\/24 23:17:39.798\r\n\r\n```\r\n\r\n```\r\n{ failed [FAILED] Failed waiting for PVC to be bound PersistentVolumeClaims [pvc-tklq7] not all in phase Bound within 5m0s: PersistentVolumeClaims [pvc-tklq7] not all in phase Bound within 5m0s\r\nIn [BeforeEach] at: k8s.io\/kubernetes\/test\/e2e\/storage\/generic_persistent_volume-disruptive.go:113 @ 03\/12\/24 00:11:41.3\r\n\r\nThere were additional failures detected after the initial failure. These are visible in the timeline\r\n}\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\nin kops ci.\r\nshould I transfer it to https:\/\/github.com\/kubernetes\/kops\/issues\r\n\r\n### Relevant SIG(s)\r\n\r\n\/sig cluster-lifecycle\r\nfor job owner\r\n\r\n\/sig storage apps api-machinery cloud-provider","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @siyuanfoundation \r\nFor context of etcd. thank you."],"labels":["sig\/storage","sig\/api-machinery","sig\/cluster-lifecycle","sig\/apps","kind\/failing-test","sig\/cloud-provider","needs-triage"]},{"title":"Adding huge pages test","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n","Adding label `do-not-merge\/contains-merge-commits` because PR contains merge commits, which are not allowed in this repository.\nUse `git rebase` to reapply your commits on top of the target branch. Detailed instructions for doing so can be found [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/github-workflow.md#4-keep-your-branch-in-sync).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123879#\" title=\"Author self-approved\">ndixita<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mrunalp](https:\/\/github.com\/mrunalp) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mrunalp\"]} -->","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\/test pull-crio-cgroupv1-node-e2e-hugepages","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","\r\n\/test pull-crio-cgroupv1-node-e2e-hugepages\r\n\r\n","@ndixita: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-crio-cgroupv1-node-e2e-hugepages | d9eefa2fb357ccd21e4733d5af9587895443053e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123879\/pull-crio-cgroupv1-node-e2e-hugepages\/1767767103940595712) | false | `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123879). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Andixita). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","area\/kubelet","sig\/node","size\/L","cncf-cla: yes","sig\/testing","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","do-not-merge\/contains-merge-commits","needs-triage","do-not-merge\/needs-kind"]},{"title":"[Flaking Test] [sig-storage] PersistentVolumes-local","body":"### Which jobs are flaking?\n\n- ci-containerd-e2e-ubuntu-gce\r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/96846744\/4a248891-5a02-41c9-b210-375dc1fbc542)\r\n\n\n### Which tests are flaking?\n\n- Kubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2\r\n- Kubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2\r\n- Kubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2\r\n\r\n(There are multiple tests that are flaking.)\n\n### Since when has it been flaking?\n\n02-27\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#gce-ubuntu-master-containerd\n\n### Reason for failure (if possible)\n\n```\r\n[FAILED] error dialing backend: dial timeout, backstop\r\nIn [DeferCleanup (Each)] at: k8s.io\/kubernetes\/test\/e2e\/storage\/utils\/local.go:142 @ 02\/27\/24 14:55:49.793\r\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig storage","comments":["\/cc @carlory ","\/sig api-machinery\r\n\r\nIt's not a storage issue.\r\n\r\n```\r\ncontainer&container=agnhost-container&stderr=true&stdout=true)\r\nSTEP: Creating local PVCs and PVs - test\/e2e\/storage\/persistent_volumes-local.go:1054 @ 03\/11\/24 09:50:13.717\r\nMar 11 09:50:13.717: INFO: Creating a PV followed by a PVC\r\nMar 11 09:50:13.915: INFO: Unexpected error: \r\n    <*fmt.wrapError | 0xc0007c4ea0>: \r\n    PV Create API error: persistentvolumes \"local-pv5xxvc\" is forbidden: [expression 'object.metadata.namespace == 'production'' resulted in error: no such key: namespace, expression 'object.metadata.namespace == 'staging'' resulted in error: no such key: namespace]\r\n    {\r\n        msg: \"PV Create API error: persistentvolumes \\\"local-pv5xxvc\\\" is forbidden: [expression 'object.metadata.namespace == 'production'' resulted in error: no such key: namespace, expression 'object.metadata.namespace == 'staging'' resulted in error: no such key: namespace]\",\r\n        err: <*errors.StatusError | 0xc002c42820>{\r\n            ErrStatus: {\r\n                TypeMeta: {Kind: \"\", APIVersion: \"\"},\r\n                ListMeta: {\r\n                    SelfLink: \"\",\r\n                    ResourceVersion: \"\",\r\n                    Continue: \"\",\r\n                    RemainingItemCount: nil,\r\n                },\r\n                Status: \"Failure\",\r\n                Message: \"persistentvolumes \\\"local-pv5xxvc\\\" is forbidden: [expression 'object.metadata.namespace == 'production'' resulted in error: no such key: namespace, expression 'object.metadata.namespace == 'staging'' resulted in error: no such key: namespace]\",\r\n                Reason: \"Forbidden\",\r\n                Details: {\r\n                    Name: \"local-pv5xxvc\",\r\n                    Group: \"\",\r\n                    Kind: \"persistentvolumes\",\r\n                    UID: \"\",\r\n                    Causes: nil,\r\n                    RetryAfterSeconds: 0,\r\n                },\r\n                Code: 403,\r\n            },\r\n        },\r\n    }\r\n```","https:\/\/storage.googleapis.com\/k8s-triage\/index.html?ci=0&pr=1&text=expression%20%27object.metadata.namespace%20%3D%3D%20%27production%27%27%20resulted%20in%20error%3A%20no%20such%20key%3A%20namespace&test=PersistentVolumes-local","https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/e2e-kops-grid-cilium-eni-u2004-k28-ko28\/1767217393060286464\r\n\r\ntime: 16:14:22.454\r\n```\r\n{ failed [FAILED] PV Create API error: persistentvolumes \"local-pvxlxcj\" is forbidden: [expression 'object.metadata.namespace == 'production'' resulted in error: no such key: namespace, expression 'object.metadata.namespace == 'staging'' resulted in error: no such key: namespace]\r\nIn [BeforeEach] at: test\/e2e\/storage\/persistent_volumes-local.go:889 @ 03\/11\/24 16:14:22.454\r\n```\r\n\r\n\r\nwebhook name is validation-webhook-with-match-conditions.k8s.io\r\n\r\ntime: 16:14:22.437852 \r\n\r\n```\r\nW0311 16:14:22.437852      10 webhook.go:233] Failed evaluating match conditions, failing closed validation-webhook-with-match-conditions.k8s.io: [expression 'object.metadata.namespace == 'production'' resulted in error: no such key: namespace, expression 'object.metadata.namespace == 'staging'' resulted in error: no such key: namespace]\r\n```\r\n\r\ne2e test link: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/3ec6a387955b1240ad6d795663513f1ee12ceaec\/test\/e2e\/apimachinery\/webhook.go#L942\r\n\r\nIt's fixed by https:\/\/github.com\/kubernetes\/kubernetes\/pull\/120484 in 1.29","> Kubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2\r\nKubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2\r\nKubernetes e2e suite [It] [sig-storage] PersistentVolumes-local [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2\r\n\r\nThe failures reason is timeout, it seems not a storage issue.\r\n\r\n","\/assign\r\n\/triage accepted"],"labels":["sig\/storage","sig\/api-machinery","kind\/flake","triage\/accepted"]},{"title":"Improve the efficiency of synchronizing Pod status","body":"Use a single List api instead of multiple Get api. \r\nAbout: #123875\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nMy k8s node is a big machine(500C CPU & 2000GB Memory), the maxPods of kubelet could be set to 200+.\r\nWhen I delete 200+ pods or create 200+ pods, the synchronization of Pod status by the kubelet becomes too slow(about 230s).\r\nMaking a large number of GET API calls can indeed increase the load on the API server.\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\n\/area kubelet","\/assign @tallclair ","\/cc @wojtek-t ","LGTM label has been added.  <details>Git tree hash: 07b132ee157760e70e7a767878a6a1da13372bae<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123877#\" title=\"Author self-approved\">FengyunPan2<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123877#pullrequestreview-1945497132\" title=\"LGTM\">matthyx<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->","\/hold"],"labels":["area\/kubelet","lgtm","sig\/node","size\/M","cncf-cla: yes","do-not-merge\/release-note-label-needed","do-not-merge\/hold","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"[Flaking Test] master-integration TestPolicyAdmission","body":"### Which jobs are flaking?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1767221924246589440\r\n\n\n### Which tests are flaking?\n\n- https:\/\/k8s.io\/kubernetes\/test\/integration\/apiserver: cel\r\n\r\nRUN   TestPolicyAdmission\/.v1.bindings\/create\n\n### Since when has it been flaking?\n\n03\/12\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#integration-master\n\n### Reason for failure (if possible)\n\n```\r\nFailed;Failed;Failed;Failed;Failed;  === RUN   TestPolicyAdmission\/.v1.bindings\/create\r\nI0311 16:32:55.565220  108006 policy_source.go:224] refreshing policies\r\n    admission_test_util.go:354: version: v1beta1, phase:validation, converted:true error: no request received\r\n        --- FAIL: TestPolicyAdmission\/.v1.bindings\/create (0.05s)\r\n;=== RUN   TestPolicyAdmission\/.v1.bindings\r\n    --- FAIL: TestPolicyAdmission\/.v1.bindings (0.05s)\r\n;=== RUN   TestPolicyAdmission\/.v1.configmaps\/create\r\n    admission_test_util.go:354: version: v1beta1, phase:validation, converted:true error: no request received\r\n        --- FAIL: TestPolicyAdmission\/.v1.configmaps\/create (0.01s)\r\n;=== RUN   TestPolicyAdmission\/.v1.configmaps\r\n    --- FAIL: TestPolicyAdmission\/.v1.configmaps (0.10s)\r\n;=== RUN   TestPolicyAdmission\r\n```\n\n### Anything else we need to know?\n\nValidatingAdmissionPolicy related test\n\n### Relevant SIG(s)\n\n\/sig api-machinery\r\n","comments":["\/cc @jiahuif @cici37 ","\/assign\r\n\/triage accepted","It only flake once until now. Keep observing. \r\n"],"labels":["sig\/api-machinery","kind\/flake","triage\/accepted"]},{"title":"The efficiency of synchronizing Pod status too slow.","body":"### What would you like to be added?\r\n\r\nUse a single List api instead of multiple Get api.\r\n\r\n### Why is this needed?\r\n\r\nMy k8s node is a big machine(500C CPU & 2000GB Memory), the maxPods of kubelet could be set to 200+.\r\nWhen I delete 200+ pods or create 200+ pods, the synchronization of Pod status by the kubelet becomes too slow(about 230s).\r\nMaking a large number of GET API calls can indeed increase the load on the API server.\r\n\r\n\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign","\/sig node\r\n","do we have more precise numbers about the (in)efficiency? for example, a graph of pod count vs execution time","> do we have more precise numbers about the (in)efficiency? for example, a graph of pod count vs execution time\r\n\r\nI agree with @ffromani it would be nice to see how the new patch improves things."],"labels":["sig\/node","kind\/feature","needs-triage"]},{"title":"WIP - All Pods option for `kubectl logs` ","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThis adds an `--all-pods` option to get all pod logs from replicaSets or services by looking at the label selector and finding corresponding pods in the namespace. It reduces the need to look for logs based on labels. \r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes https:\/\/github.com\/kubernetes\/kubectl\/issues\/1520\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```\r\nAdded new flag for `kubectl logs` called `--all-pods` to get all pods from a object that uses a pod selector\r\n\r\nkubectl logs <deploy\/sts\/daemonset\/svc>\/<name> --all-pods [--prefix]\r\n```\r\n\r\n\r\n\r\n\r\nusage\r\n```bash\r\n\u250c\u2500[cmwylie19@Cases-MacBook-Pro] - [~\/kubernetes] - [2024-03-11 06:37:09]\r\n\u2514\u2500[0] <git:(1520 570a1ede3fd\u2731) > go run cmd\/kubectl\/kubectl.go logs svc\/y --all-pods\r\n\/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n\/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n\/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n\/docker-entrypoint.sh: Configuration complete; ready for start up\r\n2024\/03\/07 21:10:16 [notice] 1#1: using the \"epoll\" event method\r\n2024\/03\/07 21:10:16 [notice] 1#1: nginx\/1.25.4\r\n2024\/03\/07 21:10:16 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n2024\/03\/07 21:10:16 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n2024\/03\/07 21:10:16 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker processes\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 29\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 30\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 31\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 32\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 33\r\n2024\/03\/07 21:10:16 [notice] 1#1: start worker process 34\r\n\/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n\/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n\/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n\/docker-entrypoint.sh: Configuration complete; ready for start up\r\n2024\/03\/07 21:15:54 [notice] 1#1: using the \"epoll\" event method\r\n2024\/03\/07 21:15:54 [notice] 1#1: nginx\/1.25.4\r\n2024\/03\/07 21:15:54 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n2024\/03\/07 21:15:54 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n2024\/03\/07 21:15:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker processes\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 29\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 30\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 31\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 32\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 33\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 34\r\n\/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n\/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n\/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n\/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n\/docker-entrypoint.sh: Configuration complete; ready for start up\r\n2024\/03\/07 21:15:54 [notice] 1#1: using the \"epoll\" event method\r\n2024\/03\/07 21:15:54 [notice] 1#1: nginx\/1.25.4\r\n2024\/03\/07 21:15:54 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n2024\/03\/07 21:15:54 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n2024\/03\/07 21:15:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker processes\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 29\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 30\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 31\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 32\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 33\r\n2024\/03\/07 21:15:54 [notice] 1#1: start worker process 34\r\n\u250c\u2500[cmwylie19@Cases-MacBook-Pro] - [~\/kubernetes] - [2024-03-11 06:38:24]\r\n\u2514\u2500[0] <git:(1520 570a1ede3fd\u2731) > go run cmd\/kubectl\/kubectl.go logs deploy\/y --all-pods --prefix\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n[pod\/y-844fbc9777-vhl79\/nginx] 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-vhl79\/nginx] 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n[pod\/y-844fbc9777-vhl79\/nginx] \/docker-entrypoint.sh: Configuration complete; ready for start up\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: using the \"epoll\" event method\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: nginx\/1.25.4\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker processes\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 29\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 30\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 31\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 32\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 33\r\n[pod\/y-844fbc9777-vhl79\/nginx] 2024\/03\/07 21:10:16 [notice] 1#1: start worker process 34\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n[pod\/y-844fbc9777-hz4x4\/nginx] \/docker-entrypoint.sh: Configuration complete; ready for start up\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: using the \"epoll\" event method\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: nginx\/1.25.4\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker processes\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 29\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 30\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 31\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 32\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 33\r\n[pod\/y-844fbc9777-hz4x4\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 34\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: \/docker-entrypoint.d\/ is not empty, will attempt to perform configuration\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Looking for shell scripts in \/docker-entrypoint.d\/\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/10-listen-on-ipv6-by-default.sh\r\n[pod\/y-844fbc9777-95rhd\/nginx] 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-95rhd\/nginx] 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in \/etc\/nginx\/conf.d\/default.conf\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Sourcing \/docker-entrypoint.d\/15-local-resolvers.envsh\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/20-envsubst-on-templates.sh\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Launching \/docker-entrypoint.d\/30-tune-worker-processes.sh\r\n[pod\/y-844fbc9777-95rhd\/nginx] \/docker-entrypoint.sh: Configuration complete; ready for start up\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: using the \"epoll\" event method\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: nginx\/1.25.4\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: OS: Linux 5.15.49-linuxkit\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker processes\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 29\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 30\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 31\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 32\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 33\r\n[pod\/y-844fbc9777-95rhd\/nginx] 2024\/03\/07 21:15:54 [notice] 1#1: start worker process 34\r\n```","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @cmwylie19! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @cmwylie19. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123874#\" title=\"Author self-approved\">cmwylie19<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [soltysh](https:\/\/github.com\/soltysh) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"soltysh\"]} -->","@cmwylie19 thank you for working on it. \r\n\r\nAs a user, I think it is better to have less command-line options. \r\n\r\nWhy not make this behavior the new default?\r\n\r\nI guess in most cases the users would like to see the logs of all pods. Because if you want to see the log of a particular pod, then you just need to use the existing `kubectl logs pods\/...`.\r\n\r\nWhat do you think?","> @cmwylie19 thank you for working on it.\r\n> \r\n> As a user, I think it is better to have less command-line options.\r\n> \r\n> Why not make this behavior the new default?\r\n> \r\n> I guess in most cases the users would like to see the logs of all pods. Because if you want to see the log of a particular pod, then you just need to use the existing `kubectl logs pods\/...`.\r\n> \r\n> What do you think?\r\n\r\nMy pleasure to be on this!\r\n\r\n@eddiezane was echoing a similar sentiment. \r\n\r\nI agree that less command is a better experience and that this should be the default.\r\n\r\nI will change this PR to meet that criteria. Thanks for the input!\r\n","We are definitely in favor of less flags and pretty often opposed to adding new ones. That said we are also pretty averse to changing default behaviors.\n\nThere's truly an xkcd for everything https:\/\/xkcd.com\/1172\/.\n\nWe'll add it to the agenda for our next meeting.\n\n\/ok-to-test\n\nAlso note that we are past code freeze for 1.30 so this will unfortunately not ship until 1.31.","> \/retest\r\n\/retest\r\n","\/retest","@eddiezane \r\n\r\n> We'll add it to the agenda for our next meeting.\r\n\r\nDid you discuss the topic? I would love to have the default of \"print logs of all pods of the deployment\"\r\n","It's on the agenda for April 3rd at 9am PT. Join us if you can!\n\nhttps:\/\/docs.google.com\/document\/d\/1r0YElcXt6G5mOWxwZiXgGu_X6he3F--wKwg-9UBc29I\/edit?usp=drivesdk"],"labels":["area\/kubectl","size\/L","kind\/feature","sig\/cli","cncf-cla: yes","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","ok-to-test","needs-priority","needs-triage"]},{"title":"Soft eviction of pods with long grace periods blocks hard evictions when under resource pressure","body":"### What happened?\n\nWhen kubelet detects that it's under resource pressure, it first attempts to do soft evictions, until the hard eviction threshold is reached. When a pod is soft-evicted, it respects the configured max pod grace period seconds, and until the pod has shut down, kubelet will not attempt to soft OR hard evict another pod, even if the hard eviction threshold is reached.\r\n\r\nAs a result, one pod taking a long time to shut down can cause kubelet to run out of resources. From [this comment](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/016d8b11ae250ca933fe83de312bbe7c4945cd20\/pkg\/kubelet\/eviction\/eviction_manager.go#L405) and [this comment](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/016d8b11ae250ca933fe83de312bbe7c4945cd20\/pkg\/kubelet\/eviction\/eviction_manager.go#L603) this behavior seems to be by design\r\n\r\nIn our case, we saw one soft eviction take 7 hours to complete, and meanwhile, resources usage kept climbing without any automation trying to save the node. Had other pods gotten soft evicted while this pod shut down, this would not be an issue. Manual intervention prevented it from reaching hard-eviction thresholds, but had that not happened, this would have entirely exhausted the node with no automated action\n\n### What did you expect to happen?\n\nI would expect that kubelet would keep trying to soft evict other pods if one is taking a long time to shut down. Or at the very least, start hard evicting pods if the hard eviction threshold is reached. It could also hard-evict the pod that was soft-evicted but is taking a long time to shut down.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create two pods that get scheduled to the same node that have emptyDir volumes and a prestop hook that just sleeps forever\r\n1. Start filling up those emptyDir volumes with `dd` until soft eviction threshold is reached\r\n1. Watch as kubelet soft-evicts one pod\r\n1. Continue filling up the emptyDir volumes with `dd`\r\n1. Kubelet will not evict (hard or soft) even as the resource is totally exhausted\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.27.11\r\nKustomize Version: v5.0.1\r\nServer Version: v1.27.11\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\naws\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\nAlmaLinux9\/CentOS Stream 8\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","\/priority important-longterm"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","needs-triage"]},{"title":"Do not send 'failed with error' JWTs to the webhook authenticator","body":"This change prevents authentication tokens that are handled and fail authentication via the service account or JWT authenticator from being sent to any configured webhook authenticator.  The old behavior of forwarding these tokens to the webhook authenticator can be temporarily restored by disabling the StrictAuthenticationTokenHandling feature gate.\r\n\r\nSome important implementation details:\r\n\r\ntokenAuthenticators are now split into opaqueTokenAuthenticators and jwtSchemaAuthenticators but the webhook authenticator is modeled as as jwtSchemaAuthenticator because it just needs to come at the end. opaqueTokenAuthenticators and jwtSchemaAuthenticators are unioned and will continue to the next authenticator on error.\r\n\r\nBootstrap tokens are handled before service account tokens (this should be a no-op since they have no schema overlap).\r\n\r\nSwap the ordering of the service account authenticators so that new style tokens are attempted first.  The service account authenticators are unioned together because we do not validate that their issuers do not overlap.  This should also be a no-op with a slight performance improvement since legacy tokens are rare in comparison to the new style service account tokens.\r\n\r\njwtSchemaAuthenticators are unioned but will fail on the first error. Validation already guarantees that the JWT issuers of these authenticators never overlap, so the only behavioral change is that if a incoming JWT has an issuer that matches either the service account authenticators or the any JWT authenticators, they will never be sent to the webhook authenticator.  Previously, if all of the prior JWT schema authenticators failed to validate the token, it would be sent to the webhook authenticator so that it could attempt to validate the token.\r\n\r\nNote that any non-JWT tokens will continue to be sent to the webhook authenticator, even if they fail with an error on any one of the previous opaqueTokenAuthenticators.\r\n\r\n\/kind bug\r\n\/kind api-change\r\n\/sig auth\r\n\/triage accepted\r\n\/milestone v1.31\r\n\r\n```release-note\r\nAuthentication tokens that are handled and fail authentication via the service account or JWT authenticator are no longer sent to any configured webhook authenticator.  The old behavior of forwarding these tokens to the webhook authenticator can be temporarily restored by disabling the StrictAuthenticationTokenHandling feature gate.\r\n```\r\n\r\nRemaining tasks:\r\n\r\n- [ ] Discuss at SIG Auth meeting\r\n- [ ] Get an API review\r\n- [ ] Add integration tests","comments":["[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123871#\" title=\"Author self-approved\">enj<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)~~ [enj]\n- ~~[pkg\/kubeapiserver\/authenticator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubeapiserver\/authenticator\/OWNERS)~~ [enj]\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS)~~ [enj]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/hold\r\n\r\nFor remaining tasks noted above.","Changelog suggestion (may be incorrect; I've based what I wrote on what I understood the existing text to mean)\r\n```diff\r\n-Authentication tokens that are handled and fail authentication via the service account or JWT authenticator are no longer sent to any configured webhook authenticator.  The old behavior of forwarding these tokens to the webhook authenticator can be temporarily restored by disabling the StrictAuthenticationTokenHandling feature gate.\r\n+Revised how the API server treats authentication tokens that fail verification.\r\n+For authentication tokens that are handled via the service account or JWT authenticator, and that fail authentication,\r\n+these are no longer sent to any configured webhook authenticator.  The previous behavior of forwarding such\r\n+tokens to the webhook authenticator can be restored by disabling the `StrictAuthenticationTokenHandling`\r\n+feature gate (which is beta, and is enabled by default).\r\n```"],"labels":["kind\/bug","area\/apiserver","sig\/api-machinery","release-note","size\/L","kind\/api-change","sig\/auth","approved","cncf-cla: yes","do-not-merge\/hold","needs-priority","triage\/accepted"]},{"title":"KEP: Support for Cloud Native Confidential Computing: Integrity Measurement and Attestation Services","body":"# Title\r\nSupport for Cloud Native Confidential Computing: Integrity Measurement and Attestation Services\r\n\r\n# Authors\r\nWenhui Zhang\r\n<wenhuizhang.psu@gmail.com>\r\n\r\n# Owning SIG\r\nSIG-Security\r\nSIG-Node\r\nParticipating SIGs\r\nSIG-Cloud Provider\r\nSIG-Network\r\nSIG-Auth\r\n\r\n# Status\r\nDraft (2024-03-11)\r\nTargeted Release: [Kubernetes Version TBD]\r\n\r\n\r\n## Summary\r\nThis KEP proposes the introduction of Cloud Native Confidential Computing support in Kubernetes, focusing on two main aspects: Integrity Measurement at Boot Time and Attestation (covering both Secure Boot and Remote Attestation). The proposal includes the addition of two new services: the Trusted Container Loader and the Secure Cluster\/POD Service. These enhancements aim to provide stronger security guarantees for containerized workloads, enabling a more robust cloud-native infrastructure.\r\n\r\n## Motivation\r\n\r\nWith the increasing adoption of cloud-native technologies, the need for enhanced security mechanisms that protect sensitive data and computation at every layer of the cloud stack has become paramount. Confidential computing addresses this need by providing hardware-based isolation and encryption. However, Kubernetes lacks native support for key features of confidential computing, such as secure boot, remote attestation, and integrity measurement. This proposal seeks to fill these gaps, thereby enhancing the security posture of Kubernetes clusters.\r\n\r\n## Proposal\r\n\r\n### Trusted Container Loader\r\n\r\nThe Trusted Container Loader service will be responsible for securely loading container images into the enclave environments. It will ensure that only trusted and verified images are executed, leveraging integrity measurement at boot time to validate the container's integrity.\r\n\r\n### Secure Cluster\/POD Service\r\nThe Secure Cluster\/POD Service will manage the lifecycle of secure enclaves and their communication within the Kubernetes cluster. It will provide APIs for attestation, enabling both secure boot verification and remote attestation to ensure that the execution environment is secure and has not been tampered with.\r\n\r\n## Design \r\n\r\n### Integrity Measurement at Boot Time\r\n\r\nThis feature will measure and verify the integrity of the execution environment and the container workload at boot time. It ensures that the environment and the container have not been tampered with or modified maliciously.\r\n\r\n### Attestation\r\n\r\nAttestation will provide mechanisms for both secure boot verification and remote attestation:\r\n\r\n1. Secure Boot Verification: Ensures that the Kubernetes node and the container runtime have booted with verified and trusted firmware and software.\r\n\r\n2. Remote Attestation: Allows external entities to verify the integrity and trustworthiness of the node and container environments, ensuring that they are running in a secure and unmodified state.\r\n\r\n## Testing\r\n\r\nComprehensive testing strategies, including unit, integration, and e2e tests, will be developed to ensure the reliability and security of the proposed features.\r\n\r\n## Graduation Criteria\r\n\r\nAlpha: Initial implementation of the Trusted Container Loader and Secure Cluster\/POD Service, available for early feedback.\r\n\r\nBeta: Full functionality with improved security analysis, community feedback addressed, and broader testing coverage.\r\n\r\nStable: Proven scalability, performance, and security guarantees, with widespread adoption and positive community feedback.\r\n\r\n## Risks and Mitigations\r\n\r\nCompatibility: Ensuring compatibility with existing Kubernetes deployments and workloads. Mitigation includes thorough testing and providing configuration options for gradual adoption.\r\n\r\nPerformance: Potential performance overhead introduced by security measures. Mitigation includes optimizing the implementation and providing tunable parameters for trade-offs between security and performance.\r\n\r\n## Documentation\r\n\r\nComprehensive documentation will be provided, covering the setup, configuration, and usage of the new features, along with best practices for secure deployment.\r\n\r\n## Drawbacks\r\nThe main drawback is the potential increase in complexity and overhead for cluster management. However, the security benefits and alignment with confidential computing principles justify the effort.\r\n\r\n\r\n\r\n### Why is this needed?\r\n\r\nWith the increasing adoption of cloud-native technologies, the need for enhanced security mechanisms that protect sensitive data and computation at every layer of the cloud stack has become paramount. Confidential computing addresses this need by providing hardware-based isolation and encryption. However, Kubernetes lacks native support for key features of confidential computing, such as secure boot, remote attestation, and integrity measurement. This proposal seeks to fill these gaps, thereby enhancing the security posture of Kubernetes clusters.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig security node\r\n","https:\/\/www.kubernetes.dev\/resources\/keps\/\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/README.md"],"labels":["sig\/node","kind\/feature","sig\/security","needs-triage"]},{"title":"[Flaking Test] [sig-storage] In-tree Volumes [Driver: local] [LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]","body":"### Which jobs are flaking?\n\n- ci-kubernetes-e2e-ubuntu-gce-containerd\r\n\r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/96846744\/06c0f780-4ff6-4f0e-9e1e-3ebd21ed20f9)\r\n\n\n### Which tests are flaking?\n\nKubernetes e2e suite: [It] [sig-storage] In-tree Volumes [Driver: local] [LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]\n\n### Since when has it been flaking?\n\n03-11\r\n\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#gce-ubuntu-master-containerd\n\n### Reason for failure (if possible)\n\n```\r\n{ failed [FAILED] unable to upgrade connection: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix \/run\/containerd\/containerd.sock: connect: no such file or directory\"\r\nIn [DeferCleanup (Each)] at: k8s.io\/kubernetes\/test\/e2e\/storage\/utils\/local.go:163 @ 03\/11\/24 06:06:54.114\r\n}\r\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig storage","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @carlory ","It is not a storage issue. because the containerd is failing. ","\/sig node\r\n```\r\nKubernetes e2e suite: [It] [sig-storage] In-tree Volumes [Driver: local] [LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]\u00a0\r\n\r\n{ failed [FAILED] unable to upgrade connection: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix \/run\/containerd\/containerd.sock: connect: no such file or directory\"\r\nIn [DeferCleanup (Each)] at: k8s.io\/kubernetes\/test\/e2e\/storage\/utils\/local.go:163 @ 03\/11\/24 06:06:54.114\r\n}\r\n```\r\n\r\n\r\n"],"labels":["sig\/storage","sig\/node","kind\/flake","needs-triage"]},{"title":"Set pd-balanced as default disk type for GCE disks","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n`pd-balanced` is the default persistent storage type across entire Google Compute Engine (GCE).\r\nFor GKE it's the default since v1.24.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nChange the default disk type for the legacy GCE storage provider; the default type is now `pd-balanced`.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n- [Other doc]: Doc: https:\/\/cloud.google.com\/kubernetes-engine\/docs\/how-to\/custom-boot-disks#specify\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123863#\" title=\"Author self-approved\">azylinski<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cheftako](https:\/\/github.com\/cheftako) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cheftako\"]} -->","Changelog suggestion\r\n```diff\r\n-Use \"pd-balanced\" as default GCE disk type\r\n+Change the default disk type for the legacy GCE storage provider; the default type is now `pd-balanced`.\r\n```","> Changelog suggestion\r\n> \r\n> ```diff\r\n> -Use \"pd-balanced\" as default GCE disk type\r\n> +Change the default disk type for the legacy GCE storage provider; the default type is now `pd-balanced`.\r\n> ```\r\n\r\nThanks @sftim , Updated"],"labels":["kind\/bug","area\/cloudprovider","release-note","size\/S","cncf-cla: yes","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"apiserver\/pkg\/storage\/utils: remove unused EverythingFunc","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nsince we don't provide compatibility guarantees for the storage package it is okay to simply remove unused function.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/assign @wojtek-t ","LGTM label has been added.  <details>Git tree hash: a1c1d17d06ded685a3eb8e019f00c133d7a1dd0b<\/details>","\/retest ","\/lgtm\r\n\/approve\r\n\r\n[But let's merge it after code-freeze]","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123860#pullrequestreview-1927611300\" title=\"LGTM\">MadhavJivrajani<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123860#\" title=\"Author self-approved\">p0lyn0mial<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123860#issuecomment-1991887375\" title=\"Approved\">wojtek-t<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)~~ [wojtek-t]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/triage accepted"],"labels":["kind\/cleanup","area\/apiserver","lgtm","sig\/api-machinery","size\/XS","release-note-none","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"DRA: structured parameters: handling of claim without claim parameters","body":"### What would you like to be added?\n\nWithout claim parameters, it is unclear which structured model is meant to be used. If even it was clear, parameters for it might be useful.\r\n\r\nWe should add a default parameter reference to the ResourceClass. Then if a ResourceClaim has no parameter reference, that default gets used. If both are unset, the claim cannot get allocated.\n\n### Why is this needed?\n\nA claim without parameters may make sense depending on the hardware when users have no special needs.","comments":["\/sig node\r\n\/triage accepted\r\n\/priority important-soon\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/123828 is a stop-gap solution for this which hard-codes the default for \"named resources\" in the code. We should take that out again soon, in 1.31.\r\n","@ravisantoshgudimetla: you wanted to work on this. It shouldn't be hard, but you'll need to go through all the steps of modifying an in-tree API (updating both types.go, generating code, updating validation.go) before updating the scheduler plugin. Feel free to give it a try.\r\n\r\n\/assign @ravisantoshgudimetla","> We should add a default parameter reference to the ResourceClass.\r\n\r\nTo be clear, when we are adding a reference, we should also ensure that the referenced object exists, should we have a admission controller which creates the referenced resourceclassParameters object on the fly?\r\n","No. Referencing a default object which doesn't exist is the same as referencing parameters that don't exist: it's a problem when the object is needed, not when the ResourceClass gets created or updated.","> We should add a default parameter reference to the ResourceClass. Then if a ResourceClaim has no parameter reference, that default gets used. If both are unset, the claim cannot get allocated. \r\n\r\nSorry, I am a bit confused with the above statement and \r\n\r\n>  What we probably want is a field in the ResourceClass which\r\npoints to a default ResourceClaimPrameters object that should be used if one\r\nis not supplied as part of the claim.\r\n\r\nwhich was mentioned in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123828#issue-2176927920,\r\n\r\nDo we want to add new field called ResourceClaimParams which has a default value or do we want to fill in `ParametersRef` when it is nil with a ParameterRef which needs a object to be created. I believe you want the former?","We want a new field `DefaultClaimParametersRef` in `ResourceClass`. If `ResourceClaim.ParametersRef` is nil, then `ResourceClass.DefaultClaimParametersRef` gets used instead. If that is also nil, then the claim cannot be allocated.\r\n\r\n","> then ResourceClass.DefaultClaimParametersRef gets used instead. \r\n\r\nWho is going to create that? Cluster admin?","Cluster admin together with the `ResourceClass`.\r\n"],"labels":["priority\/important-soon","sig\/node","kind\/feature","triage\/accepted"]},{"title":"[Flaking Test] master-integration TestRatchetingFunctionality","body":"### Which jobs are flaking?\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1766734204889993216\r\n\r\n### Which tests are flaking?\r\n\r\nhttps:\/\/k8s.io\/apiextensions-apiserver\/test: integration\r\n\r\n- TestRatchetingFunctionality\/Enum\r\n- TestRatchetingFunctionality\/MinProperties_MaxProperties\r\n- TestRatchetingFunctionality\/CEL_Optional_OldSelf\r\n- TestRatchetingFunctionality\/[CEL_transition_rules_should_not_ratchet](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1765788576961794048)\r\n- TestRatchetingFunctionality\/[Map_Type_List_Reordering_Grandfathers_Invalid_Key](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-integration-master\/1765803926814199808)\r\n- \r\n\r\n### Since when has it been flaking?\r\n\r\nsince 3\/8\r\n\r\n\r\n### Testgrid link\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#integration-master\r\n\r\n### Reason for failure (if possible)\r\n\r\n```\r\nFailed;Failed;\r\n=== RUN   TestRatchetingFunctionality\/Enum\r\n    ratcheting_test.go:417: Performing Operation: Update CRD schema\r\n    ratcheting_test.go:417: Performing Operation: Create an instance with a soon-to-be-invalid value\r\nE0310 08:17:23.519794  101274 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"failed to create typed patch object (default\/mycrdinstance; mygroup.example.com\/v1beta1, Kind=MyCoolCRD): .enumField: field not declared in schema\"}: failed to create typed patch object (default\/mycrdinstance; mygroup.example.com\/v1beta1, Kind=MyCoolCRD): .enumField: field not declared in schema\r\n    ratcheting_test.go:419: failed integration_test.applyPatchOperation operation 1: failed to create typed patch object (default\/mycrdinstance; mygroup.example.com\/v1beta1, Kind=MyCoolCRD): .enumField: field not declared in schema\r\n        {Create an instance with a soon-to-be-invalid value {mygroup.example.com v1beta1 mycrds} mycrdinstance map[apiVersion:mygroup.example.com\/v1beta1 enumField:okValueNowBadValueLater kind:MyCoolCRD metadata:map[name:mycrdinstance namespace:default]]}\r\n    --- FAIL: TestRatchetingFunctionality\/Enum (0.03s)\r\n;=== RUN   TestRatchetingFunctionality\r\n    testserver.go:266: Resolved testserver package path to: \"\/home\/prow\/go\/src\/k8s.io\/kubernetes\/staging\/src\/k8s.io\/apiextensions-apiserver\/pkg\/cmd\/server\/testing\"\r\n    testserver.go:150: runtime-config=map[api\/all:true]\r\n    testserver.go:151: Starting apiextensions-apiserver on port 42281...\r\nI0310 08:17:19.819595  101274 serving.go:374] Generated self-signed cert (\/tmp\/apiextensions-apiserver1986275209\/apiserver.crt, \/tmp\/apiextensions-apiserver1986275209\/apiserver.key)\r\nW0310 08:17:20.646250  101274 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.\r\nI0310 08:17:20.652382  101274 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager\r\nW0310 08:17:20.652419  101274 genericapiserver.go:733] Skipping API apiextensions.k8s.io\/v1beta1 because it has no resources.\r\nW0310 08:17:20.652615  101274 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.\r\n    testserver.go:177: Waiting for \/healthz to be ok...\r\nI0310 08:17:20.663396  101274 dynamic_serving_content.go:132] \"Starting controller\" name=\"serving-cert::\/tmp\/apiextensions-apiserver1986275209\/apiserver.crt::\/tmp\/apiextensions-apiserver1986275209\/apiserver.key\"\r\nI0310 08:17:20.663688  101274 secure_serving.go:213] Serving securely on 127.0.0.1:42281\r\nI0310 08:17:20.663751  101274 tlsconfig.go:240] \"Starting DynamicServingCertificateController\"\r\nI0310 08:17:20.664002  101274 customresource_discovery_controller.go:289] Starting DiscoveryController\r\nI0310 08:17:20.664056  101274 controller.go:139] Starting OpenAPI controller\r\nI0310 08:17:20.664077  101274 controller.go:87] Starting OpenAPI V3 controller\r\nI0310 08:17:20.664095  101274 naming_controller.go:291] Starting NamingConditionController\r\nI0310 08:17:20.664112  101274 establishing_controller.go:76] Starting EstablishingController\r\nI0310 08:17:20.664130  101274 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController\r\nI0310 08:17:20.664147  101274 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController\r\nI0310 08:17:20.664158  101274 crd_finalizer.go:266] Starting CRDFinalizer\r\nW0310 08:17:20.664266  101274 reflector.go:547] k8s.io\/client-go\/informers\/factory.go:160: failed to list *v1.Service: Get \"http:\/\/127.1.2.3:12345\/api\/v1\/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused\r\nE0310 08:17:20.664344  101274 reflector.go:150] k8s.io\/client-go\/informers\/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http:\/\/127.1.2.3:12345\/api\/v1\/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused\r\nI0310 08:17:20.871007  101274 handler.go:286] Adding GroupVersion mygroup.example.com v1beta1 to ResourceManager\r\nW0310 08:17:21.841671  101274 reflector.go:547] k8s.io\/client-go\/informers\/factory.go:160: failed to list *v1.Service: Get \"http:\/\/127.1.2.3:12345\/api\/v1\/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused\r\nE0310 08:17:21.841775  101274 reflector.go:150] k8s.io\/client-go\/informers\/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http:\/\/127.1.2.3:12345\/api\/v1\/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused\r\nW0310 08:17:23.396815  101274 warnings.go:70] unknown field \"alpha\"\r\nW0310 08:17:23.396861  101274 warnings.go:70] unknown field \"beta\"\r\nW0310 08:17:23.396868  101274 warnings.go:70] unknown field \"delta\"\r\nW0310 08:17:23.396872  101274 warnings.go:70] unknown field \"epsilon\"\r\nW0310 08:17:23.396877  101274 warnings.go:70] unknown field \"gamma\"\r\nW0310 08:17:23.396881  101274 warnings.go:70] unknown field \"spec\"\r\n--- FAIL: TestRatchetingFunctionality (5.87s)\r\n;\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Relevant SIG(s)\r\n\r\n\/sig api-machinery\r\n","comments":["\/assign @alexzielenski \r\n\/triage accepted"],"labels":["sig\/api-machinery","kind\/flake","triage\/accepted"]},{"title":"[Flaking Test] [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","body":"### Which jobs are flaking?\n\nhttps:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=RuntimeClass%20should%20reject%20a%20Pod%20requesting%20a%20deleted%20RuntimeClass\r\n- ci-cos-containerd-node-e2e\r\n- ci-kubernetes-node-e2e-containerd\r\n- ci-kubernetes-e2e-node-canary\n\n### Which tests are flaking?\n\n[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]\n\n### Since when has it been flaking?\n\nlong time ago\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#ci-node-e2e\n\n### Reason for failure (if possible)\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-e2e-containerd\/1766506703232176128\r\n```\r\n[FAILED] should be forbidden\r\nExpected an error to have occurred.  Got:\r\n    <nil>: nil\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/common\/node\/runtimeclass.go:380 @ 02\/27\/24 08:12:21.967\r\n```\n\n### Anything else we need to know?\n\nold issue related: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/82352.\n\n### Relevant SIG(s)\n\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","The next runs have been all healthy. We can observe if for a few more days, and close it if it doesn't fail again."],"labels":["sig\/node","kind\/flake","needs-triage"]},{"title":"[Flaking Test] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","body":"### Which jobs are flaking?\n\nhttps:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=should%20be%20able%20to%20convert%20a%20non%20homogeneous%20list%20of%20CRs&xjob=calico\r\n- ci-kubernetes-e2e-capz-master-windows\r\n- ci-kubernetes-cloud-provider-kind-conformance-parallel\r\n- ci-kubernetes-e2e-kubeadm-kinder-rootless-latest\r\n- ci-kubernetes-e2e-capz-master-windows-hyperv\r\n\r\n\r\n\r\n\n\n### Which tests are flaking?\n\n- [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]\r\n\r\n\n\n### Since when has it been flaking?\n\nstorage.googleapis.com shows it flaked for a long period.\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#capz-windows-master\n\n### Reason for failure (if possible)\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-capz-master-windows\/1766944844321656832\r\n\r\n```\r\nSTEP: Verifying the service has paired with the endpoint - k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:345 @ 03\/10\/24 22:08:00.114\r\nI0310 22:08:01.114971 2463 util.go:427] Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1\r\n< Exit [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] - k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:126 @ 03\/10\/24 22:08:01.317 (8.596s)\r\n> Enter [It] should be able to convert a non homogeneous list of CRs [Conformance] - k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:177 @ 03\/10\/24 22:08:01.317\r\nI0310 22:08:01.317904 2463 util.go:506] >>> kubeConfig: \/home\/prow\/go\/src\/sigs.k8s.io\/windows-testing\/capz\/capz-conf-y1unem.kubeconfig\r\nI0310 22:08:31.630253 2463 crd_conversion_webhook.go:501] error waiting for conversion to succeed during setup: Post \"https:\/\/capz-conf-y1unem-4d42d29d.uksouth.cloudapp.azure.com:6443\/apis\/stable.example.com\/v2\/namespaces\/crd-webhook-5821\/e2e-test-crd-webhook-7762-crds\": context deadline exceeded\r\nI0310 22:08:31.630417 2463 crd_conversion_webhook.go:486] Unexpected error: \r\n    <context.deadlineExceededError>: \r\n    context deadline exceeded\r\n    \r\n        {}\r\n[FAILED] context deadline exceeded\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:486 @ 03\/10\/24 22:08:31.63\r\n< Exit [It] should be able to convert a non homogeneous list of CRs [Conformance] - k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:177 @ 03\/10\/24 22:08:31.63 (30.313s)\r\n\r\n\r\n{ failed [FAILED] context deadline exceeded\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/apimachinery\/crd_conversion_webhook.go:486 @ 03\/10\/24 22:08:31.63\r\n}\r\n```\n\n### Anything else we need to know?\n\nsome issues that may be related: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/93705\n\n### Relevant SIG(s)\n\n\/sig api-machinery\r\n\/sig windows\r\nsee this in a windows ci board, but may not be related. Add the sig for triage.","comments":["I looked into this today for sig-windows. It appears the test fails when it cannot reach the API server.  I can only find one instance of it failing for sig-windows main job, the hyper-v jobs have known networking issues and hence the failure.\r\n\r\nIt may be a timing issues since we are hitting this block https:\/\/github.com\/kubernetes\/kubernetes\/blob\/634fc1b4836b3a500e0d715d71633ff67690526a\/test\/e2e\/apimachinery\/crd_conversion_webhook.go#L499-L502","Looking at the other non-windows failures it seems like mostly occurs with many test failures where the API Server is not reachable. As an example:\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-local-e2e\/1764995137261277184","The call to `Create` got stuck and only made a single call instead of many over the 30 second timeout. See the logs:\r\n\r\n```\r\n0310 22:08:01.317904 2463 util.go:506] >>> kubeConfig: \/home\/prow\/go\/src\/sigs.k8s.io\/windows-testing\/capz\/capz-conf-y1unem.kubeconfig\r\nI0310 22:08:31.630253 2463 crd_conversion_webhook.go:501] error waiting for conversion to succeed during setup: Post \"https:\/\/capz-conf-y1unem-4d42d29d.uksouth.cloudapp.azure.com:6443\/apis\/stable.example.com\/v2\/namespaces\/crd-webhook-5821\/e2e-test-crd-webhook-7762-crds\": context deadline exceeded\r\nI0310 22:08:31.630417 2463 crd_conversion_webhook.go:486] Unexpected error: \r\n    <context.deadlineExceededError>: \r\n    context deadline exceeded\r\n    \r\n        {}\r\n```\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/634fc1b4836b3a500e0d715d71633ff67690526a\/test\/e2e\/apimachinery\/crd_conversion_webhook.go#L497 and the whole block timed out","\/assign @jsturtevant\r\nCould you continue working on this issue? Thank you.\r\n\/triage accepted "],"labels":["sig\/api-machinery","kind\/flake","sig\/windows","triage\/accepted"]},{"title":"promote DisableNodeKubeProxyVersion feature gate to beta","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nIn KEP, we plan to promote the DisableNodeKubeProxyVersion feature gate to the beta version in version 1.31\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nGraduated the `DisableNodeKubeProxyVersion` feature gate to beta. By default, the kubelet no longer attempts to set the `.status.kubeProxyVersion` field for its associated Node.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-network\/4004-deprecate-kube-proxy-version\r\n```\r\n","comments":["This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123845#\" title=\"Author self-approved\">HirazawaUi<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[test\/e2e\/cloud\/gcp\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/cloud\/gcp\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","\/cc @danwinship","Changelog suggestion\r\n```diff\r\n-Set the DisableNodeKubeProxyVersion feature gate to be enabled by default.. it will stop setting the KubeProxyVersion field.\r\n+Graduated the `DisableNodeKubeProxyVersion` feature gate to beta.\r\n+By default, the kubelet no longer attempts to set the `.status.kubeProxyVersion` field for its associated\r\n+Node.\r\n```","\/remove-sig api-machinery","\/triage accepted\r\n","\/sig network"],"labels":["area\/test","sig\/network","area\/kubelet","sig\/node","release-note","size\/S","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/testing","area\/code-generation","sig\/cloud-provider","needs-priority","triage\/accepted"]},{"title":"[code-generator] align generated builder comments","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThis  PR aligns the generated comments generated by the applyconfiguration generator as it should either use `can be build` or `can be built` everywhere and not use both.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Welcome @QuentinBisson! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @QuentinBisson. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123844#\" title=\"Author self-approved\">QuentinBisson<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sttts\"]} -->","\/ok-to-test\r\n\/assign @deads2k\r\nOnly a type fix. Thank you.\r\n\/triage accepted\r\n","Do you know how I could regenerate the existing comments  to fix the typo?","@QuentinBisson: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-kind-ipv6 | 0619e0c50e45bfd7839fb823396886bffd8dd17b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123844\/pull-kubernetes-e2e-kind-ipv6\/1767645133009850368) | true | `\/test pull-kubernetes-e2e-kind-ipv6`\npull-kubernetes-verify | 0619e0c50e45bfd7839fb823396886bffd8dd17b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123844\/pull-kubernetes-verify\/1767645135048282112) | true | `\/test pull-kubernetes-verify`\npull-kubernetes-e2e-gce | 0619e0c50e45bfd7839fb823396886bffd8dd17b | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123844\/pull-kubernetes-e2e-gce\/1767645132774969344) | true | `\/test pull-kubernetes-e2e-gce`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123844). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AQuentinBisson). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","@deads2k  maybe you can help? Is there a makefile I should run?"],"labels":["kind\/cleanup","sig\/api-machinery","size\/XS","release-note-none","cncf-cla: yes","area\/code-generation","ok-to-test","needs-priority","triage\/accepted"]},{"title":"Data lost when data is applied as data while not lost if data is applied as stringData in secret","body":"If we create secret with data type data and apply new data via secret then its old data is lost and new data is override by it while in case of stringData type data old data is not lost.\r\n\r\nWe can follow below steps to check it.\r\n\r\n1. Create secret with data type values like below:\r\n\r\n\r\n**# cat data-test-secrets.yaml**\r\n```\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: data-test-secret\r\ntype: Opaque\r\ndata:\r\n  testkey: \"dGVzdAo=\"\r\n```\r\n2. Apply test template for creation of data type secret.\r\n\r\n**# kubectl apply -f data-test-secrets.yaml**\r\n`secret\/data-test-secret created`\r\n#\r\n3. Checking data of secret having key as testkey\r\n\r\n**# kubectl get secret -o yaml data-test-secret**\r\n```\r\napiVersion: v1\r\ndata:\r\n  testkey: dGVzdAo=\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"testkey\":\"dGVzdAo=\"},\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"data-test-secret\",\"namespace\":\"test-system\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-10T10:22:37Z\"\r\n  name: data-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"578963891\"\r\n  uid: 1849763a-3296-4fd8-8462-ec411b09a7a3\r\ntype: Opaque\r\n```\r\n\r\n4. Create secret template with data type new values like below:\r\n\r\n**# cat 2data-test-secrets.yaml**\r\n```\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: data-test-secret\r\ntype: Opaque\r\ndata:\r\n  newtestkey: \"dGVzdAo=\"\r\n```\r\n\r\n5. Apply new test template for update of secret data.\r\n**# kubectl apply -f 2data-test-secrets.yaml**\r\n`secret\/data-test-secret configured`\r\n#\r\n\r\n6. Checking data of secret having key as testkey ( old key added via old template) removed now and newtestkey ( new key added via new template) added now.\r\n\r\n**# kubectl get secret -o yaml data-test-secret**\r\n```\r\napiVersion: v1\r\ndata:\r\n  newtestkey: dGVzdAo=\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"newtestkey\":\"dGVzdAo=\"},\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"data-test-secret\",\"namespace\":\"test-system\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-10T10:22:37Z\"\r\n  name: data-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"578975054\"\r\n  uid: 1849763a-3296-4fd8-8462-ec411b09a7a3\r\ntype: Opaque\r\n\r\n```\r\n\r\nHowever, if create secret with stringData then old data will not be removed. We can follow below steps to check it.\r\n\r\n1. Create secret with string data type values like below:\r\n\r\n**# cat stringData-test-secrets.yaml**\r\n```\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: stringdata-test-secret\r\ntype: Opaque\r\nstringData:\r\n  testkey: \"test\"\r\n```\r\n\r\n2. Apply test template for creation of string data type secret.\r\n**# kubectl apply -f stringData-test-secrets.yaml**\r\n`secret\/stringdata-test-secret created`\r\n#\r\n\r\n3. Checking data of secret having key as testkey\r\n\r\n**# kubectl get secret -o yaml stringdata-test-secret**\r\n```\r\napiVersion: v1\r\ndata:\r\n  testkey: dGVzdA==\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"stringdata-test-secret\",\"namespace\":\"test-system\"},\"stringData\":{\"testkey\":\"test\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-10T10:38:30Z\"\r\n  name: stringdata-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"579000870\"\r\n  uid: 31704d46-29a2-425c-8bb0-c06ae13527a1\r\ntype: Opaque\r\n```\r\n\r\n\r\n4. Create secret template with string data type new values like below:\r\n\r\n**# cat 2stringData-test-secrets.yaml**\r\n```\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: stringdata-test-secret\r\ntype: Opaque\r\nstringData:\r\n  newtestkey: \"test\"\r\n```\r\n\r\n5. Apply new test template for update of secret data.\r\n**# kubectl apply -f 2stringData-test-secrets.yaml**\r\n`secret\/stringdata-test-secret configured`\r\n#\r\n\r\n6. Checking data of secret having key as testkey ( old key added via old template) not removed now and newtestkey ( new key added via new template) added now.\r\n\r\n**# kubectl get secret -o yaml stringdata-test-secret**\r\n```\r\napiVersion: v1\r\ndata:\r\n  newtestkey: dGVzdA==\r\n  testkey: dGVzdA==\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"stringdata-test-secret\",\"namespace\":\"test-system\"},\"stringData\":{\"newtestkey\":\"test\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-10T10:38:30Z\"\r\n  name: stringdata-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"579006012\"\r\n  uid: 31704d46-29a2-425c-8bb0-c06ae13527a1\r\ntype: Opaque\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig api-machinery cli","`--server-side` can help you. \r\n\r\n```shell\r\n(\u2388|kind-kind:N\/A)\u279c  ~ kubectl apply -f tt2 --server-side\r\nsecret\/stringdata-test-secret serverside-applied\r\n(\u2388|kind-kind:N\/A)\u279c  ~ kubectl get -f tt2 -oyaml\r\napiVersion: v1\r\ndata:\r\n  newtestkey: dGVzdA==\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"name\":\"stringdata-test-secret\",\"namespace\":\"default\"},\"stringData\":{\"newtestkey\":\"test\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-11T02:58:52Z\"\r\n  name: stringdata-test-secret\r\n  namespace: default\r\n  resourceVersion: \"1986060\"\r\n  uid: c1924ad2-cf41-47fa-b17c-7920ccdf52cc\r\ntype: Opaque\r\n```","Observed the same behaviour while reproducing. ","Could you test out solution posted at https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123843#issuecomment-1987540307 ? Thank you.","Tested https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123843#issuecomment-1987540307 solution and its working.\r\n\r\nwhen i try to configure secret with `--server-side`, unless the data type new data is override by old data.\r\nThanks.\r\n","> Without **--server-side** flag\r\n\r\n![withoutserverside](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/56723673\/4fc551be-8cbd-41c8-9926-3b6339796500)\r\n\r\n> With **--server-side** flag\r\n\r\n![serverside](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/56723673\/0616becc-7b90-4673-b72f-635dd3c49d5c)\r\n","Thanks @carlory,\r\nYes i tried --server-side option then data was removed ( non persisting ) from stringData as well.\r\n\r\n```\r\n$ kubectl apply -f 2stringData-test-secrets.yaml --server-side\r\nsecret\/stringdata-test-secret serverside-applied\r\n\r\n$ kubectl get secret stringdata-test-secret -o yaml\r\napiVersion: v1\r\ndata:\r\n  newtestkey: dGVzdA==\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"name\":\"stringdata-test-secret\",\"namespace\":\"test-system\"},\"stringData\":{\"newtestkey\":\"test\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-15T07:29:23Z\"\r\n  name: stringdata-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"595549018\"\r\n  uid: 19f992c4-3f6c-4f47-a28a-5d841d889792\r\ntype: Opaque\r\n$\r\n```\r\n\r\nHowever , my requirement is like it would like to persist data of secret. So i have below query on it:\r\n\r\n1. As we tested above after using stringData old data was persisting . So can we use this as a option to persist data ? As i could not found this as a documented behavior or is it some kind of bug with stringData ?\r\n2. Why this kind of difference in behavior is happening between data type input and stringData type input ? Is it like data type by default uses option server-side and stringData do not use it?\r\n3.  Also, further, we have observed that if we use kubectl patch instead of kubectl apply then data will persist for data type input for secret as well. Why this kind of behaviors difference for data type input for secret during kubectl apply and kubectl patch ?\r\n\r\nFor eg:\r\n\r\n```\r\n$ kubectl patch secret data-test-secret --patch-file 2data-test-secrets.yaml\r\nsecret\/data-test-secret patched\r\n$\r\n$ kubectl get secret data-test-secret -o yaml\r\napiVersion: v1\r\ndata:\r\n  newtestkey: dGVzdAo=\r\n  testkey: dGVzdAo=\r\nkind: Secret\r\nmetadata:\r\n  annotations:\r\n    kubectl.kubernetes.io\/last-applied-configuration: |\r\n      {\"apiVersion\":\"v1\",\"data\":{\"testkey\":\"dGVzdAo=\"},\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"data-test-secret\",\"namespace\":\"test-system\"},\"type\":\"Opaque\"}\r\n  creationTimestamp: \"2024-03-15T07:49:39Z\"\r\n  name: data-test-secret\r\n  namespace: test-system\r\n  resourceVersion: \"595595966\"\r\n  uid: d2580400-fe78-44b2-bfa8-68672e403eaf\r\ntype: Opaque\r\n$\r\n```","Please any suggestions on above queries on data and stringData behavior during kubectl apply or kubectl patch ?","Mixing data and stringdata is not recommended. there's a conversion between data and stringData fields. the stringData field isn't persistent to etcd.\r\n\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/core\/v1\/conversion.go#L402\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/core\/v1\/zz_generated.conversion.go#L7684"],"labels":["sig\/api-machinery","sig\/cli","needs-triage"]},{"title":"kubeadm: increase ut converage for config\/upgradeconfiguration","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nkubeadm: increase ut converage for config\/upgradeconfiguration\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNONE\r\n```\r\n","comments":["\/triage accepted\r\n\/priority backlog\r\n","LGTM label has been added.  <details>Git tree hash: 8993432d0dc079c471557b8ff28c7d8fb9a611e3<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123842#\" title=\"Author self-approved\">my-git9<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123842#pullrequestreview-1927581866\" title=\"Approved\">neolit123<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[cmd\/kubeadm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubeadm\/OWNERS)~~ [neolit123]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["priority\/backlog","kind\/cleanup","lgtm","sig\/cluster-lifecycle","size\/L","release-note-none","approved","area\/kubeadm","cncf-cla: yes","triage\/accepted"]},{"title":"the --enable-hostpath-provisioner option is deprecated ","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind deprecation\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nSee https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123804#issuecomment-1985508047\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nPart of #123804\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nkube-controller-manager: the --enable-hostpath-provisioner option is deprecated and will be removed in a future release\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @jsafrane ","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123841#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-controller-manager\/config\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/config\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/cc @msau42 ","Changelog suggestion\r\n```diff\r\n-kube-controller-manager: the --enable-hostpath-provisioner option is deprecated and will be removed in a future release\r\n+kube-controller-manager: deprecated the `--enable-hostpath-provisioner` command line flag; it will be removed in a future release.\r\n```","\/remove-sig api-machinery"],"labels":["sig\/storage","release-note","size\/XS","kind\/api-change","sig\/apps","cncf-cla: yes","area\/code-generation","needs-priority","kind\/deprecation","needs-triage"]},{"title":"Static pods with restartPolicy: OnFailure remain pending if kubelet restarts after container create but before start.","body":"### What happened?\n\nGiven a static pod with `restartPolicy: OnFailure`, and  a Kubelet crash while re-creating a failed container in the static pod, the pod may remain pending indefinitely.\r\n\r\n1. Run a cluster with the patch at https:\/\/github.com\/hoskeri\/kubernetes\/commit\/60b103a1df145a688a72ebe1473deddec57109e0. The patch causes kubelet to crash when attempting to start a container that's been created after a failure.\r\n2. Start a static pod with `restartPolicy: OnFailure`.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: staticpod-failure-tester\r\nspec: \r\n  restartPolicy: OnFailure\r\n  hostNetwork: true\r\n  initContainers:\r\n  - name: staticpod-failure-init-1\r\n    image: \"debian:unstable\"\r\n    command:\r\n    - echo \r\n    - \"staticpod-failure-tester-init\"\r\n  containers:\r\n  - name: staticpod-failure-tester-main\r\n    image: \"redis:latest\"\r\n    command:\r\n    - \"redis-server\"\r\n```\r\n\r\n\r\n4. Arm the crash to occur on the next StartContainer of our container.\r\n\r\necho -n staticpod-failure-tester-main > \/run\/crash-container-names\r\n\r\n6. Kill the shim process and `crictl stopp $pod`. \r\n\r\n(Careful, modify the command to be more specific if other containers are running in your env)\r\n\r\n`kill -9 $SHIM_PID; crictl pods -q|xargs crictl stopp`\r\n\r\n7. Kubelet should create a replacement container for the killed container in the same sandbox.\r\n\r\n9. Kubelet crashes and restarts on StartContainer.\r\n\r\n10. After the first crash, kubelet will not trigger the crash again, because it will not attempt a StartContainer again. \r\nInstead, `computePodActions` wants to stop the existing sandbox, but this does not converge because containerd ignores the `StopContainer` request. \r\n\r\n```\r\ninfo msg=\"Container to stop \\\"469b9ca1059f90783892692b7ecf22b967d015a390446b44ee29894026b7e46a\\\" must be in running or unknown state, current state \\\"CONTAINER_CREATED\\\"\"\r\n```\r\n\r\n```\r\nkuberuntime_manager.go:1058] \"computePodActions got for pod\" podActions=\"KillPod: true, CreateSandbox: false, UpdatePodResources: false, Attempt: 3, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"default\/staticpod-failure-tester-i-ea7ab8cd-6939-4748-b544-21d797194f5e\"\r\n```\r\n\r\n11. The pod remains `Pending` indefinitely.\r\n\r\nThe real-world version of the above sequence involves a reboot of control plane host vm while Kubelet is still recovering pods from a previous crash - so two reboots in quick succession. \r\n\r\nBased on my reading of the code, it seems to me that the core issue is in the `containerSucceeded` function treats an uninitialized container as having succeeded. We incorrectly read the golang zero value of `containerStatus.ExitCode` as a successful exit code of `0` from the container. \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/8f80e0146726c42edefdfaeda6123872a5ec0981\/pkg\/kubelet\/kuberuntime\/kuberuntime_manager.go#L535\r\n\r\nThis is not an issue if the restartPolicy is `Always` because https:\/\/github.com\/kubernetes\/kubernetes\/blob\/8f80e0146726c42edefdfaeda6123872a5ec0981\/pkg\/kubelet\/kuberuntime\/kuberuntime_manager.go#L851 is not reached in that case.\r\n\r\n\n\n### What did you expect to happen?\n\nThe static pod should either end at a terminal phase, or restart the successfully.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee reproduction steps in the main report. \r\n\r\nHere's a strawman change that fixes the issue for my test case. \r\n\r\nhttps:\/\/github.com\/hoskeri\/kubernetes\/commit\/5270c2916f78b78aab6845de94cd4d06b271ecd3\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.30.0-alpha.3.320+19df52c41a25ce\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nNA\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n\r\nPRETTY_NAME=\"Debian GNU\/Linux trixie\/sid\"\r\nNAME=\"Debian GNU\/Linux\"\r\nVERSION_CODENAME=trixie\r\nID=debian\r\nHOME_URL=\"https:\/\/www.debian.org\/\"\r\nSUPPORT_URL=\"https:\/\/www.debian.org\/support\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.debian.org\/\"\r\n\r\n$ uname -a\r\nLinux i-ea7ab8cd-6939-4748-b544-21d797194f5e 6.6.15-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.6.15-2 (2024-02-04) x86_64 GNU\/Linux\r\n```\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n```\r\nType:          io.containerd.grpc.v1\r\nID:            cri\r\nRequires:     \r\n               io.containerd.event.v1\r\n               io.containerd.service.v1\r\nPlatforms:     linux\/amd64\r\nExports:      \r\n               CRIVersionAlpha      v1alpha2\r\n               CRIVersion           v1\r\n```\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nNA\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","Out of curiosity, what is the use case for running a static pod with a restart policy other than `Always`? It seems like anything other than always would be somewhat arbitrary, since the state would be tracked unreliably?","> After the first crash, kubelet will not trigger the crash again, because it will not attempt a StartContainer again.\r\nInstead, computePodActions wants to stop the existing sandbox, but this does not converge because containerd ignores the StopContainer request.\r\n\r\n@hoskeri it seems like there are two issues:\r\n 1. kubelet tries to kill the container because it thinks that the container is in a running state. Did you see containerd\/cri reporting the container as running? If so, there might be a containerd issue.\r\n 2. kubelet not starting the a new replacement container because it considered the pod succeeded. \r\n\r\nDid you verify that with restart policy \"always\", the problem wouldn't occur (meaning (1) wouldn't block (2))?\r\n\r\n> Out of curiosity, what is the use case for running a static pod with a restart policy other than Always? It seems like anything other than always would be somewhat arbitrary, since the state would be tracked unreliably?\r\n\r\n+1 to this question too.","> Out of curiosity, what is the use case for running a static pod with a restart policy other than `Always`? It seems like anything other than always would be somewhat arbitrary, since the state would be tracked unreliably?\r\n\r\nI agree, we are switching the restartPolicy to Always as the fix for this issue in our application. The earlier setting may have been an oversight, or a misunderstanding of what OnFailure does. \r\n\r\nHowever, I think this is still a bug, or atleast a footgun that needs to be documented. \r\n\r\n- restartPolicy: OnFailure is definitely applicable to jobs\/cronjobs, for example. \r\n- I think the same issue applies to initContainers? I have not yet confirmed if that's the case. \r\n- ","> > After the first crash, kubelet will not trigger the crash again, because it will not attempt a StartContainer again.\r\n> > Instead, computePodActions wants to stop the existing sandbox, but this does not converge because containerd ignores the StopContainer request.\r\n> \r\n> @hoskeri it seems like there are two issues:\r\n> \r\n> 1. kubelet tries to kill the container because it thinks that the container is in a running state. Did you see containerd\/cri reporting the container as running? If so, there might be a containerd issue.\r\n\r\nNo, it did not, the container was not started after create, and the state as seen by kubelet remained at 'created'. \r\n\r\n> 2. kubelet not starting the a new replacement container because it considered the pod succeeded.\r\n> \r\n> Did you verify that with restart policy \"always\", the problem wouldn't occur (meaning (1) wouldn't block (2))?\r\n\r\nI have not verified this, but I think restartPolicy: Always works because the 'created' container is started, and then killPod can succeed in killing the now running container. \r\n \r\n> > Out of curiosity, what is the use case for running a static pod with a restart policy other than Always? It seems like anything other than always would be somewhat arbitrary, since the state would be tracked unreliably?\r\n> \r\n> +1 to this question too.\r\n\r\nWe are switching to Always for our application, but there might be other cases (see my other comment) where this might still be a problem. \r\n\r\n","> No, it did not, the container was not started after create, and the state as seen by kubelet remained at 'created'.\r\n\r\nIn this case, kubelet shouldn't even try to stop the container. I was confused as why kubelet repeatedly trying to do that (not just a race condition during a specific window).\r\n\r\n> I have not verified this, but I think restartPolicy: Always works because the 'created' container is started, and then killPod can succeed in killing the now running container.\r\n\r\nI didn't trace the code, but it's also possible that kubelet wouldn't get past the \"killing container\" phase if containerd returns an error on stopping the container. Then it'd never actually reach the creating\/starting containers phase. \r\n","It sounds like there are some legit issues here that should be fixed, but I wonder if we should also stop supporting non-`Always` restart policies on static pods. We would probably want to just overwrite the restart policy on creation rather than restrict it through validation, for backwards compatibility.","Setting  needs-information since we need to understand the use-case better.\r\n\r\n\/triage needs-information","@AnishShah: The label(s) `triage\/needs-information.` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123839#issuecomment-1989572818):\n\n>Setting  needs-information since we need to understand the use-case better.\r\n>\r\n>\/triage needs-information.\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/triage needs-information","> > No, it did not, the container was not started after create, and the state as seen by kubelet remained at 'created'.\r\n> \r\n> In this case, kubelet shouldn't even try to stop the container. I was confused as why kubelet repeatedly trying to do that (not just a race condition during a specific window).\r\n> \r\n\r\nKubelet is trying to stop the container because `KillPod: true`. \r\n\r\n```\r\nkuberuntime_manager.go:1058] \"computePodActions got for pod\" podActions=\"KillPod: true, CreateSandbox: false, UpdatePodResources: false, Attempt: 3, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"default\/staticpod-failure-tester-i-ea7ab8cd-6939-4748-b544-21d797194f5e\"\r\n```\r\n\r\n> > I have not verified this, but I think restartPolicy: Always works because the 'created' container is started, and then killPod can succeed in killing the now running container.\r\n> \r\n> I didn't trace the code, but it's also possible that kubelet wouldn't get past the \"killing container\" phase if containerd returns an error on stopping the container. Then it'd never actually reach the creating\/starting containers phase.\r\n\r\ncontainerd does not return an error. The 'CONTAINER_STARTED' message is an informative message. No error is returned. https:\/\/github.com\/containerd\/containerd\/blob\/f8fbdfdd6f191bf93321501170d3f85675890601\/internal\/cri\/server\/container_stop.go#L82\r\n\r\n\r\n","Hi @AnishShah, Could you please clarify what more-information is needed? \r\n\r\n- more evidence that a bug exists? \r\n- more details on the precise root cause? \r\n- anything else?","@hoskeri , during sig-node CI bug triaging, we discussed this issue and wanted to understand whether we have a use-case to support `restartPolicy: OnFailure` in Static pods."],"labels":["kind\/bug","sig\/node","triage\/needs-information","needs-triage"]},{"title":"Codegen tools need to ignore struct fields named `_`","body":"This is apparently a trick used by some Go devs to force fields to be named in struct initializing code.\r\n\r\nFrom: https:\/\/github.com\/kubernetes\/gengo\/issues\/133\r\n\r\nRunning deepcopy-gen against structs that have `_` fields in them generates invalid go code.\r\n\r\nSource struct:\r\n\r\n```\r\ntype CreateBucketConfiguration struct {\r\n\t_ struct{} `type:\"structure\"`\r\n\r\n\tLocationConstraint BucketLocationConstraint `type:\"string\" enum:\"true\"`\r\n}\r\ntype BucketLocationConstraint string\r\n```\r\n\r\n`doc.go` directives to deepcopy:\r\n\r\n```\r\n\/\/ +k8s:deepcopy-gen=package,register\r\n```\r\n\r\ndeepcopy directives above structs that the above struct is a child of  (not sure if this makes a difference or not - this is autogenerated code from the operator sdk)\r\n\r\n```\r\n\/\/ +k8s:deepcopy-gen:interfaces=k8s.io\/apimachinery\/pkg\/runtime.Object\r\n```\r\n\r\nOutput by deepcopy-gen:\r\n```\r\n\/\/ DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\r\nfunc (in *CreateBucketConfiguration) DeepCopyInto(out *CreateBucketConfiguration) {\r\n\t*out = *in\r\n\tout._ = in._\r\n\treturn\r\n}\r\n```\r\n\r\nSetting or accessing `_` fields is invalid. \r\n\r\nI applied the following patch locally and it outputs correct code (ie does not attempt to copy the `_` field).  No idea if this is the _correct_ way to solve the problem though.\r\n\r\n```\r\ndiff --git a\/vendor\/k8s.io\/gengo\/examples\/deepcopy-gen\/generators\/deepcopy.go b\/vendor\/k8s.io\/gengo\/examples\/deepcopy-gen\/generators\/deepcopy.go\r\nindex 4548108..1d90578 100644\r\n--- a\/vendor\/k8s.io\/gengo\/examples\/deepcopy-gen\/generators\/deepcopy.go\r\n+++ b\/vendor\/k8s.io\/gengo\/examples\/deepcopy-gen\/generators\/deepcopy.go\r\n@@ -823,6 +823,9 @@ func (g *genDeepCopy) doStruct(t *types.Type, sw *generator.SnippetWriter) {\r\n\r\n \t\/\/ Now fix-up fields as needed.\r\n \tfor _, m := range ut.Members {\r\n+\t\tif m.Name == \"_\" {\r\n+\t\t\tcontinue\r\n+\t\t}\r\n \t\tft := m.Type\r\n \t\tuft := underlyingType(ft)\r\n```\r\n\r\nAfter applying this patch, output is as expected:\r\n\r\n```\r\n\/\/ DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\r\nfunc (in *CreateBucketConfiguration) DeepCopyInto(out *CreateBucketConfiguration) {\r\n\t*out = *in\r\n\treturn\r\n}\r\n```\r\n\r\n","comments":["\/sig api-machinery","\/assign @thockin \r\nFor context. Thank you.\r\n\/triage accepted","I filed it because I don't have time to tackle it in the near future but didn't want to lose track of it.  Assigning to me is not right :)","\/unassign\r\nSorry :)"],"labels":["sig\/api-machinery","triage\/accepted"]},{"title":"kube-proxy does not appear to be creating iptables entries","body":"### What happened?\n\nWhen setting up a new cluster using Kubernetes 1.29.2 on Debian 12.5 (\"bookworm\"), it appears that the necessary iptables entries to permit access to services, etc., are not being created by kube-proxy. Upon reaching the step in setting up the first control-plane node, post `kubeadm init`, at which it is necessary to add a networking option, the pods of the networking option invariably fail complaining that it is impossible to reach the Kubernetes API server via the Kubernetes service.\r\n\r\nThings at this point appear normal except for the failed networking option pod:\r\n\r\n```\r\n\u276f kubectl get node\r\nNAME                STATUS   ROLES           AGE   VERSION\r\nprincess-celestia   Ready    control-plane   55m   v1.29.2\r\n\r\n\u276f kubectl get pod -A\r\nNAMESPACE     NAME                                        READY   STATUS              RESTARTS       AGE\r\nkube-system   coredns-76f75df574-62dx4                    0\/1     ContainerCreating   0              55m\r\nkube-system   coredns-76f75df574-m6dhg                    0\/1     ContainerCreating   0              55m\r\nkube-system   etcd-princess-celestia                      1\/1     Running             3 (23m ago)    55m\r\nkube-system   kube-apiserver-princess-celestia            1\/1     Running             3 (22m ago)    55m\r\nkube-system   kube-controller-manager-princess-celestia   1\/1     Running             2 (23m ago)    55m\r\nkube-system   kube-proxy-7pwnn                            1\/1     Running             0              21m\r\nkube-system   kube-scheduler-princess-celestia            1\/1     Running             2 (23m ago)    55m\r\nkube-system   weave-net-mnm4v                             1\/2     CrashLoopBackOff    24 (25s ago)   50m\r\n```\r\n\r\nThis example is from Weave, but the equivalent error also occurs in Flannel, leading me to conclude that the issue is not with them:\r\n\r\n```\r\nFATA: 2024\/03\/09 23:25:51.801940 [kube-peers] Could not get peers: Get \"https:\/\/[fdc9:b01a:cafe:60::1]:443\/api\/v1\/nodes\": dial tcp [fdc9:b01a:cafe:60::1]:443: connect: network is unreachable\r\nFailed to get peers\r\n```\r\n\r\nThe kube-proxy pod log shows no calls to iptables:\r\n\r\n```\r\nI0309 23:20:38.208141       1 server_others.go:72] \"Using iptables proxy\"\r\nI0309 23:20:38.212206       1 server.go:1050] \"Successfully retrieved node IP(s)\" IPs=[\"172.16.0.129\"]\r\nI0309 23:20:38.213216       1 conntrack.go:58] \"Setting nf_conntrack_max\" nfConntrackMax=262144\r\nI0309 23:20:38.220948       1 server.go:652] \"kube-proxy running in dual-stack mode\" primary ipFamily=\"IPv4\"\r\nI0309 23:20:38.220959       1 server_others.go:168] \"Using iptables Proxier\"\r\nI0309 23:20:38.221908       1 proxier.go:245] \"Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses\"\r\nI0309 23:20:38.221976       1 server.go:865] \"Version info\" version=\"v1.29.2\"\r\nI0309 23:20:38.221981       1 server.go:867] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\r\nI0309 23:20:38.222805       1 config.go:315] \"Starting node config controller\"\r\nI0309 23:20:38.222817       1 shared_informer.go:311] Waiting for caches to sync for node config\r\nI0309 23:20:38.222829       1 config.go:188] \"Starting service config controller\"\r\nI0309 23:20:38.222837       1 shared_informer.go:311] Waiting for caches to sync for service config\r\nI0309 23:20:38.222939       1 config.go:97] \"Starting endpoint slice config controller\"\r\nI0309 23:20:38.222943       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config\r\nI0309 23:20:38.323006       1 shared_informer.go:318] Caches are synced for endpoint slice config\r\nI0309 23:20:38.323015       1 shared_informer.go:318] Caches are synced for node config\r\nI0309 23:20:38.323020       1 shared_informer.go:318] Caches are synced for service config\r\n```\r\n\r\nand while the chains and some relevant entries are seen, the essential ones appear to be missing, per the following output from _ip6tables-save_ and _iptables-save_:\r\n\r\n```\r\nroot@princess-celestia:~# ip6tables-save\r\n# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024\r\n*mangle\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:KUBE-IPTABLES-HINT - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:03 2024\r\n# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024\r\n*filter\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:KUBE-EXTERNAL-SERVICES - [0:0]\r\n:KUBE-FIREWALL - [0:0]\r\n:KUBE-FORWARD - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-NODEPORTS - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\n:KUBE-PROXY-FIREWALL - [0:0]\r\n:KUBE-SERVICES - [0:0]\r\n-A INPUT -j KUBE-FIREWALL\r\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A INPUT -m comment --comment \"kubernetes health check service ports\" -j KUBE-NODEPORTS\r\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\r\n-A OUTPUT -j KUBE-FIREWALL\r\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000\/0x4000 -j ACCEPT\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\n-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a\/128 -p tcp -m comment --comment \"kube-system\/kube-dns:dns-tcp has no endpoints\" -m tcp --dport 53 -j REJECT --reject-with icmp6-port-unreachable\r\n-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a\/128 -p tcp -m comment --comment \"kube-system\/kube-dns:metrics has no endpoints\" -m tcp --dport 9153 -j REJECT --reject-with icmp6-port-unreachable\r\n-A KUBE-SERVICES -d fdc9:b01a:cafe:60::a\/128 -p udp -m comment --comment \"kube-system\/kube-dns:dns has no endpoints\" -m udp --dport 53 -j REJECT --reject-with icmp6-port-unreachable\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:03 2024\r\n# Generated by ip6tables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:03 2024\r\n*nat\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-MARK-MASQ - [0:0]\r\n:KUBE-NODEPORTS - [0:0]\r\n:KUBE-POSTROUTING - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\n:KUBE-SEP-ZW3YEZJQTUKK7ANJ - [0:0]\r\n:KUBE-SERVICES - [0:0]\r\n:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]\r\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\r\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000\/0x4000\r\n-A KUBE-POSTROUTING -m mark ! --mark 0x4000\/0x4000 -j RETURN\r\n-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000\/0x0\r\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -j MASQUERADE --random-fully\r\n-A KUBE-SEP-ZW3YEZJQTUKK7ANJ -s fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8\/128 -m comment --comment \"default\/kubernetes:https\" -j KUBE-MARK-MASQ\r\n-A KUBE-SEP-ZW3YEZJQTUKK7ANJ -p tcp -m comment --comment \"default\/kubernetes:https\" -m tcp -j DNAT --to-destination [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443\r\n-A KUBE-SERVICES -d fdc9:b01a:cafe:60::1\/128 -p tcp -m comment --comment \"default\/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y\r\n-A KUBE-SERVICES ! -d ::1\/128 -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS\r\n-A KUBE-SVC-NPX46M4PTMTKRN6Y ! -s fdc9:b01a:cafe::\/56 -d fdc9:b01a:cafe:60::1\/128 -p tcp -m comment --comment \"default\/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-MARK-MASQ\r\n-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default\/kubernetes:https -> [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443\" -j KUBE-SEP-ZW3YEZJQTUKK7ANJ\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:03 2024\r\n```\r\n\r\n```\r\nroot@princess-celestia:~# iptables-save\r\n# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024\r\n*mangle\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:KUBE-IPTABLES-HINT - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:35 2024\r\n# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024\r\n*filter\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:KUBE-EXTERNAL-SERVICES - [0:0]\r\n:KUBE-FIREWALL - [0:0]\r\n:KUBE-FORWARD - [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-NODEPORTS - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\n:KUBE-PROXY-FIREWALL - [0:0]\r\n:KUBE-SERVICES - [0:0]\r\n:WEAVE-NPC - [0:0]\r\n:WEAVE-NPC-DEFAULT - [0:0]\r\n:WEAVE-NPC-EGRESS - [0:0]\r\n:WEAVE-NPC-EGRESS-ACCEPT - [0:0]\r\n:WEAVE-NPC-EGRESS-CUSTOM - [0:0]\r\n:WEAVE-NPC-EGRESS-DEFAULT - [0:0]\r\n:WEAVE-NPC-INGRESS - [0:0]\r\n-A INPUT -j KUBE-FIREWALL\r\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A INPUT -m comment --comment \"kubernetes health check service ports\" -j KUBE-NODEPORTS\r\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\r\n-A OUTPUT -j KUBE-FIREWALL\r\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\r\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A KUBE-FIREWALL ! -s 127.0.0.0\/8 -d 127.0.0.0\/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP\r\n-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000\/0x4000 -j ACCEPT\r\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\r\n-A WEAVE-NPC -m state --state RELATED,ESTABLISHED -j ACCEPT\r\n-A WEAVE-NPC -d 224.0.0.0\/4 -j ACCEPT\r\n-A WEAVE-NPC -m physdev --physdev-out vethwe-bridge --physdev-is-bridged -j ACCEPT\r\n-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-DEFAULT\r\n-A WEAVE-NPC -m state --state NEW -j WEAVE-NPC-INGRESS\r\n-A WEAVE-NPC-EGRESS -m state --state RELATED,ESTABLISHED -j ACCEPT\r\n-A WEAVE-NPC-EGRESS -m physdev --physdev-in vethwe-bridge --physdev-is-bridged -j RETURN\r\n-A WEAVE-NPC-EGRESS -m addrtype --dst-type LOCAL -j RETURN\r\n-A WEAVE-NPC-EGRESS -d 224.0.0.0\/4 -j RETURN\r\n-A WEAVE-NPC-EGRESS -m state --state NEW -j WEAVE-NPC-EGRESS-DEFAULT\r\n-A WEAVE-NPC-EGRESS -m state --state NEW -m mark ! --mark 0x40000\/0x40000 -j WEAVE-NPC-EGRESS-CUSTOM\r\n-A WEAVE-NPC-EGRESS-ACCEPT -j MARK --set-xmark 0x40000\/0x40000\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:35 2024\r\n# Generated by iptables-save v1.8.9 (nf_tables) on Sat Mar  9 17:33:35 2024\r\n*nat\r\n:PREROUTING ACCEPT [0:0]\r\n:INPUT ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n:POSTROUTING ACCEPT [0:0]\r\n:KUBE-KUBELET-CANARY - [0:0]\r\n:KUBE-MARK-MASQ - [0:0]\r\n:KUBE-NODEPORTS - [0:0]\r\n:KUBE-POSTROUTING - [0:0]\r\n:KUBE-PROXY-CANARY - [0:0]\r\n:KUBE-SERVICES - [0:0]\r\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\r\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\r\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000\/0x4000\r\n-A KUBE-POSTROUTING -m mark ! --mark 0x4000\/0x4000 -j RETURN\r\n-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000\/0x0\r\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -j MASQUERADE --random-fully\r\n-A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS\r\nCOMMIT\r\n# Completed on Sat Mar  9 17:33:35 2024\r\n```\r\n\n\n### What did you expect to happen?\n\nOnce `kubeadm init` had completed, installation of a network option should proceed and complete normally; it (and other pods) should be able to access the _kubernetes_ service.\r\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRather than repeat the details of every command:\r\n\r\nTake a vanilla, minimal Debian 12.5 installation, add containerd as the runtime, and then `kubeadm init`. Specifically, I use the cluster configuration file:\r\n\r\n```\r\n---\r\napiVersion: kubeadm.k8s.io\/v1beta3\r\nkind: InitConfiguration\r\n\r\nlocalAPIEndpoint:\r\n  advertiseAddress: fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8\r\n\r\nnodeRegistration:\r\n  taints: []\r\n\r\n---\r\napiVersion: kubeadm.k8s.io\/v1beta3\r\nkind: ClusterConfiguration\r\nkubernetesVersion: 1.29.2\r\n\r\nclusterName: harmony\r\n\r\napiServer:\r\n  certSANs:\r\n    - \"princess-celestia.arkane-systems.lan\"\r\n    - \"172.16.0.129\"\r\n  timeoutForControlPlane: 4m0s\r\n\r\netcd:\r\n  local:\r\n    dataDir: \/var\/lib\/etcd\r\n\r\nnetworking:\r\n  dnsDomain: cluster.local\r\n  serviceSubnet: fdc9:b01a:cafe:60::\/112,10.96.0.0\/16\r\n  podSubnet: fdc9:b01a:cafe:f4::\/56,10.244.0.0\/16\r\n```\r\n\r\nto set up for IPv6 networking, using the command `kubeadm init --config .\/cluster.conf`, although using different subnet configurations makes no difference.\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.2\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nNone.\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Debian GNU\/Linux 12 (bookworm)\"\r\nNAME=\"Debian GNU\/Linux\"\r\nVERSION_ID=\"12\"\r\nVERSION=\"12 (bookworm)\"\r\nVERSION_CODENAME=bookworm\r\nID=debian\r\nHOME_URL=\"https:\/\/www.debian.org\/\"\r\nSUPPORT_URL=\"https:\/\/www.debian.org\/support\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.debian.org\/\"\r\n$ uname -a\r\nLinux princess-celestia 6.1.0-18-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU\/Linux\r\n\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nNone except `kubeadm`.\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n# containerd --version\r\ncontainerd containerd.io 1.6.28 ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig network\r\n","Clarification: this does work using a default IPv4 configuration. It's only bringing IPv6 into it that seems to make it fail.\r\n","Can you validate from the nodes that plain connectivity works and you are able to connect to the apiserver?\r\nFirst try the advertised address\r\n```\r\ncurl -k -v https:\/\/[fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443\r\n```\r\nand if it does work try the service address\r\n```\r\ncurl -k -v https:\/\/[fdc9:b01a:cafe:60::1]:443\r\n```\r\n","Connecting to the API server at the advertised address works fine; from the service address does not.\r\n","hmm, one thing is weird\r\n\r\n> I0309 23:20:38.220948       1 server.go:652] \"kube-proxy running in dual-stack mode\" primary ipFamily=\"IPv4\"\r\n\r\nbut it seems you get your ip6tables rules correctly\r\n\r\n> -A KUBE-SERVICES -d fdc9:b01a:cafe:60::1\/128 -p tcp -m comment --comment \"default\/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y\r\n\r\ngoes to\r\n\r\n> -A KUBE-SVC-NPX46M4PTMTKRN6Y ! -s fdc9:b01a:cafe::\/56 -d fdc9:b01a:cafe:60::1\/128 -p tcp -m comment --comment \"default\/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-MARK-MASQ\r\n> -A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default\/kubernetes:https -> [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443\"\r\n\r\nand to \r\n\r\n> -A KUBE-SEP-ZW3YEZJQTUKK7ANJ -s fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8\/128 -m comment --comment \"default\/kubernetes:https\" -j KUBE-MARK-MASQ\r\n> -A KUBE-SEP-ZW3YEZJQTUKK7ANJ -p tcp -m comment --comment \"default\/kubernetes:https\" -m tcp -j DNAT --to-destination [fdc9:b01a:9d26:0:8aae:ddff:fe0a:99d8]:6443\r\n\r\nso it should redirect the traffic \r\n\r\ndo you have ipv6 forwarding enabled?\r\n```\r\nsysctl -w net.ipv6.conf.all.forwarding=1\r\n```\r\n\r\nwe run IPv6 only CI using kubecadm with kind and it is working https:\/\/testgrid.k8s.io\/conformance-kind#kind%20(IPv6),%20master%20(dev)","I do:\r\n\r\n```\r\ncluster@princess-celestia:~$ cat \/etc\/sysctl.d\/kubernetes.conf\r\nnet.bridge.bridge-nf-call-ip6tables = 1\r\nnet.bridge.bridge-nf-call-iptables = 1\r\nnet.ipv4.ip_forward = 1\r\nnet.ipv6.conf.all.forwarding = 1\r\n\r\ncluster@princess-celestia:~$ cat \/proc\/sys\/net\/ipv6\/conf\/all\/forwarding\r\n1\r\n```\r\n\r\n(same results on all nodes). If the iptables are doing the right thing (I must confess I'm not as up on iptables as I ought to be.) then - well, it's a puzzler to me.\r\n\r\nI've had clusters working in the past (k8s 1.27, earlier versions of Debian bookworm) dual-stack with IPv6 primary myself, too, which only makes it more confusing to me. Not like I've suddenly changed my setup procedure, just updated the versions of the software involved.","ok, let's start over, can you do paste the the versions of the components involved and images that has changed in a working cluster and a failing one?","Without having looked at this in much detail: the fact that there are weave-related rules in the ipv4 dump but not in the ipv6 dump seems suspicious. Is it possible you configured kube-proxy for dual-stack but configured your CNI plugin for single-stack?"],"labels":["kind\/bug","sig\/network","needs-triage"]},{"title":"CRDs with invalid CABundles fail to serve requests not requiring conversion","body":"### What happened?\n\nA CRDs with an invalid conversion webhook CABundle failed to serve requests not requiring conversion.\r\n\n\n### What did you expect to happen?\n\nSince creating cr-1.yaml doesn't require conversion, I would have expected either:\r\n\r\n1. An error response when attempting to create\/update the CRD with an invalid CABundle\r\n2. Only an error when conversion is required to server a request\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n\r\nCreate a mult-version CRD with a conversion webhook configured with an invalid CABundle:\r\n\r\n```yaml\r\napiVersion: apiextensions.k8s.io\/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: replicant.stable.example.com\r\nspec:\r\n  group: stable.example.com\r\n  versions:\r\n    - name: v1\r\n      served: true\r\n      storage: true\r\n      schema:\r\n        openAPIV3Schema:\r\n          type: object\r\n          properties:\r\n            spec:\r\n              type: object\r\n              properties:\r\n                myfield:\r\n                  type: integer\r\n    - name: v2\r\n      served: true\r\n      storage: false\r\n      schema:\r\n        openAPIV3Schema:\r\n          type: object\r\n          properties:\r\n            spec:\r\n              type: object\r\n              properties:\r\n                myfield:\r\n                  type: integer\r\n  scope: Namespaced\r\n  names:\r\n    plural: replicant\r\n    singular: replicants\r\n    kind: Replicant\r\n  conversion:\r\n    strategy: Webhook\r\n    webhook:\r\n      conversionReviewVersions: [v1]\r\n      clientConfig:\r\n        service:\r\n          namespace: default\r\n          name: example-conversion-webhook-server\r\n          path: \/convert\r\n        caBundle: \"Cg==\"\r\n```\r\n\r\n```\r\n$ kubectl apply -f crd.yaml\r\ncustomresourcedefinition.apiextensions.k8s.io\/replicant.stable.example.com created\r\n```\r\n\r\nRead the CRD back:\r\n\r\n```\r\n$ kubectl get crd replicant.stable.example.com -oyaml\r\napiVersion: apiextensions.k8s.io\/v1\r\nkind: CustomResourceDefinition\r\n...\r\n```\r\n\r\nAttempt to create a CR using the storage version:\r\n\r\n```yaml\r\napiVersion: stable.example.com\/v1\r\nkind: Replicant\r\nmetadata:\r\n  name: cr1\r\nspec:\r\n  myfield: 1\r\n```\r\n\r\n```\r\n$ kubectl apply cr1.yaml\r\nError from server (InternalError): error when retrieving current configuration of:\r\nResource: \"stable.example.com\/v1, Resource=replicant\", GroupVersionKind: \"stable.example.com\/v1, Kind=Replicant\"\r\nName: \"cr1\", Namespace: \"default\"\r\nfrom server for: \"cr-1.yaml\": Internal error occurred: error resolving resource\r\n\r\n$ grep \"customresource_handler\" \/tmp\/local-kube-apiserver.log\r\nE0308 12:57:41.892888 1158130 customresource_handler.go:301] unable to load root certificates: unable to parse bytes as PEM block\r\n\r\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nAll babsically. master...{introduction of CRD conversion}\r\n\r\n\r\n\n\n### Cloud provider\n\nAll\n\n### OS version\n\nAll\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","> An error response when attempting to create\/update the CRD with an invalid CABundle\r\n\r\nI would have expected this... are we not parsing the CA bundle in CRD validation? We should do this in a ratcheting way","> > An error response when attempting to create\/update the CRD with an invalid CABundle\n> \n> \n> \n> I would have expected this... are we not parsing the CA bundle in CRD validation? We should do this in a ratcheting way\n\nWe are not. I'm okay with ratcheting validation.","\/sig api-machinery"],"labels":["kind\/bug","sig\/api-machinery","needs-triage"]},{"title":"Nodelifecycle: Emit event when deletion failed","body":"The nodelifecycle controller emits an event before it deletes a node. Failures doing so for example due to a webhook are pretty hidden though, as they are only logged in the controller-manager.\r\n\r\nThis change makes us emit an event for failing to delete a node including the error as well.\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\/sig cloud-provider\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\n\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123834#\" title=\"Author self-approved\">alvaroaleman<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [wlan0](https:\/\/github.com\/wlan0) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/cloud-provider\/controllers\/nodelifecycle\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/controllers\/nodelifecycle\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"wlan0\"]} -->","\/retest-required","\/retest"],"labels":["kind\/cleanup","area\/cloudprovider","size\/XS","release-note-none","cncf-cla: yes","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"Add integration tests for CEL logic around email verified","body":"Could you add [integrations tests](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/oidc\/oidc_test.go#L561) using `claims.email_verified` in\r\n1. username.expression\r\n2. extra[*].valueExpression\r\n3. claimValidationRules[*].expression\r\n\r\n_Originally posted by @aramase in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123737#discussion_r1516908029_\r\n            ","comments":["\/sig auth\r\n\/triage accepted"],"labels":["sig\/auth","triage\/accepted"]},{"title":"Add PrivateEndpoint option to APIServerTracing configuration","body":"### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdd an option, PrivateEndpoint for the APIServer's tracing configuration.  When enabled, the APIServer will use the context from incoming requests to make sampling decisions, and for trace context propagation.\r\n\r\nAlternative to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123807\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103186\r\n\r\n#### Special notes for your reviewer:\r\n\r\nSee discussions in:\r\n* https:\/\/github.com\/kubernetes\/kubernetes\/pull\/94942#discussion_r657114027\r\n* https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103186#issuecomment-1970288470\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nAdd PrivateEndpoint option to APIServerTracing configuration to use the trace context from apiserver requests.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-instrumentation\/0034-distributed-tracing-kep.md\r\n```\r\n\r\n@kubernetes\/sig-instrumentation-approvers\r\n@liggitt ","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig instrumentation","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123824#\" title=\"Author self-approved\">dashpole<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz), [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/apis\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\",\"liggitt\"]} -->"],"labels":["area\/test","area\/apiserver","sig\/api-machinery","release-note","size\/L","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/instrumentation","sig\/testing","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"Support for Topology Aware Routing in winkernel proxier","body":"### What would you like to be added?\r\n\r\nTopology Aware Routing was introduced in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/99522 and at the time of introduction it was introduced for `iptable` and `ipvs` proxiers. The feature is in beta since 1.23 but support is missing for Windows worker nodes.\r\n\r\n### Why is this needed?\r\n\r\nWindows worker nodes are supported by various distributions and cloud providers. A lot of them already automatically set `topology.kubernetes.io\/zone` based on the service provider's availability zone information for these nodes. if you have a multi-zone environment with Windows worker nodes, it is not possible to benefit from Topology Aware Routing, therefore all the benefits highlighted in https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-network\/2433-topology-aware-hints are missed. If the cluster has active Linux workloads in these zones, interacting with these services through Windows workload may actually turn out to be costly - both performance & bandwidth wise.\r\n\r\nIt looks like there may not be a technical blockers (HNS, for eg) to have this. Today all the endpoint registration logic in winkernel proxier resides in `syncProxyRules` where iTP, eTP policies are evaluated - this diverges from iptables, ipvs proxiers that have evolved to use `CategorizeEndpoints` from `topology.go` for this purpose, which also handles selection of endpoints matching node's topology zone. \r\n\r\nIf there are technical reasons for this not being supported, the lack of support should be documented and tracked.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig windows\r\n\/area kube-proxy","@daschott @princepereira Do you know any blockers for support this?"],"labels":["area\/kube-proxy","kind\/feature","sig\/windows","needs-triage"]},{"title":"Fix field path on CustomResource status update validations","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/119340 was merged in for 1.28 and adjusted validation of custom resources, in particular, it adjusted the the way that status sub resources were validated.\r\n\r\nI'm observing is that on 1.27, prior to this PR, if I have a validation failure I get a message such as `Invalid Value: status.myStruct.myField: ...`, but, in 1.28, the same inputs result in the message changing to `Invalid Value: myStruct.myField: ...` , we have dropped the `status.` prefix from the path.\r\n\r\nThis appears to be because the initial `fldPath` passed into the validation is nil in the PR. Passing in an existing path of `status` fixes the issue and reverts back to the original behaviour.\r\n\r\nCC @sttts @deads2k\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFixes a missing `status.` prefix on custom resource validation error messages.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["LGTM label has been added.  <details>Git tree hash: 24d0ed1592d001d1bcd985ce129980a7e7ab3d57<\/details>","\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: 6b83697704ea454b81d8ed1b0e06632c15c0e266<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123822#\" title=\"Author self-approved\">JoelSpeed<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123822#pullrequestreview-1931634887\" title=\"Approved\">roycaihw<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123822#issuecomment-1992354845\" title=\"Approved\">sttts<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [sttts]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/triage accepted"],"labels":["kind\/bug","lgtm","sig\/api-machinery","release-note","size\/XS","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"fix cpu app container must use same CPU NUMA as initContainer","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123816\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNow app container must allocate the same CPU as initContainer;  This change will allow app container use other NUMA;\r\nFor example, there is two NUMA node both has 40 Core CPU; NUMA 0 used 10Core, left 30C free, NUMA 1 is empty and 40C free;\r\nWhen create a pod having initContainer with 1C  and app container with 32C:\r\n1. initContainer will use NUMA 0 , and mark the CPU as reusableCPU;\r\n3. app container will failed, because in the topology caculation, NUMA 1 is passed by reusableCPU's NUMA; NUMA 0 will is passed by resource not enough;\r\n\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: xiaozhouX  (c017f96fb65c92def5d37fc7c814ce036a300623, a3c84015fa40b3881c8d2f9f25b8a4c857c04919)<\/li><\/ul>","Welcome @AllenXu93! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @AllenXu93. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123820#\" title=\"Author self-approved\">AllenXu93<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [derekwaynecarr](https:\/\/github.com\/derekwaynecarr) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cm\/cpumanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/cpumanager\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"derekwaynecarr\"]} -->","\/ok-to-test\r\n\/triage accepted\r\n\/priority important-longterm\r\n\/cc","at glance (need to spend time doing a proper review) this fix looks ok for the 2-NUMA-nodes cases, but we need to check it's correct also for the 4-NUMA-nodes and 8-NUMA nodes cases","> at glance (need to spend time doing a proper review) this fix looks ok for the 2-NUMA-nodes cases, but we need to check it's correct also for the 4-NUMA-nodes and 8-NUMA nodes cases\r\n\r\nThis bug has no relationship with NUMA numbers, it will appear if there is at least two NUMA and topology policy is `single-numa-node`:\r\n1. init-container allocate on one NUMA\r\n2. The app container will must allocate on the CPU that have same NUMA node as init-container.\r\n3. If init-container's CPU NUMA have not enough CPU for app container, pod will failed with `TopologyAffinityError `","> > at glance (need to spend time doing a proper review) this fix looks ok for the 2-NUMA-nodes cases, but we need to check it's correct also for the 4-NUMA-nodes and 8-NUMA nodes cases\r\n> \r\n> This bug has no relationship with NUMA numbers, it will appear if there is at least two NUMA and topology policy is `single-numa-node`:\r\n> \r\n>     1. init-container allocate on one NUMA\r\n> \r\n>     2. The app container will must allocate on the CPU that have same NUMA node as init-container.\r\n> \r\n>     3. If init-container's CPU NUMA have not enough CPU for app container, pod will failed with `TopologyAffinityError `\r\n\r\nSure, that's clear. What we need to verify is that the proposed way to compute the topology hints causes regressions on more complex scenarios with 4 or 8 NUMA nodes. Let's make sure we have enough test coverage.","> > > at glance (need to spend time doing a proper review) this fix looks ok for the 2-NUMA-nodes cases, but we need to check it's correct also for the 4-NUMA-nodes and 8-NUMA nodes cases\r\n> > \r\n> > \r\n> > This bug has no relationship with NUMA numbers, it will appear if there is at least two NUMA and topology policy is `single-numa-node`:\r\n> > ```\r\n> > 1. init-container allocate on one NUMA\r\n> > \r\n> > 2. The app container will must allocate on the CPU that have same NUMA node as init-container.\r\n> > \r\n> > 3. If init-container's CPU NUMA have not enough CPU for app container, pod will failed with `TopologyAffinityError `\r\n> > ```\r\n> \r\n> Sure, that's clear. What we need to verify is that the proposed way to compute the topology hints causes regressions on more complex scenarios with 4 or 8 NUMA nodes. Let's make sure we have enough test coverage.\r\n\r\nOK, I will add some unit test for reuse CPU scenarios.","\/test pull-kubernetes-e2e-kind-ipv6","\/retest","I think this PR can fix https:\/\/github.com\/kubernetes\/kubernetes\/issues\/94220"],"labels":["kind\/bug","area\/kubelet","sig\/node","release-note","size\/M","cncf-cla: yes","priority\/important-longterm","ok-to-test","triage\/accepted"]},{"title":"fix: handle socket file detection on Windows ","body":"Update socket file detection logic to use os.Stat as per upstream Go fix for https:\/\/github.com\/golang\/go\/issues\/33357. This resolves the issue where socket files could not be properly identified on Windows systems.\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\nUpdate to use os.Stat instead of os.Lstat when deleting the device-plugins socket file, rather than deleting all files in the \/var\/lib\/kubelet\/device-plugins\/ directory.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nrelated issue: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/97554\r\nFixes #\r\n\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: fakecore \/ (4060ee60c1d2e5ba1fba1f8729adfc211cee1b6f)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @fakecore! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @fakecore. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123819#\" title=\"Author self-approved\">fakecore<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [random-liu](https:\/\/github.com\/random-liu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"random-liu\"]} -->","\/check-required-labels","\/ok-to-test","\/retest","@fakecore I don't see commit https:\/\/github.com\/golang\/go\/commit\/628b1015b972eabcc0a678ab69a74601239c40a4 cherry-picked to any of the current Go release branches.\r\nI think merging this PR should wait until Kubernetes is built using a Go version containing the above mentioned commit.","> @fakecore I don't see commit [golang\/go@628b101](https:\/\/github.com\/golang\/go\/commit\/628b1015b972eabcc0a678ab69a74601239c40a4) cherry-picked to any of the current Go release branches. I think merging this PR should wait until Kubernetes is built using a Go version containing the above mentioned commit.\r\n\r\nI agree. Looks like a nice fix, but we need to make sure the dependencies are satisfied. I'll check what's the best triage labeling for this kind of state. Holding meanwhile.","\/hold\r\n\r\nper https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123819#issuecomment-1987113850","@cblecker @hakman @ffromani \r\nThank you for the review. It was my mistake, I should have checked the release status of Go and the dependencies of Kubernetes. I will be more cautious next time.","> @cblecker @hakman @ffromani Thank you for the review. It was my mistake, I should have checked the release status of Go and the dependencies of Kubernetes. I will be more cautious next time.\r\n\r\nIt's fine, no problem at all! It's just this PR unfortunately will have to wait for a while before it can be merged. Once the golang dep is updated, it should go in smoothly though."],"labels":["area\/kubelet","kind\/cleanup","sig\/node","size\/XS","release-note-none","cncf-cla: yes","do-not-merge\/hold","ok-to-test","needs-priority","needs-triage"]},{"title":"remove stale comment","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#70672 forgot to remove the stale comment\r\n\r\n\/cc @liggitt \r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123818#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [kow3ns](https:\/\/github.com\/kow3ns) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/integration\/deployment\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/deployment\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"kow3ns\"]} -->"],"labels":["area\/test","kind\/cleanup","size\/XS","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","needs-priority","needs-triage"]},{"title":"fix slow volume csi unit test","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nbefore:\r\n\r\n```\r\n(\u2388|kind-kind:N\/A)\u279c  kubernetes git:(master) go test   k8s.io\/kubernetes\/pkg\/volume\/csi -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/volume\/csi\t38.506s\tcoverage: 79.0% of statements\r\n```\r\n\r\nafter:\r\n\r\n```\r\n(\u2388|kind-kind:N\/A)\u279c  kubernetes git:(fix-slow-volume-csi-test) go test  k8s.io\/kubernetes\/pkg\/volume\/csi -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/volume\/csi\t18.857s\tcoverage: 79.0% of statements\r\n```\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nRef #123685\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123817#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"thockin\"]} -->","@carlory: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | bfdcb2af0957719573ff8e1d1b5311293affa7a6 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123817\/pull-kubernetes-unit\/1766015656433553408) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123817). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Acarlory). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["kind\/cleanup","sig\/storage","size\/M","release-note-none","cncf-cla: yes","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"Pod Failed with TopologyAffinityError because of init-container CPU NUMA Topology BUG","body":"### What happened?\n\nMy Node is 250Gi memory and 64 Core CPU, which has two NUMA node, both is 125Gi and 32 Core CPU, topology policy is single-numa-node.\r\nCreate 2 pod,  with init-container 1 Core CPU and 1Gi memory, app container 26 Core and 32Gi memory.\r\nThe first pod create success.\r\nThe second pod will failed with TopologyAffinityError.\r\n\r\nResult from `lscpu`:\r\n```\r\nNUMA node0 CPU(s):     0-15,32-47\r\nNUMA node1 CPU(s):     16-31,48-63\r\n```\r\n\r\nIn `\/var\/lib\/kubelet\/cpu_manager_state` file: \r\n```\r\n{\r\n  \"policyName\": \"static\",\r\n  \"defaultCpuSet\": \"1,3,15-31,47-63\",\r\n  \"entries\": {\r\n    \"69aa6124-4e0e-4c66-94ea-feae7df25b68\": {  # the second Pod, init-container use NUMA 0\r\n      \"init-container\": \"35\"\r\n    },\r\n    \"e49ac32c-22a3-4c8c-8be3-721bd1fd9eaa\": {   # the first Pod, use NUMA 0\r\n      \"centos\": \"0,2,4-14,32,34,36-46\",\r\n      \"init-container\": \"33\"\r\n    }\r\n  },\r\n  \"checksum\": 3748757820\r\n}\r\n```\r\n\r\n\n\n### What did you expect to happen?\n\nBoth two pods can be created successfully.\r\nFirst pod use NUMA 0;\r\nThe sceond pod init-container can use either NUMA 0 or NUMA 1, but app container use NUMA 1;\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create one pod, which use CPU, for example 10 of 32(Numa Node) CPU of the node\r\n2. Create another pod, which need init-container 1Core CPU, and app container need most of NUMA Node CPU,  like 25 Core (greater than the left of the first NUMA)\r\n3.  The second pod will failed with TopologyAffinityError.\n\n### Anything else we need to know?\n\nI think the bug is in `generateCPUTopologyHints ` function:  https:\/\/github.com\/kubernetes\/kubernetes\/blob\/7ea3d0245a63fbbba698f1cb939831fe8143db3e\/pkg\/kubelet\/cm\/cpumanager\/policy_static.go#L623 \r\n1. when allocate init-container CPU, CPUManager will record as cpuToReuse for pod. https:\/\/github.com\/kubernetes\/kubernetes\/blob\/7ea3d0245a63fbbba698f1cb939831fe8143db3e\/pkg\/kubelet\/cm\/cpumanager\/policy_static.go#L345  \r\n2. before call `generateCPUTopologyHints`, get cpuToReuse of the pod, as reusable https:\/\/github.com\/kubernetes\/kubernetes\/blob\/7ea3d0245a63fbbba698f1cb939831fe8143db3e\/pkg\/kubelet\/cm\/cpumanager\/policy_static.go#L587\r\n3. when caculate app container CPU, CPUManager will filter the NUMA Node which is not equal to reusableCPU's NUMA Node\r\n4. Even if NUMA 1 is the only one that fits CPU resource acquirement, it still passed in calculating TopologyHint; NUMA 0 fits with reusableCPU, but CPU left is not enough. \n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:47:36Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.17\", GitCommit:\"a7736eaf34d823d7652415337ac0ad06db9167fc\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T11:42:04Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig node","\/triage accepted\r\n\r\nyes, this is a bug which leads to TopologyAffinityError which can be avoided.","\/cc","\/priority important-longterm"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","triage\/accepted"]},{"title":"Default ipFamilyPolicy value","body":"### What would you like to be added?\n\nMake the default Service `ipFamilyPolicy` value configurable in `apiserver` or set it as `PreferDualStack` by default.\n\n### Why is this needed?\n\nAlmost all public Helm charts don't provide a value to configure Service `ipFamilyPolicy`, so I would like to set it to `PreferDualStack` by default, especially for `kubernetes` default service.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig network","> Almost all public Helm charts don't provide a value to configure Service `ipFamilyPolicy`, so I would like to set it to `PreferDualStack` by default, \r\n\r\nI don't think Kubernetes should compensate for that problem, have you tried to open issues to the corresponding charts to set that value? or maybe a webhook?\r\n\r\n> especially for `kubernetes` default service.\r\n\r\nThe kubernetes default service is more involved than that , long history here https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-network\/2438-dual-stack-apiserver\r\n\r\n","Um, when does it matter? Really?"],"labels":["sig\/network","kind\/feature","needs-triage"]},{"title":"fix stateful set pod recreation and event spam","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n- do not emit events when pod reaches terminal phase\r\n- do not try to recreate pod until the old pod has been removed from\r\n  etcd storage\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #122709\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nEmition of RecreatingFailedPod and RecreatingTerminatedPod events has been removed from stateful set lifecycle.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/triage accepted\r\n\/priority important-soon\r\n\/assign @soltysh","LGTM label has been added.  <details>Git tree hash: 7aa2668daacc5ea3b06841104fe670296e1a2ed9<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123809#\" title=\"Author self-approved\">atiratree<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123809#pullrequestreview-1939351116\" title=\"Approved\">soltysh<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/controller\/statefulset\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/statefulset\/OWNERS)~~ [soltysh]\n- ~~[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)~~ [soltysh]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","unrelated\r\n\/test pull-kubernetes-e2e-gce"],"labels":["kind\/bug","area\/test","priority\/important-soon","lgtm","release-note","size\/L","sig\/apps","approved","cncf-cla: yes","sig\/testing","tide\/merge-method-squash","area\/e2e-test-framework","triage\/accepted"]},{"title":"Fix kube-aggregator support of v1.Service ExternalName","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nCurrently when [kube-aggregator's handler_proxy](https:\/\/github.com\/kubernetes\/kube-aggregator\/blob\/0484f16783b2442a3739ebdd1bbfa0d194719823\/pkg\/apiserver\/handler_proxy.go#L135) attempts to resolve a `v1.Service` with a type of `ExternalName`, the request will fail due to a TLS error unless the external name matches the [value set here](https:\/\/github.com\/kubernetes\/kube-aggregator\/blob\/0484f16783b2442a3739ebdd1bbfa0d194719823\/pkg\/apiserver\/handler_proxy.go#L209). This forces the use of workarounds like setting `insecureSkipTLSVerify: true` in the `APIService`, which is obviously not ideal.\r\n\r\nThe use of extension API servers that are external to the cluster seems to be a supported use case (although not the common approach) based on [this section of the docs](https:\/\/kubernetes.io\/docs\/concepts\/extend-kubernetes\/api-extension\/apiserver-aggregation\/#aggregation-layer):\r\n\r\n> The most common way to implement the APIService is to run an extension API server in Pod(s) that run in your cluster.\r\n\r\nThis PR adds `ServiceHostnameResolver`, which is an additional interface a `ServiceResolver` can optionally implement. This allows the TLS config's `ServerName` property to be properly set when `ServiceHostnameResolver` is implemented and returns a valid hostname.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123571\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: toddtreece \/ name: Todd Treece  (ca69391040d87a2dea3e9c6385a7e4701edf10a8, 33146558bf6cd92e8ce58ce72171f763969046bc, d1dadd4f748ce8f8432765b46c1d9137ffd5e5c8, 42eb7a7bfcfe2a83c9e848630e0df70f1af0fb40, 310f43410833b5d920cdbf43a883e17dc3c98536, 632ac18f328d7be5498b99ad29489e15aea5c796)<\/li><\/ul>","Welcome @toddtreece! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @toddtreece. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","\/ok-to-test\r\n\/assign @deads2k\r\nFor aggregtor. Thank you.\r\n\/triage accepted","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123808#\" title=\"Author self-approved\">toddtreece<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [deads2k](https:\/\/github.com\/deads2k). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","@toddtreece: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-integration | 42eb7a7bfcfe2a83c9e848630e0df70f1af0fb40 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123808\/pull-kubernetes-integration\/1770832626119086080) | true | `\/test pull-kubernetes-integration`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123808). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Atoddtreece). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["kind\/bug","area\/apiserver","sig\/api-machinery","size\/L","kind\/api-change","cncf-cla: yes","do-not-merge\/release-note-label-needed","ok-to-test","needs-priority","triage\/accepted"]},{"title":"APIServerTracing: Respect trace context only for system:master","body":"### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nMove the tracing filter to after the authorization filter, and only respect the incoming context for requests from the privileged `system:master` group.\r\n\r\nThis has some downsides:\r\n\r\n* Tracing from authorization isn't part of the APIServer trace\r\n* The start time of the trace will be after authorization\r\n* Exemplars won't work, since the trace filter comes after the metric filter.\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103186\r\n\r\n#### Special notes for your reviewer:\r\n\r\nSee discussions in:\r\n* https:\/\/github.com\/kubernetes\/kubernetes\/pull\/94942#discussion_r657114027\r\n* https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103186#issuecomment-1970288470\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nRespect the incoming trace context for authenticated requests to the kube-apiserver for APIServer tracing.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-instrumentation\/0034-distributed-tracing-kep.md\r\n```\r\n\r\n@kubernetes\/sig-instrumentation-approvers\r\n@liggitt ","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123807#\" title=\"Author self-approved\">dashpole<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","@liggitt this now only trusts the trace context for requests for the `system:master` group.","cc @richabanker "],"labels":["kind\/bug","area\/test","area\/apiserver","sig\/api-machinery","release-note","size\/L","cncf-cla: yes","sig\/testing","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"[Flaking] [kind-e2e-parallel] [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota","body":"### Which jobs are flaking?\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-kind-e2e-parallel\/1764798590191931392\r\n\r\n### Which tests are flaking?\r\n\r\nKubernetes e2e suite.[It] [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota\r\n\r\n### Since when has it been flaking?\r\n\r\n[https:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=should%20surface%20a%20failure[\u2026]ition%20on%20a%20common%20issue%20like%20exceeded%20quota](https:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=should%20surface%20a%20failure%20condition%20on%20a%20common%20issue%20like%20exceeded%20quota) shows it flakes several times. The flake rate is not high though\r\n\r\n### Testgrid link\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#kind-master-parallel\r\n\r\n### Reason for failure (if possible)\r\n\r\n```\r\n   I0305 00:05:23.622506 67387 replica_set.go:286] Unexpected error: \r\n      <*errors.errorString | 0xc000bc93c0>: \r\n      rs controller never added the failure condition for replica set \"condition-test\": []v1.ReplicaSetCondition(nil)\r\n      {\r\n          s: \"rs controller never added the failure condition for replica set \\\"condition-test\\\": []v1.ReplicaSetCondition(nil)\",\r\n      } \r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Relevant SIG(s)\r\n\r\n\/sig apps","comments":["\/cc @kubernetes\/release-team-release-signal","audit events from https:\/\/gcsweb.k8s.io\/gcs\/kubernetes-jenkins\/logs\/ci-cos-containerd-e2e-ubuntu-gce\/1760837621589741568\/artifacts\/bootstrap-e2e-master\/:\r\n\r\n```\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"ca1eff84-5dfa-465a-b8d1-40afd6808d31\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\",\"verb\":\"create\",\"user\":{\"username\":\"kubecfg\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"34.123.68.233\"],\"userAgent\":\"e2e.test\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e -- [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"creationTimestamp\":null},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29800\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.072067Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.075529Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"050d4c0f-4d58-4adb-a607-72c7a58a263a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29800\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29800\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29801\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.076774Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.080857Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"00c67c33-e72d-4c77-83a9-edde73bd01fc\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\",\"verb\":\"create\",\"user\":{\"username\":\"kubecfg\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"34.123.68.233\"],\"userAgent\":\"e2e.test\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e -- [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29812\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.167180Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.174247Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"731d4b0e-e8b7-4399-a5c6-0c37719e6a0b\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29801\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29801\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.194242Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.201151Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"1b093ccc-f017-47b6-b843-782beff190c4\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29816\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29819\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.204848Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.219042Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"a3844f3e-ee2f-4982-9958-3e6c9f8d4715\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29816\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.214942Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.234133Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b261b94b-5d3b-45b2-9a40-f99ceb909524\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29819\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29819\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29825\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.237573Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.244588Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"f63d18b6-c926-42ae-8820-a84d0c204c7c\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29825\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29825\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.245730Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.251169Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"dca1ef4e-4df7-4019-b3f1-8e69af618471\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29812\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29812\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29831\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:observedGeneration\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.272376Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.282369Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"1cc9057f-b305-48dc-8963-14099b4e9ca6\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29831\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29831\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29835\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.287725Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.297890Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b4507774-c72d-4f8b-b2be-b7eac887b12a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29835\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29835\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29898\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.235799Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.239876Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"51fee89f-53a7-49d8-a576-5ae6c397149d\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29898\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29898\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29903\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.417395Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.432982Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"cb0f9fc4-71b8-44ca-b275-3a409afaf32e\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29903\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29903\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":3,\"availableReplicas\":3,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29913\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":3,\"availableReplicas\":3,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.935552Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.940312Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"f02736e0-2576-4ec7-a1f4-a57db8a4ef03\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29913\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29913\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":2,\"fullyLabeledReplicas\":2,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":2,\"fullyLabeledReplicas\":2,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.255884Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.266189Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"c305b3ec-f263-4f42-8b75-e1e31aea3748\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29827\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32475\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.276465Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.284868Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"0a14d34f-9951-4b31-b44a-0910892fb8c6\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32471\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.276296Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.285851Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"79c344b9-c182-48e7-b30e-36746d50b889\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32471\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on replicasets.apps \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"group\":\"apps\",\"kind\":\"replicasets\"},\"code\":409},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on replicasets.apps \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"group\":\"apps\",\"kind\":\"replicasets\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.288038Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.295107Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"8c4188c7-6983-4ff5-a312-9e4e0491c68a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29827\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.288891Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.295446Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"718af4ee-f102-4744-b982-ee77bc443a8a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32475\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32475\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32478\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.298716Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.306941Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b07b28f9-1eb6-4761-acae-a1b7e4ccd02e\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32476\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.303337Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.308146Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"17699e6e-d7d5-4103-882f-17c8efd41e48\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32476\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32481\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:observedGeneration\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.314063Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.322841Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n```\r\n\r\nIt seems a bug.\r\n\r\n\/sig apimachinery\r\n\/sig apps","@carlory: The label(s) `sig\/apimachinery` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123806#issuecomment-1985006013):\n\n>audit events from https:\/\/gcsweb.k8s.io\/gcs\/kubernetes-jenkins\/logs\/ci-cos-containerd-e2e-ubuntu-gce\/1760837621589741568\/artifacts\/bootstrap-e2e-master\/:\r\n>\r\n>```\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"ca1eff84-5dfa-465a-b8d1-40afd6808d31\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\",\"verb\":\"create\",\"user\":{\"username\":\"kubecfg\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"34.123.68.233\"],\"userAgent\":\"e2e.test\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e -- [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"creationTimestamp\":null},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29800\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.072067Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.075529Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"050d4c0f-4d58-4adb-a607-72c7a58a263a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29800\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29800\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29801\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.076774Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.080857Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"00c67c33-e72d-4c77-83a9-edde73bd01fc\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\",\"verb\":\"create\",\"user\":{\"username\":\"kubecfg\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"34.123.68.233\"],\"userAgent\":\"e2e.test\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e -- [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29812\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.167180Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.174247Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"731d4b0e-e8b7-4399-a5c6-0c37719e6a0b\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29801\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29801\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.194242Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.201151Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"1b093ccc-f017-47b6-b843-782beff190c4\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29816\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29819\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.204848Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.219042Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"a3844f3e-ee2f-4982-9958-3e6c9f8d4715\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29816\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29816\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.214942Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.234133Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b261b94b-5d3b-45b2-9a40-f99ceb909524\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29819\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29819\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29825\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.237573Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.244588Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"f63d18b6-c926-42ae-8820-a84d0c204c7c\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:apiserver\",\"uid\":\"c44b5c74-3275-4939-aad5-05a79c5b59fc\",\"groups\":[\"system:masters\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-apiserver\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29825\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29825\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-apiserver\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:used\":{\"f:pods\":{}}}},\"subresource\":\"status\"},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"2\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.245730Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.251169Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"dca1ef4e-4df7-4019-b3f1-8e69af618471\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29812\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29812\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29831\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:observedGeneration\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.272376Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.282369Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"1cc9057f-b305-48dc-8963-14099b4e9ca6\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29831\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29831\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29835\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:57.287725Z\",\"stageTimestamp\":\"2024-02-23T01:38:57.297890Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b4507774-c72d-4f8b-b2be-b7eac887b12a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29835\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29835\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29898\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.235799Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.239876Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"51fee89f-53a7-49d8-a576-5ae6c397149d\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29898\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29898\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29903\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.417395Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.432982Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"cb0f9fc4-71b8-44ca-b275-3a409afaf32e\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29903\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29903\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":3,\"availableReplicas\":3,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29913\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:58Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":3,\"fullyLabeledReplicas\":3,\"readyReplicas\":3,\"availableReplicas\":3,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:38:58.935552Z\",\"stageTimestamp\":\"2024-02-23T01:38:58.940312Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"f02736e0-2576-4ec7-a1f4-a57db8a4ef03\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29913\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"29913\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":2,\"fullyLabeledReplicas\":2,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":2,\"fullyLabeledReplicas\":2,\"readyReplicas\":2,\"availableReplicas\":2,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.255884Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.266189Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"c305b3ec-f263-4f42-8b75-e1e31aea3748\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29827\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32475\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.276465Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.284868Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"0a14d34f-9951-4b31-b44a-0910892fb8c6\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32471\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.276296Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.285851Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"79c344b9-c182-48e7-b30e-36746d50b889\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32471\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on replicasets.apps \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"group\":\"apps\",\"kind\":\"replicasets\"},\"code\":409},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32471\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on replicasets.apps \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"group\":\"apps\",\"kind\":\"replicasets\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.288038Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.295107Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"8c4188c7-6983-4ff5-a312-9e4e0491c68a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"29827\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"29827\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"1\"}}},\"responseObject\":{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Operation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.288891Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.295446Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"718af4ee-f102-4744-b982-ee77bc443a8a\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/api\/v1\/namespaces\/replicaset-7849\/resourcequotas\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:resourcequota-controller\",\"uid\":\"6b16b3ec-8dea-4c0c-8a72-02ae4429c44b\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:resourcequota-controller\",\"objectRef\":{\"resource\":\"resourcequotas\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32475\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32475\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\"},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"responseObject\":{\"kind\":\"ResourceQuota\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"d99683b0-5cfa-4981-9ef6-6c756b97b012\",\"resourceVersion\":\"32478\",\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:spec\":{\"f:hard\":{\".\":{},\"f:pods\":{}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:hard\":{\".\":{},\"f:pods\":{}},\"f:used\":{\".\":{},\"f:pods\":{}}}},\"subresource\":\"status\"}]},\"spec\":{\"hard\":{\"pods\":\"2\"}},\"status\":{\"hard\":{\"pods\":\"2\"},\"used\":{\"pods\":\"0\"}}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.298716Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.306941Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:resourcequota-controller\\\" of ClusterRole \\\"system:controller:resourcequota-controller\\\" to ServiceAccount \\\"resourcequota-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"b07b28f9-1eb6-4761-acae-a1b7e4ccd02e\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32476\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:availableReplicas\":{},\"f:fullyLabeledReplicas\":{},\"f:observedGeneration\":{},\"f:readyReplicas\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":1,\"fullyLabeledReplicas\":1,\"readyReplicas\":1,\"availableReplicas\":1,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.303337Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.308146Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io\/v1\",\"level\":\"RequestResponse\",\"auditID\":\"17699e6e-d7d5-4103-882f-17c8efd41e48\",\"stage\":\"ResponseComplete\",\"requestURI\":\"\/apis\/apps\/v1\/namespaces\/replicaset-7849\/replicasets\/condition-test\/status\",\"verb\":\"update\",\"user\":{\"username\":\"system:serviceaccount:kube-system:replicaset-controller\",\"uid\":\"5f8d6a93-7d89-4283-b610-d244f12a4b39\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\"]},\"sourceIPs\":[\"::1\"],\"userAgent\":\"kube-controller-manager\/v1.30.0 (linux\/amd64) kubernetes\/9fa043e\/system:serviceaccount:kube-system:replicaset-controller\",\"objectRef\":{\"resource\":\"replicasets\",\"namespace\":\"replicaset-7849\",\"name\":\"condition-test\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"apiGroup\":\"apps\",\"apiVersion\":\"v1\",\"resourceVersion\":\"32476\",\"subresource\":\"status\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32476\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"responseObject\":{\"kind\":\"ReplicaSet\",\"apiVersion\":\"apps\/v1\",\"metadata\":{\"name\":\"condition-test\",\"namespace\":\"replicaset-7849\",\"uid\":\"316dd66f-eb13-4f9c-a686-f8680f41eee8\",\"resourceVersion\":\"32481\",\"generation\":1,\"creationTimestamp\":\"2024-02-23T01:38:57Z\",\"labels\":{\"name\":\"condition-test\"},\"managedFields\":[{\"manager\":\"e2e.test\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:38:57Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:replicas\":{},\"f:selector\":{},\"f:template\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:name\":{}}},\"f:spec\":{\"f:containers\":{\"k:{\\\"name\\\":\\\"httpd\\\"}\":{\".\":{},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{}}}}}},{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"apps\/v1\",\"time\":\"2024-02-23T01:40:05Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:status\":{\"f:observedGeneration\":{},\"f:replicas\":{}}},\"subresource\":\"status\"}]},\"spec\":{\"replicas\":3,\"selector\":{\"matchLabels\":{\"name\":\"condition-test\"}},\"template\":{\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"name\":\"condition-test\"}},\"spec\":{\"containers\":[{\"name\":\"httpd\",\"image\":\"registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4\",\"resources\":{},\"terminationMessagePath\":\"\/dev\/termination-log\",\"terminationMessagePolicy\":\"File\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":0,\"dnsPolicy\":\"ClusterFirst\",\"securityContext\":{},\"schedulerName\":\"default-scheduler\"}}},\"status\":{\"replicas\":0,\"observedGeneration\":1}},\"requestReceivedTimestamp\":\"2024-02-23T01:40:05.314063Z\",\"stageTimestamp\":\"2024-02-23T01:40:05.322841Z\",\"annotations\":{\"authorization.k8s.io\/decision\":\"allow\",\"authorization.k8s.io\/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"system:controller:replicaset-controller\\\" of ClusterRole \\\"system:controller:replicaset-controller\\\" to ServiceAccount \\\"replicaset-controller\/kube-system\\\"\"}}\r\n>```\r\n>\r\n>It seems a bug.\r\n>\r\n>\/sig apimachinery\r\n>\/sig apps\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig api-machinery","The quota hard is 2, but created pod number is 3. \r\nThere's conflict in kube-apiserver. \r\n\r\n```\r\nOperation cannot be fulfilled on resourcequotas \\\"condition-test\\\": the object has been modified; please apply your changes to the latest version and try again\",\"reason\":\"Conflict\",\"details\":{\"name\":\"condition-test\",\"kind\":\"resourcequotas\"},\"code\":409}\r\n```","> The quota hard is 2, but created pod number is 3.\r\n> There's conflict in kube-apiserver.\r\n\r\nIs this a bug?","Hi folks,\r\nI'm interested in working on this. Can anyone assign me? \r\nThanks.","\/assign @flavianmissi","\/triage accepted","I started looking into this today, here's a short update of the little progress I made thus far:\r\n* spent some time understanding the test which is the source of the failure: https:\/\/github.com\/openshift\/kubernetes\/blob\/master\/test\/e2e\/apps\/replica_set.go?plain=1#L284\r\n* built and run the tests locally (here I struggled a little with ginkgo, but I think I got it now)\r\n* trying (and failing) to reproduce the failure by running the test repeatedly in isolation using `GINKGO_UNTIL_IT_FAILS=true` (left it running for over an hour without any luck)\r\n* changed approach to run everything under `test\/e2e\/apps` in parallel instead, still using `GINKGO_UNTIL_IT_FAILS=true`\r\nThe flake is probably caused by a race condition, but more investigation is required.\r\n\r\nI'll keep at it.","Small update that I won't be working on this during this week - will be back at it normally starting next Monday.\r\nIn the meanwhile, this is what I ended up with from last week, as shared with my coworkers internally.\r\n\r\nI've combed through kube-apiserver as well as kube-controller-manager logs and put what I found to be relevant on this gist: https:\/\/gist.github.com\/flavianmissi\/48e74926cbd48ff60736d933c7527ec0. The full logs can be found here: https:\/\/gcsweb.k8s.io\/gcs\/kubernetes-jenkins\/logs\/ci-cos-containerd-e2e-ubuntu-gce\/1760837621589741568\/artifacts\/bootstrap-e2e-master\/.\r\nI'm not sure if my interpretation of the kas logs is correct, but from what I see it looks like the LIST call to list resource quotas happens before (at 01:38:56.783880) the actual resource quota is created by the test (at 01:38:57.075670). Am I reading this right?"],"labels":["sig\/api-machinery","kind\/flake","sig\/apps","triage\/accepted"]},{"title":"DRA: E2E: check pod condition to detect when pod has been checked by the scheduler.","body":"### What would you like to be added?\n\nIn test\/e2e\/dra\/dra.go:\r\n```\r\n\/\/ There's no way to be sure that the scheduler has checked the pod.\r\n```\r\n\r\nAldo pointed out that:\r\n> There is. There should be a Schedulable condition set to false in the PodStatus.\r\n\r\nLet's use that instead of sleeping.\r\n\r\n\/sig node\r\n\/triage accepted\r\n\/priority long-term\r\n\n\n### Why is this needed?\n\nMore robust test.","comments":["@pohly: The label(s) `priority\/long-term` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123805):\n\n>### What would you like to be added?\n>\n>In test\/e2e\/dra\/dra.go:\r\n>```\r\n>\/\/ There's no way to be sure that the scheduler has checked the pod.\r\n>```\r\n>\r\n>Aldo pointed out that:\r\n>> There is. There should be a Schedulable condition set to false in the PodStatus.\r\n>\r\n>Let's use that instead of sleeping.\r\n>\r\n>\/sig node\r\n>\/triage accepted\r\n>\/priority long-term\r\n>\n>\n>### Why is this needed?\n>\n>More robust test.\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/node","kind\/feature","triage\/accepted"]},{"title":"Should we deprecate and remove the in-tree volume plugin hostpath dynamic provisioning feature?","body":"The in-tree volume plugin hostpath supports dynamic provisioning a volume for a claim when the kube-controller-manager starts with `--enable-hostpath-provisioner=true`.\r\n\r\nIt creates a local \/tmp\/%\/%s directory as a new PersistentVolume, default \/tmp\/hostpath_pv\/%s. It is meant for development and testing only and WILL NOT WORK in a multi-node cluster.\r\n\r\nThere are 3 problems I want to talk about:\r\n\r\n1. e2e tests don't have a test case for hostpath dynamic provisioning. https:\/\/github.com\/kubernetes\/kubernetes\/blob\/2ec63e0d28951bb525a5bce0d9459afa1c71c0bd\/test\/e2e\/storage\/drivers\/in_tree.go#L649\r\n    ```golang\r\n    var _ storageframework.TestDriver = &hostPathDriver{}\r\n    var _ storageframework.PreprovisionedVolumeTestDriver = &hostPathDriver{}\r\n    var _ storageframework.InlineVolumeTestDriver = &hostPathDriver{}\r\n    ```\r\n    the hostPathDriver doesn't implement the `DynamicPVTestDriver` interface, so the e2e tests don't have a test case for in-tree hostpath dynamic provisioning.\r\n2. The cluster created by kubeadm doesn't make the feature work as expected. because the kube-controller-manager pod doesn't have a hostPath volume mounted at \/tmp\/hostpath_pv. https:\/\/github.com\/kubernetes\/kubernetes\/blob\/2ec63e0d28951bb525a5bce0d9459afa1c71c0bd\/cmd\/kubeadm\/app\/phases\/controlplane\/volumes.go#L64\r\n    ```golang\r\n    \/\/ HostPath volumes for the controller manager\r\n    \/\/ Read-only mount for the certificates directory\r\n    \/\/ TODO: Always mount the K8s Certificates directory to a static path inside of the container\r\n    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, kubeadmconstants.KubeCertificatesVolumeName, cfg.CertificatesDir, cfg.CertificatesDir, true, &hostPathDirectoryOrCreate)\r\n    \/\/ Read-only mount for the ca certs (\/etc\/ssl\/certs) directory\r\n    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, caCertsVolumeName, caCertsVolumePath, caCertsVolumePath, true, &hostPathDirectoryOrCreate)\r\n    \/\/ Read-only mount for the controller manager kubeconfig file\r\n    controllerManagerKubeConfigFile := filepath.Join(kubeadmconstants.KubernetesDir, kubeadmconstants.ControllerManagerKubeConfigFileName)\r\n    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, kubeadmconstants.KubeConfigVolumeName, controllerManagerKubeConfigFile, controllerManagerKubeConfigFile, true, &hostPathFileOrCreate)\r\n    \/\/ Mount for the flexvolume directory (\/usr\/libexec\/kubernetes\/kubelet-plugins\/volume\/exec by default)\r\n    \/\/ Flexvolume dir must NOT be readonly as it is used for third-party plugins to integrate with their storage backends via unix domain socket.\r\n    flexvolumeDirVolumePath, idx := kubeadmapi.GetArgValue(cfg.ControllerManager.ExtraArgs, \"flex-volume-plugin-dir\", -1)\r\n    if idx == -1 {\r\n      flexvolumeDirVolumePath = defaultFlexvolumeDirVolumePath\r\n    }\r\n    mounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, flexvolumeDirVolumeName, flexvolumeDirVolumePath, flexvolumeDirVolumePath, false, &hostPathDirectoryOrCreate)\r\n    ```\r\n3. There are lots of projects using `--enable-hostpath-provisioner=true`. Please see https:\/\/cs.k8s.io\/?q=enable-hostpath-provisioner&i=nope&files=&excludeFiles=&repos=. But they use kubeadm to create a cluster, so the dynamic provisioning of in-tree hostpath is never used by those projects. I don't know why they enable the flag.\r\n\r\nThere are 3 in-tree plugins that support dynamic provisioning: hostpath, rbd (removed in 1.31), and portworxVolume (will be removed in a future release once its csi migration is completed). \r\n\r\nNow, the community has various CSI drivers users can use for dynamic provisioning. So let us deprecate and remove the in-tree volume plugin hostpath dynamic provisioning feature if the feature is never used.\r\n\r\nWhat do you think? \r\n\r\n\/cc @xing-yang @jsafrane @pacoxu @neolit123\r\n\r\n\/sig storage\r\n\/kind bug\r\n\/area kubeadm\r\n\r\nrelated-to: [add some e2e test for in-tree volume plugin to verify HonorPVReclaimPolicy](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123151#issuecomment-1932971681)\r\n```[tasklist]\r\n### Tasks\r\n- [ ] Mark the  --enable-hostpath-provisioner option in KCM as deprecated\r\n- [ ] Remove ENABLE_HOSTPATH_PROVISIONER from hack\/local-up-cluster.sh and remove --enable-hostpath-provisioner option in 1.32\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @xing-yang @jsafrane @pacoxu @neolit123 @deepakkinni","> The cluster created by kubeadm doesn't make the feature work as expected. because the kube-controller-manager pod doesn't have a hostPath volume mounted at \/tmp\/hostpath_pv.\r\n\r\nyes, but kubeadm allows you to pass clusterconfiguration.controllerManager.extraVolumes\r\nso i don't think there is a need for a kubeadm change here to comply with the feature.\r\n\r\nit's also important to note that not everyone is using kubeadm.\r\n\r\n","what i'm interested in doing is removing the flex volume support from kubeadm. seems to me the feature in k8s core is stale and there was a plan to remove it completely.\r\n\r\n```\r\n\/\/ Mount for the flexvolume directory (\/usr\/libexec\/kubernetes\/kubelet-plugins\/volume\/exec by default)\r\n\/\/ Flexvolume dir must NOT be readonly as it is used for third-party plugins to integrate with their storage backends via unix domain socket.\r\nflexvolumeDirVolumePath, idx := kubeadmapi.GetArgValue(cfg.ControllerManager.ExtraArgs, \"flex-volume-plugin-dir\", -1)\r\nif idx == -1 {\r\n  flexvolumeDirVolumePath = defaultFlexvolumeDirVolumePath\r\n}\r\nmounts.NewHostPathMount(kubeadmconstants.KubeControllerManager, flexvolumeDirVolumeName, flexvolumeDirVolumePath, flexvolumeDirVolumePath, false, &hostPathDirectoryOrCreate)\r\n```\r\n","The flexVolume is deprecated but still avaliable and no plan to remove support. because some users still may use the volume plugin. some discussions can be found in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122071#discussion_r1442946560 and the volume doc is updated by https:\/\/github.com\/kubernetes\/website\/pull\/44657","> yes, but kubeadm allows you to pass clusterconfiguration.controllerManager.extraVolumes\r\nso i don't think there is a need for a kubeadm change here to comply with the feature.\r\n\r\nYou're right. I haven't seen that before. Unfortunately, those manfiests in https:\/\/cs.k8s.io\/?q=enable-hostpath-provisioner&i=nope&files=&excludeFiles=&repos= don't have extra volumes when `--enable-hostpath-provisioner=true`.\r\n\r\nAn example is  https:\/\/github.com\/kubernetes\/kubeadm\/blob\/main\/kinder\/pkg\/kubeadm\/config.go#L136-L143 @neolit123 \r\n","> You're right. I haven't seen that before. Unfortunately, those manfiests in https:\/\/cs.k8s.io\/?q=enable-hostpath-provisioner&i=nope&files=&excludeFiles=&repos= don't have extra volumes when --enable-hostpath-provisioner=true.\r\n\r\nexternal tools (or users) that use kubeadm can add the extravolumes if they need to.\r\n\r\n> An example is https:\/\/github.com\/kubernetes\/kubeadm\/blob\/main\/kinder\/pkg\/kubeadm\/config.go#L136-L143 @neolit123\r\n\r\nthis is something kinder copied from kind:\r\nhttps:\/\/github.com\/kubernetes-sigs\/kind\/blob\/c83316d25e1c40f1c66982b637edf3ee34a5e0d4\/pkg\/cluster\/internal\/kubeadm\/config.go#L204\r\n\r\ni think it can be removed from kinder because we don't need it.\r\n\r\nit was added to kind here:\r\nhttps:\/\/github.com\/kubernetes-sigs\/kind\/pull\/397\r\n\r\n>> kind creates a default host-path StorageClass but cannot create\r\nPersistentVolumes because the enable-hostpath-provisioner flag is not\r\nset on kube-controller-manager.\r\n\r\n@joejulian @BenTheElder \r\n\r\n> e2e tests don't have a test case for hostpath dynamic provisioning.\r\n\r\nif the feature should be kept a test must be added for it.\r\nup to sig storage.\r\n","\/remove-area kubeadm","\/cc @msau42 \r\n\r\n","I am for removing the hostpath provisioner.\r\n\r\nFor the record, it can be still used in hack\/local-up-cluster.sh, so we would need to remove it from there too: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/7ea3d0245a63fbbba698f1cb939831fe8143db3e\/hack\/local-up-cluster.sh#L258\r\n\r\n[Officially](https:\/\/kubernetes.io\/docs\/reference\/using-api\/deprecation-policy\/#deprecating-a-flag-or-cli), we would need to mark the ` --enable-hostpath-provisioner` option in KCM as deprecated with a release note,  send deprecation notices and wait for a few (2?) releases.","@jsafrane Can we merge #123841  in 1.30?","kind's default provisioner these days is \"implementing PVs\" using https:\/\/github.com\/rancher\/local-path-provisioner so I _think_ we can drop this, we can version-gate disabling it in kind on some future k8s release to let users transition if there are any still.\r\n\r\n> For the record, it can be still used in hack\/local-up-cluster.sh, so we would need to remove it from there too:\r\n\r\ncc @dims \r\n\r\nI don't think we should just deprecate out a feature like this without:\r\n1) A KEP, to capture consensus from the wider project\r\n2) Actually searching for usage and exploring impact","Consider for example non-kubernetes-repo users: https:\/\/github.com\/search?q=enable-hostpath-provisioner&type=code","> A KEP, to capture consensus from the wider project\r\n\r\n@BenTheElder @jsafrane add a KEP in https:\/\/github.com\/kubernetes\/enhancements\/pull\/4548. Can you help review it?"],"labels":["kind\/bug","sig\/storage","needs-triage"]},{"title":"make failed static build CGO_ENABLED=0  cannot find package","body":" static build CGO_ENABLED=0: k8s.io\/kubernetes\/cmd\/kube-apiserver k8s.io\/kubernetes\/cmd\/kube-controller-manager k8s.io\/kubernetes\/cmd\/kube-scheduler k8s.io\/kubernetes\/cmd\/kube-proxy\r\npkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:6:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/cpu\" in any of:\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/k8s.io\/kubernetes\/vendor\/github.com\/shirou\/gopsutil\/v3\/cpu (vendor tree)\r\n        \/usr\/local\/go\/src\/github.com\/shirou\/gopsutil\/v3\/cpu (from $GOROOT)\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/github.com\/shirou\/gopsutil\/v3\/cpu (from $GOPATH)\r\npkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:7:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/disk\" in any of:\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/k8s.io\/kubernetes\/vendor\/github.com\/shirou\/gopsutil\/v3\/disk (vendor tree)\r\n        \/usr\/local\/go\/src\/github.com\/shirou\/gopsutil\/v3\/disk (from $GOROOT)\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/github.com\/shirou\/gopsutil\/v3\/disk (from $GOPATH)\r\npkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:8:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/mem\" in any of:\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/k8s.io\/kubernetes\/vendor\/github.com\/shirou\/gopsutil\/v3\/mem (vendor tree)\r\n        \/usr\/local\/go\/src\/github.com\/shirou\/gopsutil\/v3\/mem (from $GOROOT)\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/github.com\/shirou\/gopsutil\/v3\/mem (from $GOPATH)\r\npkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:9:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/net\" in any of:\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/k8s.io\/kubernetes\/vendor\/github.com\/shirou\/gopsutil\/v3\/net (vendor tree)\r\n        \/usr\/local\/go\/src\/github.com\/shirou\/gopsutil\/v3\/net (from $GOROOT)\r\n        \/go\/src\/k8s.io\/kubernetes\/_output\/dockerized\/go\/src\/github.com\/shirou\/gopsutil\/v3\/net (from $GOPATH)\r\n!!! [0308 01:21:05] Call tree:\r\n!!! [0308 01:21:05]  1: \/go\/src\/k8s.io\/kubernetes\/hack\/lib\/golang.sh:722 kube::golang::build_some_binaries(...)\r\n!!! [0308 01:21:05]  2: \/go\/src\/k8s.io\/kubernetes\/hack\/lib\/golang.sh:878 kube::golang::build_binaries_for_platform(...)\r\n!!! [0308 01:21:05]  3: hack\/make-rules\/build.sh:27 kube::golang::build_binaries(...)\r\n!!! [0308 01:21:05] Call tree:\r\n!!! [0308 01:21:05]  1: hack\/make-rules\/build.sh:27 kube::golang::build_binaries(...)\r\n!!! [0308 01:21:05] Call tree:\r\n!!! [0308 01:21:05]  1: hack\/make-rules\/build.sh:27 kube::golang::build_binaries(...)\r\nmake: *** [Makefile:92: all] Error 1\r\n!!! [0308 01:21:05] Call tree:\r\n!!! [0308 01:21:05]  1: build\/..\/build\/common.sh:480 kube::build::run_build_command_ex(...)\r\n!!! [0308 01:21:05]  2: build\/release-images.sh:40 kube::build::run_build_command(...)\r\nMakefile:450: recipe for target 'quick-release-images' failed\r\nmake: *** [quick-release-images] Error 1\r\n\r\nbelow is my go env:\r\nGO111MODULE=\"on\"\r\nGOARCH=\"amd64\"\r\nGOBIN=\"\"\r\nGOCACHE=\"\/root\/.cache\/go-build\"\r\nGOENV=\"\/root\/.config\/go\/env\"\r\nGOEXE=\"\"\r\nGOEXPERIMENT=\"\"\r\nGOFLAGS=\"\"\r\nGOHOSTARCH=\"amd64\"\r\nGOHOSTOS=\"linux\"\r\nGOINSECURE=\"\"\r\nGOMODCACHE=\"\/root\/GOPATH\/pkg\/mod\"\r\nGONOPROXY=\"\"\r\nGONOSUMDB=\"\"\r\nGOOS=\"linux\"\r\nGOPATH=\"\/root\/GOPATH\"\r\nGOPRIVATE=\"\"\r\nGOPROXY=\"https:\/\/goproxy.cn,direct\"\r\nGOROOT=\"\/usr\/local\/go\"\r\nGOSUMDB=\"sum.golang.org\"\r\nGOTMPDIR=\"\"\r\nGOTOOLDIR=\"\/usr\/local\/go\/pkg\/tool\/linux_amd64\"\r\nGOVCS=\"\"\r\nGOVERSION=\"go1.17.10\"\r\nGCCGO=\"gccgo\"\r\nAR=\"ar\"\r\nCC=\"gcc\"\r\nCXX=\"g++\"\r\nCGO_ENABLED=\"1\"\r\nGOMOD=\"\/root\/kubernetes-1.23.6\/go.mod\"\r\nCGO_CFLAGS=\"-g -O2\"\r\nCGO_CPPFLAGS=\"\"\r\nCGO_CXXFLAGS=\"-g -O2\"\r\nCGO_FFLAGS=\"-g -O2\"\r\nCGO_LDFLAGS=\"-g -O2\"\r\nPKG_CONFIG=\"pkg-config\"\r\nGOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=\/tmp\/go-build4169701844=\/tmp\/go-build -gno-record-gcc-switches\"","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","> pkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:6:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/cpu\" in any of:\r\n\r\nlooks like github.com\/shirou\/gopsutil is used in a scheduler plugin.\r\n\r\n\/sig scheduling\r\n\r\n@WhiteStart \r\ndoes it help if you set GOROOT too?\r\n","> > pkg\/scheduler\/framework\/plugins\/podcapacityofnode\/podcapacityofnode.go:6:2: cannot find package \"github.com\/shirou\/gopsutil\/v3\/cpu\" in any of:\r\n> \r\n> looks like github.com\/shirou\/gopsutil is used in a scheduler plugin.\r\n> \r\n> \/sig scheduling\r\n> \r\n> @WhiteStart does it help if you set GOROOT too?\r\nI customized a scheduler\uff0cand import \"github.com\/shirou\/gopsutil\/v3\/cpu\". what does \/sig scheduling mean? sorry i do not get it\r\n"],"labels":["sig\/scheduling","needs-triage"]},{"title":"Fixing some typo in README.md file","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\nI have found some type in the README.md file and fix those\r\n\r\n#### What type of PR is this?\r\n\r\nUpdate documentation\r\n\r\n<!--\r\n\/kind documentation\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Special notes for your reviewer:\r\nAs a new contributor, please let me know if I need to update anything else\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nNONE\r\n\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @NayeemShaMd. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123800#\" title=\"Author self-approved\">NayeemShaMd<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [johnbelamaric](https:\/\/github.com\/johnbelamaric) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"johnbelamaric\"]} -->"],"labels":["kind\/documentation","size\/XS","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage","do-not-merge\/needs-sig"]},{"title":"Add cri staging repository","body":"\r\n\r\n#### What type of PR is this?\r\n\r\n\r\n\/kind feature\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nNone\r\n\r\n#### Special notes for your reviewer:\r\nRequires https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123795 and https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123796\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nTBD\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123797#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cblecker](https:\/\/github.com\/cblecker), [derekwaynecarr](https:\/\/github.com\/derekwaynecarr), [wojtek-t](https:\/\/github.com\/wojtek-t) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[cmd\/kubemark\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubemark\/OWNERS)**\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[pkg\/probe\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/probe\/OWNERS)**\n- **[staging\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/OWNERS)**\n- ~~[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)~~ [saschagrunert]\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cblecker\",\"derekwaynecarr\",\"wojtek-t\"]} -->"],"labels":["area\/test","area\/kubelet","sig\/node","release-note","size\/L","kind\/feature","area\/release-eng","cncf-cla: yes","sig\/testing","sig\/release","do-not-merge\/work-in-progress","needs-priority","area\/dependency","needs-triage"]},{"title":"Decouple `kubelet\/cri\/remote` package from `pkg\/features`","body":"\r\n#### What type of PR is this?\r\n\r\n\r\n\/kind cleanup\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nImporting the `k8s.io\/kubernetes\/pkg\/features` package in the remote runtime implementation makes it harder to separate the functionalities at some later point in time.\r\n\r\nWe now decouple them by checking if the feature is enabled directly in the kubelet service creation path.\r\n\r\n#### Which issue(s) this PR fixes:\r\nNone\r\n\r\n#### Special notes for your reviewer:\r\nPTAL @kubernetes\/sig-node-pr-reviews \r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123796#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sjenning](https:\/\/github.com\/sjenning) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sjenning\"]} -->","\/lgtm","LGTM label has been added.  <details>Git tree hash: 3634db41b8afdd8566381c877290958dba55872d<\/details>","\/triage accepted\r\n\/priority backlog\r\n\r\nbecause \"at some later point in time\". Nevertheless, nice and free improvement IMO"],"labels":["priority\/backlog","area\/kubelet","kind\/cleanup","lgtm","sig\/node","size\/S","release-note-none","cncf-cla: yes","triage\/accepted"]},{"title":"Decouple `kubelet\/cri\/remote` package from `kubelet\/metrics`","body":"\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nImporting the `k8s.io\/kubernetes\/pkg\/kubelet\/metrics` package in the remote runtime implementation makes it harder to separate the functionalities at some later point in time. We now decouple both packages by extending the CRI API services to allow a callback on `GetContainerEvents`. This callback can be used to do additional work if the connection got established, because `GetContainerEvents` will go into blocking mode after that.\r\n\r\n#### Which issue(s) this PR fixes:\r\nNone\r\n\r\n#### Special notes for your reviewer:\r\nPTAL @harche @mrunalp @haircommander \r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["PTAL @kubernetes\/sig-node-pr-reviews ","\/lgtm","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123795#pullrequestreview-1922255820\" title=\"Approved\">endocrimes<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123795#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- ~~[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)~~ [saschagrunert]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->","LGTM label has been added.  <details>Git tree hash: d0f440cf7d41a5829c5a147bc8faf79b20a5fb96<\/details>","\/triage accepted\r\n\/priority backlog","LGTM, good idea!"],"labels":["priority\/backlog","area\/kubelet","kind\/cleanup","lgtm","sig\/node","size\/S","release-note-none","cncf-cla: yes","triage\/accepted"]},{"title":"RFE: add more node conditions to reflect missing node features","body":"### What happened?\n\nThe kubelet tolerates check for features in the runtime, the base system or the node in general. In most[1] cases it handles the missing features gracefully, soft-disabiling parts of the code, in some other cases it fails loudly.\r\n\r\nMost of these conditions are signaled with log entries, which is good, but requires the cluster operator to go and inspect the logs, perhaps grepping for hints. We should make these conditions easier to spot and consume.\r\n\r\n`NodeConditions` seems like a good fit, but in general we should bubble up this information as conditions somehow to make them available in the cluster.\r\n\r\nExamples of these generic conditions which are just visible in the logs:\r\n\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122561\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123749\r\n- [more to come]\r\n\r\n\r\n+++\r\n\r\n[1] to the best of the reporter's knowledge\n\n### What did you expect to happen?\n\nSome form of kubernetes object, perhaps `node.Status.NodeCondition[*]` should be published to signal lack of features. The node should be set as degraded perhaps\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nplease see the examples listed above for reproducing conditions. There is not a single common flow to trigger this condition\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n1.29+\r\nlikely to be present in earlier version, but proposing to work in 1.31 or later\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nany\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\nany\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nany\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nany\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nnot relevant\r\n<\/details>\r\n","comments":["\/sig node\r\n\/sig instrumentation","There are some flows which should bubble up as Node Conditions (or any other better\/equivalent mechanism). These flows should be carefully pick (not everything probably should bubble up as condition, to avoid explosion of information) on a case-by-cases scenario, which can probably be handled only at KEP\/code review stage.\r\n\r\nQuestions I have:\r\n- would NodeConditions be a good fit? Any better object?\r\n- Besides adding (and removing when appropriate) a condition, should we do something like changing node status? Is there something in between like Degraded or so we can use?\r\n","This sounds like what https:\/\/github.com\/kubernetes\/node-problem-detector can already do. If not, please explain why.\r\n\r\n\/priority awaiting-more-evidence","\/remove-kind bug\r\n\/kind feature\r\n\r\nI don't see a bug.","> This sounds like what https:\/\/github.com\/kubernetes\/node-problem-detector can already do. If not, please explain why.\r\n> \r\n> \/priority awaiting-more-evidence\r\n\r\nThat's a great point, I was looking into NPD indeed.\r\nI think there are a set of issues which only the kubelet can detect. At glance, NPD brings these examples\r\n```\r\nInfrastructure daemon issues: ntp service down;\r\nHardware issues: Bad CPU, memory or disk;\r\nKernel issues: Kernel deadlock, corrupted file system;\r\nContainer runtime issues: Unresponsive runtime daemon;\r\n```\r\nand these are indeed problems an agent external to the kubelet can safely detect. But let's take one of the examples I linked above (looking for more): a kubelet feature gate is enabled, but the runtime doesn't offer the capability required by the feature, because it's too old. This is something easy to detect from the kubelet. OTOH, the NPD _can_ totally detect the same problem, but that will require duplicating kubelet logic.\r\n\r\nSame goes for cgroups controllers say. NPD can totally detect misconfigurations, but since the kubelet owns them (on a kubernetes node), this will mean replicating the check logic.","\/assign\r\n\/triage accepted","\/cc","Not sure if this actually involves sig-instrumentation (unless you are planning to add metrics or logs).\r\n\r\nNPD could potentially give you half of the answer: e.g. SwapProvisioned: true\/false\/unknown, but the kubelet would definitely be able to give the complete picture.  You could also consider a metric with a single boolean value.  Too many conditions can increase the size of the node object, and marginally increase the load on the APIServer in large clusters.","We could add `SchedulingDisabled` as an actual condition (that the kubelet always sets or unsets according to the Node's `.spec`)","\/remove-priority awaiting-more-evidence"],"labels":["sig\/node","kind\/feature","sig\/instrumentation","needs-priority","triage\/accepted"]},{"title":"[Flaking] [capz-windows-master] [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly","body":"### Which jobs are flaking?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-capz-master-windows\/1765354341419454464\n\n### Which tests are flaking?\n\n- Kubernetes e2e suite: [It] [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]\u00a0\r\n\r\n\n\n### Since when has it been flaking?\n\nIt is a very old flake. Link to some old threads\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/115470\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/108711\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/113479\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#capz-windows-master\n\n### Reason for failure (if possible)\n\n```\r\n{ failed [FAILED] Timed out after 30.001s.\r\nExpected\r\n    <*errors.errorString | 0xc000c97e90>: \r\n    failed to match regexp \"GET \/echo\\\\?msg=prestop\" in output \"I0306 13:11:27.045103   12464 log.go:245] Started HTTP server on port 8080\\nI0306 13:11:27.391364   12464 log.go:245] Started UDP server on port  8081\\n\"\r\n    {\r\n        s: \"failed to match regexp \\\"GET \/echo\\\\\\\\?msg=prestop\\\" in output \\\"I0306 13:11:27.045103   12464 log.go:245] Started HTTP server on port 8080\\\\nI0306 13:11:27.391364   12464 log.go:245] Started UDP server on port  8081\\\\n\\\"\",\r\n    }\r\nto be nil\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/common\/node\/lifecycle_hook.go:129 @ 03\/06\/24 13:12:09.177\r\n}\r\n```\r\n\r\n@claudiubelu investigated in this issue before and here are some comments from https:\/\/github.com\/kubernetes\/kubernetes\/issues\/108711#issuecomment-1116286496\r\n\r\n> Regarding the [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance] test, it seems that this issue pretty much only occurs with capz, and only in the capz environments [1]. In capz, we do use calico, so that might be the issue.\r\n> A small explanation on what's happening in the test: it spawns a webserver pod, and another pod exec pod, which has gracefulTerminationPod (meaning that it won't be killed immediately, and it is allowed to run a prestop hook) and which has a prestop hook, which execs a curl to the pod webserver, which sometimes ends up with an exit code 7 Bad Access, and the request doesn't reach the webserver pod.\r\n\r\n> Additionally, I think it's worth noting that there are 2 different prestop lifecycle hook jobs: an exec prestop (flaky), and a http prestop (not flaky) [4]. In both scenarios, the webserver pod is contacted, but by different actors: In the exec prestop case, the exec pod is exec'd into and curl is used, while for the http prestop case, the node's kubelet is responsible for accessing the http pod (which means that it's not dependent on the client pod's networking).\r\n\r\n\r\n\r\n\n\n### Anything else we need to know?\n\nA calico issue is open https:\/\/github.com\/projectcalico\/calico\/issues\/7332 to address the problem. This issue is not responded by calico community yet.\n\n### Relevant SIG(s)\n\n\/sig windows node network","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","@justaugustus @claudiubelu If this flake is related to calico network, can we use `hostNetwork:true` for this test in windows e2e? I mean that we can set `hostNetwork: true` in this e2e test. "],"labels":["sig\/network","sig\/node","kind\/flake","sig\/windows","needs-triage"]},{"title":"client-go: reflector name defaulting seems broken by Go modules","body":"### What happened?\n\nClient-go emits logs like\r\n```\r\nListing and watching *v1.Namespace from pkg\/mod\/k8s.io\/client-go@v0.29.2\/tools\/cache\/reflector.go:229\r\n```\r\n\n\n### What did you expect to happen?\n\nLog says \"from <somewhere more useful>\"\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nIncrease verbosity of anything using client-go, check logs\n\n### Anything else we need to know?\n\nCode is from https:\/\/github.com\/kubernetes\/kubernetes\/blame\/05cb0a55c88e0cdcfe2fb184328ad9be53e94d5c\/staging\/src\/k8s.io\/client-go\/tools\/cache\/reflector.go#L290. `client-go\/tools\/cache\/` is no longer accurate, with go modules this will actually be something like `client-go@v0.29.1\/tools\/cache\/` generally. The blame shows its 6 years old which was before go modules, which would make sense.\r\n\r\nThis also means this has probably been an issue for many years, and it only impacts logs that are generally disabled, so this is certainly not a high priority\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\nN\/A\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\nN\/A\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig api-machinery"],"labels":["kind\/bug","sig\/api-machinery","needs-triage"]},{"title":"Test for UnstructureList decode diff between strict and non-strict.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/sig api-machinery\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-unit\r\n\r\n```\r\nmake test WHAT=.\/staging\/src\/k8s.io\/apimachinery\/pkg\/runtime\/serializer\/json\r\n+++ [0306 18:38:10] Set GOMAXPROCS automatically to 20\r\n+++ [0306 18:38:10] Running tests without code coverage and with -race\r\n--- FAIL: TestDecodeUnstructuredList (0.00s)\r\n    unstructured_test.go:44: diff strict lax:\r\n          unstructured.UnstructuredList{\r\n          \tObject: map[string]any{\r\n          \t\t\"apiVersion\": string(\"json.example.com\/v1alpha1\"),\r\n        - \t\t\"items\":      []any{map[string]any{\"spec\": map[string]any{\"abc\": int64(123)}}},\r\n          \t\t\"kind\":       string(\"FooList\"),\r\n          \t},\r\n          \tItems: []unstructured.Unstructured{\r\n          \t\t{\r\n          \t\t\tObject: map[string]any{\r\n        + \t\t\t\t\"apiVersion\": string(\"json.example.com\/v1alpha1\"),\r\n        + \t\t\t\t\"kind\":       string(\"Foo\"),\r\n          \t\t\t\t\"spec\":       map[string]any{\"abc\": int64(123)},\r\n          \t\t\t},\r\n          \t\t},\r\n          \t},\r\n          }\r\nFAIL\r\nFAIL\tk8s.io\/apimachinery\/pkg\/runtime\/serializer\/json\t0.102s\r\nFAIL\r\n```","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123783#\" title=\"Author self-approved\">benluddy<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","@benluddy: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | 073b4ab1544220afe3dd17938680d91c0941b4e1 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123783\/pull-kubernetes-unit\/1765525980639662080) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123783). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Abenluddy). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["sig\/api-machinery","size\/M","cncf-cla: yes","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"kubelet: check that the pod is finished before removing the container.","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nWhen pods with ephemeral storage limits exceed their capacity, the kubelet tries to evict and kill the pod gracefully. We noticed that the pod status changes to `ContainerStatusUnknown`. This is due to a race condition removing the container before pod status can be updated.\r\n\r\nIn this change, we try to ensure that the pod is finished before removing the container.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes #122160 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nFixes an issue where the pod status changes to ContainerStatusUnknown when it is evicted due to exceeding the ephemeral storage limit.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n\r\n```docs\r\n\r\n```\r\n","comments":["Hi @AnishShah. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/sig node","\/ok-to-test\r\n\/triage accepted\r\n\/priority important-soon\r\n\r\nusing the same priority as the corresponding issue","Would this PR need a changelog entry? I think the behavior of the cluster would change.","\/retest",">  Would this PR need a changelog entry? I think the behavior of the cluster would change.\r\n\r\n@sftim, my understanding is that we are not seeing the expected behavior due to a race condition. So it is not changing the behavior but fixing the issue.\r\n","If we fix a bug, we usually add a change log entry. (If there is nothing to fix, does it need merging?)","@SergeyKanzhelev and @bobbypage to help review the change","cc: @ndixita ","\/assign @SergeyKanzhelev \r\n\/assign @bobbypage \r\n\/assign @mrunalp ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123782#\" title=\"Author self-approved\">AnishShah<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [mrunalp](https:\/\/github.com\/mrunalp) and additionally assign [pohly](https:\/\/github.com\/pohly) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n- **[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)**\n- **[test\/e2e\/node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mrunalp\",\"pohly\"]} -->","\/retest","I modified the existing eviction test to check the terminated reason is not `ContainerStatusUnknown`. I ran the test multiple times with this command `ginkgo .\/test\/e2e -focus=\".*evicted pods should be terminal.*\" -repeat=100` in clusters with and without this fix. The test failed in the cluster without the fix after 22nd attempts whereas the test did not fail in the cluster with this fix."],"labels":["kind\/bug","area\/test","priority\/important-soon","area\/kubelet","sig\/node","release-note","size\/S","cncf-cla: yes","sig\/testing","ok-to-test","area\/e2e-test-framework","triage\/accepted"]},{"title":"kublet prober infinite Readiness check - no Liveness probe defeating self-heal ","body":"### What happened?\n\npod(container) Readiness and Liveness probe are non-blocking routines. And if readiness probe is failing, a liveness probe can trigger restart and possibly self-heal.\r\n\r\nHowever, encountered a case where;\r\n- coredns pod starts, but an external automation causes IP removal on node. the cni IPAM is forced to sync the resource state and the coredns pod network ns is torn down and rebuilt - container ID change, but pod remains ID unchanged\r\n```\r\nFeb 28 16:33:53 ... kubelet.go:2456] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\" event={\"ID\":\"001d42a0-1729-44fc-9959-b6e751ee44d9\",\"Type\":\"ContainerStarted\",\"Data\":\"9e324b5e26ac15640355f3dd86bcdb80f81f380827b435abc85365cd67fcc1f2\"}\r\nFeb 28 16:33:54 ... kubelet.go:2456] \"SyncLoop (PLEG): event for pod\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\" event={\"ID\":\"001d42a0-1729-44fc-9959-b6e751ee44d9\",\"Type\":\"ContainerStarted\",\"Data\":\"86d1a5bcff3978fcbaae12fc6259adade96747cc14c3aa3374102d41b34c1636\"}\r\n```\r\n\r\n-  no startUp probe in coredns, so container ready, and doProbe readiness probe is sent\r\n```\r\nFeb 28 16:33:54 ... kubelet.go:2528] \"SyncLoop (probe)\" probe=\"readiness\" status=\"\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\"\r\n```\r\n\r\n- this http probe fails with a http status code 503, aand a liveness probe is never issued and self-heal\/restart triggered\r\n```\r\nFeb 28 16:33:54 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\" podUID=\"001d42a0-1729-44fc-9959-b6e751ee44d9\" containerName=\"coredns\" probeResult=\"failure\" output=\"HTTP probe failed with statuscode: 503\"\r\nFeb 28 16:33:56 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\" podUID=\"001d42a0-1729-44fc-9959-b6e751ee44d9\" containerName=\"coredns\" probeResult=\"failure\" output=\"HTTP probe failed with statuscode: 503\"\r\nFeb 29 00:18:22 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"kube-system\/coredns-f88c6698d-zmjgk\" podUID=\"001d42a0-1729-44fc-9959-b6e751ee44d9\" containerName=\"coredns\" probeResult=\"failure\" output=\"HTTP probe failed with statuscode: 503\"\r\n```\r\n---\r\n- It is just unclear why the liveness probe on coredns spec is never sent\r\nis the getWorker here to [UpdatePodStatus](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/546f7c30860dcdecb75c544230a1b7cdf5bd5958\/pkg\/kubelet\/prober\/prober_manager.go#L289) after checking startup probe not introducing an inadvertent wait for readiness?\n\n### What did you expect to happen?\n\n- kubernetes self-heal attempt of a pod that is stuck\/failing probe\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    test: liveness\r\n  name: liveness-http\r\nspec:\r\n  containers:\r\n  - name: liveness\r\n    image: registry.k8s.io\/liveness\r\n    args:\r\n    - \/server\r\n    livenessProbe:\r\n      failureThreshold: 5\r\n      httpGet:\r\n        path: \/healthz\r\n        port: 8080\r\n        httpHeaders:\r\n        - name: Custom-Header\r\n          value: Awesome\r\n      initialDelaySeconds: 60 || 300\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 5\r\n    readinessProbe:\r\n      failureThreshold: 3\r\n      httpGet:\r\n        path: \/healthz\r\n        port: 8080\r\n        httpHeaders:\r\n        - name: Custom-Header\r\n          value: Awesome\r\n      periodSeconds: 10\r\n      successThreshold: 1\r\n      timeoutSeconds: 1\r\n  restartPolicy: Always\r\n```\r\n- test pod above\r\n- remove the IP assigned to a pod externally, after starting and readiness probe(unimportant), but before the liveness probe \r\n- forcing the node IPAM to re-sync and a similar error is encountered;\r\n```\r\nMar 06 20:18:37 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"gateway-ns\/liveness-http\" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName=\"liveness\" probeResult=failure output=\"HTTP probe failed with statuscode: 500\"\r\nMar 06 20:18:47 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"gateway-ns\/liveness-http\" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName=\"liveness\" probeResult=failure output=\"HTTP probe failed with statuscode: 500\"\r\nMar 06 20:18:57 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"gateway-ns\/liveness-http\" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName=\"liveness\" probeResult=failure output=\"HTTP probe failed with statuscode: 500\"\r\nMar 06 20:19:07 ... prober.go:107] \"Probe failed\" probeType=\"Readiness\" pod=\"gateway-ns\/liveness-http\" podUID=868de996-ff69-44bd-b73c-feaeb2234839 containerName=\"liveness\" probeResult=failure output=\"HTTP probe failed with statuscode: 500\"\r\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.1\r\nKustomize Version: v5.0.4...\r\nServer Version: v1.29.1...-eks-...\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nEKS\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https:\/\/amazonlinux.com\/\"\r\n\r\n$ uname -a\r\ninux ....compute.internal 5.10.198-187.748.amzn2.x86_64 #1 SMP Tue Oct 24 19:49:54 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig Node","@AbeOwlu , to reproduce this issue, how to remove the IP assigned to a pod externally and force node IPAM to re-sync? Is this an issue with AWS VPC CNI?\r\n\r\n\/triage needs-information","HI @AnishShah , thanks for looking into this...\r\n- and you're accurate this was initially seen on aws cni, and [this issue was raised with the cni](https:\/\/github.com\/aws\/amazon-vpc-cni-k8s\/issues\/2834)\r\n- was testing this on calico cni with `calicoctl ipam release --force` and it may be a similar state... should confirm this and update with more information soon.\r\n\r\n- from checking the containerd logs, it does seem the CRI attempts to tear down the container sandbox and recreate it, but the CNI does not responde in the aws cni scenario. So the container orchestrator may actually be handling this case as expected."],"labels":["kind\/bug","sig\/node","triage\/needs-information","needs-triage"]},{"title":"Remove setting NoRouteCreated condition","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nCondition `NetworkUnavailable` with message `NoRouteCreated` is set in two other places\r\n\r\n1. kubelet https:\/\/github.com\/kubernetes\/kubernetes\/blob\/v1.29.2\/pkg\/kubelet\/kubelet_node_status.go#L345\r\n2. route controller https:\/\/github.com\/kubernetes\/kubernetes\/blob\/v1.29.2\/staging\/src\/k8s.io\/cloud-provider\/controllers\/route\/route_controller.go#L408\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123777#\" title=\"Author self-approved\">linxiulei<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [cheftako](https:\/\/github.com\/cheftako) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"cheftako\"]} -->","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @aojea "],"labels":["kind\/cleanup","area\/cloudprovider","size\/L","release-note-none","cncf-cla: yes","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"A Job might finish with ready!=0, terminating!=0","body":"### What happened?\r\n\r\nWhen a Job is declared Failed, the running Pods still count as ready.\r\nThis causes problems for higher level controllers that use the Failed\/Completed conditions to do usage accounting. If the job is marked as finished before all the pods finish, the accounting is inaccurate.\r\n\r\n### What did you expect to happen?\r\n\r\nA few options (non-necessarily exclusive), in order of my preference:\r\n1. The job not to be declared Completed\/Failed until all the Pods have finished. This is possibly a breaking change. As a mitigation, we can use the `FailureTarget` or `SuccessCriteriaMet` conditions to provide early feedback to controllers that a Job is marked for failure.\r\n2. ~Do not count Pods with a deletionTimestamp as ready. (Possibly a breaking change)~\r\n3. ~We just consider ready=0 if setting a finished condition~\r\n4. Continue syncing a Job even after the Failed condition is added, until ready\/terminating fields are zero.\r\n\r\nOptions 2 and 3 don't satisfy the requirement of proper accounting.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. Use this Job (note backoffLimit: 0):\r\n```yaml\r\napiVersion: batch\/v1\r\nkind: Job\r\nmetadata:\r\n  name: indexed-job\r\n  labels:\r\n    jobgroup: indexedjob\r\nspec:\r\n  completions: 4\r\n  parallelism: 4\r\n  backoffLimit: 0\r\n  completionMode: Indexed\r\n  podReplacementPolicy: Failed\r\n  template:\r\n    metadata:\r\n      labels:\r\n        jobgroup: indexedjob\r\n    spec:\r\n      restartPolicy: Never\r\n      containers:\r\n      - name: 'worker'\r\n        image: 'centos:7'\r\n        command:\r\n        - \"sh\"\r\n        - \"-c\"\r\n        - \"echo 'hello' && sleep 120 && echo 'bye'\r\n```\r\n2. Delete one of the pods with kubectl\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\nANY\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nANY\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","cc @soltysh @deads2k for thoughts.\r\n\r\nI listed the options according to my preference towards a consistent API, even if it's somewhat of a breaking change.","Another alternative would be to do something like 3, but 1 when using  `podReplacementPolicy: Failed` or `podFailurePolicies` which are beta features.","\/cc ","\/sig apps\r\n\/wg batch","Reading this:\r\n\r\nWhen a Job is declared Failed, the running Pods still count as ready.\r\n\r\nThis seems to be WAI.\r\n\r\nReady and running are not related states. ","The issue extends also for `terminating` (status using the same job):\r\n```\r\n  status:\r\n    conditions:\r\n    - lastProbeTime: \"2024-03-07T08:38:21Z\"\r\n      lastTransitionTime: \"2024-03-07T08:38:21Z\"\r\n      message: Job has reached the specified backoff limit\r\n      reason: BackoffLimitExceeded\r\n      status: \"True\"\r\n      type: Failed\r\n    failed: 4\r\n    ready: 2\r\n    startTime: \"2024-03-07T08:37:08Z\"\r\n    terminating: 1\r\n    uncountedTerminatedPods: {}\r\n```","\/cc","Since this affects both `ready` and `terminating` I think we could fold the fix under `PodReplacmentPolicy`, because it explicitly in [Motivation](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/3939-allow-replacement-when-fully-terminated#motivation) says: \"This new field can also be used by queueing controllers, such as Kueue, to track the number of terminating pods to calculate quotas.\". The KEP has also similar statements as [goal](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/3939-allow-replacement-when-fully-terminated#goals) \"Job controller will have a new status field where we include the number of terminating pods.\", and [Story 3](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/3939-allow-replacement-when-fully-terminated#story-3).\r\n\r\nThis is not achieved if Job controller stops tracking \"terminating\" pods before the pods complete. In particular, once Kueue \r\n (or another Job scheduling controller) sees that a job is \"Failed\" it is free to admit more jobs to run, but the resources might still be occupied by the terminating pods (Kueue does not have a way to know the number of terminating pods after \"Failed\" or \"Complete\" is added). This may lead to undesired scale up by Cluster Autoscaler, because for a short while there are more pods that node resources.\r\n\r\nIf we want to protect by beta-level feature, then I would suggest `PodReplacementPolicy`, and wait in `enactJobFinished` for `ready=0` and `terminating=0`, probably [here](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/2ec63e0d28951bb525a5bce0d9459afa1c71c0bd\/pkg\/controller\/job\/job_controller.go#L1335-L1339) where we wait for `uncountedTerminaedPods` to be empty.\r\n\r\nThis is a fully non-breaking change, just \"fixing\" beta-level feature.\r\n\r\nHaving said that I would also be for breaking change and do the waiting regardless of the feature gate.","This was discovered during recent work on https:\/\/github.com\/kubernetes\/enhancements\/issues\/4368 and affects the recent changes to the API field comments. I propose to revert the inacurate comments here https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123792, and track it also in the graduation criteria for the feature, opened the KEP update: https:\/\/github.com\/kubernetes\/enhancements\/pull\/4542. ","We should probably stress the fact, this is true only for failed jobs, not succeeded. The latter would be a bug, the former is a separate discussion. I'm looking at this situation like this, what needs to happen that this job won't fail? Why do you think we need to wait for the remaining 3 pods to fail? Is it possible the job will fix itself and fulfill the contract of a success? The only place where I can think of, where this kind of situation might be problematic is when you have the [SuccessPolicy](https:\/\/github.com\/kubernetes\/enhancements\/issues\/3998) specified, that's the only place where I could see a job with a failed pod should not fail. In all other cases, I think the extra wait doesn't give us anything, does it? \r\n\r\n@tenzen-y this particular case described above might be important for you, when you'll be working on your feature. Probably worth adding an explicit test case covering that.","> Why do you think we need to wait for the remaining 3 pods to fail?\r\n\r\nWell, I don't think this is needed for the basic job functionality.\r\n\r\nNonetheless, this is expected for job schedulers like Kueue. The 3 remaining pods still consume resources for 30s by default. Then, Kueue seeing the Job is \"Failed\" schedules a new one and we are running overcommitted with unschedulable pods, and Cluster Autoscaler intervenes creating new nodes, which is not desired by user who pays for the extra nodes. \r\n\r\nWe aimed solving this as part of PodReplacementPolicy by introducing the `terminating` field that Kueue could watch. It works ok for terminating pods as long as the Job is not Failed itself. ","> Why do you think we need to wait for the remaining 3 pods to fail? Is it possible the job will fix itself and fulfill the contract of a success?\r\n\r\nThe Job is already meant to fail at this point. The only motivation for waiting is to clearly signal that the are no more pods running for this Job and no more status updates are necessary. Adding a Failed condition only at the end is an easy way for users or controllers to know when the job has reached the final state.\r\n\r\nAn alternate solution would be to continue syncing a Job even after the Failed condition is added, until ready\/terminating fields are zero.","\/assign @dejanzele\r\nSince you volunteered to work on this once we reach an agreement.","> @tenzen-y this particular case described above might be important for you, when you'll be working on your feature. Probably worth adding an explicit test case covering that.\r\n\r\n@soltysh Regarding the JobSuccessPolicy test case, I added a dedicated unit test here: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/e216742672aa1bfd10b5cc84fa9191eddadeac72\/pkg\/controller\/job\/job_controller_test.go#L3837-L3900\r\n","I put this topic for the next WG meeting, but I hope we can reach consensus before that, given that the test-freeze is close.","@rphillips and I were discussing this. One concern I have is (for example see https:\/\/github.com\/kubernetes\/kubernetes\/issues\/122824#issuecomment-1899224434).\r\n\r\nBy design, readiness probes run during terminating to help with gracefulshutdown of services. So it may be a bit confusing but it is possible for a terminating pod to be considered ready. Ready just means that the readiness probe was successfully.\r\n\r\nI think this change could be fine as it should be self contained to Job\/CronJob but we should mention that I don't think it is a bug for a pod that is terminating to also be considered ready.","From the WG Batch meeting, the sentiment was that a solution close to option 1 would be the best path forward. However, the implications are too big to consider in 1.30. It should be revisited for 1.31, either as a standalone KEP or as part of failure policies, replacement policies or success policies.","I also updated the issue description to incorporate motivation and extra thoughts on each option.","> From the WG Batch meeting, the sentiment was that a solution close to option 1 would be the best path forward. However, the implications are too big to consider in 1.30. It should be revisited for 1.31, either as a standalone KEP or as part of failure policies, replacement policies or success policies.\r\n\r\nSince the SuccessPolicy is still in the alpha stage, it might be a good starting point.\r\n","> Nonetheless, this is expected for job schedulers like Kueue. The 3 remaining pods still consume resources for 30s by default. Then, Kueue seeing the Job is \"Failed\" schedules a new one and we are running overcommitted with unschedulable pods, and Cluster Autoscaler intervenes creating new nodes, which is not desired by user who pays for the extra nodes.\r\n\r\nShouldn't then kueue to not only wait for terminated condition, but also for the remaining pods to finish as well? \r\n\r\n> From the WG Batch meeting, the sentiment was that a solution close to option 1 would be the best path forward. However, the implications are too big to consider in 1.30. It should be revisited for 1.31, either as a standalone KEP or as part of failure policies, replacement policies or success policies.\r\n\r\nI don't think that's a good approach. I'm personally leaning towards option 4, which is backwards compatible, with eventual doc updates to strengthen the contract of the API. \r\n\r\nI will say that I'm relieved to hear this is not blocking 1.30, and we can continue the discussions for 1.31 :smile: ","> Shouldn't then kueue to not only wait for terminated condition, but also for the remaining pods to finish as well?\r\n\r\nIt could but, with separation of concerns in mind, Kueue only reacts to the Job status.\r\n\r\nI still prefer option 1 for its simplicity: users and controllers only need to look at one place to determine whether everything is done for the job.\r\n\r\nBut option 4 is acceptable, IMO.\r\n","For 4 I think one would need to see if kubelet actually sets ready to 0 once the pod is completed (failed or success). \r\n\r\nWe run readiness probes during terminating so they could be true or false while terminating. I have not looked into what happens once a pod is complete (if ready gets set to false). \r\n\r\nI personally like scoping this to 1 (or job controller) as there is more control over this behavior. If we want to go the route that kubelet sets ready to false when pods are failed, it could impact other workloads.","I also prefer, for simplicity of API clients, to only add terminal conditions (like Failed or Complete) when no more updates to the status. Otherwise clients need to monitor until ready=0 and terminating=0, maybe in the future for more things, which could become problematic.\r\n\r\nAlso, I'm not sure why delaying setting the Failed condition is  a \"breaking change\". It delays setting the Failed condition for deleted pods by graceful termination time, which is typically 30s. However, from the API client POV the behavior is consistent. \r\n\r\nWhen I introduced initially the validation rule for terminating=0 when Failed=true, when working on `managedBy` I didn't realize there is any change, because the API server kept rejecting setting of Failed condition by the Job controller, until terminating=0. All conformance tests for Job were passing. So this is not a change that would \"break a client\", but it would make it wait longer in some cases.\r\n\n\nHaving said that I'm also ok with 4, but maybe then we should have an extra condition 'Terminal' to indicate no more updates, to provide future a proof check for the API clients?"],"labels":["kind\/bug","sig\/apps","needs-triage","wg\/batch"]},{"title":"WIP: release-1.27: hack\/tools: bump honnef.co\/go\/tools to v0.4.6","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThat version is required for Go 1.22 because of\r\nhttps:\/\/github.com\/dominikh\/go-tools\/releases\/tag\/2023.1.6.\r\n\r\n#### Which issue(s) this PR fixes:\r\nRelated-to: https:\/\/github.com\/kubernetes\/release\/issues\/3280#issuecomment-1977253034\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123769#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/OWNERS)**\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/hack\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sttts\"]} -->","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-verify\r\n","@pohly: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify | e0a4ffacce48667e96ea312def0526dc4dd4eb68 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123769\/pull-kubernetes-verify\/1765431684280356864) | true | `\/test pull-kubernetes-verify`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123769). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Apohly). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["kind\/cleanup","needs-rebase","size\/S","release-note-none","cncf-cla: yes","do-not-merge\/cherry-pick-not-approved","do-not-merge\/work-in-progress","needs-priority","area\/dependency","needs-triage","do-not-merge\/needs-sig"]},{"title":"release 1.28: hack\/tools: bump honnef.co\/go\/tools to v0.4.6","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThat version is required for Go 1.22 because of\r\nhttps:\/\/github.com\/dominikh\/go-tools\/releases\/tag\/2023.1.6.\r\n\r\n#### Which issue(s) this PR fixes:\r\nRelated-to: https:\/\/github.com\/kubernetes\/release\/issues\/3280#issuecomment-1977253034\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123768#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/OWNERS)**\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/hack\/OWNERS)~~ [pohly]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-verify\r\n","@pohly: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify | d8046772d5addfd5018739f7327567b8724846b8 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123768\/pull-kubernetes-verify\/1765430658261323776) | true | `\/test pull-kubernetes-verify`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123768). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Apohly). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["kind\/cleanup","needs-rebase","size\/S","release-note-none","cncf-cla: yes","do-not-merge\/cherry-pick-not-approved","do-not-merge\/work-in-progress","needs-priority","area\/dependency","needs-triage","do-not-merge\/needs-sig"]},{"title":"failed to delete cgroup paths","body":"### What happened?\n\nPods stuck in terminating state\r\n\r\nlots of log entries like this one\r\n```\r\njournalctl -u kubelet --since -1m -f | grep \"failed to delete cgroup paths\"\r\nFeb 28 05:57:26 worker kubelet[592400]: E0228 05:57:26.352096  592400 pod_workers.go:1300] \"Error syncing pod, skipping\" err=\"failed to delete cgroup paths for [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : unable to destroy cgroup paths for cgroup [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : Failed to remove paths: map[:\/sys\/fs\/cgroup\/unified\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice blkio:\/sys\/fs\/cgroup\/blkio\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpu:\/sys\/fs\/cgroup\/cpu,cpuacct\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpuacct:\/sys\/fs\/cgroup\/cpu,cpuacct\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice cpuset:\/sys\/fs\/cgroup\/cpuset\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice devices:\/sys\/fs\/cgroup\/devices\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice freezer:\/sys\/fs\/cgroup\/freezer\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice hugetlb:\/sys\/fs\/cgroup\/hugetlb\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice memory:\/sys\/fs\/cgroup\/memory\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice misc:\/sys\/fs\/cgroup\/misc\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice name=systemd:\/sys\/fs\/cgroup\/systemd\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice net_cls:\/sys\/fs\/cgroup\/net_cls,net_prio\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice net_prio:\/sys\/fs\/cgroup\/net_cls,net_prio\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice perf_event:\/sys\/fs\/cgroup\/perf_event\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice pids:\/sys\/fs\/cgroup\/pids\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice rdma:\/sys\/fs\/cgroup\/rdma\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice]\" pod=\"nams-prod-de\/attachment-upload-service-cb986ddbc-zz85t\" podUID=\"c5659845-8967-408a-aa34-1223384f0ade\"\r\n```\n\n### What did you expect to happen?\n\nPods terminating and cgroups paths deleted\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nNot sure. In one cluster I have seen the issue multiple times. Other clusters with the same configuration do not show the same symptoms. \n\n### Anything else we need to know?\n\nUnable to delete cgroup scope\r\n```\r\nrmdir \/sys\/fs\/cgroup\/unified\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice\/cri-containerd-1495ffbe17ae4d061ef354162d59a10fa7536e6512f8d280ceb1a21fa31fe684.scope\r\nrmdir: failed to remove '\/sys\/fs\/cgroup\/unified\/kubepods.slice\/kubepods-burstable.slice\/kubepods-burstable-podc5659845_8967_408a_aa34_1223384f0ade.slice\/cri-containerd-1495ffbe17ae4d061ef354162d59a10fa7536e6512f8d280ceb1a21fa31fe684.scope': Device or resource busy\r\n```\r\n\r\nWorkaround\r\n```\r\napt install cgroup-tools\r\nlscgroup | grep <part-of-pod-id>\r\nfor cgroup in $(lscgroup | grep <part-of-pod-id>); do cgdelete -r $cgroup; done\r\n```\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10\", GitCommit:\"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99\", GitTreeState:\"clean\", BuildDate:\"2023-10-18T11:44:31Z\", GoVersion:\"go1.20.10\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nKustomize Version: v4.5.7\r\nServer Version: version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.28.5\", GitCommit:\"506050d61cf291218dfbd41ac93913945c9aa0da\", GitTreeState:\"clean\", BuildDate:\"2023-12-19T13:32:53Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nnone, kubeadm based cluster on VMware VMs\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n\r\n$ uname -a\r\nLinux gtloclvu14373 5.15.0-94-generic #104~20.04.1-Ubuntu SMP Tue Jan 16 13:34:09 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nKubermatic Kubernetes Platform v2.24\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd containerd.io 1.6.28 ae07eda36dd25f8a1b98dfbf587313b99c0190bb\r\n\r\nrunc version 1.1.12\r\ncommit: v1.1.12-0-g51d5e94\r\nspec: 1.0.2-dev\r\ngo: go1.20.13\r\nlibseccomp: 2.5.1\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nCanal v3.26.1\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","\/assign @harche \r\n","@vitality411 thanks for reporting the issue. Can you attach the kubelet logs from that run? ","@harche I don't have the kubelet logs from that run anymore. Attached you will find a current problem. This is just the last 10 minutes of kubelet logs. syslog since last rotation 7h ago is already 1.2G. \r\n\r\n[journal.gz](https:\/\/github.com\/kubernetes\/kubernetes\/files\/14597445\/journal.gz)\r\n"],"labels":["kind\/bug","sig\/node","needs-triage"]},{"title":"[Flaking Test] [sig-network] Networking Granular Checks: Services should update endpoints: http (gce-ubuntu-master-containerd,master-blocking)","body":"### Which jobs are flaking?\r\n\r\ngce-ubuntu-master-containerd\r\n\r\nProw: https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-ubuntu-gce-containerd\/1765185224909524992\r\n\r\n### Which tests are flaking?\r\n\r\nKubernetes e2e suite.[It] [sig-network] Networking Granular Checks: Services should update endpoints: http\r\n\r\n### Since when has it been flaking?\r\n\r\n03-06 (intermittently)\r\n\r\n### Testgrid link\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#gce-ubuntu-master-containerd\r\n\r\n### Reason for failure (if possible)\r\n\r\n`{ failed [FAILED] failed dialing endpoint (recovery), did not find expected responses... \r\nTries 46\r\nCommand curl -g -q -s 'http:\/\/10.64.2.79:9080\/dial?request=hostname&protocol=http&host=10.0.6.39&port=80&tries=1'\r\nretrieved map[bootstrap-e2e-minion-group-5dtl:{} netserver-1:{} netserver-2:{} netserver-3:{}]\r\nexpected map[netserver-1:{} netserver-2:{} netserver-3:{}]\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/network\/networking.go:341 @ 03\/06\/24 01:32:57.492\r\n}`\r\n\r\n### Anything else we need to know?\r\n\r\nmultiple failures observed intermittently in the Traige dashboard: https:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=Services%20should%20update%20endpoints\r\n\r\n### Relevant SIG(s)\r\n\r\n\/sig network release","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @aojea","\/retitle [Flaking Test] [sig-network] Networking Granular Checks: Services should update endpoints: http (gce-ubuntu-master-containerd,master-blocking)","> 03-06 (intermittently)\r\n\r\nIt flaked in ci-kubernetes-e2e-gci-gce-alpha-enabled-default before. \r\n- https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-cos-k8sbeta-alphafeatures\/1755848671338958848\r\n- https:\/\/storage.googleapis.com\/k8s-triage\/index.html?date=2024-02-12&test=Services%20should%20update%20endpoints&xjob=cilium\r\n- 02-28 https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-cos-k8sstable2-alphafeatures\/1762683642208849920\r\n\r\n\r\n"],"labels":["sig\/network","kind\/flake","sig\/release","needs-triage"]},{"title":" self nominate ffromani to be a sig-node reviewer ","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nself nominate @ffromani  to be a sig-node reviewer\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #N\/A\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nPer https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-node\/sig-node-contributor-ladder.md#reviewer \r\n\r\n* contributing to the [SIG from early 2020](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/87645) to date\r\n* kubernetes member [since early 2021](https:\/\/github.com\/kubernetes\/org\/issues\/2560)\r\n* regular member of sig-node-ci subgroup\r\n* contributing to the kubelet mostly focusing on the resource managers (cpu, device, topology) and e2e tests. By proposing myself as reviewe, I wish to step up and keep contributing in these areas but broaden the perspective to other areas of the kubelet\r\n* PRs reviewed: https:\/\/github.com\/kubernetes\/kubernetes\/pulls?q=is%3Apr+reviewed-by%3Affromani+label%3Asig%2Fnode+ \r\n* PRs authored: https:\/\/github.com\/kubernetes\/kubernetes\/pulls?q=is%3Apr+author%3Affromani+label%3Asig%2Fnode+\r\n* KEPs reviewed: https:\/\/github.com\/kubernetes\/enhancements\/pulls?q=is%3Apr+reviewed-by%3Affromani+label%3Asig%2Fnode+\r\n* KEPs authored: https:\/\/github.com\/kubernetes\/enhancements\/pulls?q=is%3Apr+author%3Affromani+label%3Asig%2Fnode+","comments":["\/sig node","Huge +1 (non-binding) ","\/test pull-kubernetes-e2e-kind","+1!","\r\n+1!\r\n\r\n","\/cc @kubernetes\/sig-node-leads ","@pacoxu: GitHub didn't allow me to request PR reviews from the following users: kubernetes\/sig-node-leads.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) and repo collaborators can review this PR, and authors cannot review their own PRs.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123757#issuecomment-1982161639):\n\n>\/cc @kubernetes\/sig-node-leads \n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","LGTM label has been added.  <details>Git tree hash: 428a25875d9ca8931fdbf5ed0f623bc7f36ef50f<\/details>","\/lgtm\r\n\/approve\r\n\r\nThanks @ffromani for your contribution. ","\/assign @thockin \r\n\r\nper bot suggestion","Thanks and congrats!\r\n\r\n\/approve","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123757#issuecomment-1986546434\" title=\"Approved\">dchen1107<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123757#\" title=\"Author self-approved\">ffromani<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123757#pullrequestreview-1925981806\" title=\"LGTM\">SergeyKanzhelev<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123757#issuecomment-1987049161\" title=\"Approved\">thockin<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)~~ [thockin]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","+1","\/triage accepted\r\n+1"],"labels":["lgtm","sig\/node","size\/XS","kind\/feature","release-note-none","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"Sync `cri-api` approvers with kubelet `cri` package","body":"\r\n\r\n#### What type of PR is this?\r\n\r\n\r\n\/kind cleanup\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nWe now move the SIG Node (and CRI) approvers to be able to approve `pkg\/kubelet\/cri\/remote` PRs.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nNone\r\n#### Special notes for your reviewer:\r\nPTAL @kubernetes\/sig-node-leads @kubernetes\/sig-node-pr-reviews \r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123756#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [feiskyer](https:\/\/github.com\/feiskyer) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cri\/remote\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cri\/remote\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"feiskyer\"]} -->","\/lgtm\r\n\/assign @mrunalp","LGTM label has been added.  <details>Git tree hash: 5e3cda88e9e8f9f83b98a172973177c4a146075a<\/details>","\/triage accepted\r\n\/priority important-longterm"],"labels":["area\/kubelet","kind\/cleanup","lgtm","sig\/node","size\/XS","release-note-none","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"[KEP2400] Avoid logging that swap cgroup controller is missing for every container","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nBefore this PR, if swap FG is enabled but cgroup swap controller is missing, kubelet would add a log entry for every container saying `No swap cgroup controller present`.\r\n\r\nIn this PR this log entry is deleted. A log entry would still fire up only once, when the kubelet would first try to configure swap resources, and no more.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123728\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-node\/2400-node-swap\/README.md\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123749#\" title=\"Author self-approved\">iholder101<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [yujuhong](https:\/\/github.com\/yujuhong) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"yujuhong\"]} -->","\/sig node","\/triage accepted\r\n\/priority backlog","\/priority important-longterm\r\n\r\nraising because https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123728#issuecomment-1981181318","@liggitt You asked for this PR as a follow up. I realize I wasn't sure if you meant you wanted this in 1.30 or that we should do this before GA?\r\n\r\nEither way, I don't think this warrants an exception but wanted your thoughts on priority?","> Before this PR, if swap FG is enabled but cgroup swap controller is missing, kubelet would add a log entry for every container saying `No swap cgroup controller present`.\r\n\r\nMy read of the code is that this log entry existed outside the feature gate already, so I don't think this *has* to be in 1.30, but I wouldn't object to it being included. Will defer to node leads to make the call","\/cc @mrunalp @dchen1107 ","\/lgtm\r\n","LGTM label has been added.  <details>Git tree hash: acaa9fe73132f802eaca12f8b29ebf68f57c2bc5<\/details>","ping @mrunalp @dchen1107 \r\n\r\nDo we want this in for 1.30?"],"labels":["priority\/backlog","area\/kubelet","lgtm","sig\/node","size\/XS","kind\/feature","release-note-none","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"Removed Useless `AddFlags` statements from options.go","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nWhile triaging your project, our bug fixing tool generated the following message:\r\n> \"In file [options.go](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/app\/options\/options.go#L247-L248) there is an expression that has no effect and is used in a void context is most likely redundant and may indicate a bug.\"\r\n\r\nAfter manually triaging your project, we found out that the lines\r\n```go\r\n\ts.DaemonSetController.AddFlags(fss.FlagSet(names.DaemonSetController))\r\n\ts.DeprecatedFlags.AddFlags(fss.FlagSet(\"deprecated\"))\r\n```\r\nhave no effects at all since there implementations look like below:\r\n```go\r\nfunc (o *DaemonSetControllerOptions) AddFlags(fs *pflag.FlagSet) {\r\n\tif o == nil {\r\n\t\treturn\r\n\t}\r\n}\r\n```\r\n```go\r\nfunc (o *DeprecatedControllerOptions) AddFlags(fs *pflag.FlagSet) {\r\n\tif o == nil {\r\n\t\treturn\r\n\t}\r\n}\r\n```\r\n\r\nWhich is why I have removed the statements from the options.go file as a cleanup procedure.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #None \r\n\r\n#### Special notes for your reviewer:\r\nNONE\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\nNONE\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n\r\n## Sponsorship and Support\r\nThis work is done by the security researchers from OpenRefactory and is supported by the [Open Source Security Foundation (OpenSSF)](https:\/\/openssf.org\/): [Project Alpha-Omega](https:\/\/alpha-omega.dev\/). Alpha-Omega is a project partnering with open source software project maintainers to systematically find new, as-yet-undiscovered vulnerabilities in open source code - and get them fixed \u2013 to improve global software supply chain security.\r\n\r\nThe bug is found by running CodeQL by OpenRefactory and then manually triaging the results.","comments":["Hi @fazledyn-or. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123748#\" title=\"Author self-approved\">fazledyn-or<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mikedanese](https:\/\/github.com\/mikedanese) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mikedanese\"]} -->","\/ok-to-test\r\n\/assign @jpbetz \r\nfor KCM\r\nThank you.\r\n\/triage accepted","@fazledyn-or: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-kind | 93a9da1f05d79a172fd9836dbc6f8ea03fb169f7 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123748\/pull-kubernetes-e2e-kind\/1767644164582805504) | true | `\/test pull-kubernetes-e2e-kind`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123748). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Afazledyn-or). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["kind\/cleanup","sig\/api-machinery","size\/XS","release-note-none","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"pod topologySpreadConstraints nodeAffinityPolicy: Ignore not work\uff1f","body":"### What happened?\r\n\r\ncrd\r\n```\r\napiVersion: apps\/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: myapp-deployment\r\nspec:\r\n  replicas: 6\r\n  selector:\r\n    matchLabels:\r\n      app: myapp\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: myapp\r\n    spec:\r\n      containers:\r\n      - name: myapp-container\r\n        image: nginx\r\n```\r\n\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: web-server\r\n  labels:\r\n    app: myapp\r\nspec:\r\n  containers:\r\n  - name: web-server\r\n    image: nginx\r\n  affinity:\r\n    nodeAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n        nodeSelectorTerms:\r\n        - matchExpressions:\r\n          - key: node-type\r\n            operator: In\r\n            values:\r\n            - web\r\n  topologySpreadConstraints:\r\n  - maxSkew: 1\r\n    topologyKey: kubernetes.io\/hostname\r\n    whenUnsatisfiable: DoNotSchedule\r\n    labelSelector:\r\n      matchLabels:\r\n        app: myapp\r\n    nodeAffinityPolicy: Ignore\r\n```\r\n\r\n```\r\nroot@29-b:~# kubectl get pod -n k8s  -owide -w\r\nNAME                                READY   STATUS    RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES\r\nmyapp-deployment-7546cdb555-d55cf   1\/1     Running   0          57m   10.0.1.118   worker-03   <none>           <none>\r\nmyapp-deployment-7546cdb555-dt4vs   1\/1     Running   0          57m   10.0.0.129   worker-01   <none>           <none>\r\nmyapp-deployment-7546cdb555-j9v5z   1\/1     Running   0          57m   10.0.1.61    worker-03   <none>           <none>\r\nmyapp-deployment-7546cdb555-kc9zf   1\/1     Running   0          57m   10.0.3.79    master-01   <none>           <none>\r\nmyapp-deployment-7546cdb555-tzbnj   1\/1     Running   0          57m   10.0.2.182   worker-02   <none>           <none>\r\nmyapp-deployment-7546cdb555-zjmsw   1\/1     Running   0          57m   10.0.0.94    worker-01   <none>           <none>\r\nweb-server                          0\/1     Pending   0          12m   <none>       <none>      <none>           <none>\r\n```\r\n\r\n2m16s       Warning   FailedScheduling    pod\/web-server                           0\/4 nodes are available: 4 node(s) didn't match Pod's node affinity\/selector. preemption: 0\/4 nodes are available: 4 Preemption is not helpful for scheduling.\r\n\r\n\r\nnodeTaintsPolicy has the same problem\r\n\r\n### What did you expect to happen?\r\n\r\nignore nodeaffinity\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\napply my crd,node without that label\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n^Croot@29-b:~ kubectl versionon\r\nClient Version: v1.29.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.0\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nvmware\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\nroot@29-b:~# cat \/etc\/os-release\r\nPRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nUBUNTU_CODENAME=jammy\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig scheduling"],"labels":["kind\/bug","sig\/scheduling","needs-triage"]},{"title":"[PSA] Potential pitfalls with dependency bumps ","body":"_Note: this issue is only meant to document the situation that currently exists because discussions in slack are ephemeral and hard to discover.\r\n\r\n## TL;DR\r\n\r\nhttps:\/\/github.com\/golang\/go\/issues\/65573 is a proposal accepted and targeting Go 1.23 that will address the issue being described here! \r\n\r\nIn the meantime, please exercise extra caution in the following scenarios:\r\n- Backporting dependency changes - please check what the `go` directives are in these dependencies before we backport them.\r\n- Upgrading `go` directives in community owned, single branch dependencies such as k\/utils, k\/gengo, k\/kube-openapi etc. \r\n\r\n## Context\r\n\r\ngo1.21 introduced 2 immensely helpful features: \r\n\r\n1. Default `GODEBUG` settings are now taken based on the `go` directive in `go.mod`s (ref: https:\/\/go.dev\/doc\/godebug). This is particularly helpful to maintain compatibility behaviour when Go makes breaking(-but-compatible) changes as part of its minor releases. This is also especially important since Kubernetes now bumps minor versions of Go on its release branches: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-release\/3744-stay-on-supported-go-versions \r\n2. The `go` directive in `go.mod` now specifies the _minimum_ go version required to build your own code. We couldn't do this before, and that meant older go toolchains were allowed to compile code that was meant for newer go versions, even though it would lead to breaking changes: https:\/\/go.dev\/blog\/toolchain \r\n\r\n## The Problem\r\n\r\nWhile (1) is highly desirable, (2) interferes with its efficacy of being able to default `GODEBUG` settings, illustrated below.\r\n\r\nThe currently supported release branches of Kubernetes have their versions of Go bumped to go1.21.x (same as `master`). What this means is code is tested, built and released using go1.21.x:\r\n- [Ref for K8s 1.27](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/b9e2ad67ad146db566be5a6db140d47e52c8adb2\/build\/dependencies.yaml#L97-L98)\r\n- [Ref for K8s 1.28](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/c8dcb00be9961ec36d141d2e4103f85f92bcf291\/build\/dependencies.yaml#L116-L117)\r\n- [Ref for K8s 1.29](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/4b8e819355d791d96b7e9d9efe4cbafae2311c88\/build\/dependencies.yaml#L120-L121)\r\n\r\nHowever, the `go` directive in `go.mod` is as follows for the currently supported Kubernetes versions:\r\n- [K8s 1.27](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/b9e2ad67ad146db566be5a6db140d47e52c8adb2\/go.mod#L9): `go 1.20`\r\n- [K8s 1.28](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/c8dcb00be9961ec36d141d2e4103f85f92bcf291\/go.mod#L9): `go 1.20`\r\n- [K8s 1.29](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/4b8e819355d791d96b7e9d9efe4cbafae2311c88\/go.mod#L9): `go 1.21`\r\n\r\nEssentially, we leave the `go` directive in the `go.mod`s to indicate the Go version with which a Kubernetes minor was first shipped. Because of go1.21's `GODEBUG` defaulting, we get to preserve that behaviour for our users even though we now continue to _build_ with higher versions of Go (to stay well within the support window of Go). \r\n\r\nHowever, [many of the fixes](https:\/\/github.com\/kubernetes\/release\/issues\/2815#issuecomment-1373891562) needed to bump minor versions of Go on release branches involves backporting dependency bumps, for example:\r\n\r\nIf dependency `D` has a `go` directive in its `go.mod` of `go 1.21`, was bumped at `master` to fix a bug and we need to backport this to our release branches that has a `go` directive of `go 1.20`, the bump will essentially force the `go` directive in our release branches to `go 1.21` because of (2) - the `go` directive required is going to be the `max(all dep. go directives)`. This is not desirable since we now also loose the compatibility behaviour of go1.20 that our users would expect.\r\n\r\n## What Do We Do?\r\n\r\nhttps:\/\/github.com\/golang\/go\/issues\/65573 is a proposal that is being worked on in the Go community to address this dichotomy between (1) and (2) and will hopefully land in go1.23.\r\n\r\nIn the meantime, we need to exercise extra caution in the following scenarios:\r\n- Backporting dependency changes - please check what the `go` directives are in these dependencies before we backport them.\r\n- Upgrading `go` directives in community owned, single branch dependencies such as k\/utils, k\/gengo, k\/kube-openapi etc. If we bump the `go` directive here and later need to absorb a fix in one of these dependencies into k\/k via backporting, we will run into the same issue.\r\n  - I bit the bullet on this one already: https:\/\/github.com\/kubernetes\/utils\/pull\/304, thanks to @liggitt for catching that error!\r\n\r\nThere is also work happening to safeguard accidental bumps:\r\n- https:\/\/github.com\/kubernetes\/kube-openapi\/issues\/463\r\n- https:\/\/github.com\/kubernetes\/gengo\/issues\/264\r\n- https:\/\/github.com\/kubernetes\/utils\/issues\/305\r\n\r\n\/sig architecture\r\n\/area code-organization\r\n\/triage accepted\r\n\/cc @kubernetes\/dep-approvers  ","comments":["Fantastic write-up, thanks for laying everything out so clearly","Thank you for the write up. This will be really helpful when we engage with OSS projects asking to bump go versions back down where possible.","Definitely appreciate the explanation and path to resolution.  ","\ud83d\udcaf Thank you very much!","updated description, the Go proposal was accepted and is targeting Go 1.23"],"labels":["sig\/architecture","area\/code-organization","triage\/accepted"]},{"title":"ReadWriteMany access mode should not be allowed with hostPath volumes.","body":"### What happened?\n\nWe know that `hostPath` doesn't support `ReadWriteMany` access mode from the docs[1]. But if we attempt to create a PV and PVC with `hostPath` option enabled and the access mode with `ReadWriteMany`. Kubernetes allows this to be created but when the end goal is to have a common volume shared by many pods of a deployment running on multiple nodes. This setup causes confusion to the users, as they assumed everything is setup perfectly since there was no error while creating the PV and PVC. \r\nWhen the user tries to write some files from one of the pods, the data will not be accessible to the other pods, since it is mounting the `hostpath` and writing onto it.\r\n\r\nThis was observed with deployment that was deployed on GKE.\r\n \r\n[1] https:\/\/kubernetes.io\/docs\/concepts\/storage\/persistent-volumes\/#access-modes:~:text=%2D-,HostPath,-%E2%9C%93`\n\n### What did you expect to happen?\n\nKubernetes should not allow creating PV's with `hostPath` option enabled along with Access mode set to `ReadWriteMany`. We should provide some kind of error message or have a web-hook that blocks the creation of these PV's.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n- Create a PV and PVC with following configuration.\r\n\r\n```\r\n--- PV---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: test-pv\r\nspec:\r\n  storageClassName: standard\r\n  capacity:\r\n    storage: 10Gi\r\n  accessModes:\r\n    - ReadWriteMany\r\n  persistentVolumeReclaimPolicy: Retain\r\n  hostPath:\r\n    path: \"\/tmp\/data\"\r\n\r\n---PVC---\r\napiVersion: v1\r\nkind: PersistentVolumeClaim\r\nmetadata:\r\n  name: test-pvc\r\nspec:\r\n  storageClassName: standard\r\n  accessModes:\r\n    - ReadWriteMany\r\n  resources:\r\n    requests:\r\n      storage: 10Gi\r\n  volumeName: test-pv\r\n\r\n``` \r\n\r\n- Create a deployment that mounts the above volume at `\/tmp\/data` path like below\r\n```\r\nvolumeMounts:\r\n        - name: test-data\r\n          mountPath: \/tmp\/data\r\n      volumes:\r\n        - name: test-data\r\n          persistentVolumeClaim:\r\n            claimName: test-pvc\r\n```\r\n\r\n- This will be successfully created but the user will not be able to access the shared volume since the `hostPath` option is mounting the host filesystem path of the node the pod is running. \r\n\r\n\r\n\r\n\r\n\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"27+\", GitVersion:\"v1.27.9-dispatcher\", GitCommit:\"8b508a33aafcd3ba51641b6b2ef203adbdd9de1e\", GitTreeState:\"clean\", BuildDate:\"2023-12-21T23:22:51Z\", GoVersion:\"go1.20.12\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.13-gke.1052000\", GitCommit:\"c88fda4c3b7ef204c20602c368811d8ba2fdd2e6\", GitTreeState:\"clean\", BuildDate:\"2024-01-23T17:45:47Z\", GoVersion:\"go1.20.13 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nGoogle Cloud Platform \r\nGKE\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nPRETTY_NAME=\"Debian GNU\/Linux 11 (bullseye)\"\r\nNAME=\"Debian GNU\/Linux\"\r\nVERSION_ID=\"11\"\r\nVERSION=\"11 (bullseye)\"\r\nVERSION_CODENAME=bullseye\r\nID=debian\r\nHOME_URL=\"https:\/\/www.debian.org\/\"\r\nSUPPORT_URL=\"https:\/\/www.debian.org\/support\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.debian.org\/\"\r\n\r\n$ uname -a\r\nLinux cs-779332008923-default 6.1.58+ #1 SMP PREEMPT_DYNAMIC Mon Jan 29 15:19:25 UTC 2024 x86_64 GNU\/Linux\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig storage\r\n"],"labels":["kind\/bug","sig\/storage","needs-triage"]},{"title":"fetch npd from github releases","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nStops depending on NPD being pushed to gs:\/\/kubernetes-release (the bucket currently behind dl.k8s.io, still in google.com instead of kubernetes.io GCP) by a Googler that happens to still have direct push access. Fetches from github releases instead.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\r\n\/sig testing node\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123741#\" title=\"Author self-approved\">BenTheElder<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[cluster\/gce\/gci\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/gci\/OWNERS)~~ [BenTheElder]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/lgtm\r\n\r\n\/cc @Random-Liu @vteratipally @hakman","\/lgtm cancel\r\n\r\nJust a nit for the URL","@BenTheElder artifact promotion may also be a good idea in the future (similar to https:\/\/github.com\/kubernetes\/k8s.io\/pull\/6368).","@hakman I don't believe the project should keep hosting subproject binaries ourselves. It's an unnecessary expense and infra. Discussion in #sig-k8s-infra","Users will have to switch anyhow as it won't be gs:\/\/kubernetes-release and NPD has been publishing binaries to github already.","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["kind\/cleanup","area\/provider\/gcp","sig\/node","needs-rebase","size\/XS","release-note-none","approved","cncf-cla: yes","sig\/testing","do-not-merge\/hold","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"Bump npd from v0.8.16 to v0.8.17","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n[change log about v0.8.17](https:\/\/github.com\/kubernetes\/node-problem-detector\/releases\/tag\/v0.8.17)\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nnone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nnone\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123740#\" title=\"Author self-approved\">bzsuni<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[build\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/build\/OWNERS)**\n- **[cluster\/addons\/node-problem-detector\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/addons\/node-problem-detector\/OWNERS)**\n- **[cluster\/gce\/gci\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cluster\/gce\/gci\/OWNERS)**\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n- **[test\/kubemark\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/kubemark\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","Waiting for https:\/\/github.com\/kubernetes\/k8s.io\/pull\/6523","NPD v0.8.17 has been released.","\/test pull-kubernetes-e2e-gce ","\/lgtm","LGTM label has been added.  <details>Git tree hash: 72084e7c50e6a6031b5e648140b78a0bc3de9735<\/details>","cc @pacoxu ","Code freeze will be end until v1.30 release(planned in April 17th). "],"labels":["area\/test","kind\/cleanup","sig\/scalability","lgtm","area\/provider\/gcp","sig\/node","size\/S","release-note-none","cncf-cla: yes","sig\/testing","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"Cleanup manual conversions","body":"These might have been needed in the past, but no longer.  The ones that are left are a bad situation.\r\n\r\nWhen we do codegen we: a) remove generated files; b) set a build tag to ignore generated files.  Then conversion-gen tool searches for specially-named conversion functions.  When there is a cross-API dependency, it can't find the functions (because (a) and (b) above) so they generate a compile error.  We should do better.  For now, docs++.\r\n\r\n\/kind cleanup\r\n\r\n```release-note\r\nNONE\r\n```\r\n","comments":["\/retest\r\n",">  When there is a cross-API dependency, it can't find the functions (because (a) and (b) above) so they generate a compile error. \r\n\r\nI haven't looked deeply into conversion-gen, so here are some assumptions I'm making (forgive me if its an oversimplification):\r\n\r\n1. Generates a list of all GVKs (and their runtime type)\r\n2. Pair all GVKs with all other GVKs of the same GroupKind (to find the list of necessary conversions)\r\n3. Generate autoConvert_* and registration for all GVK pairs\r\n4. Find all manually implemented conversion functions\r\n6. Filter out all manually implemented conversions functions from the list of GVK pairs\r\n7. Generate Convert_* for remaining GVK pairs\r\n\r\nIIUC the issue is that to determine # 4 the codebase needs to be compilable. This fails due to missing symbol references after removing generated code and enabling a build tag.\r\n\r\nI'm surprised that we need a valid full build for this. I would expect it to be possible to perform a best-effort typecheck (where type check\/symbol errors are repalced with an Error AST node, or similar).\r\n Is that not possible in gengo\r\n","LGTM label has been added.  <details>Git tree hash: 9b50a1beae8bae297b935201a03dcf9be5824355<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123736#pullrequestreview-1921028057\" title=\"LGTM\">alexzielenski<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123736#\" title=\"Author self-approved\">thockin<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)~~ [thockin]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","> IIUC the issue is that to determine # 4 the codebase needs to be compilable. This fails due to missing symbol references after removing generated code and enabling a build tag.\r\n\r\nMore than that, we always exclude generated code when generating code.  Fundamentally, it's (IMO) sloppy and horrible that we just search for functions with the right name and fingerprint.  I think it would be better to do something like:\r\n* Types which have manual conversions say so with a tag.\r\n* Conversion-gen doesn't need the function to exist in order to \"find it\" - it just emits calls to the manual function or the named-by-convention function.\r\n\r\nEdit: a few notes\r\n\r\nConsider the case of generating for (e.g.) `storage.VolumeAttachmentSource`, which contains a reference to `core.PersistentVolumeSpec`.  `genConversion.GenerateType()` calls `generateConversion(t, peerType)` and `generateConversion(peerType, t)`. At that point in time we know which package we are generating into (storage) and which packages the types in question come from (core and core\/v1).  We do not actually know which of those two names is the \"internal\" name, except by pattern-matching (gross).\r\n\r\nWe can't keep a lookaside to know which types have already been generated to track which are internal, because we can't (I think) be sure that the dep will already have been handled - the sort is alphabetic, not topological.\r\n\r\nSo, in the absence of extra information, this is harder than I made it sound.","\/triage accepted"],"labels":["kind\/cleanup","lgtm","size\/M","release-note-none","sig\/auth","sig\/apps","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"KEP-4176: Add a new static policy SpreadPhysicalCPUsPreferredOption","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nto implement https:\/\/github.com\/kubernetes\/enhancements\/issues\/4176\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/issues\/4176\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nUser can choose a different static policy option `SpreadPhysicalCPUsPreferredOption` to spread cpus across physical cpus for some specific applications\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/issues\/4176\r\n- [Usage]: https:\/\/github.com\/kubernetes\/website\/pull\/45216\r\n- [Other doc]: [<link>](https:\/\/github.com\/kubernetes\/website\/pull\/45217)\r\n```\r\n","comments":["\/triage accepted\r\n\/priority important-longterm","\/cc","\/cc @ffromani @klueska \r\n\r\nRecently I've encountered some internal priorities that required my attention, which regrettably slowed down my progress on this story. Sorry for leaving limited time for you to help review the codes. ","> \/cc @ffromani @klueska\r\n> \r\n> Recently I've encountered some internal priorities that required my attention, which regrettably slowed down my progress on this story. Sorry for leaving limited time for you to help review the codes.\r\n\r\nI'll have a look today, but please be aware the code freeze is already in effect when I'm writing this.","@Jeffwan: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-conformance-image-test | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-conformance-image-test\/1765527046823350272) | false | `\/test pull-kubernetes-conformance-image-test`\npull-kubernetes-kind-dra | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-kind-dra\/1765527062535213056) | false | `\/test pull-kubernetes-kind-dra`\npull-kubernetes-e2e-gci-gce-ipvs | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-gci-gce-ipvs\/1765527060853297152) | false | `\/test pull-kubernetes-e2e-gci-gce-ipvs`\npull-kubernetes-e2e-capz-windows-master | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-capz-windows-master\/1765527054364708864) | false | `\/test pull-kubernetes-e2e-capz-windows-master`\npull-kubernetes-e2e-gci-gce-autoscaling | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-gci-gce-autoscaling\/1765527056046624768) | false | `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\npull-kubernetes-conformance-kind-ipv6-parallel | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-conformance-kind-ipv6-parallel\/1765527046886264832) | false | `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\npull-kubernetes-e2e-autoscaling-hpa-cm | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-autoscaling-hpa-cm\/1765527058768728064) | false | `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\npull-kubernetes-local-e2e | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-local-e2e\/1765527051843932160) | false | `\/test pull-kubernetes-local-e2e`\npull-kubernetes-e2e-autoscaling-hpa-cpu | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-autoscaling-hpa-cpu\/1765527057502048256) | false | `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\npull-kubernetes-e2e-kind-kms | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-kind-kms\/1765527055190986752) | false | `\/test pull-kubernetes-e2e-kind-kms`\npull-publishing-bot-validate | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-publishing-bot-validate\/1765527053517459456) | false | `\/test pull-publishing-bot-validate`\npull-kubernetes-e2e-storage-kind-disruptive | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-storage-kind-disruptive\/1765527068419821568) | false | `\/test pull-kubernetes-e2e-storage-kind-disruptive`\ncheck-dependency-stats | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/check-dependency-stats\/1765527048484294656) | false | `\/test check-dependency-stats`\npull-kubernetes-e2e-gci-gce-ingress | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-gci-gce-ingress\/1765527059200741376) | false | `\/test pull-kubernetes-e2e-gci-gce-ingress`\npull-kubernetes-cross | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-cross\/1765527047649628160) | false | `\/test pull-kubernetes-cross`\npull-kubernetes-e2e-gce-providerless | 8ebc40dd11ae313a4fa9d788800ae4bc6b384903 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123733\/pull-kubernetes-e2e-gce-providerless\/1765527047205031936) | false | `\/test pull-kubernetes-e2e-gce-providerless`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123733). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3AJeffwan). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123733#\" title=\"Author self-approved\">Jeffwan<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [derekwaynecarr](https:\/\/github.com\/derekwaynecarr) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cm\/cpumanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/cpumanager\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"derekwaynecarr\"]} -->","@ffromani I carelessly rebase the remote and generate lots of unrelated messages and it has been fixed. Please ignore them. Could you take another look? I follow your advice to refactor the code and add more unit tests to cover the code I touched.  I didn't add integration tests at this moment and did local manual e2e experiment instead.\r\n\r\nIf it meet the production quality, I will reach out to release team to see whether it's still possible to extend it, If not, let's wait for next cycle.\r\n\r\n\/cc @klueska \r\n","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","> @ffromani I carelessly rebase the remote and generate lots of unrelated messages and it has been fixed. Please ignore them. Could you take another look? I follow your advice to refactor the code and add more unit tests to cover the code I touched. I didn't add integration tests at this moment and did local manual e2e experiment instead.\r\n> \r\n> If it meet the production quality, I will reach out to release team to see whether it's still possible to extend it, If not, let's wait for next cycle.\r\n> \r\n> \/cc @klueska\r\n\r\nsure, I'll have another look ASAP","\/cc","\/remove-area apiserver\r\n\/remove-area cloudprovider\r\n\/remove-area code-generation\r\n\/remove-area conformance\r\n\/remove-area dependency\r\n\/remove-area e2e-test-framework\r\n\/remove-area kube-proxy\r\n\/remove-area kubectl\r\n\/remove-area provider\/gcp\r\n\/remove-area release-eng\r\n\/remove-sig api-machinery\r\n\/remove-sig apps\r\n\/remove-sig architecture\r\n\/remove-sig auth\r\n\/remove-sig autoscaling\r\n\/remove-sig cli\r\n\/remove-sig cloud-provider\r\n\/remove-sig instrumentation\r\n\/remove-sig network\r\n\/remove-sig release\r\n\/remove-sig scalability\r\n\/remove-sig scheduling\r\n\/remove-sig storage\r\n\/remove-wg structured-logging"],"labels":["area\/test","area\/kubelet","sig\/node","release-note","size\/L","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/testing","priority\/important-longterm","triage\/accepted"]},{"title":"DRA: scheduler: index claim and class parameters to simplify lookup","body":"### What would you like to be added?\n\n`lookupClassParameters` and `lookupClaimParameters` currently iterate over all objects in the informer cache to find the one which was generated for the vendor parameter object. This should use an indexer.\r\n\r\n\/sig node\r\n\/priority backlog-longterm\r\n\/triage accepted\r\n\/lifecycle frozen\r\n\n\n### Why is this needed?\n\nPerformance.\r\n","comments":["@pohly: The label(s) `priority\/backlog-longterm` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123731):\n\n>### What would you like to be added?\n>\n>`lookupClassParameters` and `lookupClaimParameters` currently iterate over all objects in the informer cache to find the one which was generated for the vendor parameter object. This should use an indexer.\r\n>\r\n>\/sig node\r\n>\/priority backlog-longterm\r\n>\/triage accepted\r\n>\/lifecycle frozen\r\n>\n>\n>### Why is this needed?\n>\n>Performance.\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","I'd like to do it, can I pick it up? @pohly","Sure, go for it. Beware that https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516 needs to get merged first.\r\n\r\n\/assign @carlory "],"labels":["sig\/node","kind\/feature","lifecycle\/frozen","triage\/accepted"]},{"title":"[KEP-2400] Only log swapControllerAvailable at startup","body":"              it still looks like we'll be logging `klog.InfoS(\"No swap cgroup controller present\"` for every container for the whole kubelet lifetime... that seems like log spam, right?\r\n\r\n_Originally posted by @liggitt in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122745#discussion_r1513387636_\r\n            ","comments":["We want to only log this once on kubelet startup. Spamming the logs for every container\/pod is not really that useful.\r\n\r\n","\/sig node\r\n","\/triage accepted\r\n\/priority important-longterm\r\n\r\nIIUC this is part of the KEP graduation process (otherwise I'll have probably set priority backlog)"],"labels":["sig\/node","priority\/important-longterm","triage\/accepted"]},{"title":"Changes for update loadbalancer.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @princepereira. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123725#\" title=\"Author self-approved\">princepereira<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/proxy\/winkernel\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/pkg\/proxy\/winkernel\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->"],"labels":["sig\/network","size\/L","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","do-not-merge\/cherry-pick-not-approved","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"kubelet: fix slow dra unit test","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\ndra's unit tests are running slowly, it takes about 100 seconds to complete, we should make it run faster\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes  part of #123685\r\n\r\n#### Special notes for your reviewer:\r\nChange the constant `PluginClientTimeout` to a variable so that we can override it in our unit tests and reset its value after the change is made.\r\n\r\ntime required before modification: \r\n```\r\ngo test .\/... -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\t94.097s\tcoverage: 83.8% of statements\r\n```\r\ntime required after modification\r\n```\r\ngo test .\/... -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\t1.356s\tcoverage: 83.8% of statements\r\n```\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/triage accepted\r\n\/priority important-longterm\r\n\/cc @klueska ","\/test pull-kubernetes-node-e2e-crio-dra","Can you follow the guide in https:\/\/gist.github.com\/liggitt\/6a3a2217fa5f846b52519acfc0ffece0 to run `stress` on this UT to make sure it not flakes after we shorten the timeout?","I ran 20,000 tests using stress and it performed well in my local environment.\r\n\r\nBut the CPU of the CI environment may be weaker than my local computer (may cause flakes), I adjusted the `PluginClientTimeout` and `timeout` to 20ms and 40ms respectively.\r\n```\r\ngo test .\/... -race -c                         \r\n\u279c  dra \r\n\u279c  dra stress .\/dra.test             \r\n5s: 64 runs so far, 0 failures\r\n10s: 128 runs so far, 0 failures\r\n15s: 208 runs so far, 0 failures\r\n20s: 276 runs so far, 0 failures\r\n25s: 352 runs so far, 0 failures\r\n30s: 424 runs so far, 0 failures\r\n35s: 496 runs so far, 0 failures\r\n40s: 576 runs so far, 0 failures\r\n45s: 644 runs so far, 0 failures\r\n50s: 720 runs so far, 0 failures\r\n55s: 792 runs so far, 0 failures\r\n1m0s: 864 runs so far, 0 failures\r\n1m5s: 943 runs so far, 0 failures\r\n1m10s: 1012 runs so far, 0 failures\r\n1m15s: 1088 runs so far, 0 failures\r\n1m20s: 1161 runs so far, 0 failures\r\n1m25s: 1232 runs so far, 0 failures\r\n1m30s: 1309 runs so far, 0 failures\r\n1m35s: 1380 runs so far, 0 failures\r\n1m40s: 1456 runs so far, 0 failures\r\n1m45s: 1529 runs so far, 0 failures\r\n1m50s: 1600 runs so far, 0 failures\r\n1m55s: 1678 runs so far, 0 failures\r\n2m0s: 1749 runs so far, 0 failures\r\n2m5s: 1824 runs so far, 0 failures\r\n2m10s: 1899 runs so far, 0 failures\r\n2m15s: 1970 runs so far, 0 failures\r\n2m20s: 2047 runs so far, 0 failures\r\n2m25s: 2117 runs so far, 0 failures\r\n2m30s: 2192 runs so far, 0 failures\r\n2m35s: 2267 runs so far, 0 failures\r\n2m40s: 2338 runs so far, 0 failures\r\n2m45s: 2416 runs so far, 0 failures\r\n2m50s: 2485 runs so far, 0 failures\r\n2m55s: 2560 runs so far, 0 failures\r\n3m0s: 2635 runs so far, 0 failures\r\n3m5s: 2707 runs so far, 0 failures\r\n3m10s: 2784 runs so far, 0 failures\r\n3m15s: 2855 runs so far, 0 failures\r\n3m20s: 2928 runs so far, 0 failures\r\n3m25s: 3003 runs so far, 0 failures\r\n3m30s: 3075 runs so far, 0 failures\r\n3m35s: 3152 runs so far, 0 failures\r\n3m40s: 3223 runs so far, 0 failures\r\n3m45s: 3296 runs so far, 0 failures\r\n3m50s: 3371 runs so far, 0 failures\r\n3m55s: 3445 runs so far, 0 failures\r\n4m0s: 3520 runs so far, 0 failures\r\n\r\n.........\r\n\r\n23m30s: 20755 runs so far, 0 failures\r\n23m35s: 20829 runs so far, 0 failures\r\n23m40s: 20902 runs so far, 0 failures\r\n23m45s: 20976 runs so far, 0 failures\r\n23m50s: 21049 runs so far, 0 failures\r\n23m55s: 21123 runs so far, 0 failures\r\n24m0s: 21197 runs so far, 0 failures\r\n24m5s: 21270 runs so far, 0 failures\r\n24m10s: 21344 runs so far, 0 failures\r\n24m15s: 21418 runs so far, 0 failures\r\n24m20s: 21491 runs so far, 0 failures\r\n24m25s: 21565 runs so far, 0 failures\r\n24m30s: 21638 runs so far, 0 failures\r\n24m35s: 21712 runs so far, 0 failures\r\n24m40s: 21786 runs so far, 0 failures\r\n24m45s: 21861 runs so far, 0 failures\r\n24m50s: 21935 runs so far, 0 failures\r\n24m55s: 22007 runs so far, 0 failures\r\n25m0s: 22082 runs so far, 0 failures\r\n25m5s: 22154 runs so far, 0 failures\r\n25m10s: 22229 runs so far, 0 failures\r\n25m15s: 22303 runs so far, 0 failures\r\n25m20s: 22376 runs so far, 0 failures\r\n25m25s: 22450 runs so far, 0 failures\r\n25m30s: 22524 runs so far, 0 failures\r\n25m35s: 22598 runs so far, 0 failures\r\n25m40s: 22672 runs so far, 0 failures\r\n25m45s: 22744 runs so far, 0 failures\r\n25m50s: 22819 runs so far, 0 failures\r\n25m55s: 22893 runs so far, 0 failures\r\n26m0s: 22966 runs so far, 0 failures\r\n26m5s: 23040 runs so far, 0 failures\r\n26m10s: 23114 runs so far, 0 failures\r\n\r\n```","LGTM label has been added.  <details>Git tree hash: 31f7947525deb2c5956d183dcf303939a77378ee<\/details>","\/assign @yujuhong @klueska","\/cc @pohly \r\n\r\n> https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850#issuecomment-1988422238  We have a 3 minute timeout for a package.  \r\n> https:\/\/github.com\/kubernetes\/kubernetes\/blob\/ebc1a7b7fb52f30021b7e66f77070dac8e6b839b\/hack\/make-rules\/test.sh#L60-L64\r\n\r\nBTW,  dra is around 3 mins now and may hit this problem later(when adding new UTs)","If we have unit tests that are slow because they wait for a timeout, can't we run them in parallel?\r\n","> If we have unit tests that are slow because they wait for a timeout, can't we run them in parallel?\r\n\r\nWe can run test cases in parallel using `t.Parallel()`.\r\nDo you mean running all packages in parallel?\r\n","The package needs to support running its own tests in parallel. `pkg\/kubelet\/cm\/dra` doesn't and is slow because it contains several tests which need to time out before they finish. If those ran in parallel, the overall runtime would be about the same as that timeout.","I'm working on it.\r\n","> go test .\/... -race -c                         \r\n> \u279c  dra \r\n> \u279c  dra stress .\/dra.test\r\n\r\n@HirazawaUi: this doesn't work as expected. What happens is that the first invocation populates the unit test cache and then all following invocations finish immediately without actually running the tests anew.\r\n\r\nYou have to use `stress .\/dra.test -test.count=1` to disable the cache.\r\n","https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123892 parallelizes the test.\r\n\r\nWe can still think about shortening the timeout, but I prefer to start with that other PR.\r\n","> We can still think about shortening the timeout, but I prefer to start with that other PR.\r\n\r\nThe result is still the same.\r\n```\r\ngo test .\/... -race -c  \r\n\u279c  dra stress .\/dra.test -test.count=1\r\n5s: 64 runs so far, 0 failures\r\n10s: 128 runs so far, 0 failures\r\n15s: 199 runs so far, 0 failures\r\n20s: 272 runs so far, 0 failures\r\n25s: 343 runs so far, 0 failures\r\n30s: 416 runs so far, 0 failures\r\n35s: 487 runs so far, 0 failures\r\n\r\n31m50s: 27585 runs so far, 0 failures\r\n31m55s: 27658 runs so far, 0 failures\r\n32m0s: 27729 runs so far, 0 failures\r\n```","> #123892 parallelizes the test.\r\n> \r\n> We can still think about shortening the timeout, but I prefer to start with that other PR.\r\n\r\nIt's still faster to reduce timeouts compared to running tests in parallel, isn't it :)","That depends: there were lots of problems getting the timeout in https:\/\/github.com\/kubernetes\/kubernetes\/blob\/30422d5f2247a68ac3d8d10ca103de4e1535656f\/pkg\/kubelet\/cm\/dra\/manager_test.go#L120 right (https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122703, https:\/\/github.com\/kubernetes\/kubernetes\/pull\/121774). \r\n\r\nI'm now very reluctant to change timing conditions again.\r\n","> I'm now very reluctant to change timing conditions again.\r\n\r\nall right, we can close this PR now. just curious, I want to know why we didn't set `\r\nPluginClientTimeout`\r\n as a kubelet configuration field?",">  I want to know why we didn't set  PluginClientTimeout as a kubelet configuration field?\r\n\r\n@bart0sh: do you know why?\r\n\r\n> all right, we can close this PR now\r\n\r\nLet's keep it open until @bart0sh has commented. If he prefers this PR, then we can also merge this instead.","@pohly I'm ok with decreasing the timeout, but prefer that [my comment](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720\/files#r1520022341) is addressed.","\/test pull-kubernetes-unit","> @pohly I'm ok with decreasing the timeout, but prefer that [my comment](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720\/files#r1520022341) is addressed.\r\n\r\n@bart0sh I have made the modifications as suggested.","\/test pull-kubernetes-e2e-gce","\/lgtm","LGTM label has been added.  <details>Git tree hash: e307d2542a3be788d78534c32a629b8104c9208e<\/details>","\/assign @klueska \r\n\r\nCan you add the approval?\r\n\r\n\/cc @kubernetes\/release-team \r\n\r\nMilestone please :pray: - this prevents a potential test flake.","@pohly: GitHub didn't allow me to request PR reviews from the following users: kubernetes\/release-team.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) and repo collaborators can review this PR, and authors cannot review their own PRs.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720#issuecomment-2011517115):\n\n>\/assign @klueska \r\n>\r\n>Can you add the approval?\r\n>\r\n>\/cc @kubernetes\/release-team \r\n>\r\n>Milestone please :pray: - this prevents a potential test flake.\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/approve","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720#\" title=\"Author self-approved\">HirazawaUi<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720#issuecomment-2015261951\" title=\"Approved\">klueska<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720#pullrequestreview-1925049085\" title=\"LGTM\">pacoxu<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [saad-ali](https:\/\/github.com\/saad-ali) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/kubelet\/cm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/OWNERS)~~ [klueska]\n- ~~[pkg\/kubelet\/pluginmanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/pluginmanager\/OWNERS)~~ [klueska]\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"saad-ali\"]} -->"],"labels":["area\/kubelet","kind\/cleanup","lgtm","sig\/storage","sig\/node","size\/M","release-note-none","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"Upgrade kustomize-in-kubectl to v5.3.0","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\/sig cli\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThe newer kustomize Version adds support for Helm OCI Registries and includes many more features: https:\/\/github.com\/kubernetes-sigs\/kustomize\/releases\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nThis upgrades kustomize-in-kubectl to v5.3.0.\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThis PR was generated using this `.\/hack\/update-kustomize.sh` script.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nUpgrades bundled `kubectl kustomize` to v5.3.0\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @Skaronator! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @Skaronator. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123716#\" title=\"Author self-approved\">Skaronator<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[LICENSES\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/LICENSES\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n- **[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)**\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)**\n- **[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/csi-translation-lib\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/csi-translation-lib\/OWNERS)**\n- **[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)**\n- **[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n- **[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)**\n- **[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"liggitt\"]} -->","Didn't test anything locally. Waiting for the CI to successfully build it (or not)","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["kind\/bug","sig\/network","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","release-note","needs-rebase","size\/L","sig\/auth","sig\/cli","cncf-cla: yes","sig\/instrumentation","needs-ok-to-test","sig\/architecture","do-not-merge\/work-in-progress","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"loadbalancer tests should not assume particular cloud providers do\/don't support particular features","body":"Various tests in `test\/e2e\/network\/` use `e2skipper.SkipUnlessProviderIs()` (or in one case `SkipIfProviderIs`) to limit the platforms they test on.\r\n\r\nThere are three problems with this:\r\n- All cloud providers are now out-of-tree, so keeping these correct now requires cross-tree syncing.\r\n- Skipping all \"minor\" cloud providers by default means that people implementing cloud providers don't get told that they've failed to implement certain functionality, and have no easy way of testing the functionality even if they do implement it.\r\n- The fact that this is necessary at all implies that we have features that may or may not work in any given cluster, where we provide no feedback to the user about whether or not the feature worked.\r\n \r\nFor example, the test \"[It should only allow access from service loadbalancer source ranges\"](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/v1.30.0-alpha.3\/test\/e2e\/network\/loadbalancer.go#L530) is `SkipUnlessProviderIs(\"gce\", \"gke\", \"aws\", \"azure\")`. Assuming that that skip is actually correct, that implies that every other cloud provider doesn't implement this feature, and users who try to use it to protect their Service will just silently lose.\r\n\r\nIdeally, for each of these tests, it should be possible for the test case to figure out, in an entirely platform-independent way, whether the cloud provider supports the feature or not. The obvious way to do this would be to have the cloud provider signal via LoadBalancerStatus. (We obviously can't require providers to signal that they _don't_ implement features they don't know about yet, so this would have to be done in the form of signaling when the provider _did_ implement a particular requested feature.)\r\n\r\n(There are also a few non-loadbalancer tests that use `SkipUnlessProviderIs` too: most of these are just GCE-specific tests which will soon never get run in ordinary e2e and probably should be moved out of tree? The other one I see is \"It should recreate its iptables rules if they are deleted\", which requires a provider in `framework.ProvidersWithSSH`, but that test could be rewritten to use a privileged hostNetwork pod rather than needing ssh...)\r\n\r\n(Noticed while adding cloud-provider-kind testing support, https:\/\/github.com\/kubernetes\/test-infra\/pull\/31659)\r\n\r\n\/sig network\r\n\/sig testing","comments":["yeah, this is a long standing issue, there should be another issues about this but is worth to repeat it\r\n\r\nNevertheless, the goal now is to completely remove them https:\/\/github.com\/kubernetes\/kubernetes\/issues\/122828","\r\n\/triage accepted"],"labels":["sig\/network","sig\/testing","triage\/accepted"]},{"title":"Support authentication for trace configuration","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR introduces authentication for K8s trace configuration, enabling K8s trace spans to be emitted to trace storage backends that require essential authentication (such as an OpenTelemetry Collector with an auth plugin or a cloud provider's trace storage product).\r\n\r\nUp until now, K8s trace configuration has supported options for endpoint and sampling rate per million.\r\n\r\nIn certain scenarios, K8s can emit spans directly to a provider\u2019s trace storage product, eliminating the need for an OpenTelemetry Collector, which would incur additional resource costs and operational complexity. Therefore, authentication is necessary for K8s in this context.\r\n\r\nFurthermore, in scenarios where an OpenTelemetry Collector with authentication is used to enhance security, K8s also requires authentication.\r\n\r\nIn summary, introducing authentication for trace configuration is essential.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123531\r\n\r\n#### Special notes for your reviewer:\r\nCC @dashpole \r\nCould you please help review this? Thanks!\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nIntroduces authentication for K8s trace configuration, enabling K8s trace spans to be emitted to trace storage backends that require essential authentication (such as an OpenTelemetry Collector with an auth plugin or a cloud provider's trace storage product).\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: CasperLiu \/ name: Casper Liu  (dd39dbb83ea2952960c3478db23a6861a7358620)<\/li><\/ul>","Welcome @CasperLiu! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @CasperLiu. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123712#\" title=\"Author self-approved\">CasperLiu<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/apis\/config\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/apis\/config\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/tracing\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/tracing\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/tracing\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/tracing\/api\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","\/hold\r\nSee https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123531#issuecomment-1978986352","\/triage accepted","@CasperLiu How are you going proceed with this, considering above @dashpole's comment? ","\/cc"],"labels":["area\/kubelet","sig\/node","release-note","size\/M","kind\/feature","cncf-cla: yes","sig\/instrumentation","needs-ok-to-test","sig\/architecture","do-not-merge\/hold","needs-priority","triage\/accepted"]},{"title":"WIP: Add image garbage collection lock to kubelet","body":"\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nAvoid images being deleted which are still required because a container creation is currently in progress. This fixes a rare race between the image garbage collection and the kuberuntime manager.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123631\r\n\r\n#### Special notes for your reviewer:\r\ncc @Shubham1320 @tomergayer @kannon92 @haircommander @kubernetes\/sig-node-pr-reviews  \r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFixed rare bug where the kubelet image garbage collection races with container creation and removed images in use.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123711#\" title=\"Author self-approved\">saschagrunert<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","\/hold \r\n\r\nper https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123631#issuecomment-1983670720"],"labels":["kind\/bug","area\/kubelet","sig\/node","release-note","size\/M","cncf-cla: yes","do-not-merge\/work-in-progress","do-not-merge\/hold","needs-priority","needs-triage"]},{"title":"Goroutine leakage with `TestFrameworkHandler_IterateOverWaitingPods`","body":"### Which jobs are failing?\n\nReproduce with `go test -v -race .\/pkg\/scheduler -run TestFrameworkHandler_IterateOverWaitingPods -count=1`\r\n\r\nCopied from https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123686#issuecomment-1978109639\r\n\r\nThere is still a chance of receiving the leaked error as following on my mac (even the test is succeed)\r\n```\r\n    --- PASS: TestFrameworkHandler_IterateOverWaitingPods\/pods_with_different_profiles_are_waiting_on_permit_stage (0.10s)\r\nE0305 15:13:34.760997   76753 framework.go:1500] \"WARNING: test kept at least one goroutine running after test completion\" logger=\"TestFrameworkHandler_IterateOverWaitingPods\/pods_with_different_profiles_are_waiting_on_permit_stage leaked goroutine\" pod=\"pod4\" callstack=<\r\n\tgoroutine 359 [running]:\r\n\tk8s.io\/klog\/v2\/internal\/dbg.Stacks(0x0)\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/internal\/dbg\/dbg.go:35 +0x8c\r\n\tk8s.io\/klog\/v2\/ktesting.tlogger.fallbackLogger({0x140006b4480, {0x0, 0x0}, {0x140004a0880, 0x2, 0x2}})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/ktesting\/testinglogger.go:253 +0x1b4\r\n\tk8s.io\/klog\/v2\/ktesting.tlogger.Info({0x140006b4480, {0x0, 0x0}, {0x140004a0880, 0x2, 0x2}}, 0x4, {0x1040a5591, 0x15}, {0x140007b9d60, ...})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/ktesting\/testinglogger.go:277 +0x11c\r\n\tgithub.com\/go-logr\/logr.Logger.Info({{0x104a7b8a8?, 0x14000694360?}, 0x0?}, {0x1040a5591, 0x15}, {0x140007b9d60, 0x2, 0x2})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/github.com\/go-logr\/logr@v1.4.1\/logr.go:280 +0xe8\r\n\tk8s.io\/kubernetes\/pkg\/scheduler\/framework\/runtime.(*frameworkImpl).WaitOnPermit(0x14000292fc8, {0x104a75f20, 0x140007ac280}, 0x14000406d88)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/framework\/runtime\/framework.go:1500 +0x1a8\r\n\tk8s.io\/kubernetes\/pkg\/scheduler.(*Scheduler).bindingCycle(0x140005a38c0, {0x104a75f20, 0x140007ac280}, 0x14000992180, {0x104a9a9c0, 0x14000292fc8}, {{0x104089a73, 0x5}, 0x1, 0x1, ...}, ...)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/schedule_one.go:278 +0xa8\r\n\tk8s.io\/kubernetes\/pkg\/scheduler.(*Scheduler).scheduleOne.func1()\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/schedule_one.go:125 +0x194\r\n\tcreated by k8s.io\/kubernetes\/pkg\/scheduler.(*Scheduler).scheduleOne in goroutine 270\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/schedule_one.go:118 +0x620\r\n >\r\nPASS\r\n```\r\n\r\nPod will wait in the binding cycle from its own goroutines, is it possible that these goroutines may leak?\r\nSo at the end of the testing, I allowed these pods to pass the permit stage (using the code below), but I also got new errors\r\n\r\n\t\t\tfwk1, ok := scheduler.Profiles[testSchedulerProfile1]\r\n\t\t\tif !ok {\r\n\t\t\t\tt.Fatalf(\"unable get profile\")\r\n\t\t\t}\r\n\t\t\tfwk1.IterateOverWaitingPods(func(pod framework.WaitingPod) {\r\n\t\t\t\tpod.Allow(fakePermit)\r\n\t\t\t})\r\n\r\n\t\t\twg.Add(waitSchedulingPodNumber)\r\n\t\t\tstopFn2, err := eventBroadcaster.StartEventWatcher(func(obj runtime.Object) {\r\n\t\t\t\te, ok := obj.(*eventsv1.Event)\r\n\t\t\t\tif !ok || (e.Reason != \"Scheduled\") {\r\n\t\t\t\t\treturn\r\n\t\t\t\t}\r\n\t\t\t\twg.Done()\r\n\t\t\t})\r\n\t\t\tif err != nil {\r\n\t\t\t\tt.Fatal(err)\r\n\t\t\t}\r\n\t\t\tdefer stopFn2()\r\n\r\n\t\t\twg.Wait()\r\nnew leak error as following....\r\n\r\n```\r\n=== RUN   TestFrameworkHandler_IterateOverWaitingPods\/pods_with_different_profiles_are_waiting_on_permit_stage\r\nE0305 15:10:07.033132   74647 scheduling_queue.go:890] \"WARNING: test kept at least one goroutine running after test completion\" logger=\"TestFrameworkHandler_IterateOverWaitingPods\/pods_with_same_profile_are_waiting_on_permit_stage leaked goroutine\" callstack=<\r\n\tgoroutine 290 [running]:\r\n\tk8s.io\/klog\/v2\/internal\/dbg.Stacks(0x0)\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/internal\/dbg\/dbg.go:35 +0x8c\r\n\tk8s.io\/klog\/v2\/ktesting.tlogger.fallbackLogger({0x140007947e0, {0x0, 0x0}, {0x0, 0x0, 0x0}})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/ktesting\/testinglogger.go:253 +0x1b4\r\n\tk8s.io\/klog\/v2\/ktesting.tlogger.Info({0x140007947e0, {0x0, 0x0}, {0x0, 0x0, 0x0}}, 0x2, {0x103ecea58, 0x1a}, {0x0, ...})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/k8s.io\/klog\/v2@v2.120.1\/ktesting\/testinglogger.go:277 +0x11c\r\n\tgithub.com\/go-logr\/logr.Logger.Info({{0x10489b8a8?, 0x140002eab70?}, 0x14000b219d8?}, {0x103ecea58, 0x1a}, {0x0, 0x0, 0x0})\r\n\t\t\/Users\/archelurong\/.gvm\/pkgsets\/go1.20\/global\/pkg\/mod\/github.com\/go-logr\/logr@v1.4.1\/logr.go:280 +0xe8\r\n\tk8s.io\/kubernetes\/pkg\/scheduler\/internal\/queue.(*PriorityQueue).Pop(0x14000674000, {{0x10489b8a8?, 0x140002eab70?}, 0x140003da050?})\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/internal\/queue\/scheduling_queue.go:890 +0xec\r\n\tk8s.io\/kubernetes\/pkg\/scheduler.(*Scheduler).scheduleOne(0x140006d6180, {0x104895f20, 0x140003da050})\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/schedule_one.go:68 +0x5c\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntilWithContext.func1()\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:259 +0x2c\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.BackoffUntil.func1(0x14000b21ec8?)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:226 +0x40\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.BackoffUntil(0x14000b21f68, {0x104870a48, 0x14000a98420}, 0x1, 0x14000176900)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:227 +0x90\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil(0x14000a8c768, 0x0, 0x0, 0x1, 0x14000176900)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:204 +0x80\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntilWithContext({0x104895f20, 0x140003da050}, 0x1400061c1f0, 0x0, 0x0, 0x1)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:259 +0x80\r\n\tk8s.io\/apimachinery\/pkg\/util\/wait.UntilWithContext(...)\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/wait\/backoff.go:170\r\n\tcreated by k8s.io\/kubernetes\/pkg\/scheduler.(*Scheduler).Run in goroutine 218\r\n\t\t\/Users\/archelurong\/go\/src\/kubernetes\/pkg\/scheduler\/scheduler.go:418 +0x104\r\n >\r\n```\n\n### Which tests are failing?\n\nNone\n\n### Since when has it been failing?\n\nNone\n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\n_No response_\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig scheduling\r\ncc @NoicFank","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/remove-kind failing-test\r\n\/kind cleanup","\/kind flake\r\n\/cc @pohly ","> WARNING: test kept at least one goroutine running after test completion\r\n\r\nThis is pretty normal in Kubernetes because very few of our objects support waiting for completion of their goroutines. I probably need to tone down this warning a bit further to avoid confusion:\r\n- Don't log it as error.\r\n- Only print the warning if the log message that triggers it gets emitted long after test completion - 10 seconds?\r\n","See https:\/\/github.com\/kubernetes\/klog\/pull\/401"],"labels":["kind\/cleanup","sig\/scheduling","kind\/flake","needs-triage"]},{"title":"remotecommand: treat unexpected EOF as error to avoid returning success in case of network blip","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nVersion 4 (and newer) of the remote command protocol specify that it is expected to get a `metav1.Status` response after termination of the remote process. However, the implementation of the client side of this protocol considered an `EOF` of the error stream (on which this response is expected) to be a successful termination, even if no such `Status` message was received.\r\n\r\nThis patch changes that behaviour for v4 and up, by letting the (already protocol-specific) error decoder handle the zero length message case. In the v4 error decoder, then, we consider this case an error (as we do with any other unexpected response). The reason I think this change is necessary is that it is possible to observe this EOF in case of a network blip somewhere along the (reasonably long) path from client-go to the CRI.\r\n\r\nNotably, this fixes the case where `StreamWithContext` could return successfully (i.e. nil error) even if the program exited with a non-zero exit code, or didn't exit at all. This has caused flakes in cilium's CI in the past, as it heavily relies on this mechanism. The related cilium issue is https:\/\/github.com\/cilium\/cilium\/issues\/31067.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\n\r\n#### Special notes for your reviewer:\r\n\r\nProbably worth reviewing commit by commit - I intentionally put the test changes into the first two commits to demonstrate that they pass with the existing implementation. Then, the implementation change in the third can be done without changing tests, increasing confidence. \r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFixes an issue where remotecommand StreamWithContext could succeed even though the underlying transport failed.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: bimmlerd \/ name: David Bimmler  (41bb20cfc80b7fe99e41f27d254cb7ff862c0ac2, fe6bf006441b9afd4cb01103a3058d80b2fcbea7, 498f9e98de9bff41a8b5f59fdac7d5c6da245d28)<\/li><\/ul>","Welcome @bimmlerd! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @bimmlerd. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/ok-to-test\r\n@bimmlerd please sign the CLA :)","we have keepalive frames to detect network hiccups, can the streaming layer detect the abrupt network connection drop and cascade the error instead of relying on the size of the message?\r\n\r\n\/cc @liggitt @seans3 ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123705#\" title=\"Author self-approved\">bimmlerd<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/tools\/remotecommand\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/tools\/remotecommand\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","\/retest (Test above seems to have hit https:\/\/github.com\/kubernetes\/kubernetes\/issues\/119582)","> we have keepalive frames to detect network hiccups, can the streaming layer detect the abrupt network connection drop and cascade the error instead of relying on the size of the message?\r\n\r\n@aojea I've traced this a bit further down the stack, and what I've found so far is the following:\r\n\r\n1. The layers are as follows: v4 streaming protocol over SPDY over TLS over TCP.\r\n2. On the client, the EOF bubbles up directly from TCP all the way to the streaming protocol- SPDY gets it here https:\/\/github.com\/moby\/spdystream\/blob\/master\/spdy\/read.go#L165, from there it bubbles up to the `io.ReadAll` and we hit the default case in `errorstream.go`.\r\n3. Given that we get an EOF from TCP (versus some timeout error), I believe that the server closes the connection, possibly because it doesn't get the keep-alive pings you mention. \r\n \r\nNotably, my current repro occurs with repeating cycles of 20s 100% packet loss, 10s no packet loss. The EOF is received only once the 20s are through, ie once TCP is flowing again - hence I guess the server closes the conn without sending an error. It's debatable whether that's another bug, or the root cause.\r\n\r\nMy repro is just a kind cluster with cilium installed (`make kind` etc in cilium\/cilium), then I docker exec onto the control plane node, and run\r\n```\r\nwhile true; do \r\n  tc qdisc add dev eth0 root netem loss 100%\r\n  echo \"DOWN\"; sleep 20 \r\n  tc qdisc del dev eth0 root netem loss 100%\r\n  echo \"UP\"; sleep 10; \r\ndone\r\n```\r\nonce `cilium connectivity test --test client-ingress-knp` has set up and is starting its tests.\r\n ","FWIW, I think there's additionally a bug in the upgradeaware proxy code in the API server - see https:\/\/github.com\/cilium\/cilium\/issues\/31067#issuecomment-1985200305. Do I attempt a fix of that in this PR or open a separate one?","\/assign @aojea \r\n\/triage accepted"],"labels":["kind\/bug","area\/apiserver","sig\/api-machinery","release-note","size\/L","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"Completed Job leaves orphaned pod","body":"### What happened?\n\nWe have a job template:\r\n```\r\napiVersion: batch\/v1\r\nkind: Job\r\nmetadata:\r\n  name: REPLACE_ME\r\nspec:\r\n  backoffLimit: 6\r\n  ttlSecondsAfterFinished: 300\r\n  template:\r\n    metadata:\r\n      name: test\r\n    spec:\r\n      serviceAccountName: job-sa\r\n      containers:\r\n      - name: document-test\r\n        resources:\r\n          limits:\r\n            memory: 2000Mi\r\n            cpu: 2000m\r\n          requests:\r\n            memory: 500Mi\r\n            cpu: 500m\r\n        image: REPLACE_ME\r\n        imagePullPolicy: IfNotPresent\r\n        volumeMounts:\r\n          - name: secrets-volume\r\n            mountPath: \/app\/secrets\r\n      restartPolicy: Never\r\n      volumes:\r\n        - name: secrets-volume\r\n          secret:\r\n            secretName: ****-secrets\r\n```\r\n\r\nAfter launching a random number of jobs (10-600), all jobs are deleted after completion, whether the status is succeeded or failed. However, this does not remove all associated pods and leaves a random number of orphaned pods.\r\n\r\nThe problem arose suddenly, there were no changes to the teamplate. There are also no changes in the args for the container.\n\n### What did you expect to happen?\n\nAs before, I expected that all associated pods would be deleted after deleting the jobs.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nIf there needed i can share my full job part.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nserverVersion:\r\n  buildDate: \"2024-01-02T20:34:38Z\"\r\n  compiler: gc\r\n  gitCommit: 3f8ed3d5017d988600f597734a4851930eda35a6\r\n  gitTreeState: clean\r\n  gitVersion: v1.27.9-eks-5e0fdde\r\n  goVersion: go1.20.12\r\n  major: \"1\"\r\n  minor: 27+\r\n  platform: linux\/amd64\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\naws\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:2.3:o:amazon:amazon_linux:2\"\r\nHOME_URL=\"https:\/\/amazonlinux.com\/\"\r\nSUPPORT_END=\"2025-06-30\"\r\n$ uname -a\r\nLinux 5.10.209-198.858.amzn2.x86_64 #1 SMP Tue Feb 13 18:46:41 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig apps\r\n","Do you see any webhooks interfering with this? Anything in the logs?","Are the pods terminating by chance?","@kannon92 \r\n> Do you see any webhooks interfering with this? Anything in the logs?\r\n\r\nNo, there nothing in webhooks, nothing anomaly in logs\r\n```\r\nstatus:\r\n  phase: Succeeded\r\n  conditions:\r\n    - type: Initialized\r\n      status: 'True'\r\n      lastProbeTime: null\r\n      reason: PodCompleted\r\n  containerStatuses:\r\n    - name: loader\r\n      state:\r\n        terminated:\r\n          exitCode: 0\r\n          reason: Completed\r\n      lastState: {}\r\n      ready: false\r\n```\r\n\r\n\r\n> Are the pods terminating by chance?\r\n\r\nJobs all complete without problems, pods complete their work randomly. I didn't find any logic. This could be a random under. Out of 100 launched, today, for example, 8 were not deleted.","@mimowo @alculquicondor any suggestions on what to look for here?","Could you post the entire yaml representing one of the pods? I'm particularlt interested in metadata. The deletion timestamp, finalizers, and owner references.","Also, how is the job deleted, is it kubectl or another API client. Check if orphan propagation policy is used, in kubectl this is the --cascade orphan option.","@mimowo \r\n\r\n> Could you post the entire yaml representing one of the pods? I'm particularlt interested in metadata. The deletion timestamp, finalizers, and owner references.\r\n\r\nYeah, sure\r\n````\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: ***-e94f460d-2e51-4a5d-8ca9-23ca3bee5c09-2gr44\r\n  generateName: ***-e94f460d-2e51-4a5d-8ca9-23ca3bee5c09-\r\n  namespace: ***\r\n  uid: 4900d9e2-2f92-4932-a649-063cfddca64a\r\n  resourceVersion: '863645282'\r\n  creationTimestamp: '2024-03-05T11:44:41Z'\r\n  labels:\r\n    batch.kubernetes.io\/controller-uid: 28e527fa-9c7f-4c12-84ea-824b24bb5e3d\r\n    batch.kubernetes.io\/job-name: ***-e94f460d-2e51-4a5d-8ca9-23ca3bee5c09\r\n    controller-uid: 28e527fa-9c7f-4c12-84ea-824b24bb5e3d\r\n    job-name: ***-e94f460d-2e51-4a5d-8ca9-23ca3bee5c09\r\n  managedFields:\r\n    - manager: kube-controller-manager\r\n      operation: Update\r\n      apiVersion: v1\r\n      time: '2024-03-05T11:44:41Z'\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:metadata:\r\n          f:generateName: {}\r\n          f:labels:\r\n            .: {}\r\n            f:batch.kubernetes.io\/controller-uid: {}\r\n            f:batch.kubernetes.io\/job-name: {}\r\n            f:controller-uid: {}\r\n            f:job-name: {}\r\n        f:spec:\r\n          f:containers:\r\n            k:{\"name\":\"****-loader\"}:\r\n              .: {}\r\n              f:args: {}\r\n              f:image: {}\r\n              f:imagePullPolicy: {}\r\n              f:name: {}\r\n              f:resources:\r\n                .: {}\r\n                f:limits:\r\n                  .: {}\r\n                  f:cpu: {}\r\n                  f:memory: {}\r\n                f:requests:\r\n                  .: {}\r\n                  f:cpu: {}\r\n                  f:memory: {}\r\n              f:terminationMessagePath: {}\r\n              f:terminationMessagePolicy: {}\r\n              f:volumeMounts:\r\n                .: {}\r\n                k:{\"mountPath\":\"\/app\/secrets\"}:\r\n                  .: {}\r\n                  f:mountPath: {}\r\n                  f:name: {}\r\n          f:dnsPolicy: {}\r\n          f:enableServiceLinks: {}\r\n          f:restartPolicy: {}\r\n          f:schedulerName: {}\r\n          f:securityContext: {}\r\n          f:serviceAccount: {}\r\n          f:serviceAccountName: {}\r\n          f:terminationGracePeriodSeconds: {}\r\n          f:volumes:\r\n            .: {}\r\n            k:{\"name\":\"secrets-volume\"}:\r\n              .: {}\r\n              f:name: {}\r\n              f:secret:\r\n                .: {}\r\n                f:defaultMode: {}\r\n                f:secretName: {}\r\n    - manager: kubelet\r\n      operation: Update\r\n      apiVersion: v1\r\n      time: '2024-03-05T11:44:51Z'\r\n      fieldsType: FieldsV1\r\n      fieldsV1:\r\n        f:status:\r\n          f:conditions:\r\n            k:{\"type\":\"ContainersReady\"}:\r\n              .: {}\r\n              f:lastProbeTime: {}\r\n              f:lastTransitionTime: {}\r\n              f:reason: {}\r\n              f:status: {}\r\n              f:type: {}\r\n            k:{\"type\":\"Initialized\"}:\r\n              .: {}\r\n              f:lastProbeTime: {}\r\n              f:lastTransitionTime: {}\r\n              f:reason: {}\r\n              f:status: {}\r\n              f:type: {}\r\n            k:{\"type\":\"Ready\"}:\r\n              .: {}\r\n              f:lastProbeTime: {}\r\n              f:lastTransitionTime: {}\r\n              f:reason: {}\r\n              f:status: {}\r\n              f:type: {}\r\n          f:containerStatuses: {}\r\n          f:hostIP: {}\r\n          f:phase: {}\r\n          f:podIP: {}\r\n          f:podIPs:\r\n            .: {}\r\n            k:{\"ip\":\"****\"}:\r\n              .: {}\r\n              f:ip: {}\r\n          f:startTime: {}\r\n      subresource: status\r\n  selfLink: >-\r\n    ****\r\nstatus:\r\n  phase: Succeeded\r\n  conditions:\r\n    - type: Initialized\r\n      status: 'True'\r\n      lastProbeTime: null\r\n      lastTransitionTime: '2024-03-05T11:44:41Z'\r\n      reason: PodCompleted\r\n    - type: Ready\r\n      status: 'False'\r\n      lastProbeTime: null\r\n      lastTransitionTime: '2024-03-05T11:44:49Z'\r\n      reason: PodCompleted\r\n    - type: ContainersReady\r\n      status: 'False'\r\n      lastProbeTime: null\r\n      lastTransitionTime: '2024-03-05T11:44:49Z'\r\n      reason: PodCompleted\r\n    - type: PodScheduled\r\n      status: 'True'\r\n      lastProbeTime: null\r\n      lastTransitionTime: '2024-03-05T11:44:41Z'\r\n  hostIP: ***\r\n  podIP: ***\r\n  podIPs:\r\n    - ip: ***\r\n  startTime: '2024-03-05T11:44:41Z'\r\n  containerStatuses:\r\n    - name: ***-loader\r\n      state:\r\n        terminated:\r\n          exitCode: 0\r\n          reason: Completed\r\n          startedAt: '2024-03-05T11:44:41Z'\r\n          finishedAt: '2024-03-05T11:44:48Z'\r\n          containerID: >-\r\n            ***\r\n      lastState: {}\r\n      ready: false\r\n      restartCount: 0\r\n      image: >-\r\n        ***\r\n      imageID: >-\r\n        ***\r\n      containerID: >-\r\n        ***\r\n      started: false\r\n  qosClass: Burstable\r\nspec:\r\n  volumes:\r\n    - name: aws-iam-token\r\n      projected:\r\n        sources:\r\n          - serviceAccountToken:\r\n              audience: sts.amazonaws.com\r\n              expirationSeconds: 86400\r\n              path: token\r\n        defaultMode: 420\r\n    - name: secrets-volume\r\n      secret:\r\n        secretName: *****-secrets\r\n        defaultMode: 420\r\n    - name: kube-api-access-gkhb6\r\n      projected:\r\n        sources:\r\n          - serviceAccountToken:\r\n              expirationSeconds: 3607\r\n              path: token\r\n          - configMap:\r\n              name: kube-root-ca.crt\r\n              items:\r\n                - key: ca.crt\r\n                  path: ca.crt\r\n          - downwardAPI:\r\n              items:\r\n                - path: namespace\r\n                  fieldRef:\r\n                    apiVersion: v1\r\n                    fieldPath: metadata.namespace\r\n        defaultMode: 420\r\n  containers:\r\n    - name: ******-loader\r\n      image: >-\r\n        ********\r\n      args:\r\n        - npm\r\n        - run\r\n        - loader\r\n        - ****\r\n      env:\r\n        - name: AWS_STS_REGIONAL_ENDPOINTS\r\n          value: regional\r\n        - name: AWS_DEFAULT_REGION\r\n          value: AWS_DEFAULT_REGION\r\n        - name: AWS_REGION\r\n          value: AWS_REGION\r\n        - name: AWS_ROLE_ARN\r\n          value: AWS_ROLE_ARN\r\n        - name: AWS_WEB_IDENTITY_TOKEN_FILE\r\n          value: \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n      resources:\r\n        limits:\r\n          cpu: '2'\r\n          memory: 2000Mi\r\n        requests:\r\n          cpu: 500m\r\n          memory: 500Mi\r\n      volumeMounts:\r\n        - name: secrets-volume\r\n          mountPath: \/app\/secrets\r\n        - name: kube-api-access-gkhb6\r\n          readOnly: true\r\n          mountPath: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\r\n        - name: aws-iam-token\r\n          readOnly: true\r\n          mountPath: \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\r\n      terminationMessagePath: \/dev\/termination-log\r\n      terminationMessagePolicy: File\r\n      imagePullPolicy: IfNotPresent\r\n  restartPolicy: Never\r\n  terminationGracePeriodSeconds: 30\r\n  dnsPolicy: ClusterFirst\r\n  serviceAccountName: *******-job-sa\r\n  serviceAccount: ******-job-sa\r\n  nodeName: ********.compute.internal\r\n  securityContext: {}\r\n  schedulerName: default-scheduler\r\n  tolerations:\r\n    - key: node.kubernetes.io\/not-ready\r\n      operator: Exists\r\n      effect: NoExecute\r\n      tolerationSeconds: 300\r\n    - key: node.kubernetes.io\/unreachable\r\n      operator: Exists\r\n      effect: NoExecute\r\n      tolerationSeconds: 300\r\n  priority: 0\r\n  enableServiceLinks: true\r\n  preemptionPolicy: PreemptLowerPriority\r\n```\r\n\r\n> Also, how is the job deleted, is it kubectl or another API client. Check if orphan propagation policy is used, in kubectl this is the --cascade orphan option.\r\n\r\nIn job we use `ttlSecondsAfterFinished` for deleting jobs and their pods. We dont enable `Control plane logging ` so i cant share any logs from kube-controller-manager.\r\n\r\n","Ok, so there are no finalizers blocking the deletion, and the pod is completed, so it is eligible for garbage collection, but you need to have 12500 of such pods (this is controlled by the `--terminated-pod-gc-threshold` param: https:\/\/kubernetes.io\/docs\/reference\/command-line-tools-reference\/kube-controller-manager\/). \r\n\r\nThis is WAI when a job is deleted with the `propagationPolicy: Orphan` (in kubectl this is `--cascade=orphan`). So I suspect this is how the job was deleted, but don't know CronJob that well. cc @atiratree in case you are familiar with this.","The TTLSecondsAfterFinished is handled using Foreground:\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blame\/release-1.27\/pkg\/controller\/ttlafterfinished\/ttlafterfinished_controller.go#L240\r\n\r\nAny chance you can confirm who was the actor that called DELETE on the jobs? Maybe you have some form of audit logs?\r\n\r\nBut I agree that it looks like the job-controller is not at fault here, as the Pods don't have a finalizer or ownerReference.","btw we use job, not cronjob for this case.\r\nAs I said above, we have `Control plane logging` completely turned off in our eks clusters.\r\n\r\n@alculquicondor \r\nAs I understand it, these logs are needed for debugging. I will try to enable logging on one of our test clusters where this problem with pods is recreated and will get back to you with the details."],"labels":["kind\/bug","sig\/apps","needs-triage"]},{"title":"[WIP] Add e2e tests for stateful set maxUnavailable","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n\/kind failing-test\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nAdds e2e tests to assert stateful sets honor pod management policies and minReadySeconds when maxUnavailable is specified.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n[KEP]: https:\/\/github.com\/knelasevero\/enhancements\/blob\/c857622d31e6a1c2a0c0102261351e3a20152f62\/keps\/sig-apps\/961-maxunavailable-for-statefulset\/README.md#e2e-tests\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: bersalazar \/ name: Bernardo Salazar  (f7a4630e3e2942e40accd6dda8859fe6f616eab4)<\/li><\/ul>","Welcome @bersalazar! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @bersalazar. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","One of the tests is a failing test covering this:\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/112307","Thanks @bersalazar ! Will have a look soon :)","\/ok-to-test","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-apps-kind-alpha-features","@knelasevero: The specified target(s) for `\/test` were not found.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-ec2-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-cloud-provider-loadbalancer`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-containerd-flaky`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123703#issuecomment-1985444407):\n\n>\/test pull-kubernetes-e2e-apps-kind-alpha-features\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123703#\" title=\"Author self-approved\">bersalazar<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [soltysh](https:\/\/github.com\/soltysh) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e\/apps\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/apps\/OWNERS)**\n- **[test\/e2e\/feature\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/feature\/OWNERS)**\n- **[test\/e2e\/framework\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/framework\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"soltysh\"]} -->","\/test pull-kubernetes-e2e-kind-alpha-features","\/test pull-kubernetes-e2e-kind-alpha-features","Superseded by: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123915.\r\nKeeping open temporarily for reference. "],"labels":["kind\/bug","area\/test","size\/L","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","kind\/failing-test","do-not-merge\/work-in-progress","ok-to-test","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"Automated cherry pick of #123532: Prevent watch cache starvation, by moving its watch to","body":"**NOTE to reviewers**: There's no straightforward way to cherry pick it to 1.27 due to:\r\n- `conditionalProgressRequester` is implemented in 1.28+\r\n- The unit test (2nd commit in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123532\/commits) is NOT cherry picked due to some of its testing dependencies are not available in 1.27 (e.g. https:\/\/github.com\/kubernetes\/kubernetes\/pull\/118495). I guess we should keep the cherry pick as minimum as possible.\r\n\r\nPartial cherry pick of #123532 on release-1.27.\r\n\r\n#123532: Prevent watch cache starvation, by moving its watch to\r\n\r\nFor details on the cherry pick process, see the [cherry pick requests](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) page.\r\n\r\n```release-note\r\nPrevent watch cache starvation by moving its watch to separate RPC and add a SeparateCacheWatchRPC feature flag to disable this behavior\r\n```","comments":["This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123700#\" title=\"Author self-approved\">mengqiy<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/pkg\/features\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.27\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->","I'm -1 on this cherrypick. This is re-doing parts of the PR, we're not cherrypicking tests and the risk coming from it os non-negligible imho. Given where we're in the release cycle with 1.27 release, I think we shouldn't do that.","\/hold","> I'm -1 on this cherrypick.\r\n\r\nI tend to agree @wojtek-t ","I would disagree, the lack of `conditionalProgressRequester` doesn't influence the fix, just the test. I think there is low risk of test changes breaking the release.","\/triage accepted","> I'm -1 on this cherrypick. This is re-doing parts of the PR, we're not cherrypicking tests and the risk coming from it os non-negligible imho. Given where we're in the release cycle with 1.27 release, I think we shouldn't do that.\r\n\r\nI don't see the increased risk relative to the master \/ 1.29 \/ 1.28 versions ... the non-test part of this PR is functionally identical to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123532 for the parts that existed in 1.27 (the feature-gate, context creation, and ListerWatcher change).\r\n\r\nI agree that we must have test coverage, so I'd expect a backport of TestWatchStreamSeparation, even if it means copying back more unit-test changes.\r\n\r\nSince 1.27 was when we started passing through client-initiated watches to etcd consistently (https:\/\/github.com\/kubernetes\/kubernetes\/pull\/115096), which is what made this issue more apparent, I think this could reasonably be treated as a regression and we should resolve it in 1.27 if possible.","@wojtek-t @dims If no objection, I can start to backport the test part.","@mengqiy go for it please! (separate commit is good!)","It seems we have to expose `RequestWatchProgress(ctx context.Context) error` in `k8s.io\/apiserver\/pkg\/storage.Interface` and add `RequestWatchProgress` method in `store` in `staging\/src\/k8s.io\/apiserver\/pkg\/storage\/etcd3\/store.go`.\r\nIIUC it is a must-have for the new unit test to work. But I'm not sure it's the right thing to do here since it mutate additional non-test code.\r\n@serathius Do you think if there is good way to back port the unit test?","@serathius You authored many changes in the `cache` package between 1.27 and 1.28. I guess you have more context about how to make this backport safer. I'd be great if you can take over this for 1.27.","an alternative to taking this back to 1.27 is to gate \/ disable-by-default the change introduced in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/115096 in 1.27+ until the etcd behavior with a congested watch stream is resolved.\r\n\r\nGiven the new information about etcd watch behavior in https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072#issuecomment-1994505223, that may be preferable.","> an alternative to taking this back to 1.27 is to gate \/ disable-by-default the change introduced in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/115096 in 1.27+ until the etcd behavior with a congested watch stream is resolved.\r\n\r\n+1","Pushed a 2-line change: Flip the feature gate to default off in `pkg\/features\/kube_features.go` and `staging\/src\/k8s.io\/apiserver\/pkg\/features\/kube_features.go`","@liggitt PTAL","> Pushed a 2-line change: Flip the feature gate to default off in `pkg\/features\/kube_features.go` and `staging\/src\/k8s.io\/apiserver\/pkg\/features\/kube_features.go`\r\n\r\nwith a covering unit test, I don't see why we wouldn't default this on\r\n\r\nwithout a covering unit test, I wouldn't be in favor of backporting\r\n\r\nI still think undoing the 1.27 watch change by default in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123935 and backporting would be even better, though that PR is stuck on test failures because of other changes since 1.27 that need to be sorted out"],"labels":["area\/apiserver","sig\/api-machinery","release-note","size\/M","cncf-cla: yes","do-not-merge\/cherry-pick-not-approved","do-not-merge\/hold","needs-priority","triage\/accepted","do-not-merge\/needs-kind"]},{"title":"DRA: kubelet: avoid API version dependency","body":"### What would you like to be added?\n\nIn `pkg\/kubelet\/cm\/dra\/plugin\/noderesources.go` and `api.proto`, kubelet depends on a specific version of the resource.k8s.io API because it needs to receive resource information from a plugin in that format and copies it into a NodeResourceSlice.\r\n\r\nIt also pulls ResourceClaim and extracts the allocation result for the plugin.\r\n\r\nInstead, kubelet should provide a generic REST proxy for NodeResourceSlice and ResourceClaim and let the plugin decide in which version it wants to access those resources. The plugin then must publish its own NodeResourceSlice objects and get the ResourceClaim.\r\n\r\nWe also need more E2E tests.\n\n### Why is this needed?\n\nThe resource.k8s.io API is likely to evolve as while we continue to work on enhancing structured parameters. Allowing a cluster setup where kubelet is several releases behind the control plane and DRA drivers is desirable because updating kubelet is very intrusive.\r\n\r\n\/sig node\r\n\/triage accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign","comments":[],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","triage\/accepted"]},{"title":"DRA: scheduler: refactor AssumeCache + event handler","body":"### What would you like to be added?\n\nThe assume cache is now shared between volumbinding and dynamicresources plugin. It should be moved into the scheduler framework.\r\n\r\nEvents that make pods scheduleable currently are triggered by the informer cache, not the assume cache. For \"claim was deallocated\", this leads to a small, unlikely race if a pod gets scheduled and stopped so quickly that the informer cache doesn't ever see the \"claim is allocated\" state. The event handler should react to changes in the assume cache because that cache is guaranteed to receive the \"claim is allocated\" state which cause some pod to not get scheduled, because by definition the cache must have listed some other claim as using resources needed for that pod.\r\n\r\n\n\n### Why is this needed?\n\nCode ownership, race condition.\r\n\r\n\/sig node\r\n\/triage accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign","comments":["cc @Huang-Wei since you asked the same question I did"],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","triage\/accepted"]},{"title":"DRA: scheduler: refactor foreachPodResourceClaim","body":"### What would you like to be added?\n\n`foreachPodResourceClaim` in `pkg\/scheduler\/framework\/plugins\/dynamicresources\/dynamicresources.go` could be changed so that it gathers all information about a claim, including structured claim and class parameters. Instead of two slices of the same size (`claims` and `informationsForClaim`) we should only have one.\n\n### Why is this needed?\n\n- `isSchedulableAfterClaimChange` can then trigger only for claims with structured parameters and ignore the others\r\n- Some checks currently only done in `PreFilter` can already be done in `PreEnqueue`.\r\n\r\nTogether this reduces the overall work.\r\n\r\n\/sig node\r\n\/triage accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign","comments":["Unit test coverage for `isSchedulableAfterClaimParametersChange` and `isSchedulableAfterClassParametersChange` needs to be improved, which is related to this.\r\n"],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","triage\/accepted"]},{"title":"DRA: kubelet: exponential backoff in NodeResourceSlices controller","body":"### What would you like to be added?\n\n`pkg\/kubelet\/cm\/dra\/plugin\/noderesources.go` has to retry after failures. This currently uses a fixed 5 second delay to avoid busy-looping. Exponential backoff might be better.\r\n\n\n### Why is this needed?\n\n\/sig node\r\n\/triage-accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign","comments":["\/triage accepted"],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","triage\/accepted"]},{"title":"DRA: kubelet: decide about resync period","body":"### What would you like to be added?\n\nThe NodeResourceSlices controller currently uses a `resyncPeriod = time.Duration(10 * time.Minute)` in `pkg\/kubelet\/cm\/dra\/plugin\/noderesources.go`.\r\n\r\nThis could get increased or disabled entirely.\n\n### Why is this needed?\n\nPeriodic resyncing should only be needed if the controller is faulty, which hopefully isn't the case.\r\n\r\n\/sig node\r\n\/triage-accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","needs-triage"]},{"title":"DRA: beta: CEL validation","body":"### What would you like to be added?\r\n\r\nAt the moment, `staging\/src\/k8s.io\/dynamic-resource-allocation\/structured\/namedresources\/cel\/compile.go` uses 1.0 as version for everything in its CEL environment to grant the alpha API CEL expressions access to things defined in the 1.30 release. We need to change that to the real version before promotion to beta.\r\n\r\n### Why is this needed?\r\n\r\nNew expression must not use CEL expressions which become invalid on a downgrade, so only features already available in the previous release are valid.\r\n","comments":["\/sig node\r\n\/triage-accepted\r\n\/priority important-longterm\r\n\/lifecycle frozen\r\n\/assign\r\n","\/triage accepted","Once https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123687 and https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516 are merged to the same branch, a [RemovedVersion](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/46f017a90b54b5abfc0a66a6ecf19e304aa7da95\/staging\/src\/k8s.io\/apiserver\/pkg\/cel\/environment\/environment.go#L177) can be added to the DRA specific implementation to prevent future versions from enabling both copies of the library."],"labels":["sig\/node","kind\/feature","priority\/important-longterm","lifecycle\/frozen","triage\/accepted"]},{"title":"\u2602\ufe0fSlow Unit Test \ud83c\udf02","body":"https:\/\/prow.k8s.io\/job-history\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-unit-1-27 takes ~35min\r\nhttps:\/\/prow.k8s.io\/job-history\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-unit taks ~40min\r\n\r\nThere are some adding UT. **New** below is comparing with v1.27. (The test or package may be old, but it takes more than 1m in UT in master, and not take more than 1m in v1.27.)\r\n\r\n- [ ] **New** k8s.io\/apiserver\/pkg\/admission\/plugin\/policy: validating 1m24s\r\n- [ ] k8s.io\/apiserver\/pkg: server\r\n- [ ] k8s.io\/apiserver\/pkg\/server: filters\r\n- [ ] k8s.io\/apiserver\/pkg\/server: options\r\n- [ ] **New**  k8s.io\/apiserver\/pkg\/server\/options: encryptionconfig\r\n- [ ] k8s.io\/apiserver\/pkg\/storage: cacher **~3m** we have a 3 minute timeout https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850\r\n- [ ] k8s.io\/apiserver\/pkg\/storage: etcd3\r\n- [ ] k8s.io\/apiserver\/pkg\/util: flowcontrol \r\n- [ ] **New** k8s.io\/apiserver\/pkg\/util: peerproxy\r\n- [ ] **New** k8s.io\/kubectl\/pkg\/cmd: apply\r\n- [ ] k8s.io\/kubernetes\/pkg\/controller: endpoint\r\n- [ ] k8s.io\/kubernetes\/pkg\/controller: endpointslice\r\n- [ ] k8s.io\/kubernetes\/pkg\/controller: nodeipam\r\n- [ ] k8s.io\/kubernetes\/pkg\/controller: statefulset\r\n- [ ] **New** k8s.io\/kubernetes\/pkg\/kubelet\/cm: dra **~3m** \u26a0\ufe0f we have a 3 minute timeout\r\n- [ ] k8s.io\/kubernetes\/pkg\/kubelet\/volumemanager: reconciler\r\n- [ ] k8s.io\/kubernetes\/pkg\/registry\/core\/pod: storage\r\n- [ ] k8s.io\/kubernetes\/pkg\/registry\/core\/service: ipallocator\r\n- [ ] k8s.io\/kubernetes\/pkg\/registry\/core\/service: storage\r\n- [ ] k8s.io\/kubernetes\/pkg\/scheduler\/framework\/plugins: volumebinding\r\n- [ ] k8s.io\/kubernetes\/pkg\/volume: csi\r\n\r\n\/sig node api-machinery cli \r\nfor new ones.\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/98486 we did a similar check in 2021 before.\r\n\r\n---\r\n\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/123850#issuecomment-1988422238  We have a 3 minute timeout for a package.  \r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/ebc1a7b7fb52f30021b7e66f77070dac8e6b839b\/hack\/make-rules\/test.sh#L60-L64","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/area test\r\n\/kind cleanup\r\n\/help wanted","\/sig node api-machinery cli\r\n","\/help","@HirazawaUi: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `\/remove-help` command.\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123685):\n\n>\/help\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra  https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123720\r\n\r\ntime required before modification: \r\n```\r\ngo test .\/... -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\t94.097s\tcoverage: 83.8% of statements\r\n```\r\ntime required after modification\r\n```\r\ngo test .\/... -cover -count=1\r\nok  \tk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\t1.356s\tcoverage: 83.8% of statements\r\n```","I will try to decrease the running time of the `apiserver\/pkg\/storage\/cacher` pkg."],"labels":["area\/test","kind\/cleanup","sig\/node","sig\/api-machinery","help wanted","sig\/cli","needs-triage"]},{"title":"CustomResourceDefinition Watch test is flaky when run multiple times on a same cluster","body":"### What happened?\r\n\r\nWe run conformance test on a same cluster multiple times. The test failed initially with\r\n\r\n```\r\n  INFO: Unexpected error: failed to list events in namespace \"crd-watch-7911\": \r\n      <*url.Error | 0xc004873770>: \r\n      Get \"https:\/\/<CP_IP_AND_PORT>\/api\/v1\/namespaces\/crd-watch-7911\/events\": dial tcp <CP_IP_AND_PORT>: connect: connection refused\r\n      {\r\n          Op: \"Get\",\r\n          URL: \"https:\/\/<CP_IP_AND_PORT>\/api\/v1\/namespaces\/crd-watch-7911\/events\",\r\n          Err: <*net.OpError | 0xc0006c3220>{\r\n              Op: \"dial\",\r\n              Net: \"tcp\",\r\n              Source: nil,\r\n              Addr: <*net.TCPAddr | 0xc003832750>{\r\n                  <redacted>\r\n                  Zone: \"\",\r\n              },\r\n              Err: <*os.SyscallError | 0xc002a6f680>{\r\n                  Syscall: \"connect\",\r\n                  Err: <syscall.Errno>0x6f,\r\n              },\r\n          },\r\n      }\r\n  [FAILED] in [DeferCleanup (Each)] - test\/e2e\/framework\/debug\/dump.go:44 @ 02\/20\/24 23:38:32.653\r\n```\r\n\r\nThen, all the subsequent tests failed with the following error\r\n\r\n```\r\n  [FAILED] failed to create CustomResourceDefinition: customresourcedefinitions.apiextensions.k8s.io \"noxus.mygroup.example.com\" already exists\r\n  In [It] at: test\/e2e\/apimachinery\/crd_watch.go:71 @ 02\/21\/24 23:36:15.799\r\n```\r\n\r\n****\r\n\r\n### What did you expect to happen?\r\n\r\nThe conformance test should withstand transient errors and crd should be cleaned up after the failed run\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nThe same issue can be reproduced by creating the same CRD on the cluster and run the conformance test. \r\n\r\n(Fix can also be verified by running conformance tests at least twice after CRD creation - the first run can fail, but the second run should pass)\r\n\r\n```\r\napiVersion: apiextensions.k8s.io\/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: noxus.mygroup.example.com\r\nspec:\r\n  conversion:\r\n    strategy: None\r\n  group: mygroup.example.com\r\n  names:\r\n    categories:\r\n    - all\r\n    kind: WishIHadChosenNoxu\r\n    listKind: NoxuItemList\r\n    plural: noxus\r\n    shortNames:\r\n    - foo\r\n    - bar\r\n    - abc\r\n    - def\r\n    singular: nonenglishnoxu\r\n  scope: Cluster\r\n  versions:\r\n  - name: v1beta1\r\n    schema:\r\n      openAPIV3Schema:\r\n        type: object\r\n        x-kubernetes-preserve-unknown-fields: true\r\n    served: true\r\n    storage: true\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.5\", GitCommit:\"6b1d87acf3c8253c123756b9e61dac642678305f\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:10:43Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nThis is a bare metal cluster\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["\/area flake\r\n\/sig api-machinery\r\n","@neolit123: The label(s) `area\/flake` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123683#issuecomment-1978248653):\n\n>\/area flake\r\n>\/sig api-machinery\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/kind flake","\/assign @jpbetz \r\nAs one of the authors of test\/e2e\/apimachinery\/crd_watch.go \r\nThank you.\r\n\/triage accepted"],"labels":["kind\/bug","sig\/api-machinery","kind\/flake","triage\/accepted"]},{"title":"Fix IPPVS unit tests for windows","body":"#### What type of PR is this?\r\n\r\n\/sig windows\r\n\/sig testing\r\n\r\n\/kind failing-test\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nCurrently, there are some unit tests for In-Place Pod Vertical Scaling that fail on Windows:\r\n- [Request128MBLimit256MB](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/kuberuntime\/kuberuntime_container_windows_test.go#L164)\r\n- [RequestZeroCPU](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/kuberuntime\/kuberuntime_container_windows_test.go#L164)\r\n\r\nThose tests are expecting the wrong values for `CpuMaximum`.\r\n\r\n#### Special notes for your reviewer:\r\n- function to [calculateWindowsResources](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/kuberuntime\/kuberuntime_container_windows.go)\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\/cc @claudiubelu","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123682#\" title=\"Author self-approved\">fabi200123<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sjenning](https:\/\/github.com\/sjenning) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sjenning\"]} -->","\/test pull-kubernetes-verify","\/triage accepted\r\n\/priority important-longterm\r\n\/lgtm","LGTM label has been added.  <details>Git tree hash: 95ee42389a635ea3f48856f96f3994150548c5c3<\/details>"],"labels":["area\/kubelet","lgtm","sig\/node","size\/XS","release-note-none","sig\/windows","cncf-cla: yes","sig\/testing","priority\/important-longterm","kind\/failing-test","triage\/accepted"]},{"title":"[WIP] Featuregates the rest","body":"\/sig api-machinery\r\n\/kind cleanup\r\n\r\n```release-note\r\nNONE\r\n```","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123681#\" title=\"Author self-approved\">deads2k<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [mikedanese](https:\/\/github.com\/mikedanese), [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/cloud-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/cloud-controller-manager\/OWNERS)**\n- ~~[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)~~ [deads2k]\n- **[cmd\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-scheduler\/OWNERS)**\n- ~~[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS)~~ [deads2k]\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)~~ [deads2k]\n- ~~[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)~~ [deads2k]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"mikedanese\",\"sttts\"]} -->","@deads2k: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-linter-hints | c0ebc8e83a6c57b1c5f09faaab2629632be99a59 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123681\/pull-kubernetes-linter-hints\/1764760444657995776) | false | `\/test pull-kubernetes-linter-hints`\npull-kubernetes-verify-lint | c0ebc8e83a6c57b1c5f09faaab2629632be99a59 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123681\/pull-kubernetes-verify-lint\/1764760443823329280) | true | `\/test pull-kubernetes-verify-lint`\npull-kubernetes-unit | c0ebc8e83a6c57b1c5f09faaab2629632be99a59 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123681\/pull-kubernetes-unit\/1764760440543383552) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123681). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Adeads2k). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","kind\/cleanup","sig\/scheduling","area\/apiserver","area\/cloudprovider","sig\/api-machinery","size\/L","release-note-none","cncf-cla: yes","sig\/testing","do-not-merge\/work-in-progress","sig\/cloud-provider","needs-priority","needs-triage"]},{"title":"make k8s.io\/apiserver options accomodate different featuregates","body":"\/kind cleanup\r\n\/priority important-soon\r\n\/sig api-machinery\r\n\r\nUpdate the k8s.io\/apiserver to allow composition of featuregates and the kube-apiserver in particular using the capability.\r\n\r\n\r\n```release-note\r\nNONE\r\n```","comments":["[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123680#\" title=\"Author self-approved\">deads2k<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/server\/options\/OWNERS)~~ [deads2k]\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [deads2k]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","capacity problem\r\n\/retest","@deads2k: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-conformance-kind-ga-only-parallel | aba7544deabc3bc47d96f3ff1316688c05883dc3 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123680\/pull-kubernetes-conformance-kind-ga-only-parallel\/1764756037576953856) | true | `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\npull-kubernetes-unit | aba7544deabc3bc47d96f3ff1316688c05883dc3 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123680\/pull-kubernetes-unit\/1764756037631479808) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-integration | aba7544deabc3bc47d96f3ff1316688c05883dc3 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123680\/pull-kubernetes-integration\/1764756037400793088) | true | `\/test pull-kubernetes-integration`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123680). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Adeads2k). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/assign @jpbetz \r\n\/triage accepted\r\n\/retest"],"labels":["priority\/important-soon","kind\/cleanup","area\/apiserver","sig\/api-machinery","size\/M","release-note-none","approved","cncf-cla: yes","triage\/accepted"]},{"title":"kubelet: Add logs for userns custom mappings parsing","body":"This just adds a few more log lines that can be useful when configuring custom mappings for the kubelet userns support.\r\n\r\ncc @giuseppe @mrunalp @haircommander \r\n\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nTo show better logs to users.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nKEP: kep.k8s.io\/127\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123678#\" title=\"Author self-approved\">rata<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [yujuhong](https:\/\/github.com\/yujuhong) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"yujuhong\"]} -->","\/triage accepted\r\n\/priority backlog"],"labels":["priority\/backlog","area\/kubelet","kind\/cleanup","sig\/node","size\/XS","release-note-none","cncf-cla: yes","triage\/accepted"]},{"title":"[Failing Test] gce-master-scale-performance","body":"### Which jobs are failing?\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#gce-master-scale-performance\n\n### Which tests are failing?\n\nci-kubernetes-e2e-gce-scale-performance.Overall\r\nkubetest.ClusterLoaderV2\n\n### Since when has it been failing?\n\n02-25-24 to 02-27-24\r\n\r\nlatest since 03-03-24\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#gce-master-scale-performance\n\n### Reason for failure (if possible)\n\n```\r\nI0303 17:17:36.980573   17996 prometheus.go:439] Waiting for Prometheus stack to become healthy...\r\nW0303 17:17:37.019005   17996 util.go:72] error while calling prometheus api: the server is currently unable to handle the request (get services http:prometheus-k8s:9090), response: k8s\ufffd\r\n\f\r\nv1Status]\r\n\r\n\ufffd\ufffd\ufffdFailure3no endpoints available for service \"prometheus-k8s\"\"ServiceUnavailable0\ufffd\ufffd\"\ufffd\r\n\r\n      \"involvedObject\": {\r\n        \"kind\": \"StatefulSet\",\r\n        \"namespace\": \"monitoring\",\r\n        \"name\": \"prometheus-k8s\",\r\n        \"uid\": \"9d65236a-7d47-4c6d-882d-807df0b57eed\",\r\n        \"apiVersion\": \"apps\/v1\",\r\n        \"resourceVersion\": \"520714\"\r\n      },\r\n      \"reason\": \"FailedCreate\",\r\n      \"message\": \"create Pod prometheus-k8s-0 in StatefulSet prometheus-k8s failed error: pods \\\"prometheus-k8s-0\\\" is forbidden: error looking up service account monitoring\/prometheus-k8s: serviceaccount \\\"prometheus-k8s\\\" not found\",\r\n      \"source\": {\r\n        \"component\": \"statefulset-controller\"\r\n      },\r\n      \"firstTimestamp\": \"2024-03-03T17:17:32Z\",\r\n      \"lastTimestamp\": \"2024-03-03T17:17:32Z\",\r\n      \"count\": 5,\r\n      \"type\": \"Warning\",\r\n      \"eventTime\": null,\r\n      \"reportingComponent\": \"statefulset-controller\",\r\n      \"reportingInstance\": \"\"\r\n    }\r\n    \r\n    ```\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n\/sig scalability","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","last run passed. We will watch this to see if the failure are fixed.","This one has failed again, please take a look https:\/\/testgrid.k8s.io\/sig-release-master-informing#gce-master-scale-performance","@wojtek-t it seems it times out waiting for the prometheus stack to be ready\r\n\r\n> F0309 17:32:22.412466   17806 clusterloader.go:326] Error while setting up prometheus stack: timed out waiting for the condition\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-scale-performance\/1766509724091551744","\ud83d\udfe2 Not a release blocker for v1.30 release cut. Confirmed by @wojtek-t ","> @wojtek-t it seems it times out waiting for the prometheus stack to be ready\r\n\r\nThis flaked before for the same reason before. We need to debug it, but it definitely seems like test issue.","Can we close this or observe another week?\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#gce-master-scale-performance passed 5 times in a row.\n"],"labels":["sig\/scalability","kind\/failing-test","needs-triage"]},{"title":"cel: fix conversion of quantity to quantity","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThe code in ConvertToType checked for conversion into typeValue (= \"kubernetes.URL\") instead of conversion into quantityTypeValue (= \"kubernetes.Quantity\") and thus most likely failed with an incorrect \"type conversion error\".\r\n\r\n#### Special notes for your reviewer:\r\n\r\nFound while reading the code. Not sure whether it has any real-world impact.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\ncel: converting a quantity value into a quantity value failed.\r\n```\r\n\r\n\/assign @cici37 ","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123669#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/cel\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/cel\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","\/assign\r\n\/triage accepted"],"labels":["kind\/bug","area\/apiserver","sig\/api-machinery","release-note","size\/XS","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"controller test: use cancellation from ktesting","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n> This simplifies test code by using the automatic context cancellation support in the updated ktesting package.\r\n\r\nsee https:\/\/github.com\/kubernetes\/kubernetes\/commit\/63aa2615834393acd277abccb3f79422cb0c4452 for more details.\r\n\r\nAnd PR https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122293 is very large, in order to avoid interfering with reviewers, it is not suitable to convert all the places that need to use `tCtx := ktesting.Init(t)`. This PR is to convert the remaining test code of the controller directories.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nfollow up: \r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123399\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122293#discussion_r1478295105\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig apps","\/area test","the `pull-kubernetes-e2e-gce-storage-slow` failures would be fixed by https:\/\/github.com\/kubernetes\/test-infra\/pull\/32160","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123668#\" title=\"Author self-approved\">mengjiao-liu<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [soltysh](https:\/\/github.com\/soltysh) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controller\/cronjob\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/cronjob\/OWNERS)**\n- **[pkg\/controller\/daemon\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/daemon\/OWNERS)**\n- **[pkg\/controller\/deployment\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/deployment\/OWNERS)**\n- **[pkg\/controller\/job\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/job\/OWNERS)**\n- **[pkg\/controller\/podautoscaler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/podautoscaler\/OWNERS)**\n- **[pkg\/controller\/resourceclaim\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/resourceclaim\/OWNERS)**\n- **[pkg\/controller\/tainteviction\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/tainteviction\/OWNERS)**\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"soltysh\"]} -->","\/retest","\/test pull-kubernetes-verify"],"labels":["area\/test","kind\/cleanup","sig\/scheduling","sig\/storage","sig\/node","sig\/autoscaling","size\/XL","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","needs-priority","needs-triage"]},{"title":"cel: add semantic version type","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n[Structured parameters for Dynamic Resource Allocation](https:\/\/github.com\/kubernetes\/enhancements\/issues\/4381) use CEL expressions to filter resource instances by their attributes. One of the possible attribute value types is a semantic version string. The corresponding filter could be \"attribute value >= 2.0.0\".\r\n\r\n#### Which issue(s) this PR fixes:\r\nRelated-to: https:\/\/github.com\/kubernetes\/enhancements\/issues\/4381\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThe key question from an API perspective is: can Kubernetes rely on some particular semver implementation ~~to validate API fields (not in this PR, but calling `semver.Parse` is what validation of the DRA API would do) and~~ to implement the version comparsion (this PR)? See https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516#discussion_r1509193118 for @liggitt's comments in the context of a different solution for the same problem.\r\n\r\ngithub.com\/blang\/semver\/v4 was chosen for this in this PR because it looks like a stable, high-quality implementation and also seems to be the most widely used semver implementation in Kubernetes.\r\n\r\nRegarding the implementation: it is derived from the quantity type and supports the same operations, with one exception: comparison across types (like `<semver> == <int>` ?) is not supported.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nCEL: semver is a new type for semantic version string parsing and comparison.\r\n```\r\n\r\n\/cc @liggitt @klueska @cici37 ","comments":["> can Kubernetes rely on some particular semver implementation to validate API fields (not in this PR, but calling semver.Parse is what validation of the DRA API would do)\r\n\r\nThe alternative is that validation is done with a regular expression. There is even one in the spec:\r\nhttps:\/\/semver.org\/#is-there-a-suggested-regular-expression-regex-to-check-a-semver-string\r\n\r\nThe error message when validation fails is going to be less useful (similar to others, like `a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\"`), but that's okay.\r\n","If we are not comfortable with adding this directly to the base CEL environment in 1.30, then I can add it instead to the CEL environment that gets prepared for DRA expressions. Then it's clearly alpha and we have more time to evolve it before promoting to the base CEL environment.\r\n","I added one commit at the end to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516 which:\r\n- adds the same semver lib to the DRA CEL environment (i.e. doesn't touch the base lib)\r\n- uses regexp for validation instead of `semver\/v4.Parse`\r\n\r\nThat commit is probably easier to merge for 1.30 because it's all in alpha-quality code.","@cici37: I pushed [an update](https:\/\/github.com\/kubernetes\/kubernetes\/compare\/00d24f7340a673bc06d86e81b120e89ff8689a67..f07e6294543985b8ca4f44cfd24caea8e1801113).\r\n\r\nDid I add the necessary checks for cost and library registration correctly? The cost estimates are exactly the same as for quantity. Parsing a semver string shouldn't be more expensive.\r\n\r\nIs this perhaps okay for 1.30? If we can merge it quickly (today?) then I can drop the corresponding code from https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516 and just use this here.\r\n\r\nIf we end up not merging https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516, then this PR could get reverted unless it's seen as useful on its own.\r\n","\/retest pull-kubernetes-integration pull-kubernetes-e2e-gce ","@pohly: The `\/retest` command does not accept any targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123664#issuecomment-1978541726):\n\n>\/retest pull-kubernetes-integration pull-kubernetes-e2e-gce \n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-e2e-gce pull-kubernetes-integration","\/test pull-kubernetes-e2e-gce \t\r\n","@jiahuif could you help with reviewing this one? Thanks","@cici37, @jiahuif: [update](https:\/\/github.com\/kubernetes\/kubernetes\/compare\/f07e6294543985b8ca4f44cfd24caea8e1801113..a083cf61b1596843f787a6ffe735c8da17454620) pushed.\r\n\r\n","\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: a278d15a95425c23a29b5000cdf58fe05e046444<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123664#issuecomment-1979715278\" title=\"Approved\">cici37<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123664#\" title=\"Author self-approved\">pohly<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sttts\"]} -->","\/assign @liggitt \r\n\r\nFor approval as he has some context here.\r\n","\/lgtm","\/triage accepted"],"labels":["area\/apiserver","lgtm","sig\/api-machinery","release-note","size\/XL","kind\/feature","cncf-cla: yes","do-not-merge\/hold","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"Add riscv64 support.","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdds support to build Kubernetes binaries for riscv64 architecture.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nNONE\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: JasenChao \/ (8f5eefe4df319bdaf076d69c5436b2a46a050f9e)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @JasenChao! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @JasenChao. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123661#\" title=\"Author self-approved\">JasenChao<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [enj](https:\/\/github.com\/enj) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"enj\"]} -->","> For the new architecture, is CI coverage required?\r\n\r\nYes, see https:\/\/github.com\/kubernetes\/sig-release\/blob\/master\/release-engineering\/platforms\/guide.md\r\n\r\n> It is not enough for builds to work as it gets bit-rotted quickly when we vendor in new changes, update versions of things we use etc. So we need a good set of tests that exercise a wide battery of jobs in this new architecture.\r\n\r\nPrevious attempts to add riscv64 seemed to stall on this step (https:\/\/github.com\/kubernetes\/kubernetes\/pull\/86011, https:\/\/github.com\/kubernetes\/kubernetes\/pull\/116686)"],"labels":["size\/S","kind\/feature","release-note-none","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage","do-not-merge\/needs-sig"]},{"title":"Bump version of cuda-vector-add used for tests","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nImage is now based on Ubuntu 20.04 and introduce CUDA 11.4\r\n\r\n```bash\r\n$crane manifest registry.k8s.io\/e2e-test-images\/cuda-vector-add:2.4\r\n{\r\n   \"schemaVersion\": 2,\r\n   \"mediaType\": \"application\/vnd.docker.distribution.manifest.list.v2+json\",\r\n   \"manifests\": [\r\n      {\r\n         \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n         \"size\": 3064,\r\n         \"digest\": \"sha256:f5f908f00b4393d1567fa7766e1daca4a8f6b11223be90eafeceab8d17f184d9\",\r\n         \"platform\": {\r\n            \"architecture\": \"amd64\",\r\n            \"os\": \"linux\"\r\n         }\r\n      },\r\n      {\r\n         \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\r\n         \"size\": 3260,\r\n         \"digest\": \"sha256:fd66b88f90a16009fc71febf2cf52da559c94ac5227d1e2636a55680d56b9c04\",\r\n         \"platform\": {\r\n            \"architecture\": \"arm64\",\r\n            \"os\": \"linux\"\r\n         }\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["\/release-note-none\r\n\/priority backlog\r\ntriage accepted","\/triage accepted","cc @dims @mkumatag","\/approve\r\n\/lgtm","LGTM label has been added.  <details>Git tree hash: 2620ea5bf1e22667d4ea9501b620c87dd834f4a4<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123658#\" title=\"Author self-approved\">ameukam<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123658#issuecomment-1975485615\" title=\"LGTM\">dims<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[test\/utils\/image\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/utils\/image\/OWNERS)~~ [dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","I don't think default jobs will really use this image, wondering if we have any specific jobs which uses these images?\r\n\r\n\/hold","\/test ?","@mkumatag: The following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123658#issuecomment-1975640855):\n\n>\/test ?\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","> I don't think default jobs will really use this image, wondering if we have any specific jobs which uses these images?\r\n> \r\n> \/hold\r\n\r\nci-kubernetes-e2e-gce-device-plugin-gpu uses it.\r\n\r\nhttps:\/\/storage.googleapis.com\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-device-plugin-gpu\/1764463338802122752\/artifacts\/bootstrap-e2e-minion-group-m2jc\/images-containerd.log\r\n\r\n","> > I don't think default jobs will really use this image, wondering if we have any specific jobs which uses these images?\r\n> > \/hold\r\n> \r\n> ci-kubernetes-e2e-gce-device-plugin-gpu uses it.\r\n> \r\n> https:\/\/storage.googleapis.com\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-device-plugin-gpu\/1764463338802122752\/artifacts\/bootstrap-e2e-minion-group-m2jc\/images-containerd.log\r\n\r\nI did triggered that here https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123658#issuecomment-1975643633 but test [failed](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123658\/pull-kubernetes-e2e-gce-device-plugin-gpu\/1764496564132253696), not sure why?!","> > > I don't think default jobs will really use this image, wondering if we have any specific jobs which uses these images?\r\n> > > \/hold\r\n> > \r\n> > \r\n> > ci-kubernetes-e2e-gce-device-plugin-gpu uses it.\r\n> > https:\/\/storage.googleapis.com\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gce-device-plugin-gpu\/1764463338802122752\/artifacts\/bootstrap-e2e-minion-group-m2jc\/images-containerd.log\r\n> \r\n> I did triggered that here [#123658 (comment)](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123658#issuecomment-1975643633) but test [failed](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123658\/pull-kubernetes-e2e-gce-device-plugin-gpu\/1764496564132253696), not sure why?!\r\n\r\n@mkumatag Apparently, the driver failed to load the correct firmware:\r\n\r\n```bash\r\nMar 04 04:20:31.886678 e2e-38c3063591-8baae-minion-group-k2cv kernel: nvidia 0000:00:04.0: Direct firmware load for nvidia\/535.154.05\/gsp_tu10x.bin failed with error -2\r\nMar 04 04:20:31.887125 e2e-38c3063591-8baae-minion-group-k2cv kernel: NVRM: RmFetchGspRmImages: No firmware image found\r\nMar 04 04:20:31.887186 e2e-38c3063591-8baae-minion-group-k2cv kernel: NVRM: GPU 0000:00:04.0: RmInitAdapter failed! (0x61:0x56:1599)\r\nMar 04 04:20:31.902065 e2e-38c3063591-8baae-minion-group-k2cv kernel: NVRM: GPU 0000:00:04.0: rm_init_adapter failed, device minor number 0\r\n```\r\n\r\nFrom https:\/\/storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/123658\/pull-kubernetes-e2e-gce-device-plugin-gpu\/1764496564132253696\/artifacts\/e2e-38c3063591-8baae-minion-group-k2cv\/kern.log\r\n\r\nCurrently investigated: https:\/\/kubernetes.slack.com\/archives\/CCK68P2Q2\/p1708914356010229. \r\n\r\nNot related this PR.","I see this https:\/\/github.com\/kubernetes\/test-infra\/pull\/32147 merged, do you wanna retrigger the tests?\r\n\r\nbtw: I don't mind removing the hold on this PR.","\/test pull-kubernetes-e2e-gce-device-plugin-gpu\r\n","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu\r\n\r\n","\/test pull-kubernetes-e2e-gce-device-plugin-gpu\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-blocking#gce-device-plugin-gpu-master looks green today.","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/test","priority\/backlog","kind\/cleanup","lgtm","needs-rebase","size\/XS","release-note-none","approved","cncf-cla: yes","sig\/testing","do-not-merge\/hold","triage\/accepted"]},{"title":"[WIP] Switch versions of go-jose and go-oidc","body":"- `square\/go-jose` has been `archived by the owner` since Feb 27, 2023 there is an active community fork here now https:\/\/github.com\/go-jose\/go-jose\/\r\n- `coreos\/go-oidc` switched over to `go-jose\/go-jose` on Dec 27, 2022 in https:\/\/github.com\/coreos\/go-oidc\/pull\/360\r\n\r\nWe are late to the party :)\r\n\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig auth\r\n\/release-note-none","\/kind cleanup","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123649#\" title=\"Author self-approved\">dims<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)~~ [dims]\n- ~~[LICENSES\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/LICENSES\/OWNERS)~~ [dims]\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [dims]\n- ~~[pkg\/controller\/serviceaccount\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/serviceaccount\/OWNERS)~~ [dims]\n- ~~[pkg\/registry\/core\/serviceaccount\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/registry\/core\/serviceaccount\/OWNERS)~~ [dims]\n- ~~[pkg\/serviceaccount\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/serviceaccount\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kms\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kms\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)~~ [dims]\n- ~~[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)~~ [dims]\n- ~~[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)~~ [dims]\n- ~~[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)~~ [dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/retest","\/assign @liggitt @aramase @enj ","\/hold\r\n\r\nThere are behavioral changes in go-oidc v3 that don't match what I would expect from KAS (will link the old PR later).","https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123253#issuecomment-1940379993\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/117437#issuecomment-1520735668\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/114772#pullrequestreview-1515045221\r\n\r\nNot opposed to making the switch but it needs super careful review which we haven't had bandwidth for yet","apologies, meant to put WIP, but forgot. definitely NOT for 1.30","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/test","sig\/network","kind\/cleanup","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","needs-rebase","size\/XXL","release-note-none","sig\/auth","sig\/apps","approved","sig\/cli","cncf-cla: yes","sig\/instrumentation","sig\/testing","sig\/architecture","do-not-merge\/work-in-progress","do-not-merge\/hold","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"Add JWKS fetch and provider status metrics for jwt authenticator","body":"\/kind feature\r\n\r\n- Add JWKS fetch and provider status metrics for jwt authenticator\r\n\r\n```release-note\r\nkube-apiserver: JWT authenticator now report the following metrics:\r\n- apiserver_authentication_jwt_authenticator_jwks_fetch_last_timestamp_seconds\r\n- apiserver_authentication_jwt_authenticator_jwks_fetch_last_key_set_hash\r\n- apiserver_authentication_jwt_authenticator_provider_status_timestamp_seconds\r\n```\r\n\r\n```docs\r\n[KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-auth\/3331-structured-authentication-configuration\r\n```","comments":["\/sig auth\r\n\/triage accepted\r\n\/priority important-soon","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123642#\" title=\"Author self-approved\">aramase<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controlplane\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controlplane\/OWNERS)**\n- **[pkg\/kubeapiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubeapiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/plugin\/pkg\/authenticator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/plugin\/pkg\/authenticator\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/assign enj","\/milestone v1.30","\/assign liggitt","\/milestone v1.31\r\n\r\nDeferring until next release.","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["priority\/important-soon","area\/apiserver","sig\/api-machinery","release-note","needs-rebase","size\/XL","kind\/feature","sig\/auth","cncf-cla: yes","triage\/accepted"]},{"title":"graduate MatchLabelKeysInPodAffinity to Beta","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n- graduate MatchLabelKeysInPodAffinity to Beta\r\n- add the feature gate switching test.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123156\r\nPart of https:\/\/github.com\/kubernetes\/enhancements\/issues\/3633\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nGraduate MatchLabelKeys\/MismatchLabelKeys feature in PodAffinity\/PodAntiAffinity to Beta\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/issues\/3633\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @alculquicondor @Huang-Wei ","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","\/retest \r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/123621","\/assign @deads2k\r\nfor API, as alpha reviewer.","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123638#pullrequestreview-1917145951\" title=\"Approved\">alculquicondor<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123638#\" title=\"Author self-approved\">sanposhiho<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [deads2k](https:\/\/github.com\/deads2k). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- ~~[test\/integration\/scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/scheduler\/OWNERS)~~ [alculquicondor,sanposhiho]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/retest","LGTM label has been added.  <details>Git tree hash: 0ef01e3cb74fdb6de415f4b973e96efd8f040299<\/details>","Hi, Enhancements Team here, This PR needs `approve` label so that we can mark https:\/\/github.com\/kubernetes\/enhancements\/issues\/3633 as `tracked for code freeze`","\/remove-sig api-machinery\r\n\/sig scheduling"],"labels":["area\/test","sig\/scheduling","lgtm","release-note","size\/L","kind\/api-change","kind\/feature","sig\/apps","cncf-cla: yes","sig\/testing","area\/code-generation","needs-priority","api-review","needs-triage"]},{"title":"Presubmit jobs for image building for test\/images","body":"### What would you like to be added?\n\nAdd the prow presubmit jobs for testing the changes in the test\/images directory\n\n### Why is this needed?\n\nThis is needed to test and confirm the changes made in the test\/image directories","comments":["\/sig testing","\/triage accepted","any presubmit has to be optional please! (folks who need it should run it by hand)","> any presubmit has to be optional please! (folks who need it should run it by hand)\r\n\r\nor may be trigger the job when change happens in that dir? like we are doing it [here](https:\/\/github.com\/kubernetes\/test-infra\/blob\/f0b607a8c9587e5faf3e4fa8eb4f98a8943f72c2\/config\/jobs\/image-pushing\/k8s-staging-e2e-test-images.sh#L83)","i'd make it even more specific (like the version file or something!)","This is going to have skew with the cloudbuild anyhow, we can't securely give presubmit access to the production build env.\r\n\r\nI think it's OK if we merge something and the build fails, and usually we can tell with ... code review.\r\nThe new images aren't used until after manual promotion anyhow. We do not use these from staging.","> This is going to have skew with the cloudbuild anyhow, we can't securely give presubmit access to the production build env.\n> \n\nBig NO for cloudbuilds, wondering if we can use just a docker build.\n\n> \n> \n> I think it's OK if we merge something and the build fails, and usually we can tell with ... code review.\n> \n> The new images aren't used until after manual promotion anyhow. We do not use these from staging.\n\n"],"labels":["kind\/feature","sig\/testing","triage\/accepted"]},{"title":"The image of a running container was deleted by the image garbage collection","body":"### What happened?\n\nThe image of the container was deleted while pod was running.\r\nWhen I noticed it, it was already almost two weeks after the pod was created, and by prometheus metrics, I could see at the time that the pod was created the kubelet performed image garbage collection on the node and images were deleted.\r\n\r\nIt might be worth mentioning that nothing other than the kubelet could have deleted the image from the node.\r\n\r\nThis situation happen a while ago, and I couldn't deal with it for too long while it happened, So unfortunately I don't have data to share. But by looking in the code the following scenario seems to make sense:\r\n1. At the start of the garbage collection process the image was already in the node (for a while), but not in used yet. \r\n2. The garbage collection get the current images and pods on that node from the runtime, because this image is not in use by any pod, it is considered unused.  Ref: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/b340ef2e29b1694464bee60487160654d4bf8bbc\/pkg\/kubelet\/images\/image_gc_manager.go#L232-L247\r\n3. Before the image is actually deleted, A pod using this image is scheduling at this node. since the image is already on the node and the `imagePullPolicy` is `IfNotPresent` there is no need for pulling and the container is start to running. There is no knowledge about that the image is about to be deleted. Ref: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/b340ef2e29b1694464bee60487160654d4bf8bbc\/pkg\/kubelet\/images\/image_manager.go#L126-L143\r\n4. The garbage collection is still running, and it eventually comes to delete this image. There is no knowledge about that the image is in use now.\r\n\r\nIn short, I think that in the time between pods are detected from runtime to find the unused image to the time those images are  actually deleted (the time between step 2 to step 4), new pods could be scheduling on that node, and make use of the images that will be deleted by the garbage collection immediately afterwards. Although the chances of this scenario are rare, the troublesome time I described would be bigger when the node could contain a lot of images and pods, and the difference between `HighThresholdPercent` and `LowThresholdPercent` have an effect as well.\r\n\r\n\n\n### What did you expect to happen?\n\nThe image garbage collection should not delete used images. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSuppose an unused image X has already been on the node for some time.\r\nThe timeline is designed as following,\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/34262680\/01a8992a-95d8-44cb-bbca-8bcf249496a8)\r\nAny pod that uses image X and is scheduling on the node within the orange zone will cause this issue.\r\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nKustomize Version: v5.0.1\r\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T00:36:28Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nlocal\r\n<\/details>\r\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nCRI-O v1.27.3 \r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_","comments":["\/sig node","@tomergayer do you only see this issue with cri-o?\r\n\r\nAFAIK, if you are using docker as your container runtime, docker will block image deletion for any image which is being used by a running container (atleast this is how the docker cli behaves, not sure if the docker's api behaves in the same way)\r\n\r\nso even if the kubelet gc triggers image deletion, docker will block the image deletion eventually if it has a running container using this image\r\n\r\nAlso, what are the effects you saw on the container which was using this image once the underlying image was deleted, did the container exit?","@tomergayer this is how containerd prevents the image from getting deleted if the image is referenced by any running container\r\n\r\n[containerd code ref](https:\/\/github.com\/moby\/moby\/blob\/97a5435d33f644e7cfc0285e483306f2ea410710\/daemon\/containerd\/image_delete.go#L162)","@saschagrunert @haircommander I know that we had some cri-o garbage collection bugs reported recently. Any ideas on this one?","The issue I am thinking of https:\/\/github.com\/cri-o\/cri-o\/issues\/7143 but not sure if this is related.","@kannon92 no that's not related. This issue here is speaking about a race between image GC and container creation.\r\n","Hm, CRI-O also complains when an image is in use (as expected): https:\/\/github.com\/cri-o\/cri-o\/blob\/0f7786ab6b671828dc57d4c16b42dff1f32ec3cf\/vendor\/github.com\/containers\/storage\/store.go#L2560\r\n\r\nIf it's not in use by the runtime, then I can only imagine that the container creation RPC is still in progress but the container has already started, otherwise the creation RPC would fail. ","\/triage accepted\r\n\/priority important-longterm","@Shubham1320 you are right that the runtime might block the remove request if the image is in use, but I don't think the kubelet should to make any assumption about that.\r\n\r\n> @tomergayer this is how containerd prevents the image from getting deleted if the image is referenced by any running container\r\n> \r\n> [containerd code ref](https:\/\/github.com\/moby\/moby\/blob\/97a5435d33f644e7cfc0285e483306f2ea410710\/daemon\/containerd\/image_delete.go#L162)\r\n\r\nHm, looks like you're referring to the wrong place, actually I don't know how containerd will behave in a case you try to remove a image referenced by running container, you should take a look here: https:\/\/github.com\/containerd\/containerd\/blob\/e53663cca75a3dd9c688a65a04f79350d6bb1fbd\/internal\/cri\/server\/images\/image_remove.go#L36C31-L36C42\r\nFYI: by using ```crictl rmi``` you can send a remove request to the runtime similar to the one sent by the kubelet.\r\n\r\n@saschagrunert thanks for the PRs you opened, in kubelet as well as in cri-o. I have some thoughts about your fix in the kubelet. I wonder about the side affect of using a shared lock. Well, this lock will prevent any container (who already scheduling on that node) from starting on the node while the GC is running, and I think it's might not be acceptable in some cases (the delay that could happen in GC can also be a problem). \r\nI can think of two other approaches to deal with it:\r\n1. The problem is about images that are detected as unused by the GC, therefore may it is possible to lock only containers that use those images (the \"mark for deletion\" images), and by this way, not effect any other container. \r\n2. Prevent scheduling of new pods on the node while GC is running. this could be implemented by something like `ImageGCRunnig` node condition.\r\n \r\nThe second approach makes more sense to me, as it prevents any locking or changing in `kuberuntime`, it make it clear why pods are not  scheduling on that node, and allows you to control over it if necessary.\r\n\r\nPlease let me know what you think about it.","> 1. The problem is about images that are detected as unused by the GC, therefore may it is possible to lock only containers that use those images (the \"mark for deletion\" images), and by this way, not effect any other container.\r\n\r\nYeah locking by container image would be an option for the fix.\r\n\r\n> 2. Prevent scheduling of new pods on the node while GC is running. this could be implemented by something like `ImageGCRunnig` node condition.\r\n> \r\n> The second approach makes more sense to me, as it prevents any locking or changing in `kuberuntime`, it make it clear why pods are not scheduling on that node, and allows you to control over it if necessary.\r\n> \r\n> Please let me know what you think about it.\r\n\r\nI like that idea, but that would mean we have to integrate it into a feature like `PodDisruptionConditions`, right? ","Hm, unlike the `PodDisruptionConditions` this is for a bug fix, but because it impact the node status and scheduling behavior I agree we should integrate it into a feature. (Do you think it's worth opening an enhancement?)\r\nI'm starting to work on PR to make the idea more clear.","> Do you think it's worth opening an enhancement?\r\n\r\nI would prefer having it part of an existing one.","I have some thoughts about the implementation of the idea we discussed, currently the time that the kubelet checks for changes in node conditions depends on `nodeStatusUpdateFrequency` (default: 10 seconds). \r\nTo prevent the race between image GC and container creation by using something like `ImageGCRunnig` node condition, it is necessary that this condition be updated when the GC is triggered (disk usage over the threshold) and before the GC get the currents pods from runtime, therefore we can't wait to `nodeStatusUpdateFrequency` for update this condition. \r\nAn option to handle this could be to call the function [syncNodeStatus](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/656cb1028ea5af837e69b5c9c614b008d747ab63\/pkg\/kubelet\/kubelet_node_status.go#L518) when the update of this condition is necessary (image GC is triggered),  but that means checking and updating the entire node status, which can be a little bit overhead for this case. \r\nSo we can add a function for a fast node condition sync, which only performs an update to a specific node condition. But it would require to make a bigger change in the kubelet.\r\n\r\n@saschagrunert I'd love to hear what you think about it and whether you think it's worth adding this function and the changes that will be required for it to the kubelet for the benefit of this idea.","> which only performs an update to a specific node condition. But it would require to make a bigger change in the kubelet.\r\n\r\nI think this would be a good way forward. It may have other implications performance wise, but do you think you could propose that as a draft PR?"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","triage\/accepted"]},{"title":"WIP - simplify update-vendor.sh to use `go work sync`","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nFollowup to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123529\r\n\r\nRemoves a lot of unnecessary logic from update-vendor.sh:\r\n* Syncing all modules to the same minimum versions of dependencies is now done by `go work sync`\r\n* Topological sort before tidying is no longer required since go 1.17 prunes out unused require directives\r\n* Pruning of unused require directives now removes the self-reference accidentally left in staging modules previously\r\n\r\nI still want to poke at this to make sure some of the edges work, but wanted to get it opened:\r\n- [ ] replacing with a non-canonical dep for temporary dev work\r\n- [ ] make sure non-topological tidy doesn't depend on order any more\r\n- [ ] make sure this works to upgrade a dependency\r\n- [ ] make sure this works to downgrade a dependency\r\n- [ ] verify this results in the same version of a dependency being required by all modules that depend on it\r\n\r\ncc @dims @thockin \r\n\r\n```release-note\r\nNONE\r\n```","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123627#\" title=\"Author self-approved\">liggitt<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apimachinery\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/csi-translation-lib\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/csi-translation-lib\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kms\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kms\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/mount-utils\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/mount-utils\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)~~ [liggitt]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/network","area\/kubelet","kind\/cleanup","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","needs-rebase","size\/L","release-note-none","sig\/auth","approved","sig\/cli","cncf-cla: yes","sig\/instrumentation","sig\/architecture","do-not-merge\/work-in-progress","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"KEP-4222: Add tests for CBOR encoder handling of duplicate field names\/tags.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind cleanup\r\n\/sig api-machinery\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdd tests for CBOR encoder handling of duplicate field names\/tags.\r\n\r\nSpecifically, these cases provide coverage for unusual struct types that could conceivably be encoded to CBOR maps containing duplicate keys (which would be invalid).\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/kep.k8s.io\/4222\r\n```\r\n","comments":["Added parallel test cases for the JSON serializer as the CBOR behavior is also based on the behavior of `encoding\/json`:\r\n\r\n> The Go visibility rules for struct fields are amended for JSON when deciding which field to marshal or unmarshal. If there are multiple fields at the same level, and that level is the least nested (and would therefore be the nesting level selected by the usual Go rules), the following extra rules apply:\r\n>\r\n> 1) Of those fields, if any are JSON-tagged, only tagged fields are considered, even if there are multiple untagged fields that would otherwise conflict.\r\n>\r\n> 2) If there is exactly one field (tagged or not according to the first rule), that is selected.\r\n>\r\n> 3) Otherwise there are multiple fields, and all are ignored; no error occurs. ",":exploding_head: ","\/retest-required","\/assign @deads2k \r\n\/triage accepted","\r\n\r\n\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: 9229a4db4496fae369c46604eecfda4734204c80<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123626#\" title=\"Author self-approved\">benluddy<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123626#issuecomment-1995557535\" title=\"Approved\">deads2k<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS)~~ [deads2k]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["kind\/cleanup","lgtm","sig\/api-machinery","size\/L","release-note-none","approved","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"Automated cherry pick of #123570: Filter aggregated apiservice gv","body":"Cherry pick of #123570 on release-1.28.\r\n\r\n#123570: Filter aggregated apiservice gv\r\nalso includes https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123770\r\n\r\nFor details on the cherry pick process, see the [cherry pick requests](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) page.\r\n\r\n```release-note\r\nOpenAPI V2 will no longer publish aggregated apiserver OpenAPI for group-versions not matching the APIService specified group version\r\n```","comments":["This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/kind bug\r\n\/triage accepted\r\n\/lgtm\r\n\/approve\r\n\r\nWill let @kubernetes\/release-team-leads provide cherry-pick approval once tests pass.","@Jefftree can you add a release note to the PR similar to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123570","LGTM label has been added.  <details>Git tree hash: de0e11a6ecef904e73fe97cc1f7793b6c0c5c1d1<\/details>","\/assign @liggitt ","\/approve\r\n\/lgtm","\/hold\r\nDiscovered https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123770","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123625#issuecomment-1979918929\" title=\"LGTM\">dims<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123625#\" title=\"Author self-approved\">Jefftree<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123625#issuecomment-1974040237\" title=\"Approved\">shyamjvs<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [dims]\n- ~~[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/test\/integration\/apiserver\/OWNERS)~~ [Jefftree,dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/unhold","\/retest","\/lgtm","LGTM label has been added.  <details>Git tree hash: 805826e9202ea4b8164245a5409f80a4dfce544e<\/details>","\/cc kubernetes\/release-managers"],"labels":["kind\/bug","area\/test","lgtm","sig\/api-machinery","release-note","size\/L","approved","cncf-cla: yes","sig\/testing","do-not-merge\/cherry-pick-not-approved","needs-priority","triage\/accepted"]},{"title":"Automated cherry pick of #123570: Filter aggregated apiservice gv","body":"Cherry pick of #123570 on release-1.29.\r\n\r\n#123570: Filter aggregated apiservice gv\r\n\r\nAlso includes https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123770\r\n\r\nFor details on the cherry pick process, see the [cherry pick requests](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) page.\r\n\r\n```release-note\r\nOpenAPI V2 will no longer publish aggregated apiserver OpenAPI for group-versions not matching the APIService specified group version\r\n```","comments":["This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/kind bug\r\n\/triage accepted\r\n\/lgtm\r\n\/approve\r\n\r\nWill let @kubernetes\/release-team-leads provide cherry-pick approval once tests pass.","@Jefftree can you add a release note to the PR similar to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123570","\/assign @liggitt ","\/approve\r\n\/lgtm","\/lgtm","\/lgtm cancel","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123624#issuecomment-1979918985\" title=\"LGTM\">dims<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123624#\" title=\"Author self-approved\">Jefftree<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123624#issuecomment-1974040324\" title=\"Approved\">shyamjvs<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.29\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [dims]\n- ~~[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.29\/test\/integration\/apiserver\/OWNERS)~~ [Jefftree,dims]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","\/retest\nPod scheduling timeout ","\/retest\r\nscheduling timeout","\/lgtm","LGTM label has been added.  <details>Git tree hash: 3e7fc7518335fa8415be4c5b573f6c9467574dbc<\/details>","\/cc kubernetes\/release-managers"],"labels":["kind\/bug","area\/test","lgtm","sig\/api-machinery","release-note","size\/L","approved","cncf-cla: yes","sig\/testing","do-not-merge\/cherry-pick-not-approved","needs-priority","triage\/accepted"]},{"title":"Remove shared ref to underlying array of JSONFrameReader's Read arg.","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind bug\r\n\/sig api-machinery\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nWhen JSONFrameReader read a JSON object longer than the length of the destination slice, but not larger than the capacity of the destination slice, it would retain a reference to a slice sharing the same underlying array as the destination slice. If the underlying array is modified between calls to Read, corrupt frame data could be returned.\r\n\r\nThis is also called out in the io.Reader contract: \"Implementations must not retain p.\"\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThe current usage in `k8s.io\/apimachinery\/pkg\/runtime\/serializer\/streaming` does not appear to be affected. Even though it passes the frame reader slices with cap > len, it always either creates a new slice or ignores the slice contents on io.ErrShortBuffer.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFixed a bug in the JSON frame reader that could cause it to retain a reference to the underlying array of the byte slice passed to Read.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123620#\" title=\"Author self-approved\">benluddy<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https:\/\/github.com\/apelisse) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/util\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/util\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"apelisse\"]} -->","@benluddy: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-linter-hints | 3fe1165fe192e9dcf99941c20bc797b97097d2e3 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123620\/pull-kubernetes-linter-hints\/1763584290982989824) | false | `\/test pull-kubernetes-linter-hints`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123620). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Abenluddy). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123470\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/123621\r\n\r\n\/test pull-kubernetes-unit","\/cc @wojtek-t ","\/triage accepted"],"labels":["kind\/bug","sig\/api-machinery","release-note","size\/M","cncf-cla: yes","needs-priority","triage\/accepted"]},{"title":"show warning message only when running `kubectl get --watch-only`","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nFixes a command line warning for the `get` subcommand which was displaying that the sort was ignored when `--watch` or `--watch-only` was pass in. The warning should only be displayed when `--watch-only` is passed in as a flag.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123618\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\npassing in flag `--watch` with `get` subcommand will not display a warning message when coupled with `--sort-by` \r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\npassing in flag `--watch` with `get` subcommand will not display a warning message when coupled with `--sort-by` \r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: Eduard-Voiculescu \/ (81810d52818e07100460b56381572d49f9150395)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @Eduard-Voiculescu! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @Eduard-Voiculescu. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123619#\" title=\"Author self-approved\">Eduard-Voiculescu<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [eddiezane](https:\/\/github.com\/eddiezane) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"eddiezane\"]} -->","I think the warning still makes sense with watch or watch-only\u2026 the watch things returned will not be sorted ","That's interesting, cause here is what I get when running the command:\r\n\r\n```bash\r\n\u276f k get events --sort-by=\"{.lastTimestamp}\" -w\r\nwarning: --watch or --watch-only requested, --sort-by will be ignored\r\nLAST SEEN   TYPE      REASON              OBJECT                                                    MESSAGE\r\n4m5s        Normal    Sync                ingress\/default-ingress                                   Scheduled for sync\r\n3m50s       Normal    Killing             pod\/******                                                Stopping container ***\r\n3m47s       Normal    SuccessfulCreate    replicaset\/******                                         Created pod: ******\r\n3m44s       Warning   FailedScheduling    pod\/******                                                0\/8 nodes are available: 1 Insufficient memory, 1 node(s) didn't match Pod's node affinity\/selector, 7 Insufficient cpu. preemption: 0\/8 nodes are available: 1 Preemption is not helpful for scheduling, 7 No preemption victims found for incoming pod..\r\n3m44s       Normal    NotTriggerScaleUp   pod\/******                                                pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) didn't match Pod's node affinity\/selector, 1 max node group size reached\r\n3m20s       Normal    Scheduled           pod\/******                                                Successfully assigned staging\/****** to gke-******\r\n3m19s       Normal    Pulled              pod\/******                                                Container image \"******\" already present on machine\r\n3m19s       Normal    Created             pod\/******                                                Created container ***\r\n3m19s       Normal    Started             pod\/******                                                Started container ***\r\n3m17s       Warning   Unhealthy           pod\/******                                                Readiness probe failed: dial tcp *.*.*.*:****: connect: connection refused\r\n70s         Warning   NegCRError          servicenetworkendpointgroup\/******   failed to delete NEG ****** in ******: googleapi: Error 400: The network_endpoint_group resource 'projects\/******' is already being used by 'projects\/******', resourceInUseByAnotherResource\r\n0s          Normal    Killing             pod\/******                                                Stopping container ***\r\n0s          Warning   FailedScheduling    pod\/******                                                0\/8 nodes are available: 1 Insufficient memory, 1 node(s) didn't match Pod's node affinity\/selector, 7 Insufficient cpu. preemption: 0\/8 nodes are available: 1 Preemption is not helpful for scheduling, 7 No preemption victims found for incoming pod..\r\n0s          Normal    SuccessfulCreate    replicaset\/******                                         Created pod: ***-******\r\n0s          Normal    NotTriggerScaleUp   pod\/******                                                pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) didn't match Pod's node affinity\/selector, 1 max node group size reached\r\n0s          Warning   FailedScheduling    pod\/******                                                0\/8 nodes are available: 1 Insufficient memory, 1 node(s) didn't match Pod's node affinity\/selector, 7 Insufficient cpu. preemption: 0\/8 nodes are available: 1 Preemption is not helpful for scheduling, 7 No preemption victims found for incoming pod..\r\n0s          Normal    Scheduled           pod\/******                                                Successfully assigned ***\/****** to gke-******\r\n0s          Normal    Pulled              pod\/******                                                Container image \"******\" already present on machine\r\n0s          Normal    Created             pod\/******                                                Created container ***\r\n0s          Normal    Started             pod\/******                                                Started container ***\r\n0s          Warning   Unhealthy           pod\/******                                                Readiness probe failed: dial tcp *.*.*.*:****: connect: connection refused\r\n0s          Warning   Unhealthy           pod\/******                                                Readiness probe failed: dial tcp *.*.*.*:****: connect: connection refused\r\n```\r\n\r\nThe events that have already occurred are all sorted. And also, all the new events, seem to be sorted, just the time is not updated.","> And also, all the new events, seem to be sorted, just the time is not updated.\r\n\r\nNew events are output in the order they occur, without any sort criteria applied\r\n\r\n> The events that have already occurred are all sorted.\r\n\r\nI think you're right, --sort-by does apply to existing objects at the start, but isn't reliable for the overall output. I'm ok with tweaking the warning, but --sort-by doesn't *actually* sort _all_ the output when used with watch or watch-only, so someone using it in combination with those options likely isn't getting the result they expect"],"labels":["area\/kubectl","release-note","size\/XS","sig\/cli","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"warning message for `get` subcommand with `--watch`","body":"### What happened?\n\nWhen running subcommand `get` like this: `kubernetes get events --sort-by=\"{.lastTimestamp}\" -w` a warning message will display: `warning: --watch or --watch-only requested, --sort-by will be ignored` which is incorrect in `watch` mode. It is only correct in `--watch-only` mode.\n\n### What did you expect to happen?\n\nWarning message to only display when `--watch-only` flag is passed in. \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun `kubernetes get events --sort-by=\"{.lastTimestamp}\" -w`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\nClient Version: v1.27.1\r\nKustomize Version: v5.0.1\r\nServer Version: v1.27.8-gke.1067004\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig cli"],"labels":["kind\/bug","sig\/cli","needs-triage"]},{"title":"When the node is in the NotReady state for 5 minutes, the DamonSet Pod on the node is still in the Running state.","body":"### What happened?\r\n\r\nWhen the kubelet on the node server2 stopped for 5 minutes, it was found that the Pod of DamonSet on the server2 node was still in the Running state. as follows:\r\n![9bb35de7f03cfaf2a9a16225b7e3c4b](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/110345347\/7b8c6a8e-7ac0-4cfc-b8b9-ecce896408d3)\r\n\r\n\r\n### What did you expect to happen?\r\n\r\nWhen the node is in the NotReady state for 5 minutes, the Pod of the DamonSet on the node should be in the NodeLost state.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. In a k8s cluster, stop the kubelet on one of the nodes.\r\n2. After 5 minutes, observe the running status of the DamonSet Pod on this node.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version v1.28.2\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nlocal\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\nCRI-O v1.28.2\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\nCalico \r\n<\/details>\r\n\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\n\/kind bug","This might be WAI. Did you check the tolerations in the pod spec?\r\n\r\n\/triage needs-information\r\n\/priority backlog","> This might be WAI. Did you check the tolerations in the pod spec?\r\n> \r\n> \/triage needs-information \/priority backlog\r\n\r\nAll DeamonSets remain in Running status after the kubelet crash, and Pod has not added a tolerance."],"labels":["kind\/bug","priority\/backlog","sig\/node","triage\/needs-information","needs-triage"]},{"title":"Add --no-headers option to config get-clusters cli","body":"\/kind bug\r\n\r\nThis PR adds --no-headers option cli flag for displaying the output of 'kubectl config get-clusters' without the headers \r\n\r\nFixes #123466 --no-headers available for k config get-contexts but not get-clusters\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: Shubham1320 \/ (063a5987bf3ca39890d5de21a8d670609e0e985c)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @Shubham1320! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @Shubham1320. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123610#\" title=\"Author self-approved\">Shubham1320<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https:\/\/github.com\/apelisse) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"apelisse\"]} -->","@Shubham1320: Reiterating the mentions to trigger a notification: \n@kubernetes\/sig-cli-pr-reviews\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123610#issuecomment-1986715092):\n\n>@kubernetes\/sig-cli-pr-reviews\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","@seans3 @ardaguclu can you please give an estimate of when can I expect a review on this PR? "],"labels":["kind\/bug","area\/kubectl","size\/M","sig\/cli","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage"]},{"title":"Pod Crashloop backoff requeue issues","body":"When stepping through the pod crashloop backoff implementation, I noticed a few issues:\r\n\r\n1. Backoff errors are unsuccessfully suppressed:\r\n  - This code is non-functional, since there will always be a non-error result present (`ConfigPodSandbox`) with a nil error: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/5cf4fbe524ca1479607a4880949a032064556f76\/pkg\/kubelet\/kubelet.go#L1964-L1971\r\n2. The propogated backoff error means that the pod worker always immediately requeues the pod: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/5cf4fbe524ca1479607a4880949a032064556f76\/pkg\/kubelet\/pod_workers.go#L1495-L1497\r\n  - If the backoff error were suppressed, then the default 1m sync frequency would be used, which would cause very long initial backoffs.\r\n\r\nThe result of these issues are:\r\n1. Log spam: `E0301 01:06:01.286649    1869 pod_workers.go:1294] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"alpine\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=alpine pod=restarting-pod_default(ca0d5836-1435-497e-9a41-547fbcc10484)\\\"\" pod=\"default\/restarting-pod\" podUID=ca0d5836-1435-497e-9a41-547fbcc10484` get's logged in a frequent loop\r\n2. Kubelet does unnecessary work to reexamine pods and containers in backoff\r\n3. (minor) the error retry period is 10s, which adds up to 10s to the backoff time\r\n\r\nI think the ideal fix is to propagate the backoff retry time up from SyncPod [1] to the pod worker [2], so that the pod can be requeued with the exact (jittered) right amount of retry time. One option is to use a custom error type to encapsulate the backoff time, which I started working on here: https:\/\/github.com\/tallclair\/kubernetes\/commit\/5d89eb2538cc8929540476f436d4a1b92c9babee\r\n\r\nOf course, there is very little test coverage across these code paths too.\r\n\r\n[1]: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/5cf4fbe524ca1479607a4880949a032064556f76\/pkg\/kubelet\/kuberuntime\/kuberuntime_manager.go#L1325\r\n[2]: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/5cf4fbe524ca1479607a4880949a032064556f76\/pkg\/kubelet\/pod_workers.go#L1484\r\n\r\n\/sig node\r\n\/kind bug","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @lauralorenz \r\n\/cc @SergeyKanzhelev @thockin @smarterclayton ","\/priority important-longterm"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","needs-triage"]},{"title":"Bump GPU Installer","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nBumping GPU Installer to resolve test failures\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123491\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123600#\" title=\"Author self-approved\">upodroid<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [neolit123](https:\/\/github.com\/neolit123) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"neolit123\"]} -->","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","We are getting somewhere\r\n\r\n```\r\n mahamed \ue0b0 MAHAALI-M-2PY9 \ue0b0 ~ \ue0b0 $ \ue0b0 k logs nvidia-driver-installer-5wlx5 -c nvidia-driver-installer\r\nI0301 14:28:51.921855    3396 installer.go:446] Getting the default GPU driver version\r\nI0301 14:28:51.922432    3396 artifacts.go:134] Start to download gpu_default_version artifact from bucket: cos-tools, object: 17800.147.22\/lakitu\/gpu_default_version to \/tmp\/tmp289263239\/gpu_default_version.\r\nI0301 14:28:52.050673    3396 gcs_client.go:75] Sucessfully downloaded gcs object from bucket: cos-tools, object: 17800.147.22\/lakitu\/gpu_default_version to \/tmp\/tmp289263239\/gpu_default_version.\r\nI0301 14:28:52.050708    3396 artifacts.go:139] Sucessfully downloaded gpu_default_version artifact from bucket: cos-tools, object: 17800.147.22\/lakitu\/gpu_default_version to \/tmp\/tmp289263239\/gpu_default_version.\r\nI0301 14:28:52.050983    3396 install.go:289] Installing GPU driver version 535.154.05\r\nI0301 14:28:52.051073    3396 cache.go:76] error: failed to read file \/root\/home\/kubernetes\/bin\/nvidia\/.cache: open \/root\/home\/kubernetes\/bin\/nvidia\/.cache: no such file or directory\r\nI0301 14:28:52.051094    3396 artifacts.go:147] Start to check whether artifact: 17800.147.22\/lakitu\/nvidia-drivers-535.154.05.tgz exists in bucket: cos-tools\r\nI0301 14:28:52.072812    3396 gcs_client.go:116] Successfully find object: 17800.147.22\/lakitu\/nvidia-drivers-535.154.05.tgz from bucket: cos-tools\r\nI0301 14:28:52.072838    3396 installer.go:137] Configuring driver installation directories\r\nI0301 14:28:52.141915    3396 installer.go:722] Downloading GPU driver installer version 535.154.05\r\nI0301 14:28:52.143114    3396 utils.go:88] Downloading GPU driver installer from https:\/\/storage.googleapis.com\/nvidia-drivers-us-public\/tesla\/535.154.05\/NVIDIA-Linux-x86_64-535.154.05.run\r\nI0301 14:28:53.667191    3396 artifacts.go:134] Start to download nvidia-drivers-535.154.05.tgz artifact from bucket: cos-tools, object: 17800.147.22\/lakitu\/nvidia-drivers-535.154.05.tgz to \/usr\/local\/nvidia\/nvidia-drivers-535.154.05.tgz.\r\nI0301 14:28:54.079798    3396 gcs_client.go:75] Sucessfully downloaded gcs object from bucket: cos-tools, object: 17800.147.22\/lakitu\/nvidia-drivers-535.154.05.tgz to \/usr\/local\/nvidia\/nvidia-drivers-535.154.05.tgz.\r\nI0301 14:28:54.079906    3396 artifacts.go:139] Sucessfully downloaded nvidia-drivers-535.154.05.tgz artifact from bucket: cos-tools, object: 17800.147.22\/lakitu\/nvidia-drivers-535.154.05.tgz to \/usr\/local\/nvidia\/nvidia-drivers-535.154.05.tgz.\r\nI0301 14:28:57.880090    3396 modules.go:171] loading module: \/usr\/sbin\/insmod \/usr\/local\/nvidia\/drivers\/nvidia.ko\r\nI0301 14:28:58.713464    3396 modules.go:171] loading module: \/usr\/sbin\/insmod \/usr\/local\/nvidia\/drivers\/nvidia-uvm.ko\r\nI0301 14:28:58.974266    3396 modules.go:171] loading module: \/usr\/sbin\/insmod \/usr\/local\/nvidia\/drivers\/nvidia-modeset.ko\r\nI0301 14:28:59.106122    3396 modules.go:171] loading module: \/usr\/sbin\/insmod \/usr\/local\/nvidia\/drivers\/nvidia-drm.ko\r\nI0301 14:29:05.884306    3396 installer.go:311] Installing userspace libraries...\r\nI0301 14:29:05.884344    3396 installer.go:323] Installer arguments:\r\n[\/tmp\/extract\/nvidia-installer --utility-prefix=\/usr\/local\/nvidia --opengl-prefix=\/usr\/local\/nvidia --x-prefix=\/usr\/local\/nvidia --install-libglvnd --no-install-compat32-libs --log-file-name=\/usr\/local\/nvidia\/nvidia-installer.log --silent --accept-license --no-kernel-module]\r\nE0301 14:29:05.898435    3396 utils.go:355]\r\nE0301 14:29:05.899075    3396 utils.go:355] WARNING: You specified the '--no-kernel-modules' command line option, nvidia-installer will not install any kernel modules as part of this driver installation, and it will not remove existing NVIDIA kernel modules not part of an earlier NVIDIA driver installation.  Please ensure that NVIDIA kernel modules matching this driver version are installed separately.\r\nE0301 14:29:05.899147    3396 utils.go:355]\r\nE0301 14:29:05.899177    3396 utils.go:355]\r\nE0301 14:29:05.899230    3396 utils.go:355] WARNING: nvidia-installer was forced to guess the X library path '\/usr\/local\/nvidia\/lib64' and X module path '\/usr\/local\/nvidia\/lib64\/xorg\/modules'; these paths were not queryable from the system.  If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK\/development package for your distribution and reinstall the driver.\r\nE0301 14:29:05.899295    3396 utils.go:355]\r\nE0301 14:29:05.901432    3396 utils.go:355]\r\nE0301 14:29:05.901595    3396 utils.go:355] WARNING: This NVIDIA driver package includes Vulkan components, but no Vulkan ICD loader was detected on this system. The NVIDIA Vulkan ICD will not function without the loader. Most distributions package the Vulkan loader; try installing the \"vulkan-loader\", \"vulkan-icd-loader\", or \"libvulkan1\" package.\r\nE0301 14:29:05.901775    3396 utils.go:355]\r\nI0301 14:29:12.800685    3396 installer.go:327] Done installing userspace libraries\r\nI0301 14:29:12.800908    3396 cache.go:64] Updated cached version as\r\nI0301 14:29:12.800926    3396 cache.go:66] KERNEL_OPEN=Y\r\nI0301 14:29:12.800946    3396 cache.go:66] BUILD_ID=17800.147.22\r\nI0301 14:29:12.800953    3396 cache.go:66] DRIVER_VERSION=535.154.05\r\nI0301 14:29:12.800968    3396 installer.go:59] Verifying GPU driver installation\r\nE0301 14:29:13.055304    3396 install.go:534] failed to verify installation: failed to verify GPU driver installation: exit status 6\r\n```","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","\/test pull-kubernetes-e2e-gce-device-plugin-gpu","@upodroid: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-gce-device-plugin-gpu | 03c3156e235be89771826483269457d4b857626c | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123600\/pull-kubernetes-e2e-gce-device-plugin-gpu\/1763662014049161216) | false | `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123600). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aupodroid). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["kind\/bug","area\/test","needs-rebase","size\/M","release-note-none","cncf-cla: yes","sig\/testing","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"Improving legibility of kubectl describe configmap output","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubectl\/issues\/1559\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nAdded an extra line between two different key value pairs under data when running kubectl describe configmap\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: siddhantvirus \/ (cb560a72e0f63a28bdc7b8518f9de89d59f76be5)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @siddhantvirus! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @siddhantvirus. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123597#\" title=\"Author self-approved\">siddhantvirus<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [seans3](https:\/\/github.com\/seans3) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"seans3\"]} -->"],"labels":["kind\/bug","area\/kubectl","release-note","size\/XS","sig\/cli","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage"]},{"title":"Ephemeral volumes cannot be reclaimed when Pod unadmitted by Kubelet","body":"### What happened?\n\nWhen I create a pod that needs to use aGeneric ephemeral volume, it also depends on the device plugin. \r\nHowever, when the pod is scheduled to a specified node, my device plugin is restarted.\r\nIn this case, the pod is unadmitted to be created on this node. Then the pod's status changed to UnexpectedAdmissionError.\r\nAlthough the finally available pod was recreated after the device plugin was ready, the pv associated with the pod whose status is UnexpectedAdmissionError is not reclaimed.\r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/46313756\/96efe990-54c0-41c6-8d10-9d06898b6967)\r\n\n\n### What did you expect to happen?\n\nI know that the pod with UnexpectedAdmissionError state still exists for users to confirm what happened to the cluster at that time, but I think the pv should be released. Otherwise, my storage resources will be wasted.\r\nWorse case is that if this happens a few more times, my cluster's storage resources may be exhausted and new pods will not be scheduled in.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI modified the kubelet code to make the pods I created with a specific name return a rejection directly, and this problem can occur directly.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nKubernetes: 1.28.1\n\n### Cloud provider\n\n<details>\r\nNo\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig storage"],"labels":["kind\/bug","sig\/storage","needs-triage"]},{"title":"Eviction manager: When stats calls failed, we evict the pod that we couldn't get stats for.","body":"### What happened?\r\n\r\nAs I was investigating the eviction tests, I discovered an interesting problem. We are seeing a wide range of flakes where the one pod that should not be evicted gets evicted in the eviction test.  \r\n\r\nEviction tests work by assigning one pod that it should never be evicted and then it ranks these pods and evicts the other pods. If there is a stats API failure, we evict the pod that we couldn't get stats for.\r\n\r\nThis seems to be by design (cc @derekwaynecarr)\r\n\r\n### What did you expect to happen?\r\n\r\nWe should maybe consider a more deterministic approach if stats fails.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nTry running the eviction tests on the PR.\r\n\r\n### Anything else we need to know?\r\n\r\nCode that causes this is here:\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/eviction\/helpers.go#L747\r\n\r\n\r\n\r\n### Kubernetes version\r\n\r\nall supported\r\n\r\n### Cloud provider\r\n\r\nna\r\n\r\n### OS version\r\n\r\nna\r\n\r\n### Install tools\r\n\r\nna\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\ncrio and containerd\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n_No response_","comments":["\/kind failing-test","\/triage accepted\r\n\r\nThis seems to be one of the causes for eviction test flakiness.\r\n\r\nI've seen in mostly in disk stats.","Some ideas could be to use QoS in case of stats failure. We could say that BestEffort gets evicted before Burstable and Burstable before Guaranteed. This could help in the case of stats failure so we can control the eviction a bit better.\r\n\r\nAn interesting thing about this is this doesn't help disk based eviction as ephemeral-storage is never considered guaranteed (guaraneteed is only for cpu\/memory).\r\n\r\n","Pid stats is another area where we are seeing failures but that is related to all pid stats seemed to be returning 0 so any stats gathering is not working.\r\n","cc @pacoxu ","\/sig node\r\n?","\/priority important-longterm ","Yeah overloading the compare prioritization with error checking appears on face to be problematic.  I'd try to split that out into two separate funcs one for get stats another for the compare.  The inability to get stats for pod a may be due to a block of the resource by pod b that will be ignored because of the failure it caused?  ","Brought this up to sig-node.\r\n\r\nWe should look into ways to populate cache more aggressively. Qos could be an option but it is worth investigating why stats gathering fails."],"labels":["kind\/bug","sig\/node","priority\/important-longterm","kind\/failing-test","triage\/accepted"]},{"title":"test\/e2e: do not use global variable for image","body":"\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nWe have \"-kube-test-repo-list\" command line flag to override the image registry. If we store it in global variable, then that overriding cannot take effect.\r\n\r\nAnd this can cause puzzling bugs, e.g.: containerIsUnused() function will compare incorrect image address.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFix \"-kube-test-repo-list\" e2e flag may not take effect\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @huww98. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123587#\" title=\"Author self-approved\">huww98<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [neolit123](https:\/\/github.com\/neolit123) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"neolit123\"]} -->","interesting, how this was working before then and nobody noticed until now?","Maybe because forks are using KUBE_TEST_REPO_LIST env. The flag was added last year.","\/triage accepted\r\n\/priority important-longterm\r\n\/ok-to-test","\/retest","the `pull-kubernetes-e2e-gce-storage-slow` failures would be fixed by https:\/\/github.com\/kubernetes\/test-infra\/pull\/32160","\/retest"],"labels":["kind\/bug","area\/test","sig\/network","sig\/scheduling","sig\/storage","sig\/node","sig\/api-machinery","release-note","sig\/autoscaling","size\/L","sig\/apps","sig\/windows","sig\/cli","cncf-cla: yes","sig\/testing","priority\/important-longterm","ok-to-test","area\/e2e-test-framework","triage\/accepted"]},{"title":"kubeadm: persist dns resourceRequirements on upgrade","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\/kind bug\r\n\/kind feature\r\n#### What this PR does \/ why we need it:\r\nkubeadm - keep dns resourceRequirements on upgrade\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: [`Fixes kubernetes\/kubeadm#3033`, or `Fixes ([paste link of issue](https:\/\/github.com\/kubernetes\/kubeadm\/issues\/3033))`](https:\/\/github.com\/kubernetes\/kubeadm\/issues\/3033).\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubeadm\/issues\/3033\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nkeep dns resourceRequirements after kubeadm upgrade\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><\/a><br\/><br \/>The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: maxl99  (a673545fe3dd442184f80c8102ea41021a1a02e4)<\/li><\/ul>","Welcome @maxl99! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @maxl99. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123586#\" title=\"Author self-approved\">maxl99<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [pacoxu](https:\/\/github.com\/pacoxu) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kubeadm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubeadm\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"pacoxu\"]} -->"],"labels":["kind\/bug","sig\/cluster-lifecycle","release-note","size\/L","kind\/feature","area\/kubeadm","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage"]},{"title":"Rename APIServer trace span name to conform to http server guidelines","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\nThe span name of APIServer is not conformed to [http server's best practice](https:\/\/github.com\/open-telemetry\/opentelemetry-specification\/blob\/v1.1.0\/specification\/trace\/api.md#span). \r\n\r\nFor now the span names are all \"KubernetesAPI\" \r\n\r\nAfter this change, the span name will be like following\r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/11855957\/53edad94-004c-4ba7-a604-05279b31d1c0)\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\nFix #112059\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n```docs\r\n\r\n```\r\n","comments":["Hi @fatsheep9146. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","@dashpole could you help add ok-to-test label?","\/ok-to-test","Should this have the method in the span name?  https:\/\/opentelemetry.io\/docs\/specs\/semconv\/http\/http-spans\/#name\r\n\r\n> HTTP server span names SHOULD be `{method} {http.route}`","> Should this have the method in the span name? https:\/\/opentelemetry.io\/docs\/specs\/semconv\/http\/http-spans\/#name\r\n> \r\n> > HTTP server span names SHOULD be `{method} {http.route}`\r\n\r\nYes you are right, I will fix it","\/assign @dashpole \r\nFor context\r\n\/triage accepted","> Should this have the method in the span name? https:\/\/opentelemetry.io\/docs\/specs\/semconv\/http\/http-spans\/#name\r\n> \r\n> > HTTP server span names SHOULD be `{method} {http.route}`\r\n\r\n@dashpole \r\nDone, now the span name is `{method} {http.route}`\r\n\r\n![image](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/11855957\/0851ef97-39cc-4429-ac1e-c5a6df8edbcd)\r\n","\/test pull-kubernetes-integration","LGTM label has been added.  <details>Git tree hash: 67acfc53b2ccae4d76cda96a7c2877cef66164f6<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123584#pullrequestreview-1938985358\" title=\"LGTM\">dashpole<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123584#\" title=\"Author self-approved\">fatsheep9146<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https:\/\/github.com\/apelisse) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"apelisse\"]} -->","\/assign apelisse\r\n\r\npinging @apelisse  for approval"],"labels":["area\/test","area\/apiserver","lgtm","sig\/api-machinery","size\/M","kind\/feature","release-note-none","cncf-cla: yes","sig\/testing","ok-to-test","needs-priority","triage\/accepted"]},{"title":"Cannot upgrade APIVersion and add new field at the same time with server-side apply","body":"### What happened?\r\n\r\nWhen SSA-ing an object that has been created with an old storage version (i.e. v1alpha1) to the latest storage version (i.e. v1beta1) and adding a new property property that only exists in the new version, the attempt fails with an error.\r\n\r\n### What did you expect to happen?\r\n\r\nThe API server should use the schema of the new version specified in the SSA payload instead of using the old version and upgrade the object and add the new property.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. Create an empty cluster (we have been using kind for testing)\r\n2. Apply a new CRD with an alpha version:\r\n```yaml\r\n---\r\napiVersion: apiextensions.k8s.io\/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: examples.example.com\r\nspec:\r\n  group: example.com\r\n  names:\r\n    kind: Example\r\n    listKind: ExampleList\r\n    plural: examples\r\n    singular: example\r\n  scope: Cluster\r\n  versions:\r\n  - name: v1alpha1\r\n    served: true\r\n    storage: true\r\n    subresources:\r\n      status: {}\r\n    schema:\r\n      openAPIV3Schema:\r\n        type: object\r\n        properties:\r\n          spec:\r\n            type: object\r\n            properties:\r\n              alphaField:\r\n                type: string\r\n```\r\n\r\n3. Apply a sample object\r\n```yaml\r\n---\r\napiVersion: example.com\/v1alpha1\r\nkind: Example\r\nmetadata:\r\n  name: example\r\nspec:\r\n  alphaField: \"test\"\r\n```\r\n\r\n4. Update the CRD and add a new version `v1beta1` that contains a new field `spec.betaField`.\r\n```yaml\r\n---\r\napiVersion: apiextensions.k8s.io\/v1\r\nkind: CustomResourceDefinition\r\nmetadata:\r\n  name: examples.example.com\r\nspec:\r\n  group: example.com\r\n  names:\r\n    kind: Example\r\n    listKind: ExampleList\r\n    plural: examples\r\n    singular: example\r\n  scope: Cluster\r\n  versions:\r\n  - name: v1alpha1\r\n    served: true\r\n    storage: false    \r\n    subresources:\r\n      status: {}\r\n    schema:\r\n      openAPIV3Schema:\r\n        type: object\r\n        properties:\r\n          spec:\r\n            type: object\r\n            properties:\r\n              alphaField:\r\n                type: string\r\n  - name: v1beta1\r\n    served: true\r\n    storage: true    \r\n    subresources:\r\n      status: {}\r\n    schema:\r\n      openAPIV3Schema:\r\n        type: object\r\n        properties:\r\n          spec:\r\n            type: object\r\n            properties:\r\n              alphaField:\r\n                type: string\r\n              betaField:\r\n                type: string\r\n```\r\n\r\n5. Add `betaField` to the object and apply it with `kubectl apply --server-side`\r\n```yaml\r\n---\r\napiVersion: example.com\/v1beta1\r\nkind: Example\r\nmetadata:\r\n  name: example\r\nspec:\r\n  alphaField: \"test\"\r\n  betaField: \"test2\"\r\n```\r\n\r\n\r\nKubectl will fail with an error:\r\n\r\n```\r\nError from server: failed to convert new object: .spec.betaField: field not declared in schema\r\n```\r\n\r\nIf using CSA `kubectl apply` it works without an issue:\r\n```\r\nexample.example.com\/example configured\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.2\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\nn.a. since everything was run locally with Kind.\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n$ cat \/etc\/os-release\r\nNAME=\"Fedora Linux\"\r\nVERSION=\"39 (Server Edition)\"\r\nID=fedora\r\nVERSION_ID=39\r\nVERSION_CODENAME=\"\"\r\nPLATFORM_ID=\"platform:f39\"\r\nPRETTY_NAME=\"Fedora Linux 39 (Server Edition)\"\r\nANSI_COLOR=\"0;38;2;60;110;180\"\r\nLOGO=fedora-logo-icon\r\nCPE_NAME=\"cpe:\/o:fedoraproject:fedora:39\"\r\nHOME_URL=\"https:\/\/fedoraproject.org\/\"\r\nDOCUMENTATION_URL=\"https:\/\/docs.fedoraproject.org\/en-US\/fedora\/f39\/system-administrators-guide\/\"\r\nSUPPORT_URL=\"https:\/\/ask.fedoraproject.org\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugzilla.redhat.com\/\"\r\nREDHAT_BUGZILLA_PRODUCT=\"Fedora\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=39\r\nREDHAT_SUPPORT_PRODUCT=\"Fedora\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=39\r\nSUPPORT_END=2024-11-12\r\nVARIANT=\"Server Edition\"\r\nVARIANT_ID=server\r\n$ uname -a\r\nLinux localhost.localdomain 6.7.4-200.fc39.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Feb  5 22:21:14 UTC 2024 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n$ kind version\r\nkind v0.22.0 go1.20.13 linux\/amd64\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\nn.a.\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\nn.a.\r\n<\/details>\r\n","comments":["\/sig api-machinery","\/assign @Jefftree \r\nFor SSA.\r\nThank you.\r\n\/triage accepte","@jiahuif: The label(s) `triage\/accepte` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123582#issuecomment-1997918367):\n\n>\/assign @Jefftree \r\n>For SSA.\r\n>Thank you.\r\n>\/triage accepte\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/triage accepted","I also encounter this issue in my current project. The current workaround I have is using [Clearing manageFields](https:\/\/kubernetes.io\/docs\/reference\/using-api\/server-side-apply\/#clearing-managedfields) before doing the apiVersion and schema upgrade."],"labels":["kind\/bug","sig\/api-machinery","triage\/accepted"]},{"title":"add imagePullSecrets in case of debugging node","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\nadd capability of `imagePullSecrets` in case of debug node\r\n\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nin case of enterprise, private registry is the first choice, if execute `kubectl debug node\/mynode -it --image=${private_registry}\/busybox` it will report image pull error, so we need add `imagePullSecrets` options. \r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubectl\/issues\/1567\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nCLI change: add `--image-pull-secrets` options for `debug` command in `kubectl`, for example `kubectl debug node\/mynode -it --image=busybox --image-pull-secrets=myRegistryKeySecretName1 --image-pull-secrets=myRegistryKeySecretName2`\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: warjiang \/ (f4120c2e6a12c9c70cff1990e209293799d5eb75)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @warjiang! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @warjiang. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123581#\" title=\"Author self-approved\">warjiang<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/kubectl","release-note","needs-rebase","size\/M","kind\/feature","sig\/cli","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage"]},{"title":"make kubeadm return error if api-server is not accessible","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nWhen api-server is not accessible, Kubeadm keep trying to publish new bootstrap token and stuck there.\r\n```bash\r\n# kubeadm token create --print-join-command --v=5\r\nI0229 05:44:03.289883 2080320 token.go:122] [token] validating mixed arguments\r\nI0229 05:44:03.289939 2080320 token.go:131] [token] getting Clientsets from kubeconfig file\r\nI0229 05:44:03.289964 2080320 cmdutil.go:81] Using kubeconfig file: \/root\/.kube\/config\r\nI0229 05:44:03.291276 2080320 token.go:246] [token] loading configurations\r\nI0229 05:44:03.291649 2080320 interface.go:432] Looking for default routes with IPv4 addresses\r\nI0229 05:44:03.291661 2080320 interface.go:437] Default route transits interface \"enp25s0f3\"\r\nI0229 05:44:03.292364 2080320 interface.go:209] Interface enp25s0f3 is up\r\nI0229 05:44:03.292420 2080320 interface.go:257] Interface \"enp25s0f3\" has 2 addresses :[10.0.0.236\/24 fe80::3eec:efff:fe5e:f835\/64].\r\nI0229 05:44:03.292447 2080320 interface.go:224] Checking addr  10.0.0.236\/24.\r\nI0229 05:44:03.292460 2080320 interface.go:231] IP found 10.0.0.236\r\nI0229 05:44:03.292471 2080320 interface.go:263] Found valid IPv4 address 10.0.0.236 for interface \"enp25s0f3\".\r\nI0229 05:44:03.292479 2080320 interface.go:443] Found active IP 10.0.0.236\r\nI0229 05:44:03.292503 2080320 kubelet.go:218] the value of KubeletConfiguration.cgroupDriver is empty; setting it to \"systemd\"\r\nI0229 05:44:03.301420 2080320 token.go:253] [token] creating token\r\ntimed out waiting for the condition\r\n```\r\n\r\nThis is because it has not well-defined error handling logic.\r\nThis PR is trying to add error handling logic if api-server is not accessible.\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\nNONE\r\n\r\n#### Does this PR introduce a user-facing change?\r\nNONE\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\nNONE","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123576#\" title=\"Author self-approved\">bg-chun<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [chendave](https:\/\/github.com\/chendave) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kubeadm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubeadm\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"chendave\"]} -->","\/ok-to-test","> hello, looks like the whole function needs a refactor UpdateOrCreateTokens\r\n> \r\n> here is some pseudo code of what we should do:\r\n> \r\n> ```\r\n> for _, token := range tokens {\r\n>   var lastError error\r\n>   err = wait.PollUntilContextTimeout(\r\n>   ....\r\n> \r\n>   \/\/ 1. make API call to get the secret\r\n>   \/\/ 2. if the secret exists and failIfExists is true exit the poll check using apierrors.IsAlreadyExists(), for any other error retry the poll.\r\n>   \/\/ 3. don't use  CreateOrUpdateSecret, the function already has poll inside, just call Create()...\r\n> \r\n>   ) \/\/ poll end\r\n> }\r\n> ```\r\n> \r\n> does that make sense to you?\r\n> \r\n> we are close to code freeze for 1.30, so might not be a good idea to merge such a change, but you can try updating it before the 6th of March.\r\n\r\n+2 sounds good.\r\nDo you have preference for timeout duration?","> +2 sounds good.\r\nDo you have preference for timeout duration?\r\n\r\nthere is already a PollUntilContextTimeout in there. it has the correct interval\/timeout.\r\n\r\n```\r\n\t\terr = wait.PollUntilContextTimeout(\r\n\t\t\tcontext.Background(),\r\n\t\t\tkubeadmconstants.KubernetesAPICallRetryInterval,\r\n\t\t\tkubeadmapi.GetActiveTimeouts().KubernetesAPICall.Duration,\r\n\t\t\ttrue, func(_ context.Context) (bool, error) {\r\n```"],"labels":["sig\/cluster-lifecycle","size\/XS","area\/kubeadm","cncf-cla: yes","do-not-merge\/release-note-label-needed","ok-to-test","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"[Flaking Test] [sig-node] PreStop should call prestop when killing a pod [Conformance] in capz-windows-master","body":"### Which jobs are flaking?\n\nI find a flake in https:\/\/testgrid.k8s.io\/sig-release-master-informing#capz-windows-master\r\n\n\n### Which tests are flaking?\n\n- Kubernetes e2e suite.[It] [sig-node] PreStop should call prestop when killing a pod [Conformance]\n\n### Since when has it been flaking?\n\nhttps:\/\/storage.googleapis.com\/k8s-triage\/index.html?test=PreStop%20should%20call%20prestop%20when%20killing%20a%20pod\r\nk8s-triage shows it flakes for a period.\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#capz-windows-master\n\n### Reason for failure (if possible)\n\n```\r\nI0228 10:23:33.490720 2493 pre_stop.go:153] Unexpected error: validating pre-stop.: \r\n    <wait.errInterrupted>: \r\n    timed out waiting for the condition\r\n    {\r\n        cause: <*errors.errorString | 0xc00061d0b0>{\r\n            s: \"timed out waiting for the condition\",\r\n        },\r\n    }\r\n[FAILED] validating pre-stop.: timed out waiting for the condition\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e\/node\/pre_stop.go:153 @ 02\/28\/24 10:23:33.491\r\n```\n\n### Anything else we need to know?\n\na preview discussion: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/115136\n\n### Relevant SIG(s)\n\n\/sig windows node\r\n\/cc @marosset @claudiubelu ","comments":["\/triage accepted\r\n\r\nThis is a bug in the interaction with calico https:\/\/github.com\/projectcalico\/calico\/issues\/7332"],"labels":["sig\/node","kind\/flake","sig\/windows","triage\/accepted"]},{"title":"feature: add name formats library to CEL","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdds library for validation of common name formats used in kubernetes ecosystem. Very useful for users whose resources template other k8s resources, and necessary for KEP-4153: Declarative Validation\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n\/assign @jpbetz\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/issues\/4153\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","@cici37 Do we have a checklist of all the things we need to add a library?","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123572#\" title=\"Author self-approved\">alexzielenski<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [jpbetz](https:\/\/github.com\/jpbetz). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/cel\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/cel\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","@alexzielenski: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-unit | aeec260d24db8109d714f053ab2ca2fc26b6b26a | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123572\/pull-kubernetes-unit\/1763359906234109952) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-verify | aeec260d24db8109d714f053ab2ca2fc26b6b26a | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123572\/pull-kubernetes-verify\/1763359908046049280) | true | `\/test pull-kubernetes-verify`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123572). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aalexzielenski). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/triage accepted"],"labels":["area\/apiserver","sig\/api-machinery","size\/XL","kind\/feature","cncf-cla: yes","do-not-merge\/release-note-label-needed","needs-priority","triage\/accepted"]},{"title":"Allow registration of Extension API Servers that are not Kubernetes Services","body":"I ran into this issue in the kube-aggregator project that accurately describes our ask but we saw it was closed with no comment.\r\n\r\nhttps:\/\/github.com\/kubernetes\/kube-aggregator\/issues\/24\r\n\r\n> My company is migrating to Kubernetes. We have a custom software networking stack (Discovery, Routing, Traffic Control, etc) that we do not intend to deprecate as part of the migration. This has generally not been a problem; the loosely coupled nature of the k8s design makes it possible for us to create and manage the API objects we care about (Deployments, Pods, Nodes, etc) while integrating with our custom networking stack.\r\n> \r\n> While working on deploying the k8s metrics server, I was surprised to learn that the metrics server (or any API server extension) MUST be deployed as a k8s Service. https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.21\/#apiservicespec-v1-apiregistration-k8s-io\r\n> \r\n> This seems like an unnecessary requirement. We can work around it by creating an ExternalName Service (we don't run kube-dns so it doesn't really do much) and then adding the necessary CNAME to our own internal DNS resolver.\r\n> \r\n> We would prefer to just configure a DNS name and the CN of the extension API server's certificate and call it a day.\r\n\r\n\r\nWe are in a similar boat. We, too, have a custom networking stack and a custom CA and we have resorted to non-ideal workarounds to get this to work. We would love to be able to pass an external address for the Extension API Server that also works with TLS (as today, the TLS validation in the assumes the server name ends in `.svc` https:\/\/github.com\/kubernetes\/kube-aggregator\/blob\/master\/pkg\/apiserver\/handler_proxy.go#L209)\r\n\r\nSince this is pretty important to us, we are willing to do the work to support this!","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","a better place to ask would be on the support channels. please see:\r\nhttps:\/\/git.k8s.io\/kubernetes\/SUPPORT.md\r\n\r\n\/kind support\r\n\/close\r\n\r\n","@dims: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123571#issuecomment-1970298039):\n\n>a better place to ask would be on the support channels. please see:\r\n>https:\/\/git.k8s.io\/kubernetes\/SUPPORT.md\r\n>\r\n>\/kind support\r\n>\/close\r\n>\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","> a better place to ask would be on the support channels. please see: https:\/\/git.k8s.io\/kubernetes\/SUPPORT.md\r\n> \r\n> \/kind support \/close\r\n\r\nThis isn't really a support request. This is a feature request as this functionality is not supported by the current implementation of the Extension API functionality. \r\n\r\nI'm happy to reopen this somewhere else and be pointed at to the right process for making a feature request.","@aliaqel-stripe we are running into this as well for our use case in grafana. have you posted this anywhere else?","> @aliaqel-stripe we are running into this as well for our use case in grafana. have you posted this anywhere else?\r\n\r\nI posted it in the kubernetes slack like @dims suggested in the support README, but I haven't gotten any responses:\r\nhttps:\/\/kubernetes.slack.com\/archives\/C09NXKJKA\/p1709680439971829","\/reopen\r\nBecause more work is being done on this issue.\r\n\/remove-kind support","@jiahuif: Reopened this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123571#issuecomment-1997926394):\n\n>\/reopen\r\n>Because more work is being done on this issue.\r\n>\/remove-kind support\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/kind bug","> This isn't really a support request. This is a feature request as this functionality is not supported by the current implementation of the Extension API functionality.\r\n\r\n\/sig api-machinery\r\n\/remove-kind bug\r\n\/kind feature\r\n","> I ran into this issue in the kube-aggregator project that accurately describes our ask but we saw it was closed with no comment.\r\n\r\n\/kind bug\r\n"],"labels":["kind\/bug","sig\/api-machinery","kind\/feature","needs-kind","needs-triage"]},{"title":"WIP - Bump gengo api\/rpc to isolated submodules","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nTesting with update of gengo submodules to pick up https:\/\/github.com\/googleapis\/go-genproto\/pull\/1087 as part of https:\/\/github.com\/kubernetes\/kubernetes\/issues\/113366\r\n\r\nWe still have one transitive dependency on genproto, which is `google.golang.org\/genproto\/protobuf\/field_mask`, via grpc-gateway v1, which is referenced by etcd. This is dropped in grpc-gateway v2 (https:\/\/github.com\/grpc-ecosystem\/grpc-gateway\/pull\/3317) and etcd 3.6 updates to grpc-gateway v2 (https:\/\/github.com\/etcd-io\/etcd\/pull\/16595).\r\n\r\nThis isn't intended to merge quite yet, just testing with the partial bumps we have so far\r\n\r\n```release-note\r\nNONE\r\n```","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123569#\" title=\"Author self-approved\">liggitt<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)~~ [liggitt]\n- ~~[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apimachinery\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/csi-translation-lib\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/csi-translation-lib\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kms\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kms\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)~~ [liggitt]\n- ~~[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)~~ [liggitt]\n- ~~[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)~~ [liggitt]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->","@liggitt: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify | c6be5ac0081ad9f3f853b3d8d89ad2d323274d08 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123569\/pull-kubernetes-verify\/1762945117804564480) | true | `\/test pull-kubernetes-verify`\npull-kubernetes-integration | c6be5ac0081ad9f3f853b3d8d89ad2d323274d08 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123569\/pull-kubernetes-integration\/1762945101862014976) | true | `\/test pull-kubernetes-integration`\npull-kubernetes-e2e-capz-windows-master | c6be5ac0081ad9f3f853b3d8d89ad2d323274d08 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123569\/pull-kubernetes-e2e-capz-windows-master\/1762945108572901376) | false | `\/test pull-kubernetes-e2e-capz-windows-master`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123569). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aliggitt). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/network","area\/kubelet","kind\/cleanup","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","needs-rebase","size\/XL","release-note-none","sig\/auth","approved","sig\/cli","cncf-cla: yes","sig\/instrumentation","sig\/architecture","do-not-merge\/work-in-progress","area\/code-generation","sig\/cloud-provider","needs-priority","area\/dependency","needs-triage"]},{"title":"[WIP] Graduate APIServerTracing to stable","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\/hold\r\n\r\nUpdate APIServerTracing to stable.\r\n\r\nGA graduation requirements: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-instrumentation\/647-apiserver-tracing#graduation-requirements\r\n\r\nThis is blocked by:\r\n\r\n* https:\/\/github.com\/kubernetes\/community\/pull\/7737\r\n* https:\/\/github.com\/kubernetes\/kubernetes\/issues\/103186\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nPart of https:\/\/github.com\/kubernetes\/enhancements\/issues\/647\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nGraduate APIServerTracing to stable\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nPlease use the following format for linking documentation:\r\n```docs\r\n- [KEP]: [<link>](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-instrumentation\/647-apiserver-tracing)\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig instrumentation","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123566#\" title=\"Author self-approved\">dashpole<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/apis\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->"],"labels":["area\/test","area\/apiserver","sig\/api-machinery","release-note","size\/S","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/instrumentation","sig\/testing","do-not-merge\/work-in-progress","do-not-merge\/hold","needs-priority","needs-triage"]},{"title":"wip add coverage tests for sidecar KEP","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123563#\" title=\"Author self-approved\">matthyx<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->"],"labels":["area\/test","sig\/node","size\/L","cncf-cla: yes","sig\/testing","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"kube-proxy: Inconsistent behaviors about disabling health check server and metrics server","body":"### What happened?\n\nWhile looking at https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123545, I tried to validate the current behaviors about \"healthz-bind-address\" and \"metrics-bind-address\", and found some inconsistencies between the docs and the code.\r\n\r\n1. According to the following usages, setting \"--healthz-bind-address\" and \"--metrics-bind-address\" to empty values should disable the two servers, however making such configurations doesn't really work. So it's either a doc issue or a code issue.\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/54f9807e1e84981b2053f4daf779f5ed19962144\/cmd\/kube-proxy\/app\/server.go#L163-L164\r\n\r\n2. Regardless of whether disabling the two servers should be supported or not, the validation code for the two addresses are inconsistent, it tolerates HealthzBindAddress to be empty but not MetricsBindAddress. kube-proxy wouldn't be up when MetricsBindAddress is empty.\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/54f9807e1e84981b2053f4daf779f5ed19962144\/pkg\/proxy\/apis\/config\/validation\/validation.go#L73-L76\r\n\r\n3. `HealthzServer` is only initialized when `HealthzBindAddress` is not empty. However, it's used by `NodeEligibleHandler` unconditionally, which would cause kube-proxy to panic when it's nil.\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/54f9807e1e84981b2053f4daf779f5ed19962144\/cmd\/kube-proxy\/app\/server.go#L644-L646\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/54f9807e1e84981b2053f4daf779f5ed19962144\/cmd\/kube-proxy\/app\/server.go#L954-L956\r\n\r\nIt seems that regardless of whether we want to support disabling the two servers, some cleanup would be needed. But I'm not sure what's the expected behavior here.\r\n\r\ncc @danwinship @aojea \r\n\r\n\/sig network\r\n\/area kube-proxy\n\n### What did you expect to happen?\n\nThe documented way to disable health check server and metrics server doesn't work. If it's a real use case, it should be fixed. Otherwise, the usage doc should be fixed.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSetting \"--healthz-bind-address\" and \"--metrics-bind-address\" to empty values, the two servers aren't disabled.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","`This parameter is ignored if a config file is specified by --config`..., What do you configure your kube-proxy?","> `This parameter is ignored if a config file is specified by --config`..., What do you configure your kube-proxy?\r\n\r\n@cyclinder I'm aware of `--config` stuff. The problem exists regardless of how the config is provided. None of the following configs can disabling the two servers, the first one can't even pass the validation. I opened the issue not to figure out how to really disable them, but to point out the contradictory code and doc and confirm which is the actual expectation before opening a PR to fix it: not supporting disabling them (as it seems they have been broken for quite some time), or supporting disabling them.\r\n\r\n```\r\ncontainers:\r\n- command:\r\n  - \/usr\/local\/bin\/kube-proxy\r\n  - --healthz-bind-address=\"\"\r\n  - --metrics-bind-address=\"\"\r\n  - --hostname-override=$(NODE_NAME)\r\n  - --conntrack-max-per-core=0\r\n```\r\n```\r\ncontainers:\r\n- command:\r\n  - \/usr\/local\/bin\/kube-proxy\r\n  - --healthz-bind-address=\r\n  - --metrics-bind-address=\r\n  - --hostname-override=$(NODE_NAME)\r\n  - --conntrack-max-per-core=0\r\n```\r\n```\r\ncontainers:\r\n- command:\r\n  - \/usr\/local\/bin\/kube-proxy\r\n  - --config=\/var\/lib\/kube-proxy\/config.conf\r\n  - --hostname-override=$(NODE_NAME)\r\n# \/var\/lib\/kube-proxy\/config.conf\r\nhealthzBindAddress: \"\"\r\nmetricsBindAddress: \"\"\r\n```","@tnqn thanks for the clarification. I think the main issue is kube-proxy can't distinguish well between the empty value and not set. How about setting it to a pointer?  https:\/\/github.com\/kubernetes\/kubernetes\/blob\/656cb1028ea5af837e69b5c9c614b008d747ab63\/pkg\/proxy\/apis\/config\/types.go#L194\r\n\r\n- If we don't set it, the value should be nil, we disable the health\/metrics server.\r\n- If we set it, the value may be empty, we give it a default value.\r\n\r\n\/cc @danwinship @aojea "],"labels":["kind\/bug","sig\/network","area\/kube-proxy","needs-triage"]},{"title":"[Swap][Tests][KEP2400] Add swap serial stress tests, improve NodeConformance tests and adapt NoSwap behavior","body":"#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/120798\r\n\r\n#### What this PR does \/ why we need it:\r\nThis PR does the following:\r\n1. Havily refactors and cleans swap node-e2e tests.\r\n2. Adapt swap NodeConformance test to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122745 which drops `UnlimitedSwap` and introduces `NoSwap` behavior.\r\n3. Adds serial tests for swap.\r\n\r\nTo expand on (3), two serial tests are now added which validate the following scenarios:\r\n1. `\"should be able over-commit the node memory\"` - a pod is deployed which stresses memory. The test expects that once the pod uses all of the node's memory some of its memory would be swapped away.\r\n2. `\"should be able to use more memory than memory limits\"` - a pod with memory limits stresses memory. Eventually it goes beyond its memory limits and instead of being OOM killed some of its memory is swapped away keeping it alive.\r\n\r\n#### Special notes for your reviewer:\r\nThe original version of this PR was here: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/120430\r\n\r\nDue to unfortunate situations I had to be out of work for a while, therefore my PR was closed and continued here: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122175\r\n\r\nFortunately I was able to come back to work and therefore the PR is now continued here.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-node\/2400-node-swap\/README.md\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123557#\" title=\"Author self-approved\">iholder101<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https:\/\/github.com\/dchen1107) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dchen1107\"]} -->","\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","PR is not ready to review yet.\r\n\r\n\/uncc @dchen1107 @andrewsykim ","\/cc\r\n\r\nwhen it is ready","\/kind feature\r\n\r\n","\r\n\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","We have crio ci failures in upstream.\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-node-cri-o#ci-crio-cgroupv1-node-e2e-features\r\nhttps:\/\/testgrid.k8s.io\/sig-node-cri-o#ci-crio-cgroupv1-node-e2e-conformance\r\nhttps:\/\/testgrid.k8s.io\/sig-node-cri-o#ci-crio-cgroupv2-node-e2e-conformance\r\n\r\nI think you are impacted by this.","\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial\r\n\/test pull-kubernetes-node-e2e-containerd","\/reopen","@iholder101: Reopened this PR.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123557#issuecomment-1991939024):\n\n>\/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial\r\n\/test pull-kubernetes-node-e2e-containerd","\/retest","@kannon92\r\n\r\nNodeConformance tests have passed on `e2e-containerd`.\r\nThe serial lanes failed for seemingly unrelated reasons, but swap tests have passed there as well [[1](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767575322049384448)][[2](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-ubuntu-serial\/1767575321990664192)].\r\n\r\nThe PR is ready to review :) PTAL","\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial\r\n\r\nLooks like these tests are timing out. Maybe your test is making them exceed the time limit allocated to them.","> \/test pull-kubernetes-node-swap-fedora-serial\r\n> \/test pull-kubernetes-node-swap-ubuntu-serial\r\n\r\n(re-launching tests as I've pushed a minor change to satisfy the linter)\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial\r\n\r\n> Looks like these tests are timing out. Maybe your test is making them exceed the time limit allocated to them.\r\n\r\nCloud be.\r\nI'll keep an eye on that. Thanks!","\/test pull-kubernetes-node-e2e-containerd pull-kubernetes-dependencies","Unrelated failure [[1](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767611214742949888)].\r\n\r\n\/test pull-kubernetes-node-swap-fedora-serial","Looking at the serial logs, it looks like the agnhost container is being OOMed for fedora: https:\/\/storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767625656390127616\/artifacts\/n1-standard-2-fedora-coreos-39-20240210-3-0-gcp-x86-64-436ed8ef\/serial-1.log\r\n\r\nI have been trying to chase down a really nasty bug with serial jobs that you seem to be hitting with ubuntu. \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/issues\/123589\r\n\r\nLooking at those serial logs, I am seeing that we are hitting a OOM also for dd.\r\n\r\n","> Looking at the serial logs, it looks like the agnhost container is being OOMed for fedora: [storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767625656390127616\/artifacts\/n1-standard-2-fedora-coreos-39-20240210-3-0-gcp-x86-64-436ed8ef\/serial-1.log](https:\/\/storage.googleapis.com\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767625656390127616\/artifacts\/n1-standard-2-fedora-coreos-39-20240210-3-0-gcp-x86-64-436ed8ef\/serial-1.log)\r\n> \r\n> I have been trying to chase down a really nasty bug with serial jobs that you seem to be hitting with ubuntu.\r\n> \r\n> #123589\r\n> \r\n> Looking at those serial logs, I am seeing that we are hitting a OOM also for dd.\r\n\r\nSo I've been tweaking things a bit, and all of the tests passed on a GCE instance (identical to the one we use on CI) for 20 times in a row. Let's see if CI is on our side.\r\n\r\n\/test pull-kubernetes-node-swap-fedora\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","Swap tests passed on both lanes, although they have failed. [[1](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1767858931914248192)][[2](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-ubuntu-serial\/1767858931855527936)].\r\n\r\n@kannon92 do you mind help me figure out the flakiness of these lanes before we add the tests presented in this PR? Since the tests passed for 20 times in a row on an identical GCE instance, I wonder whether the lane is flaky by nature or if the newly introduced tests actually hurt its stability.\r\n\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-ec2-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-cloud-provider-loadbalancer`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-containerd-flaky`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-conformance-fedora-serial`\n* `\/test pull-kubernetes-node-swap-conformance-ubuntu-serial`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123557#issuecomment-1998453198):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-swap-conformance-fedora-serial\r\n\/test pull-kubernetes-node-swap-conformance-ubuntu-serial\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","\/test pull-kubernetes-node-swap-conformance-fedora-serial\r\n\/test pull-kubernetes-node-swap-conformance-ubuntu-serial\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","@kannon92 Looking good! :tada: \r\n\r\n\/test pull-kubernetes-node-swap-fedora-serial","The fact that `swap-fedora-serial` is failing even without running swap conformance tests shows it is probably flaky and that this PR is not the cause.\r\n\r\nTrying one last time.\r\n\r\n\/test pull-kubernetes-node-swap-fedora-serial","\/test pull-kubernetes-node-swap-conformance-fedora-serial\r\n\/test pull-kubernetes-node-swap-conformance-ubuntu-serial\r\n\/test pull-kubernetes-node-swap-fedora-serial\r\n\/test pull-kubernetes-node-swap-ubuntu-serial","\/retest","@iholder101: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-swap-fedora-serial | 635f936fcc3ffe902ef228defef4cfec367b2fbc | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123557\/pull-kubernetes-node-swap-fedora-serial\/1769991888988803072) | false | `\/test pull-kubernetes-node-swap-fedora-serial`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123557). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aiholder101). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","sig\/node","size\/L","kind\/feature","release-note-none","cncf-cla: yes","sig\/testing","needs-priority","area\/e2e-test-framework","needs-triage"]},{"title":"Scheduler: Avoid scheduling pods to nodes where the allocatable resource is insufficient for the pod limit resource","body":"### What would you like to be added?\r\n\r\nShould we consider pod `limit resource` during scheduling to prevent pod scheduled to node where the allocatable resource is insufficient for the pod limit resource? \r\n\r\nThere are two impl ways:\r\n1. soft solution: Maintain current behavior but prefer nodes with sufficient allocatable resource for the pod limit resource.\r\n2. hard solution: Reject node with insufficient allocatable resource for the pod limit resource.\r\n\r\nIs in-tree plugin better than out-of-tree plugin to implement?\r\n\r\n# Current Behavior Example\r\ne.g. \r\npod \r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: init-demo\r\nspec:\r\n  restartPolicy: Never\r\n  nodeSelector:\r\n    kubernetes.io\/hostname: xxx\r\n  containers:\r\n    - name: nginx\r\n      image: nginx\r\n      ports:\r\n        - containerPort: 80\r\n      volumeMounts:\r\n        - name: workdir\r\n          mountPath: \/usr\/share\/nginx\/html\r\n      resources:\r\n        limits:\r\n          memory: '100Gi'\r\n          cpu: 100\r\n        requests:\r\n          memory: '1Gi'\r\n          cpu: 1\r\n  dnsPolicy: Default\r\n  volumes:\r\n    - name: workdir\r\n      emptyDir: {}\r\n\r\n```\r\n\r\nnode\r\n<img width=\"835\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/18323315\/c2f17a75-ed71-4a6b-bb3e-d24e3fd9dcd1\">\r\n\r\ncurrently, pod will be scheduled to this node successfully. We expect it can't be scheduled to this node for hard solution, or this node is not the first choice for soft solution.\r\n\r\n\/sig scheduling\r\n\/triage accepted\r\n\/assign @Huang-Wei \r\n\r\n### Why is this needed?\r\n\r\nthe limit resource on pod will be meaningless, if the value of pod limit resource larger than node allocatable resource. Since the real limit resource of this pod will be the allocatable resource on this node. \r\n\r\nFor example, nodeA with allocatable cpu 4c, podA with request cpu 2c, limit cpu 6c, and podA located on nodeA. Then the real limit cpu of this pod will be 4c, instead of 6c.","comments":["\/triage accepted","\/triage accepted","If you want the pod scheduled to the node which has more resources, I think noderesource plugin with`least_allocated` can do this.\r\nIf you want the pod scheduled to a node which has at lease 100 cpu, why don't you make request equal to 100 ?","Thanks for reply. Let me give a more appropriate example. \r\n\r\nThere are:\r\n- `nodeA` with allocatable 4c8Gi (and available 4c8Gi, while no pods locate on it), \r\n- `nodeB` with allocatable 8c16Gi (and available 6c12Gi, others 2c4Gi is requested). \r\n- `podA` with request 3c6Gi, limit 6c10Gi.\r\n\r\nI expected podA will `not` locate on nodeA, since podsA will `never get a chance` to use 6c10Gi on nodeA due to maximum allocatable resource on nodeA is 4c8Gi. \r\n\r\nBut above propose (hard way) may change current behavior, since `nodeA` will not pass the filter stage. So, another soft way is letting `nodeA` passes filters, but prefer `nodeB` in score stage. \r\n\r\nBy the way, Using noderesource plugin with lease_allocated will prefer `nodeA` in above case. Generally, I do not care the allocated resource  on node, I hope the allocatable resource on node could afford podA limit resource, so the podA could get a chance to use it's limit resource.\r\n\r\nWhy not set podsA with request resource to 6c10Gi(equal to limit)?\r\n-------\r\nBecause most of the time, the resource usage of podA does not exceed 3c6Gi, setting to 6c10Gi  would waste resources. But in some times, podsA need `burst` to maximum 6c10Gi for a relatively short period of time"],"labels":["sig\/scheduling","kind\/feature","triage\/accepted"]},{"title":"DRA: kubelet: checkpoint checksum changes when API gets extended","body":"### What happened?\n\nWhen adding the `StructuredData` field to `resourcev1alpha2.ResourceHandle`, old checkpoint files became invalid because the checksum that gets calculated by hashing the `spew` output in `pkg\/util\/hash` no longer matches. This caused kubelet to fail to start with:\r\n```\r\nE0228 10:46:47.293483  363727 run.go:74] \"command failed\" err=\"failed to run Kubelet: failed to create claimInfo cache: error calling GetOrCreate() on checkpoint state: failed to get checkpoint dra_manager_state: checkpoint is corrupted\"\r\n```\r\n\r\nHere's what the checksum in a unit test gets calculcated for:\r\n```\r\n\"(*state.DRAManagerCheckpoint){Version:(string)v1 Entries:(state.ClaimInfoStateList)[{DriverName:(string)[test-driver.cdi.k8s.io](http:\/\/test-driver.cdi.k8s.io\/) ClassName:(string)class-name ClaimUID:(types.UID)067798be-454e-4be4-9047-1aa06aea63f7 ClaimName:(string)example Namespace:(string)default PodUIDs:(sets.Set[string])map[139cdb46-f989-4f17-9561-ca10cfb509a6:{}] ResourceHandles:([]v1alpha2.ResourceHandle)[{DriverName:(string)[test-driver.cdi.k8s.io](http:\/\/test-driver.cdi.k8s.io\/) Data:(string){\\\"a\\\": \\\"b\\\"} StructuredData:(*v1alpha2.StructuredResourceHandle)<nil>}] CDIDevices:(map[string][]string)map[[test-driver.cdi.k8s.io](http:\/\/test-driver.cdi.k8s.io\/):[[example.com\/example=cdi-example]]}]](http:\/\/example.com\/example=cdi-example]]%7D]) Checksum:(checksum.Checksum)0}\"\r\n```\r\n\r\nNote the new `StructuredData:(*v1alpha2.StructuredResourceHandle)<nil>`.\r\n\r\n\n\n### What did you expect to happen?\n\nNot sure what the right behavior is. Perhaps kubelet should have ignored the invalid checkpoint?\r\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRevert the pkg\/kubelet\/cm\/dra\/state\/state_checkpoint_test.go update in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123516.\r\n\n\n### Anything else we need to know?\n\nThe state.DRAManagerCheckpoint contains a version field, which is set to v1.I think that's where things fall apart: that version must exactly match the type that gets serialized, which is not the case here. The type changes over time, but the version is fixed.\r\n\r\nLet's reconsider what should get checkpointed and how we can make that stable. Also, is checkpointing really worth it? What bad things happen if we remove it?\n\n### Kubernetes version\n\nmaster\r\n\n\n### Cloud provider\n\nn\/a\r\n\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n\/sig node\r\n\/cc @bart0sh @klueska \r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @moshe010 ","We originally didn't have checkpointing and it was added by @moshe010 in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/115912 to fix https:\/\/github.com\/kubernetes\/kubernetes\/issues\/115902","The reasons for checkpoint were: \r\n1.  https:\/\/github.com\/kubernetes\/kubernetes\/pull\/115912 (what Kevin mentioned)\r\n2. part of the pod resource API DAR extensions concerns raised on what happen if kubelet restart and we get GetPodResourcesRequest. The request should return the DRA claimed devices but if we didn't checkpoint the DRA cache is empty and we would return empty resources\r\n3. pod force delete see [2]\r\n\r\nMy original PR didn't checkpoint the ResourceHandle is was added in PR [2]. Maybe  we should exclude ResourceHandle  from the checksum hash calculation \r\n\r\n\r\n\r\n[1] https:\/\/github.com\/kubernetes\/enhancements\/pull\/3738#discussion_r1099076427 \r\n[2] -https:\/\/github.com\/kubernetes\/kubernetes\/pull\/119307 ","So the cache makes sense. What doesn't make sense to me is how the consistency of that on-disk file is checked and how it is versioned.\r\n\r\nShouldn't the checksum be calculated for the bytes on disk (version-independent) instead of the in-memory representation (version-dependent)? Or is the goal to protect against \"I can't read this data anymore\" instead of \"bit has flipped on disk\"?\r\n\r\nThe assumption is that the content can always be decoded from what was written in the past into the type which is used now. This works with JSON if the newer type is a true superset of the old one, but that's not necessarily guaranteed. The proper way of doing this is to define a scheme which includes conversion between different versions.\r\n\r\nThis cannot be done when embedding API types from resource.k8s.io because conversion between current v1alpha2 and a future v1beta1 can only be done in the apiserver, not client-side.\r\n\r\nI don't see a quick solution here.\r\n\r\nIn the short-term, how do we deal with version upgrades? Require that users drain the node (thus clearing the cache) before upgrading or downgrading kubelet?","I am still trying to understand the problem. \r\n1. It the problem that we checksum the version? \r\n2. Is the problem that we don't have conversion mechanism to move from version X -> version Y on upgrade? \r\n3. Is the issue related only for upgrade or there are other cases which checksum changes? \r\n\r\nIn general we added the version to checkpoint file so that in the future when the DRA checkpoint struct changed we can have mechanism to convert old checkpoint version to new checkpoint version (in the initial PR we didn't implement conversion mechanism as I thought it will be added on DRA checkpoint struct change) . If you take a look on the CPU Manager checkpoint  [1] you will see that it detecting if the scheme matches to V1 first and if not it assume V2. We wanted that it will part of the checkpoint file so we won't need such detection and in the file we will know that correct version of the DRA checkpoint struct. \r\n\r\nNow if someone change the DRA checkpoint scheme and didn't implement migration flow (similar to CPU manager [2]) or change the version that indeed an issue, \r\n\r\n[1] - https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/cpumanager\/state\/state_checkpoint.go#L98 \r\n[2] - https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/cpumanager\/state\/state_checkpoint.go#L108 ","The problem is that after an upgrade of kubelet from <1.30 to 1.30, it won't be able to read an existing, non-empty checkpoint file. Worse, it will refuse to start because of that.\r\n\r\nThe root cause is the lack of proper versioning and lack of conversion code.\r\n\r\n> Is the issue related only for upgrade or there are other cases which checksum changes?\r\n\r\nAny version change potentially has this problem (including downgrades). I'm not aware of other failure scenarios.","Since it is not a GA feature, we cannot fix this in 1.30. But marking it as important long-term.\r\n\r\n\/priority important-longterm"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","needs-triage"]},{"title":"Patching metadata.labels field of volumeclaimtemplate in a statefulset is forbidden","body":"### What happened?\n\nWhen I try to patch STS's volumeClaimTemplate metadata.labels, It fails to patch it with error message \"Forbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template' ....\"\n\n### What did you expect to happen?\n\nIt should allow to patch the fields in volumeclaimtemplate which are allowed to be patched in PVC\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create a statefulset with volumeclaimtemplate in it. It will create statefulset pods and PVC using volumeclaimtemplate\r\n2. Try to edit labels inside volumeclaimtemplate of statefulset\r\n3. It fails with above mentioned error\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.28.0\r\nKustomize version: v5.0.4-0.20230601165947-6ce0bf390ce3 \r\nServer Version: v1.28.3-gke.1286000\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nGCP\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\nContainerd 1.7.0\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","I think this is a feature request rather than a bug. Not sure.","> When I try to patch STS's volumeClaimTemplate metadata.labels, It fails to patch it with error message \"Forbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template' ....\"\r\n\r\n\/sig apps\r\n","I think its currently a bug as it doesnot allow to mutate the values which it actually should allow (pvc labels) and can be fixed by a new feature"],"labels":["kind\/bug","sig\/apps","needs-triage"]},{"title":"A new controller adds\/removes finalizer to VAC for protection","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/119298\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nAdded a new controller, volumeattributesclass-protection-controller, into the kube-controller-manager.\r\nThe new controller manages a protective finalizer on VolumeAttributesClass objects.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/issues\/3751\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","Changelog suggestion (early feedback)\r\n```diff\r\n-kube-controller-manager adds a new controller named volumeattributesclass-protection-controller which adds\/removes finalizer to VAC for protection.\r\n+Added a new controller, volumeattributesclass-protection-controller, into the kube-controller-manager.\r\n+The new controller manages a protective finalizer on VolumeAttributesClass objects.\r\n```","\/cc @sunnylovestiramisu @msau42 ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123549#\" title=\"Author self-approved\">carlory<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [xing-yang](https:\/\/github.com\/xing-yang) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[pkg\/controller\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/volume\/OWNERS)**\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n- **[plugin\/pkg\/auth\/authorizer\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/plugin\/pkg\/auth\/authorizer\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"xing-yang\"]} -->","the `pull-kubernetes-e2e-gce-storage-slow` failures would be fixed by https:\/\/github.com\/kubernetes\/test-infra\/pull\/32160","\/test pull-kubernetes-e2e-gce-storage-slow","The design is using a cache instead of each time listing all the PVCs to see VAC usage. This PR is using Lister only?","@sunnylovestiramisu \r\n\r\nIt uses informer cache and custom index instead of maintaining another cache. It follows the pvc-protection-controller style.\r\n\r\nIn the controller manager, only pv controller has to need another cache instead of informer caches. Why does it need those caches, please see [this doc](https:\/\/courageous-begonia-1b47e6.netlify.app\/docs\/components\/binder-controller\/#local-cache).\r\n\r\nMaintaining an another cache is unnecessary for the vac-protection-controller. And informer cache is enough for the vac-protection-controller. It is simpler and easier to implement, while also saving more memory.","> This PR is using Lister only?\r\n\r\nthe pv\/pvc listers should be removed because it's not used. the getPVsAssignedToVAC and getPVCsAssignedToVAC funcs use indexer to list pvc\/pv.","> The design is using a cache instead of each time listing all the PVCs to see VAC usage.\r\n\r\nListing objects from informer cache won't call the kube-apiserver.\r\n","\/cc @jsafrane ","\/cc\r\n\r\nreview the RBAC changes\r\n\r\n\/triage accepted","@carlory we are trying to avoid the pvc protection controller style because of this issue:  https:\/\/github.com\/kubernetes\/kubernetes\/issues\/109282#issuecomment-2000019682 \r\n\r\nOur customers have ran into this a few times and we are looking into changing the existing pvc controller protection logic.","This PR only finds PVs\/PVCs objects from informers (local cache), and never asks api server. it shouldn't have a performance issue. but the current implementation has the race condition, as @jsafrane said in https:\/\/github.com\/kubernetes\/enhancements\/pull\/4529#discussion_r1521198356.\r\n\r\nIf we want to solve the race, listing objects from kube-apiserver is the only way. It will be a performance issue and worse than pvc protection controller.\r\n\r\nThere is no good solution for this performance issue. cc @sunnylovestiramisu \r\n","@carlory discussed offline with @msau42, we do not want to list from api server for accuracy for now, since we stated in the https:\/\/github.com\/kubernetes\/enhancements\/pull\/4529\/files that this is a best effort operation.\r\n\r\nThis is not a one way door either, if after the feature implemented, there are customers asking for accuracy, we can revisit the decision here."],"labels":["sig\/storage","sig\/api-machinery","release-note","size\/XXL","kind\/feature","sig\/auth","sig\/apps","cncf-cla: yes","do-not-merge\/work-in-progress","needs-priority","triage\/accepted"]},{"title":"Delete a stale code comment","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\ndelete the comment \"\/ \/ TODO: move to reconstruct.go and remove old code there.\"\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\n\r\n#### Special notes for your reviewer:\r\ncleanup after https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123442 is merged.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNONE\r\n```\r\n","comments":["Thank you @Songjoy \r\n\/lgtm\r\n\/sig storage\r\n\/assign @jsafrane ","LGTM label has been added.  <details>Git tree hash: 0cbe2dbff5a784e0f97a7818fdb4393303a62c64<\/details>","@Songjoy: The `\/retest` command does not accept any targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-csi-serial`\n* `pull-kubernetes-e2e-gce-storage-slow`\n* `pull-kubernetes-e2e-gce-storage-snapshot`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-e2e-storage-kind-disruptive`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123548#issuecomment-1968255441):\n\n>\/retest to rerun all failed tests\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-e2e-gce-storage-slow","\/test pull-kubernetes-e2e-gce-csi-serial","\/test pull-kubernetes-e2e-gce-storage-snapshot","\/test pull-kubernetes-e2e-gce","\/retitle Delete a stale code comment","\/triage accepted\r\n\/priority backlog\r\n\/retest","\/lgtm","the `pull-kubernetes-e2e-gce-storage-slow` failures would be fixed by https:\/\/github.com\/kubernetes\/test-infra\/pull\/32160","I've lgtmed it, can you give it an approved label? Thank you @jsafrane","\/test pull-kubernetes-e2e-gce-storage-slow","\/approve","[APPROVALNOTIFIER] This PR is **APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123548#issuecomment-1985841713\" title=\"Approved\">jsafrane<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123548#\" title=\"Author self-approved\">Songjoy<\/a>*\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\nThe pull request process is described [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process)\n\n<details >\nNeeds approval from an approver in each of these files:\n\n- ~~[pkg\/kubelet\/volumemanager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/volumemanager\/OWNERS)~~ [jsafrane]\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[]} -->"],"labels":["priority\/backlog","area\/kubelet","kind\/cleanup","lgtm","sig\/storage","sig\/node","size\/XS","release-note-none","approved","cncf-cla: yes","triage\/accepted"]},{"title":"update prometheus\/client_golang to v1.19.0","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\nupdate prometheus\/client_golang to v1.19.0\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes # None\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nupdate prometheus\/client_golang to v1.19.0\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\nNone\r\n```\r\n","comments":["Hi @shijinye. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123547#\" title=\"Author self-approved\">shijinye<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k), [dims](https:\/\/github.com\/dims), [sttts](https:\/\/github.com\/sttts) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/OWNERS)**\n- **[LICENSES\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/LICENSES\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/cli-runtime\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cli-runtime\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n- **[staging\/src\/k8s.io\/cloud-provider\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cloud-provider\/OWNERS)**\n- **[staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cluster-bootstrap\/OWNERS)**\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-base\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-base\/OWNERS)**\n- **[staging\/src\/k8s.io\/component-helpers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/component-helpers\/OWNERS)**\n- **[staging\/src\/k8s.io\/controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/cri-api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/cri-api\/OWNERS)**\n- **[staging\/src\/k8s.io\/csi-translation-lib\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/csi-translation-lib\/OWNERS)**\n- **[staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/dynamic-resource-allocation\/OWNERS)**\n- **[staging\/src\/k8s.io\/endpointslice\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/endpointslice\/OWNERS)**\n- **[staging\/src\/k8s.io\/kms\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kms\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-controller-manager\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-proxy\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-scheduler\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-scheduler\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)**\n- **[staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/legacy-cloud-providers\/OWNERS)**\n- **[staging\/src\/k8s.io\/metrics\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/metrics\/OWNERS)**\n- **[staging\/src\/k8s.io\/pod-security-admission\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/pod-security-admission\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-cli-plugin\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-controller\/OWNERS)**\n- **[vendor\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/vendor\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\",\"dims\",\"sttts\"]} -->","\/ok-to-test\r\n\r\ncan you please add some context on why we are updating?","@shijinye: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-node-e2e-containerd | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-node-e2e-containerd\/1762814426987106304) | true | `\/test pull-kubernetes-node-e2e-containerd`\npull-kubernetes-node-e2e-containerd-1-7-dra | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-node-e2e-containerd-1-7-dra\/1762814429503688704) | false | `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\npull-kubernetes-node-e2e-crio-dra | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-node-e2e-crio-dra\/1762814428669022208) | false | `\/test pull-kubernetes-node-e2e-crio-dra`\npull-kubernetes-integration | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-integration\/1762814424030121984) | true | `\/test pull-kubernetes-integration`\npull-kubernetes-e2e-kind | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-e2e-kind\/1762814424126590976) | true | `\/test pull-kubernetes-e2e-kind`\npull-kubernetes-conformance-kind-ga-only-parallel | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-conformance-kind-ga-only-parallel\/1762814424478912512) | true | `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\npull-kubernetes-e2e-kind-ipv6 | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-e2e-kind-ipv6\/1762814424181116928) | true | `\/test pull-kubernetes-e2e-kind-ipv6`\npull-kubernetes-typecheck | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-typecheck\/1762814430338355200) | true | `\/test pull-kubernetes-typecheck`\npull-kubernetes-e2e-gce-providerless | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-e2e-gce-providerless\/1762814423916875776) | false | `\/test pull-kubernetes-e2e-gce-providerless`\npull-kubernetes-e2e-gce | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-e2e-gce\/1762814423849766912) | true | `\/test pull-kubernetes-e2e-gce`\npull-kubernetes-kind-dra | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-kind-dra\/1762814427825967104) | false | `\/test pull-kubernetes-kind-dra`\npull-kubernetes-e2e-kind-kms | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-e2e-kind-kms\/1762814426148245504) | false | `\/test pull-kubernetes-e2e-kind-kms`\npull-kubernetes-linter-hints | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-linter-hints\/1762814432854937600) | false | `\/test pull-kubernetes-linter-hints`\npull-kubernetes-unit | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-unit\/1762814425313579008) | true | `\/test pull-kubernetes-unit`\npull-kubernetes-verify-lint | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-verify-lint\/1762814432024465408) | true | `\/test pull-kubernetes-verify-lint`\npull-kubernetes-verify | c942b674f5f26e02de9af2433b933aeed01b2dc5 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123547\/pull-kubernetes-verify\/1762814431185604608) | true | `\/test pull-kubernetes-verify`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123547). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Ashijinye). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","I've got a project which imports github.com\/prometheus\/common v0.48.0 and k8s.io\/component-base v0.29.2 directly. This is causing build failures due to \r\n`could not import k8s.io\/component-base\/metrics\/testutil (-: # k8s.io\/component-base\/metrics\/testutil\r\n\/home\/XXX\/go\/pkg\/mod\/k8s.io\/component-base@v0.29.2\/metrics\/testutil\/metrics.go:73:59: undefined: expfmt.FmtText) (typecheck)\r\n\t\"k8s.io\/component-base\/metrics\/testutil\"\r\n`\r\nSo this update is required to fix this","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Any updates on this?","@shijinye do you think you could rebase your PR?","\/assign @dims\r\nFor dependency reviews\r\n\/triage accepted"],"labels":["sig\/network","area\/kubelet","kind\/cleanup","area\/kube-proxy","area\/apiserver","area\/kubectl","area\/cloudprovider","sig\/storage","sig\/node","sig\/api-machinery","sig\/cluster-lifecycle","release-note","needs-rebase","size\/XL","sig\/auth","sig\/cli","cncf-cla: yes","sig\/instrumentation","sig\/architecture","area\/code-generation","sig\/cloud-provider","ok-to-test","needs-priority","area\/dependency","triage\/accepted"]},{"title":"KEP-4222: Add unit tests for decoding into interface{} type","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind cleanup\r\n\/sig api-machinery\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/kep.k8s.io\/4222\r\n```\r\n","comments":["Welcome @ssuriyan7! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @ssuriyan7. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123546#\" title=\"Author self-approved\">ssuriyan7<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","\/cc\r\n\/ok-to-test","\/lgtm\r\n\r\nThanks! I added the known failures to the tracker issue so that they can be resolved after (hopefully) a v2.7.0 bump.","LGTM label has been added.  <details>Git tree hash: 9cc5452ebf53c0e3f94640975f515029767aa4ab<\/details>","\/assign @deads2k ","\/retest-required","\/retest-required","\/retest\r\nhttps:\/\/github.com\/kubernetes\/test-infra\/issues\/32157","\/triage accepted"],"labels":["kind\/cleanup","lgtm","sig\/api-machinery","size\/L","release-note-none","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"fix:When the --bind-address parameter of kube-proxy is configured as ipv6, the ip address of metrics listens to 127.0.0.1 by default, instead of::1","body":"\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123544\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nWhen the `--bind-address` parameter of kube-proxy is configured as ipv6, the ip address of metrics listens to ::1 by default,\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123545#\" title=\"Author self-approved\">yangjunmyfm192085<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [thockin](https:\/\/github.com\/thockin) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[cmd\/kube-proxy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-proxy\/OWNERS)**\n- **[pkg\/proxy\/apis\/config\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/proxy\/apis\/config\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"thockin\"]} -->","\/test pull-kubernetes-verify","The issue that triggered this PR is a confusion around docs and field usage. The field (probably archaic) is not well documented. ","You MUST NOT mess with the `--bind-address` option! It should basically be obsoleted and removed. Perhaps replaced with a proxy-mode=iptables(nftables?) unique option, since it has _no function_ for proxy-mode=ipvs. You should not introduce some semantics to it!\r\n\r\nPlease see https:\/\/github.com\/kubernetes\/kubernetes\/pull\/119525.\r\n\r\n\/cc @danwinship ","\/cc\r\n"],"labels":["kind\/bug","sig\/network","area\/kube-proxy","release-note","size\/M","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"Docs: When the `--bind-address` parameter of kube-proxy is configured as ipv6, the ip address of metrics listens to 127.0.0.1 by default, instead of::1","body":"### What happened?\n\nThe `--bind-address` parameter of kube-proxy is configured as ipv6\r\n```\r\n--bind-address=aaaa:bbbb::2e8\r\n\r\n```\r\nThe metrics port of kube-proxy listens to the ipv4 address\r\n\u200b```\r\nnetstat -nlap|grep 10249\r\ntcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      5412\/kube-proxy     \r\n```\n\n### What did you expect to happen?\n\nWhen the `--bind-address` parameter of kube-proxy is configured as ipv6, the ip address of metrics listens to ::1 by default\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThe `--bind-address` parameter of kube-proxy is configured as ipv6\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\nClient Version: v1.28.3\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.3\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig network\r\n","\/assign","Thank you for this. \r\n\r\nPlease use `--metrics-bind-address` flag to change metric listening address, docs are here: https:\/\/kubernetes.io\/docs\/reference\/command-line-tools-reference\/kube-proxy\/\r\n\r\nas for the `--bind-address`, the docs are a bit confusing wrt to this field.\r\n\r\n\/kind support","--metrics-bind-address          ipport\u00a0\u00a0\u00a0\u00a0\u00a0Default: 127.0.0.1:10249\r\n\u00a0 | The IP address and port for the metrics server to serve on, defaulting to \"127.0.0.1:10249\" (if --bind-address is unset or IPv4), or \"[::1]:10249\" (if --bind-address is IPv6).\r\n\r\nSet metrics-bind-address to ::1, the effect is as follows\uff1a\r\ntcp6       0      0 ::1:10249               :::*                    LISTEN      30567\/kube-proxy \r\n\r\nSo the document can also be updated. --bind-address may  not be useful.","\/triage accepted"],"labels":["kind\/bug","sig\/network","triage\/accepted"]},{"title":"PROPOSAL - extend 'scale' subresource API to support `pod-deletion-cost`","body":"# Background\r\n\r\nThe idea of letting users customize the way Deployments (ReplicaSets) remove Pods when `replicas` are decreased has been floating around [since at least 2017](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/45509), with [other issues dating back to 2015](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/4301). \r\n\r\nSince [Kubernetes 1.22](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/CHANGELOG\/CHANGELOG-1.22.md#api-change-8), the [`controller.kubernetes.io\/pod-deletion-cost`](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/replicaset\/#pod-deletion-cost) annotation proposed in [KEP-2255](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/2255-pod-cost) is available in BETA.\r\n\r\nThere have been several other proposals, but this one should supersede them:\r\n\r\n- https:\/\/github.com\/kubernetes\/kubernetes\/issues\/107598\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/3189\r\n\r\n# Problem\r\n\r\n## Problem 1: It's too hard to get\/update pod-deletion cost\r\n\r\nIt is currently too hard to get\/update the `controller.kubernetes.io\/pod-deletion-cost` annotation for all Pods in a Deployment\/ReplicaSet. This makes it difficult to use `pod-deletion-cost` in practice.\r\n\r\nThe main issue is that the `pod-deletion-cost` annotation must be updated BEFORE the `replicas` count is decreased, this means that any system that wants to use `pod-deletion-cost` must:\r\n\r\n- Track which Pods are currently in the Deployment (possibly under multiple ReplicaSets)\r\n- Clean up any `pod-deletion-cost` annotations that were not used.\r\n- When scaling down, update the `pod-deletion-cost` of the Pods that will be deleted, and THEN update the `replicas` count.\r\n\r\nThis difficulty often prompts people to use the `pod-deletion-cost` annotation in a way that is NOT recommended, such as making a controller to update the `pod-deletion-cost` annotation even when no scale-down is happening (which is a [stated anti-pattern](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/replicaset\/#pod-deletion-cost)).\r\n\r\n## Problem 2: HorizontalPodAutoscaler cant use pod-deletion-cost\r\n\r\nThere is no sensible way to extend the [HorizontalPodAutoscaler](https:\/\/kubernetes.io\/docs\/tasks\/run-application\/horizontal-pod-autoscale\/) resource to be able to make use of `pod-deletion-cost` when scaling Deployments. This is because introducing complicated Pod-specific logic to update `pod-deletion-cost` annotations is inevitably going to be brittle.\r\n\r\n\r\n# Proposal\r\n\r\n## Overview\r\n\r\nThe general idea is to make it easier to read\/write the [`controller.kubernetes.io\/pod-deletion-cost`](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/replicaset\/#pod-deletion-cost) annotation for all Pods in the Deployment\/ReplicaSet. To achieve this, we can extend the existing [`Scale` v1](https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.29\/#scalestatus-v1-autoscaling) subresource to be able to read\/write the `controller.kubernetes.io\/pod-deletion-cost` annotations of Pods in the Deployment\/ReplicaSet.\r\n\r\n## Current State\r\n\r\nWe already have a special [`Scale` v1](https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.29\/#scalestatus-v1-autoscaling) subresource, which can be used by autoscalers to do things like:\r\n\r\n- GET: the current replicas of a Deployment\r\n- GET: the current label selector of a Deployment\r\n- PATCH: the current replicas of a Deployment\r\n\r\n__Example 1:__ [GET: `\/apis\/apps\/v1\/namespaces\/{namespace}\/deployments\/{name}\/scale`](https:\/\/dev-k8sref-io.web.app\/docs\/workloads\/scale-v1\/):\r\n\r\n```shell\r\n> curl -s localhost:8001\/apis\/apps\/v1\/namespaces\/my-namespace\/deployments\/my-deployment\/scale | yq .\r\nkind: Scale\r\napiVersion: autoscaling\/v1\r\nmetadata:\r\n  name: my-deployment\r\n  namespace: my-namespace\r\nspec:\r\n    replicas: 2\r\nstatus:\r\n    replicas: 2\r\n    selector: my-label=my-label-value\r\n```\r\n\r\n_NOTE: the HorizontalPodAutoscaler already uses this API to do its scaling in a resource-agnostic way_\r\n\r\n## Future State\r\n\r\nWe can extend the [`Scale` v1](https:\/\/kubernetes.io\/docs\/reference\/generated\/kubernetes-api\/v1.29\/#scalestatus-v1-autoscaling) subresource with two new fields:\r\n\r\n- `spec.podDeletionCosts`: used to PATCH the `controller.kubernetes.io\/pod-deletion-cost` annotation on specific Pods\r\n- `status.podDeletionCosts`: used to GET the current `pod-deletion-cost` of Pods in the Deployment\r\n\r\n__Example 1:__ [GET: `\/apis\/apps\/v1\/namespaces\/{namespace}\/deployments\/{name}\/scale`](https:\/\/dev-k8sref-io.web.app\/docs\/workloads\/scale-v1\/):\r\n\r\n```shell\r\n> curl -s localhost:8001\/apis\/apps\/v1\/namespaces\/my-namespace\/deployments\/my-deployment\/scale | yq .\r\nkind: Scale\r\napiVersion: autoscaling\/v1\r\nmetadata:\r\n  name: my-deployment\r\n  namespace: my-namespace\r\nspec:\r\n  replicas: 3\r\n\r\n  ## NOTE: this is empty, because this is a GET, not a PATCH \r\n  ##.      (we could make this be non-empty, but it would be redundant)\r\n  podDeletionCosts: {}\r\n\r\nstatus:\r\n  replicas: 3\r\n  selector: my-label=my-label-value\r\n\r\n  ## all pods with non-empty `controller.kubernetes.io\/pod-deletion-cost` annotations\r\n  ## NOTE: this might include Pods from multiple ReplicaSets, if the Deployment is rolling out\r\n  ## NOTE: a value of 0 means the annotation is explicitly set to 0, not that it is missing\r\n  ##.      (pods with no annotation are NOT included in this list, for efficiency)\r\n  podDeletionCosts:\r\n    pod-1: 0\r\n    pod-2: 100\r\n    pod-3: 200\r\n```\r\n\r\n__Example 2:__ [`kubectl patch ... --subresource=scale`](https:\/\/kubernetes.io\/docs\/tasks\/manage-kubernetes-objects\/update-api-object-kubectl-patch\/#scale-kubectl-patch):\r\n\r\n```shell\r\n# this command does two things:\r\n#  1. scale the Deployment to 2 replica\r\n#  2. set the `pod-deletion-cost` of `pod-1` to -100 (making it much more likely to be deleted)\r\nkubectl patch deployment my-deployment \\\r\n  --namespace my-namespace \\\r\n  --subresource='scale' \\\r\n  --type='merge' \\\r\n  --patch='{\"spec\": {\"replicas\": 2, \"podDeletionCosts\": {\"pod-1\": -100}}}'\r\n```\r\n\r\n## Benefits \/ Drawbacks\r\n\r\n__The main benefits of this approach are:__\r\n\r\n- Autoscaler systems can easily check what the current `pod-deletion-cost` are, and then update them during scale-down as appropriate. No need to make hundreds of Pod GET requests.\r\n- Autoscaler systems can use a single PATCH to change `spec.replicas` AND update the `pod-deletion-cost` of Pods.\r\n- It does not require significant changes to how scaling is implemented. We can piggyback on the existing work of `pod-deletion-cost` annotations.\r\n\r\n__The main drawbacks are:__\r\n\r\n- This still only applies to Deployments\/ReplicaSets (because `pod-deletion-cost` is a feature of ReplicaSets)\r\n- It is slightly strange to automatically update the `controller.kubernetes.io\/pod-deletion-cost` annotation:\r\n    - However, we should remember that the Pods are \"managed\" by the Deployment, so it IS appropriate for the ReplicaSet controller to update the Pod's definition.\r\n\r\n# User Stories\r\n\r\n## User 1: Manual Scaling\r\n\r\nAs a user, I want to be able to scale down a Deployment and influence which Pods are deleted based on my knowledge of the current state of the system.\r\n\r\nFor example, say I am running a stateful application with 3 replicas:\r\n\r\n- I know that `pod-1` is currently idle, but `pod-2` and `pod-3` are both busy.\r\n- I want to scale down to 2 replicas, but I want to make sure that `pod-1` is deleted first, because it is idle.\r\n\r\nTo achieve this, I can do the following:\r\n\r\n1. Verify the Deployment is not currently rolling out (so there is only one active ReplicaSet)\r\n1. Use `kubectl get ... --subresource=scale` to see the current `pod-deletion-cost` of all Pods in the Deployment\r\n1. Use `kubectl patch ... --subresource=scale` to BOTH:\r\n    - set set `replicas` to `2`\r\n    - update the `pod-deletion-cost` of `pod-1` to a value that makes it more likely to be deleted\r\n\r\n## User 2: Custom Autoscalers\r\n\r\nAs a developer of a custom autoscaler, I want to use application-specific metrics to influence which Pods are deleted during scale-down to minimize the impact on my application and its users.\r\n\r\nTo achieve this, I can do the following:\r\n\r\n1. Keep track of what the Pods are doing, so I can make informed decisions about which pods are best to delete.\r\n2. When the time comes to scale down:\r\n     - Use the `Scale` subresource to read the `pod-deletion-cost` of all Pods in the Deployment\r\n     - Use the `Scale` subresource to update the `replicas` AND the `pod-deletion-cost` of Pods as appropriate\r\n\r\n## User 3: HorizontalPodAutoscaler\r\n\r\n_At least initially, the HorizontalPodAutoscaler will not directly use this feature, because it is primarily concerned with scaling `replicas` based on a metric, and does not know about application-specific factors that might influence which Pods should be deleted._\r\n\r\n_However, this feature will make it easier for the HorizontalPodAutoscaler to be extended to have \"pod-deletion-cost\" awareness in the future._","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig autoscaling\r\n\/sig apps\r\n\/wg batch","\/cc","If we had 10000 Pods for a workload, how would using this subresource work? Any concerns there?","I'm a bit worried about the scale too. It reminds me a little of services blowing up with too many pod IPs.\r\nAnd what would the semantic be for patching? If one only wants to update a subset of pods, does the patch support that (e.g. SSA).\r\n\r\nAlso, it seems like a new style of interaction where the handler of the scale subresource needs to patch annotations in all pods. How would we do that? What are the risks? Could this cause requests to stall if we fan out a bunch of patches?\r\nThis ties an annotation very explicitly to a non-annotation field. Maybe the annotation should first move to a proper API field (or something different like Pod Disruption Budgets).\r\n\r\nI'm not very familiar with the annotation, but from a quick read would more expect that users run a cron or something that updates them at certain intervals. Updating them at scale down certainly works, but these annotations feel like metrics so I wonder if there is a better system to handle this in general (maybe using metrics API).","If no pod-deletion-cost is set as a parameter of the scale operation (only pods name\/ip), it could be implicitly -MAX_INT by default","> I'm a bit worried about the scale too. It reminds me a little of services blowing up with too many pod IPs. And what would the semantic be for patching? If one only wants to update a subset of pods, does the patch support that (e.g. SSA).\r\n\r\nThe idea of only updating specific Pods is inherent to the design of `spec.podDeletionCosts`. Think of `spec.podDeletionCosts` as a \"transient\" list of \"requests\" to update the `pod-deletion-cost` annotations of specific Pods, the goal of the Deployment\/ReplicaSet controller is to make it empty by applying it to the Pods.\r\n\r\nFor example, if a 3-replica Deployment (`my-deployment`) has a ReplicaSet which has 3 Pods (`pod-1`, `pod-2`, and `pod-3`), consider the following scale patch on `my-deployment`:\r\n\r\n```yaml\r\nspec:\r\n  replicas: 2\r\n  podDeletionCosts:\r\n    pod-1: -100\r\n```\r\n\r\nThe deployment controller can answer this request by doing the following (IN ORDER):\r\n\r\n1. Updating the `pod-deletion-cost` annotation of `pod-1` to be `-100`\r\n2. Updating the `spec.replicas` of the deployment to be `2`\r\n\r\nWe can be quite lax about processing `spec.podDeletionCosts`. For example, if the controller encounters a non-existent Pod name (or one not owned by the Deployment\/ReplicaSet being patched), then it can just ignore that key and continue silently (possibly creating an event to bubble this up to the user).\r\n\r\n---\r\n\r\n> Also, it seems like a new style of interaction where the handler of the scale subresource needs to patch annotations in all pods. How would we do that? What are the risks? Could this cause requests to stall if we fan out a bunch of patches? This ties an annotation very explicitly to a non-annotation field. Maybe the annotation should first move to a proper API field (or something different like Pod Disruption Budgets).\r\n\r\nUnless the user requests a patch of all pods (by listing every pod in `spec.podDeletionCosts`), the controller should only need to make a small number of patches. But as a protection mechanism, we could have a timeout or a maximum number of `spec.podDeletionCosts` keys.\r\n\r\nRegarding whether we should introduce a new Pod field, and remove the `controller.kubernetes.io\/pod-deletion-cost` annotation, I think this was not done previously in [KEP-2255](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/2255-pod-cost) because such a field would not be applicable to all Pods (only ones in ReplicaSets), and we don't really like extending the Pod API.\r\n\r\n---\r\n\r\n> I'm not very familiar with the annotation, but from a quick read would more expect that users run a cron or something that updates them at certain intervals. Updating them at scale down certainly works, but these annotations feel like metrics so I wonder if there is a better system to handle this in general (maybe using metrics API).\r\n\r\nThe [KEP-2255](https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/2255-pod-cost#risks-and-mitigations) actually explicitly recommends against updating the `pod-deletion-cost` using automated methods, and only recommends people update it just before scale down. (Which is also [noted in the docs](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/replicaset\/#pod-deletion-cost)).\r\n\r\nIt's really important NOT think of `pod-deletion-cost` as a metric, because the actual value is not important. Rather, the relative values across all Pods in a ReplicaSet (at the moment of scale-down) is what matters. During a scale-down, the controller will remove Pods with the lowest `pod-deletion-cost` first (with 0 being the implied default if no `pod-deletion-cost` annotation is set on a Pod).\r\n","> If we had 10000 Pods for a workload, how would using this subresource work? Any concerns there?\r\n\r\n@sftim Here is my thinking about Deployments with MANY pods.\r\n\r\n__In the \"update\" direction:__\r\n\r\n- The controller only needs to take an action for Pods explicitly listed in `spec.podDeletionCosts`, the overall number of Pods that happen to be in the Deployment is not relevant. (Unless the user does something like try and update all 10,000 Pods at the same time, but we could have a \"max updates per request\" for scale patches to mitigate this).\r\n\r\n__In the \"read\" direction:__\r\n\r\n- The `status.podDeletionCosts` (note: `status`) should contain only the Pods which have a `pod-deletion-cost` annotation set, which should be small or empty in most cases.\r\n    - If the `pod-deletion-cost` annotation is used correctly (only setting it just before scale-down, not with a controller), the number of Pods with the annotation should be very low (only non-0 during scale-down), and the frequency of changes to the annotations should be very small.\r\n- Rather than checking the annotations of all Pods for each request (by looping over all the owned Pods), we can store this information in the `status` field of the ReplicaSet (similar to our current `status.availableReplicas` and `status.fullyLabeledReplicas`, but rather than a single integer, as a mapping of `pod_name -> pod_deletion_cost`).\r\n     - The ReplicaSet controller is already checking things about all the Pods it \"owns\", and watching for changes in \"ready\" status, so we can use a similar watcher pattern to reconcile the new `status.podDeletionCosts` field of the ReplicaSet in an efficient way.\r\n\r\n","I'm worried that this won't scale and will be a burden to maintain. You don't need to convince me, but SIG Apps and SIG Node might want to see your working."],"labels":["sig\/autoscaling","sig\/apps","needs-triage","wg\/batch"]},{"title":"jsonpath: refactor to use slices.Contains","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nRefactor `jsonpath.UnquoteExtend` to use [`slices.Contains`](https:\/\/pkg.go.dev\/slices#Contains) instead of self-written `contains` function.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\n\r\n#### Special notes for your reviewer:\r\n\r\n`toBytes` copied from https:\/\/github.com\/kubernetes\/kubernetes\/blob\/3852d1c0c15c90bd747ac7fd25e9c818b1f1c429\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/value\/encrypt\/envelope\/kmsv2\/envelope.go#L512-L520\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\n\r\n```\r\n","comments":["Welcome @alexandear! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @alexandear. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","\/assign @aojea\r\nas client-go owner.\r\n\/ok-to-test\r\n\/triage accepted","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123539#\" title=\"Author self-approved\">alexandear<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [aojea](https:\/\/github.com\/aojea). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/client-go\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"aojea\"]} -->"],"labels":["kind\/cleanup","sig\/api-machinery","size\/S","release-note-none","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"Add the util pkg to commonize job util functions","body":"#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n \r\nBoth [CronJob](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/47737eca1e5daf486d6a2ef4c642655531e776d9\/pkg\/controller\/cronjob\/utils.go#L290), and [Job](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/47737eca1e5daf486d6a2ef4c642655531e776d9\/pkg\/controller\/job\/utils.go#L26) controllers use a similar `IsJobFunction` implementation.\r\n\r\nThis PR adds the `util` package under `pkg\/controller\/job` to place functions used by different controllers (such `IsJobFinished`), effectively eliminating the code duplication.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nFixes #123445\r\n\r\n#### Special notes for your reviewer:\r\nI took the `IsJobFinished` implementation of the `CronJob` package because it uses and additional function which, in turn, is used by the CronJob controller (`GetFinishedStatus`). That's why I decided to expose both and wrote the unit test on the latter based on the `utils_test.go` tests of the job package.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @mimowo ","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123537#\" title=\"Author self-approved\">kaisoz<\/a>*, *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123537#pullrequestreview-1910872392\" title=\"Approved\">Vivekgaddigi<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [kow3ns](https:\/\/github.com\/kow3ns) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/controller\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/OWNERS)**\n- **[test\/e2e\/apps\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e\/apps\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"kow3ns\"]} -->","LGTM label has been added.  <details>Git tree hash: a72fcaf14bbe1f94e7dfc105629dfaa6e868f925<\/details>","\/assign @kow3ns ","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/test","kind\/cleanup","lgtm","needs-rebase","size\/L","release-note-none","sig\/apps","cncf-cla: yes","sig\/testing","needs-priority","needs-triage"]},{"title":"(kubelet) Re-register with API server when Node is not found","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nIf the Node object created by kubelet is mistakenly deleted from the API (either by a human or a controller), kubelet will be effectively cordoned from the cluster with no recourse. This PR resets kubelet's registration when the periodic node status update fails due to the Node object not being found after retries have been exhausted.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #71398 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nChanged how the kubelet handles deletion of its Node object. The kubelet now attempts to recreate its associated Node if the object is removed after the kubelet is already running.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123535#\" title=\"Author self-approved\">cartermckinnon<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [derekwaynecarr](https:\/\/github.com\/derekwaynecarr) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"derekwaynecarr\"]} -->","\/sig node","Changelog suggestion (we frame changes in the past tense)\r\n```diff\r\n-Kubelet will now re-create its Node object if it no longer exists.\r\n+Changed how the kubelet handles deletion of its Node object.\r\n+The kubelet now attempts to recreate its associated Node if the object is removed after the kubelet is\r\n+already running.\r\n```\r\n\r\nHow about that?","With the Karpenter autoscaler, people trigger graceful removal of unwanted nodes using `kubectl delete nodes\/foo`. Will this change affect that flow?","I don\u2019t think so, no. If kubelet happens to re-register after the Node object is deleted but before the instance is terminated, the node lifecycle controller would clean up the re-created Node object shortly thereafter.\r\n\r\n@tzneal sanity check me? ","Kubelet does have the `registerNode` option to disable creation of the Node object entirely, but Karpenter is (or has?) moving away from creating the Node object itself. I could always put this behind a config option, default off, if there's concern about fallout.","Overall, I like the idea. Per our offline discussion, I think that this should be behind a flag, and I think that the flag should initially default to disabled. That will ensure that nobody notices a change in behavior unless it's intentional, and we can change the default on a later version, if desired.\r\n\r\nMy main concerns are about the confusing behavior: I delete my node and it keeps magically reappearing. Even though people should either terminate the underlying instance as well or use other mechanisms like taints and cordoning, it could be challenging to troubleshoot. An event explaining what's happening would be ideal, but I don't know that it's going to be practical to include that here.\r\n\r\nPushing off to a future version will also enable tools that potentially rely on certain behavior to adjust."],"labels":["kind\/bug","area\/kubelet","sig\/node","release-note","size\/S","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"Support endpoint authentication for K8s trace configuration","body":"### What would you like to be added?\r\n\r\nSo far, grpc format endpoint is supported for K8s trace configuration.\r\nIf user wants to use an authentication for trace, it is not feasible.\r\nI want to support grpc authentication for trace configuration.\r\n\r\n### Why is this needed?\r\n\r\nOnly grpc without authentication for trace config is not enough, we need to support\r\ngrpc authentication for trace","comments":["\/sig instrumentation","I'm hoping to keep the current file-based configuration minimal.  In the future, I would like to move to the OpenTelemetry file configuration once it is stable.  Can you use the `OTEL_EXPORTER_OTLP_HEADERS` env var instead?","\/triage accepted","> I'm hoping to keep the current file-based configuration minimal. In the future, I would like to move to the OpenTelemetry file configuration once it is stable. Can you use the `OTEL_EXPORTER_OTLP_HEADERS` env var instead?\r\n\r\nDavid, \r\n\r\nThanks for your suggestion! \r\n\r\nWe would try this solution with OTEL_EXPORTER_OTLP_HEADERS.","\/assign"],"labels":["kind\/feature","sig\/instrumentation","triage\/accepted"]},{"title":"cpu allocation for static policy should not only limited to Guaranteed Pod.","body":"### What happened?\r\n\r\ni am running a pod which has two containers, the first is the main container(called container1) which has gpu resources, the second is a sidecar container(called container2) with no gpu which just do normal workload, besides both of the  request\/limits are equal.\r\n\r\nTo achieve optimal performance, I have enabled both the CPU Manager and Topology Manager with options below\r\n```\r\ntopologyManagerPolicy: restricted\r\ncpuManagerPolicy: static\r\ncpuManagerPolicyOptions:\r\n  align-by-socket: \"true\"\r\n  distribute-cpus-across-numa: \"true\"\r\n```\r\n\r\nwhen i applied the yaml and the pod become running, both containers are bound to CPU cores, result is below\r\n```\r\n{\r\n  \"policyName\": \"static\",\r\n  \"defaultCpuSet\": \"0-7,14-15,22-23,26-63\",\r\n  \"entries\": {\r\n    \"2825b50a-bc6f-44c5-b04c-e6b8b68d8dfe\": {\r\n      \"container1\": \"8-13,16-21\",\r\n      \"container2\": \"24-25\"\r\n    }\r\n  },\r\n```\r\nI don't expect the second container  also bind to CPUs, as it occupies CPU resources on a new NUMA node, resulting in fewer CPU resources corresponding to the unused GPU card on that NUMA. \r\n\r\nif i make the second container request and limits different, the qosClass of the pod becomes burstable,  both of the container will use shared pool resources.\r\n```\r\n{\r\n  \"policyName\": \"static\",\r\n  \"defaultCpuSet\": \"0-63\",\r\n  \"checksum\": 1058907510\r\n}\r\n```\r\n\r\n### What did you expect to happen?\r\n\r\nthe main container(limits==requests) bind cpus but the sidecar container(limits != requests) should not,  but use the shared pool resources,\r\nwe should not rely on 'guarantee pod' as a condition but rather on 'containers' since resource allocation is done at the granularity of containers.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nBelow is the yaml file, you just change the resource of the second container with two conditions, request and limit resources are equal and not equal, then check cpu allocation result.\r\n```\r\napiVersion: apps\/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: guarantee\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      gpu: baichuan\r\n  template:\r\n    metadata:\r\n      labels:\r\n        gpu: baichuan\r\n    spec:\r\n      containers:\r\n      - command:\r\n        - sh\r\n        - -c\r\n        - while true;do sleep 1s;done\r\n        name: container1\r\n        image: ubuntu:20.04\r\n        imagePullPolicy: IfNotPresent\r\n        resources:\r\n          limits:\r\n            cpu: \"12\"\r\n            memory: 5Gi\r\n            nvidia.com\/gpu: \"2\"\r\n          requests:\r\n            cpu: \"12\"\r\n            memory: 5Gi\r\n            nvidia.com\/gpu: \"2\"\r\n      - command:\r\n        name: container2\r\n        image: nginx\r\n        imagePullPolicy: IfNotPresent\r\n        ports:\r\n        - containerPort: 80\r\n          name: http\r\n          protocol: TCP\r\n        resources:\r\n          limits:\r\n            cpu: \"2\"\r\n            memory: 2Gi\r\n          requests:\r\n            cpu: \"2\"\r\n            memory: 2Gi\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.1\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig node","could you help triage this @ffromani or @swatisehgal ?","this is something we talked about in the past quite a few times. I'll look to find references, but as it is today it will surely be a RFE, not a bug","\/triage accepted\r\n\/remove-kind bug\r\n\/kind feature","> this is something we talked about in the past quite a few times. I'll look to find references, but as it is today it will surely be a RFE, not a bug\r\n\r\nI agree, Could you provide a link to the discussion at that time?\r\n\r\nIn my opinion, pod condition is just a special case of container condition, that is one pod one container, one pod with multiple containers  are more commonly cases. it should not occupy resources we expect other containers to bind.\r\n\r\nContainer condition is more accurate and flexible, that is the fact.","xref: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/116086."],"labels":["sig\/node","kind\/feature","triage\/accepted"]},{"title":"fix: typos error info","body":"#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\ntypo\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @testwill. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123528#\" title=\"Author self-approved\">testwill<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","\/ok-to-test\n\/lgtm","LGTM label has been added.  <details>Git tree hash: 16c5f92e210da5b611e1fca775b1f9d7e2f04d3c<\/details>"],"labels":["lgtm","size\/S","release-note-none","cncf-cla: yes","ok-to-test","needs-priority","needs-triage","do-not-merge\/needs-sig","do-not-merge\/needs-kind"]},{"title":"Fixing description of the `meta.resourceVersion` field","body":"#### What type of PR is this?\r\n\r\n\/kind documentation\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nThis PR is fixing the description of the `metadata.revisionVersion` field where the last sentence seems to be unfinished as can be seen in the [staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\/types.go](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.29\/staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\/types.go#L169) file. The changes in the rest of the files are auto-generated by running the `make update` command.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```","comments":["Welcome @jtyr! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @jtyr. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123524#\" title=\"Author self-approved\">jtyr<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/OWNERS)**\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","\/assign @jpbetz \r\nFor API Review.\r\n\/triage accepted"],"labels":["kind\/documentation","sig\/api-machinery","size\/M","kind\/api-change","release-note-none","cncf-cla: yes","needs-ok-to-test","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"Consider exposing node metadata.creationTimestamp via downward api","body":"### What would you like to be added?\n\nA pod's metadata.creationTimestamp would be useful information inside a container. This could be accomplished via downward API similar to other metadata currently available.\n\n### Why is this needed?\n\nConsider a cronjob execution. When executed, the cronjob creates a job, and the job creates a pod with container(s). \r\nIf the containers' logic depends on the time the cronjob was scheduled having the pod creationTime would be more useful than simply obtaining the current time.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\nfor triage","Another related feature is the ability for the `batch.kubernetes.io\/cronjob-scheduled-timestamp` annotation that gets added to a job to be passed onto the child pod.","Hey @steven-herchak,\r\n\r\nI think the first one can be done as a downward API but not sure of reasons why it wasn't included. Maybe @thockin has an idea?\r\n\r\nDownward API tends to expose variables in the pod spec. I'm not really sure how that relates to higher level workloads so I'm not sure if cronjobs would fit in that case.\r\n\r\n","Opened up https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123533 as I wanted to learn a bit more about how downward API actually works. \r\n","I like the idea of annotating the CronJob's timings into the Job and its Pods:\r\n- the actual creation timestamp of the Job\r\n  - create a [suspended](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/#suspending-a-job) Job\r\n  - capture the Job's creation timestamp **and set the annotation in the Pod template**\r\n  - unsuspend the Job (can be atomic with adding the annotation)\r\n  - this would mean registering a new annotation key\r\n- when the CronJob _should_ have created the Job: [`batch.kubernetes.io\/cronjob-scheduled-timestamp`](https:\/\/kubernetes.io\/docs\/reference\/labels-annotations-taints\/#batch-kubernetes-io-cronjob-scheduled-timestamp)\r\n  -  added at Job creation time\r\n\r\n\r\nHowever, is it worth putting contributor time into that idea? Only if someone feels it's valuable.","Mostly the reason is that we only exposed things we had concrete demand for, rather than \"everything\".\r\n\r\nWe do support annotations in the volume-creating downward API, but not the env-creating one (I think it was not needed and syntax was ambiguous).","I guess one thing to note is that (IIRC) you can't implement the equivalent out of tree, because:\r\n- at admission time, `metadata.creationTimestamp` isn't set, so a mutating admission controller can't intervene\r\n  - _have I got this right? I didn't check._\r\n- after admission time, the Pod is liable to get bound to a node and its containers created, before a watch \/ informer cna catch up and mutate the annotations on the existing Pod (two process that race, and as soon as the Pod is running, it may be too late to expose the field","I opened up https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123533 and got some good feedback from @thockin. https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123533#issuecomment-1972038281\r\n\r\nI will be on paternity leave for most of 1.31 so if anyone is interested in bringing this up as a KEP, please feel free! I even have a good starting point if anyone is interested.\r\n\r\nThe main action items will be to get some RFC (request for comments) on other downward api fields in the podspec and see if we could implement ones that have actual usecases in a KEP.\r\n\r\nAnd for \r\n\r\n> Another related feature is the ability for the batch.kubernetes.io\/cronjob-scheduled-timestamp annotation that gets added to a job to be passed onto the child pod.\r\n\r\nI would maybe consider opening that as a separate issue and explore that in the batch CronJob controller. One could add that label\/annotation to the PodSpec and you would get that for free as part of the downward api support for labels\/annotations. "],"labels":["sig\/node","kind\/feature","needs-triage"]},{"title":"WIP: testing split disk e2e tests","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-e2e-diskpressure`\n* `\/test pull-crio-cgroupv2-imagefs-e2e-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1965171194):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\r\n","\/test pull-crio-cgroupv2-imagefs-e2e-separatedisktest\r\n","\r\n\/test pull-crio-cgroupv2-imagefs-e2e-separatedisktest\r\n\r\n","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1965492457):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk\r\n\/test pull-crio-cgroupv2-imagefs-separatedisktest\r\n","\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk\r\n","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1969966429):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\r\n","\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk\r\n","\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\r\n\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1969989000):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1970094730):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\r\n\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk","\/test","@kannon92: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-crio-cgroupv2-splitfs-splitdisk`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `check-dependency-stats`\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-gce-cos-alpha-features`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#issuecomment-1971609663):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-crio-cgroupv2-splitfs-splitdisk\r\n","\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1\r\n\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2\r\n\/test pull-kubernetes-node-swap-ubuntu-serial\r\n","\/test pull-crio-cgroupv2-splitfs-splitdisk\r\n\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\r\n","\/test pull-crio-cgroupv2-splitfs-splitdisk\r\n\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e","\/test pull-crio-cgroupv2-splitfs-splitdisk\r\n\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123518#\" title=\"Author self-approved\">kannon92<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","\/test pull-crio-cgroupv1-node-e2e-features\r\n","\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e","@kannon92: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-crio-cgroupv2-imagefs-e2e-separatedisktest | 461ee37360a2dea314d97d329c023eeb60ba9d30 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-crio-cgroupv2-imagefs-e2e-separatedisktest\/1762214142342598656) | false | `\/test pull-crio-cgroupv2-imagefs-e2e-separatedisktest`\npull-crio-cgroupv2-splitfs-e2e-splitdisk | 0b4bfb860ec1f66b9207b0fd8e974d015dcfb510 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-crio-cgroupv2-splitfs-e2e-splitdisk\/1762956431574700032) | false | `\/test pull-crio-cgroupv2-splitfs-e2e-splitdisk`\npull-kubernetes-node-kubelet-serial-crio-cgroupv1 | 0b4bfb860ec1f66b9207b0fd8e974d015dcfb510 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-kubernetes-node-kubelet-serial-crio-cgroupv1\/1763296491234922496) | false | `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\npull-kubernetes-node-kubelet-serial-crio-cgroupv2 | 0b4bfb860ec1f66b9207b0fd8e974d015dcfb510 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-kubernetes-node-kubelet-serial-crio-cgroupv2\/1763296491302031360) | false | `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\npull-crio-cgroupv2-splitfs-splitdisk | 1dcc1cca466971b7e7e8ed8898c3802c00d0eeaa | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-crio-cgroupv2-splitfs-splitdisk\/1764773999407009792) | false | `\/test pull-crio-cgroupv2-splitfs-splitdisk`\npull-crio-cgroupv1-node-e2e-features | d40fb2a00179ed3d2ca76b620e53ebd46a208fae | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-crio-cgroupv1-node-e2e-features\/1765032300807983104) | false | `\/test pull-crio-cgroupv1-node-e2e-features`\npull-kubernetes-node-crio-cgrpv2-splitfs-e2e | d40fb2a00179ed3d2ca76b620e53ebd46a208fae | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-kubernetes-node-crio-cgrpv2-splitfs-e2e\/1765187281209004032) | false | `\/test pull-kubernetes-node-crio-cgrpv2-splitfs-e2e`\npull-kubernetes-linter-hints | aa606dda926949365286bec0376ddf98ed65cf49 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123518\/pull-kubernetes-linter-hints\/1765380929515163648) | false | `\/test pull-kubernetes-linter-hints`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123518). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Akannon92). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/test","area\/kubelet","sig\/node","size\/L","release-note-none","cncf-cla: yes","sig\/testing","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"test: bump delay to decrease chance of test flake","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind flake\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdds more delay to the container as the test flakes indicate that we're receiving less than the expected delay.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123470\r\n\r\n#### Special notes for your reviewer:\r\n\r\nI've had no luck reproducing this flake locally. I've run the test thousands of times using stress-ng to stress all of the CPUs on my node. Its still flaking in the test grid though, so hopefully this will resolve that.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123515#\" title=\"Author self-approved\">tzneal<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","\/retest\r\n\/triage accepted\r\n\/priority important-longterm","\/cc @SergeyKanzhelev @kannon92 ","@tzneal: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify | 52dc7e56b23b0d671c956b3d285450d2d569194e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123515\/pull-kubernetes-verify\/1764647323700301824) | true | `\/test pull-kubernetes-verify`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123515). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Atzneal). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->"],"labels":["area\/kubelet","sig\/node","size\/XS","kind\/flake","release-note-none","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"Duplicated shortname \"vs\"","body":"### What happened?\n\nHello.\r\n\r\nI just noticed that \"vs\" shortname is set for both \"virtualservices\" and \"volumesnapshots\".\r\n\r\n`$ for i in $(k api-resources); do if [ \"$(echo $i | awk '{print NF}')\" == \"5\" ]; then echo $i; fi; done | sort -k2 | grep vs\r\nvolumesnapshotclasses               vsclass,vsclasses        snapshot.storage.k8s.io            false        VolumeSnapshotClass\r\nvolumesnapshotcontents              vsc,vscs                 snapshot.storage.k8s.io            false        VolumeSnapshotContent\r\nvirtualservices                     vs                       networking.istio.io                true         VirtualService\r\nvolumesnapshots                     vs                       snapshot.storage.k8s.io            true         VolumeSnapshot\r\n`\r\n\n\n### What did you expect to happen?\n\nBy the above, it looks like my system...\r\n`]$ kubectl version\r\nW0226 11:09:40.682098 1183782 gcp.go:119] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.26+; use gcloud instead.\r\nTo learn more, consult https:\/\/cloud.google.com\/blog\/products\/containers-kubernetes\/kubectl-auth-changes-in-gke\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nKustomize Version: v4.5.7\r\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10-gke.1101000\", GitCommit:\"375ed214cfa092ed25d2472c1709db5d7dcda078\", GitTreeState:\"clean\", BuildDate:\"2023-11-06T09:23:17Z\", GoVersion:\"go1.20.10 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n`\r\n...considers \"vs\" as the shortname for \"volumesnapshots\" since \"kubectl get vs -A\" returns no virtualservices even I having thousands of them in the cluster. =)\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI noticed this behavior on 1.26 and 1.27 clusters (Google's  GKE). \r\n\r\nI think the below would helps to reproduce in bash.\r\n`export IFS=$'\\n'\r\nfor i in $(k api-resources); do if [ \"$(echo $i | awk '{print NF}')\" == \"5\" ]; then echo $i; fi; done | sort -k2 | grep vs`\r\n\r\nIt should show both virtualservices and volumesnapshots with \"vs\" shortname.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nW0226 11:09:40.682098 1183782 gcp.go:119] WARNING: the gcp auth plugin is deprecated in v1.22+, unavailable in v1.26+; use gcloud instead.\r\nTo learn more, consult https:\/\/cloud.google.com\/blog\/products\/containers-kubernetes\/kubectl-auth-changes-in-gke\r\nWARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\r\nClient Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\nKustomize Version: v4.5.7\r\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.10-gke.1101000\", GitCommit:\"375ed214cfa092ed25d2472c1709db5d7dcda078\", GitTreeState:\"clean\", BuildDate:\"2023-11-06T09:23:17Z\", GoVersion:\"go1.20.10 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nGCP\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n$ cat \/etc\/os-release\r\nNAME=\"Red Hat Enterprise Linux\"\r\nVERSION=\"8.6 (Ootpa)\"\r\nID=\"rhel\"\r\nID_LIKE=\"fedora\"\r\nVERSION_ID=\"8.6\"\r\nPLATFORM_ID=\"platform:el8\"\r\nPRETTY_NAME=\"Red Hat Enterprise Linux 8.6 (Ootpa)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:\/o:redhat:enterprise_linux:8::baseos\"\r\nHOME_URL=\"https:\/\/www.redhat.com\/\"\r\nDOCUMENTATION_URL=\"https:\/\/access.redhat.com\/documentation\/red_hat_enterprise_linux\/8\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugzilla.redhat.com\/\"\r\n\r\nREDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=8.6\r\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"8.6\"\r\n$ uname -a\r\nLinux gcp-anthos01 4.18.0-372.9.1.el8.x86_64 #1 SMP Fri Apr 15 22:12:19 EDT 2022 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","both \"vs\" objects are custom resources. while Istio is not a k8s subproject, \"volumesnapshots\" originated by the K8S group SIG Storage, so that's closer to core k8s:\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/blob\/cf6ee34e37f00d838872d368ec66d7a0b40ee4e6\/keps\/sig-storage\/177-volume-snapshot\/README.md?plain=1#L457\r\n\r\neither way, kubectl likely just decides to pick one of them.\r\n\r\n> It should show both virtualservices and volumesnapshots with \"vs\" shortname.\r\n\r\nthat's a SIG CLI request\r\n\/sig cli\r\n\r\n\r\n ","Best we can do with respect to this topic is to add a warning about this ambiguity and that was added by https:\/\/github.com\/kubernetes\/kubernetes\/pull\/117668 ","> Best we can do with respect to this topic is to add a warning about this ambiguity and that was added by #117668\r\n\r\nBesides this warning, it\u00b4d be kind if major k8s projects would observe some agreement on not to overlap each other\u00b4s shortnames. =)\r\n\r\nIstio is not a minor peripheral project and, AFAIK, they \"reserved\" 'vs' first.\r\n\r\nMany thousands of people around the globe may be using vs shortname in their scripts which broke when the new vs arose. \r\n\r\nRegards!","Thanks for dropping your thoughts @ericitaquera. As kubectl maintainers, we always recommend to use fully qualified resource names to prevent such short name collisions ","\/remove-kind bug\r\n\r\nI don't see any fault here.","Actually, for me `vs` returns VirtualServices, meaning it's not consistent.\r\n\r\nWhen kubectl prints a warning like\r\n```\r\nWarning: short name \"vs\" could also match lower priority resource volumesnapshots.snapshot.storage.k8s.io\r\n```\r\n\r\nwhat priority does it actually mean? How can I see those priorities? How can I be sure that for my clusters `vs` will always be the same resource type?\r\n"],"labels":["sig\/cli","needs-kind","needs-triage"]},{"title":"Move ConsistentListFromCache to Beta default","body":"\/kind feature\r\n\r\nRef https:\/\/github.com\/kubernetes\/enhancements\/issues\/2340\r\n```release-note\r\nMove ConsistentListFromCache feature flag to Beta and enable it by default\r\n```\r\n","comments":["Skipping CI for Draft Pull Request.\nIf you want CI signal for your change, please convert it to an actual PR.\nYou can still manually trigger a test run with `\/test all`","\/test all","\/test all","\/test all","\/test all","\/retest","\/retest","\/cc @wojtek-t ","\/retest","\/retest\r\n\r\nDon't have any idea how to debug `test-cmd: run_recursive_resources_tests` tests.\r\n\r\ncc @dead2k","Managed to debug the test, look like we ConsistentListFromCache allowed serving non-recursive lists with RV=\"\" from cache, but didn't request progress notification as I didn't think we need to add it to `WaitUntilFreshAndGet` function.","\/retest","\/retest","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123513#\" title=\"Author self-approved\">serathius<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/features\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/util\/flowcontrol\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/util\/flowcontrol\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","We got a pass! Unfortunately I found that the integration tests are flaking due to bug in etcd. https:\/\/github.com\/etcd-io\/etcd\/issues\/17507\r\n\r\n:( \r\n\r\n\/test pull-kubernetes-integration","\/test pull-kubernetes-integration","@serathius: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-integration | 09c7bfd311507bdda0a37e685f6335a9697a7572 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123513\/pull-kubernetes-integration\/1764698703815249920) | true | `\/test pull-kubernetes-integration`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123513). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aserathius). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/triage accepted","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/test","area\/apiserver","sig\/api-machinery","release-note","needs-rebase","size\/M","kind\/feature","cncf-cla: yes","sig\/testing","needs-priority","triage\/accepted"]},{"title":"[WIP] Introduce informer fifo\/handler metrics","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdds informer fifo\/handler metrics, it depends on [another code](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123367)\r\n\r\n\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\n\r\nPartially implements https:\/\/github.com\/kubernetes\/enhancements\/issues\/4346\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nIntroduce informer fifo\/handler metrics\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-api-machinery\/4346-informer-metrics\r\n\r\n```\r\n\r\n\/sig api-machinery\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @chenk008. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123510#\" title=\"Author self-approved\">chenk008<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [ncdc](https:\/\/github.com\/ncdc) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/client-go\/tools\/cache\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/tools\/cache\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"ncdc\"]} -->"],"labels":["sig\/api-machinery","release-note","size\/L","kind\/feature","cncf-cla: yes","needs-ok-to-test","do-not-merge\/work-in-progress","needs-priority","needs-triage"]},{"title":"Add conditional endPort printing for NetworkPolicies","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind feature\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nAdds a conditional to the networkpolicy print logic to print information about endPort.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123506\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nFixed a bug where `kubectl describe` incorrectly displayed NetworkPolicy port ranges\r\n(showing only the starting port).\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: kennedn \/ (5fb3ecd9c188f0722c7c8aae037a48feb3f1fa41)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @kennedn! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @kennedn. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123507#\" title=\"Author self-approved\">kennedn<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","Similar to https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123316","We avoid referencing issues in commit messages\r\n\/retitle Add conditional endPort printing for NetworkPolicies","Changelog suggestion\r\n```diff\r\n-Added port range information when describing a NetworkPolicy that had endPort set.\r\n+Fixed a bug where `kubectl describe` incorrectly displayed NetworkPolicy port ranges\r\n+(showing only the starting port).\r\n```","\/ok-to-test","\/triage accepted"],"labels":["area\/kubectl","release-note","size\/S","kind\/feature","sig\/cli","cncf-cla: yes","ok-to-test","needs-priority","triage\/accepted"]},{"title":"Enhance describe of NetworkPolicy to include information about endPort","body":"### What would you like to be added?\n\nCurrently the endPort is not considered when describing a given NetworkPolicy. For example the following network policy:\r\n\r\n```yaml\r\napiVersion: networking.k8s.io\/v1\r\nkind: NetworkPolicy\r\nmetadata:\r\n  name: test-network-policy\r\n  namespace: default\r\nspec:\r\n  podSelector:\r\n    matchLabels:\r\n      role: db\r\n  policyTypes:\r\n  - Ingress\r\n  - Egress\r\n  ingress:\r\n  - from:\r\n    - ipBlock:\r\n        cidr: 172.17.0.0\/16\r\n    ports:\r\n    - protocol: TCP\r\n      port: 6379\r\n      endPort: 6381\r\n  egress:\r\n  - to:\r\n    - ipBlock:\r\n        cidr: 10.0.0.0\/24\r\n    ports:\r\n    - protocol: TCP\r\n      port: 5978\r\n      endPort: 5980\r\n```\r\n\r\nWill only show information on the starting port when described:\r\n\r\n```bash\r\nName:         test-network-policy\r\nNamespace:    default\r\nCreated on:   2024-02-26 10:42:41 +0000 GMT\r\nLabels:       <none>\r\nAnnotations:  <none>\r\nSpec:\r\n  PodSelector:     role=db\r\n  Allowing ingress traffic:\r\n    To Port: 6379\/TCP\r\n    From:\r\n      IPBlock:\r\n        CIDR: 172.17.0.0\/16\r\n        Except:\r\n  Allowing egress traffic:\r\n    To Port: 5978\/TCP\r\n    To:\r\n      IPBlock:\r\n        CIDR: 10.0.0.0\/24\r\n        Except:\r\n  Policy Types: Ingress, Egress\r\n```\n\n### Why is this needed?\n\nWe have had a few cases where the above behavior has caused confusion.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig cli\r\n","This is a duplicate of https:\/\/github.com\/kubernetes\/kubectl\/issues\/1565\r\n\/triage duplicate"],"labels":["kind\/feature","sig\/cli","triage\/duplicate","needs-triage"]},{"title":"fix: function name typos","body":"#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\nfunction name typos\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @testwill. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123505#\" title=\"Author self-approved\">testwill<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https:\/\/github.com\/liggitt) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/api\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"liggitt\"]} -->"],"labels":["size\/S","release-note-none","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage","do-not-merge\/needs-sig","do-not-merge\/needs-kind"]},{"title":"deprecate ioutil","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind cleanup\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nioutil is deprecated. So I use `os` package instead of ioutil.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @sivchari. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123502#\" title=\"Author self-approved\">sivchari<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [xing-yang](https:\/\/github.com\/xing-yang) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/volume\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/volume\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"xing-yang\"]} -->"],"labels":["kind\/cleanup","sig\/storage","size\/L","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage"]},{"title":"Allow admin to set the default bandwidth limit per port (network interface) for a container in a pod.","body":"### What would you like to be added?\r\nCPU\/Memory like bandwidth in ResourceQuota\/LimitRange\r\n\r\nImprove k8s to handle ingress-bandwidth and egress-bandwidth with ResourceQuota and LimitRange\r\n\r\nAllow to set bandwidth settings in ResourceQuota as follows.\r\n\r\napiVersion: v1\r\nkind: ResourceQuota\r\nmetadata:\r\n  name: pod-and-network-quota\r\n  namespace: foo-ns\r\nspec:\r\n  hard:\r\n    pods: 4\r\n    ingress-bandwidth: \"1G\"  <===\r\n    egress-bandwidth: \"1G\"   <===\r\n\r\nAllow to set bandwidth settings in LimitRange as follows.\r\n\r\napiVersion: v1\r\nkind: LimitRange\r\nmetadata:\r\n  name: cpu-and-network-limitrange\r\nspec:\r\n  limits:\r\n  - type: \"Pod\" \r\n    default:\r\n      cpu: 500m\r\n      ingress-bandwidth: \"10M\"  <===\r\n      egress-bandwidth: \"10M\"   <===\r\n    defaultRequest:\r\n      cpu: 500m\r\n    max: # max and min define the limit range\r\n      cpu: \"1\" \r\n      ingress-bandwidth: \"100M\"  <===\r\n      egress-bandwidth: \"100M\"   <===\r\n    min:\r\n      cpu: 100m\r\n      ingress-bandwidth: \"1M\"    <===\r\n      egress-bandwidth: \"1M\"     <===\r\n\r\nExpected behaviors\r\n\r\n    Reject pod creation when (*):\r\n        ResourceQuota has ingress\/egress-bandwidth parmaeters.\r\n        User hadn't set kubernetes.io\/ingress-bandwidth and kubernetes.io\/egress-bandwidth to their pod.\r\n        LimitRange has no default values for ingress\/egress-bandwidth.\r\n    Set default value to kubernetes.io\/ingress-bandwidth and kubernetes.io\/egress-bandwidth of a pod when:\r\n        LimitRange has some default values for ingress\/egress-bandwidth.\r\n    Reject pod creation when:\r\n        The value of kubernetes.io\/ingress-bandwidth or kubernetes.io\/egress-bandwidth of a pod is larger\/lower than the max\/min value of LimitRange.\r\n    Reject pod creation when:\r\n        The total kubernetes.io\/ingress-bandwidth values of pods running on the namespace exceeds the ResourceQuota's value.\r\n\r\n(*) It's best to reject to pod creation request when pod has no bandwidth annoatation even though ResourceQuota has bandwidth setting, since pod which has no bandwidth annotation can exceeds the quota.\r\nFeature to allow admin to set bandwith limit per port in a container.\r\nPod level bandwidth limit should be supported by ResourceQuota and LimitRange\r\n\r\n### Why is this needed?\r\n\r\nUsers can set kubernetes.io\/ingress-bandwidth and kubernetes.io\/egress-bandwidth to their pods.\r\nBut there is no way through which admit can force it to users. \r\nSuch feature is very necessary in PaaS usage.","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig network","@kubernetes\/sig-network-feature-requests ","xref some open issues: \r\n- better bandwidth control: https:\/\/github.com\/kubernetes\/enhancements\/pull\/2808\r\n- QoS: https:\/\/github.com\/kubernetes\/enhancements\/issues\/3008\r\n\r\nAt the moment, all bandwidth control is implementation-centric.  Some don't support it at all, or support only one direction.  Until we support it as a semi-standard (and TBD what that means, really) we can't really have standard quota.\r\n\r\nI think the QoS issue is closest to how we really want to define the API, and that *MUST* solve for quota, IMO.\r\n\r\nFor bookkeeping I am going to close this, since those KEPs cover.","@thockin \r\n- https:\/\/github.com\/kubernetes\/enhancements\/pull\/2808: issue has been discussed partially and agreed on some API for its solutions .\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/3008 : Currently it has been discussed, changed for cpu and memory only no bandwidth scenario discussed.\r\n\r\nI am redefining complete use case here and reopening it to keep in track for possible solution.\r\n\/reopen","@kundan2707: Reopened this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123501#issuecomment-2007193761):\n\n>@thockin \r\n>- https:\/\/github.com\/kubernetes\/enhancements\/pull\/2808: issue has been discussed partially and agreed on some API for its solutions .\r\n>- https:\/\/github.com\/kubernetes\/enhancements\/issues\/3008 : Currently it has been discussed, changed for cpu and memory only no bandwidth scenario discussed.\r\n>\r\n>I am redefining complete use case here and reopening it to keep in track for possible solution.\r\n>\/reopen\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/network","kind\/feature","needs-triage"]},{"title":"Failure cluster [ea667941...] ci-kubernetes-e2e-windows-win2022-containerd-gce-master and ci-kubernetes-e2e-windows-containerd-gce-master failing somewhat consistently","body":"### Failure cluster [ea6679417165f10786e6](https:\/\/go.k8s.io\/triage#ea6679417165f10786e6)\r\n\r\n<img width=\"981\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/23304\/d8572ab4-3694-4df4-bfbe-3ad09f1a9e63\">\r\n\r\n\r\n##### Error text:\r\n```\r\nerror during .\/hack\/e2e-internal\/e2e-up.sh: exit status 1\r\n```\r\n#### Recent failures:\r\n[2\/25\/2024, 12:41:24 PM ci-kubernetes-e2e-windows-win2022-containerd-gce-master](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-windows-win2022-containerd-gce-master\/1761808618274951168)\r\n[2\/25\/2024, 12:35:25 PM ci-kubernetes-e2e-windows-containerd-gce-master](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-windows-containerd-gce-master\/1761807110275862528)\r\n[2\/25\/2024, 8:40:25 AM ci-kubernetes-e2e-windows-win2022-containerd-gce-master](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-windows-win2022-containerd-gce-master\/1761747969108873216)\r\n[2\/25\/2024, 8:35:24 AM ci-kubernetes-e2e-windows-containerd-gce-master](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-windows-containerd-gce-master\/1761746709811040256)\r\n[2\/25\/2024, 4:39:24 AM ci-kubernetes-e2e-windows-win2022-containerd-gce-master](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-windows-win2022-containerd-gce-master\/1761687318806138880)\r\n\r\n\r\n\/kind failing-test\r\n<!-- If this is a flake, please add: \/kind flake -->\r\n\r\n<!-- Please assign a SIG using: \/sig SIG-NAME -->\r\n\/sig windows","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Some more questionable CI jobs\r\n\r\n- ci-kubernetes-e2e-capz-master-windows-service-proxy\r\n  - https:\/\/testgrid.k8s.io\/sig-windows-experimental#capz-master-windows-service-proxy&width=20\r\n  - https:\/\/prow.k8s.io\/?job=ci-kubernetes-e2e-capz-master-windows-service-proxy\r\n- ci-kubernetes-e2e-capz-master-windows-hyperv\r\n  - https:\/\/testgrid.k8s.io\/sig-windows-experimental#capz-master-windows-hyperv&width=20\r\n  - https:\/\/prow.k8s.io\/?job=ci-kubernetes-e2e-capz-master-windows-hyperv\r\n- ci-gce-pd-csi-driver-latest-k8s-master-windows-2019\r\n  -  https:\/\/testgrid.k8s.io\/provider-gcp-compute-persistent-disk-csi-driver#ci-windows-2019-provider-gcp-compute-persistent-disk-csi-driver&width=20\r\n  - https:\/\/prow.k8s.io\/?job=ci-gce-pd-csi-driver-latest-k8s-master-windows-2019\r\n- hourly-soak-tests-capz-windows-2019\r\n  - https:\/\/testgrid.k8s.io\/sig-windows-soak-tests#hourly-soak-tests-capz-windows-2019&width=20\r\n  - https:\/\/prow.k8s.io\/?job=hourly-soak-tests-capz-windows-2019\r\n"],"labels":["sig\/windows","kind\/failing-test","needs-triage"]},{"title":"add StreamDropEventTime variable","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind cleanup\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\nthis pr add a variable to kubelet. \r\nin containerd cri there is a timeout value for queue of events that's hard coded.\r\nI want to refactor and use a variable. In order to do that i need a new variable in config struct and a default value for that\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```docs\r\nkubelet: set a variable for event subscriber timeout\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: amghazanfari \/ (094752fbe364b5a65269c388adbe4e99a94cfd58)<\/li><li>:white_check_mark:login: amghazanfari \/ (094752fbe364b5a65269c388adbe4e99a94cfd58, a07c5344ef1274e4580ed9e47e4ee872f432492e)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Welcome @amghazanfari! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","Hi @amghazanfari. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123485#\" title=\"Author self-approved\">amghazanfari<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","\/assign klueska"],"labels":["kind\/cleanup","size\/XS","cncf-cla: yes","needs-ok-to-test","do-not-merge\/release-note-label-needed","needs-priority","needs-triage","do-not-merge\/needs-sig"]},{"title":"`util.NextPod` leaks goroutine","body":"\/sig scheduling\r\n\/kind cleanup\r\n\r\nRef: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122627#discussion_r1451648811\r\n\r\nA test helper function `util.NextPod` creates a goroutine to pop a Pod from the scheduling queue. This goroutine never stops until the scheduling queue is `Close()`ed or `testCtx.Scheduler.NextPod(logger)` is somehow stopped for waiting for a new Pod. Ideally, when reaching timeout, we should somehow stop `testCtx.Scheduler.NextPod(logger)` from waiting for Pod to be enqueued to activeQ.\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/util\/util.go#L1163","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","We can replace this `timeout` function with `PollUntilContextTimeout` and it should be enough? \r\nI think we no longer need to maintain this function.\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/781da7595167f872662db3bb62290d1b0522f666\/test\/integration\/util\/util.go#L1127","Not enough, the core of this problem is that we cannot cancel once we start `Scheduler.NextPod`.\r\nEven if we change `timeout` function to whatever, we will hit this problem. So, we need:\r\n\r\n> Ideally, when reaching timeout, we should somehow stop testCtx.Scheduler.NextPod(logger) from waiting for Pod to be enqueued to activeQ.","Or, alternatively and much more easily, maybe we can just replace all `NextPod` with `NextPodOrDie` and remove `NextPod`. `NextPodOrDie` kills the entire test if reaching timeout and we could care less about leaked goroutine then. ","> Or, alternatively and much more easily, maybe we can just replace all NextPod with NextPodOrDie and remove NextPod. NextPodOrDie kills the entire test if reaching timeout and we could care less about leaked goroutine then.\r\n\r\nI don't think we can do this.\r\n\r\nFrom the test case:\r\n```\r\n\t\/\/ Pod2 and Pod3 are not expected to be popped out.\r\n\t\/\/ - Although the failure reason has been lifted, Pod2 still won't be moved to active due to\r\n\t\/\/   the node event's preCheckForNode().\r\n\t\/\/ - Regarding Pod3, the NodeTaintChange event is irrelevant with its scheduling failure.\r\n\tpodInfo = testutils.NextPod(t, testCtx)\r\n\tif podInfo != nil {\r\n\t\tt.Fatalf(\"Unexpected pod %v get popped out\", podInfo.Pod.Name)\r\n\t}\r\n```\r\nThe expected behavior of this case is that we receive a timeout to make sure no pods are added to `activeQ`.\r\nSo, reaching timeout is acutally a success.\r\n\r\nSo, I think:\r\noption 1: using sth like this in `p.Pop()`:\r\n```\r\ndone := make(chan struct{})\r\ngo func() {\r\n \tfor p.activeQ.Len() == 0 {\r\n\t\tp.cond.Wait()\r\n\t}\r\n  close(done)\r\n}()\r\nselect {\r\ncase <-time.After(timeout):\r\n  \/\/ timed out\r\ncase <-done:\r\n  \/\/ Wait returned\r\n```\r\n\r\noption2: do sth in the test code\r\n\r\nThis part is where it confused me.\r\nThe current code is fine as we closed schdulerQueue after called `NextPod`, and after [#122627](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122627) I think there is no test case using this this function anymore(seeems that you refactored the test), so are we trying to solve a potential problem?","> after https:\/\/github.com\/kubernetes\/kubernetes\/pull\/122627 I think there is no test case using this this function anymore\r\n\r\nOh, then we can just remove NextPod in the PR."],"labels":["kind\/cleanup","sig\/scheduling","needs-triage"]},{"title":"Failure cluster [4aa4462c...]  ci-kubernetes-e2e-gci-gce-autoscaling totally broken","body":"### Failure cluster [4aa4462cf0539fca9c76](https:\/\/go.k8s.io\/triage#4aa4462cf0539fca9c76)\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-autoscaling-cluster-autoscaler#gci-gce-autoscaling&width=20\r\n\r\n##### Error text:\r\n```\r\nerror during .\/hack\/ginkgo-e2e.sh --ginkgo.focus=\\[Feature:ClusterSizeAutoscalingScaleUp\\]|\\[Feature:ClusterSizeAutoscalingScaleDown\\]|\\[Feature:InitialResources\\] --ginkgo.skip=\\[Flaky\\] --minStartupPods=8 --report-dir=\/logs\/artifacts --disable-log-dump=true --cluster-ip-range=10.64.0.0\/14: exit status 1\r\n```\r\n#### Recent failures:\r\n[2\/23\/2024, 5:05:21 PM ci-kubernetes-e2e-gci-gce-autoscaling](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761150274350616576)\r\n[2\/23\/2024, 4:33:21 PM ci-kubernetes-e2e-gci-gce-autoscaling](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761142220913643520)\r\n[2\/23\/2024, 11:17:59 AM ci-kubernetes-e2e-gci-gce-autoscaling](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761062858868658176)\r\n[2\/23\/2024, 10:46:59 AM ci-kubernetes-e2e-gci-gce-autoscaling](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761055056947318784)\r\n[2\/23\/2024, 10:15:59 AM ci-kubernetes-e2e-gci-gce-autoscaling](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761047255969697792)\r\n\r\n\r\n\/kind failing-test\r\n<!-- If this is a flake, please add: \/kind flake -->\r\n\r\n<!-- Please assign a SIG using: \/sig SIG-NAME -->","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig autoscaling","After https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123490 and https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123488 are merged,\r\n\r\nRecent two run failed for `There were additional failures detected after the initial failure. These are visible in the timeline`.\r\n```\r\n\r\nKubernetes e2e suite: [It] [sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb [Feature:ClusterSizeAutoscalingScaleDown]\u00a0\r\nfailed [FAILED] timeout waiting 20m0s for appropriate cluster size In [It] at: \r\nk8s.io\/kubernetes\/test\/e2e\/autoscaling\/cluster_size_autoscaling.go:740 @ 02\/26\/24 05:47:47.972  \r\nThere were additional failures detected after the initial failure. These are visible in the timeline }\r\n\r\n```\r\n\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-e2e-gci-gce-autoscaling\/1761980250801049600\r\n\r\n```\r\n\r\nKubernetes e2e suite: [It] [sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]\u00a0 \r\n{ failed [FAILED] only 0 pods started out of 1 In [It] at:\r\n k8s.io\/kubernetes\/test\/e2e\/autoscaling\/cluster_size_autoscaling.go:536 @ 02\/26\/24 08:03:39.004 }\r\n\r\n```\r\n\r\n\r\nCurrent failing test list\r\n\r\n1. Kubernetes e2e suite.[It] [sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb [Feature:ClusterSizeAutoscalingScaleDown]\r\n2. Kubernetes e2e suite.[It] [sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining system pods with pdb [Feature:ClusterSizeAutoscalingScaleDown]\r\n3. Kubernetes e2e suite.[It] [sig-autoscaling] Cluster size autoscaling [Slow] shouldn't be able to scale down when rescheduling a pod is required, but pdb doesn't allow drain [Feature:ClusterSizeAutoscalingScaleDown]\r\n4. Kubernetes e2e suite.[It] [sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down when rescheduling a pod is required and pdb allows for it [Feature:ClusterSizeAutoscalingScaleDown]\r\n5. Kubernetes e2e suite.[It] [sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pod requesting volume is pending [Feature:ClusterSizeAutoscalingScaleUp]"],"labels":["sig\/autoscaling","kind\/failing-test","needs-triage"]},{"title":"Automated cherry pick of #123003: bugfix: dont skip reconcile for unchanged policy if last sync","body":"Cherry pick of #123003 on release-1.28.\n\n#123003: bugfix: dont skip reconcile for unchanged policy if last sync\n\nFor details on the cherry pick process, see the [cherry pick requests](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) page.\n\n```release-note\n\n```","comments":["This cherry pick PR is for a release branch and has not yet been approved by [Release Managers](https:\/\/k8s.io\/releases\/release-managers).\nAdding the `do-not-merge\/cherry-pick-not-approved` label.\n\nTo merge this cherry pick, it must first be approved (`\/lgtm` + `\/approve`) by the relevant OWNERS.\n\nIf you **didn't cherry-pick** this change to [**all supported release branches**](https:\/\/k8s.io\/releases\/patch-releases), please leave a comment describing why other cherry-picks are not needed to speed up the review process.\n\nIf you're not sure is it required to cherry-pick this change to all supported release branches, please consult the [cherry-pick guidelines](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/cherry-picks.md) document.\n\n**AFTER** it has been approved by code owners, please leave the following comment on a line **by itself, with no leading whitespace**: **\/cc kubernetes\/release-managers**\n\n(This command will request a cherry pick review from [Release Managers](https:\/\/github.com\/orgs\/kubernetes\/teams\/release-managers) and should work for all GitHub users, whether they are members of the Kubernetes GitHub organization or not.)\n\nFor details on the patch release process and schedule, see the [Patch Releases](https:\/\/k8s.io\/releases\/patch-releases) page.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123477#\" title=\"Author self-approved\">ritazh<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https:\/\/github.com\/jpbetz) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/apiserver\/pkg\/admission\/plugin\/validatingadmissionpolicy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/staging\/src\/k8s.io\/apiserver\/pkg\/admission\/plugin\/validatingadmissionpolicy\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/release-1.28\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"jpbetz\"]} -->","@alexzielenski @kubernetes\/release-managers  PTAL","\/lgtm\r\n\/approve","LGTM label has been added.  <details>Git tree hash: fb2e7e888a9bfa05de0b649b9861fd3c91ebe2d1<\/details>","\/lgtm\r\n\/triage accepted\r\n@kubernetes\/release-managers For cherrypick\r\n"],"labels":["area\/test","area\/apiserver","lgtm","sig\/api-machinery","size\/L","cncf-cla: yes","sig\/testing","do-not-merge\/cherry-pick-not-approved","needs-priority","triage\/accepted","do-not-merge\/needs-kind"]},{"title":"DRA: kubelet dies with \"concurrent map iteration and map write\"","body":"### What happened?\n\nI was running `_output\/bin\/ginkgo -p -focus=\"DynamicResourceAllocation\" .\/test\/e2e` with a local-up-cluster.sh cluster when kubelet died.\r\n\r\nHere's the output:\r\n```\r\nI0223 18:04:20.818486  387925 round_trippers.go:553] GET https:\/\/localhost:6444\/apis\/resource.k8s.io\/v1alpha2\/namespaces\/dra-2883\/resourceclaims\/external-claim 200 OK in 4 milliseconds\r\nI0223 18:04:20.819006  387925 round_trippers.go:553] GET https:\/\/localhost:6444\/apis\/resource.k8s.io\/v1alpha2\/namespaces\/dra-2883\/resourceclaims\/external-claim 200 OK in 6 milliseconds\r\nI0223 18:04:20.819223  387925 round_trippers.go:553] GET https:\/\/localhost:6444\/apis\/resource.k8s.io\/v1alpha2\/namespaces\/dra-2883\/resourceclaims\/external-claim 200 OK in 7 milliseconds\r\nI0223 18:04:20.819258  387925 round_trippers.go:553] GET https:\/\/localhost:6444\/apis\/resource.k8s.io\/v1alpha2\/namespaces\/dra-2922\/resourceclaims\/tester-1-my-inline-claim-m4q4f 200 OK in 7 milliseconds\r\nfatal error: concurrent map iteration and map write\r\n\r\ngoroutine 51298 [running]:\r\nreflect.mapiternext(0x4f85c9?)\r\n        runtime\/map.go:1392 +0x13\r\nreflect.(*MapIter).Next(0xc008e711c0?)\r\n        reflect\/value.go:2005 +0x74\r\nencoding\/json.mapEncoder.encode({0x7f6120609460?}, 0xc00c6a0840, {0x3d33520?, 0xc001f11f60?, 0x33bb960?}, {0xa?, 0x0?})\r\n        encoding\/json\/encode.go:745 +0x334\r\nencoding\/json.structEncoder.encode({{{0xc000998908, 0x8, 0x8}, 0xc000db00f0, 0xc000db0120}}, 0xc00c6a0840, {0x3c4d040?, 0xc001f11f10?, 0x0?}, {0x0, ...})\r\n        encoding\/json\/encode.go:704 +0x21e\r\nencoding\/json.arrayEncoder.encode({0x7f6172cb4108?}, 0xc00c6a0840, {0x34474e0?, 0xc0067ff150?, 0x0?}, {0x0?, 0x0?})\r\n        encoding\/json\/encode.go:847 +0xcf\r\nencoding\/json.sliceEncoder.encode({0x50ac65?}, 0xc00c6a0840, {0x34474e0?, 0xc0067ff150?, 0x33bb960?}, {0xa?, 0x0?})\r\n        encoding\/json\/encode.go:820 +0x347\r\nencoding\/json.structEncoder.encode({{{0xc00066f448, 0x3, 0x4}, 0xc000db01b0, 0xc000db01e0}}, 0xc00c6a0840, {0x393ec60?, 0xc0067ff140?, 0xc00a0091d0?}, {0x0, ...})\r\n        encoding\/json\/encode.go:704 +0x21e\r\nencoding\/json.(*encodeState).reflectValue(0xc00c6a0840, {0x393ec60?, 0xc0067ff140?, 0xc008e71830?}, {0xe0?, 0x9d?})\r\n        encoding\/json\/encode.go:321 +0x73\r\nencoding\/json.(*encodeState).marshal(0x41535b?, {0x393ec60?, 0xc0067ff140?}, {0xd0?, 0x91?})\r\n        encoding\/json\/encode.go:297 +0xc5\r\nencoding\/json.Marshal({0x393ec60, 0xc0067ff140})\r\n        encoding\/json\/encode.go:163 +0xd0\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state.(*DRAManagerCheckpoint).MarshalCheckpoint(0xc0067ff050)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state\/checkpoint.go:69 +0x4e\r\nk8s.io\/kubernetes\/pkg\/kubelet\/checkpointmanager.(*impl).CreateCheckpoint(0xc000ab3ad0, {0x3e85742, 0x11}, {0x4433e40?, 0xc0067ff050?})\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/checkpointmanager\/checkpoint_manager.go:69 +0xc9\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state.(*stateCheckpoint).store(0xc000a17180, {0xc001f11808, 0x11, 0x11})\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state\/state_checkpoint.go:147 +0xb7\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state.(*stateCheckpoint).Store(0xc000a17180, {0xc001f11808, 0x11, 0x11})\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/state\/state_checkpoint.go:139 +0x7e\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra.(*claimInfoCache).syncToCheckpoint(0xc000db02d0)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/claiminfo.go:222 +0x268\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra.(*ManagerImpl).PrepareResources(0xc000c030c8, 0xc004e0e008)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/dra\/manager.go:214 +0xbe5\r\nk8s.io\/kubernetes\/pkg\/kubelet\/cm.(*containerManagerImpl).PrepareDynamicResources(0x0?, 0x1?)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/cm\/container_manager_linux.go:1017 +0x22\r\nk8s.io\/kubernetes\/pkg\/kubelet.(*Kubelet).PrepareDynamicResources(0xc0006283c0?, 0x3e9bf34?)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/kubelet.go:3039 +0x25\r\nk8s.io\/kubernetes\/pkg\/kubelet\/kuberuntime.(*kubeGenericRuntimeManager).SyncPod(0xc000a46300, {0x4436f20, 0xc00c01edf0}, 0xc004e0e008, 0xc00b914090, {0x62e0fa0, 0x0, 0x0}, 0xc000287db0)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/kuberuntime\/kuberuntime_manager.go:1145 +0x1762\r\nk8s.io\/kubernetes\/pkg\/kubelet.(*Kubelet).SyncPod(0xc00096d808, {0x4436ee8, 0xc0079a19f0}, 0x2, 0xc004e0e008, 0x0, 0xc00b914090)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/kubelet.go:1955 +0x2a83\r\nk8s.io\/kubernetes\/pkg\/kubelet.(*podWorkers).podWorkerLoop.func1({0x0, {0x2, {0xc16e51e5189c05b9, 0x1b1d167c2fb, 0x627e340}, 0xc004e0e008, 0x0, 0x0, 0x0}}, 0xc000a01900, ...)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/pod_workers.go:1283 +0x1ca\r\nk8s.io\/kubernetes\/pkg\/kubelet.(*podWorkers).podWorkerLoop(0xc000a01900, {0xc002d5b6e0, 0x24}, 0xc002518de0)\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/pod_workers.go:1288 +0x49b\r\nk8s.io\/kubernetes\/pkg\/kubelet.(*podWorkers).UpdatePod.func1()\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/pod_workers.go:950 +0x118\r\ncreated by k8s.io\/kubernetes\/pkg\/kubelet.(*podWorkers).UpdatePod in goroutine 351\r\n        k8s.io\/kubernetes\/pkg\/kubelet\/pod_workers.go:945 +0x20db\r\n\r\n...\r\n```\r\n\r\n\/assign @bart0sh \r\n\/sig node\r\ncc @klueska \n\n### What did you expect to happen?\n\nkubelet should have kept running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun DRA E2E tests. It might be necessary to use my `dra-structured-parameters` branch, it has some additional E2E tests, some of which are stressing kubelet more than before.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nMaster + local changes.\n\n### Cloud provider\n\nn.a.\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/assign @moshe010 ","\/triage accepted","@moshe010: do you have time to work on this?\n","@pohly  I am sorry I didn't even notice that issue is assign to me. \r\nI will join today CDI call today to understand more about the issue for you. \r\n","so it seem the problem is in the cache.state.Store(claimInfoStateList). \r\nSo I think the issue is that we guard the syncToCheckpoint with RLock. In case we we have call of GetOrCreate() [2] while we have call to cache.state.Store(..) it can be a problem first try to get and another try to write. \r\nI think we need to change lines [2] to be just  cache.Lock() and defer cache.Unlock()\r\n\r\n@bart0sh maybe you can check it if it solves the issue\r\n\r\n[1] - https:\/\/github.com\/kubernetes\/kubernetes\/blob\/b3926d137cd2964cd3a04088ded30845910547b1\/pkg\/kubelet\/cm\/dra\/claiminfo.go#L143\r\n[2] - https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/dra\/claiminfo.go#L214-L215\r\n\r\n","@moshe010 Would it be better to use `cache.Lock()` where we change the cache?","Do you have any other places in mind except syncToCheckpoint? \r\n\r\nWe currently do RLock on hasPodReference, get and Lock on add and delete.","@moshe010 \r\n> @bart0sh maybe you can check it if it solves the issue\r\n\r\nI wish I could. I run e2e tests multiple times as @pohly suggested, but never saw the traceback.","@pohly do you happen to have a full traceback? It should show conflicting code flow I guess. It would help a lot to investigate the issue.","> Do you have any other places in mind except syncToCheckpoint?\r\n\r\nNo, I don't, but I also could be wrong with my suggestion. Does https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/dra\/claiminfo.go#L222 changes cache object? If it does, then your suggestion makes sense.","> > Do you have any other places in mind except syncToCheckpoint?\r\n> \r\n> No, I don't, but I also could be wrong with my suggestion. Does https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/dra\/claiminfo.go#L222 changes cache object? If it does, then your suggestion makes sense.\r\n\r\nAfter reviewing again the code I am not sure the cache object changes so I am not sure about my comment above. ","\/priority important-longterm","\/cc @harche ","I tried to reproduce it, but so far without success.\r\n"],"labels":["kind\/bug","sig\/node","priority\/important-longterm","triage\/accepted"]},{"title":"Implement per-request watch cache opt-out","body":"#### What type of PR is this?\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123187\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nAdd `skipCache` parameter to LIST request that allows users to bypass server cache and debug staleness or corruption. Not intended for regular client usage.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nPlease use the following format for linking documentation:\r\n- https:\/\/github.com\/kubernetes\/enhancements\/issues\/2340\r\n\r\n\/sig api-machinery\r\n\/cc @deads2k @wojtek-t \r\n","comments":["This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123472#\" title=\"Author self-approved\">serathius<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [deads2k](https:\/\/github.com\/deads2k). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiextensions-apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/OWNERS)**\n- **[staging\/src\/k8s.io\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/OWNERS)**\n- **[staging\/src\/k8s.io\/code-generator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/code-generator\/OWNERS)**\n- **[staging\/src\/k8s.io\/kube-aggregator\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kube-aggregator\/OWNERS)**\n- **[staging\/src\/k8s.io\/sample-apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/sample-apiserver\/OWNERS)**\n- **[test\/integration\/apiserver\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/integration\/apiserver\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->","Split PR in two commits, first makes changes, second includes only auto-generated code. ","I'm torn.\r\n\r\nOn one hand, I see the utility of this. Particularly for the introduction of Consistent Reads From Cache based KEPs.\r\n\r\nOn the other hand, the potential for misuse (accidental mostly, but the ability for anyone with API access to more easily DOS the system is also possible?) worries me.  I'd feel a lot better if we could mitigate that in some way. Just brainstorming some possibilities:\r\n\r\n1. Add synthetic latency to all requests using this.\r\n2. Do something in priority and fairness to manage these types of requests (@MikeSpreitzer is this an option?)\r\n3. Require a special verb in RBAC to allow use of this flag (e.g. \"debug\" or \"get-debug\" or something) @liggitt how bad is this?\r\n4. Require cluster operators set an apiserver flag to use debug capabilities (doesn't seem particularly well suited to cloud provider managed clusters?)\r\n5. Exclude this from get\/list options in client-go?  Or require some special way to set it?\r\n6. Only enable this when one of the new Consistent Reads From Cache based enhancements is enabled?","@serathius: The following tests **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-verify-lint | e17e5766c626c46ba98813aa5916ec20dba90275 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123472\/pull-kubernetes-verify-lint\/1761069561823629312) | true | `\/test pull-kubernetes-verify-lint`\npull-kubernetes-unit | e17e5766c626c46ba98813aa5916ec20dba90275 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123472\/pull-kubernetes-unit\/1761069557029539840) | true | `\/test pull-kubernetes-unit`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123472). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Aserathius). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","7. Create a new endpoint that is only accessible privately\/through debug socket?","> 1. Add synthetic latency to all requests using this.\r\n\r\n> 2. Do something in priority and fairness to manage these types of requests (@MikeSpreitzer is this an option?)\r\n\r\nThis is already handled by P&F as we are just increasing types of requests served from cache. Watch cache can also be disabled so we will maintain proper P&F cost estimations. Unfortunately I expect that when all requests are migrated to watch cache, the non-cache P&F estimations might degrade in accuracy, but that's a separate problem to address.\r\n\r\n> 3. Require a special verb in RBAC to allow use of this flag (e.g. \"debug\" or \"get-debug\" or something) @liggitt how bad is this?\r\n\r\nI originally proposed to limit it to `cluster-admin` role. However, don't think we will be able to implement it. Reached out to sig-security https:\/\/kubernetes.slack.com\/archives\/C019LFTGNQ3\/p1708691673173579 \r\ncc @Smarticu5 @micahhausler @dims for ideas\r\n\r\n> 4. Require cluster operators set an apiserver flag to use debug capabilities (doesn't seem particularly well suited to cloud provider managed clusters?)\r\n\r\nThis would not work, as the main reason for this request type is to debug the current in memory state of watch cache, for staleness or corruption. Such apiserver flag will never be turned on by default, and enabling it will destroy the evidence.\r\n\r\n> 5. Exclude this from get\/list options in client-go?  Or require some special way to set it?\r\n\r\nI was originally proposing using HTTP headers to make it not visible in API. cc @ligging @deads2k \r\n\r\n\r\n","> Require a special verb in RBAC to allow use of this flag (e.g. \"debug\" or \"get-debug\" or something) @liggitt how bad is this?\r\n\r\nHow about a special APF FlowSchema handling? You can do these, but you need a matching FlowSchema, and by default there isn't one (so your request isn't allowed, unless you're the control plane as `system:masters`).\r\n\r\nYou might make this to fix that:\r\n```yaml\r\n---\r\n# this is a fictional example\r\napiVersion: flowcontrol.apiserver.k8s.io\/v1beta1\r\nkind: FlowSchema\r\nmetadata:\r\n name: debug-skip-cache\r\nspec:\r\n distinguisherMethod:\r\n   type: ByGroup\r\n matchingPrecedence: 1000\r\n priorityLevelConfiguration:\r\n   name: sre-team\r\n rules:\r\n   - resourceRules:\r\n       - apiGroups:\r\n           - '*'\r\n         clusterScope: true\r\n         namespaces:\r\n           - '*'\r\n         resources:\r\n           - '*'\r\n         verbs:\r\n           - 'list'\r\n         # novel field\r\n         queryParameters:\r\n           skipCache:\r\n             - \"true\"\r\n     subjects:\r\n       - group:\r\n           name: on-call-troubleshooting\r\n         kind: Group\r\n```","> This is already handled by P&F as we are just increasing types of requests served from cache. Watch cache can also be disabled so we will maintain proper P&F cost estimations. Unfortunately I expect that when all requests are migrated to watch cache, the non-cache P&F estimations might degrade in accuracy, but that's a separate problem to address.\r\n\r\nOK, this makes me feel a bit better.\r\n\r\nFor alpha, if we feature gate this either with it's own gate or only enable it when at least one of the consistent read from cache features is enabled, I'd be satisfied for alpha.","I echo with some of the concerns @jpbetz pointed around potential misuse of such a backdoor knob in the API.\r\n\r\nLet me try summarizing my thoughts here:\r\n- There are two aspects of this read staleness issue - detection (knowing that the cache is stale) and debugging (knowing how stale\/what entries of the cache are stale)\r\n- The API server metric approach proposed [here](https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/2340-Consistent-reads-from-cache\/README.md#what-if-the-watch-cache-is-stale) is a good way for detection (both for clients and cluster operator). In fact we could go a bit further and tell the client as part of the API response how stale the server cache was - by providing the time (or RV) it last received an update\/bookmark event from etcd. This approach can work even better for watches where \"staleness\" can be tracked purely client-side (when bookmarks are enabled) based on \"how long it's been since it last received any event\"\r\n- Now regarding the debugging mechanism - who do we intend its audience to be? The cluster operator\/provider or any general API client?\r\n  - If it's the latter, we should assume sooner or later it will become a \"feature\" clients will come to rely upon - potentially in ways that could make the cluster less reliable (an example of such a naive bad client - https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072). And now to mitigate the added risk with this new mechanism, we are forced to add even more complexity to our system (like the discussion above on APF scoring for such calls)\r\n  - If it's the former, we have more robust ways to avoid exposing this altogether to API clients. I like @apelisse idea above regarding a separate debug endpoint which the cluster operator\/provider could use in a sandboxed way (for e.g not expose that HTTP handler under client-facing load balancers). I'd also like for us to think a step further and see if we can avoid building the debugging mechanism into the API server altogether. For e.g, what if we could use an offline tool that lists objects at given RV from etcd and returns a k8s-style response. We could use that to compare against the response from API server consistent read at the same RV\r\n\r\nIn general, making clients smarter\/more responsible for debugging server-side cache staleness issues is an anti-pattern imho. I know we're not perfectly there with our API semantics today, but this change could get us deeper into that marsh.","@jpbetz @shyamjvs - I'm actually also on the fence with it:\r\n1) on one hand I actually agree that in case of issues, we need some mechanism to be able to debug what is happening or as an operator be able to list contents\r\n2) otoh, I definitely share the same concerns that you have - this is not a field that (a) any application (b) any non-operator persona should ever use\r\n\r\nSo, we need a mechanism, but we also want to hide it from the users.\r\nThat's why I liked what was proposed in the KEP of limiting the ability to set it to \"cluster-admin\" role. I've just skimmed through the linked slack conversation but TBH didn't fully understand where the problem is there...\r\n\r\nGoing to the options suggested above:\r\n(a) introducing a dedicated verb sounds ugly to me, but I would say it's roughly equivalent so if API approvers are ok, that would work for me\r\n(b) this option definitely should NOT be propagated to any client libraries - it should be hard to use it\r\n(c) the dedicated new endpoint sounds interesting, but the question is how to expose it  - I thin that many people don't enable debug handler which would defeat its purpose. I'm also a bit worried about additional complexity of it...","I just chatted with Marek about this and the other option is:\r\n\r\n8) Expose this option only as an internal API - we have a precedent for something like this here:\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\/types.go#L1473-L1475\r\nWe may additionally define an http header that would be activating this field.\r\n\r\n@jpbetz @shyamjvs - thoughts? ^^\r\n\r\nThe more I think about this option the more I like it actually :)\r\nBut having some discussions with David before, I'm afraid he will actually not like it...\r\n","> I was originally proposing using HTTP headers to make it not visible in API. cc @ligging @deads2k\r\n\r\n> But having some discussions with David before, I'm afraid he will actually not like it...\r\n\r\nWhile an API object is a very obvious (this is part of our API), anything that our kube-apiserver accepts header-wise becomes part of our API whether we expose it in client-go or not. So adding a header without the corresponding Options change makes it harder for us to see, but doesn't reduce our API surface area in a meaningful way.\r\n\r\nIdeas like restricting who could set this option could prove difficult to express, but should be doable.",">  limiting the ability to set it to \"cluster-admin\" role\r\n\r\nWe can't assume that a cluster has RBAC authz active.","> So adding a header without the corresponding Options change makes it harder for us to see, but doesn't reduce our API surface area in a meaningful way.\r\n\r\n+1. To really make it opaque to \"external\" clients we'd need to disambiguate them from the \"cluster operator\" client imo. An RBAC role (like cluster admin) probably comes closest to achieving this but still isn't an enforcement.\r\n\r\nBut before trying to think of ways to restrict the new API, are we sure we even want to build one in the first place? Going back to:\r\n\r\n> For e.g, what if we could use an offline tool that lists objects at given RV from etcd and returns a k8s-style response. We could use that to compare against the response from API server consistent read at the same RV\r\n\r\nSince we expect to perform this debug operation mainly when things go south, what if we just built (or enhanced a tool like [auger](https:\/\/github.com\/jpbetz\/auger)) to serve that purpose for now? We can go back to the API option in future if we still feel the need.","> anything that our kube-apiserver accepts header-wise becomes part of our API whether we expose it in client-go or not. So adding a header without the corresponding Options change makes it harder for us to see, but doesn't reduce our API surface area in a meaningful way\r\n\r\n@deads2k - while I agree with the first part [it becomes part of our API], I don't agree with the second part.\r\nWe can definitely make it visible for us [let's add a metric showing if someone is using it].\r\nAnd making it hard to use is actually the goal here - ideally I don't want anyone to ever use that so the harder it is, the better.","When I wrote \"harder for us to see\", I was referring to future-us tracking the flow of information through our apiserver.  And future-us (or someone else) remembering the full surface area of our API accurately.\r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/pull\/123532 provides an interesting congruent case.  The API surface area of this client logically includes a magic key in a context.  Whether all paths will remember it, whether all cases properly handle it, whether calls on which it has no effect add it, are all things we're unable to determine.  Reading a header value outside of our options consumption will produce similar parallel paths the in the kube-apiserver that I think we should avoid.  ","> reject any setting at all by normal users?\r\n\r\nGetting down to specifics for how to do this.  We could \r\n\r\n1. expand `request.RequestInfo` to track `ParameterOptions runtime.Object` that could be the appropriate options struct or nil\r\n2. `filters.GetAuthorizerAttributes` can be modified so that if the current verb is `list` and the `requestInfo.ParameterOptions` has the `DebugWatchCacheDirective` set the returned `attribs` will have the verb set to `debuglist` or `debugwatch` depending on which is accurate.\r\n\r\nThis would allow \r\n1. users with List powers cannot use the new debug option\r\n2. users with * on verbs can use the debug option (this covers all types of \"super\" users)\r\n3. user without List but with debuglist powers **will** be able to see all content.\r\n4. audit logs will show which kind of list is being used.\r\n5. audit log PolicyRules can differentiate between list and debuglist to more deeply audit debuglist and debugwatch\r\n\r\nThoughts?","> 2\\. `filters.GetAuthorizerAttributes` can be modified so that if the current verb is `list` and the `requestInfo.ParameterOptions` has the `DebugWatchCacheDirective` set the returned `attribs` will have the verb set to `debuglist` or `debugwatch` depending on which is accurate.\r\n\r\nGetAuthorizerAttributes returning a different verb than requestInfo.Verb, and having different parts of the stack see the same request as using different verbs is *very* confusing to me.\r\n\r\n> 1. users with List powers cannot use the new debug option\r\n\r\nThis is good\r\n\r\n> 2. users with * on verbs can use the debug option (this covers all types of \"super\" users)\r\n\r\nThis is good\r\n\r\n> 3. user without List but with debuglist powers will be able to see all content.\r\n\r\nI think this is bad. I'd expect `list` AND `debuglist` permission to be required to be able to do this operation.\r\n\r\n> 4. audit logs will show which kind of list is being used.\r\n\r\nI agree the fact that debug list was used should show up in audit logs, but I'd be ok with that being in the RequestURI (already present in audit at metadata level) or an audit annotation.\r\n\r\n> 5. audit log PolicyRules can differentiate between list and debuglist to more deeply audit debuglist and debugwatch\r\n\r\nI'm not sure why we'd need this if the only people who can do it have to pass an additional authorization check.","I just realized DeleteCollection parses parameters into ListOptions and uses those to list the objects it is going to delete. Is this option expected to apply there as well?",">   user without List but with debuglist powers will be able to see all content.\r\n>I think this is bad. I'd expect list AND debuglist permission to be required to be able to do this operation.\r\n\r\nI don't see it as particularly bad, but it doesn't bother me to adjust.  A starting point for those less familiar would be here: https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/apiserver\/pkg\/endpoints\/filters\/authorization.go#L61-L66 where the authorization filter could conditionally execute a second .Authorize check based on its copy of the Authorizer attributes.\r\n\r\n> I'm not sure why we'd need this if the only people who can do it have to pass an additional authorization check.\r\n\r\n\"Don't log anything for normal requests, but definitely log metadata for those doing debuglist\" since it could lead to apiserver problems (see the recent \"bypass watch cache\" bug) so we can clearly indicate blame.  I don't have strong feelings, but it certainly seemed nice to offer.","> I just realized DeleteCollection parses parameters into ListOptions and uses those to list the objects it is going to delete. Is this option expected to apply there as well?\r\n\r\nI wouldn't have caught it early, but I would expect similar logic to apply since the same cost is incurred by the storage doing the List.","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/triage accepted"],"labels":["area\/test","area\/apiserver","sig\/api-machinery","release-note","needs-rebase","size\/XXL","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/testing","area\/code-generation","needs-priority","triage\/accepted"]},{"title":"Flaky UT TestTerminationOrderingSidecarsInReverseOrder of pkg\/kubelet: kuberuntime ","body":"### Which jobs are flaking?\n\nhttps:\/\/prow.ppc64le-cloud.cis.ibm.net\/job-history\/gs\/ppc64le-kubernetes\/logs\/periodic-kubernetes-unit-test-ppc64le\n\n### Which tests are flaking?\n\n`TestTerminationOrderingSidecarsInReverseOrder` of `pkg\/kubelet\/kuberuntime\/kuberuntime_termination_order_test.go`\n\n### Since when has it been flaking?\n\nHave observed this recently.\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/ibm-unit-tests-ppc64le#Periodic%20unit%20test%20suite%20on%20ppc64le\n\n### Reason for failure (if possible)\n\n```\r\n{Failed;  === RUN   TestTerminationOrderingSidecarsInReverseOrder\r\n    kuberuntime_termination_order_test.go:179: expected delay for container sc3 = 1, got 0\r\n--- FAIL: TestTerminationOrderingSidecarsInReverseOrder (4.00s)\r\n;}\r\n```\n\n### Anything else we need to know?\n\nReproducible on a local x86_64 machine:\r\n```\r\n[root@maxwells1 kubernetes]# \/root\/go\/bin\/stress .\/kuberuntime.test -test.run TestTerminationOrderingSidecarsInReverseOrder\r\n5s: 0 runs so far, 0 failures\r\n10s: 4 runs so far, 0 failures\r\n---\r\n---\r\n3m50s: 172 runs so far, 0 failures\r\n3m55s: 176 runs so far, 0 failures\r\n4m0s: 180 runs so far, 0 failures\r\n\r\n\/tmp\/go-stress-20240223T025228-4285150077\r\n--- FAIL: TestTerminationOrderingSidecarsInReverseOrder (4.00s)\r\n    kuberuntime_termination_order_test.go:179: expected delay for container sc2 = 2, got 1\r\n    kuberuntime_termination_order_test.go:179: expected delay for container sc3 = 1, got 0\r\nFAIL\r\n\r\n\r\nERROR: exit status 1\r\n\r\n4m5s: 184 runs so far, 1 failures (0.54%)\r\n4m10s: 188 runs so far, 1 failures (0.53%)\r\n4m15s: 192 runs so far, 1 failures (0.52%)\r\n```\n\n### Relevant SIG(s)\n\n\/sig node","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @tzneal "],"labels":["sig\/node","kind\/flake","needs-triage"]},{"title":"node: memory manager: fix the metrics tests","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\/kind failing-test\r\n\r\n#### What this PR does \/ why we need it:\r\nImprove the reliability of the tests by saving and recovering the correct kubelet configuration\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #123454 \r\n\r\n#### Special notes for your reviewer:\r\nThis should improve the test signal but not sure it's all we need to fix the tests\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n","comments":["\/test pull-crio-cgroupv1-node-e2e-resource-managers\r\n\/test pull-kubernetes-e2e-capz-windows-serial-slow\r\n\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd\r\n\/test pull-kubernetes-node-kubelet-serial-cpu-manager\r\n\/test pull-kubernetes-node-kubelet-serial-memory-manager\r\n\/test pull-kubernetes-node-kubelet-serial-topology-manager","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123468#\" title=\"Author self-approved\">ffromani<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [sergeykanzhelev](https:\/\/github.com\/sergeykanzhelev) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[test\/e2e_node\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/e2e_node\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"sergeykanzhelev\"]} -->","\/sig node","\/test","@ffromani: The `\/test` command needs one or more targets.\nThe following commands are available to trigger required jobs:\n* `\/test pull-cadvisor-e2e-kubernetes`\n* `\/test pull-cos-containerd-e2e-ubuntu-gce`\n* `\/test pull-kubernetes-conformance-kind-ga-only-parallel`\n* `\/test pull-kubernetes-coverage-unit`\n* `\/test pull-kubernetes-dependencies`\n* `\/test pull-kubernetes-dependencies-go-canary`\n* `\/test pull-kubernetes-e2e-gce`\n* `\/test pull-kubernetes-e2e-gce-100-performance`\n* `\/test pull-kubernetes-e2e-gce-big-performance`\n* `\/test pull-kubernetes-e2e-gce-cos`\n* `\/test pull-kubernetes-e2e-gce-cos-canary`\n* `\/test pull-kubernetes-e2e-gce-cos-no-stage`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-http-connect`\n* `\/test pull-kubernetes-e2e-gce-scale-performance-manual`\n* `\/test pull-kubernetes-e2e-kind`\n* `\/test pull-kubernetes-e2e-kind-ipv6`\n* `\/test pull-kubernetes-integration`\n* `\/test pull-kubernetes-integration-go-canary`\n* `\/test pull-kubernetes-kubemark-e2e-gce-scale`\n* `\/test pull-kubernetes-node-e2e-containerd`\n* `\/test pull-kubernetes-typecheck`\n* `\/test pull-kubernetes-unit`\n* `\/test pull-kubernetes-unit-go-canary`\n* `\/test pull-kubernetes-update`\n* `\/test pull-kubernetes-verify`\n* `\/test pull-kubernetes-verify-go-canary`\n* `\/test pull-kubernetes-verify-lint`\n\nThe following commands are available to trigger optional jobs:\n* `\/test check-dependency-stats`\n* `\/test pull-ci-kubernetes-unit-windows`\n* `\/test pull-crio-cgroupv1-node-e2e-eviction`\n* `\/test pull-crio-cgroupv1-node-e2e-features`\n* `\/test pull-crio-cgroupv1-node-e2e-hugepages`\n* `\/test pull-crio-cgroupv1-node-e2e-resource-managers`\n* `\/test pull-crio-cgroupv2-imagefs-e2e-diskpressure`\n* `\/test pull-crio-cgroupv2-imagefs-e2e-separatedisktest`\n* `\/test pull-crio-cgroupv2-node-e2e-eviction`\n* `\/test pull-e2e-gce-cloud-provider-disabled`\n* `\/test pull-kubernetes-conformance-image-test`\n* `\/test pull-kubernetes-conformance-kind-ga-only`\n* `\/test pull-kubernetes-conformance-kind-ipv6-parallel`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv1-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-eviction`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-features`\n* `\/test pull-kubernetes-cos-cgroupv2-containerd-node-e2e-serial`\n* `\/test pull-kubernetes-crio-node-memoryqos-cgrpv2`\n* `\/test pull-kubernetes-cross`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cm`\n* `\/test pull-kubernetes-e2e-autoscaling-hpa-cpu`\n* `\/test pull-kubernetes-e2e-capz-azure-disk`\n* `\/test pull-kubernetes-e2e-capz-azure-disk-vmss`\n* `\/test pull-kubernetes-e2e-capz-azure-file`\n* `\/test pull-kubernetes-e2e-capz-azure-file-vmss`\n* `\/test pull-kubernetes-e2e-capz-conformance`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-feature-vpa`\n* `\/test pull-kubernetes-e2e-capz-windows-alpha-features`\n* `\/test pull-kubernetes-e2e-capz-windows-master`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n* `\/test pull-kubernetes-e2e-capz-windows-serial-slow-hpa`\n* `\/test pull-kubernetes-e2e-containerd-gce`\n* `\/test pull-kubernetes-e2e-ec2`\n* `\/test pull-kubernetes-e2e-ec2-arm64`\n* `\/test pull-kubernetes-e2e-ec2-conformance`\n* `\/test pull-kubernetes-e2e-ec2-conformance-arm64`\n* `\/test pull-kubernetes-e2e-gce-canary`\n* `\/test pull-kubernetes-e2e-gce-correctness`\n* `\/test pull-kubernetes-e2e-gce-cos-alpha-features`\n* `\/test pull-kubernetes-e2e-gce-csi-serial`\n* `\/test pull-kubernetes-e2e-gce-device-plugin-gpu`\n* `\/test pull-kubernetes-e2e-gce-disruptive-canary`\n* `\/test pull-kubernetes-e2e-gce-kubelet-credential-provider`\n* `\/test pull-kubernetes-e2e-gce-network-proxy-grpc`\n* `\/test pull-kubernetes-e2e-gce-providerless`\n* `\/test pull-kubernetes-e2e-gce-serial`\n* `\/test pull-kubernetes-e2e-gce-serial-canary`\n* `\/test pull-kubernetes-e2e-gce-storage-disruptive`\n* `\/test pull-kubernetes-e2e-gce-storage-slow`\n* `\/test pull-kubernetes-e2e-gce-storage-snapshot`\n* `\/test pull-kubernetes-e2e-gci-gce-autoscaling`\n* `\/test pull-kubernetes-e2e-gci-gce-ingress`\n* `\/test pull-kubernetes-e2e-gci-gce-ipvs`\n* `\/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2`\n* `\/test pull-kubernetes-e2e-kind-alpha-beta-features`\n* `\/test pull-kubernetes-e2e-kind-alpha-features`\n* `\/test pull-kubernetes-e2e-kind-beta-features`\n* `\/test pull-kubernetes-e2e-kind-canary`\n* `\/test pull-kubernetes-e2e-kind-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-evented-pleg`\n* `\/test pull-kubernetes-e2e-kind-ipv6-canary`\n* `\/test pull-kubernetes-e2e-kind-ipvs-dual-canary`\n* `\/test pull-kubernetes-e2e-kind-kms`\n* `\/test pull-kubernetes-e2e-kind-multizone`\n* `\/test pull-kubernetes-e2e-kind-nftables`\n* `\/test pull-kubernetes-e2e-storage-kind-disruptive`\n* `\/test pull-kubernetes-e2e-ubuntu-gce-network-policies`\n* `\/test pull-kubernetes-kind-dra`\n* `\/test pull-kubernetes-kind-json-logging`\n* `\/test pull-kubernetes-kind-text-logging`\n* `\/test pull-kubernetes-kubemark-e2e-gce-big`\n* `\/test pull-kubernetes-linter-hints`\n* `\/test pull-kubernetes-local-e2e`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-arm64-ubuntu-serial-gce`\n* `\/test pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e`\n* `\/test pull-kubernetes-node-crio-cgrpv2-e2e-kubetest2`\n* `\/test pull-kubernetes-node-crio-cgrpv2-imagefs-e2e`\n* `\/test pull-kubernetes-node-crio-e2e`\n* `\/test pull-kubernetes-node-crio-e2e-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-1-7-dra`\n* `\/test pull-kubernetes-node-e2e-containerd-alpha-features`\n* `\/test pull-kubernetes-node-e2e-containerd-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-features`\n* `\/test pull-kubernetes-node-e2e-containerd-features-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-kubetest2`\n* `\/test pull-kubernetes-node-e2e-containerd-serial-ec2`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode`\n* `\/test pull-kubernetes-node-e2e-containerd-standalone-mode-all-alpha`\n* `\/test pull-kubernetes-node-e2e-crio-dra`\n* `\/test pull-kubernetes-node-kubelet-credential-provider`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-alpha-features`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-cpu-manager-kubetest2`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1`\n* `\/test pull-kubernetes-node-kubelet-serial-crio-cgroupv2`\n* `\/test pull-kubernetes-node-kubelet-serial-hugepages`\n* `\/test pull-kubernetes-node-kubelet-serial-memory-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-pod-disruption-conditions`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager`\n* `\/test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2`\n* `\/test pull-kubernetes-node-swap-fedora`\n* `\/test pull-kubernetes-node-swap-fedora-serial`\n* `\/test pull-kubernetes-node-swap-ubuntu-serial`\n* `\/test pull-kubernetes-unit-experimental`\n* `\/test pull-publishing-bot-validate`\n\nUse `\/test all` to run the following jobs that were automatically triggered:\n* `pull-kubernetes-conformance-kind-ga-only-parallel`\n* `pull-kubernetes-conformance-kind-ipv6-parallel`\n* `pull-kubernetes-dependencies`\n* `pull-kubernetes-e2e-ec2`\n* `pull-kubernetes-e2e-ec2-conformance`\n* `pull-kubernetes-e2e-gce`\n* `pull-kubernetes-e2e-gce-canary`\n* `pull-kubernetes-e2e-kind`\n* `pull-kubernetes-e2e-kind-ipv6`\n* `pull-kubernetes-integration`\n* `pull-kubernetes-linter-hints`\n* `pull-kubernetes-node-e2e-containerd`\n* `pull-kubernetes-typecheck`\n* `pull-kubernetes-unit`\n* `pull-kubernetes-verify`\n* `pull-kubernetes-verify-lint`\n\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123468#issuecomment-1961204276):\n\n>\/test\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/test pull-kubernetes-node-arm64-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-arm64-ubuntu-serial-gce","\/test pull-kubernetes-integration ","\/test pull-kubernetes-node-e2e-containerd-serial-ec2","\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-e2e-kind-ipv6","\/test pull-kubernetes-e2e-gce","\/test pull-kubernetes-node-kubelet-serial-containerd","latest failure is (surprisingly?) unrelated to the mm metrics test?","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd\r\n\/test pull-kubernetes-verify","So, we have:\r\n\r\n```\r\n> Enter [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - set up framework | framework.go:200 @ 03\/04\/24 21:04:01.234\r\nSTEP: Creating a kubernetes client - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:220 @ 03\/04\/24 21:04:01.234\r\nSTEP: Building a namespace api object, basename memorymanager-metrics - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:259 @ 03\/04\/24 21:04:01.234\r\nI0304 21:04:01.239148 17026 framework.go:275] Skipping waiting for service account\r\n< Exit [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - set up framework | framework.go:200 @ 03\/04\/24 21:04:01.239 (5ms)\r\n> Enter [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:33 @ 03\/04\/24 21:04:01.239\r\n< Exit [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:33 @ 03\/04\/24 21:04:01.239 (0s)\r\n> Enter [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:33 @ 03\/04\/24 21:04:01.239\r\n< Exit [BeforeEach] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:33 @ 03\/04\/24 21:04:01.239 (0s)\r\n> Enter [BeforeEach] when querying \/metrics - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:47 @ 03\/04\/24 21:04:01.239\r\nSTEP: Stopping the kubelet - k8s.io\/kubernetes\/test\/e2e_node\/util.go:219 @ 03\/04\/24 21:04:01.249\r\nI0304 21:04:01.296018 17026 util.go:368] Get running kubelet with systemctl:   UNIT                            LOAD   ACTIVE SUB     DESCRIPTION\r\n  kubelet-20240304T194858.service loaded active running \/tmp\/node-e2e-20240304T194858\/kubelet --kubeconfig \/tmp\/node-e2e-20240304T194858\/kubeconfig --root-dir \/var\/lib\/kubelet --v 4 --config-dir \/tmp\/node-e2e-20240304T194858\/kubelet.conf.d --hostname-override i-0ac0e520963f28681 --container-runtime-endpoint unix:\/\/\/run\/containerd\/containerd.sock --config \/tmp\/node-e2e-20240304T194858\/kubelet-config --kernel-memcg-notification=true --feature-gates=DisableKubeletCloudCredentialProviders=true --image-credential-provider-config=\/tmp\/node-e2e-20240304T194858\/credential-provider.yaml --image-credential-provider-bin-dir=\/tmp\/node-e2e-20240304T194858 --cluster-domain=cluster.local --cgroup-driver=systemd --runtime-cgroups=\/system.slice\/containerd.service --cgroup-driver=systemd\r\n\r\nLOAD   = Reflects whether the unit definition was properly loaded.\r\nACTIVE = The high-level unit activation state, i.e. generalization of SUB.\r\nSUB    = The low-level unit activation state, values depend on unit type.\r\n1 loaded units listed.\r\n, kubelet-20240304T194858\r\nW0304 21:04:01.402270   17026 util.go:500] Health check on \"http:\/\/127.0.0.1:10248\/healthz\" failed, error=Head \"http:\/\/127.0.0.1:10248\/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused\r\nSTEP: Starting the kubelet - k8s.io\/kubernetes\/test\/e2e_node\/util.go:235 @ 03\/04\/24 21:04:01.406\r\nW0304 21:04:01.509422   17026 util.go:500] Health check on \"http:\/\/127.0.0.1:10248\/healthz\" failed, error=Head \"http:\/\/127.0.0.1:10248\/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused\r\n< Exit [BeforeEach] when querying \/metrics - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:47 @ 03\/04\/24 21:04:06.511 (5.273s)\r\n> Enter [It] should report pinning failures when the memorymanager allocation is known to fail - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:102 @ 03\/04\/24 21:04:06.511\r\nSTEP: Creating the test pod which will be rejected for memory request which is too big - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:103 @ 03\/04\/24 21:04:06.511\r\nW0304 21:04:06.514565   17026 warnings.go:70] would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"memmngrcnt\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"memmngrcnt\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"memmngrcnt\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"memmngrcnt\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\r\nI0304 21:04:06.516344 17026 memory_manager_metrics_test.go:175] begin listing pods: 2 found\r\nI0304 21:04:06.516371 17026 memory_manager_metrics_test.go:177] kubelet-container-manager-5669\/guaranteed994e56e4-0ded-417b-9990-edd5e60b4fd6 node i-0ac0e520963f28681 status Succeeded QoS Guaranteed message  reason  (1 container statuses recorded)\r\nI0304 21:04:06.516379 17026 memory_manager_metrics_test.go:179] \tContainer guaranteed994e56e4-0ded-417b-9990-edd5e60b4fd6 ready: false, restart count 0\r\nI0304 21:04:06.516392 17026 memory_manager_metrics_test.go:177] memorymanager-metrics-7056\/memmngrpodqxfln node i-0ac0e520963f28681 status Pending QoS Guaranteed message  reason  (0 container statuses recorded)\r\nI0304 21:04:06.516401 17026 memory_manager_metrics_test.go:183] end listing pods: 2 found\r\nSTEP: Checking the memorymanager metrics right after the kubelet restart, with pod failed to admit - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:117 @ 03\/04\/24 21:04:06.516\r\nSTEP: Giving the Kubelet time to start up and produce metrics - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:128 @ 03\/04\/24 21:04:06.516\r\nSTEP: getting Kubelet metrics from the metrics API - k8s.io\/kubernetes\/test\/e2e_node\/cpu_manager_metrics_test.go:167 @ 03\/04\/24 21:04:06.516\r\nSTEP: getting Kubelet metrics from the metrics API - k8s.io\/kubernetes\/test\/e2e_node\/cpu_manager_metrics_test.go:167 @ 03\/04\/24 21:04:21.536\r\nSTEP: getting Kubelet metrics from the metrics API - k8s.io\/kubernetes\/test\/e2e_node\/cpu_manager_metrics_test.go:167 @ 03\/04\/24 21:04:36.545\r\nSTEP: getting Kubelet metrics from the metrics API - k8s.io\/kubernetes\/test\/e2e_node\/cpu_manager_metrics_test.go:167 @ 03\/04\/24 21:04:51.556\r\n[FAILED] Timed out after 60.000s.\r\nExpected\r\n    <string>: KubeletMetrics\r\nto match keys: {\r\n.\"kubelet_memory_manager_pinning_requests_total\"[]:\r\n\tExpected\r\n\t    <string>: Sample\r\n\tto match fields: {\r\n\t.Value:\r\n\t\tExpected\r\n\t\t    <model.SampleValue>: 2\r\n\t\tto be ==\r\n\t\t    <int>: 1\r\n\t}\r\n\t\r\n}\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:129 @ 03\/04\/24 21:05:06.516\r\n< Exit [It] should report pinning failures when the memorymanager allocation is known to fail - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:102 @ 03\/04\/24 21:05:06.516 (1m0.005s)\r\n> Enter [DeferCleanup (Each)] when querying \/metrics - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:72 @ 03\/04\/24 21:05:06.516\r\nSTEP: Stopping the kubelet - k8s.io\/kubernetes\/test\/e2e_node\/util.go:219 @ 03\/04\/24 21:05:06.523\r\nI0304 21:05:06.548492 17026 util.go:368] Get running kubelet with systemctl:   UNIT                            LOAD   ACTIVE SUB     DESCRIPTION\r\n  kubelet-20240304T194858.service loaded active running \/tmp\/node-e2e-20240304T194858\/kubelet --kubeconfig \/tmp\/node-e2e-20240304T194858\/kubeconfig --root-dir \/var\/lib\/kubelet --v 4 --config-dir \/tmp\/node-e2e-20240304T194858\/kubelet.conf.d --hostname-override i-0ac0e520963f28681 --container-runtime-endpoint unix:\/\/\/run\/containerd\/containerd.sock --config \/tmp\/node-e2e-20240304T194858\/kubelet-config --kernel-memcg-notification=true --feature-gates=DisableKubeletCloudCredentialProviders=true --image-credential-provider-config=\/tmp\/node-e2e-20240304T194858\/credential-provider.yaml --image-credential-provider-bin-dir=\/tmp\/node-e2e-20240304T194858 --cluster-domain=cluster.local --cgroup-driver=systemd --runtime-cgroups=\/system.slice\/containerd.service --cgroup-driver=systemd\r\n\r\nLOAD   = Reflects whether the unit definition was properly loaded.\r\nACTIVE = The high-level unit activation state, i.e. generalization of SUB.\r\nSUB    = The low-level unit activation state, values depend on unit type.\r\n1 loaded units listed.\r\n, kubelet-20240304T194858\r\nW0304 21:05:06.653396   17026 util.go:500] Health check on \"http:\/\/127.0.0.1:10248\/healthz\" failed, error=Head \"http:\/\/127.0.0.1:10248\/healthz\": read tcp 127.0.0.1:44000->127.0.0.1:10248: read: connection reset by peer\r\nSTEP: Starting the kubelet - k8s.io\/kubernetes\/test\/e2e_node\/util.go:235 @ 03\/04\/24 21:05:06.657\r\nW0304 21:05:06.739242   17026 util.go:500] Health check on \"http:\/\/127.0.0.1:10248\/healthz\" failed, error=Head \"http:\/\/127.0.0.1:10248\/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused\r\n< Exit [DeferCleanup (Each)] when querying \/metrics - k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:72 @ 03\/04\/24 21:05:11.745 (5.229s)\r\n> Enter [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:34 @ 03\/04\/24 21:05:11.745\r\nI0304 21:05:11.745748 17026 helper.go:121] Waiting up to 7m0s for all (but 0) nodes to be ready\r\n< Exit [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:34 @ 03\/04\/24 21:05:11.747 (1ms)\r\n> Enter [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:35 @ 03\/04\/24 21:05:11.747\r\n< Exit [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:35 @ 03\/04\/24 21:05:11.747 (0s)\r\n> Enter [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - dump namespaces | framework.go:218 @ 03\/04\/24 21:05:11.747\r\nSTEP: dump namespace information after failure - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:297 @ 03\/04\/24 21:05:11.747\r\nSTEP: Collecting events from namespace \"memorymanager-metrics-7056\". - k8s.io\/kubernetes\/test\/e2e\/framework\/debug\/dump.go:42 @ 03\/04\/24 21:05:11.747\r\nSTEP: Found 1 events. - k8s.io\/kubernetes\/test\/e2e\/framework\/debug\/dump.go:46 @ 03\/04\/24 21:05:11.748\r\nI0304 21:05:11.748360 17026 dump.go:53] At 2024-03-04 21:04:06 +0000 UTC - event for memmngrpodqxfln: {kubelet i-0ac0e520963f28681} UnexpectedAdmissionError: Allocate failed due to [memorymanager] failed to get the default NUMA affinity, no NUMA nodes with enough memory is available, which is unexpected\r\nI0304 21:05:11.749206 17026 resource.go:168] POD  NODE  PHASE  GRACE  CONDITIONS\r\nI0304 21:05:11.749229 17026 resource.go:178] \r\nI0304 21:05:11.750364 17026 dump.go:109] \r\nLogging node info for node i-0ac0e520963f28681\r\nI0304 21:05:11.751605 17026 dump.go:114] Node Info: &Node{ObjectMeta:{i-0ac0e520963f28681    0179d058-d76f-4236-a5fe-fea5c41712ee 4114 0 2024-03-04 19:49:10 +0000 UTC <nil> <nil> map[beta.kubernetes.io\/arch:amd64 beta.kubernetes.io\/os:linux kubernetes.io\/arch:amd64 kubernetes.io\/hostname:i-0ac0e520963f28681 kubernetes.io\/os:linux] map[volumes.kubernetes.io\/controller-managed-attach-detach:true] [] [] [{kubelet Update v1 2024-03-04 19:49:10 +0000 UTC FieldsV1 {\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:volumes.kubernetes.io\/controller-managed-attach-detach\":{}},\"f:labels\":{\".\":{},\"f:beta.kubernetes.io\/arch\":{},\"f:beta.kubernetes.io\/os\":{},\"f:kubernetes.io\/hostname\":{}}}} } {kubelet Update v1 2024-03-04 21:05:06 +0000 UTC FieldsV1 {\"f:metadata\":{\"f:labels\":{\"f:kubernetes.io\/arch\":{},\"f:kubernetes.io\/os\":{}}},\"f:status\":{\"f:allocatable\":{\"f:cpu\":{},\"f:ephemeral-storage\":{},\"f:example.com\/resource\":{},\"f:hugepages-2Mi\":{},\"f:memory\":{}},\"f:capacity\":{\"f:example.com\/resource\":{},\"f:hugepages-2Mi\":{}},\"f:conditions\":{\"k:{\\\"type\\\":\\\"DiskPressure\\\"}\":{\"f:lastHeartbeatTime\":{},\"f:lastTransitionTime\":{},\"f:message\":{},\"f:reason\":{},\"f:status\":{}},\"k:{\\\"type\\\":\\\"MemoryPressure\\\"}\":{\"f:lastHeartbeatTime\":{}},\"k:{\\\"type\\\":\\\"PIDPressure\\\"}\":{\"f:lastHeartbeatTime\":{}},\"k:{\\\"type\\\":\\\"Ready\\\"}\":{\"f:lastHeartbeatTime\":{},\"f:lastTransitionTime\":{},\"f:message\":{},\"f:reason\":{},\"f:status\":{}}},\"f:images\":{},\"f:nodeInfo\":{\"f:containerRuntimeVersion\":{}}}} status}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{53607378944 0} {<nil>} 52350956Ki BinarySI},example.com\/resource: {{0 0} {<nil>} 0 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8103153664 0} {<nil>} 7913236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{48246640970 0} {<nil>} 48246640970 DecimalSI},example.com\/resource: {{0 0} {<nil>} 0 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{7841009664 0} {<nil>} 7657236Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-03-04 21:05:06 +0000 UTC,LastTransitionTime:2024-03-04 19:49:10 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-03-04 21:05:06 +0000 UTC,LastTransitionTime:2024-03-04 20:01:53 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-03-04 21:05:06 +0000 UTC,LastTransitionTime:2024-03-04 19:49:10 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-03-04 21:05:06 +0000 UTC,LastTransitionTime:2024-03-04 20:41:59 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.31.91.175,},NodeAddress{Type:Hostname,Address:i-0ac0e520963f28681,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:ec2ac97c04ac656aae9f9169a562abc2,SystemUUID:ec2ac97c-04ac-656a-ae9f-9169a562abc2,BootID:83a00488-fb56-4d28-a2d4-5aab2106d304,KernelVersion:6.1.77-99.164.amzn2023.x86_64,OSImage:Amazon Linux 2023,ContainerRuntimeVersion:containerd:\/\/2.0.0-beta.2-222-g9a2b85561,KubeletVersion:v1.30.0-alpha.3.375+4ddf96c672283f,KubeProxyVersion:v1.30.0-alpha.3.375+4ddf96c672283f,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/perl@sha256:c613344cdd31c5055961b078f831ef9d9199fc9111efe6e81bea3f00d78bd979 registry.k8s.io\/e2e-test-images\/perl:5.26],SizeBytes:325397679,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/node-perf\/tf-wide-deep@sha256:91ab3b5ee22441c99370944e2e2cb32670db62db433611b4e3780bdee6a8e5a1 registry.k8s.io\/e2e-test-images\/node-perf\/tf-wide-deep:1.3],SizeBytes:213443508,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/volume\/nfs@sha256:c3e380bb5a92095288b4dd586a250c560612afbabf53fb2a057e99fff75f038f registry.k8s.io\/e2e-test-images\/volume\/nfs:1.4],SizeBytes:95831867,},ContainerImage{Names:[registry.k8s.io\/node-problem-detector\/node-problem-detector@sha256:d65a5c35dc7e948ff6d0bbe15b5e272c55790acbc9db4963ad91dc9939b4e293 registry.k8s.io\/node-problem-detector\/node-problem-detector:v0.8.13],SizeBytes:57958233,},ContainerImage{Names:[registry.k8s.io\/etcd@sha256:44a8e24dcbba3470ee1fee21d5e88d128c936e9b55d4bc51fbef8086f8ed123b registry.k8s.io\/etcd:3.5.12-0],SizeBytes:57236178,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/agnhost@sha256:cc249acbd34692826b2b335335615e060fdb3c0bca4954507aa3a1d1194de253 registry.k8s.io\/e2e-test-images\/agnhost:2.47],SizeBytes:52529911,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22 registry.k8s.io\/e2e-test-images\/httpd:2.4.38-4],SizeBytes:40764257,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/node-perf\/npb-is@sha256:8285539c79625b192a5e33fc3d21edc1a7776fb9afe15fae3b5037a7a8020839 registry.k8s.io\/e2e-test-images\/node-perf\/npb-is:1.2],SizeBytes:39706325,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/node-perf\/npb-ep@sha256:90b5cfc5451428aad4dd6af9960640f2506804d35aa05e83c11bf0a46ac318c8 registry.k8s.io\/e2e-test-images\/node-perf\/npb-ep:1.2],SizeBytes:39705373,},ContainerImage{Names:[gcr.io\/cadvisor\/cadvisor@sha256:e6c562b5e983f13624898b5b6a902c71999580dc362022fc327c309234c485d7 gcr.io\/cadvisor\/cadvisor:v0.47.2],SizeBytes:33756591,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/sample-device-plugin@sha256:333840c95378b183bb004a858fd456a2db0116e285b45f57c42e25a17fe130b5 registry.k8s.io\/e2e-test-images\/sample-device-plugin:1.7],SizeBytes:17861073,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/nonroot@sha256:ee9f50b3c64b174d296d91ca9f69a914ac30e59095dfb462b2b518ad28a63655 registry.k8s.io\/e2e-test-images\/nonroot:1.4],SizeBytes:17747885,},ContainerImage{Names:[docker.io\/nfvpe\/sriov-device-plugin@sha256:518499ed631ff84b43153b8f7624c1aaacb75a721038857509fe690abdf62ddb docker.io\/nfvpe\/sriov-device-plugin:v3.1],SizeBytes:12638340,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/nginx@sha256:5c99cf6a02adda929b10321dbf4ecfa00d87be9ba4fb456006237d530ab4baa1 registry.k8s.io\/e2e-test-images\/nginx:1.14-4],SizeBytes:6978614,},ContainerImage{Names:[registry.k8s.io\/nvidia-gpu-device-plugin@sha256:4b036e8844920336fa48f36edeb7d4398f426d6a934ba022848deed2edbf09aa],SizeBytes:6819465,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/ipc-utils@sha256:647d092bada3b46c449d875adf31d71c1dd29c244e9cca6a04fddf9d6bcac136 registry.k8s.io\/e2e-test-images\/ipc-utils:1.3],SizeBytes:4013020,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/nonewprivs@sha256:8ac1264691820febacf3aea5d152cbde6d10685731ec14966a9401c6f47a68ac registry.k8s.io\/e2e-test-images\/nonewprivs:1.3],SizeBytes:3263463,},ContainerImage{Names:[registry.k8s.io\/e2e-test-images\/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9 registry.k8s.io\/e2e-test-images\/busybox:1.36.1-1],SizeBytes:2223659,},ContainerImage{Names:[registry.k8s.io\/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097 registry.k8s.io\/pause:3.9],SizeBytes:321520,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}\r\nI0304 21:05:11.751631 17026 dump.go:116] \r\nLogging kubelet events for node i-0ac0e520963f28681\r\nI0304 21:05:11.752595 17026 dump.go:121] \r\nLogging pods the kubelet thinks is on node i-0ac0e520963f28681\r\nW0304 21:05:11.758398   17026 metrics_grabber.go:111] Can't find any pods in namespace kube-system to grab metrics from\r\nI0304 21:05:11.769542 17026 kubelet_metrics.go:206] \r\nLatency metrics for node i-0ac0e520963f28681\r\nEND STEP: dump namespace information after failure - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:297 @ 03\/04\/24 21:05:11.769 (22ms)\r\n< Exit [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - dump namespaces | framework.go:218 @ 03\/04\/24 21:05:11.769 (22ms)\r\n> Enter [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - tear down framework | framework.go:215 @ 03\/04\/24 21:05:11.769\r\nSTEP: Destroying namespace \"memorymanager-metrics-7056\" for this suite. - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:360 @ 03\/04\/24 21:05:11.769\r\n< Exit [DeferCleanup (Each)] [sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] - tear down framework | framework.go:215 @ 03\/04\/24 21:05:11.771 (2ms)\r\n```\r\n\r\nI think it's better and safer to relax the test conditions instead of looking for exact matches. Fixing in a little while.","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-e2e-gce\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/triage accepted","\/priority important-longterm","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd\r\n","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","@ffromani: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-e2e-capz-windows-serial-slow | 52c68c8886642a7777042f1f816f10715760d61e | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123468\/pull-kubernetes-e2e-capz-windows-serial-slow\/1760977366655438848) | false | `\/test pull-kubernetes-e2e-capz-windows-serial-slow`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123468). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Affromani). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","the MM metrics tests are consistently passing (or at least much less flaky). We're getting unrelated failures, which should be tracked separately","\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd","https:\/\/testgrid.k8s.io\/sig-node-release-blocking#node-kubelet-serial-containerd\r\nThis fix a flake in sig-node-release-blocking.\r\n\r\n\/cc @bart0sh @kannon92 \r\n\/assign @SergeyKanzhelev @klueska @mrunalp ","@swatisehgal could you take a look?","Thanks @swatisehgal for your deep and thoughtful review, this is very very appreciated. Unfortunately I need to handle other activities now but I will address your points ASAP. Just wanted to give a quick and deserved acknowledgement.","> I see CI is green and happy and I am trying really hard to be happy with the changes here but there are still some loose ends that I can't fully make sense of so please bear with me :)\r\n\r\nQuite the contrary! I appreciate the careful review and avoiding cutting corners, appreciate the extra care. Thanks for that!\r\n\r\n> Our first test is to check the state of memory manager metric right after kubelet restart where we have a **strict** check and basically want to ensure that both \"kubelet_memory_manager_pinning_requests_total\" and \"kubelet_memory_manager_pinning_errors_total\" have zero values which makes perfect sense to me.\r\n> \r\n> If the environment is not clean, the test fails because of this strict first check and rightly so! However, in the subsequent tests (one where pod fails at admission and another one where it succeeds) we lax the checks.\r\n> \r\n> Now what is not clear to me is that if we ensure that our starting point is a clean environment and we have no leaked pod why do we need to make the other checks less strict and how does that help?\r\n\r\nThe problem here is that the tests are [serial](https:\/\/onsi.github.io\/ginkgo\/#serial-specs), not [ordered](https:\/\/onsi.github.io\/ginkgo\/#ordered-containers). So ginkgo will run all the e2e_node tests one after the other (vs concurrently) but the execution order may change, and this is confirmed looking at [testgrid](https:\/\/prow.k8s.io\/pr-history\/?org=kubernetes&repo=kubernetes&pr=123468): for example https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123468\/pull-kubernetes-node-kubelet-serial-containerd\/1767593914295914496 vs https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123468\/pull-kubernetes-node-kubelet-serial-containerd\/1765067157089030144 vs https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123468\/pull-kubernetes-node-kubelet-serial-containerd\/1765030498595246080\r\n\r\nSo, the fact that we have a clean env doesn't guarantee (unless I'm missing something big!) that another test doesn't interject and pollute the environment.\r\nOTOH, relaxing the conditions about `kubelet_memory_manager_pinning_{errors,requests}_total` being zero right after a reboot doesn't make sense, and because of serial tests nothing can interfere with it, so it's still good.\r\n>\r\n> Let's take the second test (`should report pinning failures when the memorymanager allocation is known to fail`) as an example here, we have created **1 pod** after kubelet restarts and yet are okay if the metric indicates there are more than 1 pod requests and failures [value > = 1](https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123468\/commits\/6691032fec98d181a74df77513a1adec2f70e33c#diff-6f3fbea74d4f9dfbdb8ee8b6f9415a1a266750ff6ebc7946ae496f18e1687fbcR139). Both the metrics incrementing by 1 and only 1 is indicative of the fact that the metrics are being correctly updated and if that is not the case, it means that they are not getting updated as expected and the test **should** fail. I am worried that in the future these relaxed checks could result in false positives and impression of a happy and healthy CI when that is not the case.\r\n\r\nThis is true, I can't argue with that.\r\n\r\nWhat drove me here is that if we make the test strict as I wish to, then THIS test will become red if the environment is polluted because another test is buggy\/leaking, but the culprit would be the other test which instead reports green.\r\nOn top of that, since the serial tests are run periodically (vs before merge) there's an ample time window on which e2e test may break which complicates the debug and the troubleshooting.\r\n\r\nThe obvious solution would be to track and fix all the e2e tests and fix the actual bugs. This is undoubtly better, but I can't commit to that unfortunately.\r\n\r\nSo I guess I'm between a rock and a hard place :\\\r\n\r\n> Also I don't fully understand the comment about not being able to afford strict checks due to capacity constraints. Let's assume that some tests are added in the future and fail to clean the environment properly. In that case, the CI would end up turning RED because of the failure of the first strict check and someone from the community would _have_ to investigate the issue. How do the laxed checks help us here?\r\n\r\nNo, they won't.\r\n\r\nThing is, a strict test which fails because a polluted environment will be under scrutiny when the problem is elsewhere.\r\nI'd love to have a better mechanism to highlight the real culprit, rather than defend the poor messenger :)\r\n\r\nBut I concur this is going in the direction of hiding the problem, which is not the right thing to do. I guess I'll go back to the drawing board.\r\n\r\n","I'll add my 2 cent here and say that maybe we should consider a proper cleanup before the test run than making the tests less stricter.\r\n\r\nHaving a cleanup on our side hiding the problem - true but same claim goes for laxing the tests.\r\nAlso while fixing the actual culprit is the right approach it doesn't guaranteed we won't hit the same issue in the future by a different test.\r\n\r\nHaving a proper cleanup from our side is indeed a workaround but it at least guarantees that we won't face the same issue again.","> Having a cleanup on our side hiding the problem - true but same claim goes for laxing the tests. Also while fixing the actual culprit is the right approach it doesn't guaranteed we won't hit the same issue in the future by a different test.\r\n\r\nI get what you mean, and this is probably something we should try. However there's a fine line on which we can relax the test without making it incorrect, which was the spot I was aiming for.\r\n \r\n> Having a proper cleanup from our side is indeed a workaround but it at least guarantees that we won't face the same issue again.\r\n\r\nThat's a good point, so I'll probably try this approach\r\n"],"labels":["kind\/bug","area\/test","sig\/node","size\/L","release-note-none","cncf-cla: yes","sig\/testing","priority\/important-longterm","kind\/failing-test","triage\/accepted"]},{"title":"`--no-headers` available for `k config get-contexts` but not `get-clusters`","body":"### What happened?\n\n```\r\n$ kubectl config get-clusters --no-headers\r\nerror: unknown flag: --no-headers\r\n```\r\n\r\nwhereas\r\n\r\n```\r\n$ kubectl config get-contexts --no-headers\r\n      arn:aws:eks:eu-west-1:....\r\n      ...\r\n```\n\n### What did you expect to happen?\n\nThe list of clusters to be displayed without the `NAME` header\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nRun `kubectl config get-clusters --no-headers`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n:$ kubectl version\r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.0-eks-c417bb3\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nnot relevant to the issue\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\nMacOS\r\n$ sw_vers\r\nProductName:            macOS\r\nProductVersion:         14.2.1\r\nBuildVersion:           23C71\r\n\r\n$ uname -a\r\nDarwin MACC02DV7WCMD6R 23.2.0 Darwin Kernel Version 23.2.0: Wed Nov 15 21:54:10 PST 2023; root:xnu-10002.61.3~2\/RELEASE_X86_64 x86_64 i386 Darwin\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n`Homebrew`\r\n\r\n```\r\n$ brew info kubernetes-cli\r\n==> kubernetes-cli: stable 1.29.2 (bottled), HEAD\r\nKubernetes command-line interface\r\nhttps:\/\/kubernetes.io\/docs\/reference\/kubectl\/\r\n\/usr\/local\/Cellar\/kubernetes-cli\/1.29.2 (234 files, 60.3MB) *\r\n  Poured from bottle using the formulae.brew.sh API on 2024-02-15 at 11:59:04\r\nFrom: https:\/\/github.com\/Homebrew\/homebrew-core\/blob\/HEAD\/Formula\/k\/kubernetes-cli.rb\r\nLicense: Apache-2.0\r\n==> Dependencies\r\nBuild: bash \u2714, coreutils \u2714, go \u2714\r\n==> Options\r\n--HEAD\r\n\tInstall HEAD version\r\n==> Caveats\r\nBash completion has been installed to:\r\n  \/usr\/local\/etc\/bash_completion.d\r\n\r\n```\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig cli\r\n"],"labels":["kind\/bug","sig\/cli","needs-triage"]},{"title":"PV nodeAffinity matchExpressions problem with array items and `in` operator since 1.27.0","body":"### What happened?\r\n\r\nSince v1.27.0, the `nodeAffinity` for `PersistentVolumes` appear to have added extra validation on whether the values exist, despite using the `in` operator. This works as expected on 1.26.14, and no longer works on 1.27.0 onwards (tested up to 1.29.2).\r\n\r\nI am unsure whether this is an intentional breaking change in 1.27.0 or something wrong in our manifests and we've just so happened to get away with it for a few years. The only thing that stands out to me in the changelog for 1.27.0 is some changes to the schedulers `Filter` plugin but this may well be unrelated.\r\n\r\n### What did you expect to happen?\r\n\r\nWhen using the `in` operator, I'd expect the expression to only apply to nodes where the match is truey.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nNote, using Kind for easy reproduction\r\n\r\n## v1.26.14 -  working\r\n\r\n### creating a kind cluster\r\n~~~ shell\r\ncat <<EOF > kind_conf\r\nkind: Cluster\r\napiVersion: kind.x-k8s.io\/v1alpha4\r\nnodes:\r\n- role: control-plane\r\n  image: kindest\/node:v1.26.14\r\n- role: worker\r\n  image: kindest\/node:v1.26.14\r\n- role: worker\r\n  image: kindest\/node:v1.26.14\r\nEOF\r\n\r\n$ kind create cluster --name 12614-test --config kind_conf\r\n$ k get no\r\nNAME                       STATUS   ROLES           AGE   VERSION\r\n12614-test-control-plane   Ready    control-plane   8h    v1.26.14\r\n12614-test-worker          Ready    <none>          8h    v1.26.14\r\n12614-test-worker2         Ready    <none>          8h    v1.26.14\r\n# labelling a node to ensure pod is scheduled on specific worker\r\n$ k label node 12614-test-worker nginx=nginx\r\n~~~\r\n\r\n~~~ shell\r\ncat <<EOF > manifest.yaml\r\n---\r\napiVersion: apps\/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  labels:\r\n    app: nginx\r\n  name: nginx\r\n  namespace: default\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  serviceName: nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: nginx\r\n                operator: Exists\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:latest\r\n        imagePullPolicy: IfNotPresent\r\n        volumeMounts:\r\n        - mountPath: \/var\/lib\/nginx\r\n          name: default-nginx-data\r\n  volumeClaimTemplates:\r\n  - apiVersion: v1\r\n    kind: PersistentVolumeClaim\r\n    metadata:\r\n      name: default-nginx-data\r\n    spec:\r\n      accessModes:\r\n      - ReadWriteMany\r\n      resources:\r\n        requests:\r\n          storage: 5Gi\r\n      storageClassName: my_sc\r\n      volumeMode: Filesystem\r\n---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: default-nginx-data-nginx-0\r\n  namespace: default\r\n  labels:\r\n    app: default-nginx-data-nginx-0\r\nspec:\r\n  storageClassName: my_sc\r\n  capacity:\r\n    storage: 5Gi\r\n  accessModes:\r\n    - ReadWriteMany\r\n  hostPath:\r\n    path: \/srv\/k8s\/default\/nginx-0\r\n  claimRef:\r\n    namespace: default\r\n    name: default-nginx-data-nginx-0\r\n  nodeAffinity:\r\n    required:\r\n      nodeSelectorTerms:\r\n      - matchExpressions:\r\n        - key: kubernetes.io\/hostname\r\n          operator: In\r\n          values:\r\n            - 12614-test-worker\r\n            - 12614-prod-worker\r\nEOF\r\n# Apply the manifest\r\n$ k apply -f manifest.yaml\r\n~~~\r\n\r\nSince `12614-test-worker` exists in the hostname values of this cluster, the pod schedules as expected. `12614-prod-worker` node does not exist in this environment, and so is ignored.\r\n\r\n~~~ shell\r\n$ k get po\r\nNAME      READY   STATUS    RESTARTS   AGE\r\nnginx-0   1\/1     Running   0          6m2s\r\n~~~\r\n## v1.27.0 -  not working\r\n\r\n### creating a kind cluster\r\n~~~ shell\r\ncat <<EOF > kind_conf\r\nkind: Cluster\r\napiVersion: kind.x-k8s.io\/v1alpha4\r\nnodes:\r\n- role: control-plane\r\n  image: kindest\/node:v1.27.0\r\n- role: worker\r\n  image: kindest\/node:v1.27.0\r\n- role: worker\r\n  image: kindest\/node:v1.27.0\r\nEOF\r\n\r\n$ kind create cluster --name 1270-test --config kind_conf\r\n# labelling a node to ensure pod is scheduled on specific worker\r\n$ k label node 1270-test-worker nginx=nginx\r\n~~~\r\n\r\n~~~ shell\r\ncat <<EOF > manifest.yaml\r\n---\r\napiVersion: apps\/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  labels:\r\n    app: nginx\r\n  name: nginx\r\n  namespace: default\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  serviceName: nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: nginx\r\n                operator: Exists\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:latest\r\n        imagePullPolicy: IfNotPresent\r\n        volumeMounts:\r\n        - mountPath: \/var\/lib\/nginx\r\n          name: default-nginx-data\r\n  volumeClaimTemplates:\r\n  - apiVersion: v1\r\n    kind: PersistentVolumeClaim\r\n    metadata:\r\n      name: default-nginx-data\r\n    spec:\r\n      accessModes:\r\n      - ReadWriteMany\r\n      resources:\r\n        requests:\r\n          storage: 5Gi\r\n      storageClassName: my_sc\r\n      volumeMode: Filesystem\r\n---\r\napiVersion: v1\r\nkind: PersistentVolume\r\nmetadata:\r\n  name: default-nginx-data-nginx-0\r\n  namespace: default\r\n  labels:\r\n    app: default-nginx-data-nginx-0\r\nspec:\r\n  storageClassName: my_sc\r\n  capacity:\r\n    storage: 5Gi\r\n  accessModes:\r\n    - ReadWriteMany\r\n  hostPath:\r\n    path: \/srv\/k8s\/default\/nginx-0\r\n  claimRef:\r\n    namespace: default\r\n    name: default-nginx-data-nginx-0\r\n  nodeAffinity:\r\n    required:\r\n      nodeSelectorTerms:\r\n      - matchExpressions:\r\n        - key: kubernetes.io\/hostname\r\n          operator: In\r\n          values:\r\n            - 1270-test-worker\r\n            - 1270-prod-worker\r\nEOF\r\n# Apply the manifest\r\n$ k apply -f manifest.yaml\r\n~~~\r\n\r\nSame expectations as 1.26.14, but in this case, the pod stays stuck in pending state due to the following;\r\n\r\n~~~ shell\r\n$ k get no\r\nNAME                      STATUS   ROLES           AGE     VERSION\r\n1270-test-control-plane   Ready    control-plane   5m1s    v1.27.0\r\n1270-test-worker          Ready    <none>          4m34s   v1.27.0\r\n1270-test-worker2         Ready    <none>          4m39s   v1.27.0\r\n$ k get po\r\nNAME      READY   STATUS    RESTARTS   AGE\r\nnginx-0   0\/1     Pending   0          5m56s\r\n$ kubectl events --for pod\/nginx-0\r\nLAST SEEN   TYPE      REASON             OBJECT        MESSAGE\r\n3m6s        Warning   FailedScheduling   Pod\/nginx-0   nodeinfo not found for node name \"1270-prod-worker\"\r\n~~~\r\n\r\nI have tested this all the way up to 1.29.2.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\nv1.27.0 through to v1.29.2\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n[root@stage1 ~]# cat \/etc\/os-release\r\nNAME=\"CentOS Linux\"\r\nVERSION=\"7 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"7\"\r\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:\/o:centos:centos:7\"\r\nHOME_URL=\"https:\/\/www.centos.org\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.centos.org\/\"\r\n\r\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT=\"centos\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n[root@stage1 ~]# uname -a\r\nLinux stage1.domain.com 3.10.0-1160.108.1.el7.x86_64 #1 SMP Thu Jan 25 16:17:31 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig storage","In a similar vein, we have noticed the same issue in K8s v1.27 when the `kubernetes.io\/hostname` label doesn't match the node name. We see `nodeinfo not found for <hostname>`, despite (I think) the nodeinfo util supposed to be called using a node name.","\/sig scheduling"],"labels":["kind\/bug","sig\/scheduling","sig\/storage","needs-triage"]},{"title":"grpc: set localhost Authority to unix client calls","body":"#### What type of PR is this?\r\n\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nI'm building CRI proxy by Rust\/tonic, so it's also better to fix it in CRI.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nxerf: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/112597\r\n\r\n#### Special notes for your reviewer:\r\n\r\nNop\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNone\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n```docs\r\nNone\r\n```\r\n","comments":["[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123463#\" title=\"Author self-approved\">k82cn<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [feiskyer](https:\/\/github.com\/feiskyer) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cri\/remote\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cri\/remote\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"feiskyer\"]} -->","@feiskyer , @tallclair , WDYT?","\/triage accepted\r\n\/priority important-longterm\r\n\/cc @SergeyKanzhelev @pacoxu @kannon92 "],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/XS","release-note-none","cncf-cla: yes","priority\/important-longterm","triage\/accepted"]},{"title":"[Flaking Test] [sig-apps][sig-architecture] sig-release-master-informing (Conformance - EC2 - master)","body":"### Which jobs are flaking?\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#Conformance%20-%20EC2%20-%20master&show-stale-tests=\r\n\r\n### Which tests are flaking?\r\n\r\n1. [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]\r\n2. [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]\r\n\r\n### Since when has it been flaking?\r\n\r\nThese tests flake regularly in almost 2-3 days intervals. `Might cause a NO\/GO to upcoming RCs! `\r\n\r\n### Testgrid link\r\n\r\nhttps:\/\/testgrid.k8s.io\/sig-release-master-informing#Conformance%20-%20EC2%20-%20master&show-stale-tests=\r\n\r\n### Reason for failure (if possible)\r\n\r\n_No response_\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Relevant SIG(s)","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig apps architecture\r\n\/cc @kubernetes\/release-team-release-signal \r\n","\/cc @alculquicondor @soltysh @janetkuo \r\n- some related issue before:\r\n  -  https:\/\/github.com\/kubernetes\/kubernetes\/issues\/118928\r\n  -  https:\/\/github.com\/kubernetes\/kubernetes\/issues\/69601 ","The tests failed [FAILED] as Conformance test suite needs a cluster with at least 2 nodes. \r\n\/cc @ameukam \r\n- This may be an infra problem according to the failing reason. \r\n\r\nOr can we skip if the infra is flaky when we don't have enough nodes to run the test?\r\n\r\n\/sig k8s-infra","@pacoxu There were 3 instances created and deleted:\r\n\r\n```bash\r\nI0304 04:46:53.400815    3239 deployer.go:155] deleted instance id: i-0b5893d63f85b60bc\r\nI0304 04:46:53.718314    3239 deployer.go:155] deleted instance id: i-098bba2873b0ff3df\r\nI0304 04:46:54.032940    3239 deployer.go:155] deleted instance id: i-097b5ecb2c80639c4\r\n```\r\nI think this meet Conformance node requirement. What the failing error that suggests it's an infrastructure problem ?","@dims is currently working a lot on this.","```\r\n\r\nKubernetes e2e suite: [It] [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]\r\n\r\n{ failed [FAILED] \r\nConformance requires at least two nodes In [It] at: \r\nExpected\r\n    <int>: 1\r\nto be >\r\n    <int>: 1\r\nk8s.io\/kubernetes\/test\/e2e\/architecture\/conformance.go:45 @ 03\/03\/24 17:38:46.024 }\r\n\r\n\r\nI0303 17:39:59.427966 61877 daemon_set.go:140] pods: {\"kind\":\"PodList\",\"apiVersion\":\"v1\",\"metadata\":{\"resourceVersion\":\"15763\"},\"items\":null}\r\n\r\n< Exit [AfterEach] [sig-apps] Daemon set [Serial] - k8s.io\/kubernetes\/test\/e2e\/apps\/daemon_set.go:122 @ 03\/03\/24 17:39:59.45 (55ms)\r\n> Enter [DeferCleanup (Each)] [sig-apps] Daemon set [Serial] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:34 @ 03\/03\/24 17:39:59.45\r\nI0303 17:39:59.450363 61877 helper.go:121] Waiting up to 7m0s for all (but 0) nodes to be ready\r\n< Exit [DeferCleanup (Each)] [sig-apps] Daemon set [Serial] - k8s.io\/kubernetes\/test\/e2e\/framework\/node\/init\/init.go:34 @ 03\/03\/24 17:39:59.461 (11ms)\r\n> Enter [DeferCleanup (Each)] [sig-apps] Daemon set [Serial] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:35 @ 03\/03\/24 17:39:59.461\r\n< Exit [DeferCleanup (Each)] [sig-apps] Daemon set [Serial] - k8s.io\/kubernetes\/test\/e2e\/framework\/metrics\/init\/init.go:35 @ 03\/03\/24 17:39:59.461 (0s)\r\n> Enter [DeferCleanup (Each)] [sig-apps] Daemon set [Serial] - dump namespaces | framework.go:218 @ 03\/03\/24 17:39:59.461\r\nSTEP: dump namespace information after failure - k8s.io\/kubernetes\/test\/e2e\/framework\/framework.go:297 @ 03\/03\/24 17:39:59.461\r\nSTEP: Collecting events from namespace \"daemonsets-3276\". - k8s.io\/kubernetes\/test\/e2e\/framework\/debug\/dump.go:42 @ 03\/03\/24 17:39:59.461\r\nSTEP: Found 0 events. - k8s.io\/kubernetes\/test\/e2e\/framework\/debug\/dump.go:46 @ 03\/03\/24 17:39:59.472\r\n\r\n```\r\n\r\n> What the failing error that suggests it's an infrastructure problem ?\r\n\r\nBoth test cases are failed for `Conformance requires at least two nodes`.\r\n\r\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-ec2-conformance-latest\/1764326435192836096\r\n\r\n\r\n```\r\n\r\ngzip: stdin: invalid compressed data--format violated\r\ntar: Unexpected EOF in archive\r\ntar: Unexpected EOF in archive\r\ntar: Error is not recoverable: exiting now\r\n+ [[ false == true ]]\r\nCloud-init v. 23.4.4-0ubuntu0~22.04.1 finished at Sun, 03 Mar 2024 16:40:23 +0000. Datasource DataSourceEc2Local.  Up 69.42 seconds\r\n```\r\nhttps:\/\/storage.googleapis.com\/kubernetes-jenkins\/logs\/ci-kubernetes-ec2-conformance-latest\/1764326435192836096\/artifacts\/logs\/i-026cc699a8ec50dbe\/cloud-init-output.log Error in  cloud-init-output.log\r\n seems to be the root cause.","\/remove-sig k8s-infra\r\n\r\nthe infra provided by sig k8s infra is working fine, the problem here is the EC2 e2e cluster bringup which is going to be one of cluster-lifecycle, cloud-provider \/ AWS, or sig testing.","\/assign dims","It flakes in EC2: both arm and amd.\r\n\r\n- https:\/\/testgrid.k8s.io\/sig-release-master-informing#Conformance%20-%20EC2%20-%20arm64%20-%20master\r\n- https:\/\/testgrid.k8s.io\/sig-release-master-informing#Conformance%20-%20EC2%20-%20master\r\n"],"labels":["kind\/flake","sig\/apps","sig\/architecture","needs-triage"]},{"title":"Pod fail to start as \/sys\/fs\/cgroup\/devices\/kubepods.slice\/kubepods-burstable.slice doesn't exist ","body":"### What happened?\n\nPod failed to start with the following issue:\r\n\r\nWarning  FailedCreatePodContainer  29m   kubelet                      unable to ensure pod container exists: failed to create container for [kubepods burstable pod71ab4a03-d88b-4cbb-bd26-41c5b0ce3663] : Timeout waiting for systemd to create kubepods-burstable-pod71ab4a03_d88b_4cbb_bd26_41c5b0ce3663.slice\n\n### What did you expect to happen?\n\nThe pod to be successfully created and complete after running.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI  am unsure. once i reboot the node, the issue is resolved and the pods can start on it. \r\n\r\n\n\n### Anything else we need to know?\n\nWhen i see logs from journalctl -u  kubelet:\r\nI see a lot of pods logs like below:\r\n```\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974117 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/ab2b8cef-f0fb-4de0-ab3a-3e04cde0f37d\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974152 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/d656b533-ebb5-408a-a515-77ebb6d11217\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974184 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/fe8c66f0-4aa5-4784-9585-a9d093b977fe\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974212 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/52ca3ffa-54d4-4dc8-b1be-df53183faa74\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974243 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/8f258a92-843f-45bb-a8ec-372c796f1205\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974275 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/dec19472-1e63-476c-b66f-7e2a1f8b8501\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974302 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/47002ae2-3a51-44c0-9d4c-51a469cac463\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974334 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/5ede6e21-4159-4736-857e-3346b8899bb3\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974367 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/a587eec9-42d8-43ff-9491-0db1ceefb296\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974396 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/b384e091-a24e-4846-91b0-5255fb301718\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974424 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/ed7a5c77-70e8-4a6d-8363-6df0177f492c\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974467 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/0c7305e5-b30b-4330-8112-a88aad9d9361\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974496 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/4d777a20-2f14-4efc-ad4e-1aee46b207de\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974523 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/f67c5158-8129-4db3-bb1c-2ec408a0f498\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974553 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/3a6b8f57-32a8-42bd-bb56-4f858e154cc1\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974600 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/a9d40fe7-fdbd-4d3e-a3f7-fc1435ad09c0\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974632 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/996f6de3-b63c-492c-8597-704e9338958e\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974658 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/b55779a4-4c59-4cdd-8584-7785df73e7db\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974686 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/c3942c0a-2c76-4a95-8ae5-c8d7cca797d1\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974719 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/005d6b12-57d2-4779-b02a-8c9e5611b381\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974756 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/2ffbb08f-f3f6-40ea-80c9-9c38d130a282\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974764 \u00a0160756 pod_container_manager_linux.go:191] \"Failed to delete cgroup paths\" cgroupName=[kubepods burstable pod939ec36a-261c-46c4-80b4-af638d670e05] err=\"unable to destroy cgroup paths for cgroup [kubepods burstable pod939ec36a-261c-46c4-80b4-af638d670e05] : Timed out while waiting for systemd to remove kubepods-burstable-pod939ec36a_261c_46c4_80b4_af638d670e05.slice\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974785 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/30355c4a-55ad-4bf9-afa4-84ee5fe834ca\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974816 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/73dd1ad7-32ea-4ab3-a666-6745f607be33\/volumes\"\r\nFeb 22 15:16:14 isaac-hil-ovx-07 kubelet[160756]: I0222 15:16:14.974843 \u00a0160756 kubelet_getters.go:306] \"Path does not exist\" path=\"\/var\/lib\/kubelet\/pods\/c952e257-ae07-4389-b177-d5f117346801\/volumes\"\r\n```\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nlocal on prem cluster\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.6 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\nSUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\nBUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\nPRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n\r\n$ uname -a\r\n# Linux isaac-hil-ovx-07 5.15.0-86-generic #96~20.04.1-Ubuntu SMP Thu Sep 21 13:23:37 UTC 2023 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nrunc --version\r\nrunc version 1.1.9\r\ncommit: v1.1.9-0-gccaecfc\r\nspec: 1.0.2-dev\r\ngo: go1.20.8\r\nlibseccomp: 2.5.1\r\n\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\ncontainerd --version\r\ncontainerd containerd.io 1.6.24 61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node\r\nfor triage\r\n\r\n> version.Info{Major:\"1\", Minor:\"25\"\r\n\r\nthis version is not in support\r\nhttps:\/\/kubernetes.io\/releases\/","@trashadewan, it is difficult to fix the issue without repro step. Can you provide any repro steps or logs?\r\n\r\n\/triage needs-information\r\n\/priority backlog","@trashadewan Can you provide the repro steps? ","\r\nI wasn't able to reproduce the issue at all\r\nUpgraded to 22.04 and since then didn't see the issue\r\n\r\n\r\nSent from Outlook for iOS<https:\/\/aka.ms\/o0ukef>\r\n________________________________\r\nFrom: Sivakajan Sivaparan ***@***.***>\r\nSent: Tuesday, March 12, 2024 2:02:56 AM\r\nTo: kubernetes\/kubernetes ***@***.***>\r\nCc: Trasha Dewan ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [kubernetes\/kubernetes] Pod fail to start as \/sys\/fs\/cgroup\/devices\/kubepods.slice\/kubepods-burstable.slice doesn't exist (Issue #123459)\r\n\r\n\r\n@trashadewan<https:\/\/github.com\/trashadewan> Can you provide the repro steps?\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123459#issuecomment-1991086589>, or unsubscribe<https:\/\/github.com\/notifications\/unsubscribe-auth\/ABYYS3FNPKJW7CWTXRMLKW3YX3AEBAVCNFSM6AAAAABDV4SIOWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSOJRGA4DMNJYHE>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n"],"labels":["kind\/bug","priority\/backlog","sig\/node","triage\/needs-information","needs-triage"]},{"title":"Informer: Single TweakListOptionsFunc type","body":"### What would you like to be added?\n\nCurrently, every SharedInformerFactory has an associated `factory_interfaces.go` file. In this generated file, there's a type called `TweakListOptionsFunc func(*\"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\".ListOptions)`. This could be moved to a single type somewhere shared (perhaps in `k8s.io\/apimachinery\/pkg\/apis\/meta\/v1`), so that this common type can be passed around more easily.\n\n### Why is this needed?\n\nI'm trying to use generics to make a simple controller framework, and this is one of the sticking points preventing me from presenting a common interface for managing both internal and external types. This likely ties in to the larger discussion on #106846","comments":["\/sig api-machinery","cc @deads2k \r\nAs followup of https:\/\/github.com\/kubernetes\/kubernetes\/issues\/106846 , and one of the authors of `factory_interfaces.go` files.\r\n\/triage accepted"],"labels":["sig\/api-machinery","kind\/feature","triage\/accepted"]},{"title":"Allow users to set KUBECTX","body":"### What happened?\n\n```bash\r\nexport KUBECONFIG=xyz\r\n```\r\n\r\nis a thing\r\n\r\nbut when we call:\r\n\r\n```bash\r\nkubectl config use-context abc\r\n```\r\n\r\nit is global - it writes to the file and therefore not safe against multiples sessions or processes. This is not just a missing feature - this is a bug waiting to happen.\r\n\r\nPlease allow users to:\r\n\r\n```bash\r\nexport KUBECTX=abc\r\n```\r\n\r\nand of course if abc doesn't exist in xyz (the current config) -it should throw an error\r\n\n\n### What did you expect to happen?\n\nexpect env var for context\/ctx\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\neasy to understand\/repro\n\n### Anything else we need to know?\n\nnope\n\n### Kubernetes version\n\n<details>\r\n\r\nClient Version: v1.29.0\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.28.5-eks-5e0fdde\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\n\r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig cli\r\nfor feedback\r\n","\/triage accepted\r\n\/remove-kind bug\r\n\/kind feature\r\nAgreed and this is something that is intended to be worked on in the general overhaul of the kubeconfig that is being attempted."],"labels":["kind\/feature","sig\/cli","triage\/accepted"]},{"title":"[sig-node] Failure cluster [c13abd3c...] flakes from Memory Manager Metrics","body":"### Failure cluster [c13abd3c1cd0bf5be087](https:\/\/go.k8s.io\/triage#c13abd3c1cd0bf5be087)\r\n\r\n##### Error text:\r\n```\r\n[FAILED] Timed out after 60.000s.\r\nExpected\r\n    <string>: KubeletMetrics\r\nto match keys: {\r\n.\"kubelet_memory_manager_pinning_requests_total\"[]:\r\n\tExpected\r\n\t    <string>: Sample\r\n\tto match fields: {\r\n\t.Value:\r\n\t\tExpected\r\n\t\t    <model.SampleValue>: 2\r\n\t\tto be ==\r\n\t\t    <int>: 1\r\n\t}\r\n\t\r\n}\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e_node\/memory_manager_metrics_test.go:122 @ 02\/21\/24 06:40:31.854\r\n\r\n```\r\n#### Recent failures:\r\n[2\/22\/2024, 7:30:31 AM ci-cgroupv2-containerd-node-arm64-e2e-serial-ec2](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-arm64-e2e-serial-ec2\/1760642962640867328)\r\n[2\/22\/2024, 4:10:17 AM ci-kubernetes-node-kubelet-serial-containerd](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-kubelet-serial-containerd\/1760592630795735040)\r\n[2\/22\/2024, 1:33:30 AM ci-cgroupv2-containerd-node-arm64-al2023-e2e-serial-ec2-eks](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-arm64-al2023-e2e-serial-ec2-eks\/1760552867929788416)\r\n[2\/22\/2024, 12:09:14 AM ci-kubernetes-node-kubelet-serial-containerd](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-kubelet-serial-containerd\/1760531981122146304)\r\n[2\/21\/2024, 10:00:20 PM ci-kubernetes-node-arm64-ubuntu-serial](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-kubernetes-node-arm64-ubuntu-serial\/1760499516097695744)\r\n\r\n\r\n\/kind failing-test\r\n<!-- If this is a flake, please add: \/kind flake -->\r\n\r\n\/sig node","comments":["tests were added in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/121778\r\n\r\n\/assign @Tal-or \r\n\/cc @swatisehgal @ffromani ","@dims: GitHub didn't allow me to assign the following users: Tal-or.\n\nNote that only [kubernetes members](https:\/\/github.com\/orgs\/kubernetes\/people) with read permissions, repo collaborators and people who have commented on this issue\/PR can be assigned. Additionally, issues\/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123454#issuecomment-1960468984):\n\n>tests were added in https:\/\/github.com\/kubernetes\/kubernetes\/pull\/121778\r\n>\r\n>\/assign @Tal-or \r\n>\/cc @swatisehgal @ffromani \n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/priority important-soon","@dims thank you for raising that up.\r\n\r\nI'll be back in the middle of next week and get to it ASAP","\/triage accepted\r\n\/assign @Tal-or ","https:\/\/testgrid.k8s.io\/sig-node-release-blocking#node-kubelet-serial-containerd\r\n- `[sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] when querying \/metrics should report zero pinning counters after a fresh restart`\r\n\r\nThis flakes in sig node release blocking testgrid. \r\n\r\nI see @ffromani 's PR #123468 is fixing it. Can that fix target v1.30?\r\n\r\n","> https:\/\/testgrid.k8s.io\/sig-node-release-blocking#node-kubelet-serial-containerd\r\n> \r\n>     * `[sig-node] Memory Manager Metrics [Serial] [Feature:MemoryManager] when querying \/metrics should report zero pinning counters after a fresh restart`\r\n> \r\n> \r\n> This flakes in sig node release blocking testgrid.\r\n> \r\n> I see @ffromani 's PR #123468 is fixing it. Can that fix target v1.30?\r\n\r\nI guess we can. I'm monitoring the lanes and I'm willing to keep improving the test reliability."],"labels":["priority\/important-soon","sig\/node","kind\/failing-test","triage\/accepted"]},{"title":"Fix local-up-cluster networking","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\nFixes local_up_cluster \r\nMore details here -  https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123452\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #123452 \r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\n\r\n\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/assign @dims ","\/sig testing\r\n\/kind cleanup","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123453#\" title=\"Author self-approved\">hakuna-matatah<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please ask for approval from [dims](https:\/\/github.com\/dims). For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[hack\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"dims\"]} -->","sorry for the noise, PR got accidentally closed. I was thinking I was on a different branch( in the process of making some other PR related changes)","\/test pull-kubernetes-local-e2e","@hakuna-matatah: The following test **failed**, say `\/retest` to rerun all failed tests or `\/retest-required` to rerun all mandatory failed tests:\n\nTest name | Commit | Details | Required | Rerun command\n--- | --- | --- | --- | ---\npull-kubernetes-local-e2e | eb3b85aefd5b25579522a17fef7c408c214f8982 | [link](https:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/pr-logs\/pull\/123453\/pull-kubernetes-local-e2e\/1766101889356664832) | false | `\/test pull-kubernetes-local-e2e`\n\n[Full PR test history](https:\/\/prow.k8s.io\/pr-history?org=kubernetes&repo=kubernetes&pr=123453). [Your PR dashboard](https:\/\/prow.k8s.io\/pr?query=is%3Apr%20state%3Aopen%20author%3Ahakuna-matatah). Please help us cut down on flakes by [linking to](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https:\/\/github.com\/kubernetes\/kubernetes\/issues?q=is:issue+is:open) when you hit one in your PR.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands).\n<\/details>\n<!-- test report -->","\/remove-sig api-machinery"],"labels":["kind\/cleanup","size\/XS","release-note-none","cncf-cla: yes","sig\/testing","needs-priority","needs-triage"]},{"title":"Problem with local-up-cluster.sh","body":"### What happened?\n\nWhen you start a local cluster it doesn't setup bridge networking for `containerd containers` at `\/etc\/cni\/net.d\/10-containerd-net.conflist ` when it finds `\/opt\/cni\/bin\/loopback` locally and ends up with following output i.e Worker Node would never join the cluster because there is no networking setup for the node and kubelet sees it as `CNI networking not setup for node`\r\n\r\n\r\n```\r\nWARNING : The kubelet is configured to not fail even if swap is enabled; production deployments should disable swap unless testing NodeSwap feature.\r\n2024\/02\/22 20:37:04 [INFO] generate received request\r\n2024\/02\/22 20:37:04 [INFO] received CSR\r\n2024\/02\/22 20:37:04 [INFO] generating key: rsa-2048\r\n2024\/02\/22 20:37:04 [INFO] encoded CSR\r\n2024\/02\/22 20:37:04 [INFO] signed certificate with serial number 280440300532529762741846552121807144132451209833\r\nkubelet ( 82838 ) is running.\r\nwait kubelet ready\r\nNo resources found\r\nNo resources found\r\nNo resources found\r\nNo resources found\r\n127.0.0.1   NotReady   <none>   2s    v1.30.0-alpha.2.158+6049a1bca4551f-dirty\r\nerror: timed out waiting for the condition on nodes\/127.0.0.1\r\n^CCleaning up...\r\nCleaning up...\r\n\r\n\r\n```\r\n\r\nNode doesn't become ready because and kubelet logs shows that CNI  isn't ready and node doesn't become ready no matter how long we wait.\r\n\r\n```\r\n2024\/02\/22 20:37:04 [INFO] signed certificate with serial number 280440300532529762741846552121807144132451209833\r\nkubelet ( 82838 ) is running.\r\nwait kubelet ready\r\nNo resources found\r\nNo resources found\r\nNo resources found\r\nNo resources found\r\n127.0.0.1   NotReady   <none>   2s \r\n```\n\n### What did you expect to happen?\n\nExpect worker node to become ready when `local-up-cluster.sh` seamlessly with networking setup for worker node, so it leads to following output where it will successfully bring up the cluster.\r\n\r\n```\r\nTo start using your cluster, you can open up another terminal\/tab and run:\r\n\r\n  export KUBECONFIG=\/var\/run\/kubernetes\/admin.kubeconfig\r\n  cluster\/kubectl.sh\r\n\r\nAlternatively, you can write to the default kubeconfig:\r\n\r\n  export KUBERNETES_PROVIDER=local\r\n\r\n  cluster\/kubectl.sh config set-cluster local --server=https:\/\/localhost:6443 --certificate-authority=\/var\/run\/kubernetes\/server-ca.crt\r\n  cluster\/kubectl.sh config set-credentials myself --client-key=\/var\/run\/kubernetes\/client-admin.key --client-certificate=\/var\/run\/kubernetes\/client-admin.crt\r\n  cluster\/kubectl.sh config set-context local --cluster=local --user=myself\r\n  cluster\/kubectl.sh config use-context local\r\n  cluster\/kubectl.sh\r\n  ```\r\n \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThe way current code works is, when it calls [install_cni_if_needed\r\n](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/local-up-cluster.sh#L1365) it only checks if [\/opt\/cni\/bin\/loopback](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/local-up-cluster.sh#L1256C19-L1256C40) is present , and it doesn't check nor configure  if  [`bridge networking config file for containerd containers`](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/local-up-cluster.sh#L1217-L1252) like it sets up when  [install_cni](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/local-up-cluster.sh#L1186) is called.\n\n### Anything else we need to know?\n\nFix is to check if `\/etc\/cni\/net.d\/10-containerd-net.conflist` is also present when calling this [function](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/hack\/local-up-cluster.sh#L1254C10-L1257) as its mandatory for successful local_cluster_setup.\n\n### Kubernetes version\n\n<details>\r\n\r\n```console\r\n$ kubectl version\r\n# paste output here\r\n```\r\n\r\n\r\n```\r\ndev-dsk-hakuna-2c-0fa5574b % kubectl version                                                                                                 \r\nClient Version: v1.29.2\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.30.0-alpha.2.159+2414f23c341e2b-dirty\r\n```\r\n\r\n<\/details>\r\n\n\n### Cloud provider\n\n<details>\r\nlocal \r\n<\/details>\r\n\n\n### OS version\n\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n```\r\n\r\n```\r\nNAME=\"Amazon Linux\"\r\nVERSION=\"2\"\r\nID=\"amzn\"\r\nID_LIKE=\"centos rhel fedora\"\r\nVERSION_ID=\"2\"\r\nPRETTY_NAME=\"Amazon Linux 2\"\r\nANSI_COLOR=\"0;33\"\r\n```\r\n$ uname -a\r\n\r\n```\r\nLinux dev-dsk-hakuna-2c-0fa5574b.us-west-2.amazon.com 5.10.209-175.858.amzn2int.x86_64 #1 SMP Tue Feb 13 18:51:15 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n```\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\n\n### Install tools\n\n<details>\r\nFollow steps [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/devel\/running-locally.md)\r\n<\/details>\r\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\r\n```\r\ndev-dsk-hakuna-2c-0fa5574b % containerd --version\r\ncontainerd github.com\/containerd\/containerd v1.7.13 7c3aca7a610df76212171d200ca3811ff6096eb8\r\n```\r\n<\/details>\r\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\r\nPlugins [here](dev-dsk-hakuna-2c-0fa5574b % containerd --version\r\ncontainerd github.com\/containerd\/containerd v1.7.13 7c3aca7a610df76212171d200ca3811ff6096eb8) \r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig testing\r\n"],"labels":["kind\/bug","sig\/testing","needs-triage"]},{"title":"Support watch option on kubelet \/pods endpoint","body":"#### What type of PR is this?\r\n\r\n\/kind feature\r\n\/kind api-change\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\nSupports a `watch` option on the Kubelet API's `GET \/pods` endpoint, effectively allowing a user to make a request to the Kubelet to watch for changes to pods (new pod; change in status; etc). This feature is currently present on the Kube API and it would be nice to have parity in the Kubelet API. It would allow our application to detect changes to pods by watching the Kubelet instead of hitting the cluster API; or constantly polling the Kubelet on an interval.\r\n\r\n#### Which issue(s) this PR fixes:\r\n\r\nn\/a\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThe endpoint only provides updates relating to new pods or changes to existing pods. It does not provide an update about deleted pods. Is this necessary?\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\n```release-note\r\nSupports a `watch` option on the Kubelet API's `GET \/pods` endpoint\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nn\/a","comments":["<a href=\"https:\/\/easycla.lfx.linuxfoundation.org\/#\/?version=2\"><img src=\"https:\/\/s3.amazonaws.com\/cla-project-logo-prod\/cla-signed.svg\" alt=\"CLA Signed\" align=\"left\" height=\"28\" width=\"328\" ><br ><ul><li>:white_check_mark:login: seanvaleo \/ (5187f2ceaf83dc55bd41c8137298fca39b74fe04)<\/li><\/ul><br>The committers listed above are authorized under a signed CLA.","Welcome @seanvaleo! <br><br>It looks like this is your first PR to <a href='https:\/\/github.com\/kubernetes\/kubernetes'>kubernetes\/kubernetes<\/a> \ud83c\udf89. Please refer to our [pull request process documentation](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:\/\/go.k8s.io\/bot-commands). <br><br>You can also check if kubernetes\/kubernetes has [its own contribution guidelines](https:\/\/github.com\/kubernetes\/kubernetes\/tree\/master\/CONTRIBUTING.md). <br><br>You may want to refer to our [testing guide](https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md) if you run into trouble with your tests not passing. <br><br>If you are having difficulty getting your pull request seen, please follow the [recommended escalation practices](https:\/\/github.com\/kubernetes\/community\/blob\/master\/contributors\/guide\/pull-requests.md#why-is-my-pull-request-not-getting-reviewed). Also, for tips and tricks in the contribution process you may want to read the [Kubernetes contributor cheat sheet](https:\/\/git.k8s.io\/community\/contributors\/guide\/contributor-cheatsheet\/README.md). We want to make sure your contribution gets all the attention it needs! <br><br>Thank you, and welcome to Kubernetes. :smiley:","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","Hi @seanvaleo. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123449#\" title=\"Author self-approved\">seanvaleo<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [tallclair](https:\/\/github.com\/tallclair) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/server\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/server\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"tallclair\"]} -->","This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","thing is, kubelet endpoints are meant for internal usage and not really supported. I'm not sure we should invest in enhance them\r\n\/cc @dchen1107 @mrunalp ","@seanvaleo Please address review comment above, thanks."],"labels":["area\/kubelet","sig\/node","release-note","size\/M","kind\/api-change","kind\/feature","cncf-cla: yes","needs-ok-to-test","needs-priority","needs-triage"]},{"title":"Watch with resourceVersion=\"\" can take down control plane","body":"### What happened?\r\n\r\nI was investigating a customer case where the Kubernetes control plane was unavailable for prolonged time due to pod Watch being broken. I was suspecting an issue like https:\/\/github.com\/etcd-io\/etcd\/issues\/15402. The only clues I had was large sized of pods (>50KBs), watch cache being stale (`rage(apiserver_watch_cache_events_dispatched_total[1m])` equal zero) and a lot of errors `Fast watcher, slow processing` and occasional `prevKV=nil`.\r\n\r\nDue to lack of proper watch instrumentation in kube-apiserver, I was only able to reproduce a similar situation on my local cluster. What I found is that even in large clusters with thousands of watches, misguided clients can still open a watch with `resourceVersion=\"\"` that will directly read from etcd and if left inactive, can take down the whole control plane.\r\n\r\nIn my testing I have observed that slow\/inactive watch on throughput heavy resource opened directly to etcd can cause:\r\n* Huge memory leak on both kube-apiserver and etcd. This is caused by direct etcd watches having an infinite buffer and never kicking clients. \r\n* Overload of the apiserver-etcd watch stream, causing starvation of other watches, including a watch opened by watch cache. Starved watches experience a `prevKV=nil` error, causing them to break. If the watch opened by the watch cache is broken enough times it can cause termination of all watches and any new watch will be immediately closed. Short watches can lead to the whole APF being exhausted, as watches take the same amount of seats no matter how long they persist.\r\n\r\nThis goes against user expectation, where a large correctly running cluster with large pod objects can be taken down by single misguided client.\r\n\r\n### What did you expect to happen?\r\n\r\n* Inactive\/slow watchers are kicked to prevent infinite memory increase. This already happens for watches opened to watch cache.\r\n* It should not be possible for direct watches to etcd to cause termination of watch cache watches. Connection for watch cache should be separated to avoid cross pollution\r\n* When watch cache is enabled, users should not be able to create direct watches to etcd. We need https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/2340-Consistent-reads-from-cache\/README.md for Watch.\r\n* Window of available events should be limited by compaction window not by PrevKV availability\r\n* Kube-apiserver should provide a metrics allowing to trace state of watch requests, also separate watches to etcd and watch cache.\r\n\r\nSome of the issues listed here require improvements on etcd side. I will create separate issue in etcd repo after repeating the investigation for etcd alone.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nMy testing showed limit of 400MB\/s of watch throughput. \r\n\r\nThis was achieved by:\r\n* 100 watches with resourceVersion=\"\" to a single resource, each consuming 1 event per second\r\n* 100 write QPS of 50KB objects to a single resource\r\n\r\nTo reproduce prevKV=nil, before my workstation run out of memory I also reduced the compaction window to 5s. \r\n\r\nCode used\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"math\/rand\"\r\n\t\"path\/filepath\"\r\n\t\"sync\"\r\n\t\"sync\/atomic\"\r\n\t\"time\"\r\n\r\n\t\"k8s.io\/api\/core\/v1\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n\t\"k8s.io\/client-go\/kubernetes\"\r\n\t\"k8s.io\/client-go\/tools\/clientcmd\"\r\n\t\"k8s.io\/client-go\/util\/homedir\"\r\n)\r\n\r\nvar objectSize = 50000\r\nvar objectCount = 10\r\nvar timeBetweenRequests = time.Millisecond * 100\r\nvar maxQPS float32 = 100\r\n\r\nvar initResourceVersion = \"\"\r\nvar watchers = 100\r\nvar watcherSleep = time.Second * 1\r\n\r\nfunc main() {\r\n\tconfig, err := clientcmd.BuildConfigFromFlags(\"\", filepath.Join(homedir.HomeDir(), \".kube\", \"config\"))\r\n\tif err != nil {\r\n\t\tpanic(err.Error())\r\n\t}\r\n\tconfig.QPS = maxQPS\r\n\tconfig.Burst = 100\r\n\r\n\t\/\/ create the clientset\r\n\tclientset, err := kubernetes.NewForConfig(config)\r\n\tif err != nil {\r\n\t\tpanic(err.Error())\r\n\t}\r\n\r\n\tvar wg sync.WaitGroup\r\n\tvar events atomic.Int64\r\n\tstartWriters(clientset, &wg)\r\n\tstartWatchers(clientset, &wg, &events)\r\n\tstart := time.Now()\r\n\tevents.Store(0)\r\n\r\n\tfmt.Printf(\"Start %s\\n\", start)\r\n\tfor range time.Tick(time.Second) {\r\n\t\tfmt.Printf(\"%s events: %d\\n\", time.Since(start), events.Load())\r\n\t}\r\n\twg.Wait()\r\n}\r\n\r\nfunc startWriters(clientset kubernetes.Interface, wg *sync.WaitGroup) {\r\n\tfor i := 0; i < objectCount; i++ {\r\n\t\tname := fmt.Sprintf(\"%d\", i)\r\n\t\twg.Add(1)\r\n\t\tgo func() {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tclientset.CoreV1().ConfigMaps(\"default\").Delete(context.TODO(), name, metav1.DeleteOptions{})\r\n\t\t\t_, err := clientset.CoreV1().ConfigMaps(\"default\").Create(context.TODO(), randomConfigmap(name), metav1.CreateOptions{})\r\n\t\t\tif err != nil {\r\n\t\t\t\tpanic(err)\r\n\t\t\t}\r\n\t\t\ttime.Sleep(timeBetweenRequests)\r\n\t\t\tfor {\r\n\t\t\t\t_, err := clientset.CoreV1().ConfigMaps(\"default\").Update(context.TODO(), randomConfigmap(name), metav1.UpdateOptions{})\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tpanic(err)\r\n\t\t\t\t}\r\n\t\t\t\ttime.Sleep(timeBetweenRequests)\r\n\t\t\t}\r\n\t\t}()\r\n\t}\r\n}\r\nfunc startWatchers(clientset kubernetes.Interface, wg *sync.WaitGroup, events *atomic.Int64) {\r\n\tfor i := 0; i < watchers; i++ {\r\n\t\twg.Add(1)\r\n\t\tgo func() {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tfor {\r\n\t\t\t\twatch, err := clientset.CoreV1().ConfigMaps(\"default\").Watch(context.TODO(), metav1.ListOptions{ResourceVersion: initResourceVersion})\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tpanic(err)\r\n\t\t\t\t}\r\n\t\t\t\tfor event := range watch.ResultChan() {\r\n\t\t\t\t\tswitch event.Object.(type) {\r\n\t\t\t\t\tcase *v1.ConfigMap:\r\n\t\t\t\t\t\tevents.Add(1)\r\n\t\t\t\t\tcase *metav1.Status:\r\n\t\t\t\t\tdefault:\r\n\t\t\t\t\t\tfmt.Printf(\"Event, type: %s, obj: %+v\\n\", event.Type, event.Object)\r\n\t\t\t\t\t}\r\n\t\t\t\t\ttime.Sleep(watcherSleep)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}()\r\n\t}\r\n}\r\n\r\nfunc randomConfigmap(name string) *v1.ConfigMap {\r\n\treturn &v1.ConfigMap{\r\n\t\tObjectMeta: metav1.ObjectMeta{\r\n\t\t\tName: name,\r\n\t\t},\r\n\t\tImmutable: nil,\r\n\t\tData: map[string]string{\r\n\t\t\t\"random\": RandString(objectSize),\r\n\t\t},\r\n\t\tBinaryData: nil,\r\n\t}\r\n}\r\n\r\nconst chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\r\n\r\nfunc RandString(l int) string {\r\n\ts := make([]byte, l)\r\n\tfor i := 0; i < l; i++ {\r\n\t\ts[i] = chars[rand.Intn(len(chars))]\r\n\t}\r\n\treturn string(s)\r\n}\r\n```\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n1.29.2\r\n\r\n### Cloud provider\r\n\r\nKIND\r\n\r\n### OS version\r\n\r\nn\/a\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["\/sig api-machinery\r\n\/sig etcd\r\n\/cc @ahrtr @wojtek-t @logicalhan @mborsz @deads2k @jpbetz ","Expect this is related to https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072","Question to @wojtek-t @jpbetz @deads2k \r\n\r\nWant to propose `ConsistentWatchFromCache`, that will follow the same principles as https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/2340-Consistent-reads-from-cache\/README.md. Do we need a separate KEP?","> Want to propose `ConsistentWatchFromCache`, that will follow the same principles as https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/2340-Consistent-reads-from-cache\/README.md. Do we need a separate KEP?\r\n\r\nAlso, if I'm reading everything correctly, if we take [KEP-3157: Watch\/List](https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/3157-watch-list\/README.md) and remove the \"list\" part, it's very close to a \"consistent watch from cache\".\r\n\r\nI think I'd be okay with augmenting either of those KEPs to handle this case. I don't think a 3rd KEP covering the same underlying mechanism is needed. We could pick the best existing KEP and add this case to it, guarded by it's own feature gate and starting in alpha on the next release.\r\n\r\nWDTY @deads2k ?","+1 to pick the best existing KEP and add this case to it!","@serathius i believe there was a C++ based client that was part of the mix in https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072 (believe enabling that component triggered the issue)\r\n\r\nleft pointers here https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072#issuecomment-1959729121\r\n\r\n","Would I be correct in understanding that if `rv=\"\"` would lead to such a scenario, then any of the conditions here being true would lead to a similar situation?\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/ef19539cdc3980be2637a33b9ab01dae5da71e08\/staging\/src\/k8s.io\/apiserver\/pkg\/storage\/cacher\/cacher.go#L735\r\n\r\nEdit: nvm, I now see that the core issue is with watch exhaustion and not with every other request type.","\/triage accepted","In https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072, we saw the following:\r\n- etcd memory kept increasing during the incident\r\n- there were clients making large number of watch request w\/o resourceVersion e.g. `\/api\/v1\/watch\/pods?fieldSelector=status.phase!=Failed,status.phase!=Unknown,status.phase!=Succeeded,spec.nodeName=ip-10-32-88-156.ec2.internal&pretty=false`\r\n- `apiserver_watch_cache_events_received_total{resource=\"pods\"}` diverged between 2 apiserver instances. Same for `apiserver_watch_cache_events_dispatched_total{resource=\"pods\"}`\r\n- `prevKV=nil` are seen during the incident.\r\n- \r\nEDIT: add one more\r\n- In my repro cluster, I saw one etcd has much higher `etcd_debugging_mvcc_pending_events_total`(over 1 million) than the other etcd instances (< 20k).\r\n\r\n\r\nRepro can be found in https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123072#issuecomment-1960429338","> Also, if I'm reading everything correctly, if we take [KEP-3157: Watch\/List](https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-api-machinery\/3157-watch-list\/README.md) and remove the \"list\" part, it's very close to a \"consistent watch from cache\".\r\n\r\nYes - there is a plan to address that part there - we should just extend that KEP, the code change is pretty minimal (in fact I wanted this to happen anyway).","So from @mengqiy repro above, our observation is that when there is a large number of watches going to etcd for a resource type that has high churn - it's leading to watch events being lost. Next @chaochn47 is going to confirm if all these watches are reusing the same underlying client connection between API server -> etcd. Based on this our next steps would be understanding if the event loss is happening due to some connection-level limitation or server-side bottleneck.","For issue of\r\n\r\n>It should not be possible for direct watches to etcd to cause termination of watch cache watches. Connection for watch cache should be separated to avoid cross pollution\r\n\r\nI have confirmed that the watch starvation can be resolved if we separate watches into separate RPCs. Etcd client doesn't provide a direct access to managing streams, however client creates a create RPC for unique set of grpc metadata. A separate streams can be forced by adding unique grpc metadata in the watch context.\r\n\r\n It is important to note that having separate RPC streams impacts progress notifications needed for some features like ConsistentListFromCache. For example RequestProgressNotification will be sent to the watch stream with the same grpc metadata, meaning that we should be careful not to add metadata to watch opened by watch cache so it can receive progress notifications.\r\n \r\nOptions:\r\n* [recommended] Separate just watch opened by watch cache from all other. This should prevent watch cache being starved, while not adding a lot of overhead. This seems like make most sense short term.\r\n* Make each watch separate, this would prevent any 2 watches to starve each other. Need more testing to confirm the performance impact. This opens the question of \"why did etcd implement watch by multiplexing multiple watch calls on single watch grpc?\". Don't know the answer yet, however it seems like a bad idea to implement it's own stream multiplexing over a protocol that already provides it.\r\n\r\nDraft implementation of second option https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123483, implemented as it results in zero `PrevKV=nil` errors, making it easier to confirm. Also tested option 1, however it still results in `PrevKV=nil` errors as it doesn't prevent direct etcd watches from starving each other.","Did some testing performance of running each watch in separate RPC. Found that:\r\n* It requires additional 130 bytes per watch on etcd\r\n* It increases kube-apiserver watch CPU usage by 60%\r\n* It increases etcd watch CPU usage by 80%\r\n\r\nThis is based on testing 1000 watchers, watching a resource with 100 qps (100B objects):\r\n* kube-apiserver CPU usage grew from 3 cores to 5 cores\r\n* etcd CPU usage grew from 1 core to 1.8 cores\r\n\r\nIt might seem like a big increase, however the presented case is very extreme, that I would not expect such load to happen even in 5k clusters. And in that cases we are running already pretty big machines that 3 core difference is not as significant. cc @wojtek-t @mborsz for opinion.","cc @chaochn47 @ahrtr @jmhbnz ","> * Overload of the apiserver-etcd watch stream, causing starvation of other watches\r\n\r\nDo you mean that even running with an etcd version with the fix to https:\/\/github.com\/etcd-io\/etcd\/issues\/15402 still may run  into watch starvation due to (1) multiple sub streams sharing the same gRPC stream and (2) one of the sub stream is overloaded? What's the exact etcd version?\r\n\r\n\r\n\r\n> * This opens the question of \"why did etcd implement watch by multiplexing multiple watch calls on single watch grpc?\". Don't know the answer yet, however it seems like a bad idea to implement it's own stream multiplexing over a protocol that already provides it.\r\n\r\nDoes this answer your question? : `A single watch stream can multiplex many distinct watches by tagging events with per-watch identifiers. This multiplexing helps reducing the memory footprint and connection overhead on the core etcd cluster.`. Refer to https:\/\/etcd.io\/docs\/v3.5\/learning\/api\/#watch-streams\r\n\r\nAlso regarding \"`a protocol that already provides it`\", I am not aware of any such feature being supported by grpc. Not sure whether we are on the same page. Note that the [multiplex supported by gRPC](https:\/\/github.com\/grpc\/grpc-go\/blob\/master\/Documentation\/concurrency.md#clients) just means that a [ClientConn](https:\/\/pkg.go.dev\/google.golang.org\/grpc#ClientConn) can be reused\/shared by multiple client types, but the [multiplexing supported in etcd](https:\/\/etcd.io\/docs\/v3.5\/learning\/api\/#watch-streams) means that a single physical gprc stream can be shared by multiple logical sub stream\/watches. @dfawley does grpc support such feature?\r\n\r\n\r\n\r\n\r\n> Etcd client doesn't provide a direct access to managing streams, however client creates a create RPC for unique set of grpc metadata. A separate streams can be forced by adding unique grpc metadata in the watch context.\r\n\r\nYes, it's current implementation. One possible improvement on etcd side is to let clients to forcibly create a separate grpc stream using the [third parameter OpOption](https:\/\/github.com\/etcd-io\/etcd\/blob\/05aa9796df6a009881e2d13697faf5000915ab74\/client\/v3\/watch.go#L297). It's a backward compatible change, but we need to clearly clarify the use cases.","> Do you mean that even running with an etcd version with the fix to https:\/\/github.com\/etcd-io\/etcd\/issues\/15402 still may run into watch starvation due to (1) multiple sub streams sharing the same gRPC stream and (2) one of the sub stream is overloaded? What's the exact etcd version?\r\n\r\nYes, etcd version is v3.5.10 that should have a fix for https:\/\/github.com\/etcd-io\/etcd\/issues\/15402 implemented in https:\/\/github.com\/etcd-io\/etcd\/pull\/16750. Those starvation issues are of different type. The https:\/\/github.com\/etcd-io\/etcd\/issues\/15402 was about one type of request starving other ones. This is more about overloading watch RPC so that all watchers are not getting events. It still matters for K8s as watch opened by watch cache is much more important. ","> Does this answer your question? : A single watch stream can multiplex many distinct watches by tagging events with per-watch identifiers. This multiplexing helps reducing the memory footprint and connection overhead on the core etcd cluster.. Refer to https:\/\/etcd.io\/docs\/v3.5\/learning\/api\/#watch-streams\r\n\r\nYes, thanks.","> Also regarding \"a protocol that already provides it\", I am not aware of any such feature being supported by grpc. \r\n\r\nI meant the HTTP2 stream multiplection. Basically you can send multiple concurrent requests and get responses in any order. This can be done by just sending watch RPC per watch request. GRPC will multiplex them underneath. My testing confirms that this comes with overhead, 60%-80% on both client and etcd side. So, there is a tradeoff, by fitting all the watches on single RPC you reduce CPU usage, but all of them will share the same throughput limit. ","Based on data in https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123448#issuecomment-1963071077 **I'm  proposing to move watch cache to separate watch RPC (no change to etcd watches) and to backport it to all supported releases**. The CPU usage for single stream should be negligible. This way we address the main problem protect, watch cache starvation. For a complete watch starvation fix we can wait for consistent watch from cache.","Assuming that the proposed change will be well encapsulated (didn't yet look into the code), I would be supportive for the backport [and I agree that separateing just watchcache seems like a good tradeoff for what we should do]","Just my guess to make the problem statement of watch starvation more clear:\r\n\r\nIs this [channel](https:\/\/github.com\/etcd-io\/etcd\/blob\/b9eeaf3a18d95df3299015b2ffe3ad4cfa39f800\/server\/mvcc\/watchable_store.go#L112) shared by the 100 watchers buffer being exhausted due to the rate of consuming this channel is lower than generating data and feeding into the channel (because of gRPC limit)? This would cause all events dropped in etcd server, correct? \r\n\r\n@serathius @ahrtr","Proposed fix for watch cache starvation issue https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123532","> @dfawley does grpc support such feature?\r\n\r\nNo, gRPC does not support any kind of stream multiplexing.  I think the subsequent discussions in this thread are all accurate.  It's perfectly valid to only allow one watch per stream and start a new stream for every watcher, but it's also very normal to support multiple watches on a single stream and demultiplex to the relevant local watcher (in the application).  For example, we do this to implement xDS support, where a single ADS stream handles all subscriptions and the xdsClient internally notifies individual callbacks based upon what resources were updated.","\/cc","> So, there is a tradeoff, by fitting all the watches on single RPC you reduce CPU usage, but all of them will share the same throughput limit.\r\n\r\nJust curious if anyone has the clue what exact gRPC stream throughput limit we are hitting with thousands of watches multiplexed together? Looks like network problems between client and server is not in the scope of discussion since the reproduce runs locally.\r\n\r\n@wojtek-t @liggitt @ahrtr @serathius @dfawley @mborsz "],"labels":["kind\/bug","sig\/api-machinery","triage\/accepted","sig\/etcd"]},{"title":"Cleanup to commonize utility function for CronJob and Job","body":"This is a follow up to the discussion https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123380#discussion_r1494914346\r\n\r\nBoth controllers currently use the same `IsJobFinished` function, semantically [CronJob](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/47737eca1e5daf486d6a2ef4c642655531e776d9\/pkg\/controller\/cronjob\/utils.go#L290), and [Job](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/47737eca1e5daf486d6a2ef4c642655531e776d9\/pkg\/controller\/job\/utils.go#L26).\r\n\r\nThe proposed idea is to place them under `pkg\/controller\/job\/util`. Potentially more functions might be moved there, but the one catches attention already.","comments":["\/sig apps\r\n\/good-first-issue","@mimowo: \n\tThis request has been marked as suitable for new contributors.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/help-wanted.md#good-first-issue) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `\/remove-good-first-issue` command.\n\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubernetes\/kubernetes\/issues\/123445):\n\n>\/sig apps\r\n>\/good-first-issue\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/cc @kaisoz ","\/assign\r\n\r\nThanks @mimowo !","\/triage accepted","@mimowo \r\ni have good knowledge of golang and beginner to writing k8s controller. Currently working on issue of kubevirt. Could you please assigned me with beginner friendly task ?","\/assign","is this issue still open wanted to work on it ","Hey @hrishikeshdkakkad @kushalShukla-web!\n\nThis issue has been fixed in #123537 and is ready for approval. It didn't happen yet because approvers are busy with the code freeze and Kubecon.\n\n@anishbista60 I suggest you to have a look at the open issues with the `good-first-issue` label which are still free :). The [Getting started](https:\/\/www.kubernetes.dev\/docs\/guide\/) guide will surely help","@kaisoz Every issue has assignee working on "],"labels":["sig\/apps","help wanted","good first issue","triage\/accepted"]},{"title":"memorymanager: avoid violating NUMA node memory allocation rule","body":"\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\/kind bug\r\n\r\n#### What this PR does \/ why we need it:\r\nAccording to https:\/\/kubernetes.io\/blog\/2021\/08\/11\/kubernetes-1-22-feature-memory-manager-moves-to-beta\/#single-vs-cross-numa-node-allocation and to the design introduce in the memory manager KEP: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-node\/1769-memory-manager the NUMA node can not have both single and cross NUMA node allocations.\r\n\r\nThere're cases when the chosen affinity hint does not align with the rule above but the pod get admitted anyway.\r\n\r\nThe implications are incosistent admission\/rejection behaviors from memory manager side.\r\n\r\nIn order to fix the issue, we should validate that the affinity hint that has been chosen is not violating the above rule.\r\n\r\nA detailed example for demonstrating the issue:\r\nAvailable resources of `NodeA`\r\n``` yaml\r\nNodeA:\r\n  NUMA0:\r\n    cpu: 2\r\n    memory: 1024 MBi\r\n  NUMA1:\r\n    cpu: 2\r\n    memory: 1024 MBi\r\n```\r\nUpcoming pods:\r\n```yaml\r\nPod1: #(Guaranteed)\r\n  cpu: 1\r\n  memory: 512 MBi\r\n\r\nPod2: #(Guaranteed)\r\n  cpu: 2\r\n  memory: 1280 MBi\r\n```\r\n\r\nt0: `Pod1` get admitted and lands on NUMA0\r\nt1: current available resources state:\r\n``` yaml\r\nNodeA:\r\n  NUMA0:\r\n    cpu: 1\r\n    memory: MBi\r\n  NUMA1:\r\n    cpu: 2\r\n    memory: 1Gi\r\n```\r\nNUMA0 marked by the memory manager as single NUMA memory allocation\r\n \r\nt2: `Pod2` gets rejected (as expected) because it needs memory from both NUMAs in order to satisfy it request and NUMA0 already marked as single NUMA group so it cannot serves allocation which spans across NUMAs.\r\n\r\nBut, assuming `Pod2` would request additional CPU:\r\n``` yaml\r\nPod2 #(Guaranteed):\r\n  cpu: 3 #(instead of 2)\r\n  memory: 1280 MBi\r\n```\r\nt2: pod gets admitted which is inconsistent and violates the NUMA grouping rule.\r\n\r\n#### Which issue(s) this PR fixes:\r\nFixes #120733\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n","comments":["Hi @Tal-or. Thanks for your PR.\n\nI'm waiting for a [kubernetes](https:\/\/github.com\/orgs\/kubernetes\/people) member to verify that this patch is reasonable to test. If it is, they should reply with `\/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https:\/\/git.k8s.io\/community\/community-membership.md#member) to skip this step.\n\nOnce the patch is verified, the new status will be reflected by the `ok-to-test` label.\n\nI understand the commands that are listed [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>\n","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123443#\" title=\"Author self-approved\">Tal-or<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https:\/\/github.com\/klueska) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/kubelet\/cm\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/cm\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"klueska\"]} -->","\/cc @ffromani @swatisehgal ","\/ok-to-test\r\n\/triage accepted\r\n\/priority important-longterm","\/test pull-crio-cgroupv1-node-e2e-resource-managers\r\n\/test pull-kubernetes-node-e2e-containerd-serial-ec2\r\n\/test pull-kubernetes-node-kubelet-serial-containerd\r\n\/test pull-kubernetes-node-kubelet-serial-cpu-manager\r\n\/test pull-kubernetes-node-kubelet-serial-memory-manager\r\n\/test pull-kubernetes-node-kubelet-serial-topology-manager","\/test pull-kubernetes-node-kubelet-serial-containerd\r\n","\/retest","\/test pull-kubernetes-node-kubelet-serial-containerd"],"labels":["kind\/bug","area\/kubelet","sig\/node","size\/L","release-note-none","cncf-cla: yes","priority\/important-longterm","ok-to-test","triage\/accepted"]},{"title":"[FG:InPlacePodVerticalScaling] Pod Resize - resize stuck in \"InProgress\"","body":"### What happened?\r\n\r\nI am trying the the feature gate \"InPlacePodVerticalScaling\". I tried on multiple environment\/versions and the result is always the same. Resize is forever stuck in \"InProgress\". \r\n\r\nI followed the doc: https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/resize-container-resources\/\r\n\r\n### What did you expect to happen?\r\n\r\nResize field in pod status should go to `complete`\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\nFollow this doc: https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/resize-container-resources\/\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\nI tested with kubernetes version 1.27 and 1.29 with the same result.\r\ncontainerd version being v1.6.28.\r\n\r\nThe 2 requirements being (from my understanding):\r\nkubernetes >=v1.27\r\ncontainerd >=v1.6.9\r\n\r\n\r\n### Cloud provider\r\n\r\nI tried on GKE and locally with Minikube\r\n\r\n\r\n### OS version\r\n\r\nmac sonoma 14.1.2 (m1)","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig node","do you have any logs or detail info?","I don't have more info than this unfortunately. There are no logs, it is in `InProgress` state forever.\r\n\r\nI tested it with minikube: `minikube start --kubernetes-version=v1.29.2 --feature-gates=InPlacePodVerticalScaling=true\r\n`","when its in ``InProgress`` you can desc the pod I think, but that might lack of info as well..","Yes, i can `desc` the Pod. There are no related info on the resize unfortunately. Did you succeed to recreate the bug ?","Hi,\r\n\r\nI think this is a feature request , not a bug. Perhaps what you are asking for is, to add value `complete` in resize field in the Pod's status to indicate completion of successful resize ? As it is today it is expected behaviour, `complete` is not a defined type  \r\n\r\nhttps:\/\/github.com\/kubernetes\/kubernetes\/blob\/9043ce05c125091c0cb5519206fd90d311abd8c8\/staging\/src\/k8s.io\/api\/core\/v1\/types.go#L3089-L3098\r\n\r\nRelated info on the resize can be observed through the allocatedResources values, those have been updated to reflect the new desired CPU requests. If allocatedResources.cpu is equal to resources.limits.cpu indicates that node was able to accommodate the increased CPU resource needs.\r\n\r\n","Ok thanks @esotsal. What is the point of the `resize` field then ?","> Ok thanks @esotsal. What is the point of the `resize` field then ?\r\n\r\nIt is used internally in code to control the flow, in addition it helps to troubleshoot failures. \r\n\r\nI understand, from end user perspective\/automation, the benefit of your proposal i.e. introduction of a PodResizeStatus value to cater when Pod resources resize has been accepted by node and has been actuated.","I verified same thing by spinning up local cluster using k8s `hack\/local-up-cluster.sh` and its works as expected and I dont see any Inprogress resize status even after updating the cpu, I followed the same steps  from [here](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/resize-container-resources\/).\r\n\r\nFor reference\r\nInitially\r\n```\r\n  phase: Running\r\n  podIP: 10.88.0.4\r\n  podIPs:\r\n  - ip: 10.88.0.4\r\n  - ip: 2001:db8:4860::4\r\n  qosClass: Guaranteed\r\n  resize: InProgress\r\n  startTime: \"2024-03-06T10:27:27Z\"\r\n```\r\nEventually\r\n```\r\n# kubectl get pod qos-demo-5 --output=yaml --namespace=qos-example\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  creationTimestamp: \"2024-03-06T10:27:27Z\"\r\n  name: qos-demo-5\r\n  namespace: qos-example\r\n  resourceVersion: \"563\"\r\n  uid: b0f36536-92cb-4ea0-b80c-212fe695d2f8\r\nspec:\r\n  containers:\r\n  - image: nginx\r\n    imagePullPolicy: Always\r\n    name: qos-demo-ctr-5\r\n    resizePolicy:\r\n    - resourceName: cpu\r\n      restartPolicy: NotRequired\r\n    - resourceName: memory\r\n      restartPolicy: NotRequired\r\n    resources:\r\n      limits:\r\n        cpu: 800m\r\n        memory: 200Mi\r\n      requests:\r\n        cpu: 800m\r\n        memory: 200Mi\r\n    terminationMessagePath: \/dev\/termination-log\r\n    terminationMessagePolicy: File\r\n    volumeMounts:\r\n    - mountPath: \/var\/run\/secrets\/kubernetes.io\/serviceaccount\r\n      name: kube-api-access-w7ddb\r\n      readOnly: true\r\n  dnsPolicy: ClusterFirst\r\n  enableServiceLinks: true\r\n  nodeName: 127.0.0.1\r\n  preemptionPolicy: PreemptLowerPriority\r\n  priority: 0\r\n  restartPolicy: Always\r\n  schedulerName: default-scheduler\r\n  securityContext: {}\r\n  serviceAccount: default\r\n  serviceAccountName: default\r\n  terminationGracePeriodSeconds: 30\r\n  tolerations:\r\n  - effect: NoExecute\r\n    key: node.kubernetes.io\/not-ready\r\n    operator: Exists\r\n    tolerationSeconds: 300\r\n  - effect: NoExecute\r\n    key: node.kubernetes.io\/unreachable\r\n    operator: Exists\r\n    tolerationSeconds: 300\r\n  volumes:\r\n  - name: kube-api-access-w7ddb\r\n    projected:\r\n      defaultMode: 420\r\n      sources:\r\n      - serviceAccountToken:\r\n          expirationSeconds: 3607\r\n          path: token\r\n      - configMap:\r\n          items:\r\n          - key: ca.crt\r\n            path: ca.crt\r\n          name: kube-root-ca.crt\r\n      - downwardAPI:\r\n          items:\r\n          - fieldRef:\r\n              apiVersion: v1\r\n              fieldPath: metadata.namespace\r\n            path: namespace\r\nstatus:\r\n  conditions:\r\n  - lastProbeTime: null\r\n    lastTransitionTime: \"2024-03-06T10:27:31Z\"\r\n    status: \"True\"\r\n    type: PodReadyToStartContainers\r\n  - lastProbeTime: null\r\n    lastTransitionTime: \"2024-03-06T10:27:27Z\"\r\n    status: \"True\"\r\n    type: Initialized\r\n  - lastProbeTime: null\r\n    lastTransitionTime: \"2024-03-06T10:27:31Z\"\r\n    status: \"True\"\r\n    type: Ready\r\n  - lastProbeTime: null\r\n    lastTransitionTime: \"2024-03-06T10:27:31Z\"\r\n    status: \"True\"\r\n    type: ContainersReady\r\n  - lastProbeTime: null\r\n    lastTransitionTime: \"2024-03-06T10:27:27Z\"\r\n    status: \"True\"\r\n    type: PodScheduled\r\n  containerStatuses:\r\n  - allocatedResources:\r\n      cpu: 800m\r\n      memory: 200Mi\r\n    containerID: containerd:\/\/e4cde25e7305421229b0560b59907693d5df1aaa0a0bca9aae2544624e9070fe\r\n    image: docker.io\/library\/nginx:latest\r\n    imageID: docker.io\/library\/nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107\r\n    lastState: {}\r\n    name: qos-demo-ctr-5\r\n    ready: true\r\n    resources:\r\n      limits:\r\n        cpu: 800m\r\n        memory: 200Mi\r\n      requests:\r\n        cpu: 800m\r\n        memory: 200Mi\r\n    restartCount: 0\r\n    started: true\r\n    state:\r\n      running:\r\n        startedAt: \"2024-03-06T10:27:30Z\"\r\n  hostIP: 127.0.0.1\r\n  hostIPs:\r\n  - ip: 127.0.0.1\r\n  phase: Running\r\n  podIP: 10.88.0.4\r\n  podIPs:\r\n  - ip: 10.88.0.4\r\n  - ip: 2001:db8:4860::4\r\n  qosClass: Guaranteed\r\n```","@MathieuCesbron I believe you are hitting some corner cases, may be can you verify your node capacity vs capacity you are setting in pod. If possible providing kubelet log would help.","Thanks @Karthik-K-N for the detailed test. I will  try more and check if there is an issue in my cluster maybe.","\/triage needs-information\r\n\/assign @MathieuCesbron "],"labels":["kind\/bug","sig\/node","triage\/needs-information","needs-triage"]},{"title":"Improving kubectl get output","body":"Signed-off-by: Ritikaa96 <ritika@india.nec.com\r\n\r\n\r\n#### What this PR does \/ why we need it:\r\n Adding namespace flag & examples\r\nkubectl get --help states the following even though we can use --namespace to get details for other namespace resources.\r\n```shell\r\nDisplay one or many resources.\r\n\r\n Prints a table of the most important information about the specified resources.\r\nYou can filter the list using a label selector and the --selector flag. If the\r\ndesired resource type is namespaced you will only see results in your current\r\nnamespace unless you pass --all-namespaces.\r\n...\r\n```\r\nThe description should mention `--namespace ` as this can show objects\/resources from different namespace as well\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #https:\/\/github.com\/kubernetes\/kubectl\/issues\/1555\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\nNONE\r\n\r\n#### Does this PR introduce a user-facing change?\r\nNONE\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\nNONE","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123440#\" title=\"Author self-approved\">Ritikaa96<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [eddiezane](https:\/\/github.com\/eddiezane) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"eddiezane\"]} -->","\/sig cli\r\n\/kind bug\r\n","\/kind cleanup\r\n\/release-note-none","Made some changes PTAL @ardaguclu ","\/easycla"],"labels":["kind\/bug","kind\/cleanup","area\/kubectl","size\/S","release-note-none","sig\/cli","cncf-cla: yes","needs-priority","needs-triage"]},{"title":"[Flaking Test] [sig-node] Summary API [NodeConformance] when querying \/stats\/summary should report resource usage through the stats api","body":"### Which jobs are flaking?\n\n- ci-cgroupv2-containerd-node-e2e-ec2\r\n- ci-kubernetes-node-arm64-e2e-containerd-ec2\r\n- ci-crio-cgroupv1-evented-pleg\n\n### Which tests are flaking?\n\nE2eNode Suite [It] [sig-node] Summary API [NodeConformance] when querying \/stats\/summary should report resource usage through the stats api\n\n### Since when has it been flaking?\n\nlong time ago?\n\n### Testgrid link\n\nhttps:\/\/testgrid.k8s.io\/sig-node-containerd#ci-cgroupv2-containerd-node-e2e-ec2\n\n### Reason for failure (if possible)\n\n\r\n```\r\n{ failed [FAILED] Failed after 15.027s.\r\nExpected\r\n    <string>: Summary\r\nto match fields: {\r\n.Pods[summary-test-6628::stats-busybox-0].CPU:\r\n\tExpected\r\n\t    <string>: CPUStats\r\n\tto match fields: {\r\n\t.UsageNanoCores:\r\n\t\tExpected\r\n\t\t    <uint64>: 8215\r\n\t\tto be >=\r\n\t\t    <int>: 10000\r\n\t}\r\n\t\r\n}\r\nIn [It] at: k8s.io\/kubernetes\/test\/e2e_node\/summary_test.go:348 @ 02\/19\/24 22:13:00.409\r\n}\r\n```\n\n### Anything else we need to know?\n\nhttps:\/\/prow.k8s.io\/view\/gs\/kubernetes-jenkins\/logs\/ci-cgroupv2-containerd-node-e2e-ec2\/1759699415758540800\n\n### Relevant SIG(s)\n\n\/sig node\r\n\/cc @kannon92 \r\nit seems no only the process will fail this test, it also flake for CPU Stats","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["sig\/node","kind\/flake","needs-triage"]},{"title":"WIP: add PodReplacementPolicy for Deployments","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n\/kind feature\r\n\r\n#### What this PR does \/ why we need it\r\n\r\nA new deployment field, .spec.podReplacementPolicy, can be used to determine whether to replace terminating or terminated pods in deployments with new pods.\r\n\r\nhttps:\/\/github.com\/kubernetes\/enhancements\/issues\/3973\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\nprerequisite: https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123670\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n[KEP]: https:\/\/github.com\/kubernetes\/enhancements\/blob\/master\/keps\/sig-apps\/3973-consider-terminating-pods-deployment\/README.md\r\n```\r\n","comments":["Adding the \"do-not-merge\/release-note-label-needed\" label because no release-note block was detected, please follow our [release note process](https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md) to remove it.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123430#\" title=\"Author self-approved\">atiratree<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[cmd\/kube-controller-manager\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kube-controller-manager\/OWNERS)**\n- **[pkg\/apis\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/apis\/OWNERS)**\n- **[pkg\/controller\/deployment\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/deployment\/OWNERS)**\n- **[pkg\/controller\/replicaset\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/controller\/replicaset\/OWNERS)**\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[pkg\/registry\/apps\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/registry\/apps\/OWNERS)**\n- **[staging\/src\/k8s.io\/api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/api\/OWNERS)**\n- **[staging\/src\/k8s.io\/client-go\/applyconfigurations\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/client-go\/applyconfigurations\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubectl\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubectl\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","I have posted the implementation, but I am still missing the tests, so we will not be able to merge this feature in 1.30 due to not enough time. \r\n\r\nEven though it might be possible to finish this if we had enough review capacity, I would prefer not to for the following reasons:\r\n- to allow proper review process to make sure we have not looked over something\r\n- I have identified that we will need a new annotation `deployment.kubernetes.io\/replicaset-replicas-before-scale` (please see https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123670) to support partial scalling of older revision replica sets for deployments with TerminationComplete podReplacementPolicy. It would be great if we could merge this now, to make sure N-1 kubectl works with cluster that turn on the DeploymentPodReplacementPolicy feature.\r\n","PR needs rebase.\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>"],"labels":["area\/test","area\/kubectl","sig\/api-machinery","needs-rebase","size\/XXL","kind\/api-change","kind\/feature","sig\/apps","sig\/cli","cncf-cla: yes","sig\/testing","do-not-merge\/release-note-label-needed","do-not-merge\/work-in-progress","area\/code-generation","needs-priority","needs-triage"]},{"title":"WIP: promote PDBUnhealthyPodEvictionPolicy to GA","body":"<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues\/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nThe PodDisruptionBudget `spec.unhealthyPodEvictionPolicy` field has graduated to GA. This field may be set to `AlwaysAllow` to always allow unhealthy pods covered by the PodDisruptionBudget to be evicted.\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https:\/\/help.github.com\/en\/articles\/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n- [KEP]: https:\/\/github.com\/kubernetes\/enhancements\/tree\/master\/keps\/sig-apps\/3017-pod-healthy-policy-for-pdb\r\n```\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123428#\" title=\"Author self-approved\">atiratree<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https:\/\/github.com\/deads2k) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[pkg\/features\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/features\/OWNERS)**\n- **[pkg\/registry\/core\/pod\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/registry\/core\/pod\/OWNERS)**\n- **[pkg\/registry\/policy\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/registry\/policy\/OWNERS)**\n- **[test\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/test\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"deads2k\"]} -->"],"labels":["area\/test","sig\/node","release-note","size\/L","sig\/auth","cncf-cla: yes","sig\/testing","do-not-merge\/work-in-progress","needs-priority","needs-triage","do-not-merge\/needs-kind"]},{"title":"kubelet: allow disabling metrics","body":"\r\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https:\/\/git.k8s.io\/community\/contributors\/guide\/first-contribution.md#your-first-contribution and developer guide https:\/\/git.k8s.io\/community\/contributors\/devel\/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR\/issue labels, read here:\r\nhttps:\/\/git.k8s.io\/community\/contributors\/devel\/sig-release\/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https:\/\/git.k8s.io\/community\/contributors\/devel\/sig-testing\/testing.md\r\n4. If you want *faster* PR reviews, read how: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n\/kind feature\r\n\r\n<!--\r\nAdd one of the following kinds:\r\n\/kind bug\r\n\/kind cleanup\r\n\/kind documentation\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n\/kind api-change\r\n\/kind deprecation\r\n\/kind failing-test\r\n\/kind flake\r\n\/kind regression\r\n-->\r\n\r\n#### What this PR does \/ why we need it: Allow users to disable metrics using the respective component-base API.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https:\/\/git.k8s.io\/community\/contributors\/guide\/release-notes.md\r\n-->\r\n```release-note\r\nAllow disabling metrics for Kubelet by specifying them using the --disabled-metrics flag. \r\n```\r\n","comments":["This PR [may require API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#what-apis-need-to-be-reviewed).\n\nIf so, when the changes are ready, [complete the pre-review checklist and request an API review](https:\/\/git.k8s.io\/community\/sig-architecture\/api-review-process.md#mechanics).\n\nStatus of requested reviews is tracked in the [API Review project](https:\/\/github.com\/orgs\/kubernetes\/projects\/169).","[APPROVALNOTIFIER] This PR is **NOT APPROVED**\n\nThis pull-request has been approved by: *<a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/pull\/123426#\" title=\"Author self-approved\">rexagod<\/a>*\n**Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https:\/\/github.com\/smarterclayton) for approval. For more information see [the Kubernetes Code Review Process](https:\/\/git.k8s.io\/community\/contributors\/guide\/owners.md#the-code-review-process).\n\nThe full list of commands accepted by this bot can be found [here](https:\/\/go.k8s.io\/bot-commands?repo=kubernetes%2Fkubernetes).\n\n<details open>\nNeeds approval from an approver in each of these files:\n\n- **[api\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/api\/OWNERS)**\n- **[cmd\/kubelet\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/cmd\/kubelet\/OWNERS)**\n- **[pkg\/generated\/openapi\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/generated\/openapi\/OWNERS)**\n- **[pkg\/kubelet\/apis\/config\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/pkg\/kubelet\/apis\/config\/OWNERS)**\n- **[staging\/src\/k8s.io\/kubelet\/config\/OWNERS](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/master\/staging\/src\/k8s.io\/kubelet\/config\/OWNERS)**\n\nApprovers can indicate their approval by writing `\/approve` in a comment\nApprovers can cancel approval by writing `\/approve cancel` in a comment\n<\/details>\n<!-- META={\"approvers\":[\"smarterclayton\"]} -->","\/remove-sig api-machinery\r\n\/sig node","\/sig instrumentation","@dashpole @dgrisonnet Reiterating on our discussion from the latest SIG call, it seems there is no equivalent `metrics\/api\/vX` the same way `Configuration`s exist for logging and tracing signals. Do you think that's a blocker and should be implemented and incorporated in this patch before it goes in?","I'm OK proceeding without it if needed.  We already have other fields that will need to be moved.  But I don't think it should be that large of a change.  We don't need to migrate existing fields into it this release, if that helps","\/triage accepted\r\n\/priority important-longterm\r\n\r\nIt seems there it was discussion about the rationale for this change. Could you please record it in the commit message and\/or PR description? IOW, what's the benefit for \"Allow users to disable metrics using the respective component-base API.\" ?\r\n\r\n","Since we missed code freeze, we can take our time and get this in 1.31.  Lets try and move to an overall metrics config during tha timeframe","\/assign"],"labels":["area\/kubelet","sig\/node","release-note","size\/S","kind\/api-change","kind\/feature","cncf-cla: yes","sig\/instrumentation","priority\/important-longterm","area\/code-generation","triage\/accepted"]},{"title":"replicaset listed as having 0 replicas when pod is still in \"Running\" state","body":"### What happened?\r\n\r\nWhen updating a deployment with a new image, the previous replicaset begins to terminate. This triggers termination of all associated pods. The behavior I'm seeing is that the \"Phase\" of the pod is still \"Running\", but the status on the replicaset says that the number of replicas is \"0\". See image:\r\n\r\n![Screen Shot 2024-02-21 at 9 34 31 AM](https:\/\/github.com\/kubernetes\/kubernetes\/assets\/20423189\/681cb219-e722-4dc0-a823-889b61c9a3d2)\r\n\r\nIn the right-hand panel, `bash-5974dbdb86-rcjcl` is terminating: \r\n<img width=\"671\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/20423189\/d1c23574-ed8e-446c-8509-2e0cd75cd053\">\r\n\r\nOn the left hand panel I'm doing a constant watch of the pod itself, which lists as \"Running\": \r\n<img width=\"251\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/20423189\/3134900f-31fc-49ac-af15-85c7e2b7c0fb\">\r\n\r\nFinally, the status of the replicaset says that the number of replicas is 0: \r\n<img width=\"175\" alt=\"image\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/20423189\/a31dc184-590b-4d36-ad73-68d49cd86499\">\r\n\r\n### What did you expect to happen?\r\n\r\nFrom the [source code](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/442a69c3bdf6fe8e525b05887e57d89db1e2f3a5\/pkg\/controller\/replicaset\/replica_set_utils.go#L123), I would expect the status to be set to the \"filtered pods\" ([link](https:\/\/github.com\/kubernetes\/kubernetes\/blob\/acc55500bcca3ece1864069ddb6b9e42f3b11db6\/pkg\/controller\/controller_utils.go#L932), which is defined as the set of \"active\" pods (not PodSucceeded or PodFailed)). With this logic, the replicas should not be 0. In fact, logically I would expect that I could depend on the replicas number to determine whether all replicas have stopped running, but this is not the case in practice.\r\n\r\n### How can we reproduce it (as minimally and precisely as possible)?\r\n\r\n1. Deploy an application. Ex:\r\n[bash.yaml.txt](https:\/\/github.com\/kubernetes\/kubernetes\/files\/14363491\/bash.yaml.txt)\r\n2. Start a watch on the pod created, as well as the replicaset.\r\n3. Update the image name, so that a new replicaset is created and the old one begins to be scale down.\r\n4. Notice that the replicas are listed as 0 before the pod has finished terminating.\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Kubernetes version\r\n\r\n<details>\r\n\r\n```console\r\n$ kubectl version\r\nClient Version: v1.29.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.27.1\r\nWARNING: version difference between client (1.29) and server (1.27) exceeds the supported minor version skew of +\/-1\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Cloud provider\r\n\r\n<details>\r\n\r\n```console\r\n$ kind version\r\nkind v0.17.0 go1.19.4 darwin\/arm64\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### OS version\r\n\r\n<details>\r\n\r\n```console\r\n# On Linux:\r\n$ cat \/etc\/os-release\r\n# paste output here\r\n$ uname -a\r\n# paste output here\r\n\r\n# On Windows:\r\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\r\n# paste output here\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Install tools\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Container runtime (CRI) and version (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\r\n\r\n<details>\r\n\r\n<\/details>\r\n","comments":["This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage\/accepted` label and provide further guidance.\n\nThe `triage\/accepted` label can be added by org members by writing `\/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>","\/sig apps","https:\/\/github.com\/kubernetes\/kubernetes\/blob\/acc55500bcca3ece1864069ddb6b9e42f3b11db6\/pkg\/controller\/controller_utils.go#L945-L949\r\n\r\nThe old pod went to `Terminating` because its `DeletionTimestamp` had been set.  According to the code above,  it was filtered out since its `DeletionTimestamp` was not nil.","I try to recreate this issue. It works fine for me. I can not see the replicaset count to be zero when the pod is to 1. \r\n```\r\n kubectl get replicaset -w\r\nNAME              DESIRED   CURRENT   READY   AGE\r\nbash-66f5679547   1         0         0       0s\r\nbash-66f5679547   1         0         0       0s\r\nbash-66f5679547   1         1         0       0s\r\nbash-66f5679547   1         1         1       22s\r\nbash-6bf68d4647   1         0         0       0s\r\nbash-6bf68d4647   1         0         0       0s\r\nbash-6bf68d4647   1         1         0       0s\r\nbash-6bf68d4647   1         1         1       1s\r\nbash-66f5679547   0         1         1       71s\r\nbash-66f5679547   0         1         1       71s\r\nbash-66f5679547   0         0         0       71s\r\n```\r\n\r\n```\r\n% kubectl get pods -w\r\nNAME                    READY   STATUS    RESTARTS   AGE\r\nbash-66f5679547-x4v9t   0\/1     Pending   0          0s\r\nbash-66f5679547-x4v9t   0\/1     Pending   0          0s\r\nbash-66f5679547-x4v9t   0\/1     ContainerCreating   0          0s\r\nbash-66f5679547-x4v9t   1\/1     Running             0          22s\r\nbash-6bf68d4647-w4nxm   0\/1     Pending             0          0s\r\nbash-6bf68d4647-w4nxm   0\/1     Pending             0          0s\r\nbash-6bf68d4647-w4nxm   0\/1     ContainerCreating   0          0s\r\nbash-6bf68d4647-w4nxm   1\/1     Running             0          1s\r\nbash-66f5679547-x4v9t   1\/1     Terminating         0          71s\r\nbash-66f5679547-x4v9t   0\/1     Terminating         0          101s\r\nbash-66f5679547-x4v9t   0\/1     Terminating         0          102s\r\nbash-66f5679547-x4v9t   0\/1     Terminating         0          102s\r\nbash-66f5679547-x4v9t   0\/1     Terminating         0          102s\r\n```\r\n\r\n\r\nThe `kubectl get pods\/bash-6bf68d4647-w4nxm -oyaml` gives as\r\n<img width=\"435\" alt=\"Screenshot 2024-03-20 at 16 43 57\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/51718908\/b64e7ad5-50d1-4452-8c86-c04cd9ccacba\">\r\n\r\n\r\nThe `kubectl get replicaset\/bash-6bf68d4647-w4nxm -oyaml` gives as\r\n<img width=\"764\" alt=\"Screenshot 2024-03-20 at 16 43 36\" src=\"https:\/\/github.com\/kubernetes\/kubernetes\/assets\/51718908\/21042acd-f0a4-4726-a056-da9e3d079df0\">\r\n\r\n\r\n\r\nI tried this on minikube with kubectl, \r\n```\r\n % kubectl version\r\nClient Version: v1.29.3\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\nServer Version: v1.29.0\r\n```\r\n\r\nCan you please give your thoughts @SirNexus @neolit123 @RyanAoh. Please correct me, if I did a wrong flow. Your review is highly appreciated. ","Kindly note: that I can view the details only for the new pod and replica set. `bash-6bf68d4647`"],"labels":["kind\/bug","sig\/apps","needs-triage"]}]